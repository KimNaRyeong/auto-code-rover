[
    {
        "rel_file_path": "xarray/core/merge.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/merge.py",
        "start": 303,
        "end": 328,
        "class_name": null,
        "method_name": "merge_coordinates_without_align",
        "code": "303 def merge_coordinates_without_align(\n304     objects: \"List[Coordinates]\",\n305     prioritized: Mapping[Hashable, MergeElement] = None,\n306     exclude_dims: AbstractSet = frozenset(),\n307 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n308     \"\"\"Merge variables/indexes from coordinates without automatic alignments.\n309 \n310     This function is used for merging coordinate from pre-existing xarray\n311     objects.\n312     \"\"\"\n313     collected = collect_from_coordinates(objects)\n314 \n315     if exclude_dims:\n316         filtered: Dict[Hashable, List[MergeElement]] = {}\n317         for name, elements in collected.items():\n318             new_elements = [\n319                 (variable, index)\n320                 for variable, index in elements\n321                 if exclude_dims.isdisjoint(variable.dims)\n322             ]\n323             if new_elements:\n324                 filtered[name] = new_elements\n325     else:\n326         filtered = collected\n327 \n328     return merge_collected(filtered, prioritized)\n",
        "intended_behavior": "This method should more gracefully handle coordinate merging for in-place operations like `+=` by possibly introducing a preliminary step that aligns the coordinates of the two `DataArray` objects before attempting to merge them. The method should ensure that in-place addition behaves similarly to out-of-place addition (`A + B`), which successfully combines the arrays regardless of the order of their coordinates. This may involve adjusting how priorities are determined or how conflicts are resolved when the dimensions are the same but the coordinate values or orders differ. The ultimate goal is to allow `A += B` to successfully complete by ensuring that conflicting indexes are resolved or aligned in such a way that mirrors the successful behavior of `A = A + B`."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 216,
        "end": 3727,
        "class_name": "DataArray",
        "method_name": null,
        "code": "216 class DataArray(AbstractArray, DataWithCoords):\n217     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n218 \n219     DataArray provides a wrapper around numpy ndarrays that uses labeled\n220     dimensions and coordinates to support metadata aware operations. The API is\n221     similar to that for the pandas Series or DataFrame, but DataArray objects\n222     can have any number of dimensions, and their contents have fixed data\n223     types.\n224 \n225     Additional features over raw numpy arrays:\n226 \n227     - Apply operations over dimensions by name: ``x.sum('time')``.\n228     - Select or assign values by integer location (like numpy): ``x[:10]``\n229       or by label (like pandas): ``x.loc['2014-01-01']`` or\n230       ``x.sel(time='2014-01-01')``.\n231     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n232       dimensions (known in numpy as \"broadcasting\") based on dimension names,\n233       regardless of their original order.\n234     - Keep track of arbitrary metadata in the form of a Python dictionary:\n235       ``x.attrs``\n236     - Convert to a pandas Series: ``x.to_series()``.\n237 \n238     Getting items from or doing mathematical operations with a DataArray\n239     always returns another DataArray.\n240     \"\"\"\n241 \n242     _cache: Dict[str, Any]\n243     _coords: Dict[Any, Variable]\n244     _indexes: Optional[Dict[Hashable, pd.Index]]\n245     _name: Optional[Hashable]\n246     _variable: Variable\n247 \n248     __slots__ = (\n249         \"_cache\",\n250         \"_coords\",\n251         \"_file_obj\",\n252         \"_indexes\",\n253         \"_name\",\n254         \"_variable\",\n255         \"__weakref__\",\n256     )\n257 \n258     _groupby_cls = groupby.DataArrayGroupBy\n259     _rolling_cls = rolling.DataArrayRolling\n260     _coarsen_cls = rolling.DataArrayCoarsen\n261     _resample_cls = resample.DataArrayResample\n262     _weighted_cls = weighted.DataArrayWeighted\n263 \n264     dt = property(CombinedDatetimelikeAccessor)\n265 \n266     def __init__(\n267         self,\n268         data: Any = dtypes.NA,\n269         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n270         dims: Union[Hashable, Sequence[Hashable], None] = None,\n271         name: Hashable = None,\n272         attrs: Mapping = None,\n273         # internal parameters\n274         indexes: Dict[Hashable, pd.Index] = None,\n275         fastpath: bool = False,\n276     ):\n277         \"\"\"\n278         Parameters\n279         ----------\n280         data : array_like\n281             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n282             or castable to an ``ndarray``. If a self-described xarray or pandas\n283             object, attempts are made to use this array's metadata to fill in\n284             other unspecified arguments. A view of the array's data is used\n285             instead of a copy if possible.\n286         coords : sequence or dict of array_like objects, optional\n287             Coordinates (tick labels) to use for indexing along each dimension.\n288             The following notations are accepted:\n289 \n290             - mapping {dimension name: array-like}\n291             - sequence of tuples that are valid arguments for xarray.Variable()\n292               - (dims, data)\n293               - (dims, data, attrs)\n294               - (dims, data, attrs, encoding)\n295 \n296             Additionally, it is possible to define a coord whose name\n297             does not match the dimension name, or a coord based on multiple\n298             dimensions, with one of the following notations:\n299 \n300             - mapping {coord name: DataArray}\n301             - mapping {coord name: Variable}\n302             - mapping {coord name: (dimension name, array-like)}\n303             - mapping {coord name: (tuple of dimension names, array-like)}\n304 \n305         dims : hashable or sequence of hashable, optional\n306             Name(s) of the data dimension(s). Must be either a hashable (only\n307             for 1D data) or a sequence of hashables with length equal to the\n308             number of dimensions. If this argument is omitted, dimension names\n309             default to ``['dim_0', ... 'dim_n']``.\n310         name : str or None, optional\n311             Name of this array.\n312         attrs : dict_like or None, optional\n313             Attributes to assign to the new instance. By default, an empty\n314             attribute dictionary is initialized.\n315         \"\"\"\n316         if fastpath:\n317             variable = data\n318             assert dims is None\n319             assert attrs is None\n320         else:\n321             # try to fill in arguments from data if they weren't supplied\n322             if coords is None:\n323 \n324                 if isinstance(data, DataArray):\n325                     coords = data.coords\n326                 elif isinstance(data, pd.Series):\n327                     coords = [data.index]\n328                 elif isinstance(data, pd.DataFrame):\n329                     coords = [data.index, data.columns]\n330                 elif isinstance(data, (pd.Index, IndexVariable)):\n331                     coords = [data]\n332                 elif isinstance(data, pdcompat.Panel):\n333                     coords = [data.items, data.major_axis, data.minor_axis]\n334 \n335             if dims is None:\n336                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n337             if name is None:\n338                 name = getattr(data, \"name\", None)\n339             if attrs is None and not isinstance(data, PANDAS_TYPES):\n340                 attrs = getattr(data, \"attrs\", None)\n341 \n342             data = _check_data_shape(data, coords, dims)\n343             data = as_compatible_data(data)\n344             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n345             variable = Variable(dims, data, attrs, fastpath=True)\n346             indexes = dict(\n347                 _extract_indexes_from_coords(coords)\n348             )  # needed for to_dataset\n349 \n350         # These fully describe a DataArray\n351         self._variable = variable\n352         assert isinstance(coords, dict)\n353         self._coords = coords\n354         self._name = name\n355 \n356         # TODO(shoyer): document this argument, once it becomes part of the\n357         # public interface.\n358         self._indexes = indexes\n359 \n360         self._file_obj = None\n361 \n362     def _replace(\n363         self,\n364         variable: Variable = None,\n365         coords=None,\n366         name: Union[Hashable, None, Default] = _default,\n367         indexes=None,\n368     ) -> \"DataArray\":\n369         if variable is None:\n370             variable = self.variable\n371         if coords is None:\n372             coords = self._coords\n373         if name is _default:\n374             name = self.name\n375         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n376 \n377     def _replace_maybe_drop_dims(\n378         self, variable: Variable, name: Union[Hashable, None, Default] = _default\n379     ) -> \"DataArray\":\n380         if variable.dims == self.dims and variable.shape == self.shape:\n381             coords = self._coords.copy()\n382             indexes = self._indexes\n383         elif variable.dims == self.dims:\n384             # Shape has changed (e.g. from reduce(..., keepdims=True)\n385             new_sizes = dict(zip(self.dims, variable.shape))\n386             coords = {\n387                 k: v\n388                 for k, v in self._coords.items()\n389                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n390             }\n391             changed_dims = [\n392                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n393             ]\n394             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n395         else:\n396             allowed_dims = set(variable.dims)\n397             coords = {\n398                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n399             }\n400             indexes = propagate_indexes(\n401                 self._indexes, exclude=(set(self.dims) - allowed_dims)\n402             )\n403         return self._replace(variable, coords, name, indexes=indexes)\n404 \n405     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> \"DataArray\":\n406         if not len(indexes):\n407             return self\n408         coords = self._coords.copy()\n409         for name, idx in indexes.items():\n410             coords[name] = IndexVariable(name, idx)\n411         obj = self._replace(coords=coords)\n412 \n413         # switch from dimension to level names, if necessary\n414         dim_names: Dict[Any, str] = {}\n415         for dim, idx in indexes.items():\n416             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n417                 dim_names[dim] = idx.name\n418         if dim_names:\n419             obj = obj.rename(dim_names)\n420         return obj\n421 \n422     def _to_temp_dataset(self) -> Dataset:\n423         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n424 \n425     def _from_temp_dataset(\n426         self, dataset: Dataset, name: Hashable = _default\n427     ) -> \"DataArray\":\n428         variable = dataset._variables.pop(_THIS_ARRAY)\n429         coords = dataset._variables\n430         indexes = dataset._indexes\n431         return self._replace(variable, coords, name, indexes=indexes)\n432 \n433     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n434         \"\"\" splits dataarray along dimension 'dim' \"\"\"\n435 \n436         def subset(dim, label):\n437             array = self.loc[{dim: label}]\n438             array.attrs = {}\n439             return as_variable(array)\n440 \n441         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n442         variables.update({k: v for k, v in self._coords.items() if k != dim})\n443         indexes = propagate_indexes(self._indexes, exclude=dim)\n444         coord_names = set(self._coords) - set([dim])\n445         dataset = Dataset._construct_direct(\n446             variables, coord_names, indexes=indexes, attrs=self.attrs\n447         )\n448         return dataset\n449 \n450     def _to_dataset_whole(\n451         self, name: Hashable = None, shallow_copy: bool = True\n452     ) -> Dataset:\n453         if name is None:\n454             name = self.name\n455         if name is None:\n456             raise ValueError(\n457                 \"unable to convert unnamed DataArray to a \"\n458                 \"Dataset without providing an explicit name\"\n459             )\n460         if name in self.coords:\n461             raise ValueError(\n462                 \"cannot create a Dataset from a DataArray with \"\n463                 \"the same name as one of its coordinates\"\n464             )\n465         # use private APIs for speed: this is called by _to_temp_dataset(),\n466         # which is used in the guts of a lot of operations (e.g., reindex)\n467         variables = self._coords.copy()\n468         variables[name] = self.variable\n469         if shallow_copy:\n470             for k in variables:\n471                 variables[k] = variables[k].copy(deep=False)\n472         indexes = self._indexes\n473 \n474         coord_names = set(self._coords)\n475         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n476         return dataset\n477 \n478     def to_dataset(\n479         self,\n480         dim: Hashable = None,\n481         *,\n482         name: Hashable = None,\n483         promote_attrs: bool = False,\n484     ) -> Dataset:\n485         \"\"\"Convert a DataArray to a Dataset.\n486 \n487         Parameters\n488         ----------\n489         dim : hashable, optional\n490             Name of the dimension on this array along which to split this array\n491             into separate variables. If not provided, this array is converted\n492             into a Dataset of one variable.\n493         name : hashable, optional\n494             Name to substitute for this array's name. Only valid if ``dim`` is\n495             not provided.\n496         promote_attrs : bool, default False\n497             Set to True to shallow copy attrs of DataArray to returned Dataset.\n498 \n499         Returns\n500         -------\n501         dataset : Dataset\n502         \"\"\"\n503         if dim is not None and dim not in self.dims:\n504             raise TypeError(\n505                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n506             )\n507 \n508         if dim is not None:\n509             if name is not None:\n510                 raise TypeError(\"cannot supply both dim and name arguments\")\n511             result = self._to_dataset_split(dim)\n512         else:\n513             result = self._to_dataset_whole(name)\n514 \n515         if promote_attrs:\n516             result.attrs = dict(self.attrs)\n517 \n518         return result\n519 \n520     @property\n521     def name(self) -> Optional[Hashable]:\n522         \"\"\"The name of this array.\n523         \"\"\"\n524         return self._name\n525 \n526     @name.setter\n527     def name(self, value: Optional[Hashable]) -> None:\n528         self._name = value\n529 \n530     @property\n531     def variable(self) -> Variable:\n532         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n533         return self._variable\n534 \n535     @property\n536     def dtype(self) -> np.dtype:\n537         return self.variable.dtype\n538 \n539     @property\n540     def shape(self) -> Tuple[int, ...]:\n541         return self.variable.shape\n542 \n543     @property\n544     def size(self) -> int:\n545         return self.variable.size\n546 \n547     @property\n548     def nbytes(self) -> int:\n549         return self.variable.nbytes\n550 \n551     @property\n552     def ndim(self) -> int:\n553         return self.variable.ndim\n554 \n555     def __len__(self) -> int:\n556         return len(self.variable)\n557 \n558     @property\n559     def data(self) -> Any:\n560         \"\"\"The array's data as a dask or numpy array\n561         \"\"\"\n562         return self.variable.data\n563 \n564     @data.setter\n565     def data(self, value: Any) -> None:\n566         self.variable.data = value\n567 \n568     @property\n569     def values(self) -> np.ndarray:\n570         \"\"\"The array's data as a numpy.ndarray\"\"\"\n571         return self.variable.values\n572 \n573     @values.setter\n574     def values(self, value: Any) -> None:\n575         self.variable.values = value\n576 \n577     @property\n578     def _in_memory(self) -> bool:\n579         return self.variable._in_memory\n580 \n581     def to_index(self) -> pd.Index:\n582         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n583         arrays.\n584         \"\"\"\n585         return self.variable.to_index()\n586 \n587     @property\n588     def dims(self) -> Tuple[Hashable, ...]:\n589         \"\"\"Tuple of dimension names associated with this array.\n590 \n591         Note that the type of this property is inconsistent with\n592         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n593         consistently named properties.\n594         \"\"\"\n595         return self.variable.dims\n596 \n597     @dims.setter\n598     def dims(self, value):\n599         raise AttributeError(\n600             \"you cannot assign dims on a DataArray. Use \"\n601             \".rename() or .swap_dims() instead.\"\n602         )\n603 \n604     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n605         if utils.is_dict_like(key):\n606             return key\n607         else:\n608             key = indexing.expanded_indexer(key, self.ndim)\n609             return dict(zip(self.dims, key))\n610 \n611     @property\n612     def _level_coords(self) -> Dict[Hashable, Hashable]:\n613         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n614         coordinate name.\n615         \"\"\"\n616         level_coords: Dict[Hashable, Hashable] = {}\n617 \n618         for cname, var in self._coords.items():\n619             if var.ndim == 1 and isinstance(var, IndexVariable):\n620                 level_names = var.level_names\n621                 if level_names is not None:\n622                     (dim,) = var.dims\n623                     level_coords.update({lname: dim for lname in level_names})\n624         return level_coords\n625 \n626     def _getitem_coord(self, key):\n627         from .dataset import _get_virtual_variable\n628 \n629         try:\n630             var = self._coords[key]\n631         except KeyError:\n632             dim_sizes = dict(zip(self.dims, self.shape))\n633             _, key, var = _get_virtual_variable(\n634                 self._coords, key, self._level_coords, dim_sizes\n635             )\n636 \n637         return self._replace_maybe_drop_dims(var, name=key)\n638 \n639     def __getitem__(self, key: Any) -> \"DataArray\":\n640         if isinstance(key, str):\n641             return self._getitem_coord(key)\n642         else:\n643             # xarray-style array indexing\n644             return self.isel(indexers=self._item_key_to_dict(key))\n645 \n646     def __setitem__(self, key: Any, value: Any) -> None:\n647         if isinstance(key, str):\n648             self.coords[key] = value\n649         else:\n650             # Coordinates in key, value and self[key] should be consistent.\n651             # TODO Coordinate consistency in key is checked here, but it\n652             # causes unnecessary indexing. It should be optimized.\n653             obj = self[key]\n654             if isinstance(value, DataArray):\n655                 assert_coordinate_consistent(value, obj.coords.variables)\n656             # DataArray key -> Variable key\n657             key = {\n658                 k: v.variable if isinstance(v, DataArray) else v\n659                 for k, v in self._item_key_to_dict(key).items()\n660             }\n661             self.variable[key] = value\n662 \n663     def __delitem__(self, key: Any) -> None:\n664         del self.coords[key]\n665 \n666     @property\n667     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n668         \"\"\"List of places to look-up items for attribute-style access\n669         \"\"\"\n670         return self._item_sources + [self.attrs]\n671 \n672     @property\n673     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n674         \"\"\"List of places to look-up items for key-completion\n675         \"\"\"\n676         return [\n677             self.coords,\n678             {d: self.coords[d] for d in self.dims},\n679             LevelCoordinatesSource(self),\n680         ]\n681 \n682     def __contains__(self, key: Any) -> bool:\n683         return key in self.data\n684 \n685     @property\n686     def loc(self) -> _LocIndexer:\n687         \"\"\"Attribute for location based indexing like pandas.\n688         \"\"\"\n689         return _LocIndexer(self)\n690 \n691     @property\n692     def attrs(self) -> Dict[Hashable, Any]:\n693         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n694         return self.variable.attrs\n695 \n696     @attrs.setter\n697     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n698         # Disable type checking to work around mypy bug - see mypy#4167\n699         self.variable.attrs = value  # type: ignore\n700 \n701     @property\n702     def encoding(self) -> Dict[Hashable, Any]:\n703         \"\"\"Dictionary of format-specific settings for how this array should be\n704         serialized.\"\"\"\n705         return self.variable.encoding\n706 \n707     @encoding.setter\n708     def encoding(self, value: Mapping[Hashable, Any]) -> None:\n709         self.variable.encoding = value\n710 \n711     @property\n712     def indexes(self) -> Indexes:\n713         \"\"\"Mapping of pandas.Index objects used for label based indexing\n714         \"\"\"\n715         if self._indexes is None:\n716             self._indexes = default_indexes(self._coords, self.dims)\n717         return Indexes(self._indexes)\n718 \n719     @property\n720     def coords(self) -> DataArrayCoordinates:\n721         \"\"\"Dictionary-like container of coordinate arrays.\n722         \"\"\"\n723         return DataArrayCoordinates(self)\n724 \n725     def reset_coords(\n726         self,\n727         names: Union[Iterable[Hashable], Hashable, None] = None,\n728         drop: bool = False,\n729         inplace: bool = None,\n730     ) -> Union[None, \"DataArray\", Dataset]:\n731         \"\"\"Given names of coordinates, reset them to become variables.\n732 \n733         Parameters\n734         ----------\n735         names : hashable or iterable of hashables, optional\n736             Name(s) of non-index coordinates in this dataset to reset into\n737             variables. By default, all non-index coordinates are reset.\n738         drop : bool, optional\n739             If True, remove coordinates instead of converting them into\n740             variables.\n741 \n742         Returns\n743         -------\n744         Dataset, or DataArray if ``drop == True``\n745         \"\"\"\n746         _check_inplace(inplace)\n747         if names is None:\n748             names = set(self.coords) - set(self.dims)\n749         dataset = self.coords.to_dataset().reset_coords(names, drop)\n750         if drop:\n751             return self._replace(coords=dataset._variables)\n752         else:\n753             if self.name is None:\n754                 raise ValueError(\n755                     \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n756                 )\n757             dataset[self.name] = self.variable\n758             return dataset\n759 \n760     def __dask_tokenize__(self):\n761         from dask.base import normalize_token\n762 \n763         return normalize_token((type(self), self._variable, self._coords, self._name))\n764 \n765     def __dask_graph__(self):\n766         return self._to_temp_dataset().__dask_graph__()\n767 \n768     def __dask_keys__(self):\n769         return self._to_temp_dataset().__dask_keys__()\n770 \n771     def __dask_layers__(self):\n772         return self._to_temp_dataset().__dask_layers__()\n773 \n774     @property\n775     def __dask_optimize__(self):\n776         return self._to_temp_dataset().__dask_optimize__\n777 \n778     @property\n779     def __dask_scheduler__(self):\n780         return self._to_temp_dataset().__dask_scheduler__\n781 \n782     def __dask_postcompute__(self):\n783         func, args = self._to_temp_dataset().__dask_postcompute__()\n784         return self._dask_finalize, (func, args, self.name)\n785 \n786     def __dask_postpersist__(self):\n787         func, args = self._to_temp_dataset().__dask_postpersist__()\n788         return self._dask_finalize, (func, args, self.name)\n789 \n790     @staticmethod\n791     def _dask_finalize(results, func, args, name):\n792         ds = func(results, *args)\n793         variable = ds._variables.pop(_THIS_ARRAY)\n794         coords = ds._variables\n795         return DataArray(variable, coords, name=name, fastpath=True)\n796 \n797     def load(self, **kwargs) -> \"DataArray\":\n798         \"\"\"Manually trigger loading of this array's data from disk or a\n799         remote source into memory and return this array.\n800 \n801         Normally, it should not be necessary to call this method in user code,\n802         because all xarray functions should either work on deferred data or\n803         load data automatically. However, this method can be necessary when\n804         working with many file objects on disk.\n805 \n806         Parameters\n807         ----------\n808         **kwargs : dict\n809             Additional keyword arguments passed on to ``dask.array.compute``.\n810 \n811         See Also\n812         --------\n813         dask.array.compute\n814         \"\"\"\n815         ds = self._to_temp_dataset().load(**kwargs)\n816         new = self._from_temp_dataset(ds)\n817         self._variable = new._variable\n818         self._coords = new._coords\n819         return self\n820 \n821     def compute(self, **kwargs) -> \"DataArray\":\n822         \"\"\"Manually trigger loading of this array's data from disk or a\n823         remote source into memory and return a new array. The original is\n824         left unaltered.\n825 \n826         Normally, it should not be necessary to call this method in user code,\n827         because all xarray functions should either work on deferred data or\n828         load data automatically. However, this method can be necessary when\n829         working with many file objects on disk.\n830 \n831         Parameters\n832         ----------\n833         **kwargs : dict\n834             Additional keyword arguments passed on to ``dask.array.compute``.\n835 \n836         See Also\n837         --------\n838         dask.array.compute\n839         \"\"\"\n840         new = self.copy(deep=False)\n841         return new.load(**kwargs)\n842 \n843     def persist(self, **kwargs) -> \"DataArray\":\n844         \"\"\" Trigger computation in constituent dask arrays\n845 \n846         This keeps them as dask arrays but encourages them to keep data in\n847         memory.  This is particularly useful when on a distributed machine.\n848         When on a single machine consider using ``.compute()`` instead.\n849 \n850         Parameters\n851         ----------\n852         **kwargs : dict\n853             Additional keyword arguments passed on to ``dask.persist``.\n854 \n855         See Also\n856         --------\n857         dask.persist\n858         \"\"\"\n859         ds = self._to_temp_dataset().persist(**kwargs)\n860         return self._from_temp_dataset(ds)\n861 \n862     def copy(self, deep: bool = True, data: Any = None) -> \"DataArray\":\n863         \"\"\"Returns a copy of this array.\n864 \n865         If `deep=True`, a deep copy is made of the data array.\n866         Otherwise, a shallow copy is made, so each variable in the new\n867         array's dataset is also a variable in this array's dataset.\n868 \n869         Use `data` to create a new object with the same structure as\n870         original but entirely new data.\n871 \n872         Parameters\n873         ----------\n874         deep : bool, optional\n875             Whether the data array and its coordinates are loaded into memory\n876             and copied onto the new object. Default is True.\n877         data : array_like, optional\n878             Data to use in the new object. Must have same shape as original.\n879             When `data` is used, `deep` is ignored for all data variables,\n880             and only used for coords.\n881 \n882         Returns\n883         -------\n884         object : DataArray\n885             New object with dimensions, attributes, coordinates, name,\n886             encoding, and optionally data copied from original.\n887 \n888         Examples\n889         --------\n890 \n891         Shallow versus deep copy\n892 \n893         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n894         >>> array.copy()\n895         <xarray.DataArray (x: 3)>\n896         array([1, 2, 3])\n897         Coordinates:\n898         * x        (x) <U1 'a' 'b' 'c'\n899         >>> array_0 = array.copy(deep=False)\n900         >>> array_0[0] = 7\n901         >>> array_0\n902         <xarray.DataArray (x: 3)>\n903         array([7, 2, 3])\n904         Coordinates:\n905         * x        (x) <U1 'a' 'b' 'c'\n906         >>> array\n907         <xarray.DataArray (x: 3)>\n908         array([7, 2, 3])\n909         Coordinates:\n910         * x        (x) <U1 'a' 'b' 'c'\n911 \n912         Changing the data using the ``data`` argument maintains the\n913         structure of the original object, but with the new data. Original\n914         object is unaffected.\n915 \n916         >>> array.copy(data=[0.1, 0.2, 0.3])\n917         <xarray.DataArray (x: 3)>\n918         array([ 0.1,  0.2,  0.3])\n919         Coordinates:\n920         * x        (x) <U1 'a' 'b' 'c'\n921         >>> array\n922         <xarray.DataArray (x: 3)>\n923         array([1, 2, 3])\n924         Coordinates:\n925         * x        (x) <U1 'a' 'b' 'c'\n926 \n927         See Also\n928         --------\n929         pandas.DataFrame.copy\n930         \"\"\"\n931         variable = self.variable.copy(deep=deep, data=data)\n932         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n933         if self._indexes is None:\n934             indexes = self._indexes\n935         else:\n936             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n937         return self._replace(variable, coords, indexes=indexes)\n938 \n939     def __copy__(self) -> \"DataArray\":\n940         return self.copy(deep=False)\n941 \n942     def __deepcopy__(self, memo=None) -> \"DataArray\":\n943         # memo does nothing but is required for compatibility with\n944         # copy.deepcopy\n945         return self.copy(deep=True)\n946 \n947     # mutable objects should not be hashable\n948     # https://github.com/python/mypy/issues/4266\n949     __hash__ = None  # type: ignore\n950 \n951     @property\n952     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n953         \"\"\"Block dimensions for this array's data or None if it's not a dask\n954         array.\n955         \"\"\"\n956         return self.variable.chunks\n957 \n958     def chunk(\n959         self,\n960         chunks: Union[\n961             None,\n962             Number,\n963             Tuple[Number, ...],\n964             Tuple[Tuple[Number, ...], ...],\n965             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n966         ] = None,\n967         name_prefix: str = \"xarray-\",\n968         token: str = None,\n969         lock: bool = False,\n970     ) -> \"DataArray\":\n971         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n972 \n973         If this variable is a non-dask array, it will be converted to dask\n974         array. If it's a dask array, it will be rechunked to the given chunk\n975         sizes.\n976 \n977         If neither chunks is not provided for one or more dimensions, chunk\n978         sizes along that dimension will not be updated; non-dask arrays will be\n979         converted into dask arrays with a single block.\n980 \n981         Parameters\n982         ----------\n983         chunks : int, tuple or mapping, optional\n984             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n985             ``{'x': 5, 'y': 5}``.\n986         name_prefix : str, optional\n987             Prefix for the name of the new dask array.\n988         token : str, optional\n989             Token uniquely identifying this array.\n990         lock : optional\n991             Passed on to :py:func:`dask.array.from_array`, if the array is not\n992             already as dask array.\n993 \n994         Returns\n995         -------\n996         chunked : xarray.DataArray\n997         \"\"\"\n998         if isinstance(chunks, (tuple, list)):\n999             chunks = dict(zip(self.dims, chunks))\n1000 \n1001         ds = self._to_temp_dataset().chunk(\n1002             chunks, name_prefix=name_prefix, token=token, lock=lock\n1003         )\n1004         return self._from_temp_dataset(ds)\n1005 \n1006     def isel(\n1007         self,\n1008         indexers: Mapping[Hashable, Any] = None,\n1009         drop: bool = False,\n1010         missing_dims: str = \"raise\",\n1011         **indexers_kwargs: Any,\n1012     ) -> \"DataArray\":\n1013         \"\"\"Return a new DataArray whose data is given by integer indexing\n1014         along the specified dimension(s).\n1015 \n1016         Parameters\n1017         ----------\n1018         indexers : dict, optional\n1019             A dict with keys matching dimensions and values given\n1020             by integers, slice objects or arrays.\n1021             indexer can be a integer, slice, array-like or DataArray.\n1022             If DataArrays are passed as indexers, xarray-style indexing will be\n1023             carried out. See :ref:`indexing` for the details.\n1024             One of indexers or indexers_kwargs must be provided.\n1025         drop : bool, optional\n1026             If ``drop=True``, drop coordinates variables indexed by integers\n1027             instead of making them scalar.\n1028         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1029             What to do if dimensions that should be selected from are not present in the\n1030             DataArray:\n1031             - \"exception\": raise an exception\n1032             - \"warning\": raise a warning, and ignore the missing dimensions\n1033             - \"ignore\": ignore the missing dimensions\n1034         **indexers_kwargs : {dim: indexer, ...}, optional\n1035             The keyword arguments form of ``indexers``.\n1036 \n1037         See Also\n1038         --------\n1039         Dataset.isel\n1040         DataArray.sel\n1041         \"\"\"\n1042 \n1043         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1044 \n1045         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1046             ds = self._to_temp_dataset()._isel_fancy(\n1047                 indexers, drop=drop, missing_dims=missing_dims\n1048             )\n1049             return self._from_temp_dataset(ds)\n1050 \n1051         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1052         # lists, or zero or one-dimensional np.ndarray's\n1053 \n1054         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1055 \n1056         coords = {}\n1057         for coord_name, coord_value in self._coords.items():\n1058             coord_indexers = {\n1059                 k: v for k, v in indexers.items() if k in coord_value.dims\n1060             }\n1061             if coord_indexers:\n1062                 coord_value = coord_value.isel(coord_indexers)\n1063                 if drop and coord_value.ndim == 0:\n1064                     continue\n1065             coords[coord_name] = coord_value\n1066 \n1067         return self._replace(variable=variable, coords=coords)\n1068 \n1069     def sel(\n1070         self,\n1071         indexers: Mapping[Hashable, Any] = None,\n1072         method: str = None,\n1073         tolerance=None,\n1074         drop: bool = False,\n1075         **indexers_kwargs: Any,\n1076     ) -> \"DataArray\":\n1077         \"\"\"Return a new DataArray whose data is given by selecting index\n1078         labels along the specified dimension(s).\n1079 \n1080         .. warning::\n1081 \n1082           Do not try to assign values when using any of the indexing methods\n1083           ``isel`` or ``sel``::\n1084 \n1085             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1086             # DO NOT do this\n1087             da.isel(x=[0, 1, 2])[1] = -1\n1088 \n1089           Assigning values with the chained indexing using ``.sel`` or\n1090           ``.isel`` fails silently.\n1091 \n1092         See Also\n1093         --------\n1094         Dataset.sel\n1095         DataArray.isel\n1096 \n1097         \"\"\"\n1098         ds = self._to_temp_dataset().sel(\n1099             indexers=indexers,\n1100             drop=drop,\n1101             method=method,\n1102             tolerance=tolerance,\n1103             **indexers_kwargs,\n1104         )\n1105         return self._from_temp_dataset(ds)\n1106 \n1107     def head(\n1108         self,\n1109         indexers: Union[Mapping[Hashable, int], int] = None,\n1110         **indexers_kwargs: Any,\n1111     ) -> \"DataArray\":\n1112         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1113         values along the specified dimension(s). Default `n` = 5\n1114 \n1115         See Also\n1116         --------\n1117         Dataset.head\n1118         DataArray.tail\n1119         DataArray.thin\n1120         \"\"\"\n1121         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1122         return self._from_temp_dataset(ds)\n1123 \n1124     def tail(\n1125         self,\n1126         indexers: Union[Mapping[Hashable, int], int] = None,\n1127         **indexers_kwargs: Any,\n1128     ) -> \"DataArray\":\n1129         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1130         values along the specified dimension(s). Default `n` = 5\n1131 \n1132         See Also\n1133         --------\n1134         Dataset.tail\n1135         DataArray.head\n1136         DataArray.thin\n1137         \"\"\"\n1138         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1139         return self._from_temp_dataset(ds)\n1140 \n1141     def thin(\n1142         self,\n1143         indexers: Union[Mapping[Hashable, int], int] = None,\n1144         **indexers_kwargs: Any,\n1145     ) -> \"DataArray\":\n1146         \"\"\"Return a new DataArray whose data is given by each `n` value\n1147         along the specified dimension(s).\n1148 \n1149         See Also\n1150         --------\n1151         Dataset.thin\n1152         DataArray.head\n1153         DataArray.tail\n1154         \"\"\"\n1155         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1156         return self._from_temp_dataset(ds)\n1157 \n1158     def broadcast_like(\n1159         self, other: Union[\"DataArray\", Dataset], exclude: Iterable[Hashable] = None\n1160     ) -> \"DataArray\":\n1161         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1162 \n1163         This is equivalent to xr.broadcast(other, self)[1]\n1164 \n1165         xarray objects are broadcast against each other in arithmetic\n1166         operations, so this method is not be necessary for most uses.\n1167 \n1168         If no change is needed, the input data is returned to the output\n1169         without being copied.\n1170 \n1171         If new coords are added by the broadcast, their values are\n1172         NaN filled.\n1173 \n1174         Parameters\n1175         ----------\n1176         other : Dataset or DataArray\n1177             Object against which to broadcast this array.\n1178         exclude : iterable of hashable, optional\n1179             Dimensions that must not be broadcasted\n1180 \n1181         Returns\n1182         -------\n1183         new_da: xr.DataArray\n1184 \n1185         Examples\n1186         --------\n1187 \n1188         >>> arr1\n1189         <xarray.DataArray (x: 2, y: 3)>\n1190         array([[0.840235, 0.215216, 0.77917 ],\n1191                [0.726351, 0.543824, 0.875115]])\n1192         Coordinates:\n1193           * x        (x) <U1 'a' 'b'\n1194           * y        (y) <U1 'a' 'b' 'c'\n1195         >>> arr2\n1196         <xarray.DataArray (x: 3, y: 2)>\n1197         array([[0.612611, 0.125753],\n1198                [0.853181, 0.948818],\n1199                [0.180885, 0.33363 ]])\n1200         Coordinates:\n1201           * x        (x) <U1 'a' 'b' 'c'\n1202           * y        (y) <U1 'a' 'b'\n1203         >>> arr1.broadcast_like(arr2)\n1204         <xarray.DataArray (x: 3, y: 3)>\n1205         array([[0.840235, 0.215216, 0.77917 ],\n1206                [0.726351, 0.543824, 0.875115],\n1207                [     nan,      nan,      nan]])\n1208         Coordinates:\n1209           * x        (x) object 'a' 'b' 'c'\n1210           * y        (y) object 'a' 'b' 'c'\n1211         \"\"\"\n1212         if exclude is None:\n1213             exclude = set()\n1214         else:\n1215             exclude = set(exclude)\n1216         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1217 \n1218         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1219 \n1220         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n1221 \n1222     def reindex_like(\n1223         self,\n1224         other: Union[\"DataArray\", Dataset],\n1225         method: str = None,\n1226         tolerance=None,\n1227         copy: bool = True,\n1228         fill_value=dtypes.NA,\n1229     ) -> \"DataArray\":\n1230         \"\"\"Conform this object onto the indexes of another object, filling in\n1231         missing values with ``fill_value``. The default fill value is NaN.\n1232 \n1233         Parameters\n1234         ----------\n1235         other : Dataset or DataArray\n1236             Object with an 'indexes' attribute giving a mapping from dimension\n1237             names to pandas.Index objects, which provides coordinates upon\n1238             which to index the variables in this dataset. The indexes on this\n1239             other object need not be the same as the indexes on this\n1240             dataset. Any mis-matched index values will be filled in with\n1241             NaN, and any mis-matched dimension names will simply be ignored.\n1242         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1243             Method to use for filling index values from other not found on this\n1244             data array:\n1245 \n1246             * None (default): don't fill gaps\n1247             * pad / ffill: propagate last valid index value forward\n1248             * backfill / bfill: propagate next valid index value backward\n1249             * nearest: use nearest valid index value\n1250         tolerance : optional\n1251             Maximum distance between original and new labels for inexact\n1252             matches. The values of the index at the matching locations must\n1253             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1254         copy : bool, optional\n1255             If ``copy=True``, data in the return value is always copied. If\n1256             ``copy=False`` and reindexing is unnecessary, or can be performed\n1257             with only slice operations, then the output may share memory with\n1258             the input. In either case, a new xarray object is always returned.\n1259         fill_value : scalar, optional\n1260             Value to use for newly missing values\n1261 \n1262         Returns\n1263         -------\n1264         reindexed : DataArray\n1265             Another dataset array, with this array's data but coordinates from\n1266             the other object.\n1267 \n1268         See Also\n1269         --------\n1270         DataArray.reindex\n1271         align\n1272         \"\"\"\n1273         indexers = reindex_like_indexers(self, other)\n1274         return self.reindex(\n1275             indexers=indexers,\n1276             method=method,\n1277             tolerance=tolerance,\n1278             copy=copy,\n1279             fill_value=fill_value,\n1280         )\n1281 \n1282     def reindex(\n1283         self,\n1284         indexers: Mapping[Hashable, Any] = None,\n1285         method: str = None,\n1286         tolerance=None,\n1287         copy: bool = True,\n1288         fill_value=dtypes.NA,\n1289         **indexers_kwargs: Any,\n1290     ) -> \"DataArray\":\n1291         \"\"\"Conform this object onto the indexes of another object, filling in\n1292         missing values with ``fill_value``. The default fill value is NaN.\n1293 \n1294         Parameters\n1295         ----------\n1296         indexers : dict, optional\n1297             Dictionary with keys given by dimension names and values given by\n1298             arrays of coordinates tick labels. Any mis-matched coordinate\n1299             values will be filled in with NaN, and any mis-matched dimension\n1300             names will simply be ignored.\n1301             One of indexers or indexers_kwargs must be provided.\n1302         copy : bool, optional\n1303             If ``copy=True``, data in the return value is always copied. If\n1304             ``copy=False`` and reindexing is unnecessary, or can be performed\n1305             with only slice operations, then the output may share memory with\n1306             the input. In either case, a new xarray object is always returned.\n1307         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1308             Method to use for filling index values in ``indexers`` not found on\n1309             this data array:\n1310 \n1311             * None (default): don't fill gaps\n1312             * pad / ffill: propagate last valid index value forward\n1313             * backfill / bfill: propagate next valid index value backward\n1314             * nearest: use nearest valid index value\n1315         tolerance : optional\n1316             Maximum distance between original and new labels for inexact\n1317             matches. The values of the index at the matching locations must\n1318             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1319         fill_value : scalar, optional\n1320             Value to use for newly missing values\n1321         **indexers_kwargs : {dim: indexer, ...}, optional\n1322             The keyword arguments form of ``indexers``.\n1323             One of indexers or indexers_kwargs must be provided.\n1324 \n1325         Returns\n1326         -------\n1327         reindexed : DataArray\n1328             Another dataset array, with this array's data but replaced\n1329             coordinates.\n1330 \n1331         See Also\n1332         --------\n1333         DataArray.reindex_like\n1334         align\n1335         \"\"\"\n1336         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1337         ds = self._to_temp_dataset().reindex(\n1338             indexers=indexers,\n1339             method=method,\n1340             tolerance=tolerance,\n1341             copy=copy,\n1342             fill_value=fill_value,\n1343         )\n1344         return self._from_temp_dataset(ds)\n1345 \n1346     def interp(\n1347         self,\n1348         coords: Mapping[Hashable, Any] = None,\n1349         method: str = \"linear\",\n1350         assume_sorted: bool = False,\n1351         kwargs: Mapping[str, Any] = None,\n1352         **coords_kwargs: Any,\n1353     ) -> \"DataArray\":\n1354         \"\"\" Multidimensional interpolation of variables.\n1355 \n1356         coords : dict, optional\n1357             Mapping from dimension names to the new coordinates.\n1358             new coordinate can be an scalar, array-like or DataArray.\n1359             If DataArrays are passed as new coordates, their dimensions are\n1360             used for the broadcasting.\n1361         method: {'linear', 'nearest'} for multidimensional array,\n1362             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1363             for 1-dimensional array.\n1364         assume_sorted: boolean, optional\n1365             If False, values of x can be in any order and they are sorted\n1366             first. If True, x has to be an array of monotonically increasing\n1367             values.\n1368         kwargs: dictionary\n1369             Additional keyword passed to scipy's interpolator.\n1370         ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n1371             The keyword arguments form of ``coords``.\n1372             One of coords or coords_kwargs must be provided.\n1373 \n1374         Returns\n1375         -------\n1376         interpolated: xr.DataArray\n1377             New dataarray on the new coordinates.\n1378 \n1379         Notes\n1380         -----\n1381         scipy is required.\n1382 \n1383         See Also\n1384         --------\n1385         scipy.interpolate.interp1d\n1386         scipy.interpolate.interpn\n1387 \n1388         Examples\n1389         --------\n1390         >>> da = xr.DataArray([1, 3], [(\"x\", np.arange(2))])\n1391         >>> da.interp(x=0.5)\n1392         <xarray.DataArray ()>\n1393         array(2.0)\n1394         Coordinates:\n1395             x        float64 0.5\n1396         \"\"\"\n1397         if self.dtype.kind not in \"uifc\":\n1398             raise TypeError(\n1399                 \"interp only works for a numeric type array. \"\n1400                 \"Given {}.\".format(self.dtype)\n1401             )\n1402         ds = self._to_temp_dataset().interp(\n1403             coords,\n1404             method=method,\n1405             kwargs=kwargs,\n1406             assume_sorted=assume_sorted,\n1407             **coords_kwargs,\n1408         )\n1409         return self._from_temp_dataset(ds)\n1410 \n1411     def interp_like(\n1412         self,\n1413         other: Union[\"DataArray\", Dataset],\n1414         method: str = \"linear\",\n1415         assume_sorted: bool = False,\n1416         kwargs: Mapping[str, Any] = None,\n1417     ) -> \"DataArray\":\n1418         \"\"\"Interpolate this object onto the coordinates of another object,\n1419         filling out of range values with NaN.\n1420 \n1421         Parameters\n1422         ----------\n1423         other : Dataset or DataArray\n1424             Object with an 'indexes' attribute giving a mapping from dimension\n1425             names to an 1d array-like, which provides coordinates upon\n1426             which to index the variables in this dataset.\n1427         method: string, optional.\n1428             {'linear', 'nearest'} for multidimensional array,\n1429             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1430             for 1-dimensional array. 'linear' is used by default.\n1431         assume_sorted: boolean, optional\n1432             If False, values of coordinates that are interpolated over can be\n1433             in any order and they are sorted first. If True, interpolated\n1434             coordinates are assumed to be an array of monotonically increasing\n1435             values.\n1436         kwargs: dictionary, optional\n1437             Additional keyword passed to scipy's interpolator.\n1438 \n1439         Returns\n1440         -------\n1441         interpolated: xr.DataArray\n1442             Another dataarray by interpolating this dataarray's data along the\n1443             coordinates of the other object.\n1444 \n1445         Notes\n1446         -----\n1447         scipy is required.\n1448         If the dataarray has object-type coordinates, reindex is used for these\n1449         coordinates instead of the interpolation.\n1450 \n1451         See Also\n1452         --------\n1453         DataArray.interp\n1454         DataArray.reindex_like\n1455         \"\"\"\n1456         if self.dtype.kind not in \"uifc\":\n1457             raise TypeError(\n1458                 \"interp only works for a numeric type array. \"\n1459                 \"Given {}.\".format(self.dtype)\n1460             )\n1461         ds = self._to_temp_dataset().interp_like(\n1462             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1463         )\n1464         return self._from_temp_dataset(ds)\n1465 \n1466     def rename(\n1467         self,\n1468         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n1469         **names: Hashable,\n1470     ) -> \"DataArray\":\n1471         \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n1472 \n1473         Parameters\n1474         ----------\n1475         new_name_or_name_dict : str or dict-like, optional\n1476             If the argument is dict-like, it used as a mapping from old\n1477             names to new names for coordinates. Otherwise, use the argument\n1478             as the new name for this array.\n1479         **names: hashable, optional\n1480             The keyword arguments form of a mapping from old names to\n1481             new names for coordinates.\n1482             One of new_name_or_name_dict or names must be provided.\n1483 \n1484         Returns\n1485         -------\n1486         renamed : DataArray\n1487             Renamed array or array with renamed coordinates.\n1488 \n1489         See Also\n1490         --------\n1491         Dataset.rename\n1492         DataArray.swap_dims\n1493         \"\"\"\n1494         if names or utils.is_dict_like(new_name_or_name_dict):\n1495             new_name_or_name_dict = cast(\n1496                 Mapping[Hashable, Hashable], new_name_or_name_dict\n1497             )\n1498             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n1499             dataset = self._to_temp_dataset().rename(name_dict)\n1500             return self._from_temp_dataset(dataset)\n1501         else:\n1502             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n1503             return self._replace(name=new_name_or_name_dict)\n1504 \n1505     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> \"DataArray\":\n1506         \"\"\"Returns a new DataArray with swapped dimensions.\n1507 \n1508         Parameters\n1509         ----------\n1510         dims_dict : dict-like\n1511             Dictionary whose keys are current dimension names and whose values\n1512             are new names.\n1513 \n1514         Returns\n1515         -------\n1516         swapped : DataArray\n1517             DataArray with swapped dimensions.\n1518 \n1519         Examples\n1520         --------\n1521 \n1522         >>> arr = xr.DataArray(\n1523         ...     data=[0, 1], dims=\"x\", coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n1524         ... )\n1525         >>> arr\n1526         <xarray.DataArray (x: 2)>\n1527         array([0, 1])\n1528         Coordinates:\n1529           * x        (x) <U1 'a' 'b'\n1530             y        (x) int64 0 1\n1531 \n1532         >>> arr.swap_dims({\"x\": \"y\"})\n1533         <xarray.DataArray (y: 2)>\n1534         array([0, 1])\n1535         Coordinates:\n1536             x        (y) <U1 'a' 'b'\n1537           * y        (y) int64 0 1\n1538 \n1539         >>> arr.swap_dims({\"x\": \"z\"})\n1540         <xarray.DataArray (z: 2)>\n1541         array([0, 1])\n1542         Coordinates:\n1543             x        (z) <U1 'a' 'b'\n1544             y        (z) int64 0 1\n1545         Dimensions without coordinates: z\n1546 \n1547         See Also\n1548         --------\n1549 \n1550         DataArray.rename\n1551         Dataset.swap_dims\n1552         \"\"\"\n1553         ds = self._to_temp_dataset().swap_dims(dims_dict)\n1554         return self._from_temp_dataset(ds)\n1555 \n1556     def expand_dims(\n1557         self,\n1558         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n1559         axis=None,\n1560         **dim_kwargs: Any,\n1561     ) -> \"DataArray\":\n1562         \"\"\"Return a new object with an additional axis (or axes) inserted at\n1563         the corresponding position in the array shape. The new object is a\n1564         view into the underlying array, not a copy.\n1565 \n1566 \n1567         If dim is already a scalar coordinate, it will be promoted to a 1D\n1568         coordinate consisting of a single value.\n1569 \n1570         Parameters\n1571         ----------\n1572         dim : hashable, sequence of hashable, dict, or None\n1573             Dimensions to include on the new variable.\n1574             If provided as str or sequence of str, then dimensions are inserted\n1575             with length 1. If provided as a dict, then the keys are the new\n1576             dimensions and the values are either integers (giving the length of\n1577             the new dimensions) or sequence/ndarray (giving the coordinates of\n1578             the new dimensions).\n1579         axis : integer, list (or tuple) of integers, or None\n1580             Axis position(s) where new axis is to be inserted (position(s) on\n1581             the result array). If a list (or tuple) of integers is passed,\n1582             multiple axes are inserted. In this case, dim arguments should be\n1583             same length list. If axis=None is passed, all the axes will be\n1584             inserted to the start of the result array.\n1585         **dim_kwargs : int or sequence/ndarray\n1586             The keywords are arbitrary dimensions being inserted and the values\n1587             are either the lengths of the new dims (if int is given), or their\n1588             coordinates. Note, this is an alternative to passing a dict to the\n1589             dim kwarg and will only be used if dim is None.\n1590 \n1591         Returns\n1592         -------\n1593         expanded : same type as caller\n1594             This object, but with an additional dimension(s).\n1595         \"\"\"\n1596         if isinstance(dim, int):\n1597             raise TypeError(\"dim should be hashable or sequence/mapping of hashables\")\n1598         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n1599             if len(dim) != len(set(dim)):\n1600                 raise ValueError(\"dims should not contain duplicate values.\")\n1601             dim = dict.fromkeys(dim, 1)\n1602         elif dim is not None and not isinstance(dim, Mapping):\n1603             dim = {cast(Hashable, dim): 1}\n1604 \n1605         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n1606         ds = self._to_temp_dataset().expand_dims(dim, axis)\n1607         return self._from_temp_dataset(ds)\n1608 \n1609     def set_index(\n1610         self,\n1611         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n1612         append: bool = False,\n1613         inplace: bool = None,\n1614         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n1615     ) -> Optional[\"DataArray\"]:\n1616         \"\"\"Set DataArray (multi-)indexes using one or more existing\n1617         coordinates.\n1618 \n1619         Parameters\n1620         ----------\n1621         indexes : {dim: index, ...}\n1622             Mapping from names matching dimensions and values given\n1623             by (lists of) the names of existing coordinates or variables to set\n1624             as new (multi-)index.\n1625         append : bool, optional\n1626             If True, append the supplied index(es) to the existing index(es).\n1627             Otherwise replace the existing index(es) (default).\n1628         **indexes_kwargs: optional\n1629             The keyword arguments form of ``indexes``.\n1630             One of indexes or indexes_kwargs must be provided.\n1631 \n1632         Returns\n1633         -------\n1634         obj : DataArray\n1635             Another DataArray, with this data but replaced coordinates.\n1636 \n1637         Examples\n1638         --------\n1639         >>> arr = xr.DataArray(\n1640         ...     data=np.ones((2, 3)),\n1641         ...     dims=[\"x\", \"y\"],\n1642         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n1643         ... )\n1644         >>> arr\n1645         <xarray.DataArray (x: 2, y: 3)>\n1646         array([[1., 1., 1.],\n1647                [1., 1., 1.]])\n1648         Coordinates:\n1649           * x        (x) int64 0 1\n1650           * y        (y) int64 0 1 2\n1651             a        (x) int64 3 4\n1652         >>> arr.set_index(x=\"a\")\n1653         <xarray.DataArray (x: 2, y: 3)>\n1654         array([[1., 1., 1.],\n1655                [1., 1., 1.]])\n1656         Coordinates:\n1657           * x        (x) int64 3 4\n1658           * y        (y) int64 0 1 2\n1659 \n1660         See Also\n1661         --------\n1662         DataArray.reset_index\n1663         \"\"\"\n1664         ds = self._to_temp_dataset().set_index(\n1665             indexes, append=append, inplace=inplace, **indexes_kwargs\n1666         )\n1667         return self._from_temp_dataset(ds)\n1668 \n1669     def reset_index(\n1670         self,\n1671         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n1672         drop: bool = False,\n1673         inplace: bool = None,\n1674     ) -> Optional[\"DataArray\"]:\n1675         \"\"\"Reset the specified index(es) or multi-index level(s).\n1676 \n1677         Parameters\n1678         ----------\n1679         dims_or_levels : hashable or sequence of hashables\n1680             Name(s) of the dimension(s) and/or multi-index level(s) that will\n1681             be reset.\n1682         drop : bool, optional\n1683             If True, remove the specified indexes and/or multi-index levels\n1684             instead of extracting them as new coordinates (default: False).\n1685 \n1686         Returns\n1687         -------\n1688         obj : DataArray\n1689             Another dataarray, with this dataarray's data but replaced\n1690             coordinates.\n1691 \n1692         See Also\n1693         --------\n1694         DataArray.set_index\n1695         \"\"\"\n1696         _check_inplace(inplace)\n1697         coords, _ = split_indexes(\n1698             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n1699         )\n1700         return self._replace(coords=coords)\n1701 \n1702     def reorder_levels(\n1703         self,\n1704         dim_order: Mapping[Hashable, Sequence[int]] = None,\n1705         inplace: bool = None,\n1706         **dim_order_kwargs: Sequence[int],\n1707     ) -> \"DataArray\":\n1708         \"\"\"Rearrange index levels using input order.\n1709 \n1710         Parameters\n1711         ----------\n1712         dim_order : optional\n1713             Mapping from names matching dimensions and values given\n1714             by lists representing new level orders. Every given dimension\n1715             must have a multi-index.\n1716         **dim_order_kwargs: optional\n1717             The keyword arguments form of ``dim_order``.\n1718             One of dim_order or dim_order_kwargs must be provided.\n1719 \n1720         Returns\n1721         -------\n1722         obj : DataArray\n1723             Another dataarray, with this dataarray's data but replaced\n1724             coordinates.\n1725         \"\"\"\n1726         _check_inplace(inplace)\n1727         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n1728         replace_coords = {}\n1729         for dim, order in dim_order.items():\n1730             coord = self._coords[dim]\n1731             index = coord.to_index()\n1732             if not isinstance(index, pd.MultiIndex):\n1733                 raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n1734             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n1735         coords = self._coords.copy()\n1736         coords.update(replace_coords)\n1737         return self._replace(coords=coords)\n1738 \n1739     def stack(\n1740         self,\n1741         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n1742         **dimensions_kwargs: Sequence[Hashable],\n1743     ) -> \"DataArray\":\n1744         \"\"\"\n1745         Stack any number of existing dimensions into a single new dimension.\n1746 \n1747         New dimensions will be added at the end, and the corresponding\n1748         coordinate variables will be combined into a MultiIndex.\n1749 \n1750         Parameters\n1751         ----------\n1752         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n1753             Names of new dimensions, and the existing dimensions that they\n1754             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n1755             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n1756             all dimensions.\n1757         **dimensions_kwargs:\n1758             The keyword arguments form of ``dimensions``.\n1759             One of dimensions or dimensions_kwargs must be provided.\n1760 \n1761         Returns\n1762         -------\n1763         stacked : DataArray\n1764             DataArray with stacked data.\n1765 \n1766         Examples\n1767         --------\n1768 \n1769         >>> arr = xr.DataArray(\n1770         ...     np.arange(6).reshape(2, 3),\n1771         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1772         ... )\n1773         >>> arr\n1774         <xarray.DataArray (x: 2, y: 3)>\n1775         array([[0, 1, 2],\n1776                [3, 4, 5]])\n1777         Coordinates:\n1778           * x        (x) |S1 'a' 'b'\n1779           * y        (y) int64 0 1 2\n1780         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1781         >>> stacked.indexes[\"z\"]\n1782         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1783                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1784                    names=['x', 'y'])\n1785 \n1786         See Also\n1787         --------\n1788         DataArray.unstack\n1789         \"\"\"\n1790         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n1791         return self._from_temp_dataset(ds)\n1792 \n1793     def unstack(\n1794         self,\n1795         dim: Union[Hashable, Sequence[Hashable], None] = None,\n1796         fill_value: Any = dtypes.NA,\n1797         sparse: bool = False,\n1798     ) -> \"DataArray\":\n1799         \"\"\"\n1800         Unstack existing dimensions corresponding to MultiIndexes into\n1801         multiple new dimensions.\n1802 \n1803         New dimensions will be added at the end.\n1804 \n1805         Parameters\n1806         ----------\n1807         dim : hashable or sequence of hashable, optional\n1808             Dimension(s) over which to unstack. By default unstacks all\n1809             MultiIndexes.\n1810         fill_value: value to be filled. By default, np.nan\n1811         sparse: use sparse-array if True\n1812 \n1813         Returns\n1814         -------\n1815         unstacked : DataArray\n1816             Array with unstacked data.\n1817 \n1818         Examples\n1819         --------\n1820 \n1821         >>> arr = xr.DataArray(\n1822         ...     np.arange(6).reshape(2, 3),\n1823         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1824         ... )\n1825         >>> arr\n1826         <xarray.DataArray (x: 2, y: 3)>\n1827         array([[0, 1, 2],\n1828                [3, 4, 5]])\n1829         Coordinates:\n1830           * x        (x) |S1 'a' 'b'\n1831           * y        (y) int64 0 1 2\n1832         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1833         >>> stacked.indexes[\"z\"]\n1834         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1835                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1836                    names=['x', 'y'])\n1837         >>> roundtripped = stacked.unstack()\n1838         >>> arr.identical(roundtripped)\n1839         True\n1840 \n1841         See Also\n1842         --------\n1843         DataArray.stack\n1844         \"\"\"\n1845         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n1846         return self._from_temp_dataset(ds)\n1847 \n1848     def to_unstacked_dataset(self, dim, level=0):\n1849         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n1850         stacked coordinate.\n1851 \n1852         This is the inverse operation of Dataset.to_stacked_array.\n1853 \n1854         Parameters\n1855         ----------\n1856         dim : str\n1857             Name of existing dimension to unstack\n1858         level : int or str\n1859             The MultiIndex level to expand to a dataset along. Can either be\n1860             the integer index of the level or its name.\n1861         label : int, default 0\n1862             Label of the level to expand dataset along. Overrides the label\n1863             argument if given.\n1864 \n1865         Returns\n1866         -------\n1867         unstacked: Dataset\n1868 \n1869         Examples\n1870         --------\n1871         >>> import xarray as xr\n1872         >>> arr = xr.DataArray(\n1873         ...     np.arange(6).reshape(2, 3),\n1874         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1875         ... )\n1876         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n1877         >>> data\n1878         <xarray.Dataset>\n1879         Dimensions:  (x: 2, y: 3)\n1880         Coordinates:\n1881           * x        (x) <U1 'a' 'b'\n1882           * y        (y) int64 0 1 2\n1883         Data variables:\n1884             a        (x, y) int64 0 1 2 3 4 5\n1885             b        (x) int64 0 3\n1886         >>> stacked = data.to_stacked_array(\"z\", [\"y\"])\n1887         >>> stacked.indexes[\"z\"]\n1888         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1889                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n1890                 names=['variable', 'y'])\n1891         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n1892         >>> data.identical(roundtripped)\n1893         True\n1894 \n1895         See Also\n1896         --------\n1897         Dataset.to_stacked_array\n1898         \"\"\"\n1899 \n1900         idx = self.indexes[dim]\n1901         if not isinstance(idx, pd.MultiIndex):\n1902             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n1903 \n1904         level_number = idx._get_level_number(level)\n1905         variables = idx.levels[level_number]\n1906         variable_dim = idx.names[level_number]\n1907 \n1908         # pull variables out of datarray\n1909         data_dict = {}\n1910         for k in variables:\n1911             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n1912 \n1913         # unstacked dataset\n1914         return Dataset(data_dict)\n1915 \n1916     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> \"DataArray\":\n1917         \"\"\"Return a new DataArray object with transposed dimensions.\n1918 \n1919         Parameters\n1920         ----------\n1921         *dims : hashable, optional\n1922             By default, reverse the dimensions. Otherwise, reorder the\n1923             dimensions to this order.\n1924         transpose_coords : boolean, optional\n1925             If True, also transpose the coordinates of this DataArray.\n1926 \n1927         Returns\n1928         -------\n1929         transposed : DataArray\n1930             The returned DataArray's array is transposed.\n1931 \n1932         Notes\n1933         -----\n1934         This operation returns a view of this array's data. It is\n1935         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n1936         -- the data will be fully loaded.\n1937 \n1938         See Also\n1939         --------\n1940         numpy.transpose\n1941         Dataset.transpose\n1942         \"\"\"\n1943         if dims:\n1944             dims = tuple(utils.infix_dims(dims, self.dims))\n1945         variable = self.variable.transpose(*dims)\n1946         if transpose_coords:\n1947             coords: Dict[Hashable, Variable] = {}\n1948             for name, coord in self.coords.items():\n1949                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n1950                 coords[name] = coord.variable.transpose(*coord_dims)\n1951             return self._replace(variable, coords)\n1952         else:\n1953             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):\n1954                 warnings.warn(\n1955                     \"This DataArray contains multi-dimensional \"\n1956                     \"coordinates. In the future, these coordinates \"\n1957                     \"will be transposed as well unless you specify \"\n1958                     \"transpose_coords=False.\",\n1959                     FutureWarning,\n1960                     stacklevel=2,\n1961                 )\n1962             return self._replace(variable)\n1963 \n1964     @property\n1965     def T(self) -> \"DataArray\":\n1966         return self.transpose()\n1967 \n1968     def drop_vars(\n1969         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n1970     ) -> \"DataArray\":\n1971         \"\"\"Drop variables from this DataArray.\n1972 \n1973         Parameters\n1974         ----------\n1975         names : hashable or iterable of hashables\n1976             Name(s) of variables to drop.\n1977         errors: {'raise', 'ignore'}, optional\n1978             If 'raise' (default), raises a ValueError error if any of the variable\n1979             passed are not in the dataset. If 'ignore', any given names that are in the\n1980             DataArray are dropped and no error is raised.\n1981 \n1982         Returns\n1983         -------\n1984         dropped : Dataset\n1985 \n1986         \"\"\"\n1987         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n1988         return self._from_temp_dataset(ds)\n1989 \n1990     def drop(\n1991         self,\n1992         labels: Mapping = None,\n1993         dim: Hashable = None,\n1994         *,\n1995         errors: str = \"raise\",\n1996         **labels_kwargs,\n1997     ) -> \"DataArray\":\n1998         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n1999 \n2000         Using either `drop_vars` or `drop_sel` is encouraged\n2001 \n2002         See Also\n2003         --------\n2004         DataArray.drop_vars\n2005         DataArray.drop_sel\n2006         \"\"\"\n2007         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n2008         return self._from_temp_dataset(ds)\n2009 \n2010     def drop_sel(\n2011         self,\n2012         labels: Mapping[Hashable, Any] = None,\n2013         *,\n2014         errors: str = \"raise\",\n2015         **labels_kwargs,\n2016     ) -> \"DataArray\":\n2017         \"\"\"Drop index labels from this DataArray.\n2018 \n2019         Parameters\n2020         ----------\n2021         labels : Mapping[Hashable, Any]\n2022             Index labels to drop\n2023         errors: {'raise', 'ignore'}, optional\n2024             If 'raise' (default), raises a ValueError error if\n2025             any of the index labels passed are not\n2026             in the dataset. If 'ignore', any given labels that are in the\n2027             dataset are dropped and no error is raised.\n2028         **labels_kwargs : {dim: label, ...}, optional\n2029             The keyword arguments form of ``dim`` and ``labels``\n2030 \n2031         Returns\n2032         -------\n2033         dropped : DataArray\n2034         \"\"\"\n2035         if labels_kwargs or isinstance(labels, dict):\n2036             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n2037 \n2038         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n2039         return self._from_temp_dataset(ds)\n2040 \n2041     def dropna(\n2042         self, dim: Hashable, how: str = \"any\", thresh: int = None\n2043     ) -> \"DataArray\":\n2044         \"\"\"Returns a new array with dropped labels for missing values along\n2045         the provided dimension.\n2046 \n2047         Parameters\n2048         ----------\n2049         dim : hashable\n2050             Dimension along which to drop missing values. Dropping along\n2051             multiple dimensions simultaneously is not yet supported.\n2052         how : {'any', 'all'}, optional\n2053             * any : if any NA values are present, drop that label\n2054             * all : if all values are NA, drop that label\n2055         thresh : int, default None\n2056             If supplied, require this many non-NA values.\n2057 \n2058         Returns\n2059         -------\n2060         DataArray\n2061         \"\"\"\n2062         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2063         return self._from_temp_dataset(ds)\n2064 \n2065     def fillna(self, value: Any) -> \"DataArray\":\n2066         \"\"\"Fill missing values in this object.\n2067 \n2068         This operation follows the normal broadcasting and alignment rules that\n2069         xarray uses for binary arithmetic, except the result is aligned to this\n2070         object (``join='left'``) instead of aligned to the intersection of\n2071         index coordinates (``join='inner'``).\n2072 \n2073         Parameters\n2074         ----------\n2075         value : scalar, ndarray or DataArray\n2076             Used to fill all matching missing values in this array. If the\n2077             argument is a DataArray, it is first aligned with (reindexed to)\n2078             this array.\n2079 \n2080         Returns\n2081         -------\n2082         DataArray\n2083         \"\"\"\n2084         if utils.is_dict_like(value):\n2085             raise TypeError(\n2086                 \"cannot provide fill value as a dictionary with \"\n2087                 \"fillna on a DataArray\"\n2088             )\n2089         out = ops.fillna(self, value)\n2090         return out\n2091 \n2092     def interpolate_na(\n2093         self,\n2094         dim: Hashable = None,\n2095         method: str = \"linear\",\n2096         limit: int = None,\n2097         use_coordinate: Union[bool, str] = True,\n2098         max_gap: Union[\n2099             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n2100         ] = None,\n2101         **kwargs: Any,\n2102     ) -> \"DataArray\":\n2103         \"\"\"Fill in NaNs by interpolating according to different methods.\n2104 \n2105         Parameters\n2106         ----------\n2107         dim : str\n2108             Specifies the dimension along which to interpolate.\n2109         method : str, optional\n2110             String indicating which method to use for interpolation:\n2111 \n2112             - 'linear': linear interpolation (Default). Additional keyword\n2113               arguments are passed to :py:func:`numpy.interp`\n2114             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2115               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2116               ``method='polynomial'``, the ``order`` keyword argument must also be\n2117               provided.\n2118             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2119               respective :py:class:`scipy.interpolate` classes.\n2120 \n2121         use_coordinate : bool, str, default True\n2122             Specifies which index to use as the x values in the interpolation\n2123             formulated as `y = f(x)`. If False, values are treated as if\n2124             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2125             used. If ``use_coordinate`` is a string, it specifies the name of a\n2126             coordinate variariable to use as the index.\n2127         limit : int, default None\n2128             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2129             or None for no limit. This filling is done regardless of the size of\n2130             the gap in the data. To only interpolate over gaps less than a given length,\n2131             see ``max_gap``.\n2132         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n2133             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2134             Use None for no limit. When interpolating along a datetime64 dimension\n2135             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2136 \n2137             - a string that is valid input for pandas.to_timedelta\n2138             - a :py:class:`numpy.timedelta64` object\n2139             - a :py:class:`pandas.Timedelta` object\n2140             - a :py:class:`datetime.timedelta` object\n2141 \n2142             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2143             dimensions has not been implemented yet. Gap length is defined as the difference\n2144             between coordinate values at the first data point after a gap and the last value\n2145             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2146             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2147             For example, consider::\n2148 \n2149                 <xarray.DataArray (x: 9)>\n2150                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2151                 Coordinates:\n2152                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2153 \n2154             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2155         kwargs : dict, optional\n2156             parameters passed verbatim to the underlying interpolation function\n2157 \n2158         Returns\n2159         -------\n2160         interpolated: DataArray\n2161             Filled in DataArray.\n2162 \n2163         See also\n2164         --------\n2165         numpy.interp\n2166         scipy.interpolate\n2167         \"\"\"\n2168         from .missing import interp_na\n2169 \n2170         return interp_na(\n2171             self,\n2172             dim=dim,\n2173             method=method,\n2174             limit=limit,\n2175             use_coordinate=use_coordinate,\n2176             max_gap=max_gap,\n2177             **kwargs,\n2178         )\n2179 \n2180     def ffill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2181         \"\"\"Fill NaN values by propogating values forward\n2182 \n2183         *Requires bottleneck.*\n2184 \n2185         Parameters\n2186         ----------\n2187         dim : hashable\n2188             Specifies the dimension along which to propagate values when\n2189             filling.\n2190         limit : int, default None\n2191             The maximum number of consecutive NaN values to forward fill. In\n2192             other words, if there is a gap with more than this number of\n2193             consecutive NaNs, it will only be partially filled. Must be greater\n2194             than 0 or None for no limit.\n2195 \n2196         Returns\n2197         -------\n2198         DataArray\n2199         \"\"\"\n2200         from .missing import ffill\n2201 \n2202         return ffill(self, dim, limit=limit)\n2203 \n2204     def bfill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2205         \"\"\"Fill NaN values by propogating values backward\n2206 \n2207         *Requires bottleneck.*\n2208 \n2209         Parameters\n2210         ----------\n2211         dim : str\n2212             Specifies the dimension along which to propagate values when\n2213             filling.\n2214         limit : int, default None\n2215             The maximum number of consecutive NaN values to backward fill. In\n2216             other words, if there is a gap with more than this number of\n2217             consecutive NaNs, it will only be partially filled. Must be greater\n2218             than 0 or None for no limit.\n2219 \n2220         Returns\n2221         -------\n2222         DataArray\n2223         \"\"\"\n2224         from .missing import bfill\n2225 \n2226         return bfill(self, dim, limit=limit)\n2227 \n2228     def combine_first(self, other: \"DataArray\") -> \"DataArray\":\n2229         \"\"\"Combine two DataArray objects, with union of coordinates.\n2230 \n2231         This operation follows the normal broadcasting and alignment rules of\n2232         ``join='outer'``.  Default to non-null values of array calling the\n2233         method.  Use np.nan to fill in vacant cells after alignment.\n2234 \n2235         Parameters\n2236         ----------\n2237         other : DataArray\n2238             Used to fill all matching missing values in this array.\n2239 \n2240         Returns\n2241         -------\n2242         DataArray\n2243         \"\"\"\n2244         return ops.fillna(self, other, join=\"outer\")\n2245 \n2246     def reduce(\n2247         self,\n2248         func: Callable[..., Any],\n2249         dim: Union[None, Hashable, Sequence[Hashable]] = None,\n2250         axis: Union[None, int, Sequence[int]] = None,\n2251         keep_attrs: bool = None,\n2252         keepdims: bool = False,\n2253         **kwargs: Any,\n2254     ) -> \"DataArray\":\n2255         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2256 \n2257         Parameters\n2258         ----------\n2259         func : function\n2260             Function which can be called in the form\n2261             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2262             np.ndarray over an integer valued axis.\n2263         dim : hashable or sequence of hashables, optional\n2264             Dimension(s) over which to apply `func`.\n2265         axis : int or sequence of int, optional\n2266             Axis(es) over which to repeatedly apply `func`. Only one of the\n2267             'dim' and 'axis' arguments can be supplied. If neither are\n2268             supplied, then the reduction is calculated over the flattened array\n2269             (by calling `f(x)` without an axis argument).\n2270         keep_attrs : bool, optional\n2271             If True, the variable's attributes (`attrs`) will be copied from\n2272             the original object to the new one.  If False (default), the new\n2273             object will be returned without attributes.\n2274         keepdims : bool, default False\n2275             If True, the dimensions which are reduced are left in the result\n2276             as dimensions of size one. Coordinates that use these dimensions\n2277             are removed.\n2278         **kwargs : dict\n2279             Additional keyword arguments passed on to `func`.\n2280 \n2281         Returns\n2282         -------\n2283         reduced : DataArray\n2284             DataArray with this object's array replaced with an array with\n2285             summarized data and the indicated dimension(s) removed.\n2286         \"\"\"\n2287 \n2288         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2289         return self._replace_maybe_drop_dims(var)\n2290 \n2291     def to_pandas(self) -> Union[\"DataArray\", pd.Series, pd.DataFrame]:\n2292         \"\"\"Convert this array into a pandas object with the same shape.\n2293 \n2294         The type of the returned object depends on the number of DataArray\n2295         dimensions:\n2296 \n2297         * 0D -> `xarray.DataArray`\n2298         * 1D -> `pandas.Series`\n2299         * 2D -> `pandas.DataFrame`\n2300 \n2301         Only works for arrays with 2 or fewer dimensions.\n2302 \n2303         The DataArray constructor performs the inverse transformation.\n2304         \"\"\"\n2305         # TODO: consolidate the info about pandas constructors and the\n2306         # attributes that correspond to their indexes into a separate module?\n2307         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2308         try:\n2309             constructor = constructors[self.ndim]\n2310         except KeyError:\n2311             raise ValueError(\n2312                 \"cannot convert arrays with %s dimensions into \"\n2313                 \"pandas objects\" % self.ndim\n2314             )\n2315         indexes = [self.get_index(dim) for dim in self.dims]\n2316         return constructor(self.values, *indexes)\n2317 \n2318     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n2319         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2320 \n2321         The DataFrame is indexed by the Cartesian product of index coordinates\n2322         (in the form of a :py:class:`pandas.MultiIndex`).\n2323 \n2324         Other coordinates are included as columns in the DataFrame.\n2325         \"\"\"\n2326         if name is None:\n2327             name = self.name\n2328         if name is None:\n2329             raise ValueError(\n2330                 \"cannot convert an unnamed DataArray to a \"\n2331                 \"DataFrame: use the ``name`` parameter\"\n2332             )\n2333 \n2334         dims = dict(zip(self.dims, self.shape))\n2335         # By using a unique name, we can convert a DataArray into a DataFrame\n2336         # even if it shares a name with one of its coordinates.\n2337         # I would normally use unique_name = object() but that results in a\n2338         # dataframe with columns in the wrong order, for reasons I have not\n2339         # been able to debug (possibly a pandas bug?).\n2340         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n2341         ds = self._to_dataset_whole(name=unique_name)\n2342         df = ds._to_dataframe(dims)\n2343         df.columns = [name if c == unique_name else c for c in df.columns]\n2344         return df\n2345 \n2346     def to_series(self) -> pd.Series:\n2347         \"\"\"Convert this array into a pandas.Series.\n2348 \n2349         The Series is indexed by the Cartesian product of index coordinates\n2350         (in the form of a :py:class:`pandas.MultiIndex`).\n2351         \"\"\"\n2352         index = self.coords.to_index()\n2353         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n2354 \n2355     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n2356         \"\"\"Convert this array into a numpy.ma.MaskedArray\n2357 \n2358         Parameters\n2359         ----------\n2360         copy : bool\n2361             If True (default) make a copy of the array in the result. If False,\n2362             a MaskedArray view of DataArray.values is returned.\n2363 \n2364         Returns\n2365         -------\n2366         result : MaskedArray\n2367             Masked where invalid values (nan or inf) occur.\n2368         \"\"\"\n2369         values = self.values  # only compute lazy arrays once\n2370         isnull = pd.isnull(values)\n2371         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n2372 \n2373     def to_netcdf(self, *args, **kwargs) -> Union[bytes, \"Delayed\", None]:\n2374         \"\"\"Write DataArray contents to a netCDF file.\n2375 \n2376         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n2377 \n2378         Notes\n2379         -----\n2380         Only xarray.Dataset objects can be written to netCDF files, so\n2381         the xarray.DataArray is converted to a xarray.Dataset object\n2382         containing a single variable. If the DataArray has no name, or if the\n2383         name is the same as a co-ordinate name, then it is given the name\n2384         '__xarray_dataarray_variable__'.\n2385         \"\"\"\n2386         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n2387 \n2388         if self.name is None:\n2389             # If no name is set then use a generic xarray name\n2390             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2391         elif self.name in self.coords or self.name in self.dims:\n2392             # The name is the same as one of the coords names, which netCDF\n2393             # doesn't support, so rename it but keep track of the old name\n2394             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2395             dataset.attrs[DATAARRAY_NAME] = self.name\n2396         else:\n2397             # No problems with the name - so we're fine!\n2398             dataset = self.to_dataset()\n2399 \n2400         return dataset.to_netcdf(*args, **kwargs)\n2401 \n2402     def to_dict(self, data: bool = True) -> dict:\n2403         \"\"\"\n2404         Convert this xarray.DataArray into a dictionary following xarray\n2405         naming conventions.\n2406 \n2407         Converts all variables and attributes to native Python objects.\n2408         Useful for converting to json. To avoid datetime incompatibility\n2409         use decode_times=False kwarg in xarrray.open_dataset.\n2410 \n2411         Parameters\n2412         ----------\n2413         data : bool, optional\n2414             Whether to include the actual data in the dictionary. When set to\n2415             False, returns just the schema.\n2416 \n2417         See also\n2418         --------\n2419         DataArray.from_dict\n2420         \"\"\"\n2421         d = self.variable.to_dict(data=data)\n2422         d.update({\"coords\": {}, \"name\": self.name})\n2423         for k in self.coords:\n2424             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n2425         return d\n2426 \n2427     @classmethod\n2428     def from_dict(cls, d: dict) -> \"DataArray\":\n2429         \"\"\"\n2430         Convert a dictionary into an xarray.DataArray\n2431 \n2432         Input dict can take several forms::\n2433 \n2434             d = {'dims': ('t'), 'data': x}\n2435 \n2436             d = {'coords': {'t': {'dims': 't', 'data': t,\n2437                                   'attrs': {'units':'s'}}},\n2438                  'attrs': {'title': 'air temperature'},\n2439                  'dims': 't',\n2440                  'data': x,\n2441                  'name': 'a'}\n2442 \n2443         where 't' is the name of the dimesion, 'a' is the name of the array,\n2444         and  x and t are lists, numpy.arrays, or pandas objects.\n2445 \n2446         Parameters\n2447         ----------\n2448         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n2449 \n2450         Returns\n2451         -------\n2452         obj : xarray.DataArray\n2453 \n2454         See also\n2455         --------\n2456         DataArray.to_dict\n2457         Dataset.from_dict\n2458         \"\"\"\n2459         coords = None\n2460         if \"coords\" in d:\n2461             try:\n2462                 coords = {\n2463                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n2464                     for k, v in d[\"coords\"].items()\n2465                 }\n2466             except KeyError as e:\n2467                 raise ValueError(\n2468                     \"cannot convert dict when coords are missing the key \"\n2469                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n2470                 )\n2471         try:\n2472             data = d[\"data\"]\n2473         except KeyError:\n2474             raise ValueError(\"cannot convert dict without the key 'data''\")\n2475         else:\n2476             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n2477         return obj\n2478 \n2479     @classmethod\n2480     def from_series(cls, series: pd.Series, sparse: bool = False) -> \"DataArray\":\n2481         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n2482 \n2483         If the series's index is a MultiIndex, it will be expanded into a\n2484         tensor product of one-dimensional coordinates (filling in missing\n2485         values with NaN). Thus this operation should be the inverse of the\n2486         `to_series` method.\n2487 \n2488         If sparse=True, creates a sparse array instead of a dense NumPy array.\n2489         Requires the pydata/sparse package.\n2490 \n2491         See also\n2492         --------\n2493         xarray.Dataset.from_dataframe\n2494         \"\"\"\n2495         temp_name = \"__temporary_name\"\n2496         df = pd.DataFrame({temp_name: series})\n2497         ds = Dataset.from_dataframe(df, sparse=sparse)\n2498         result = cast(DataArray, ds[temp_name])\n2499         result.name = series.name\n2500         return result\n2501 \n2502     def to_cdms2(self) -> \"cdms2_Variable\":\n2503         \"\"\"Convert this array into a cdms2.Variable\n2504         \"\"\"\n2505         from ..convert import to_cdms2\n2506 \n2507         return to_cdms2(self)\n2508 \n2509     @classmethod\n2510     def from_cdms2(cls, variable: \"cdms2_Variable\") -> \"DataArray\":\n2511         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\n2512         \"\"\"\n2513         from ..convert import from_cdms2\n2514 \n2515         return from_cdms2(variable)\n2516 \n2517     def to_iris(self) -> \"iris_Cube\":\n2518         \"\"\"Convert this array into a iris.cube.Cube\n2519         \"\"\"\n2520         from ..convert import to_iris\n2521 \n2522         return to_iris(self)\n2523 \n2524     @classmethod\n2525     def from_iris(cls, cube: \"iris_Cube\") -> \"DataArray\":\n2526         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\n2527         \"\"\"\n2528         from ..convert import from_iris\n2529 \n2530         return from_iris(cube)\n2531 \n2532     def _all_compat(self, other: \"DataArray\", compat_str: str) -> bool:\n2533         \"\"\"Helper function for equals, broadcast_equals, and identical\n2534         \"\"\"\n2535 \n2536         def compat(x, y):\n2537             return getattr(x.variable, compat_str)(y.variable)\n2538 \n2539         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n2540             self, other\n2541         )\n2542 \n2543     def broadcast_equals(self, other: \"DataArray\") -> bool:\n2544         \"\"\"Two DataArrays are broadcast equal if they are equal after\n2545         broadcasting them against each other such that they have the same\n2546         dimensions.\n2547 \n2548         See Also\n2549         --------\n2550         DataArray.equals\n2551         DataArray.identical\n2552         \"\"\"\n2553         try:\n2554             return self._all_compat(other, \"broadcast_equals\")\n2555         except (TypeError, AttributeError):\n2556             return False\n2557 \n2558     def equals(self, other: \"DataArray\") -> bool:\n2559         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n2560         values; otherwise False.\n2561 \n2562         DataArrays can still be equal (like pandas objects) if they have NaN\n2563         values in the same locations.\n2564 \n2565         This method is necessary because `v1 == v2` for ``DataArray``\n2566         does element-wise comparisons (like numpy.ndarrays).\n2567 \n2568         See Also\n2569         --------\n2570         DataArray.broadcast_equals\n2571         DataArray.identical\n2572         \"\"\"\n2573         try:\n2574             return self._all_compat(other, \"equals\")\n2575         except (TypeError, AttributeError):\n2576             return False\n2577 \n2578     def identical(self, other: \"DataArray\") -> bool:\n2579         \"\"\"Like equals, but also checks the array name and attributes, and\n2580         attributes on all coordinates.\n2581 \n2582         See Also\n2583         --------\n2584         DataArray.broadcast_equals\n2585         DataArray.equal\n2586         \"\"\"\n2587         try:\n2588             return self.name == other.name and self._all_compat(other, \"identical\")\n2589         except (TypeError, AttributeError):\n2590             return False\n2591 \n2592     def _result_name(self, other: Any = None) -> Optional[Hashable]:\n2593         # use the same naming heuristics as pandas:\n2594         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n2595         other_name = getattr(other, \"name\", _default)\n2596         if other_name is _default or other_name == self.name:\n2597             return self.name\n2598         else:\n2599             return None\n2600 \n2601     def __array_wrap__(self, obj, context=None) -> \"DataArray\":\n2602         new_var = self.variable.__array_wrap__(obj, context)\n2603         return self._replace(new_var)\n2604 \n2605     def __matmul__(self, obj):\n2606         return self.dot(obj)\n2607 \n2608     def __rmatmul__(self, other):\n2609         # currently somewhat duplicative, as only other DataArrays are\n2610         # compatible with matmul\n2611         return computation.dot(other, self)\n2612 \n2613     @staticmethod\n2614     def _unary_op(f: Callable[..., Any]) -> Callable[..., \"DataArray\"]:\n2615         @functools.wraps(f)\n2616         def func(self, *args, **kwargs):\n2617             with np.errstate(all=\"ignore\"):\n2618                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n2619 \n2620         return func\n2621 \n2622     @staticmethod\n2623     def _binary_op(\n2624         f: Callable[..., Any],\n2625         reflexive: bool = False,\n2626         join: str = None,  # see xarray.align\n2627         **ignored_kwargs,\n2628     ) -> Callable[..., \"DataArray\"]:\n2629         @functools.wraps(f)\n2630         def func(self, other):\n2631             if isinstance(other, (Dataset, groupby.GroupBy)):\n2632                 return NotImplemented\n2633             if isinstance(other, DataArray):\n2634                 align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n2635                 self, other = align(self, other, join=align_type, copy=False)\n2636             other_variable = getattr(other, \"variable\", other)\n2637             other_coords = getattr(other, \"coords\", None)\n2638 \n2639             variable = (\n2640                 f(self.variable, other_variable)\n2641                 if not reflexive\n2642                 else f(other_variable, self.variable)\n2643             )\n2644             coords, indexes = self.coords._merge_raw(other_coords)\n2645             name = self._result_name(other)\n2646 \n2647             return self._replace(variable, coords, name, indexes=indexes)\n2648 \n2649         return func\n2650 \n2651     @staticmethod\n2652     def _inplace_binary_op(f: Callable) -> Callable[..., \"DataArray\"]:\n2653         @functools.wraps(f)\n2654         def func(self, other):\n2655             if isinstance(other, groupby.GroupBy):\n2656                 raise TypeError(\n2657                     \"in-place operations between a DataArray and \"\n2658                     \"a grouped object are not permitted\"\n2659                 )\n2660             # n.b. we can't align other to self (with other.reindex_like(self))\n2661             # because `other` may be converted into floats, which would cause\n2662             # in-place arithmetic to fail unpredictably. Instead, we simply\n2663             # don't support automatic alignment with in-place arithmetic.\n2664             other_coords = getattr(other, \"coords\", None)\n2665             other_variable = getattr(other, \"variable\", other)\n2666             with self.coords._merge_inplace(other_coords):\n2667                 f(self.variable, other_variable)\n2668             return self\n2669 \n2670         return func\n2671 \n2672     def _copy_attrs_from(self, other: Union[\"DataArray\", Dataset, Variable]) -> None:\n2673         self.attrs = other.attrs\n2674 \n2675     @property\n2676     def plot(self) -> _PlotMethods:\n2677         \"\"\"\n2678         Access plotting functions for DataArray's\n2679 \n2680         >>> d = xr.DataArray([[1, 2], [3, 4]])\n2681 \n2682         For convenience just call this directly\n2683 \n2684         >>> d.plot()\n2685 \n2686         Or use it as a namespace to use xarray.plot functions as\n2687         DataArray methods\n2688 \n2689         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)\n2690 \n2691         \"\"\"\n2692         return _PlotMethods(self)\n2693 \n2694     def _title_for_slice(self, truncate: int = 50) -> str:\n2695         \"\"\"\n2696         If the dataarray has 1 dimensional coordinates or comes from a slice\n2697         we can show that info in the title\n2698 \n2699         Parameters\n2700         ----------\n2701         truncate : integer\n2702             maximum number of characters for title\n2703 \n2704         Returns\n2705         -------\n2706         title : string\n2707             Can be used for plot titles\n2708 \n2709         \"\"\"\n2710         one_dims = []\n2711         for dim, coord in self.coords.items():\n2712             if coord.size == 1:\n2713                 one_dims.append(\n2714                     \"{dim} = {v}\".format(dim=dim, v=format_item(coord.values))\n2715                 )\n2716 \n2717         title = \", \".join(one_dims)\n2718         if len(title) > truncate:\n2719             title = title[: (truncate - 3)] + \"...\"\n2720 \n2721         return title\n2722 \n2723     def diff(self, dim: Hashable, n: int = 1, label: Hashable = \"upper\") -> \"DataArray\":\n2724         \"\"\"Calculate the n-th order discrete difference along given axis.\n2725 \n2726         Parameters\n2727         ----------\n2728         dim : hashable\n2729             Dimension over which to calculate the finite difference.\n2730         n : int, optional\n2731             The number of times values are differenced.\n2732         label : hashable, optional\n2733             The new coordinate in dimension ``dim`` will have the\n2734             values of either the minuend's or subtrahend's coordinate\n2735             for values 'upper' and 'lower', respectively.  Other\n2736             values are not supported.\n2737 \n2738         Returns\n2739         -------\n2740         difference : same type as caller\n2741             The n-th order finite difference of this object.\n2742 \n2743         .. note::\n2744 \n2745             `n` matches numpy's behavior and is different from pandas' first\n2746             argument named `periods`.\n2747 \n2748 \n2749         Examples\n2750         --------\n2751         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n2752         >>> arr.diff(\"x\")\n2753         <xarray.DataArray (x: 3)>\n2754         array([0, 1, 0])\n2755         Coordinates:\n2756         * x        (x) int64 2 3 4\n2757         >>> arr.diff(\"x\", 2)\n2758         <xarray.DataArray (x: 2)>\n2759         array([ 1, -1])\n2760         Coordinates:\n2761         * x        (x) int64 3 4\n2762 \n2763         See Also\n2764         --------\n2765         DataArray.differentiate\n2766         \"\"\"\n2767         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n2768         return self._from_temp_dataset(ds)\n2769 \n2770     def shift(\n2771         self,\n2772         shifts: Mapping[Hashable, int] = None,\n2773         fill_value: Any = dtypes.NA,\n2774         **shifts_kwargs: int,\n2775     ) -> \"DataArray\":\n2776         \"\"\"Shift this array by an offset along one or more dimensions.\n2777 \n2778         Only the data is moved; coordinates stay in place. Values shifted from\n2779         beyond array bounds are replaced by NaN. This is consistent with the\n2780         behavior of ``shift`` in pandas.\n2781 \n2782         Parameters\n2783         ----------\n2784         shifts : Mapping with the form of {dim: offset}\n2785             Integer offset to shift along each of the given dimensions.\n2786             Positive offsets shift to the right; negative offsets shift to the\n2787             left.\n2788         fill_value: scalar, optional\n2789             Value to use for newly missing values\n2790         **shifts_kwargs:\n2791             The keyword arguments form of ``shifts``.\n2792             One of shifts or shifts_kwargs must be provided.\n2793 \n2794         Returns\n2795         -------\n2796         shifted : DataArray\n2797             DataArray with the same coordinates and attributes but shifted\n2798             data.\n2799 \n2800         See also\n2801         --------\n2802         roll\n2803 \n2804         Examples\n2805         --------\n2806 \n2807         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2808         >>> arr.shift(x=1)\n2809         <xarray.DataArray (x: 3)>\n2810         array([ nan,   5.,   6.])\n2811         Coordinates:\n2812           * x        (x) int64 0 1 2\n2813         \"\"\"\n2814         variable = self.variable.shift(\n2815             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n2816         )\n2817         return self._replace(variable=variable)\n2818 \n2819     def roll(\n2820         self,\n2821         shifts: Mapping[Hashable, int] = None,\n2822         roll_coords: bool = None,\n2823         **shifts_kwargs: int,\n2824     ) -> \"DataArray\":\n2825         \"\"\"Roll this array by an offset along one or more dimensions.\n2826 \n2827         Unlike shift, roll may rotate all variables, including coordinates\n2828         if specified. The direction of rotation is consistent with\n2829         :py:func:`numpy.roll`.\n2830 \n2831         Parameters\n2832         ----------\n2833         shifts : Mapping with the form of {dim: offset}\n2834             Integer offset to rotate each of the given dimensions.\n2835             Positive offsets roll to the right; negative offsets roll to the\n2836             left.\n2837         roll_coords : bool\n2838             Indicates whether to  roll the coordinates by the offset\n2839             The current default of roll_coords (None, equivalent to True) is\n2840             deprecated and will change to False in a future version.\n2841             Explicitly pass roll_coords to silence the warning.\n2842         **shifts_kwargs : The keyword arguments form of ``shifts``.\n2843             One of shifts or shifts_kwargs must be provided.\n2844 \n2845         Returns\n2846         -------\n2847         rolled : DataArray\n2848             DataArray with the same attributes but rolled data and coordinates.\n2849 \n2850         See also\n2851         --------\n2852         shift\n2853 \n2854         Examples\n2855         --------\n2856 \n2857         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2858         >>> arr.roll(x=1)\n2859         <xarray.DataArray (x: 3)>\n2860         array([7, 5, 6])\n2861         Coordinates:\n2862           * x        (x) int64 2 0 1\n2863         \"\"\"\n2864         ds = self._to_temp_dataset().roll(\n2865             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n2866         )\n2867         return self._from_temp_dataset(ds)\n2868 \n2869     @property\n2870     def real(self) -> \"DataArray\":\n2871         return self._replace(self.variable.real)\n2872 \n2873     @property\n2874     def imag(self) -> \"DataArray\":\n2875         return self._replace(self.variable.imag)\n2876 \n2877     def dot(\n2878         self, other: \"DataArray\", dims: Union[Hashable, Sequence[Hashable], None] = None\n2879     ) -> \"DataArray\":\n2880         \"\"\"Perform dot product of two DataArrays along their shared dims.\n2881 \n2882         Equivalent to taking taking tensordot over all shared dims.\n2883 \n2884         Parameters\n2885         ----------\n2886         other : DataArray\n2887             The other array with which the dot product is performed.\n2888         dims: '...', hashable or sequence of hashables, optional\n2889             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.\n2890             If not specified, then all the common dimensions are summed over.\n2891 \n2892         Returns\n2893         -------\n2894         result : DataArray\n2895             Array resulting from the dot product over all shared dimensions.\n2896 \n2897         See also\n2898         --------\n2899         dot\n2900         numpy.tensordot\n2901 \n2902         Examples\n2903         --------\n2904 \n2905         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n2906         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n2907         >>> dm_vals = np.arange(4)\n2908         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n2909 \n2910         >>> dm.dims\n2911         ('z')\n2912 \n2913         >>> da.dims\n2914         ('x', 'y', 'z')\n2915 \n2916         >>> dot_result = da.dot(dm)\n2917         >>> dot_result.dims\n2918         ('x', 'y')\n2919 \n2920         \"\"\"\n2921         if isinstance(other, Dataset):\n2922             raise NotImplementedError(\n2923                 \"dot products are not yet supported with Dataset objects.\"\n2924             )\n2925         if not isinstance(other, DataArray):\n2926             raise TypeError(\"dot only operates on DataArrays.\")\n2927 \n2928         return computation.dot(self, other, dims=dims)\n2929 \n2930     def sortby(\n2931         self,\n2932         variables: Union[Hashable, \"DataArray\", Sequence[Union[Hashable, \"DataArray\"]]],\n2933         ascending: bool = True,\n2934     ) -> \"DataArray\":\n2935         \"\"\"Sort object by labels or values (along an axis).\n2936 \n2937         Sorts the dataarray, either along specified dimensions,\n2938         or according to values of 1-D dataarrays that share dimension\n2939         with calling object.\n2940 \n2941         If the input variables are dataarrays, then the dataarrays are aligned\n2942         (via left-join) to the calling object prior to sorting by cell values.\n2943         NaNs are sorted to the end, following Numpy convention.\n2944 \n2945         If multiple sorts along the same dimension is\n2946         given, numpy's lexsort is performed along that dimension:\n2947         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n2948         and the FIRST key in the sequence is used as the primary sort key,\n2949         followed by the 2nd key, etc.\n2950 \n2951         Parameters\n2952         ----------\n2953         variables: hashable, DataArray, or sequence of either\n2954             1D DataArray objects or name(s) of 1D variable(s) in\n2955             coords whose values are used to sort this array.\n2956         ascending: boolean, optional\n2957             Whether to sort by ascending or descending order.\n2958 \n2959         Returns\n2960         -------\n2961         sorted: DataArray\n2962             A new dataarray where all the specified dims are sorted by dim\n2963             labels.\n2964 \n2965         Examples\n2966         --------\n2967 \n2968         >>> da = xr.DataArray(\n2969         ...     np.random.rand(5),\n2970         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n2971         ...     dims=\"time\",\n2972         ... )\n2973         >>> da\n2974         <xarray.DataArray (time: 5)>\n2975         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n2976         Coordinates:\n2977           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n2978 \n2979         >>> da.sortby(da)\n2980         <xarray.DataArray (time: 5)>\n2981         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n2982         Coordinates:\n2983           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n2984         \"\"\"\n2985         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n2986         return self._from_temp_dataset(ds)\n2987 \n2988     def quantile(\n2989         self,\n2990         q: Any,\n2991         dim: Union[Hashable, Sequence[Hashable], None] = None,\n2992         interpolation: str = \"linear\",\n2993         keep_attrs: bool = None,\n2994         skipna: bool = True,\n2995     ) -> \"DataArray\":\n2996         \"\"\"Compute the qth quantile of the data along the specified dimension.\n2997 \n2998         Returns the qth quantiles(s) of the array elements.\n2999 \n3000         Parameters\n3001         ----------\n3002         q : float in range of [0,1] or array-like of floats\n3003             Quantile to compute, which must be between 0 and 1 inclusive.\n3004         dim : hashable or sequence of hashable, optional\n3005             Dimension(s) over which to apply quantile.\n3006         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n3007             This optional parameter specifies the interpolation method to\n3008             use when the desired quantile lies between two data points\n3009             ``i < j``:\n3010 \n3011                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n3012                   the fractional part of the index surrounded by ``i`` and\n3013                   ``j``.\n3014                 - lower: ``i``.\n3015                 - higher: ``j``.\n3016                 - nearest: ``i`` or ``j``, whichever is nearest.\n3017                 - midpoint: ``(i + j) / 2``.\n3018         keep_attrs : bool, optional\n3019             If True, the dataset's attributes (`attrs`) will be copied from\n3020             the original object to the new one.  If False (default), the new\n3021             object will be returned without attributes.\n3022         skipna : bool, optional\n3023             Whether to skip missing values when aggregating.\n3024 \n3025         Returns\n3026         -------\n3027         quantiles : DataArray\n3028             If `q` is a single quantile, then the result\n3029             is a scalar. If multiple percentiles are given, first axis of\n3030             the result corresponds to the quantile and a quantile dimension\n3031             is added to the return array. The other dimensions are the\n3032             dimensions that remain after the reduction of the array.\n3033 \n3034         See Also\n3035         --------\n3036         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n3037 \n3038         Examples\n3039         --------\n3040 \n3041         >>> da = xr.DataArray(\n3042         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n3043         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n3044         ...     dims=(\"x\", \"y\"),\n3045         ... )\n3046         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3047         <xarray.DataArray ()>\n3048         array(0.7)\n3049         Coordinates:\n3050             quantile  float64 0.0\n3051         >>> da.quantile(0, dim=\"x\")\n3052         <xarray.DataArray (y: 4)>\n3053         array([0.7, 4.2, 2.6, 1.5])\n3054         Coordinates:\n3055           * y         (y) float64 1.0 1.5 2.0 2.5\n3056             quantile  float64 0.0\n3057         >>> da.quantile([0, 0.5, 1])\n3058         <xarray.DataArray (quantile: 3)>\n3059         array([0.7, 3.4, 9.4])\n3060         Coordinates:\n3061           * quantile  (quantile) float64 0.0 0.5 1.0\n3062         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3063         <xarray.DataArray (quantile: 3, y: 4)>\n3064         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3065                [3.6 , 5.75, 6.  , 1.7 ],\n3066                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3067         Coordinates:\n3068           * y         (y) float64 1.0 1.5 2.0 2.5\n3069           * quantile  (quantile) float64 0.0 0.5 1.0\n3070         \"\"\"\n3071 \n3072         ds = self._to_temp_dataset().quantile(\n3073             q,\n3074             dim=dim,\n3075             keep_attrs=keep_attrs,\n3076             interpolation=interpolation,\n3077             skipna=skipna,\n3078         )\n3079         return self._from_temp_dataset(ds)\n3080 \n3081     def rank(\n3082         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n3083     ) -> \"DataArray\":\n3084         \"\"\"Ranks the data.\n3085 \n3086         Equal values are assigned a rank that is the average of the ranks that\n3087         would have been otherwise assigned to all of the values within that\n3088         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n3089 \n3090         NaNs in the input array are returned as NaNs.\n3091 \n3092         The `bottleneck` library is required.\n3093 \n3094         Parameters\n3095         ----------\n3096         dim : hashable\n3097             Dimension over which to compute rank.\n3098         pct : bool, optional\n3099             If True, compute percentage ranks, otherwise compute integer ranks.\n3100         keep_attrs : bool, optional\n3101             If True, the dataset's attributes (`attrs`) will be copied from\n3102             the original object to the new one.  If False (default), the new\n3103             object will be returned without attributes.\n3104 \n3105         Returns\n3106         -------\n3107         ranked : DataArray\n3108             DataArray with the same coordinates and dtype 'float64'.\n3109 \n3110         Examples\n3111         --------\n3112 \n3113         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3114         >>> arr.rank(\"x\")\n3115         <xarray.DataArray (x: 3)>\n3116         array([ 1.,   2.,   3.])\n3117         Dimensions without coordinates: x\n3118         \"\"\"\n3119 \n3120         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n3121         return self._from_temp_dataset(ds)\n3122 \n3123     def differentiate(\n3124         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n3125     ) -> \"DataArray\":\n3126         \"\"\" Differentiate the array with the second order accurate central\n3127         differences.\n3128 \n3129         .. note::\n3130             This feature is limited to simple cartesian geometry, i.e. coord\n3131             must be one dimensional.\n3132 \n3133         Parameters\n3134         ----------\n3135         coord: hashable\n3136             The coordinate to be used to compute the gradient.\n3137         edge_order: 1 or 2. Default 1\n3138             N-th order accurate differences at the boundaries.\n3139         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n3140             'us', 'ns', 'ps', 'fs', 'as'}\n3141             Unit to compute gradient. Only valid for datetime coordinate.\n3142 \n3143         Returns\n3144         -------\n3145         differentiated: DataArray\n3146 \n3147         See also\n3148         --------\n3149         numpy.gradient: corresponding numpy function\n3150 \n3151         Examples\n3152         --------\n3153 \n3154         >>> da = xr.DataArray(\n3155         ...     np.arange(12).reshape(4, 3),\n3156         ...     dims=[\"x\", \"y\"],\n3157         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3158         ... )\n3159         >>> da\n3160         <xarray.DataArray (x: 4, y: 3)>\n3161         array([[ 0,  1,  2],\n3162                [ 3,  4,  5],\n3163                [ 6,  7,  8],\n3164                [ 9, 10, 11]])\n3165         Coordinates:\n3166           * x        (x) float64 0.0 0.1 1.1 1.2\n3167         Dimensions without coordinates: y\n3168         >>>\n3169         >>> da.differentiate(\"x\")\n3170         <xarray.DataArray (x: 4, y: 3)>\n3171         array([[30.      , 30.      , 30.      ],\n3172                [27.545455, 27.545455, 27.545455],\n3173                [27.545455, 27.545455, 27.545455],\n3174                [30.      , 30.      , 30.      ]])\n3175         Coordinates:\n3176           * x        (x) float64 0.0 0.1 1.1 1.2\n3177         Dimensions without coordinates: y\n3178         \"\"\"\n3179         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n3180         return self._from_temp_dataset(ds)\n3181 \n3182     def integrate(\n3183         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n3184     ) -> \"DataArray\":\n3185         \"\"\" integrate the array with the trapezoidal rule.\n3186 \n3187         .. note::\n3188             This feature is limited to simple cartesian geometry, i.e. dim\n3189             must be one dimensional.\n3190 \n3191         Parameters\n3192         ----------\n3193         dim: hashable, or a sequence of hashable\n3194             Coordinate(s) used for the integration.\n3195         datetime_unit: str, optional\n3196             Can be used to specify the unit if datetime coordinate is used.\n3197             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',\n3198             'fs', 'as'}\n3199 \n3200         Returns\n3201         -------\n3202         integrated: DataArray\n3203 \n3204         See also\n3205         --------\n3206         numpy.trapz: corresponding numpy function\n3207 \n3208         Examples\n3209         --------\n3210 \n3211         >>> da = xr.DataArray(\n3212         ...     np.arange(12).reshape(4, 3),\n3213         ...     dims=[\"x\", \"y\"],\n3214         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3215         ... )\n3216         >>> da\n3217         <xarray.DataArray (x: 4, y: 3)>\n3218         array([[ 0,  1,  2],\n3219                [ 3,  4,  5],\n3220                [ 6,  7,  8],\n3221                [ 9, 10, 11]])\n3222         Coordinates:\n3223           * x        (x) float64 0.0 0.1 1.1 1.2\n3224         Dimensions without coordinates: y\n3225         >>>\n3226         >>> da.integrate(\"x\")\n3227         <xarray.DataArray (y: 3)>\n3228         array([5.4, 6.6, 7.8])\n3229         Dimensions without coordinates: y\n3230         \"\"\"\n3231         ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n3232         return self._from_temp_dataset(ds)\n3233 \n3234     def unify_chunks(self) -> \"DataArray\":\n3235         \"\"\" Unify chunk size along all chunked dimensions of this DataArray.\n3236 \n3237         Returns\n3238         -------\n3239 \n3240         DataArray with consistent chunk sizes for all dask-array variables\n3241 \n3242         See Also\n3243         --------\n3244 \n3245         dask.array.core.unify_chunks\n3246         \"\"\"\n3247         ds = self._to_temp_dataset().unify_chunks()\n3248         return self._from_temp_dataset(ds)\n3249 \n3250     def map_blocks(\n3251         self,\n3252         func: \"Callable[..., T_DSorDA]\",\n3253         args: Sequence[Any] = (),\n3254         kwargs: Mapping[str, Any] = None,\n3255     ) -> \"T_DSorDA\":\n3256         \"\"\"\n3257         Apply a function to each chunk of this DataArray. This method is experimental\n3258         and its signature may change.\n3259 \n3260         Parameters\n3261         ----------\n3262         func: callable\n3263             User-provided function that accepts a DataArray as its first parameter. The\n3264             function will receive a subset of this DataArray, corresponding to one chunk\n3265             along each chunked dimension. ``func`` will be executed as\n3266             ``func(obj_subset, *args, **kwargs)``.\n3267 \n3268             The function will be first run on mocked-up data, that looks like this array\n3269             but has sizes 0, to determine properties of the returned object such as\n3270             dtype, variable names, new dimensions and new indexes (if any).\n3271 \n3272             This function must return either a single DataArray or a single Dataset.\n3273 \n3274             This function cannot change size of existing dimensions, or add new chunked\n3275             dimensions.\n3276         args: Sequence\n3277             Passed verbatim to func after unpacking, after the sliced DataArray. xarray\n3278             objects, if any, will not be split by chunks. Passing dask collections is\n3279             not allowed.\n3280         kwargs: Mapping\n3281             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n3282             split by chunks. Passing dask collections is not allowed.\n3283 \n3284         Returns\n3285         -------\n3286         A single DataArray or Dataset with dask backend, reassembled from the outputs of\n3287         the function.\n3288 \n3289         Notes\n3290         -----\n3291         This method is designed for when one needs to manipulate a whole xarray object\n3292         within each chunk. In the more common case where one can work on numpy arrays,\n3293         it is recommended to use apply_ufunc.\n3294 \n3295         If none of the variables in this DataArray is backed by dask, calling this\n3296         method is equivalent to calling ``func(self, *args, **kwargs)``.\n3297 \n3298         See Also\n3299         --------\n3300         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,\n3301         xarray.Dataset.map_blocks\n3302         \"\"\"\n3303         from .parallel import map_blocks\n3304 \n3305         return map_blocks(func, self, args, kwargs)\n3306 \n3307     def polyfit(\n3308         self,\n3309         dim: Hashable,\n3310         deg: int,\n3311         skipna: bool = None,\n3312         rcond: float = None,\n3313         w: Union[Hashable, Any] = None,\n3314         full: bool = False,\n3315         cov: bool = False,\n3316     ):\n3317         \"\"\"\n3318         Least squares polynomial fit.\n3319 \n3320         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n3321         invalid values when `skipna = True`.\n3322 \n3323         Parameters\n3324         ----------\n3325         dim : hashable\n3326             Coordinate along which to fit the polynomials.\n3327         deg : int\n3328             Degree of the fitting polynomial.\n3329         skipna : bool, optional\n3330             If True, removes all invalid values before fitting each 1D slices of the array.\n3331             Default is True if data is stored in a dask.array or if there is any\n3332             invalid values, False otherwise.\n3333         rcond : float, optional\n3334             Relative condition number to the fit.\n3335         w : Union[Hashable, Any], optional\n3336             Weights to apply to the y-coordinate of the sample points.\n3337             Can be an array-like object or the name of a coordinate in the dataset.\n3338         full : bool, optional\n3339             Whether to return the residuals, matrix rank and singular values in addition\n3340             to the coefficients.\n3341         cov : Union[bool, str], optional\n3342             Whether to return to the covariance matrix in addition to the coefficients.\n3343             The matrix is not scaled if `cov='unscaled'`.\n3344 \n3345         Returns\n3346         -------\n3347         polyfit_results : Dataset\n3348             A single dataset which contains:\n3349 \n3350             polyfit_coefficients\n3351                 The coefficients of the best fit.\n3352             polyfit_residuals\n3353                 The residuals of the least-square computation (only included if `full=True`)\n3354             [dim]_matrix_rank\n3355                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3356             [dim]_singular_value\n3357                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3358             polyfit_covariance\n3359                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n3360 \n3361         See also\n3362         --------\n3363         numpy.polyfit\n3364         \"\"\"\n3365         return self._to_temp_dataset().polyfit(\n3366             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n3367         )\n3368 \n3369     def pad(\n3370         self,\n3371         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n3372         mode: str = \"constant\",\n3373         stat_length: Union[\n3374             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3375         ] = None,\n3376         constant_values: Union[\n3377             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3378         ] = None,\n3379         end_values: Union[\n3380             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3381         ] = None,\n3382         reflect_type: str = None,\n3383         **pad_width_kwargs: Any,\n3384     ) -> \"DataArray\":\n3385         \"\"\"Pad this array along one or more dimensions.\n3386 \n3387         .. warning::\n3388             This function is experimental and its behaviour is likely to change\n3389             especially regarding padding of dimension coordinates (or IndexVariables).\n3390 \n3391         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n3392         coordinates will be padded with the same mode, otherwise coordinates\n3393         are padded using the \"constant\" mode with fill_value dtypes.NA.\n3394 \n3395         Parameters\n3396         ----------\n3397         pad_width : Mapping with the form of {dim: (pad_before, pad_after)}\n3398             Number of values padded along each dimension.\n3399             {dim: pad} is a shortcut for pad_before = pad_after = pad\n3400         mode : str\n3401             One of the following string values (taken from numpy docs)\n3402 \n3403             'constant' (default)\n3404                 Pads with a constant value.\n3405             'edge'\n3406                 Pads with the edge values of array.\n3407             'linear_ramp'\n3408                 Pads with the linear ramp between end_value and the\n3409                 array edge value.\n3410             'maximum'\n3411                 Pads with the maximum value of all or part of the\n3412                 vector along each axis.\n3413             'mean'\n3414                 Pads with the mean value of all or part of the\n3415                 vector along each axis.\n3416             'median'\n3417                 Pads with the median value of all or part of the\n3418                 vector along each axis.\n3419             'minimum'\n3420                 Pads with the minimum value of all or part of the\n3421                 vector along each axis.\n3422             'reflect'\n3423                 Pads with the reflection of the vector mirrored on\n3424                 the first and last values of the vector along each\n3425                 axis.\n3426             'symmetric'\n3427                 Pads with the reflection of the vector mirrored\n3428                 along the edge of the array.\n3429             'wrap'\n3430                 Pads with the wrap of the vector along the axis.\n3431                 The first values are used to pad the end and the\n3432                 end values are used to pad the beginning.\n3433         stat_length : int, tuple or mapping of the form {dim: tuple}\n3434             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n3435             values at edge of each axis used to calculate the statistic value.\n3436             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n3437             statistic lengths along each dimension.\n3438             ((before, after),) yields same before and after statistic lengths\n3439             for each dimension.\n3440             (stat_length,) or int is a shortcut for before = after = statistic\n3441             length for all axes.\n3442             Default is ``None``, to use the entire axis.\n3443         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n3444             Used in 'constant'.  The values to set the padded values for each\n3445             axis.\n3446             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3447             pad constants along each dimension.\n3448             ``((before, after),)`` yields same before and after constants for each\n3449             dimension.\n3450             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3451             all dimensions.\n3452             Default is 0.\n3453         end_values : scalar, tuple or mapping of the form {dim: tuple}\n3454             Used in 'linear_ramp'.  The values used for the ending value of the\n3455             linear_ramp and that will form the edge of the padded array.\n3456             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3457             end values along each dimension.\n3458             ``((before, after),)`` yields same before and after end values for each\n3459             axis.\n3460             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3461             all axes.\n3462             Default is 0.\n3463         reflect_type : {'even', 'odd'}, optional\n3464             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n3465             default with an unaltered reflection around the edge value.  For\n3466             the 'odd' style, the extended part of the array is created by\n3467             subtracting the reflected values from two times the edge value.\n3468         **pad_width_kwargs:\n3469             The keyword arguments form of ``pad_width``.\n3470             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n3471 \n3472         Returns\n3473         -------\n3474         padded : DataArray\n3475             DataArray with the padded coordinates and data.\n3476 \n3477         See also\n3478         --------\n3479         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n3480 \n3481         Notes\n3482         -----\n3483         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n3484         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n3485         specify ``constant_values=np.nan``\n3486 \n3487         Examples\n3488         --------\n3489 \n3490         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0,1,2])])\n3491         >>> arr.pad(x=(1,2), constant_values=0)\n3492         <xarray.DataArray (x: 6)>\n3493         array([0, 5, 6, 7, 0, 0])\n3494         Coordinates:\n3495           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n3496 \n3497         >>> da = xr.DataArray([[0,1,2,3], [10,11,12,13]],\n3498                               dims=[\"x\", \"y\"],\n3499                               coords={\"x\": [0,1], \"y\": [10, 20 ,30, 40], \"z\": (\"x\", [100, 200])}\n3500             )\n3501         >>> da.pad(x=1)\n3502         <xarray.DataArray (x: 4, y: 4)>\n3503         array([[nan, nan, nan, nan],\n3504                [ 0.,  1.,  2.,  3.],\n3505                [10., 11., 12., 13.],\n3506                [nan, nan, nan, nan]])\n3507         Coordinates:\n3508           * x        (x) float64 nan 0.0 1.0 nan\n3509           * y        (y) int64 10 20 30 40\n3510             z        (x) float64 nan 100.0 200.0 nan\n3511         >>> da.pad(x=1, constant_values=np.nan)\n3512         <xarray.DataArray (x: 4, y: 4)>\n3513         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3514                 -9223372036854775808],\n3515                [                   0,                    1,                    2,\n3516                                    3],\n3517                [                  10,                   11,                   12,\n3518                                   13],\n3519                [-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3520                 -9223372036854775808]])\n3521         Coordinates:\n3522           * x        (x) float64 nan 0.0 1.0 nan\n3523           * y        (y) int64 10 20 30 40\n3524             z        (x) float64 nan 100.0 200.0 nan\n3525         \"\"\"\n3526         ds = self._to_temp_dataset().pad(\n3527             pad_width=pad_width,\n3528             mode=mode,\n3529             stat_length=stat_length,\n3530             constant_values=constant_values,\n3531             end_values=end_values,\n3532             reflect_type=reflect_type,\n3533             **pad_width_kwargs,\n3534         )\n3535         return self._from_temp_dataset(ds)\n3536 \n3537     def idxmin(\n3538         self,\n3539         dim: Hashable = None,\n3540         skipna: bool = None,\n3541         fill_value: Any = dtypes.NA,\n3542         keep_attrs: bool = None,\n3543     ) -> \"DataArray\":\n3544         \"\"\"Return the coordinate label of the minimum value along a dimension.\n3545 \n3546         Returns a new `DataArray` named after the dimension with the values of\n3547         the coordinate labels along that dimension corresponding to minimum\n3548         values along that dimension.\n3549 \n3550         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n3551         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n3552 \n3553         Parameters\n3554         ----------\n3555         dim : str, optional\n3556             Dimension over which to apply `idxmin`.  This is optional for 1D\n3557             arrays, but required for arrays with 2 or more dimensions.\n3558         skipna : bool or None, default None\n3559             If True, skip missing values (as marked by NaN). By default, only\n3560             skips missing values for ``float``, ``complex``, and ``object``\n3561             dtypes; other dtypes either do not have a sentinel missing value\n3562             (``int``) or ``skipna=True`` has not been implemented\n3563             (``datetime64`` or ``timedelta64``).\n3564         fill_value : Any, default NaN\n3565             Value to be filled in case all of the values along a dimension are\n3566             null.  By default this is NaN.  The fill value and result are\n3567             automatically converted to a compatible dtype if possible.\n3568             Ignored if ``skipna`` is False.\n3569         keep_attrs : bool, default False\n3570             If True, the attributes (``attrs``) will be copied from the\n3571             original object to the new one.  If False (default), the new object\n3572             will be returned without attributes.\n3573 \n3574         Returns\n3575         -------\n3576         reduced : DataArray\n3577             New `DataArray` object with `idxmin` applied to its data and the\n3578             indicated dimension removed.\n3579 \n3580         See also\n3581         --------\n3582         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n3583 \n3584         Examples\n3585         --------\n3586 \n3587         >>> array = xr.DataArray([0, 2, 1, 0, -2], dims=\"x\",\n3588         ...                      coords={\"x\": ['a', 'b', 'c', 'd', 'e']})\n3589         >>> array.min()\n3590         <xarray.DataArray ()>\n3591         array(-2)\n3592         >>> array.argmin()\n3593         <xarray.DataArray ()>\n3594         array(4)\n3595         >>> array.idxmin()\n3596         <xarray.DataArray 'x' ()>\n3597         array('e', dtype='<U1')\n3598 \n3599         >>> array = xr.DataArray([[2.0, 1.0, 2.0, 0.0, -2.0],\n3600         ...                       [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3601         ...                       [np.NaN, np.NaN, 1., np.NaN, np.NaN]],\n3602         ...                      dims=[\"y\", \"x\"],\n3603         ...                      coords={\"y\": [-1, 0, 1],\n3604         ...                              \"x\": np.arange(5.)**2}\n3605         ...                      )\n3606         >>> array.min(dim=\"x\")\n3607         <xarray.DataArray (y: 3)>\n3608         array([-2., -4.,  1.])\n3609         Coordinates:\n3610           * y        (y) int64 -1 0 1\n3611         >>> array.argmin(dim=\"x\")\n3612         <xarray.DataArray (y: 3)>\n3613         array([4, 0, 2])\n3614         Coordinates:\n3615           * y        (y) int64 -1 0 1\n3616         >>> array.idxmin(dim=\"x\")\n3617         <xarray.DataArray 'x' (y: 3)>\n3618         array([16.,  0.,  4.])\n3619         Coordinates:\n3620           * y        (y) int64 -1 0 1\n3621         \"\"\"\n3622         return computation._calc_idxminmax(\n3623             array=self,\n3624             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n3625             dim=dim,\n3626             skipna=skipna,\n3627             fill_value=fill_value,\n3628             keep_attrs=keep_attrs,\n3629         )\n3630 \n3631     def idxmax(\n3632         self,\n3633         dim: Hashable = None,\n3634         skipna: bool = None,\n3635         fill_value: Any = dtypes.NA,\n3636         keep_attrs: bool = None,\n3637     ) -> \"DataArray\":\n3638         \"\"\"Return the coordinate label of the maximum value along a dimension.\n3639 \n3640         Returns a new `DataArray` named after the dimension with the values of\n3641         the coordinate labels along that dimension corresponding to maximum\n3642         values along that dimension.\n3643 \n3644         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n3645         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n3646 \n3647         Parameters\n3648         ----------\n3649         dim : str, optional\n3650             Dimension over which to apply `idxmax`.  This is optional for 1D\n3651             arrays, but required for arrays with 2 or more dimensions.\n3652         skipna : bool or None, default None\n3653             If True, skip missing values (as marked by NaN). By default, only\n3654             skips missing values for ``float``, ``complex``, and ``object``\n3655             dtypes; other dtypes either do not have a sentinel missing value\n3656             (``int``) or ``skipna=True`` has not been implemented\n3657             (``datetime64`` or ``timedelta64``).\n3658         fill_value : Any, default NaN\n3659             Value to be filled in case all of the values along a dimension are\n3660             null.  By default this is NaN.  The fill value and result are\n3661             automatically converted to a compatible dtype if possible.\n3662             Ignored if ``skipna`` is False.\n3663         keep_attrs : bool, default False\n3664             If True, the attributes (``attrs``) will be copied from the\n3665             original object to the new one.  If False (default), the new object\n3666             will be returned without attributes.\n3667 \n3668         Returns\n3669         -------\n3670         reduced : DataArray\n3671             New `DataArray` object with `idxmax` applied to its data and the\n3672             indicated dimension removed.\n3673 \n3674         See also\n3675         --------\n3676         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n3677 \n3678         Examples\n3679         --------\n3680 \n3681         >>> array = xr.DataArray([0, 2, 1, 0, -2], dims=\"x\",\n3682         ...                      coords={\"x\": ['a', 'b', 'c', 'd', 'e']})\n3683         >>> array.max()\n3684         <xarray.DataArray ()>\n3685         array(2)\n3686         >>> array.argmax()\n3687         <xarray.DataArray ()>\n3688         array(1)\n3689         >>> array.idxmax()\n3690         <xarray.DataArray 'x' ()>\n3691         array('b', dtype='<U1')\n3692 \n3693         >>> array = xr.DataArray([[2.0, 1.0, 2.0, 0.0, -2.0],\n3694         ...                       [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3695         ...                       [np.NaN, np.NaN, 1., np.NaN, np.NaN]],\n3696         ...                      dims=[\"y\", \"x\"],\n3697         ...                      coords={\"y\": [-1, 0, 1],\n3698         ...                              \"x\": np.arange(5.)**2}\n3699         ...                      )\n3700         >>> array.max(dim=\"x\")\n3701         <xarray.DataArray (y: 3)>\n3702         array([2., 2., 1.])\n3703         Coordinates:\n3704           * y        (y) int64 -1 0 1\n3705         >>> array.argmax(dim=\"x\")\n3706         <xarray.DataArray (y: 3)>\n3707         array([0, 2, 2])\n3708         Coordinates:\n3709           * y        (y) int64 -1 0 1\n3710         >>> array.idxmax(dim=\"x\")\n3711         <xarray.DataArray 'x' (y: 3)>\n3712         array([0., 4., 4.])\n3713         Coordinates:\n3714           * y        (y) int64 -1 0 1\n3715         \"\"\"\n3716         return computation._calc_idxminmax(\n3717             array=self,\n3718             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n3719             dim=dim,\n3720             skipna=skipna,\n3721             fill_value=fill_value,\n3722             keep_attrs=keep_attrs,\n3723         )\n3724 \n3725     # this needs to be at the end, or mypy will confuse with `str`\n3726     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n3727     str = property(StringAccessor)\n",
        "intended_behavior": "Since the direct search for `__iadd__` yielded no results, it's plausible to infer that handling for the in-place addition operation (`+=`) might not be explicitly defined or is handled through more general mechanisms that do not specifically account for alignment issues. It may be necessary to implement or modify the `__iadd__` method (or the mechanism that indirectly handles `+=`) to ensure that it includes a step for aligning arrays before attempting the in-place merge or addition. This step should align the coordinates of the two `DataArray` objects, similarly to how coordinates would be aligned in an out-of-place addition, thereby ensuring that `A += B` is functionally equivalent to `A = A + B` with respect to coordinate alignment and merging. This could involve utilizing existing alignment mechanisms or developing new logic tailored for in-place operations that ensures compatibility between the array coordinates before proceeding with the operation."
    }
]