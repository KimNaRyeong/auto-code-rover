2025-10-15 21:37:44.573 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 21:37:46.576 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=939, output_tokens=13, cost=0.009780
2025-10-15 21:37:54.728 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1119, output_tokens=311, cost=0.020520
2025-10-15 21:37:56.472 | INFO     | registered non reproducing test 0
2025-10-15 21:38:08.501 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2269, output_tokens=542, cost=0.038950
2025-10-15 21:38:10.208 | INFO     | registered non reproducing test 1
2025-10-15 21:38:30.755 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2500, output_tokens=642, cost=0.044260
2025-10-15 21:38:32.472 | INFO     | registered non reproducing test 2
2025-10-15 21:38:32.472 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 21:38:32.475 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:38:40.785 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1231, output_tokens=364, cost=0.023230
2025-10-15 21:38:40.795 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:38:43.885 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=718, output_tokens=70, cost=0.009280
2025-10-15 21:38:43.887 | DEBUG    | Extracted a valid json.
2025-10-15 21:38:43.887 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DataArray\")",
        "search_method_in_class(\"_merge_inplace\", \"DataArray\")",
        "search_method(\"merge_coordinates_without_align\")",
        "search_method_in_class(\"__iadd__\", \"DataArray\")"
    ],
    "bug_locations": []
}
2025-10-15 21:38:43.979 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 21:38:43.980 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 21:39:03.737 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6026, output_tokens=793, cost=0.084050
2025-10-15 21:39:03.753 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:39:13.817 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7035, output_tokens=388, cost=0.081990
2025-10-15 21:39:13.824 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:39:25.324 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=742, output_tokens=421, cost=0.020050
2025-10-15 21:39:25.325 | DEBUG    | Extracted a valid json.
2025-10-15 21:39:25.325 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/merge.py",
            "class": "",
            "method": "merge_coordinates_without_align",
            "intended_behavior": "This method should more gracefully handle coordinate merging for in-place operations like `+=` by possibly introducing a preliminary step that aligns the coordinates of the two `DataArray` objects before attempting to merge them. The method should ensure that in-place addition behaves similarly to out-of-place addition (`A + B`), which successfully combines the arrays regardless of the order of their coordinates. This may involve adjusting how priorities are determined or how conflicts are resolved when the dimensions are the same but the coordinate values or orders differ. The ultimate goal is to allow `A += B` to successfully complete by ensuring that conflicting indexes are resolved or aligned in such a way that mirrors the successful behavior of `A = A + B`."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "__iadd__",
            "intended_behavior": "Since the direct search for `__iadd__` yielded no results, it's plausible to infer that handling for the in-place addition operation (`+=`) might not be explicitly defined or is handled through more general mechanisms that do not specifically account for alignment issues. It may be necessary to implement or modify the `__iadd__` method (or the mechanism that indirectly handles `+=`) to ensure that it includes a step for aligning arrays before attempting the in-place merge or addition. This step should align the coordinates of the two `DataArray` objects, similarly to how coordinates would be aligned in an out-of-place addition, thereby ensuring that `A += B` is functionally equivalent to `A = A + B` with respect to coordinate alignment and merging. This could involve utilizing existing alignment mechanisms or developing new logic tailored for in-place operations that ensures compatibility between the array coordinates before proceeding with the operation."
        }
    ]
}
2025-10-15 21:39:25.349 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/merge.py</file>
<class>None</class>
<method>merge_coordinates_without_align</method>
<code>
303 def merge_coordinates_without_align(
304     objects: "List[Coordinates]",
305     prioritized: Mapping[Hashable, MergeElement] = None,
306     exclude_dims: AbstractSet = frozenset(),
307 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
308     """Merge variables/indexes from coordinates without automatic alignments.
309 
310     This function is used for merging coordinate from pre-existing xarray
311     objects.
312     """
313     collected = collect_from_coordinates(objects)
314 
315     if exclude_dims:
316         filtered: Dict[Hashable, List[MergeElement]] = {}
317         for name, elements in collected.items():
318             new_elements = [
319                 (variable, index)
320                 for variable, index in elements
321                 if exclude_dims.isdisjoint(variable.dims)
322             ]
323             if new_elements:
324                 filtered[name] = new_elements
325     else:
326         filtered = collected
327 
328     return merge_collected(filtered, prioritized)

</code><intended_behavior>This method should more gracefully handle coordinate merging for in-place operations like `+=` by possibly introducing a preliminary step that aligns the coordinates of the two `DataArray` objects before attempting to merge them. The method should ensure that in-place addition behaves similarly to out-of-place addition (`A + B`), which successfully combines the arrays regardless of the order of their coordinates. This may involve adjusting how priorities are determined or how conflicts are resolved when the dimensions are the same but the coordinate values or orders differ. The ultimate goal is to allow `A += B` to successfully complete by ensuring that conflicting indexes are resolved or aligned in such a way that mirrors the successful behavior of `A = A + B`.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
216 class DataArray(AbstractArray, DataWithCoords):
217     """N-dimensional array with labeled coordinates and dimensions.
218 
219     DataArray provides a wrapper around numpy ndarrays that uses labeled
220     dimensions and coordinates to support metadata aware operations. The API is
221     similar to that for the pandas Series or DataFrame, but DataArray objects
222     can have any number of dimensions, and their contents have fixed data
223     types.
224 
225     Additional features over raw numpy arrays:
226 
227     - Apply operations over dimensions by name: ``x.sum('time')``.
228     - Select or assign values by integer location (like numpy): ``x[:10]``
229       or by label (like pandas): ``x.loc['2014-01-01']`` or
230       ``x.sel(time='2014-01-01')``.
231     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
232       dimensions (known in numpy as "broadcasting") based on dimension names,
233       regardless of their original order.
234     - Keep track of arbitrary metadata in the form of a Python dictionary:
235       ``x.attrs``
236     - Convert to a pandas Series: ``x.to_series()``.
237 
238     Getting items from or doing mathematical operations with a DataArray
239     always returns another DataArray.
240     """
241 
242     _cache: Dict[str, Any]
243     _coords: Dict[Any, Variable]
244     _indexes: Optional[Dict[Hashable, pd.Index]]
245     _name: Optional[Hashable]
246     _variable: Variable
247 
248     __slots__ = (
249         "_cache",
250         "_coords",
251         "_file_obj",
252         "_indexes",
253         "_name",
254         "_variable",
255         "__weakref__",
256     )
257 
258     _groupby_cls = groupby.DataArrayGroupBy
259     _rolling_cls = rolling.DataArrayRolling
260     _coarsen_cls = rolling.DataArrayCoarsen
261     _resample_cls = resample.DataArrayResample
262     _weighted_cls = weighted.DataArrayWeighted
263 
264     dt = property(CombinedDatetimelikeAccessor)
265 
266     def __init__(
267         self,
268         data: Any = dtypes.NA,
269         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
270         dims: Union[Hashable, Sequence[Hashable], None] = None,
271         name: Hashable = None,
272         attrs: Mapping = None,
273         # internal parameters
274         indexes: Dict[Hashable, pd.Index] = None,
275         fastpath: bool = False,
276     ):
277         """
278         Parameters
279         ----------
280         data : array_like
281             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
282             or castable to an ``ndarray``. If a self-described xarray or pandas
283             object, attempts are made to use this array's metadata to fill in
284             other unspecified arguments. A view of the array's data is used
285             instead of a copy if possible.
286         coords : sequence or dict of array_like objects, optional
287             Coordinates (tick labels) to use for indexing along each dimension.
288             The following notations are accepted:
289 
290             - mapping {dimension name: array-like}
291             - sequence of tuples that are valid arguments for xarray.Variable()
292               - (dims, data)
293               - (dims, data, attrs)
294               - (dims, data, attrs, encoding)
295 
296             Additionally, it is possible to define a coord whose name
297             does not match the dimension name, or a coord based on multiple
298             dimensions, with one of the following notations:
299 
300             - mapping {coord name: DataArray}
301             - mapping {coord name: Variable}
302             - mapping {coord name: (dimension name, array-like)}
303             - mapping {coord name: (tuple of dimension names, array-like)}
304 
305         dims : hashable or sequence of hashable, optional
306             Name(s) of the data dimension(s). Must be either a hashable (only
307             for 1D data) or a sequence of hashables with length equal to the
308             number of dimensions. If this argument is omitted, dimension names
309             default to ``['dim_0', ... 'dim_n']``.
310         name : str or None, optional
311             Name of this array.
312         attrs : dict_like or None, optional
313             Attributes to assign to the new instance. By default, an empty
314             attribute dictionary is initialized.
315         """
316         if fastpath:
317             variable = data
318             assert dims is None
319             assert attrs is None
320         else:
321             # try to fill in arguments from data if they weren't supplied
322             if coords is None:
323 
324                 if isinstance(data, DataArray):
325                     coords = data.coords
326                 elif isinstance(data, pd.Series):
327                     coords = [data.index]
328                 elif isinstance(data, pd.DataFrame):
329                     coords = [data.index, data.columns]
330                 elif isinstance(data, (pd.Index, IndexVariable)):
331                     coords = [data]
332                 elif isinstance(data, pdcompat.Panel):
333                     coords = [data.items, data.major_axis, data.minor_axis]
334 
335             if dims is None:
336                 dims = getattr(data, "dims", getattr(coords, "dims", None))
337             if name is None:
338                 name = getattr(data, "name", None)
339             if attrs is None and not isinstance(data, PANDAS_TYPES):
340                 attrs = getattr(data, "attrs", None)
341 
342             data = _check_data_shape(data, coords, dims)
343             data = as_compatible_data(data)
344             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
345             variable = Variable(dims, data, attrs, fastpath=True)
346             indexes = dict(
347                 _extract_indexes_from_coords(coords)
348             )  # needed for to_dataset
349 
350         # These fully describe a DataArray
351         self._variable = variable
352         assert isinstance(coords, dict)
353         self._coords = coords
354         self._name = name
355 
356         # TODO(shoyer): document this argument, once it becomes part of the
357         # public interface.
358         self._indexes = indexes
359 
360         self._file_obj = None
361 
362     def _replace(
363         self,
364         variable: Variable = None,
365         coords=None,
366         name: Union[Hashable, None, Default] = _default,
367         indexes=None,
368     ) -> "DataArray":
369         if variable is None:
370             variable = self.variable
371         if coords is None:
372             coords = self._coords
373         if name is _default:
374             name = self.name
375         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
376 
377     def _replace_maybe_drop_dims(
378         self, variable: Variable, name: Union[Hashable, None, Default] = _default
379     ) -> "DataArray":
380         if variable.dims == self.dims and variable.shape == self.shape:
381             coords = self._coords.copy()
382             indexes = self._indexes
383         elif variable.dims == self.dims:
384             # Shape has changed (e.g. from reduce(..., keepdims=True)
385             new_sizes = dict(zip(self.dims, variable.shape))
386             coords = {
387                 k: v
388                 for k, v in self._coords.items()
389                 if v.shape == tuple(new_sizes[d] for d in v.dims)
390             }
391             changed_dims = [
392                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
393             ]
394             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
395         else:
396             allowed_dims = set(variable.dims)
397             coords = {
398                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
399             }
400             indexes = propagate_indexes(
401                 self._indexes, exclude=(set(self.dims) - allowed_dims)
402             )
403         return self._replace(variable, coords, name, indexes=indexes)
404 
405     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
406         if not len(indexes):
407             return self
408         coords = self._coords.copy()
409         for name, idx in indexes.items():
410             coords[name] = IndexVariable(name, idx)
411         obj = self._replace(coords=coords)
412 
413         # switch from dimension to level names, if necessary
414         dim_names: Dict[Any, str] = {}
415         for dim, idx in indexes.items():
416             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
417                 dim_names[dim] = idx.name
418         if dim_names:
419             obj = obj.rename(dim_names)
420         return obj
421 
422     def _to_temp_dataset(self) -> Dataset:
423         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
424 
425     def _from_temp_dataset(
426         self, dataset: Dataset, name: Hashable = _default
427     ) -> "DataArray":
428         variable = dataset._variables.pop(_THIS_ARRAY)
429         coords = dataset._variables
430         indexes = dataset._indexes
431         return self._replace(variable, coords, name, indexes=indexes)
432 
433     def _to_dataset_split(self, dim: Hashable) -> Dataset:
434         """ splits dataarray along dimension 'dim' """
435 
436         def subset(dim, label):
437             array = self.loc[{dim: label}]
438             array.attrs = {}
439             return as_variable(array)
440 
441         variables = {label: subset(dim, label) for label in self.get_index(dim)}
442         variables.update({k: v for k, v in self._coords.items() if k != dim})
443         indexes = propagate_indexes(self._indexes, exclude=dim)
444         coord_names = set(self._coords) - set([dim])
445         dataset = Dataset._construct_direct(
446             variables, coord_names, indexes=indexes, attrs=self.attrs
447         )
448         return dataset
449 
450     def _to_dataset_whole(
451         self, name: Hashable = None, shallow_copy: bool = True
452     ) -> Dataset:
453         if name is None:
454             name = self.name
455         if name is None:
456             raise ValueError(
457                 "unable to convert unnamed DataArray to a "
458                 "Dataset without providing an explicit name"
459             )
460         if name in self.coords:
461             raise ValueError(
462                 "cannot create a Dataset from a DataArray with "
463                 "the same name as one of its coordinates"
464             )
465         # use private APIs for speed: this is called by _to_temp_dataset(),
466         # which is used in the guts of a lot of operations (e.g., reindex)
467         variables = self._coords.copy()
468         variables[name] = self.variable
469         if shallow_copy:
470             for k in variables:
471                 variables[k] = variables[k].copy(deep=False)
472         indexes = self._indexes
473 
474         coord_names = set(self._coords)
475         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
476         return dataset
477 
478     def to_dataset(
479         self,
480         dim: Hashable = None,
481         *,
482         name: Hashable = None,
483         promote_attrs: bool = False,
484     ) -> Dataset:
485         """Convert a DataArray to a Dataset.
486 
487         Parameters
488         ----------
489         dim : hashable, optional
490             Name of the dimension on this array along which to split this array
491             into separate variables. If not provided, this array is converted
492             into a Dataset of one variable.
493         name : hashable, optional
494             Name to substitute for this array's name. Only valid if ``dim`` is
495             not provided.
496         promote_attrs : bool, default False
497             Set to True to shallow copy attrs of DataArray to returned Dataset.
498 
499         Returns
500         -------
501         dataset : Dataset
502         """
503         if dim is not None and dim not in self.dims:
504             raise TypeError(
505                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
506             )
507 
508         if dim is not None:
509             if name is not None:
510                 raise TypeError("cannot supply both dim and name arguments")
511             result = self._to_dataset_split(dim)
512         else:
513             result = self._to_dataset_whole(name)
514 
515         if promote_attrs:
516             result.attrs = dict(self.attrs)
517 
518         return result
519 
520     @property
521     def name(self) -> Optional[Hashable]:
522         """The name of this array.
523         """
524         return self._name
525 
526     @name.setter
527     def name(self, value: Optional[Hashable]) -> None:
528         self._name = value
529 
530     @property
531     def variable(self) -> Variable:
532         """Low level interface to the Variable object for this DataArray."""
533         return self._variable
534 
535     @property
536     def dtype(self) -> np.dtype:
537         return self.variable.dtype
538 
539     @property
540     def shape(self) -> Tuple[int, ...]:
541         return self.variable.shape
542 
543     @property
544     def size(self) -> int:
545         return self.variable.size
546 
547     @property
548     def nbytes(self) -> int:
549         return self.variable.nbytes
550 
551     @property
552     def ndim(self) -> int:
553         return self.variable.ndim
554 
555     def __len__(self) -> int:
556         return len(self.variable)
557 
558     @property
559     def data(self) -> Any:
560         """The array's data as a dask or numpy array
561         """
562         return self.variable.data
563 
564     @data.setter
565     def data(self, value: Any) -> None:
566         self.variable.data = value
567 
568     @property
569     def values(self) -> np.ndarray:
570         """The array's data as a numpy.ndarray"""
571         return self.variable.values
572 
573     @values.setter
574     def values(self, value: Any) -> None:
575         self.variable.values = value
576 
577     @property
578     def _in_memory(self) -> bool:
579         return self.variable._in_memory
580 
581     def to_index(self) -> pd.Index:
582         """Convert this variable to a pandas.Index. Only possible for 1D
583         arrays.
584         """
585         return self.variable.to_index()
586 
587     @property
588     def dims(self) -> Tuple[Hashable, ...]:
589         """Tuple of dimension names associated with this array.
590 
591         Note that the type of this property is inconsistent with
592         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
593         consistently named properties.
594         """
595         return self.variable.dims
596 
597     @dims.setter
598     def dims(self, value):
599         raise AttributeError(
600             "you cannot assign dims on a DataArray. Use "
601             ".rename() or .swap_dims() instead."
602         )
603 
604     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
605         if utils.is_dict_like(key):
606             return key
607         else:
608             key = indexing.expanded_indexer(key, self.ndim)
609             return dict(zip(self.dims, key))
610 
611     @property
612     def _level_coords(self) -> Dict[Hashable, Hashable]:
613         """Return a mapping of all MultiIndex levels and their corresponding
614         coordinate name.
615         """
616         level_coords: Dict[Hashable, Hashable] = {}
617 
618         for cname, var in self._coords.items():
619             if var.ndim == 1 and isinstance(var, IndexVariable):
620                 level_names = var.level_names
621                 if level_names is not None:
622                     (dim,) = var.dims
623                     level_coords.update({lname: dim for lname in level_names})
624         return level_coords
625 
626     def _getitem_coord(self, key):
627         from .dataset import _get_virtual_variable
628 
629         try:
630             var = self._coords[key]
631         except KeyError:
632             dim_sizes = dict(zip(self.dims, self.shape))
633             _, key, var = _get_virtual_variable(
634                 self._coords, key, self._level_coords, dim_sizes
635             )
636 
637         return self._replace_maybe_drop_dims(var, name=key)
638 
639     def __getitem__(self, key: Any) -> "DataArray":
640         if isinstance(key, str):
641             return self._getitem_coord(key)
642         else:
643             # xarray-style array indexing
644             return self.isel(indexers=self._item_key_to_dict(key))
645 
646     def __setitem__(self, key: Any, value: Any) -> None:
647         if isinstance(key, str):
648             self.coords[key] = value
649         else:
650             # Coordinates in key, value and self[key] should be consistent.
651             # TODO Coordinate consistency in key is checked here, but it
652             # causes unnecessary indexing. It should be optimized.
653             obj = self[key]
654             if isinstance(value, DataArray):
655                 assert_coordinate_consistent(value, obj.coords.variables)
656             # DataArray key -> Variable key
657             key = {
658                 k: v.variable if isinstance(v, DataArray) else v
659                 for k, v in self._item_key_to_dict(key).items()
660             }
661             self.variable[key] = value
662 
663     def __delitem__(self, key: Any) -> None:
664         del self.coords[key]
665 
666     @property
667     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
668         """List of places to look-up items for attribute-style access
669         """
670         return self._item_sources + [self.attrs]
671 
672     @property
673     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
674         """List of places to look-up items for key-completion
675         """
676         return [
677             self.coords,
678             {d: self.coords[d] for d in self.dims},
679             LevelCoordinatesSource(self),
680         ]
681 
682     def __contains__(self, key: Any) -> bool:
683         return key in self.data
684 
685     @property
686     def loc(self) -> _LocIndexer:
687         """Attribute for location based indexing like pandas.
688         """
689         return _LocIndexer(self)
690 
691     @property
692     def attrs(self) -> Dict[Hashable, Any]:
693         """Dictionary storing arbitrary metadata with this array."""
694         return self.variable.attrs
695 
696     @attrs.setter
697     def attrs(self, value: Mapping[Hashable, Any]) -> None:
698         # Disable type checking to work around mypy bug - see mypy#4167
699         self.variable.attrs = value  # type: ignore
700 
701     @property
702     def encoding(self) -> Dict[Hashable, Any]:
703         """Dictionary of format-specific settings for how this array should be
704         serialized."""
705         return self.variable.encoding
706 
707     @encoding.setter
708     def encoding(self, value: Mapping[Hashable, Any]) -> None:
709         self.variable.encoding = value
710 
711     @property
712     def indexes(self) -> Indexes:
713         """Mapping of pandas.Index objects used for label based indexing
714         """
715         if self._indexes is None:
716             self._indexes = default_indexes(self._coords, self.dims)
717         return Indexes(self._indexes)
718 
719     @property
720     def coords(self) -> DataArrayCoordinates:
721         """Dictionary-like container of coordinate arrays.
722         """
723         return DataArrayCoordinates(self)
724 
725     def reset_coords(
726         self,
727         names: Union[Iterable[Hashable], Hashable, None] = None,
728         drop: bool = False,
729         inplace: bool = None,
730     ) -> Union[None, "DataArray", Dataset]:
731         """Given names of coordinates, reset them to become variables.
732 
733         Parameters
734         ----------
735         names : hashable or iterable of hashables, optional
736             Name(s) of non-index coordinates in this dataset to reset into
737             variables. By default, all non-index coordinates are reset.
738         drop : bool, optional
739             If True, remove coordinates instead of converting them into
740             variables.
741 
742         Returns
743         -------
744         Dataset, or DataArray if ``drop == True``
745         """
746         _check_inplace(inplace)
747         if names is None:
748             names = set(self.coords) - set(self.dims)
749         dataset = self.coords.to_dataset().reset_coords(names, drop)
750         if drop:
751             return self._replace(coords=dataset._variables)
752         else:
753             if self.name is None:
754                 raise ValueError(
755                     "cannot reset_coords with drop=False on an unnamed DataArrray"
756                 )
757             dataset[self.name] = self.variable
758             return dataset
759 
760     def __dask_tokenize__(self):
761         from dask.base import normalize_token
762 
763         return normalize_token((type(self), self._variable, self._coords, self._name))
764 
765     def __dask_graph__(self):
766         return self._to_temp_dataset().__dask_graph__()
767 
768     def __dask_keys__(self):
769         return self._to_temp_dataset().__dask_keys__()
770 
771     def __dask_layers__(self):
772         return self._to_temp_dataset().__dask_layers__()
773 
774     @property
775     def __dask_optimize__(self):
776         return self._to_temp_dataset().__dask_optimize__
777 
778     @property
779     def __dask_scheduler__(self):
780         return self._to_temp_dataset().__dask_scheduler__
781 
782     def __dask_postcompute__(self):
783         func, args = self._to_temp_dataset().__dask_postcompute__()
784         return self._dask_finalize, (func, args, self.name)
785 
786     def __dask_postpersist__(self):
787         func, args = self._to_temp_dataset().__dask_postpersist__()
788         return self._dask_finalize, (func, args, self.name)
789 
790     @staticmethod
791     def _dask_finalize(results, func, args, name):
792         ds = func(results, *args)
793         variable = ds._variables.pop(_THIS_ARRAY)
794         coords = ds._variables
795         return DataArray(variable, coords, name=name, fastpath=True)
796 
797     def load(self, **kwargs) -> "DataArray":
798         """Manually trigger loading of this array's data from disk or a
799         remote source into memory and return this array.
800 
801         Normally, it should not be necessary to call this method in user code,
802         because all xarray functions should either work on deferred data or
803         load data automatically. However, this method can be necessary when
804         working with many file objects on disk.
805 
806         Parameters
807         ----------
808         **kwargs : dict
809             Additional keyword arguments passed on to ``dask.array.compute``.
810 
811         See Also
812         --------
813         dask.array.compute
814         """
815         ds = self._to_temp_dataset().load(**kwargs)
816         new = self._from_temp_dataset(ds)
817         self._variable = new._variable
818         self._coords = new._coords
819         return self
820 
821     def compute(self, **kwargs) -> "DataArray":
822         """Manually trigger loading of this array's data from disk or a
823         remote source into memory and return a new array. The original is
824         left unaltered.
825 
826         Normally, it should not be necessary to call this method in user code,
827         because all xarray functions should either work on deferred data or
828         load data automatically. However, this method can be necessary when
829         working with many file objects on disk.
830 
831         Parameters
832         ----------
833         **kwargs : dict
834             Additional keyword arguments passed on to ``dask.array.compute``.
835 
836         See Also
837         --------
838         dask.array.compute
839         """
840         new = self.copy(deep=False)
841         return new.load(**kwargs)
842 
843     def persist(self, **kwargs) -> "DataArray":
844         """ Trigger computation in constituent dask arrays
845 
846         This keeps them as dask arrays but encourages them to keep data in
847         memory.  This is particularly useful when on a distributed machine.
848         When on a single machine consider using ``.compute()`` instead.
849 
850         Parameters
851         ----------
852         **kwargs : dict
853             Additional keyword arguments passed on to ``dask.persist``.
854 
855         See Also
856         --------
857         dask.persist
858         """
859         ds = self._to_temp_dataset().persist(**kwargs)
860         return self._from_temp_dataset(ds)
861 
862     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
863         """Returns a copy of this array.
864 
865         If `deep=True`, a deep copy is made of the data array.
866         Otherwise, a shallow copy is made, so each variable in the new
867         array's dataset is also a variable in this array's dataset.
868 
869         Use `data` to create a new object with the same structure as
870         original but entirely new data.
871 
872         Parameters
873         ----------
874         deep : bool, optional
875             Whether the data array and its coordinates are loaded into memory
876             and copied onto the new object. Default is True.
877         data : array_like, optional
878             Data to use in the new object. Must have same shape as original.
879             When `data` is used, `deep` is ignored for all data variables,
880             and only used for coords.
881 
882         Returns
883         -------
884         object : DataArray
885             New object with dimensions, attributes, coordinates, name,
886             encoding, and optionally data copied from original.
887 
888         Examples
889         --------
890 
891         Shallow versus deep copy
892 
893         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
894         >>> array.copy()
895         <xarray.DataArray (x: 3)>
896         array([1, 2, 3])
897         Coordinates:
898         * x        (x) <U1 'a' 'b' 'c'
899         >>> array_0 = array.copy(deep=False)
900         >>> array_0[0] = 7
901         >>> array_0
902         <xarray.DataArray (x: 3)>
903         array([7, 2, 3])
904         Coordinates:
905         * x        (x) <U1 'a' 'b' 'c'
906         >>> array
907         <xarray.DataArray (x: 3)>
908         array([7, 2, 3])
909         Coordinates:
910         * x        (x) <U1 'a' 'b' 'c'
911 
912         Changing the data using the ``data`` argument maintains the
913         structure of the original object, but with the new data. Original
914         object is unaffected.
915 
916         >>> array.copy(data=[0.1, 0.2, 0.3])
917         <xarray.DataArray (x: 3)>
918         array([ 0.1,  0.2,  0.3])
919         Coordinates:
920         * x        (x) <U1 'a' 'b' 'c'
921         >>> array
922         <xarray.DataArray (x: 3)>
923         array([1, 2, 3])
924         Coordinates:
925         * x        (x) <U1 'a' 'b' 'c'
926 
927         See Also
928         --------
929         pandas.DataFrame.copy
930         """
931         variable = self.variable.copy(deep=deep, data=data)
932         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
933         if self._indexes is None:
934             indexes = self._indexes
935         else:
936             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
937         return self._replace(variable, coords, indexes=indexes)
938 
939     def __copy__(self) -> "DataArray":
940         return self.copy(deep=False)
941 
942     def __deepcopy__(self, memo=None) -> "DataArray":
943         # memo does nothing but is required for compatibility with
944         # copy.deepcopy
945         return self.copy(deep=True)
946 
947     # mutable objects should not be hashable
948     # https://github.com/python/mypy/issues/4266
949     __hash__ = None  # type: ignore
950 
951     @property
952     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
953         """Block dimensions for this array's data or None if it's not a dask
954         array.
955         """
956         return self.variable.chunks
957 
958     def chunk(
959         self,
960         chunks: Union[
961             None,
962             Number,
963             Tuple[Number, ...],
964             Tuple[Tuple[Number, ...], ...],
965             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
966         ] = None,
967         name_prefix: str = "xarray-",
968         token: str = None,
969         lock: bool = False,
970     ) -> "DataArray":
971         """Coerce this array's data into a dask arrays with the given chunks.
972 
973         If this variable is a non-dask array, it will be converted to dask
974         array. If it's a dask array, it will be rechunked to the given chunk
975         sizes.
976 
977         If neither chunks is not provided for one or more dimensions, chunk
978         sizes along that dimension will not be updated; non-dask arrays will be
979         converted into dask arrays with a single block.
980 
981         Parameters
982         ----------
983         chunks : int, tuple or mapping, optional
984             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
985             ``{'x': 5, 'y': 5}``.
986         name_prefix : str, optional
987             Prefix for the name of the new dask array.
988         token : str, optional
989             Token uniquely identifying this array.
990         lock : optional
991             Passed on to :py:func:`dask.array.from_array`, if the array is not
992             already as dask array.
993 
994         Returns
995         -------
996         chunked : xarray.DataArray
997         """
998         if isinstance(chunks, (tuple, list)):
999             chunks = dict(zip(self.dims, chunks))
1000 
1001         ds = self._to_temp_dataset().chunk(
1002             chunks, name_prefix=name_prefix, token=token, lock=lock
1003         )
1004         return self._from_temp_dataset(ds)
1005 
1006     def isel(
1007         self,
1008         indexers: Mapping[Hashable, Any] = None,
1009         drop: bool = False,
1010         missing_dims: str = "raise",
1011         **indexers_kwargs: Any,
1012     ) -> "DataArray":
1013         """Return a new DataArray whose data is given by integer indexing
1014         along the specified dimension(s).
1015 
1016         Parameters
1017         ----------
1018         indexers : dict, optional
1019             A dict with keys matching dimensions and values given
1020             by integers, slice objects or arrays.
1021             indexer can be a integer, slice, array-like or DataArray.
1022             If DataArrays are passed as indexers, xarray-style indexing will be
1023             carried out. See :ref:`indexing` for the details.
1024             One of indexers or indexers_kwargs must be provided.
1025         drop : bool, optional
1026             If ``drop=True``, drop coordinates variables indexed by integers
1027             instead of making them scalar.
1028         missing_dims : {"raise", "warn", "ignore"}, default "raise"
1029             What to do if dimensions that should be selected from are not present in the
1030             DataArray:
1031             - "exception": raise an exception
1032             - "warning": raise a warning, and ignore the missing dimensions
1033             - "ignore": ignore the missing dimensions
1034         **indexers_kwargs : {dim: indexer, ...}, optional
1035             The keyword arguments form of ``indexers``.
1036 
1037         See Also
1038         --------
1039         Dataset.isel
1040         DataArray.sel
1041         """
1042 
1043         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1044 
1045         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1046             ds = self._to_temp_dataset()._isel_fancy(
1047                 indexers, drop=drop, missing_dims=missing_dims
1048             )
1049             return self._from_temp_dataset(ds)
1050 
1051         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1052         # lists, or zero or one-dimensional np.ndarray's
1053 
1054         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1055 
1056         coords = {}
1057         for coord_name, coord_value in self._coords.items():
1058             coord_indexers = {
1059                 k: v for k, v in indexers.items() if k in coord_value.dims
1060             }
1061             if coord_indexers:
1062                 coord_value = coord_value.isel(coord_indexers)
1063                 if drop and coord_value.ndim == 0:
1064                     continue
1065             coords[coord_name] = coord_value
1066 
1067         return self._replace(variable=variable, coords=coords)
1068 
1069     def sel(
1070         self,
1071         indexers: Mapping[Hashable, Any] = None,
1072         method: str = None,
1073         tolerance=None,
1074         drop: bool = False,
1075         **indexers_kwargs: Any,
1076     ) -> "DataArray":
1077         """Return a new DataArray whose data is given by selecting index
1078         labels along the specified dimension(s).
1079 
1080         .. warning::
1081 
1082           Do not try to assign values when using any of the indexing methods
1083           ``isel`` or ``sel``::
1084 
1085             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1086             # DO NOT do this
1087             da.isel(x=[0, 1, 2])[1] = -1
1088 
1089           Assigning values with the chained indexing using ``.sel`` or
1090           ``.isel`` fails silently.
1091 
1092         See Also
1093         --------
1094         Dataset.sel
1095         DataArray.isel
1096 
1097         """
1098         ds = self._to_temp_dataset().sel(
1099             indexers=indexers,
1100             drop=drop,
1101             method=method,
1102             tolerance=tolerance,
1103             **indexers_kwargs,
1104         )
1105         return self._from_temp_dataset(ds)
1106 
1107     def head(
1108         self,
1109         indexers: Union[Mapping[Hashable, int], int] = None,
1110         **indexers_kwargs: Any,
1111     ) -> "DataArray":
1112         """Return a new DataArray whose data is given by the the first `n`
1113         values along the specified dimension(s). Default `n` = 5
1114 
1115         See Also
1116         --------
1117         Dataset.head
1118         DataArray.tail
1119         DataArray.thin
1120         """
1121         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1122         return self._from_temp_dataset(ds)
1123 
1124     def tail(
1125         self,
1126         indexers: Union[Mapping[Hashable, int], int] = None,
1127         **indexers_kwargs: Any,
1128     ) -> "DataArray":
1129         """Return a new DataArray whose data is given by the the last `n`
1130         values along the specified dimension(s). Default `n` = 5
1131 
1132         See Also
1133         --------
1134         Dataset.tail
1135         DataArray.head
1136         DataArray.thin
1137         """
1138         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1139         return self._from_temp_dataset(ds)
1140 
1141     def thin(
1142         self,
1143         indexers: Union[Mapping[Hashable, int], int] = None,
1144         **indexers_kwargs: Any,
1145     ) -> "DataArray":
1146         """Return a new DataArray whose data is given by each `n` value
1147         along the specified dimension(s).
1148 
1149         See Also
1150         --------
1151         Dataset.thin
1152         DataArray.head
1153         DataArray.tail
1154         """
1155         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1156         return self._from_temp_dataset(ds)
1157 
1158     def broadcast_like(
1159         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1160     ) -> "DataArray":
1161         """Broadcast this DataArray against another Dataset or DataArray.
1162 
1163         This is equivalent to xr.broadcast(other, self)[1]
1164 
1165         xarray objects are broadcast against each other in arithmetic
1166         operations, so this method is not be necessary for most uses.
1167 
1168         If no change is needed, the input data is returned to the output
1169         without being copied.
1170 
1171         If new coords are added by the broadcast, their values are
1172         NaN filled.
1173 
1174         Parameters
1175         ----------
1176         other : Dataset or DataArray
1177             Object against which to broadcast this array.
1178         exclude : iterable of hashable, optional
1179             Dimensions that must not be broadcasted
1180 
1181         Returns
1182         -------
1183         new_da: xr.DataArray
1184 
1185         Examples
1186         --------
1187 
1188         >>> arr1
1189         <xarray.DataArray (x: 2, y: 3)>
1190         array([[0.840235, 0.215216, 0.77917 ],
1191                [0.726351, 0.543824, 0.875115]])
1192         Coordinates:
1193           * x        (x) <U1 'a' 'b'
1194           * y        (y) <U1 'a' 'b' 'c'
1195         >>> arr2
1196         <xarray.DataArray (x: 3, y: 2)>
1197         array([[0.612611, 0.125753],
1198                [0.853181, 0.948818],
1199                [0.180885, 0.33363 ]])
1200         Coordinates:
1201           * x        (x) <U1 'a' 'b' 'c'
1202           * y        (y) <U1 'a' 'b'
1203         >>> arr1.broadcast_like(arr2)
1204         <xarray.DataArray (x: 3, y: 3)>
1205         array([[0.840235, 0.215216, 0.77917 ],
1206                [0.726351, 0.543824, 0.875115],
1207                [     nan,      nan,      nan]])
1208         Coordinates:
1209           * x        (x) object 'a' 'b' 'c'
1210           * y        (y) object 'a' 'b' 'c'
1211         """
1212         if exclude is None:
1213             exclude = set()
1214         else:
1215             exclude = set(exclude)
1216         args = align(other, self, join="outer", copy=False, exclude=exclude)
1217 
1218         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1219 
1220         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1221 
1222     def reindex_like(
1223         self,
1224         other: Union["DataArray", Dataset],
1225         method: str = None,
1226         tolerance=None,
1227         copy: bool = True,
1228         fill_value=dtypes.NA,
1229     ) -> "DataArray":
1230         """Conform this object onto the indexes of another object, filling in
1231         missing values with ``fill_value``. The default fill value is NaN.
1232 
1233         Parameters
1234         ----------
1235         other : Dataset or DataArray
1236             Object with an 'indexes' attribute giving a mapping from dimension
1237             names to pandas.Index objects, which provides coordinates upon
1238             which to index the variables in this dataset. The indexes on this
1239             other object need not be the same as the indexes on this
1240             dataset. Any mis-matched index values will be filled in with
1241             NaN, and any mis-matched dimension names will simply be ignored.
1242         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1243             Method to use for filling index values from other not found on this
1244             data array:
1245 
1246             * None (default): don't fill gaps
1247             * pad / ffill: propagate last valid index value forward
1248             * backfill / bfill: propagate next valid index value backward
1249             * nearest: use nearest valid index value
1250         tolerance : optional
1251             Maximum distance between original and new labels for inexact
1252             matches. The values of the index at the matching locations must
1253             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1254         copy : bool, optional
1255             If ``copy=True``, data in the return value is always copied. If
1256             ``copy=False`` and reindexing is unnecessary, or can be performed
1257             with only slice operations, then the output may share memory with
1258             the input. In either case, a new xarray object is always returned.
1259         fill_value : scalar, optional
1260             Value to use for newly missing values
1261 
1262         Returns
1263         -------
1264         reindexed : DataArray
1265             Another dataset array, with this array's data but coordinates from
1266             the other object.
1267 
1268         See Also
1269         --------
1270         DataArray.reindex
1271         align
1272         """
1273         indexers = reindex_like_indexers(self, other)
1274         return self.reindex(
1275             indexers=indexers,
1276             method=method,
1277             tolerance=tolerance,
1278             copy=copy,
1279             fill_value=fill_value,
1280         )
1281 
1282     def reindex(
1283         self,
1284         indexers: Mapping[Hashable, Any] = None,
1285         method: str = None,
1286         tolerance=None,
1287         copy: bool = True,
1288         fill_value=dtypes.NA,
1289         **indexers_kwargs: Any,
1290     ) -> "DataArray":
1291         """Conform this object onto the indexes of another object, filling in
1292         missing values with ``fill_value``. The default fill value is NaN.
1293 
1294         Parameters
1295         ----------
1296         indexers : dict, optional
1297             Dictionary with keys given by dimension names and values given by
1298             arrays of coordinates tick labels. Any mis-matched coordinate
1299             values will be filled in with NaN, and any mis-matched dimension
1300             names will simply be ignored.
1301             One of indexers or indexers_kwargs must be provided.
1302         copy : bool, optional
1303             If ``copy=True``, data in the return value is always copied. If
1304             ``copy=False`` and reindexing is unnecessary, or can be performed
1305             with only slice operations, then the output may share memory with
1306             the input. In either case, a new xarray object is always returned.
1307         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1308             Method to use for filling index values in ``indexers`` not found on
1309             this data array:
1310 
1311             * None (default): don't fill gaps
1312             * pad / ffill: propagate last valid index value forward
1313             * backfill / bfill: propagate next valid index value backward
1314             * nearest: use nearest valid index value
1315         tolerance : optional
1316             Maximum distance between original and new labels for inexact
1317             matches. The values of the index at the matching locations must
1318             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1319         fill_value : scalar, optional
1320             Value to use for newly missing values
1321         **indexers_kwargs : {dim: indexer, ...}, optional
1322             The keyword arguments form of ``indexers``.
1323             One of indexers or indexers_kwargs must be provided.
1324 
1325         Returns
1326         -------
1327         reindexed : DataArray
1328             Another dataset array, with this array's data but replaced
1329             coordinates.
1330 
1331         See Also
1332         --------
1333         DataArray.reindex_like
1334         align
1335         """
1336         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1337         ds = self._to_temp_dataset().reindex(
1338             indexers=indexers,
1339             method=method,
1340             tolerance=tolerance,
1341             copy=copy,
1342             fill_value=fill_value,
1343         )
1344         return self._from_temp_dataset(ds)
1345 
1346     def interp(
1347         self,
1348         coords: Mapping[Hashable, Any] = None,
1349         method: str = "linear",
1350         assume_sorted: bool = False,
1351         kwargs: Mapping[str, Any] = None,
1352         **coords_kwargs: Any,
1353     ) -> "DataArray":
1354         """ Multidimensional interpolation of variables.
1355 
1356         coords : dict, optional
1357             Mapping from dimension names to the new coordinates.
1358             new coordinate can be an scalar, array-like or DataArray.
1359             If DataArrays are passed as new coordates, their dimensions are
1360             used for the broadcasting.
1361         method: {'linear', 'nearest'} for multidimensional array,
1362             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1363             for 1-dimensional array.
1364         assume_sorted: boolean, optional
1365             If False, values of x can be in any order and they are sorted
1366             first. If True, x has to be an array of monotonically increasing
1367             values.
1368         kwargs: dictionary
1369             Additional keyword passed to scipy's interpolator.
1370         ``**coords_kwargs`` : {dim: coordinate, ...}, optional
1371             The keyword arguments form of ``coords``.
1372             One of coords or coords_kwargs must be provided.
1373 
1374         Returns
1375         -------
1376         interpolated: xr.DataArray
1377             New dataarray on the new coordinates.
1378 
1379         Notes
1380         -----
1381         scipy is required.
1382 
1383         See Also
1384         --------
1385         scipy.interpolate.interp1d
1386         scipy.interpolate.interpn
1387 
1388         Examples
1389         --------
1390         >>> da = xr.DataArray([1, 3], [("x", np.arange(2))])
1391         >>> da.interp(x=0.5)
1392         <xarray.DataArray ()>
1393         array(2.0)
1394         Coordinates:
1395             x        float64 0.5
1396         """
1397         if self.dtype.kind not in "uifc":
1398             raise TypeError(
1399                 "interp only works for a numeric type array. "
1400                 "Given {}.".format(self.dtype)
1401             )
1402         ds = self._to_temp_dataset().interp(
1403             coords,
1404             method=method,
1405             kwargs=kwargs,
1406             assume_sorted=assume_sorted,
1407             **coords_kwargs,
1408         )
1409         return self._from_temp_dataset(ds)
1410 
1411     def interp_like(
1412         self,
1413         other: Union["DataArray", Dataset],
1414         method: str = "linear",
1415         assume_sorted: bool = False,
1416         kwargs: Mapping[str, Any] = None,
1417     ) -> "DataArray":
1418         """Interpolate this object onto the coordinates of another object,
1419         filling out of range values with NaN.
1420 
1421         Parameters
1422         ----------
1423         other : Dataset or DataArray
1424             Object with an 'indexes' attribute giving a mapping from dimension
1425             names to an 1d array-like, which provides coordinates upon
1426             which to index the variables in this dataset.
1427         method: string, optional.
1428             {'linear', 'nearest'} for multidimensional array,
1429             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1430             for 1-dimensional array. 'linear' is used by default.
1431         assume_sorted: boolean, optional
1432             If False, values of coordinates that are interpolated over can be
1433             in any order and they are sorted first. If True, interpolated
1434             coordinates are assumed to be an array of monotonically increasing
1435             values.
1436         kwargs: dictionary, optional
1437             Additional keyword passed to scipy's interpolator.
1438 
1439         Returns
1440         -------
1441         interpolated: xr.DataArray
1442             Another dataarray by interpolating this dataarray's data along the
1443             coordinates of the other object.
1444 
1445         Notes
1446         -----
1447         scipy is required.
1448         If the dataarray has object-type coordinates, reindex is used for these
1449         coordinates instead of the interpolation.
1450 
1451         See Also
1452         --------
1453         DataArray.interp
1454         DataArray.reindex_like
1455         """
1456         if self.dtype.kind not in "uifc":
1457             raise TypeError(
1458                 "interp only works for a numeric type array. "
1459                 "Given {}.".format(self.dtype)
1460             )
1461         ds = self._to_temp_dataset().interp_like(
1462             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1463         )
1464         return self._from_temp_dataset(ds)
1465 
1466     def rename(
1467         self,
1468         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1469         **names: Hashable,
1470     ) -> "DataArray":
1471         """Returns a new DataArray with renamed coordinates or a new name.
1472 
1473         Parameters
1474         ----------
1475         new_name_or_name_dict : str or dict-like, optional
1476             If the argument is dict-like, it used as a mapping from old
1477             names to new names for coordinates. Otherwise, use the argument
1478             as the new name for this array.
1479         **names: hashable, optional
1480             The keyword arguments form of a mapping from old names to
1481             new names for coordinates.
1482             One of new_name_or_name_dict or names must be provided.
1483 
1484         Returns
1485         -------
1486         renamed : DataArray
1487             Renamed array or array with renamed coordinates.
1488 
1489         See Also
1490         --------
1491         Dataset.rename
1492         DataArray.swap_dims
1493         """
1494         if names or utils.is_dict_like(new_name_or_name_dict):
1495             new_name_or_name_dict = cast(
1496                 Mapping[Hashable, Hashable], new_name_or_name_dict
1497             )
1498             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1499             dataset = self._to_temp_dataset().rename(name_dict)
1500             return self._from_temp_dataset(dataset)
1501         else:
1502             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1503             return self._replace(name=new_name_or_name_dict)
1504 
1505     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1506         """Returns a new DataArray with swapped dimensions.
1507 
1508         Parameters
1509         ----------
1510         dims_dict : dict-like
1511             Dictionary whose keys are current dimension names and whose values
1512             are new names.
1513 
1514         Returns
1515         -------
1516         swapped : DataArray
1517             DataArray with swapped dimensions.
1518 
1519         Examples
1520         --------
1521 
1522         >>> arr = xr.DataArray(
1523         ...     data=[0, 1], dims="x", coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1524         ... )
1525         >>> arr
1526         <xarray.DataArray (x: 2)>
1527         array([0, 1])
1528         Coordinates:
1529           * x        (x) <U1 'a' 'b'
1530             y        (x) int64 0 1
1531 
1532         >>> arr.swap_dims({"x": "y"})
1533         <xarray.DataArray (y: 2)>
1534         array([0, 1])
1535         Coordinates:
1536             x        (y) <U1 'a' 'b'
1537           * y        (y) int64 0 1
1538 
1539         >>> arr.swap_dims({"x": "z"})
1540         <xarray.DataArray (z: 2)>
1541         array([0, 1])
1542         Coordinates:
1543             x        (z) <U1 'a' 'b'
1544             y        (z) int64 0 1
1545         Dimensions without coordinates: z
1546 
1547         See Also
1548         --------
1549 
1550         DataArray.rename
1551         Dataset.swap_dims
1552         """
1553         ds = self._to_temp_dataset().swap_dims(dims_dict)
1554         return self._from_temp_dataset(ds)
1555 
1556     def expand_dims(
1557         self,
1558         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1559         axis=None,
1560         **dim_kwargs: Any,
1561     ) -> "DataArray":
1562         """Return a new object with an additional axis (or axes) inserted at
1563         the corresponding position in the array shape. The new object is a
1564         view into the underlying array, not a copy.
1565 
1566 
1567         If dim is already a scalar coordinate, it will be promoted to a 1D
1568         coordinate consisting of a single value.
1569 
1570         Parameters
1571         ----------
1572         dim : hashable, sequence of hashable, dict, or None
1573             Dimensions to include on the new variable.
1574             If provided as str or sequence of str, then dimensions are inserted
1575             with length 1. If provided as a dict, then the keys are the new
1576             dimensions and the values are either integers (giving the length of
1577             the new dimensions) or sequence/ndarray (giving the coordinates of
1578             the new dimensions).
1579         axis : integer, list (or tuple) of integers, or None
1580             Axis position(s) where new axis is to be inserted (position(s) on
1581             the result array). If a list (or tuple) of integers is passed,
1582             multiple axes are inserted. In this case, dim arguments should be
1583             same length list. If axis=None is passed, all the axes will be
1584             inserted to the start of the result array.
1585         **dim_kwargs : int or sequence/ndarray
1586             The keywords are arbitrary dimensions being inserted and the values
1587             are either the lengths of the new dims (if int is given), or their
1588             coordinates. Note, this is an alternative to passing a dict to the
1589             dim kwarg and will only be used if dim is None.
1590 
1591         Returns
1592         -------
1593         expanded : same type as caller
1594             This object, but with an additional dimension(s).
1595         """
1596         if isinstance(dim, int):
1597             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1598         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1599             if len(dim) != len(set(dim)):
1600                 raise ValueError("dims should not contain duplicate values.")
1601             dim = dict.fromkeys(dim, 1)
1602         elif dim is not None and not isinstance(dim, Mapping):
1603             dim = {cast(Hashable, dim): 1}
1604 
1605         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1606         ds = self._to_temp_dataset().expand_dims(dim, axis)
1607         return self._from_temp_dataset(ds)
1608 
1609     def set_index(
1610         self,
1611         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1612         append: bool = False,
1613         inplace: bool = None,
1614         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1615     ) -> Optional["DataArray"]:
1616         """Set DataArray (multi-)indexes using one or more existing
1617         coordinates.
1618 
1619         Parameters
1620         ----------
1621         indexes : {dim: index, ...}
1622             Mapping from names matching dimensions and values given
1623             by (lists of) the names of existing coordinates or variables to set
1624             as new (multi-)index.
1625         append : bool, optional
1626             If True, append the supplied index(es) to the existing index(es).
1627             Otherwise replace the existing index(es) (default).
1628         **indexes_kwargs: optional
1629             The keyword arguments form of ``indexes``.
1630             One of indexes or indexes_kwargs must be provided.
1631 
1632         Returns
1633         -------
1634         obj : DataArray
1635             Another DataArray, with this data but replaced coordinates.
1636 
1637         Examples
1638         --------
1639         >>> arr = xr.DataArray(
1640         ...     data=np.ones((2, 3)),
1641         ...     dims=["x", "y"],
1642         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1643         ... )
1644         >>> arr
1645         <xarray.DataArray (x: 2, y: 3)>
1646         array([[1., 1., 1.],
1647                [1., 1., 1.]])
1648         Coordinates:
1649           * x        (x) int64 0 1
1650           * y        (y) int64 0 1 2
1651             a        (x) int64 3 4
1652         >>> arr.set_index(x="a")
1653         <xarray.DataArray (x: 2, y: 3)>
1654         array([[1., 1., 1.],
1655                [1., 1., 1.]])
1656         Coordinates:
1657           * x        (x) int64 3 4
1658           * y        (y) int64 0 1 2
1659 
1660         See Also
1661         --------
1662         DataArray.reset_index
1663         """
1664         ds = self._to_temp_dataset().set_index(
1665             indexes, append=append, inplace=inplace, **indexes_kwargs
1666         )
1667         return self._from_temp_dataset(ds)
1668 
1669     def reset_index(
1670         self,
1671         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1672         drop: bool = False,
1673         inplace: bool = None,
1674     ) -> Optional["DataArray"]:
1675         """Reset the specified index(es) or multi-index level(s).
1676 
1677         Parameters
1678         ----------
1679         dims_or_levels : hashable or sequence of hashables
1680             Name(s) of the dimension(s) and/or multi-index level(s) that will
1681             be reset.
1682         drop : bool, optional
1683             If True, remove the specified indexes and/or multi-index levels
1684             instead of extracting them as new coordinates (default: False).
1685 
1686         Returns
1687         -------
1688         obj : DataArray
1689             Another dataarray, with this dataarray's data but replaced
1690             coordinates.
1691 
1692         See Also
1693         --------
1694         DataArray.set_index
1695         """
1696         _check_inplace(inplace)
1697         coords, _ = split_indexes(
1698             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1699         )
1700         return self._replace(coords=coords)
1701 
1702     def reorder_levels(
1703         self,
1704         dim_order: Mapping[Hashable, Sequence[int]] = None,
1705         inplace: bool = None,
1706         **dim_order_kwargs: Sequence[int],
1707     ) -> "DataArray":
1708         """Rearrange index levels using input order.
1709 
1710         Parameters
1711         ----------
1712         dim_order : optional
1713             Mapping from names matching dimensions and values given
1714             by lists representing new level orders. Every given dimension
1715             must have a multi-index.
1716         **dim_order_kwargs: optional
1717             The keyword arguments form of ``dim_order``.
1718             One of dim_order or dim_order_kwargs must be provided.
1719 
1720         Returns
1721         -------
1722         obj : DataArray
1723             Another dataarray, with this dataarray's data but replaced
1724             coordinates.
1725         """
1726         _check_inplace(inplace)
1727         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1728         replace_coords = {}
1729         for dim, order in dim_order.items():
1730             coord = self._coords[dim]
1731             index = coord.to_index()
1732             if not isinstance(index, pd.MultiIndex):
1733                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1734             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1735         coords = self._coords.copy()
1736         coords.update(replace_coords)
1737         return self._replace(coords=coords)
1738 
1739     def stack(
1740         self,
1741         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1742         **dimensions_kwargs: Sequence[Hashable],
1743     ) -> "DataArray":
1744         """
1745         Stack any number of existing dimensions into a single new dimension.
1746 
1747         New dimensions will be added at the end, and the corresponding
1748         coordinate variables will be combined into a MultiIndex.
1749 
1750         Parameters
1751         ----------
1752         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1753             Names of new dimensions, and the existing dimensions that they
1754             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1755             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1756             all dimensions.
1757         **dimensions_kwargs:
1758             The keyword arguments form of ``dimensions``.
1759             One of dimensions or dimensions_kwargs must be provided.
1760 
1761         Returns
1762         -------
1763         stacked : DataArray
1764             DataArray with stacked data.
1765 
1766         Examples
1767         --------
1768 
1769         >>> arr = xr.DataArray(
1770         ...     np.arange(6).reshape(2, 3),
1771         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1772         ... )
1773         >>> arr
1774         <xarray.DataArray (x: 2, y: 3)>
1775         array([[0, 1, 2],
1776                [3, 4, 5]])
1777         Coordinates:
1778           * x        (x) |S1 'a' 'b'
1779           * y        (y) int64 0 1 2
1780         >>> stacked = arr.stack(z=("x", "y"))
1781         >>> stacked.indexes["z"]
1782         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1783                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1784                    names=['x', 'y'])
1785 
1786         See Also
1787         --------
1788         DataArray.unstack
1789         """
1790         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1791         return self._from_temp_dataset(ds)
1792 
1793     def unstack(
1794         self,
1795         dim: Union[Hashable, Sequence[Hashable], None] = None,
1796         fill_value: Any = dtypes.NA,
1797         sparse: bool = False,
1798     ) -> "DataArray":
1799         """
1800         Unstack existing dimensions corresponding to MultiIndexes into
1801         multiple new dimensions.
1802 
1803         New dimensions will be added at the end.
1804 
1805         Parameters
1806         ----------
1807         dim : hashable or sequence of hashable, optional
1808             Dimension(s) over which to unstack. By default unstacks all
1809             MultiIndexes.
1810         fill_value: value to be filled. By default, np.nan
1811         sparse: use sparse-array if True
1812 
1813         Returns
1814         -------
1815         unstacked : DataArray
1816             Array with unstacked data.
1817 
1818         Examples
1819         --------
1820 
1821         >>> arr = xr.DataArray(
1822         ...     np.arange(6).reshape(2, 3),
1823         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1824         ... )
1825         >>> arr
1826         <xarray.DataArray (x: 2, y: 3)>
1827         array([[0, 1, 2],
1828                [3, 4, 5]])
1829         Coordinates:
1830           * x        (x) |S1 'a' 'b'
1831           * y        (y) int64 0 1 2
1832         >>> stacked = arr.stack(z=("x", "y"))
1833         >>> stacked.indexes["z"]
1834         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1835                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1836                    names=['x', 'y'])
1837         >>> roundtripped = stacked.unstack()
1838         >>> arr.identical(roundtripped)
1839         True
1840 
1841         See Also
1842         --------
1843         DataArray.stack
1844         """
1845         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
1846         return self._from_temp_dataset(ds)
1847 
1848     def to_unstacked_dataset(self, dim, level=0):
1849         """Unstack DataArray expanding to Dataset along a given level of a
1850         stacked coordinate.
1851 
1852         This is the inverse operation of Dataset.to_stacked_array.
1853 
1854         Parameters
1855         ----------
1856         dim : str
1857             Name of existing dimension to unstack
1858         level : int or str
1859             The MultiIndex level to expand to a dataset along. Can either be
1860             the integer index of the level or its name.
1861         label : int, default 0
1862             Label of the level to expand dataset along. Overrides the label
1863             argument if given.
1864 
1865         Returns
1866         -------
1867         unstacked: Dataset
1868 
1869         Examples
1870         --------
1871         >>> import xarray as xr
1872         >>> arr = xr.DataArray(
1873         ...     np.arange(6).reshape(2, 3),
1874         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1875         ... )
1876         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
1877         >>> data
1878         <xarray.Dataset>
1879         Dimensions:  (x: 2, y: 3)
1880         Coordinates:
1881           * x        (x) <U1 'a' 'b'
1882           * y        (y) int64 0 1 2
1883         Data variables:
1884             a        (x, y) int64 0 1 2 3 4 5
1885             b        (x) int64 0 3
1886         >>> stacked = data.to_stacked_array("z", ["y"])
1887         >>> stacked.indexes["z"]
1888         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1889                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1890                 names=['variable', 'y'])
1891         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
1892         >>> data.identical(roundtripped)
1893         True
1894 
1895         See Also
1896         --------
1897         Dataset.to_stacked_array
1898         """
1899 
1900         idx = self.indexes[dim]
1901         if not isinstance(idx, pd.MultiIndex):
1902             raise ValueError(f"'{dim}' is not a stacked coordinate")
1903 
1904         level_number = idx._get_level_number(level)
1905         variables = idx.levels[level_number]
1906         variable_dim = idx.names[level_number]
1907 
1908         # pull variables out of datarray
1909         data_dict = {}
1910         for k in variables:
1911             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1912 
1913         # unstacked dataset
1914         return Dataset(data_dict)
1915 
1916     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1917         """Return a new DataArray object with transposed dimensions.
1918 
1919         Parameters
1920         ----------
1921         *dims : hashable, optional
1922             By default, reverse the dimensions. Otherwise, reorder the
1923             dimensions to this order.
1924         transpose_coords : boolean, optional
1925             If True, also transpose the coordinates of this DataArray.
1926 
1927         Returns
1928         -------
1929         transposed : DataArray
1930             The returned DataArray's array is transposed.
1931 
1932         Notes
1933         -----
1934         This operation returns a view of this array's data. It is
1935         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1936         -- the data will be fully loaded.
1937 
1938         See Also
1939         --------
1940         numpy.transpose
1941         Dataset.transpose
1942         """
1943         if dims:
1944             dims = tuple(utils.infix_dims(dims, self.dims))
1945         variable = self.variable.transpose(*dims)
1946         if transpose_coords:
1947             coords: Dict[Hashable, Variable] = {}
1948             for name, coord in self.coords.items():
1949                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1950                 coords[name] = coord.variable.transpose(*coord_dims)
1951             return self._replace(variable, coords)
1952         else:
1953             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1954                 warnings.warn(
1955                     "This DataArray contains multi-dimensional "
1956                     "coordinates. In the future, these coordinates "
1957                     "will be transposed as well unless you specify "
1958                     "transpose_coords=False.",
1959                     FutureWarning,
1960                     stacklevel=2,
1961                 )
1962             return self._replace(variable)
1963 
1964     @property
1965     def T(self) -> "DataArray":
1966         return self.transpose()
1967 
1968     def drop_vars(
1969         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1970     ) -> "DataArray":
1971         """Drop variables from this DataArray.
1972 
1973         Parameters
1974         ----------
1975         names : hashable or iterable of hashables
1976             Name(s) of variables to drop.
1977         errors: {'raise', 'ignore'}, optional
1978             If 'raise' (default), raises a ValueError error if any of the variable
1979             passed are not in the dataset. If 'ignore', any given names that are in the
1980             DataArray are dropped and no error is raised.
1981 
1982         Returns
1983         -------
1984         dropped : Dataset
1985 
1986         """
1987         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
1988         return self._from_temp_dataset(ds)
1989 
1990     def drop(
1991         self,
1992         labels: Mapping = None,
1993         dim: Hashable = None,
1994         *,
1995         errors: str = "raise",
1996         **labels_kwargs,
1997     ) -> "DataArray":
1998         """Backward compatible method based on `drop_vars` and `drop_sel`
1999 
2000         Using either `drop_vars` or `drop_sel` is encouraged
2001 
2002         See Also
2003         --------
2004         DataArray.drop_vars
2005         DataArray.drop_sel
2006         """
2007         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2008         return self._from_temp_dataset(ds)
2009 
2010     def drop_sel(
2011         self,
2012         labels: Mapping[Hashable, Any] = None,
2013         *,
2014         errors: str = "raise",
2015         **labels_kwargs,
2016     ) -> "DataArray":
2017         """Drop index labels from this DataArray.
2018 
2019         Parameters
2020         ----------
2021         labels : Mapping[Hashable, Any]
2022             Index labels to drop
2023         errors: {'raise', 'ignore'}, optional
2024             If 'raise' (default), raises a ValueError error if
2025             any of the index labels passed are not
2026             in the dataset. If 'ignore', any given labels that are in the
2027             dataset are dropped and no error is raised.
2028         **labels_kwargs : {dim: label, ...}, optional
2029             The keyword arguments form of ``dim`` and ``labels``
2030 
2031         Returns
2032         -------
2033         dropped : DataArray
2034         """
2035         if labels_kwargs or isinstance(labels, dict):
2036             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2037 
2038         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2039         return self._from_temp_dataset(ds)
2040 
2041     def dropna(
2042         self, dim: Hashable, how: str = "any", thresh: int = None
2043     ) -> "DataArray":
2044         """Returns a new array with dropped labels for missing values along
2045         the provided dimension.
2046 
2047         Parameters
2048         ----------
2049         dim : hashable
2050             Dimension along which to drop missing values. Dropping along
2051             multiple dimensions simultaneously is not yet supported.
2052         how : {'any', 'all'}, optional
2053             * any : if any NA values are present, drop that label
2054             * all : if all values are NA, drop that label
2055         thresh : int, default None
2056             If supplied, require this many non-NA values.
2057 
2058         Returns
2059         -------
2060         DataArray
2061         """
2062         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2063         return self._from_temp_dataset(ds)
2064 
2065     def fillna(self, value: Any) -> "DataArray":
2066         """Fill missing values in this object.
2067 
2068         This operation follows the normal broadcasting and alignment rules that
2069         xarray uses for binary arithmetic, except the result is aligned to this
2070         object (``join='left'``) instead of aligned to the intersection of
2071         index coordinates (``join='inner'``).
2072 
2073         Parameters
2074         ----------
2075         value : scalar, ndarray or DataArray
2076             Used to fill all matching missing values in this array. If the
2077             argument is a DataArray, it is first aligned with (reindexed to)
2078             this array.
2079 
2080         Returns
2081         -------
2082         DataArray
2083         """
2084         if utils.is_dict_like(value):
2085             raise TypeError(
2086                 "cannot provide fill value as a dictionary with "
2087                 "fillna on a DataArray"
2088             )
2089         out = ops.fillna(self, value)
2090         return out
2091 
2092     def interpolate_na(
2093         self,
2094         dim: Hashable = None,
2095         method: str = "linear",
2096         limit: int = None,
2097         use_coordinate: Union[bool, str] = True,
2098         max_gap: Union[
2099             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2100         ] = None,
2101         **kwargs: Any,
2102     ) -> "DataArray":
2103         """Fill in NaNs by interpolating according to different methods.
2104 
2105         Parameters
2106         ----------
2107         dim : str
2108             Specifies the dimension along which to interpolate.
2109         method : str, optional
2110             String indicating which method to use for interpolation:
2111 
2112             - 'linear': linear interpolation (Default). Additional keyword
2113               arguments are passed to :py:func:`numpy.interp`
2114             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2115               are passed to :py:func:`scipy.interpolate.interp1d`. If
2116               ``method='polynomial'``, the ``order`` keyword argument must also be
2117               provided.
2118             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2119               respective :py:class:`scipy.interpolate` classes.
2120 
2121         use_coordinate : bool, str, default True
2122             Specifies which index to use as the x values in the interpolation
2123             formulated as `y = f(x)`. If False, values are treated as if
2124             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2125             used. If ``use_coordinate`` is a string, it specifies the name of a
2126             coordinate variariable to use as the index.
2127         limit : int, default None
2128             Maximum number of consecutive NaNs to fill. Must be greater than 0
2129             or None for no limit. This filling is done regardless of the size of
2130             the gap in the data. To only interpolate over gaps less than a given length,
2131             see ``max_gap``.
2132         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
2133             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2134             Use None for no limit. When interpolating along a datetime64 dimension
2135             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2136 
2137             - a string that is valid input for pandas.to_timedelta
2138             - a :py:class:`numpy.timedelta64` object
2139             - a :py:class:`pandas.Timedelta` object
2140             - a :py:class:`datetime.timedelta` object
2141 
2142             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2143             dimensions has not been implemented yet. Gap length is defined as the difference
2144             between coordinate values at the first data point after a gap and the last value
2145             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2146             between coordinate values at the first (last) valid data point and the first (last) NaN.
2147             For example, consider::
2148 
2149                 <xarray.DataArray (x: 9)>
2150                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2151                 Coordinates:
2152                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2153 
2154             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2155         kwargs : dict, optional
2156             parameters passed verbatim to the underlying interpolation function
2157 
2158         Returns
2159         -------
2160         interpolated: DataArray
2161             Filled in DataArray.
2162 
2163         See also
2164         --------
2165         numpy.interp
2166         scipy.interpolate
2167         """
2168         from .missing import interp_na
2169 
2170         return interp_na(
2171             self,
2172             dim=dim,
2173             method=method,
2174             limit=limit,
2175             use_coordinate=use_coordinate,
2176             max_gap=max_gap,
2177             **kwargs,
2178         )
2179 
2180     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2181         """Fill NaN values by propogating values forward
2182 
2183         *Requires bottleneck.*
2184 
2185         Parameters
2186         ----------
2187         dim : hashable
2188             Specifies the dimension along which to propagate values when
2189             filling.
2190         limit : int, default None
2191             The maximum number of consecutive NaN values to forward fill. In
2192             other words, if there is a gap with more than this number of
2193             consecutive NaNs, it will only be partially filled. Must be greater
2194             than 0 or None for no limit.
2195 
2196         Returns
2197         -------
2198         DataArray
2199         """
2200         from .missing import ffill
2201 
2202         return ffill(self, dim, limit=limit)
2203 
2204     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2205         """Fill NaN values by propogating values backward
2206 
2207         *Requires bottleneck.*
2208 
2209         Parameters
2210         ----------
2211         dim : str
2212             Specifies the dimension along which to propagate values when
2213             filling.
2214         limit : int, default None
2215             The maximum number of consecutive NaN values to backward fill. In
2216             other words, if there is a gap with more than this number of
2217             consecutive NaNs, it will only be partially filled. Must be greater
2218             than 0 or None for no limit.
2219 
2220         Returns
2221         -------
2222         DataArray
2223         """
2224         from .missing import bfill
2225 
2226         return bfill(self, dim, limit=limit)
2227 
2228     def combine_first(self, other: "DataArray") -> "DataArray":
2229         """Combine two DataArray objects, with union of coordinates.
2230 
2231         This operation follows the normal broadcasting and alignment rules of
2232         ``join='outer'``.  Default to non-null values of array calling the
2233         method.  Use np.nan to fill in vacant cells after alignment.
2234 
2235         Parameters
2236         ----------
2237         other : DataArray
2238             Used to fill all matching missing values in this array.
2239 
2240         Returns
2241         -------
2242         DataArray
2243         """
2244         return ops.fillna(self, other, join="outer")
2245 
2246     def reduce(
2247         self,
2248         func: Callable[..., Any],
2249         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2250         axis: Union[None, int, Sequence[int]] = None,
2251         keep_attrs: bool = None,
2252         keepdims: bool = False,
2253         **kwargs: Any,
2254     ) -> "DataArray":
2255         """Reduce this array by applying `func` along some dimension(s).
2256 
2257         Parameters
2258         ----------
2259         func : function
2260             Function which can be called in the form
2261             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2262             np.ndarray over an integer valued axis.
2263         dim : hashable or sequence of hashables, optional
2264             Dimension(s) over which to apply `func`.
2265         axis : int or sequence of int, optional
2266             Axis(es) over which to repeatedly apply `func`. Only one of the
2267             'dim' and 'axis' arguments can be supplied. If neither are
2268             supplied, then the reduction is calculated over the flattened array
2269             (by calling `f(x)` without an axis argument).
2270         keep_attrs : bool, optional
2271             If True, the variable's attributes (`attrs`) will be copied from
2272             the original object to the new one.  If False (default), the new
2273             object will be returned without attributes.
2274         keepdims : bool, default False
2275             If True, the dimensions which are reduced are left in the result
2276             as dimensions of size one. Coordinates that use these dimensions
2277             are removed.
2278         **kwargs : dict
2279             Additional keyword arguments passed on to `func`.
2280 
2281         Returns
2282         -------
2283         reduced : DataArray
2284             DataArray with this object's array replaced with an array with
2285             summarized data and the indicated dimension(s) removed.
2286         """
2287 
2288         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2289         return self._replace_maybe_drop_dims(var)
2290 
2291     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2292         """Convert this array into a pandas object with the same shape.
2293 
2294         The type of the returned object depends on the number of DataArray
2295         dimensions:
2296 
2297         * 0D -> `xarray.DataArray`
2298         * 1D -> `pandas.Series`
2299         * 2D -> `pandas.DataFrame`
2300 
2301         Only works for arrays with 2 or fewer dimensions.
2302 
2303         The DataArray constructor performs the inverse transformation.
2304         """
2305         # TODO: consolidate the info about pandas constructors and the
2306         # attributes that correspond to their indexes into a separate module?
2307         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2308         try:
2309             constructor = constructors[self.ndim]
2310         except KeyError:
2311             raise ValueError(
2312                 "cannot convert arrays with %s dimensions into "
2313                 "pandas objects" % self.ndim
2314             )
2315         indexes = [self.get_index(dim) for dim in self.dims]
2316         return constructor(self.values, *indexes)
2317 
2318     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2319         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2320 
2321         The DataFrame is indexed by the Cartesian product of index coordinates
2322         (in the form of a :py:class:`pandas.MultiIndex`).
2323 
2324         Other coordinates are included as columns in the DataFrame.
2325         """
2326         if name is None:
2327             name = self.name
2328         if name is None:
2329             raise ValueError(
2330                 "cannot convert an unnamed DataArray to a "
2331                 "DataFrame: use the ``name`` parameter"
2332             )
2333 
2334         dims = dict(zip(self.dims, self.shape))
2335         # By using a unique name, we can convert a DataArray into a DataFrame
2336         # even if it shares a name with one of its coordinates.
2337         # I would normally use unique_name = object() but that results in a
2338         # dataframe with columns in the wrong order, for reasons I have not
2339         # been able to debug (possibly a pandas bug?).
2340         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2341         ds = self._to_dataset_whole(name=unique_name)
2342         df = ds._to_dataframe(dims)
2343         df.columns = [name if c == unique_name else c for c in df.columns]
2344         return df
2345 
2346     def to_series(self) -> pd.Series:
2347         """Convert this array into a pandas.Series.
2348 
2349         The Series is indexed by the Cartesian product of index coordinates
2350         (in the form of a :py:class:`pandas.MultiIndex`).
2351         """
2352         index = self.coords.to_index()
2353         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2354 
2355     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2356         """Convert this array into a numpy.ma.MaskedArray
2357 
2358         Parameters
2359         ----------
2360         copy : bool
2361             If True (default) make a copy of the array in the result. If False,
2362             a MaskedArray view of DataArray.values is returned.
2363 
2364         Returns
2365         -------
2366         result : MaskedArray
2367             Masked where invalid values (nan or inf) occur.
2368         """
2369         values = self.values  # only compute lazy arrays once
2370         isnull = pd.isnull(values)
2371         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2372 
2373     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2374         """Write DataArray contents to a netCDF file.
2375 
2376         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2377 
2378         Notes
2379         -----
2380         Only xarray.Dataset objects can be written to netCDF files, so
2381         the xarray.DataArray is converted to a xarray.Dataset object
2382         containing a single variable. If the DataArray has no name, or if the
2383         name is the same as a co-ordinate name, then it is given the name
2384         '__xarray_dataarray_variable__'.
2385         """
2386         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2387 
2388         if self.name is None:
2389             # If no name is set then use a generic xarray name
2390             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2391         elif self.name in self.coords or self.name in self.dims:
2392             # The name is the same as one of the coords names, which netCDF
2393             # doesn't support, so rename it but keep track of the old name
2394             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2395             dataset.attrs[DATAARRAY_NAME] = self.name
2396         else:
2397             # No problems with the name - so we're fine!
2398             dataset = self.to_dataset()
2399 
2400         return dataset.to_netcdf(*args, **kwargs)
2401 
2402     def to_dict(self, data: bool = True) -> dict:
2403         """
2404         Convert this xarray.DataArray into a dictionary following xarray
2405         naming conventions.
2406 
2407         Converts all variables and attributes to native Python objects.
2408         Useful for converting to json. To avoid datetime incompatibility
2409         use decode_times=False kwarg in xarrray.open_dataset.
2410 
2411         Parameters
2412         ----------
2413         data : bool, optional
2414             Whether to include the actual data in the dictionary. When set to
2415             False, returns just the schema.
2416 
2417         See also
2418         --------
2419         DataArray.from_dict
2420         """
2421         d = self.variable.to_dict(data=data)
2422         d.update({"coords": {}, "name": self.name})
2423         for k in self.coords:
2424             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2425         return d
2426 
2427     @classmethod
2428     def from_dict(cls, d: dict) -> "DataArray":
2429         """
2430         Convert a dictionary into an xarray.DataArray
2431 
2432         Input dict can take several forms::
2433 
2434             d = {'dims': ('t'), 'data': x}
2435 
2436             d = {'coords': {'t': {'dims': 't', 'data': t,
2437                                   'attrs': {'units':'s'}}},
2438                  'attrs': {'title': 'air temperature'},
2439                  'dims': 't',
2440                  'data': x,
2441                  'name': 'a'}
2442 
2443         where 't' is the name of the dimesion, 'a' is the name of the array,
2444         and  x and t are lists, numpy.arrays, or pandas objects.
2445 
2446         Parameters
2447         ----------
2448         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2449 
2450         Returns
2451         -------
2452         obj : xarray.DataArray
2453 
2454         See also
2455         --------
2456         DataArray.to_dict
2457         Dataset.from_dict
2458         """
2459         coords = None
2460         if "coords" in d:
2461             try:
2462                 coords = {
2463                     k: (v["dims"], v["data"], v.get("attrs"))
2464                     for k, v in d["coords"].items()
2465                 }
2466             except KeyError as e:
2467                 raise ValueError(
2468                     "cannot convert dict when coords are missing the key "
2469                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2470                 )
2471         try:
2472             data = d["data"]
2473         except KeyError:
2474             raise ValueError("cannot convert dict without the key 'data''")
2475         else:
2476             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2477         return obj
2478 
2479     @classmethod
2480     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2481         """Convert a pandas.Series into an xarray.DataArray.
2482 
2483         If the series's index is a MultiIndex, it will be expanded into a
2484         tensor product of one-dimensional coordinates (filling in missing
2485         values with NaN). Thus this operation should be the inverse of the
2486         `to_series` method.
2487 
2488         If sparse=True, creates a sparse array instead of a dense NumPy array.
2489         Requires the pydata/sparse package.
2490 
2491         See also
2492         --------
2493         xarray.Dataset.from_dataframe
2494         """
2495         temp_name = "__temporary_name"
2496         df = pd.DataFrame({temp_name: series})
2497         ds = Dataset.from_dataframe(df, sparse=sparse)
2498         result = cast(DataArray, ds[temp_name])
2499         result.name = series.name
2500         return result
2501 
2502     def to_cdms2(self) -> "cdms2_Variable":
2503         """Convert this array into a cdms2.Variable
2504         """
2505         from ..convert import to_cdms2
2506 
2507         return to_cdms2(self)
2508 
2509     @classmethod
2510     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2511         """Convert a cdms2.Variable into an xarray.DataArray
2512         """
2513         from ..convert import from_cdms2
2514 
2515         return from_cdms2(variable)
2516 
2517     def to_iris(self) -> "iris_Cube":
2518         """Convert this array into a iris.cube.Cube
2519         """
2520         from ..convert import to_iris
2521 
2522         return to_iris(self)
2523 
2524     @classmethod
2525     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2526         """Convert a iris.cube.Cube into an xarray.DataArray
2527         """
2528         from ..convert import from_iris
2529 
2530         return from_iris(cube)
2531 
2532     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2533         """Helper function for equals, broadcast_equals, and identical
2534         """
2535 
2536         def compat(x, y):
2537             return getattr(x.variable, compat_str)(y.variable)
2538 
2539         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2540             self, other
2541         )
2542 
2543     def broadcast_equals(self, other: "DataArray") -> bool:
2544         """Two DataArrays are broadcast equal if they are equal after
2545         broadcasting them against each other such that they have the same
2546         dimensions.
2547 
2548         See Also
2549         --------
2550         DataArray.equals
2551         DataArray.identical
2552         """
2553         try:
2554             return self._all_compat(other, "broadcast_equals")
2555         except (TypeError, AttributeError):
2556             return False
2557 
2558     def equals(self, other: "DataArray") -> bool:
2559         """True if two DataArrays have the same dimensions, coordinates and
2560         values; otherwise False.
2561 
2562         DataArrays can still be equal (like pandas objects) if they have NaN
2563         values in the same locations.
2564 
2565         This method is necessary because `v1 == v2` for ``DataArray``
2566         does element-wise comparisons (like numpy.ndarrays).
2567 
2568         See Also
2569         --------
2570         DataArray.broadcast_equals
2571         DataArray.identical
2572         """
2573         try:
2574             return self._all_compat(other, "equals")
2575         except (TypeError, AttributeError):
2576             return False
2577 
2578     def identical(self, other: "DataArray") -> bool:
2579         """Like equals, but also checks the array name and attributes, and
2580         attributes on all coordinates.
2581 
2582         See Also
2583         --------
2584         DataArray.broadcast_equals
2585         DataArray.equal
2586         """
2587         try:
2588             return self.name == other.name and self._all_compat(other, "identical")
2589         except (TypeError, AttributeError):
2590             return False
2591 
2592     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2593         # use the same naming heuristics as pandas:
2594         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2595         other_name = getattr(other, "name", _default)
2596         if other_name is _default or other_name == self.name:
2597             return self.name
2598         else:
2599             return None
2600 
2601     def __array_wrap__(self, obj, context=None) -> "DataArray":
2602         new_var = self.variable.__array_wrap__(obj, context)
2603         return self._replace(new_var)
2604 
2605     def __matmul__(self, obj):
2606         return self.dot(obj)
2607 
2608     def __rmatmul__(self, other):
2609         # currently somewhat duplicative, as only other DataArrays are
2610         # compatible with matmul
2611         return computation.dot(other, self)
2612 
2613     @staticmethod
2614     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2615         @functools.wraps(f)
2616         def func(self, *args, **kwargs):
2617             with np.errstate(all="ignore"):
2618                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2619 
2620         return func
2621 
2622     @staticmethod
2623     def _binary_op(
2624         f: Callable[..., Any],
2625         reflexive: bool = False,
2626         join: str = None,  # see xarray.align
2627         **ignored_kwargs,
2628     ) -> Callable[..., "DataArray"]:
2629         @functools.wraps(f)
2630         def func(self, other):
2631             if isinstance(other, (Dataset, groupby.GroupBy)):
2632                 return NotImplemented
2633             if isinstance(other, DataArray):
2634                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2635                 self, other = align(self, other, join=align_type, copy=False)
2636             other_variable = getattr(other, "variable", other)
2637             other_coords = getattr(other, "coords", None)
2638 
2639             variable = (
2640                 f(self.variable, other_variable)
2641                 if not reflexive
2642                 else f(other_variable, self.variable)
2643             )
2644             coords, indexes = self.coords._merge_raw(other_coords)
2645             name = self._result_name(other)
2646 
2647             return self._replace(variable, coords, name, indexes=indexes)
2648 
2649         return func
2650 
2651     @staticmethod
2652     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2653         @functools.wraps(f)
2654         def func(self, other):
2655             if isinstance(other, groupby.GroupBy):
2656                 raise TypeError(
2657                     "in-place operations between a DataArray and "
2658                     "a grouped object are not permitted"
2659                 )
2660             # n.b. we can't align other to self (with other.reindex_like(self))
2661             # because `other` may be converted into floats, which would cause
2662             # in-place arithmetic to fail unpredictably. Instead, we simply
2663             # don't support automatic alignment with in-place arithmetic.
2664             other_coords = getattr(other, "coords", None)
2665             other_variable = getattr(other, "variable", other)
2666             with self.coords._merge_inplace(other_coords):
2667                 f(self.variable, other_variable)
2668             return self
2669 
2670         return func
2671 
2672     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2673         self.attrs = other.attrs
2674 
2675     @property
2676     def plot(self) -> _PlotMethods:
2677         """
2678         Access plotting functions for DataArray's
2679 
2680         >>> d = xr.DataArray([[1, 2], [3, 4]])
2681 
2682         For convenience just call this directly
2683 
2684         >>> d.plot()
2685 
2686         Or use it as a namespace to use xarray.plot functions as
2687         DataArray methods
2688 
2689         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2690 
2691         """
2692         return _PlotMethods(self)
2693 
2694     def _title_for_slice(self, truncate: int = 50) -> str:
2695         """
2696         If the dataarray has 1 dimensional coordinates or comes from a slice
2697         we can show that info in the title
2698 
2699         Parameters
2700         ----------
2701         truncate : integer
2702             maximum number of characters for title
2703 
2704         Returns
2705         -------
2706         title : string
2707             Can be used for plot titles
2708 
2709         """
2710         one_dims = []
2711         for dim, coord in self.coords.items():
2712             if coord.size == 1:
2713                 one_dims.append(
2714                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2715                 )
2716 
2717         title = ", ".join(one_dims)
2718         if len(title) > truncate:
2719             title = title[: (truncate - 3)] + "..."
2720 
2721         return title
2722 
2723     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2724         """Calculate the n-th order discrete difference along given axis.
2725 
2726         Parameters
2727         ----------
2728         dim : hashable
2729             Dimension over which to calculate the finite difference.
2730         n : int, optional
2731             The number of times values are differenced.
2732         label : hashable, optional
2733             The new coordinate in dimension ``dim`` will have the
2734             values of either the minuend's or subtrahend's coordinate
2735             for values 'upper' and 'lower', respectively.  Other
2736             values are not supported.
2737 
2738         Returns
2739         -------
2740         difference : same type as caller
2741             The n-th order finite difference of this object.
2742 
2743         .. note::
2744 
2745             `n` matches numpy's behavior and is different from pandas' first
2746             argument named `periods`.
2747 
2748 
2749         Examples
2750         --------
2751         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
2752         >>> arr.diff("x")
2753         <xarray.DataArray (x: 3)>
2754         array([0, 1, 0])
2755         Coordinates:
2756         * x        (x) int64 2 3 4
2757         >>> arr.diff("x", 2)
2758         <xarray.DataArray (x: 2)>
2759         array([ 1, -1])
2760         Coordinates:
2761         * x        (x) int64 3 4
2762 
2763         See Also
2764         --------
2765         DataArray.differentiate
2766         """
2767         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2768         return self._from_temp_dataset(ds)
2769 
2770     def shift(
2771         self,
2772         shifts: Mapping[Hashable, int] = None,
2773         fill_value: Any = dtypes.NA,
2774         **shifts_kwargs: int,
2775     ) -> "DataArray":
2776         """Shift this array by an offset along one or more dimensions.
2777 
2778         Only the data is moved; coordinates stay in place. Values shifted from
2779         beyond array bounds are replaced by NaN. This is consistent with the
2780         behavior of ``shift`` in pandas.
2781 
2782         Parameters
2783         ----------
2784         shifts : Mapping with the form of {dim: offset}
2785             Integer offset to shift along each of the given dimensions.
2786             Positive offsets shift to the right; negative offsets shift to the
2787             left.
2788         fill_value: scalar, optional
2789             Value to use for newly missing values
2790         **shifts_kwargs:
2791             The keyword arguments form of ``shifts``.
2792             One of shifts or shifts_kwargs must be provided.
2793 
2794         Returns
2795         -------
2796         shifted : DataArray
2797             DataArray with the same coordinates and attributes but shifted
2798             data.
2799 
2800         See also
2801         --------
2802         roll
2803 
2804         Examples
2805         --------
2806 
2807         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2808         >>> arr.shift(x=1)
2809         <xarray.DataArray (x: 3)>
2810         array([ nan,   5.,   6.])
2811         Coordinates:
2812           * x        (x) int64 0 1 2
2813         """
2814         variable = self.variable.shift(
2815             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2816         )
2817         return self._replace(variable=variable)
2818 
2819     def roll(
2820         self,
2821         shifts: Mapping[Hashable, int] = None,
2822         roll_coords: bool = None,
2823         **shifts_kwargs: int,
2824     ) -> "DataArray":
2825         """Roll this array by an offset along one or more dimensions.
2826 
2827         Unlike shift, roll may rotate all variables, including coordinates
2828         if specified. The direction of rotation is consistent with
2829         :py:func:`numpy.roll`.
2830 
2831         Parameters
2832         ----------
2833         shifts : Mapping with the form of {dim: offset}
2834             Integer offset to rotate each of the given dimensions.
2835             Positive offsets roll to the right; negative offsets roll to the
2836             left.
2837         roll_coords : bool
2838             Indicates whether to  roll the coordinates by the offset
2839             The current default of roll_coords (None, equivalent to True) is
2840             deprecated and will change to False in a future version.
2841             Explicitly pass roll_coords to silence the warning.
2842         **shifts_kwargs : The keyword arguments form of ``shifts``.
2843             One of shifts or shifts_kwargs must be provided.
2844 
2845         Returns
2846         -------
2847         rolled : DataArray
2848             DataArray with the same attributes but rolled data and coordinates.
2849 
2850         See also
2851         --------
2852         shift
2853 
2854         Examples
2855         --------
2856 
2857         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2858         >>> arr.roll(x=1)
2859         <xarray.DataArray (x: 3)>
2860         array([7, 5, 6])
2861         Coordinates:
2862           * x        (x) int64 2 0 1
2863         """
2864         ds = self._to_temp_dataset().roll(
2865             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2866         )
2867         return self._from_temp_dataset(ds)
2868 
2869     @property
2870     def real(self) -> "DataArray":
2871         return self._replace(self.variable.real)
2872 
2873     @property
2874     def imag(self) -> "DataArray":
2875         return self._replace(self.variable.imag)
2876 
2877     def dot(
2878         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2879     ) -> "DataArray":
2880         """Perform dot product of two DataArrays along their shared dims.
2881 
2882         Equivalent to taking taking tensordot over all shared dims.
2883 
2884         Parameters
2885         ----------
2886         other : DataArray
2887             The other array with which the dot product is performed.
2888         dims: '...', hashable or sequence of hashables, optional
2889             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
2890             If not specified, then all the common dimensions are summed over.
2891 
2892         Returns
2893         -------
2894         result : DataArray
2895             Array resulting from the dot product over all shared dimensions.
2896 
2897         See also
2898         --------
2899         dot
2900         numpy.tensordot
2901 
2902         Examples
2903         --------
2904 
2905         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2906         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
2907         >>> dm_vals = np.arange(4)
2908         >>> dm = xr.DataArray(dm_vals, dims=["z"])
2909 
2910         >>> dm.dims
2911         ('z')
2912 
2913         >>> da.dims
2914         ('x', 'y', 'z')
2915 
2916         >>> dot_result = da.dot(dm)
2917         >>> dot_result.dims
2918         ('x', 'y')
2919 
2920         """
2921         if isinstance(other, Dataset):
2922             raise NotImplementedError(
2923                 "dot products are not yet supported with Dataset objects."
2924             )
2925         if not isinstance(other, DataArray):
2926             raise TypeError("dot only operates on DataArrays.")
2927 
2928         return computation.dot(self, other, dims=dims)
2929 
2930     def sortby(
2931         self,
2932         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2933         ascending: bool = True,
2934     ) -> "DataArray":
2935         """Sort object by labels or values (along an axis).
2936 
2937         Sorts the dataarray, either along specified dimensions,
2938         or according to values of 1-D dataarrays that share dimension
2939         with calling object.
2940 
2941         If the input variables are dataarrays, then the dataarrays are aligned
2942         (via left-join) to the calling object prior to sorting by cell values.
2943         NaNs are sorted to the end, following Numpy convention.
2944 
2945         If multiple sorts along the same dimension is
2946         given, numpy's lexsort is performed along that dimension:
2947         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2948         and the FIRST key in the sequence is used as the primary sort key,
2949         followed by the 2nd key, etc.
2950 
2951         Parameters
2952         ----------
2953         variables: hashable, DataArray, or sequence of either
2954             1D DataArray objects or name(s) of 1D variable(s) in
2955             coords whose values are used to sort this array.
2956         ascending: boolean, optional
2957             Whether to sort by ascending or descending order.
2958 
2959         Returns
2960         -------
2961         sorted: DataArray
2962             A new dataarray where all the specified dims are sorted by dim
2963             labels.
2964 
2965         Examples
2966         --------
2967 
2968         >>> da = xr.DataArray(
2969         ...     np.random.rand(5),
2970         ...     coords=[pd.date_range("1/1/2000", periods=5)],
2971         ...     dims="time",
2972         ... )
2973         >>> da
2974         <xarray.DataArray (time: 5)>
2975         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2976         Coordinates:
2977           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2978 
2979         >>> da.sortby(da)
2980         <xarray.DataArray (time: 5)>
2981         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2982         Coordinates:
2983           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2984         """
2985         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2986         return self._from_temp_dataset(ds)
2987 
2988     def quantile(
2989         self,
2990         q: Any,
2991         dim: Union[Hashable, Sequence[Hashable], None] = None,
2992         interpolation: str = "linear",
2993         keep_attrs: bool = None,
2994         skipna: bool = True,
2995     ) -> "DataArray":
2996         """Compute the qth quantile of the data along the specified dimension.
2997 
2998         Returns the qth quantiles(s) of the array elements.
2999 
3000         Parameters
3001         ----------
3002         q : float in range of [0,1] or array-like of floats
3003             Quantile to compute, which must be between 0 and 1 inclusive.
3004         dim : hashable or sequence of hashable, optional
3005             Dimension(s) over which to apply quantile.
3006         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
3007             This optional parameter specifies the interpolation method to
3008             use when the desired quantile lies between two data points
3009             ``i < j``:
3010 
3011                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3012                   the fractional part of the index surrounded by ``i`` and
3013                   ``j``.
3014                 - lower: ``i``.
3015                 - higher: ``j``.
3016                 - nearest: ``i`` or ``j``, whichever is nearest.
3017                 - midpoint: ``(i + j) / 2``.
3018         keep_attrs : bool, optional
3019             If True, the dataset's attributes (`attrs`) will be copied from
3020             the original object to the new one.  If False (default), the new
3021             object will be returned without attributes.
3022         skipna : bool, optional
3023             Whether to skip missing values when aggregating.
3024 
3025         Returns
3026         -------
3027         quantiles : DataArray
3028             If `q` is a single quantile, then the result
3029             is a scalar. If multiple percentiles are given, first axis of
3030             the result corresponds to the quantile and a quantile dimension
3031             is added to the return array. The other dimensions are the
3032             dimensions that remain after the reduction of the array.
3033 
3034         See Also
3035         --------
3036         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3037 
3038         Examples
3039         --------
3040 
3041         >>> da = xr.DataArray(
3042         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3043         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3044         ...     dims=("x", "y"),
3045         ... )
3046         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3047         <xarray.DataArray ()>
3048         array(0.7)
3049         Coordinates:
3050             quantile  float64 0.0
3051         >>> da.quantile(0, dim="x")
3052         <xarray.DataArray (y: 4)>
3053         array([0.7, 4.2, 2.6, 1.5])
3054         Coordinates:
3055           * y         (y) float64 1.0 1.5 2.0 2.5
3056             quantile  float64 0.0
3057         >>> da.quantile([0, 0.5, 1])
3058         <xarray.DataArray (quantile: 3)>
3059         array([0.7, 3.4, 9.4])
3060         Coordinates:
3061           * quantile  (quantile) float64 0.0 0.5 1.0
3062         >>> da.quantile([0, 0.5, 1], dim="x")
3063         <xarray.DataArray (quantile: 3, y: 4)>
3064         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3065                [3.6 , 5.75, 6.  , 1.7 ],
3066                [6.5 , 7.3 , 9.4 , 1.9 ]])
3067         Coordinates:
3068           * y         (y) float64 1.0 1.5 2.0 2.5
3069           * quantile  (quantile) float64 0.0 0.5 1.0
3070         """
3071 
3072         ds = self._to_temp_dataset().quantile(
3073             q,
3074             dim=dim,
3075             keep_attrs=keep_attrs,
3076             interpolation=interpolation,
3077             skipna=skipna,
3078         )
3079         return self._from_temp_dataset(ds)
3080 
3081     def rank(
3082         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3083     ) -> "DataArray":
3084         """Ranks the data.
3085 
3086         Equal values are assigned a rank that is the average of the ranks that
3087         would have been otherwise assigned to all of the values within that
3088         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3089 
3090         NaNs in the input array are returned as NaNs.
3091 
3092         The `bottleneck` library is required.
3093 
3094         Parameters
3095         ----------
3096         dim : hashable
3097             Dimension over which to compute rank.
3098         pct : bool, optional
3099             If True, compute percentage ranks, otherwise compute integer ranks.
3100         keep_attrs : bool, optional
3101             If True, the dataset's attributes (`attrs`) will be copied from
3102             the original object to the new one.  If False (default), the new
3103             object will be returned without attributes.
3104 
3105         Returns
3106         -------
3107         ranked : DataArray
3108             DataArray with the same coordinates and dtype 'float64'.
3109 
3110         Examples
3111         --------
3112 
3113         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3114         >>> arr.rank("x")
3115         <xarray.DataArray (x: 3)>
3116         array([ 1.,   2.,   3.])
3117         Dimensions without coordinates: x
3118         """
3119 
3120         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3121         return self._from_temp_dataset(ds)
3122 
3123     def differentiate(
3124         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3125     ) -> "DataArray":
3126         """ Differentiate the array with the second order accurate central
3127         differences.
3128 
3129         .. note::
3130             This feature is limited to simple cartesian geometry, i.e. coord
3131             must be one dimensional.
3132 
3133         Parameters
3134         ----------
3135         coord: hashable
3136             The coordinate to be used to compute the gradient.
3137         edge_order: 1 or 2. Default 1
3138             N-th order accurate differences at the boundaries.
3139         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
3140             'us', 'ns', 'ps', 'fs', 'as'}
3141             Unit to compute gradient. Only valid for datetime coordinate.
3142 
3143         Returns
3144         -------
3145         differentiated: DataArray
3146 
3147         See also
3148         --------
3149         numpy.gradient: corresponding numpy function
3150 
3151         Examples
3152         --------
3153 
3154         >>> da = xr.DataArray(
3155         ...     np.arange(12).reshape(4, 3),
3156         ...     dims=["x", "y"],
3157         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3158         ... )
3159         >>> da
3160         <xarray.DataArray (x: 4, y: 3)>
3161         array([[ 0,  1,  2],
3162                [ 3,  4,  5],
3163                [ 6,  7,  8],
3164                [ 9, 10, 11]])
3165         Coordinates:
3166           * x        (x) float64 0.0 0.1 1.1 1.2
3167         Dimensions without coordinates: y
3168         >>>
3169         >>> da.differentiate("x")
3170         <xarray.DataArray (x: 4, y: 3)>
3171         array([[30.      , 30.      , 30.      ],
3172                [27.545455, 27.545455, 27.545455],
3173                [27.545455, 27.545455, 27.545455],
3174                [30.      , 30.      , 30.      ]])
3175         Coordinates:
3176           * x        (x) float64 0.0 0.1 1.1 1.2
3177         Dimensions without coordinates: y
3178         """
3179         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3180         return self._from_temp_dataset(ds)
3181 
3182     def integrate(
3183         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3184     ) -> "DataArray":
3185         """ integrate the array with the trapezoidal rule.
3186 
3187         .. note::
3188             This feature is limited to simple cartesian geometry, i.e. dim
3189             must be one dimensional.
3190 
3191         Parameters
3192         ----------
3193         dim: hashable, or a sequence of hashable
3194             Coordinate(s) used for the integration.
3195         datetime_unit: str, optional
3196             Can be used to specify the unit if datetime coordinate is used.
3197             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',
3198             'fs', 'as'}
3199 
3200         Returns
3201         -------
3202         integrated: DataArray
3203 
3204         See also
3205         --------
3206         numpy.trapz: corresponding numpy function
3207 
3208         Examples
3209         --------
3210 
3211         >>> da = xr.DataArray(
3212         ...     np.arange(12).reshape(4, 3),
3213         ...     dims=["x", "y"],
3214         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3215         ... )
3216         >>> da
3217         <xarray.DataArray (x: 4, y: 3)>
3218         array([[ 0,  1,  2],
3219                [ 3,  4,  5],
3220                [ 6,  7,  8],
3221                [ 9, 10, 11]])
3222         Coordinates:
3223           * x        (x) float64 0.0 0.1 1.1 1.2
3224         Dimensions without coordinates: y
3225         >>>
3226         >>> da.integrate("x")
3227         <xarray.DataArray (y: 3)>
3228         array([5.4, 6.6, 7.8])
3229         Dimensions without coordinates: y
3230         """
3231         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3232         return self._from_temp_dataset(ds)
3233 
3234     def unify_chunks(self) -> "DataArray":
3235         """ Unify chunk size along all chunked dimensions of this DataArray.
3236 
3237         Returns
3238         -------
3239 
3240         DataArray with consistent chunk sizes for all dask-array variables
3241 
3242         See Also
3243         --------
3244 
3245         dask.array.core.unify_chunks
3246         """
3247         ds = self._to_temp_dataset().unify_chunks()
3248         return self._from_temp_dataset(ds)
3249 
3250     def map_blocks(
3251         self,
3252         func: "Callable[..., T_DSorDA]",
3253         args: Sequence[Any] = (),
3254         kwargs: Mapping[str, Any] = None,
3255     ) -> "T_DSorDA":
3256         """
3257         Apply a function to each chunk of this DataArray. This method is experimental
3258         and its signature may change.
3259 
3260         Parameters
3261         ----------
3262         func: callable
3263             User-provided function that accepts a DataArray as its first parameter. The
3264             function will receive a subset of this DataArray, corresponding to one chunk
3265             along each chunked dimension. ``func`` will be executed as
3266             ``func(obj_subset, *args, **kwargs)``.
3267 
3268             The function will be first run on mocked-up data, that looks like this array
3269             but has sizes 0, to determine properties of the returned object such as
3270             dtype, variable names, new dimensions and new indexes (if any).
3271 
3272             This function must return either a single DataArray or a single Dataset.
3273 
3274             This function cannot change size of existing dimensions, or add new chunked
3275             dimensions.
3276         args: Sequence
3277             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
3278             objects, if any, will not be split by chunks. Passing dask collections is
3279             not allowed.
3280         kwargs: Mapping
3281             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3282             split by chunks. Passing dask collections is not allowed.
3283 
3284         Returns
3285         -------
3286         A single DataArray or Dataset with dask backend, reassembled from the outputs of
3287         the function.
3288 
3289         Notes
3290         -----
3291         This method is designed for when one needs to manipulate a whole xarray object
3292         within each chunk. In the more common case where one can work on numpy arrays,
3293         it is recommended to use apply_ufunc.
3294 
3295         If none of the variables in this DataArray is backed by dask, calling this
3296         method is equivalent to calling ``func(self, *args, **kwargs)``.
3297 
3298         See Also
3299         --------
3300         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
3301         xarray.Dataset.map_blocks
3302         """
3303         from .parallel import map_blocks
3304 
3305         return map_blocks(func, self, args, kwargs)
3306 
3307     def polyfit(
3308         self,
3309         dim: Hashable,
3310         deg: int,
3311         skipna: bool = None,
3312         rcond: float = None,
3313         w: Union[Hashable, Any] = None,
3314         full: bool = False,
3315         cov: bool = False,
3316     ):
3317         """
3318         Least squares polynomial fit.
3319 
3320         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3321         invalid values when `skipna = True`.
3322 
3323         Parameters
3324         ----------
3325         dim : hashable
3326             Coordinate along which to fit the polynomials.
3327         deg : int
3328             Degree of the fitting polynomial.
3329         skipna : bool, optional
3330             If True, removes all invalid values before fitting each 1D slices of the array.
3331             Default is True if data is stored in a dask.array or if there is any
3332             invalid values, False otherwise.
3333         rcond : float, optional
3334             Relative condition number to the fit.
3335         w : Union[Hashable, Any], optional
3336             Weights to apply to the y-coordinate of the sample points.
3337             Can be an array-like object or the name of a coordinate in the dataset.
3338         full : bool, optional
3339             Whether to return the residuals, matrix rank and singular values in addition
3340             to the coefficients.
3341         cov : Union[bool, str], optional
3342             Whether to return to the covariance matrix in addition to the coefficients.
3343             The matrix is not scaled if `cov='unscaled'`.
3344 
3345         Returns
3346         -------
3347         polyfit_results : Dataset
3348             A single dataset which contains:
3349 
3350             polyfit_coefficients
3351                 The coefficients of the best fit.
3352             polyfit_residuals
3353                 The residuals of the least-square computation (only included if `full=True`)
3354             [dim]_matrix_rank
3355                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3356             [dim]_singular_value
3357                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3358             polyfit_covariance
3359                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3360 
3361         See also
3362         --------
3363         numpy.polyfit
3364         """
3365         return self._to_temp_dataset().polyfit(
3366             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3367         )
3368 
3369     def pad(
3370         self,
3371         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3372         mode: str = "constant",
3373         stat_length: Union[
3374             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3375         ] = None,
3376         constant_values: Union[
3377             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3378         ] = None,
3379         end_values: Union[
3380             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3381         ] = None,
3382         reflect_type: str = None,
3383         **pad_width_kwargs: Any,
3384     ) -> "DataArray":
3385         """Pad this array along one or more dimensions.
3386 
3387         .. warning::
3388             This function is experimental and its behaviour is likely to change
3389             especially regarding padding of dimension coordinates (or IndexVariables).
3390 
3391         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3392         coordinates will be padded with the same mode, otherwise coordinates
3393         are padded using the "constant" mode with fill_value dtypes.NA.
3394 
3395         Parameters
3396         ----------
3397         pad_width : Mapping with the form of {dim: (pad_before, pad_after)}
3398             Number of values padded along each dimension.
3399             {dim: pad} is a shortcut for pad_before = pad_after = pad
3400         mode : str
3401             One of the following string values (taken from numpy docs)
3402 
3403             'constant' (default)
3404                 Pads with a constant value.
3405             'edge'
3406                 Pads with the edge values of array.
3407             'linear_ramp'
3408                 Pads with the linear ramp between end_value and the
3409                 array edge value.
3410             'maximum'
3411                 Pads with the maximum value of all or part of the
3412                 vector along each axis.
3413             'mean'
3414                 Pads with the mean value of all or part of the
3415                 vector along each axis.
3416             'median'
3417                 Pads with the median value of all or part of the
3418                 vector along each axis.
3419             'minimum'
3420                 Pads with the minimum value of all or part of the
3421                 vector along each axis.
3422             'reflect'
3423                 Pads with the reflection of the vector mirrored on
3424                 the first and last values of the vector along each
3425                 axis.
3426             'symmetric'
3427                 Pads with the reflection of the vector mirrored
3428                 along the edge of the array.
3429             'wrap'
3430                 Pads with the wrap of the vector along the axis.
3431                 The first values are used to pad the end and the
3432                 end values are used to pad the beginning.
3433         stat_length : int, tuple or mapping of the form {dim: tuple}
3434             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3435             values at edge of each axis used to calculate the statistic value.
3436             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3437             statistic lengths along each dimension.
3438             ((before, after),) yields same before and after statistic lengths
3439             for each dimension.
3440             (stat_length,) or int is a shortcut for before = after = statistic
3441             length for all axes.
3442             Default is ``None``, to use the entire axis.
3443         constant_values : scalar, tuple or mapping of the form {dim: tuple}
3444             Used in 'constant'.  The values to set the padded values for each
3445             axis.
3446             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3447             pad constants along each dimension.
3448             ``((before, after),)`` yields same before and after constants for each
3449             dimension.
3450             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3451             all dimensions.
3452             Default is 0.
3453         end_values : scalar, tuple or mapping of the form {dim: tuple}
3454             Used in 'linear_ramp'.  The values used for the ending value of the
3455             linear_ramp and that will form the edge of the padded array.
3456             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3457             end values along each dimension.
3458             ``((before, after),)`` yields same before and after end values for each
3459             axis.
3460             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3461             all axes.
3462             Default is 0.
3463         reflect_type : {'even', 'odd'}, optional
3464             Used in 'reflect', and 'symmetric'.  The 'even' style is the
3465             default with an unaltered reflection around the edge value.  For
3466             the 'odd' style, the extended part of the array is created by
3467             subtracting the reflected values from two times the edge value.
3468         **pad_width_kwargs:
3469             The keyword arguments form of ``pad_width``.
3470             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3471 
3472         Returns
3473         -------
3474         padded : DataArray
3475             DataArray with the padded coordinates and data.
3476 
3477         See also
3478         --------
3479         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3480 
3481         Notes
3482         -----
3483         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
3484         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
3485         specify ``constant_values=np.nan``
3486 
3487         Examples
3488         --------
3489 
3490         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0,1,2])])
3491         >>> arr.pad(x=(1,2), constant_values=0)
3492         <xarray.DataArray (x: 6)>
3493         array([0, 5, 6, 7, 0, 0])
3494         Coordinates:
3495           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3496 
3497         >>> da = xr.DataArray([[0,1,2,3], [10,11,12,13]],
3498                               dims=["x", "y"],
3499                               coords={"x": [0,1], "y": [10, 20 ,30, 40], "z": ("x", [100, 200])}
3500             )
3501         >>> da.pad(x=1)
3502         <xarray.DataArray (x: 4, y: 4)>
3503         array([[nan, nan, nan, nan],
3504                [ 0.,  1.,  2.,  3.],
3505                [10., 11., 12., 13.],
3506                [nan, nan, nan, nan]])
3507         Coordinates:
3508           * x        (x) float64 nan 0.0 1.0 nan
3509           * y        (y) int64 10 20 30 40
3510             z        (x) float64 nan 100.0 200.0 nan
3511         >>> da.pad(x=1, constant_values=np.nan)
3512         <xarray.DataArray (x: 4, y: 4)>
3513         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,
3514                 -9223372036854775808],
3515                [                   0,                    1,                    2,
3516                                    3],
3517                [                  10,                   11,                   12,
3518                                   13],
3519                [-9223372036854775808, -9223372036854775808, -9223372036854775808,
3520                 -9223372036854775808]])
3521         Coordinates:
3522           * x        (x) float64 nan 0.0 1.0 nan
3523           * y        (y) int64 10 20 30 40
3524             z        (x) float64 nan 100.0 200.0 nan
3525         """
3526         ds = self._to_temp_dataset().pad(
3527             pad_width=pad_width,
3528             mode=mode,
3529             stat_length=stat_length,
3530             constant_values=constant_values,
3531             end_values=end_values,
3532             reflect_type=reflect_type,
3533             **pad_width_kwargs,
3534         )
3535         return self._from_temp_dataset(ds)
3536 
3537     def idxmin(
3538         self,
3539         dim: Hashable = None,
3540         skipna: bool = None,
3541         fill_value: Any = dtypes.NA,
3542         keep_attrs: bool = None,
3543     ) -> "DataArray":
3544         """Return the coordinate label of the minimum value along a dimension.
3545 
3546         Returns a new `DataArray` named after the dimension with the values of
3547         the coordinate labels along that dimension corresponding to minimum
3548         values along that dimension.
3549 
3550         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3551         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3552 
3553         Parameters
3554         ----------
3555         dim : str, optional
3556             Dimension over which to apply `idxmin`.  This is optional for 1D
3557             arrays, but required for arrays with 2 or more dimensions.
3558         skipna : bool or None, default None
3559             If True, skip missing values (as marked by NaN). By default, only
3560             skips missing values for ``float``, ``complex``, and ``object``
3561             dtypes; other dtypes either do not have a sentinel missing value
3562             (``int``) or ``skipna=True`` has not been implemented
3563             (``datetime64`` or ``timedelta64``).
3564         fill_value : Any, default NaN
3565             Value to be filled in case all of the values along a dimension are
3566             null.  By default this is NaN.  The fill value and result are
3567             automatically converted to a compatible dtype if possible.
3568             Ignored if ``skipna`` is False.
3569         keep_attrs : bool, default False
3570             If True, the attributes (``attrs``) will be copied from the
3571             original object to the new one.  If False (default), the new object
3572             will be returned without attributes.
3573 
3574         Returns
3575         -------
3576         reduced : DataArray
3577             New `DataArray` object with `idxmin` applied to its data and the
3578             indicated dimension removed.
3579 
3580         See also
3581         --------
3582         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3583 
3584         Examples
3585         --------
3586 
3587         >>> array = xr.DataArray([0, 2, 1, 0, -2], dims="x",
3588         ...                      coords={"x": ['a', 'b', 'c', 'd', 'e']})
3589         >>> array.min()
3590         <xarray.DataArray ()>
3591         array(-2)
3592         >>> array.argmin()
3593         <xarray.DataArray ()>
3594         array(4)
3595         >>> array.idxmin()
3596         <xarray.DataArray 'x' ()>
3597         array('e', dtype='<U1')
3598 
3599         >>> array = xr.DataArray([[2.0, 1.0, 2.0, 0.0, -2.0],
3600         ...                       [-4.0, np.NaN, 2.0, np.NaN, -2.0],
3601         ...                       [np.NaN, np.NaN, 1., np.NaN, np.NaN]],
3602         ...                      dims=["y", "x"],
3603         ...                      coords={"y": [-1, 0, 1],
3604         ...                              "x": np.arange(5.)**2}
3605         ...                      )
3606         >>> array.min(dim="x")
3607         <xarray.DataArray (y: 3)>
3608         array([-2., -4.,  1.])
3609         Coordinates:
3610           * y        (y) int64 -1 0 1
3611         >>> array.argmin(dim="x")
3612         <xarray.DataArray (y: 3)>
3613         array([4, 0, 2])
3614         Coordinates:
3615           * y        (y) int64 -1 0 1
3616         >>> array.idxmin(dim="x")
3617         <xarray.DataArray 'x' (y: 3)>
3618         array([16.,  0.,  4.])
3619         Coordinates:
3620           * y        (y) int64 -1 0 1
3621         """
3622         return computation._calc_idxminmax(
3623             array=self,
3624             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
3625             dim=dim,
3626             skipna=skipna,
3627             fill_value=fill_value,
3628             keep_attrs=keep_attrs,
3629         )
3630 
3631     def idxmax(
3632         self,
3633         dim: Hashable = None,
3634         skipna: bool = None,
3635         fill_value: Any = dtypes.NA,
3636         keep_attrs: bool = None,
3637     ) -> "DataArray":
3638         """Return the coordinate label of the maximum value along a dimension.
3639 
3640         Returns a new `DataArray` named after the dimension with the values of
3641         the coordinate labels along that dimension corresponding to maximum
3642         values along that dimension.
3643 
3644         In comparison to :py:meth:`~DataArray.argmax`, this returns the
3645         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
3646 
3647         Parameters
3648         ----------
3649         dim : str, optional
3650             Dimension over which to apply `idxmax`.  This is optional for 1D
3651             arrays, but required for arrays with 2 or more dimensions.
3652         skipna : bool or None, default None
3653             If True, skip missing values (as marked by NaN). By default, only
3654             skips missing values for ``float``, ``complex``, and ``object``
3655             dtypes; other dtypes either do not have a sentinel missing value
3656             (``int``) or ``skipna=True`` has not been implemented
3657             (``datetime64`` or ``timedelta64``).
3658         fill_value : Any, default NaN
3659             Value to be filled in case all of the values along a dimension are
3660             null.  By default this is NaN.  The fill value and result are
3661             automatically converted to a compatible dtype if possible.
3662             Ignored if ``skipna`` is False.
3663         keep_attrs : bool, default False
3664             If True, the attributes (``attrs``) will be copied from the
3665             original object to the new one.  If False (default), the new object
3666             will be returned without attributes.
3667 
3668         Returns
3669         -------
3670         reduced : DataArray
3671             New `DataArray` object with `idxmax` applied to its data and the
3672             indicated dimension removed.
3673 
3674         See also
3675         --------
3676         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
3677 
3678         Examples
3679         --------
3680 
3681         >>> array = xr.DataArray([0, 2, 1, 0, -2], dims="x",
3682         ...                      coords={"x": ['a', 'b', 'c', 'd', 'e']})
3683         >>> array.max()
3684         <xarray.DataArray ()>
3685         array(2)
3686         >>> array.argmax()
3687         <xarray.DataArray ()>
3688         array(1)
3689         >>> array.idxmax()
3690         <xarray.DataArray 'x' ()>
3691         array('b', dtype='<U1')
3692 
3693         >>> array = xr.DataArray([[2.0, 1.0, 2.0, 0.0, -2.0],
3694         ...                       [-4.0, np.NaN, 2.0, np.NaN, -2.0],
3695         ...                       [np.NaN, np.NaN, 1., np.NaN, np.NaN]],
3696         ...                      dims=["y", "x"],
3697         ...                      coords={"y": [-1, 0, 1],
3698         ...                              "x": np.arange(5.)**2}
3699         ...                      )
3700         >>> array.max(dim="x")
3701         <xarray.DataArray (y: 3)>
3702         array([2., 2., 1.])
3703         Coordinates:
3704           * y        (y) int64 -1 0 1
3705         >>> array.argmax(dim="x")
3706         <xarray.DataArray (y: 3)>
3707         array([0, 2, 2])
3708         Coordinates:
3709           * y        (y) int64 -1 0 1
3710         >>> array.idxmax(dim="x")
3711         <xarray.DataArray 'x' (y: 3)>
3712         array([0., 4., 4.])
3713         Coordinates:
3714           * y        (y) int64 -1 0 1
3715         """
3716         return computation._calc_idxminmax(
3717             array=self,
3718             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
3719             dim=dim,
3720             skipna=skipna,
3721             fill_value=fill_value,
3722             keep_attrs=keep_attrs,
3723         )
3724 
3725     # this needs to be at the end, or mypy will confuse with `str`
3726     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
3727     str = property(StringAccessor)

</code><intended_behavior>Since the direct search for `__iadd__` yielded no results, it's plausible to infer that handling for the in-place addition operation (`+=`) might not be explicitly defined or is handled through more general mechanisms that do not specifically account for alignment issues. It may be necessary to implement or modify the `__iadd__` method (or the mechanism that indirectly handles `+=`) to ensure that it includes a step for aligning arrays before attempting the in-place merge or addition. This step should align the coordinates of the two `DataArray` objects, similarly to how coordinates would be aligned in an out-of-place addition, thereby ensuring that `A += B` is functionally equivalent to `A = A + B` with respect to coordinate alignment and merging. This could involve utilizing existing alignment mechanisms or developing new logic tailored for in-place operations that ensures compatibility between the array coordinates before proceeding with the operation.</intended_behavior>]
2025-10-15 21:39:25.350 | INFO     | Search completed. Bug locations: [<file>xarray/core/merge.py</file>
<class>None</class>
<method>merge_coordinates_without_align</method>
<code>
303 def merge_coordinates_without_align(
304     objects: "List[Coordinates]",
305     prioritized: Mapping[Hashable, MergeElement] = None,
306     exclude_dims: AbstractSet = frozenset(),
307 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
308     """Merge variables/indexes from coordinates without automatic alignments.
309 
310     This function is used for merging coordinate from pre-existing xarray
311     objects.
312     """
313     collected = collect_from_coordinates(objects)
314 
315     if exclude_dims:
316         filtered: Dict[Hashable, List[MergeElement]] = {}
317         for name, elements in collected.items():
318             new_elements = [
319                 (variable, index)
320                 for variable, index in elements
321                 if exclude_dims.isdisjoint(variable.dims)
322             ]
323             if new_elements:
324                 filtered[name] = new_elements
325     else:
326         filtered = collected
327 
328     return merge_collected(filtered, prioritized)

</code><intended_behavior>This method should more gracefully handle coordinate merging for in-place operations like `+=` by possibly introducing a preliminary step that aligns the coordinates of the two `DataArray` objects before attempting to merge them. The method should ensure that in-place addition behaves similarly to out-of-place addition (`A + B`), which successfully combines the arrays regardless of the order of their coordinates. This may involve adjusting how priorities are determined or how conflicts are resolved when the dimensions are the same but the coordinate values or orders differ. The ultimate goal is to allow `A += B` to successfully complete by ensuring that conflicting indexes are resolved or aligned in such a way that mirrors the successful behavior of `A = A + B`.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
216 class DataArray(AbstractArray, DataWithCoords):
217     """N-dimensional array with labeled coordinates and dimensions.
218 
219     DataArray provides a wrapper around numpy ndarrays that uses labeled
220     dimensions and coordinates to support metadata aware operations. The API is
221     similar to that for the pandas Series or DataFrame, but DataArray objects
222     can have any number of dimensions, and their contents have fixed data
223     types.
224 
225     Additional features over raw numpy arrays:
226 
227     - Apply operations over dimensions by name: ``x.sum('time')``.
228     - Select or assign values by integer location (like numpy): ``x[:10]``
229       or by label (like pandas): ``x.loc['2014-01-01']`` or
230       ``x.sel(time='2014-01-01')``.
231     - Mathematical operations (e.g., ``x - y``) vectorize across multiple
232       dimensions (known in numpy as "broadcasting") based on dimension names,
233       regardless of their original order.
234     - Keep track of arbitrary metadata in the form of a Python dictionary:
235       ``x.attrs``
236     - Convert to a pandas Series: ``x.to_series()``.
237 
238     Getting items from or doing mathematical operations with a DataArray
239     always returns another DataArray.
240     """
241 
242     _cache: Dict[str, Any]
243     _coords: Dict[Any, Variable]
244     _indexes: Optional[Dict[Hashable, pd.Index]]
245     _name: Optional[Hashable]
246     _variable: Variable
247 
248     __slots__ = (
249         "_cache",
250         "_coords",
251         "_file_obj",
252         "_indexes",
253         "_name",
254         "_variable",
255         "__weakref__",
256     )
257 
258     _groupby_cls = groupby.DataArrayGroupBy
259     _rolling_cls = rolling.DataArrayRolling
260     _coarsen_cls = rolling.DataArrayCoarsen
261     _resample_cls = resample.DataArrayResample
262     _weighted_cls = weighted.DataArrayWeighted
263 
264     dt = property(CombinedDatetimelikeAccessor)
265 
266     def __init__(
267         self,
268         data: Any = dtypes.NA,
269         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
270         dims: Union[Hashable, Sequence[Hashable], None] = None,
271         name: Hashable = None,
272         attrs: Mapping = None,
273         # internal parameters
274         indexes: Dict[Hashable, pd.Index] = None,
275         fastpath: bool = False,
276     ):
277         """
278         Parameters
279         ----------
280         data : array_like
281             Values for this array. Must be an ``numpy.ndarray``, ndarray like,
282             or castable to an ``ndarray``. If a self-described xarray or pandas
283             object, attempts are made to use this array's metadata to fill in
284             other unspecified arguments. A view of the array's data is used
285             instead of a copy if possible.
286         coords : sequence or dict of array_like objects, optional
287             Coordinates (tick labels) to use for indexing along each dimension.
288             The following notations are accepted:
289 
290             - mapping {dimension name: array-like}
291             - sequence of tuples that are valid arguments for xarray.Variable()
292               - (dims, data)
293               - (dims, data, attrs)
294               - (dims, data, attrs, encoding)
295 
296             Additionally, it is possible to define a coord whose name
297             does not match the dimension name, or a coord based on multiple
298             dimensions, with one of the following notations:
299 
300             - mapping {coord name: DataArray}
301             - mapping {coord name: Variable}
302             - mapping {coord name: (dimension name, array-like)}
303             - mapping {coord name: (tuple of dimension names, array-like)}
304 
305         dims : hashable or sequence of hashable, optional
306             Name(s) of the data dimension(s). Must be either a hashable (only
307             for 1D data) or a sequence of hashables with length equal to the
308             number of dimensions. If this argument is omitted, dimension names
309             default to ``['dim_0', ... 'dim_n']``.
310         name : str or None, optional
311             Name of this array.
312         attrs : dict_like or None, optional
313             Attributes to assign to the new instance. By default, an empty
314             attribute dictionary is initialized.
315         """
316         if fastpath:
317             variable = data
318             assert dims is None
319             assert attrs is None
320         else:
321             # try to fill in arguments from data if they weren't supplied
322             if coords is None:
323 
324                 if isinstance(data, DataArray):
325                     coords = data.coords
326                 elif isinstance(data, pd.Series):
327                     coords = [data.index]
328                 elif isinstance(data, pd.DataFrame):
329                     coords = [data.index, data.columns]
330                 elif isinstance(data, (pd.Index, IndexVariable)):
331                     coords = [data]
332                 elif isinstance(data, pdcompat.Panel):
333                     coords = [data.items, data.major_axis, data.minor_axis]
334 
335             if dims is None:
336                 dims = getattr(data, "dims", getattr(coords, "dims", None))
337             if name is None:
338                 name = getattr(data, "name", None)
339             if attrs is None and not isinstance(data, PANDAS_TYPES):
340                 attrs = getattr(data, "attrs", None)
341 
342             data = _check_data_shape(data, coords, dims)
343             data = as_compatible_data(data)
344             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
345             variable = Variable(dims, data, attrs, fastpath=True)
346             indexes = dict(
347                 _extract_indexes_from_coords(coords)
348             )  # needed for to_dataset
349 
350         # These fully describe a DataArray
351         self._variable = variable
352         assert isinstance(coords, dict)
353         self._coords = coords
354         self._name = name
355 
356         # TODO(shoyer): document this argument, once it becomes part of the
357         # public interface.
358         self._indexes = indexes
359 
360         self._file_obj = None
361 
362     def _replace(
363         self,
364         variable: Variable = None,
365         coords=None,
366         name: Union[Hashable, None, Default] = _default,
367         indexes=None,
368     ) -> "DataArray":
369         if variable is None:
370             variable = self.variable
371         if coords is None:
372             coords = self._coords
373         if name is _default:
374             name = self.name
375         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
376 
377     def _replace_maybe_drop_dims(
378         self, variable: Variable, name: Union[Hashable, None, Default] = _default
379     ) -> "DataArray":
380         if variable.dims == self.dims and variable.shape == self.shape:
381             coords = self._coords.copy()
382             indexes = self._indexes
383         elif variable.dims == self.dims:
384             # Shape has changed (e.g. from reduce(..., keepdims=True)
385             new_sizes = dict(zip(self.dims, variable.shape))
386             coords = {
387                 k: v
388                 for k, v in self._coords.items()
389                 if v.shape == tuple(new_sizes[d] for d in v.dims)
390             }
391             changed_dims = [
392                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
393             ]
394             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
395         else:
396             allowed_dims = set(variable.dims)
397             coords = {
398                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
399             }
400             indexes = propagate_indexes(
401                 self._indexes, exclude=(set(self.dims) - allowed_dims)
402             )
403         return self._replace(variable, coords, name, indexes=indexes)
404 
405     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
406         if not len(indexes):
407             return self
408         coords = self._coords.copy()
409         for name, idx in indexes.items():
410             coords[name] = IndexVariable(name, idx)
411         obj = self._replace(coords=coords)
412 
413         # switch from dimension to level names, if necessary
414         dim_names: Dict[Any, str] = {}
415         for dim, idx in indexes.items():
416             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
417                 dim_names[dim] = idx.name
418         if dim_names:
419             obj = obj.rename(dim_names)
420         return obj
421 
422     def _to_temp_dataset(self) -> Dataset:
423         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
424 
425     def _from_temp_dataset(
426         self, dataset: Dataset, name: Hashable = _default
427     ) -> "DataArray":
428         variable = dataset._variables.pop(_THIS_ARRAY)
429         coords = dataset._variables
430         indexes = dataset._indexes
431         return self._replace(variable, coords, name, indexes=indexes)
432 
433     def _to_dataset_split(self, dim: Hashable) -> Dataset:
434         """ splits dataarray along dimension 'dim' """
435 
436         def subset(dim, label):
437             array = self.loc[{dim: label}]
438             array.attrs = {}
439             return as_variable(array)
440 
441         variables = {label: subset(dim, label) for label in self.get_index(dim)}
442         variables.update({k: v for k, v in self._coords.items() if k != dim})
443         indexes = propagate_indexes(self._indexes, exclude=dim)
444         coord_names = set(self._coords) - set([dim])
445         dataset = Dataset._construct_direct(
446             variables, coord_names, indexes=indexes, attrs=self.attrs
447         )
448         return dataset
449 
450     def _to_dataset_whole(
451         self, name: Hashable = None, shallow_copy: bool = True
452     ) -> Dataset:
453         if name is None:
454             name = self.name
455         if name is None:
456             raise ValueError(
457                 "unable to convert unnamed DataArray to a "
458                 "Dataset without providing an explicit name"
459             )
460         if name in self.coords:
461             raise ValueError(
462                 "cannot create a Dataset from a DataArray with "
463                 "the same name as one of its coordinates"
464             )
465         # use private APIs for speed: this is called by _to_temp_dataset(),
466         # which is used in the guts of a lot of operations (e.g., reindex)
467         variables = self._coords.copy()
468         variables[name] = self.variable
469         if shallow_copy:
470             for k in variables:
471                 variables[k] = variables[k].copy(deep=False)
472         indexes = self._indexes
473 
474         coord_names = set(self._coords)
475         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
476         return dataset
477 
478     def to_dataset(
479         self,
480         dim: Hashable = None,
481         *,
482         name: Hashable = None,
483         promote_attrs: bool = False,
484     ) -> Dataset:
485         """Convert a DataArray to a Dataset.
486 
487         Parameters
488         ----------
489         dim : hashable, optional
490             Name of the dimension on this array along which to split this array
491             into separate variables. If not provided, this array is converted
492             into a Dataset of one variable.
493         name : hashable, optional
494             Name to substitute for this array's name. Only valid if ``dim`` is
495             not provided.
496         promote_attrs : bool, default False
497             Set to True to shallow copy attrs of DataArray to returned Dataset.
498 
499         Returns
500         -------
501         dataset : Dataset
502         """
503         if dim is not None and dim not in self.dims:
504             raise TypeError(
505                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
506             )
507 
508         if dim is not None:
509             if name is not None:
510                 raise TypeError("cannot supply both dim and name arguments")
511             result = self._to_dataset_split(dim)
512         else:
513             result = self._to_dataset_whole(name)
514 
515         if promote_attrs:
516             result.attrs = dict(self.attrs)
517 
518         return result
519 
520     @property
521     def name(self) -> Optional[Hashable]:
522         """The name of this array.
523         """
524         return self._name
525 
526     @name.setter
527     def name(self, value: Optional[Hashable]) -> None:
528         self._name = value
529 
530     @property
531     def variable(self) -> Variable:
532         """Low level interface to the Variable object for this DataArray."""
533         return self._variable
534 
535     @property
536     def dtype(self) -> np.dtype:
537         return self.variable.dtype
538 
539     @property
540     def shape(self) -> Tuple[int, ...]:
541         return self.variable.shape
542 
543     @property
544     def size(self) -> int:
545         return self.variable.size
546 
547     @property
548     def nbytes(self) -> int:
549         return self.variable.nbytes
550 
551     @property
552     def ndim(self) -> int:
553         return self.variable.ndim
554 
555     def __len__(self) -> int:
556         return len(self.variable)
557 
558     @property
559     def data(self) -> Any:
560         """The array's data as a dask or numpy array
561         """
562         return self.variable.data
563 
564     @data.setter
565     def data(self, value: Any) -> None:
566         self.variable.data = value
567 
568     @property
569     def values(self) -> np.ndarray:
570         """The array's data as a numpy.ndarray"""
571         return self.variable.values
572 
573     @values.setter
574     def values(self, value: Any) -> None:
575         self.variable.values = value
576 
577     @property
578     def _in_memory(self) -> bool:
579         return self.variable._in_memory
580 
581     def to_index(self) -> pd.Index:
582         """Convert this variable to a pandas.Index. Only possible for 1D
583         arrays.
584         """
585         return self.variable.to_index()
586 
587     @property
588     def dims(self) -> Tuple[Hashable, ...]:
589         """Tuple of dimension names associated with this array.
590 
591         Note that the type of this property is inconsistent with
592         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
593         consistently named properties.
594         """
595         return self.variable.dims
596 
597     @dims.setter
598     def dims(self, value):
599         raise AttributeError(
600             "you cannot assign dims on a DataArray. Use "
601             ".rename() or .swap_dims() instead."
602         )
603 
604     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
605         if utils.is_dict_like(key):
606             return key
607         else:
608             key = indexing.expanded_indexer(key, self.ndim)
609             return dict(zip(self.dims, key))
610 
611     @property
612     def _level_coords(self) -> Dict[Hashable, Hashable]:
613         """Return a mapping of all MultiIndex levels and their corresponding
614         coordinate name.
615         """
616         level_coords: Dict[Hashable, Hashable] = {}
617 
618         for cname, var in self._coords.items():
619             if var.ndim == 1 and isinstance(var, IndexVariable):
620                 level_names = var.level_names
621                 if level_names is not None:
622                     (dim,) = var.dims
623                     level_coords.update({lname: dim for lname in level_names})
624         return level_coords
625 
626     def _getitem_coord(self, key):
627         from .dataset import _get_virtual_variable
628 
629         try:
630             var = self._coords[key]
631         except KeyError:
632             dim_sizes = dict(zip(self.dims, self.shape))
633             _, key, var = _get_virtual_variable(
634                 self._coords, key, self._level_coords, dim_sizes
635             )
636 
637         return self._replace_maybe_drop_dims(var, name=key)
638 
639     def __getitem__(self, key: Any) -> "DataArray":
640         if isinstance(key, str):
641             return self._getitem_coord(key)
642         else:
643             # xarray-style array indexing
644             return self.isel(indexers=self._item_key_to_dict(key))
645 
646     def __setitem__(self, key: Any, value: Any) -> None:
647         if isinstance(key, str):
648             self.coords[key] = value
649         else:
650             # Coordinates in key, value and self[key] should be consistent.
651             # TODO Coordinate consistency in key is checked here, but it
652             # causes unnecessary indexing. It should be optimized.
653             obj = self[key]
654             if isinstance(value, DataArray):
655                 assert_coordinate_consistent(value, obj.coords.variables)
656             # DataArray key -> Variable key
657             key = {
658                 k: v.variable if isinstance(v, DataArray) else v
659                 for k, v in self._item_key_to_dict(key).items()
660             }
661             self.variable[key] = value
662 
663     def __delitem__(self, key: Any) -> None:
664         del self.coords[key]
665 
666     @property
667     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
668         """List of places to look-up items for attribute-style access
669         """
670         return self._item_sources + [self.attrs]
671 
672     @property
673     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
674         """List of places to look-up items for key-completion
675         """
676         return [
677             self.coords,
678             {d: self.coords[d] for d in self.dims},
679             LevelCoordinatesSource(self),
680         ]
681 
682     def __contains__(self, key: Any) -> bool:
683         return key in self.data
684 
685     @property
686     def loc(self) -> _LocIndexer:
687         """Attribute for location based indexing like pandas.
688         """
689         return _LocIndexer(self)
690 
691     @property
692     def attrs(self) -> Dict[Hashable, Any]:
693         """Dictionary storing arbitrary metadata with this array."""
694         return self.variable.attrs
695 
696     @attrs.setter
697     def attrs(self, value: Mapping[Hashable, Any]) -> None:
698         # Disable type checking to work around mypy bug - see mypy#4167
699         self.variable.attrs = value  # type: ignore
700 
701     @property
702     def encoding(self) -> Dict[Hashable, Any]:
703         """Dictionary of format-specific settings for how this array should be
704         serialized."""
705         return self.variable.encoding
706 
707     @encoding.setter
708     def encoding(self, value: Mapping[Hashable, Any]) -> None:
709         self.variable.encoding = value
710 
711     @property
712     def indexes(self) -> Indexes:
713         """Mapping of pandas.Index objects used for label based indexing
714         """
715         if self._indexes is None:
716             self._indexes = default_indexes(self._coords, self.dims)
717         return Indexes(self._indexes)
718 
719     @property
720     def coords(self) -> DataArrayCoordinates:
721         """Dictionary-like container of coordinate arrays.
722         """
723         return DataArrayCoordinates(self)
724 
725     def reset_coords(
726         self,
727         names: Union[Iterable[Hashable], Hashable, None] = None,
728         drop: bool = False,
729         inplace: bool = None,
730     ) -> Union[None, "DataArray", Dataset]:
731         """Given names of coordinates, reset them to become variables.
732 
733         Parameters
734         ----------
735         names : hashable or iterable of hashables, optional
736             Name(s) of non-index coordinates in this dataset to reset into
737             variables. By default, all non-index coordinates are reset.
738         drop : bool, optional
739             If True, remove coordinates instead of converting them into
740             variables.
741 
742         Returns
743         -------
744         Dataset, or DataArray if ``drop == True``
745         """
746         _check_inplace(inplace)
747         if names is None:
748             names = set(self.coords) - set(self.dims)
749         dataset = self.coords.to_dataset().reset_coords(names, drop)
750         if drop:
751             return self._replace(coords=dataset._variables)
752         else:
753             if self.name is None:
754                 raise ValueError(
755                     "cannot reset_coords with drop=False on an unnamed DataArrray"
756                 )
757             dataset[self.name] = self.variable
758             return dataset
759 
760     def __dask_tokenize__(self):
761         from dask.base import normalize_token
762 
763         return normalize_token((type(self), self._variable, self._coords, self._name))
764 
765     def __dask_graph__(self):
766         return self._to_temp_dataset().__dask_graph__()
767 
768     def __dask_keys__(self):
769         return self._to_temp_dataset().__dask_keys__()
770 
771     def __dask_layers__(self):
772         return self._to_temp_dataset().__dask_layers__()
773 
774     @property
775     def __dask_optimize__(self):
776         return self._to_temp_dataset().__dask_optimize__
777 
778     @property
779     def __dask_scheduler__(self):
780         return self._to_temp_dataset().__dask_scheduler__
781 
782     def __dask_postcompute__(self):
783         func, args = self._to_temp_dataset().__dask_postcompute__()
784         return self._dask_finalize, (func, args, self.name)
785 
786     def __dask_postpersist__(self):
787         func, args = self._to_temp_dataset().__dask_postpersist__()
788         return self._dask_finalize, (func, args, self.name)
789 
790     @staticmethod
791     def _dask_finalize(results, func, args, name):
792         ds = func(results, *args)
793         variable = ds._variables.pop(_THIS_ARRAY)
794         coords = ds._variables
795         return DataArray(variable, coords, name=name, fastpath=True)
796 
797     def load(self, **kwargs) -> "DataArray":
798         """Manually trigger loading of this array's data from disk or a
799         remote source into memory and return this array.
800 
801         Normally, it should not be necessary to call this method in user code,
802         because all xarray functions should either work on deferred data or
803         load data automatically. However, this method can be necessary when
804         working with many file objects on disk.
805 
806         Parameters
807         ----------
808         **kwargs : dict
809             Additional keyword arguments passed on to ``dask.array.compute``.
810 
811         See Also
812         --------
813         dask.array.compute
814         """
815         ds = self._to_temp_dataset().load(**kwargs)
816         new = self._from_temp_dataset(ds)
817         self._variable = new._variable
818         self._coords = new._coords
819         return self
820 
821     def compute(self, **kwargs) -> "DataArray":
822         """Manually trigger loading of this array's data from disk or a
823         remote source into memory and return a new array. The original is
824         left unaltered.
825 
826         Normally, it should not be necessary to call this method in user code,
827         because all xarray functions should either work on deferred data or
828         load data automatically. However, this method can be necessary when
829         working with many file objects on disk.
830 
831         Parameters
832         ----------
833         **kwargs : dict
834             Additional keyword arguments passed on to ``dask.array.compute``.
835 
836         See Also
837         --------
838         dask.array.compute
839         """
840         new = self.copy(deep=False)
841         return new.load(**kwargs)
842 
843     def persist(self, **kwargs) -> "DataArray":
844         """ Trigger computation in constituent dask arrays
845 
846         This keeps them as dask arrays but encourages them to keep data in
847         memory.  This is particularly useful when on a distributed machine.
848         When on a single machine consider using ``.compute()`` instead.
849 
850         Parameters
851         ----------
852         **kwargs : dict
853             Additional keyword arguments passed on to ``dask.persist``.
854 
855         See Also
856         --------
857         dask.persist
858         """
859         ds = self._to_temp_dataset().persist(**kwargs)
860         return self._from_temp_dataset(ds)
861 
862     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
863         """Returns a copy of this array.
864 
865         If `deep=True`, a deep copy is made of the data array.
866         Otherwise, a shallow copy is made, so each variable in the new
867         array's dataset is also a variable in this array's dataset.
868 
869         Use `data` to create a new object with the same structure as
870         original but entirely new data.
871 
872         Parameters
873         ----------
874         deep : bool, optional
875             Whether the data array and its coordinates are loaded into memory
876             and copied onto the new object. Default is True.
877         data : array_like, optional
878             Data to use in the new object. Must have same shape as original.
879             When `data` is used, `deep` is ignored for all data variables,
880             and only used for coords.
881 
882         Returns
883         -------
884         object : DataArray
885             New object with dimensions, attributes, coordinates, name,
886             encoding, and optionally data copied from original.
887 
888         Examples
889         --------
890 
891         Shallow versus deep copy
892 
893         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
894         >>> array.copy()
895         <xarray.DataArray (x: 3)>
896         array([1, 2, 3])
897         Coordinates:
898         * x        (x) <U1 'a' 'b' 'c'
899         >>> array_0 = array.copy(deep=False)
900         >>> array_0[0] = 7
901         >>> array_0
902         <xarray.DataArray (x: 3)>
903         array([7, 2, 3])
904         Coordinates:
905         * x        (x) <U1 'a' 'b' 'c'
906         >>> array
907         <xarray.DataArray (x: 3)>
908         array([7, 2, 3])
909         Coordinates:
910         * x        (x) <U1 'a' 'b' 'c'
911 
912         Changing the data using the ``data`` argument maintains the
913         structure of the original object, but with the new data. Original
914         object is unaffected.
915 
916         >>> array.copy(data=[0.1, 0.2, 0.3])
917         <xarray.DataArray (x: 3)>
918         array([ 0.1,  0.2,  0.3])
919         Coordinates:
920         * x        (x) <U1 'a' 'b' 'c'
921         >>> array
922         <xarray.DataArray (x: 3)>
923         array([1, 2, 3])
924         Coordinates:
925         * x        (x) <U1 'a' 'b' 'c'
926 
927         See Also
928         --------
929         pandas.DataFrame.copy
930         """
931         variable = self.variable.copy(deep=deep, data=data)
932         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
933         if self._indexes is None:
934             indexes = self._indexes
935         else:
936             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
937         return self._replace(variable, coords, indexes=indexes)
938 
939     def __copy__(self) -> "DataArray":
940         return self.copy(deep=False)
941 
942     def __deepcopy__(self, memo=None) -> "DataArray":
943         # memo does nothing but is required for compatibility with
944         # copy.deepcopy
945         return self.copy(deep=True)
946 
947     # mutable objects should not be hashable
948     # https://github.com/python/mypy/issues/4266
949     __hash__ = None  # type: ignore
950 
951     @property
952     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
953         """Block dimensions for this array's data or None if it's not a dask
954         array.
955         """
956         return self.variable.chunks
957 
958     def chunk(
959         self,
960         chunks: Union[
961             None,
962             Number,
963             Tuple[Number, ...],
964             Tuple[Tuple[Number, ...], ...],
965             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
966         ] = None,
967         name_prefix: str = "xarray-",
968         token: str = None,
969         lock: bool = False,
970     ) -> "DataArray":
971         """Coerce this array's data into a dask arrays with the given chunks.
972 
973         If this variable is a non-dask array, it will be converted to dask
974         array. If it's a dask array, it will be rechunked to the given chunk
975         sizes.
976 
977         If neither chunks is not provided for one or more dimensions, chunk
978         sizes along that dimension will not be updated; non-dask arrays will be
979         converted into dask arrays with a single block.
980 
981         Parameters
982         ----------
983         chunks : int, tuple or mapping, optional
984             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
985             ``{'x': 5, 'y': 5}``.
986         name_prefix : str, optional
987             Prefix for the name of the new dask array.
988         token : str, optional
989             Token uniquely identifying this array.
990         lock : optional
991             Passed on to :py:func:`dask.array.from_array`, if the array is not
992             already as dask array.
993 
994         Returns
995         -------
996         chunked : xarray.DataArray
997         """
998         if isinstance(chunks, (tuple, list)):
999             chunks = dict(zip(self.dims, chunks))
1000 
1001         ds = self._to_temp_dataset().chunk(
1002             chunks, name_prefix=name_prefix, token=token, lock=lock
1003         )
1004         return self._from_temp_dataset(ds)
1005 
1006     def isel(
1007         self,
1008         indexers: Mapping[Hashable, Any] = None,
1009         drop: bool = False,
1010         missing_dims: str = "raise",
1011         **indexers_kwargs: Any,
1012     ) -> "DataArray":
1013         """Return a new DataArray whose data is given by integer indexing
1014         along the specified dimension(s).
1015 
1016         Parameters
1017         ----------
1018         indexers : dict, optional
1019             A dict with keys matching dimensions and values given
1020             by integers, slice objects or arrays.
1021             indexer can be a integer, slice, array-like or DataArray.
1022             If DataArrays are passed as indexers, xarray-style indexing will be
1023             carried out. See :ref:`indexing` for the details.
1024             One of indexers or indexers_kwargs must be provided.
1025         drop : bool, optional
1026             If ``drop=True``, drop coordinates variables indexed by integers
1027             instead of making them scalar.
1028         missing_dims : {"raise", "warn", "ignore"}, default "raise"
1029             What to do if dimensions that should be selected from are not present in the
1030             DataArray:
1031             - "exception": raise an exception
1032             - "warning": raise a warning, and ignore the missing dimensions
1033             - "ignore": ignore the missing dimensions
1034         **indexers_kwargs : {dim: indexer, ...}, optional
1035             The keyword arguments form of ``indexers``.
1036 
1037         See Also
1038         --------
1039         Dataset.isel
1040         DataArray.sel
1041         """
1042 
1043         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1044 
1045         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1046             ds = self._to_temp_dataset()._isel_fancy(
1047                 indexers, drop=drop, missing_dims=missing_dims
1048             )
1049             return self._from_temp_dataset(ds)
1050 
1051         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1052         # lists, or zero or one-dimensional np.ndarray's
1053 
1054         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1055 
1056         coords = {}
1057         for coord_name, coord_value in self._coords.items():
1058             coord_indexers = {
1059                 k: v for k, v in indexers.items() if k in coord_value.dims
1060             }
1061             if coord_indexers:
1062                 coord_value = coord_value.isel(coord_indexers)
1063                 if drop and coord_value.ndim == 0:
1064                     continue
1065             coords[coord_name] = coord_value
1066 
1067         return self._replace(variable=variable, coords=coords)
1068 
1069     def sel(
1070         self,
1071         indexers: Mapping[Hashable, Any] = None,
1072         method: str = None,
1073         tolerance=None,
1074         drop: bool = False,
1075         **indexers_kwargs: Any,
1076     ) -> "DataArray":
1077         """Return a new DataArray whose data is given by selecting index
1078         labels along the specified dimension(s).
1079 
1080         .. warning::
1081 
1082           Do not try to assign values when using any of the indexing methods
1083           ``isel`` or ``sel``::
1084 
1085             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1086             # DO NOT do this
1087             da.isel(x=[0, 1, 2])[1] = -1
1088 
1089           Assigning values with the chained indexing using ``.sel`` or
1090           ``.isel`` fails silently.
1091 
1092         See Also
1093         --------
1094         Dataset.sel
1095         DataArray.isel
1096 
1097         """
1098         ds = self._to_temp_dataset().sel(
1099             indexers=indexers,
1100             drop=drop,
1101             method=method,
1102             tolerance=tolerance,
1103             **indexers_kwargs,
1104         )
1105         return self._from_temp_dataset(ds)
1106 
1107     def head(
1108         self,
1109         indexers: Union[Mapping[Hashable, int], int] = None,
1110         **indexers_kwargs: Any,
1111     ) -> "DataArray":
1112         """Return a new DataArray whose data is given by the the first `n`
1113         values along the specified dimension(s). Default `n` = 5
1114 
1115         See Also
1116         --------
1117         Dataset.head
1118         DataArray.tail
1119         DataArray.thin
1120         """
1121         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1122         return self._from_temp_dataset(ds)
1123 
1124     def tail(
1125         self,
1126         indexers: Union[Mapping[Hashable, int], int] = None,
1127         **indexers_kwargs: Any,
1128     ) -> "DataArray":
1129         """Return a new DataArray whose data is given by the the last `n`
1130         values along the specified dimension(s). Default `n` = 5
1131 
1132         See Also
1133         --------
1134         Dataset.tail
1135         DataArray.head
1136         DataArray.thin
1137         """
1138         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1139         return self._from_temp_dataset(ds)
1140 
1141     def thin(
1142         self,
1143         indexers: Union[Mapping[Hashable, int], int] = None,
1144         **indexers_kwargs: Any,
1145     ) -> "DataArray":
1146         """Return a new DataArray whose data is given by each `n` value
1147         along the specified dimension(s).
1148 
1149         See Also
1150         --------
1151         Dataset.thin
1152         DataArray.head
1153         DataArray.tail
1154         """
1155         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1156         return self._from_temp_dataset(ds)
1157 
1158     def broadcast_like(
1159         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1160     ) -> "DataArray":
1161         """Broadcast this DataArray against another Dataset or DataArray.
1162 
1163         This is equivalent to xr.broadcast(other, self)[1]
1164 
1165         xarray objects are broadcast against each other in arithmetic
1166         operations, so this method is not be necessary for most uses.
1167 
1168         If no change is needed, the input data is returned to the output
1169         without being copied.
1170 
1171         If new coords are added by the broadcast, their values are
1172         NaN filled.
1173 
1174         Parameters
1175         ----------
1176         other : Dataset or DataArray
1177             Object against which to broadcast this array.
1178         exclude : iterable of hashable, optional
1179             Dimensions that must not be broadcasted
1180 
1181         Returns
1182         -------
1183         new_da: xr.DataArray
1184 
1185         Examples
1186         --------
1187 
1188         >>> arr1
1189         <xarray.DataArray (x: 2, y: 3)>
1190         array([[0.840235, 0.215216, 0.77917 ],
1191                [0.726351, 0.543824, 0.875115]])
1192         Coordinates:
1193           * x        (x) <U1 'a' 'b'
1194           * y        (y) <U1 'a' 'b' 'c'
1195         >>> arr2
1196         <xarray.DataArray (x: 3, y: 2)>
1197         array([[0.612611, 0.125753],
1198                [0.853181, 0.948818],
1199                [0.180885, 0.33363 ]])
1200         Coordinates:
1201           * x        (x) <U1 'a' 'b' 'c'
1202           * y        (y) <U1 'a' 'b'
1203         >>> arr1.broadcast_like(arr2)
1204         <xarray.DataArray (x: 3, y: 3)>
1205         array([[0.840235, 0.215216, 0.77917 ],
1206                [0.726351, 0.543824, 0.875115],
1207                [     nan,      nan,      nan]])
1208         Coordinates:
1209           * x        (x) object 'a' 'b' 'c'
1210           * y        (y) object 'a' 'b' 'c'
1211         """
1212         if exclude is None:
1213             exclude = set()
1214         else:
1215             exclude = set(exclude)
1216         args = align(other, self, join="outer", copy=False, exclude=exclude)
1217 
1218         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1219 
1220         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1221 
1222     def reindex_like(
1223         self,
1224         other: Union["DataArray", Dataset],
1225         method: str = None,
1226         tolerance=None,
1227         copy: bool = True,
1228         fill_value=dtypes.NA,
1229     ) -> "DataArray":
1230         """Conform this object onto the indexes of another object, filling in
1231         missing values with ``fill_value``. The default fill value is NaN.
1232 
1233         Parameters
1234         ----------
1235         other : Dataset or DataArray
1236             Object with an 'indexes' attribute giving a mapping from dimension
1237             names to pandas.Index objects, which provides coordinates upon
1238             which to index the variables in this dataset. The indexes on this
1239             other object need not be the same as the indexes on this
1240             dataset. Any mis-matched index values will be filled in with
1241             NaN, and any mis-matched dimension names will simply be ignored.
1242         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1243             Method to use for filling index values from other not found on this
1244             data array:
1245 
1246             * None (default): don't fill gaps
1247             * pad / ffill: propagate last valid index value forward
1248             * backfill / bfill: propagate next valid index value backward
1249             * nearest: use nearest valid index value
1250         tolerance : optional
1251             Maximum distance between original and new labels for inexact
1252             matches. The values of the index at the matching locations must
1253             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1254         copy : bool, optional
1255             If ``copy=True``, data in the return value is always copied. If
1256             ``copy=False`` and reindexing is unnecessary, or can be performed
1257             with only slice operations, then the output may share memory with
1258             the input. In either case, a new xarray object is always returned.
1259         fill_value : scalar, optional
1260             Value to use for newly missing values
1261 
1262         Returns
1263         -------
1264         reindexed : DataArray
1265             Another dataset array, with this array's data but coordinates from
1266             the other object.
1267 
1268         See Also
1269         --------
1270         DataArray.reindex
1271         align
1272         """
1273         indexers = reindex_like_indexers(self, other)
1274         return self.reindex(
1275             indexers=indexers,
1276             method=method,
1277             tolerance=tolerance,
1278             copy=copy,
1279             fill_value=fill_value,
1280         )
1281 
1282     def reindex(
1283         self,
1284         indexers: Mapping[Hashable, Any] = None,
1285         method: str = None,
1286         tolerance=None,
1287         copy: bool = True,
1288         fill_value=dtypes.NA,
1289         **indexers_kwargs: Any,
1290     ) -> "DataArray":
1291         """Conform this object onto the indexes of another object, filling in
1292         missing values with ``fill_value``. The default fill value is NaN.
1293 
1294         Parameters
1295         ----------
1296         indexers : dict, optional
1297             Dictionary with keys given by dimension names and values given by
1298             arrays of coordinates tick labels. Any mis-matched coordinate
1299             values will be filled in with NaN, and any mis-matched dimension
1300             names will simply be ignored.
1301             One of indexers or indexers_kwargs must be provided.
1302         copy : bool, optional
1303             If ``copy=True``, data in the return value is always copied. If
1304             ``copy=False`` and reindexing is unnecessary, or can be performed
1305             with only slice operations, then the output may share memory with
1306             the input. In either case, a new xarray object is always returned.
1307         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1308             Method to use for filling index values in ``indexers`` not found on
1309             this data array:
1310 
1311             * None (default): don't fill gaps
1312             * pad / ffill: propagate last valid index value forward
1313             * backfill / bfill: propagate next valid index value backward
1314             * nearest: use nearest valid index value
1315         tolerance : optional
1316             Maximum distance between original and new labels for inexact
1317             matches. The values of the index at the matching locations must
1318             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1319         fill_value : scalar, optional
1320             Value to use for newly missing values
1321         **indexers_kwargs : {dim: indexer, ...}, optional
1322             The keyword arguments form of ``indexers``.
1323             One of indexers or indexers_kwargs must be provided.
1324 
1325         Returns
1326         -------
1327         reindexed : DataArray
1328             Another dataset array, with this array's data but replaced
1329             coordinates.
1330 
1331         See Also
1332         --------
1333         DataArray.reindex_like
1334         align
1335         """
1336         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1337         ds = self._to_temp_dataset().reindex(
1338             indexers=indexers,
1339             method=method,
1340             tolerance=tolerance,
1341             copy=copy,
1342             fill_value=fill_value,
1343         )
1344         return self._from_temp_dataset(ds)
1345 
1346     def interp(
1347         self,
1348         coords: Mapping[Hashable, Any] = None,
1349         method: str = "linear",
1350         assume_sorted: bool = False,
1351         kwargs: Mapping[str, Any] = None,
1352         **coords_kwargs: Any,
1353     ) -> "DataArray":
1354         """ Multidimensional interpolation of variables.
1355 
1356         coords : dict, optional
1357             Mapping from dimension names to the new coordinates.
1358             new coordinate can be an scalar, array-like or DataArray.
1359             If DataArrays are passed as new coordates, their dimensions are
1360             used for the broadcasting.
1361         method: {'linear', 'nearest'} for multidimensional array,
1362             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1363             for 1-dimensional array.
1364         assume_sorted: boolean, optional
1365             If False, values of x can be in any order and they are sorted
1366             first. If True, x has to be an array of monotonically increasing
1367             values.
1368         kwargs: dictionary
1369             Additional keyword passed to scipy's interpolator.
1370         ``**coords_kwargs`` : {dim: coordinate, ...}, optional
1371             The keyword arguments form of ``coords``.
1372             One of coords or coords_kwargs must be provided.
1373 
1374         Returns
1375         -------
1376         interpolated: xr.DataArray
1377             New dataarray on the new coordinates.
1378 
1379         Notes
1380         -----
1381         scipy is required.
1382 
1383         See Also
1384         --------
1385         scipy.interpolate.interp1d
1386         scipy.interpolate.interpn
1387 
1388         Examples
1389         --------
1390         >>> da = xr.DataArray([1, 3], [("x", np.arange(2))])
1391         >>> da.interp(x=0.5)
1392         <xarray.DataArray ()>
1393         array(2.0)
1394         Coordinates:
1395             x        float64 0.5
1396         """
1397         if self.dtype.kind not in "uifc":
1398             raise TypeError(
1399                 "interp only works for a numeric type array. "
1400                 "Given {}.".format(self.dtype)
1401             )
1402         ds = self._to_temp_dataset().interp(
1403             coords,
1404             method=method,
1405             kwargs=kwargs,
1406             assume_sorted=assume_sorted,
1407             **coords_kwargs,
1408         )
1409         return self._from_temp_dataset(ds)
1410 
1411     def interp_like(
1412         self,
1413         other: Union["DataArray", Dataset],
1414         method: str = "linear",
1415         assume_sorted: bool = False,
1416         kwargs: Mapping[str, Any] = None,
1417     ) -> "DataArray":
1418         """Interpolate this object onto the coordinates of another object,
1419         filling out of range values with NaN.
1420 
1421         Parameters
1422         ----------
1423         other : Dataset or DataArray
1424             Object with an 'indexes' attribute giving a mapping from dimension
1425             names to an 1d array-like, which provides coordinates upon
1426             which to index the variables in this dataset.
1427         method: string, optional.
1428             {'linear', 'nearest'} for multidimensional array,
1429             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
1430             for 1-dimensional array. 'linear' is used by default.
1431         assume_sorted: boolean, optional
1432             If False, values of coordinates that are interpolated over can be
1433             in any order and they are sorted first. If True, interpolated
1434             coordinates are assumed to be an array of monotonically increasing
1435             values.
1436         kwargs: dictionary, optional
1437             Additional keyword passed to scipy's interpolator.
1438 
1439         Returns
1440         -------
1441         interpolated: xr.DataArray
1442             Another dataarray by interpolating this dataarray's data along the
1443             coordinates of the other object.
1444 
1445         Notes
1446         -----
1447         scipy is required.
1448         If the dataarray has object-type coordinates, reindex is used for these
1449         coordinates instead of the interpolation.
1450 
1451         See Also
1452         --------
1453         DataArray.interp
1454         DataArray.reindex_like
1455         """
1456         if self.dtype.kind not in "uifc":
1457             raise TypeError(
1458                 "interp only works for a numeric type array. "
1459                 "Given {}.".format(self.dtype)
1460             )
1461         ds = self._to_temp_dataset().interp_like(
1462             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1463         )
1464         return self._from_temp_dataset(ds)
1465 
1466     def rename(
1467         self,
1468         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1469         **names: Hashable,
1470     ) -> "DataArray":
1471         """Returns a new DataArray with renamed coordinates or a new name.
1472 
1473         Parameters
1474         ----------
1475         new_name_or_name_dict : str or dict-like, optional
1476             If the argument is dict-like, it used as a mapping from old
1477             names to new names for coordinates. Otherwise, use the argument
1478             as the new name for this array.
1479         **names: hashable, optional
1480             The keyword arguments form of a mapping from old names to
1481             new names for coordinates.
1482             One of new_name_or_name_dict or names must be provided.
1483 
1484         Returns
1485         -------
1486         renamed : DataArray
1487             Renamed array or array with renamed coordinates.
1488 
1489         See Also
1490         --------
1491         Dataset.rename
1492         DataArray.swap_dims
1493         """
1494         if names or utils.is_dict_like(new_name_or_name_dict):
1495             new_name_or_name_dict = cast(
1496                 Mapping[Hashable, Hashable], new_name_or_name_dict
1497             )
1498             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1499             dataset = self._to_temp_dataset().rename(name_dict)
1500             return self._from_temp_dataset(dataset)
1501         else:
1502             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1503             return self._replace(name=new_name_or_name_dict)
1504 
1505     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "DataArray":
1506         """Returns a new DataArray with swapped dimensions.
1507 
1508         Parameters
1509         ----------
1510         dims_dict : dict-like
1511             Dictionary whose keys are current dimension names and whose values
1512             are new names.
1513 
1514         Returns
1515         -------
1516         swapped : DataArray
1517             DataArray with swapped dimensions.
1518 
1519         Examples
1520         --------
1521 
1522         >>> arr = xr.DataArray(
1523         ...     data=[0, 1], dims="x", coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1524         ... )
1525         >>> arr
1526         <xarray.DataArray (x: 2)>
1527         array([0, 1])
1528         Coordinates:
1529           * x        (x) <U1 'a' 'b'
1530             y        (x) int64 0 1
1531 
1532         >>> arr.swap_dims({"x": "y"})
1533         <xarray.DataArray (y: 2)>
1534         array([0, 1])
1535         Coordinates:
1536             x        (y) <U1 'a' 'b'
1537           * y        (y) int64 0 1
1538 
1539         >>> arr.swap_dims({"x": "z"})
1540         <xarray.DataArray (z: 2)>
1541         array([0, 1])
1542         Coordinates:
1543             x        (z) <U1 'a' 'b'
1544             y        (z) int64 0 1
1545         Dimensions without coordinates: z
1546 
1547         See Also
1548         --------
1549 
1550         DataArray.rename
1551         Dataset.swap_dims
1552         """
1553         ds = self._to_temp_dataset().swap_dims(dims_dict)
1554         return self._from_temp_dataset(ds)
1555 
1556     def expand_dims(
1557         self,
1558         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1559         axis=None,
1560         **dim_kwargs: Any,
1561     ) -> "DataArray":
1562         """Return a new object with an additional axis (or axes) inserted at
1563         the corresponding position in the array shape. The new object is a
1564         view into the underlying array, not a copy.
1565 
1566 
1567         If dim is already a scalar coordinate, it will be promoted to a 1D
1568         coordinate consisting of a single value.
1569 
1570         Parameters
1571         ----------
1572         dim : hashable, sequence of hashable, dict, or None
1573             Dimensions to include on the new variable.
1574             If provided as str or sequence of str, then dimensions are inserted
1575             with length 1. If provided as a dict, then the keys are the new
1576             dimensions and the values are either integers (giving the length of
1577             the new dimensions) or sequence/ndarray (giving the coordinates of
1578             the new dimensions).
1579         axis : integer, list (or tuple) of integers, or None
1580             Axis position(s) where new axis is to be inserted (position(s) on
1581             the result array). If a list (or tuple) of integers is passed,
1582             multiple axes are inserted. In this case, dim arguments should be
1583             same length list. If axis=None is passed, all the axes will be
1584             inserted to the start of the result array.
1585         **dim_kwargs : int or sequence/ndarray
1586             The keywords are arbitrary dimensions being inserted and the values
1587             are either the lengths of the new dims (if int is given), or their
1588             coordinates. Note, this is an alternative to passing a dict to the
1589             dim kwarg and will only be used if dim is None.
1590 
1591         Returns
1592         -------
1593         expanded : same type as caller
1594             This object, but with an additional dimension(s).
1595         """
1596         if isinstance(dim, int):
1597             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1598         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1599             if len(dim) != len(set(dim)):
1600                 raise ValueError("dims should not contain duplicate values.")
1601             dim = dict.fromkeys(dim, 1)
1602         elif dim is not None and not isinstance(dim, Mapping):
1603             dim = {cast(Hashable, dim): 1}
1604 
1605         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1606         ds = self._to_temp_dataset().expand_dims(dim, axis)
1607         return self._from_temp_dataset(ds)
1608 
1609     def set_index(
1610         self,
1611         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1612         append: bool = False,
1613         inplace: bool = None,
1614         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1615     ) -> Optional["DataArray"]:
1616         """Set DataArray (multi-)indexes using one or more existing
1617         coordinates.
1618 
1619         Parameters
1620         ----------
1621         indexes : {dim: index, ...}
1622             Mapping from names matching dimensions and values given
1623             by (lists of) the names of existing coordinates or variables to set
1624             as new (multi-)index.
1625         append : bool, optional
1626             If True, append the supplied index(es) to the existing index(es).
1627             Otherwise replace the existing index(es) (default).
1628         **indexes_kwargs: optional
1629             The keyword arguments form of ``indexes``.
1630             One of indexes or indexes_kwargs must be provided.
1631 
1632         Returns
1633         -------
1634         obj : DataArray
1635             Another DataArray, with this data but replaced coordinates.
1636 
1637         Examples
1638         --------
1639         >>> arr = xr.DataArray(
1640         ...     data=np.ones((2, 3)),
1641         ...     dims=["x", "y"],
1642         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1643         ... )
1644         >>> arr
1645         <xarray.DataArray (x: 2, y: 3)>
1646         array([[1., 1., 1.],
1647                [1., 1., 1.]])
1648         Coordinates:
1649           * x        (x) int64 0 1
1650           * y        (y) int64 0 1 2
1651             a        (x) int64 3 4
1652         >>> arr.set_index(x="a")
1653         <xarray.DataArray (x: 2, y: 3)>
1654         array([[1., 1., 1.],
1655                [1., 1., 1.]])
1656         Coordinates:
1657           * x        (x) int64 3 4
1658           * y        (y) int64 0 1 2
1659 
1660         See Also
1661         --------
1662         DataArray.reset_index
1663         """
1664         ds = self._to_temp_dataset().set_index(
1665             indexes, append=append, inplace=inplace, **indexes_kwargs
1666         )
1667         return self._from_temp_dataset(ds)
1668 
1669     def reset_index(
1670         self,
1671         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1672         drop: bool = False,
1673         inplace: bool = None,
1674     ) -> Optional["DataArray"]:
1675         """Reset the specified index(es) or multi-index level(s).
1676 
1677         Parameters
1678         ----------
1679         dims_or_levels : hashable or sequence of hashables
1680             Name(s) of the dimension(s) and/or multi-index level(s) that will
1681             be reset.
1682         drop : bool, optional
1683             If True, remove the specified indexes and/or multi-index levels
1684             instead of extracting them as new coordinates (default: False).
1685 
1686         Returns
1687         -------
1688         obj : DataArray
1689             Another dataarray, with this dataarray's data but replaced
1690             coordinates.
1691 
1692         See Also
1693         --------
1694         DataArray.set_index
1695         """
1696         _check_inplace(inplace)
1697         coords, _ = split_indexes(
1698             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1699         )
1700         return self._replace(coords=coords)
1701 
1702     def reorder_levels(
1703         self,
1704         dim_order: Mapping[Hashable, Sequence[int]] = None,
1705         inplace: bool = None,
1706         **dim_order_kwargs: Sequence[int],
1707     ) -> "DataArray":
1708         """Rearrange index levels using input order.
1709 
1710         Parameters
1711         ----------
1712         dim_order : optional
1713             Mapping from names matching dimensions and values given
1714             by lists representing new level orders. Every given dimension
1715             must have a multi-index.
1716         **dim_order_kwargs: optional
1717             The keyword arguments form of ``dim_order``.
1718             One of dim_order or dim_order_kwargs must be provided.
1719 
1720         Returns
1721         -------
1722         obj : DataArray
1723             Another dataarray, with this dataarray's data but replaced
1724             coordinates.
1725         """
1726         _check_inplace(inplace)
1727         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1728         replace_coords = {}
1729         for dim, order in dim_order.items():
1730             coord = self._coords[dim]
1731             index = coord.to_index()
1732             if not isinstance(index, pd.MultiIndex):
1733                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1734             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1735         coords = self._coords.copy()
1736         coords.update(replace_coords)
1737         return self._replace(coords=coords)
1738 
1739     def stack(
1740         self,
1741         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1742         **dimensions_kwargs: Sequence[Hashable],
1743     ) -> "DataArray":
1744         """
1745         Stack any number of existing dimensions into a single new dimension.
1746 
1747         New dimensions will be added at the end, and the corresponding
1748         coordinate variables will be combined into a MultiIndex.
1749 
1750         Parameters
1751         ----------
1752         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
1753             Names of new dimensions, and the existing dimensions that they
1754             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1755             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1756             all dimensions.
1757         **dimensions_kwargs:
1758             The keyword arguments form of ``dimensions``.
1759             One of dimensions or dimensions_kwargs must be provided.
1760 
1761         Returns
1762         -------
1763         stacked : DataArray
1764             DataArray with stacked data.
1765 
1766         Examples
1767         --------
1768 
1769         >>> arr = xr.DataArray(
1770         ...     np.arange(6).reshape(2, 3),
1771         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1772         ... )
1773         >>> arr
1774         <xarray.DataArray (x: 2, y: 3)>
1775         array([[0, 1, 2],
1776                [3, 4, 5]])
1777         Coordinates:
1778           * x        (x) |S1 'a' 'b'
1779           * y        (y) int64 0 1 2
1780         >>> stacked = arr.stack(z=("x", "y"))
1781         >>> stacked.indexes["z"]
1782         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1783                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1784                    names=['x', 'y'])
1785 
1786         See Also
1787         --------
1788         DataArray.unstack
1789         """
1790         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
1791         return self._from_temp_dataset(ds)
1792 
1793     def unstack(
1794         self,
1795         dim: Union[Hashable, Sequence[Hashable], None] = None,
1796         fill_value: Any = dtypes.NA,
1797         sparse: bool = False,
1798     ) -> "DataArray":
1799         """
1800         Unstack existing dimensions corresponding to MultiIndexes into
1801         multiple new dimensions.
1802 
1803         New dimensions will be added at the end.
1804 
1805         Parameters
1806         ----------
1807         dim : hashable or sequence of hashable, optional
1808             Dimension(s) over which to unstack. By default unstacks all
1809             MultiIndexes.
1810         fill_value: value to be filled. By default, np.nan
1811         sparse: use sparse-array if True
1812 
1813         Returns
1814         -------
1815         unstacked : DataArray
1816             Array with unstacked data.
1817 
1818         Examples
1819         --------
1820 
1821         >>> arr = xr.DataArray(
1822         ...     np.arange(6).reshape(2, 3),
1823         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1824         ... )
1825         >>> arr
1826         <xarray.DataArray (x: 2, y: 3)>
1827         array([[0, 1, 2],
1828                [3, 4, 5]])
1829         Coordinates:
1830           * x        (x) |S1 'a' 'b'
1831           * y        (y) int64 0 1 2
1832         >>> stacked = arr.stack(z=("x", "y"))
1833         >>> stacked.indexes["z"]
1834         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1835                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],
1836                    names=['x', 'y'])
1837         >>> roundtripped = stacked.unstack()
1838         >>> arr.identical(roundtripped)
1839         True
1840 
1841         See Also
1842         --------
1843         DataArray.stack
1844         """
1845         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
1846         return self._from_temp_dataset(ds)
1847 
1848     def to_unstacked_dataset(self, dim, level=0):
1849         """Unstack DataArray expanding to Dataset along a given level of a
1850         stacked coordinate.
1851 
1852         This is the inverse operation of Dataset.to_stacked_array.
1853 
1854         Parameters
1855         ----------
1856         dim : str
1857             Name of existing dimension to unstack
1858         level : int or str
1859             The MultiIndex level to expand to a dataset along. Can either be
1860             the integer index of the level or its name.
1861         label : int, default 0
1862             Label of the level to expand dataset along. Overrides the label
1863             argument if given.
1864 
1865         Returns
1866         -------
1867         unstacked: Dataset
1868 
1869         Examples
1870         --------
1871         >>> import xarray as xr
1872         >>> arr = xr.DataArray(
1873         ...     np.arange(6).reshape(2, 3),
1874         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
1875         ... )
1876         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
1877         >>> data
1878         <xarray.Dataset>
1879         Dimensions:  (x: 2, y: 3)
1880         Coordinates:
1881           * x        (x) <U1 'a' 'b'
1882           * y        (y) int64 0 1 2
1883         Data variables:
1884             a        (x, y) int64 0 1 2 3 4 5
1885             b        (x) int64 0 3
1886         >>> stacked = data.to_stacked_array("z", ["y"])
1887         >>> stacked.indexes["z"]
1888         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],
1889                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],
1890                 names=['variable', 'y'])
1891         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
1892         >>> data.identical(roundtripped)
1893         True
1894 
1895         See Also
1896         --------
1897         Dataset.to_stacked_array
1898         """
1899 
1900         idx = self.indexes[dim]
1901         if not isinstance(idx, pd.MultiIndex):
1902             raise ValueError(f"'{dim}' is not a stacked coordinate")
1903 
1904         level_number = idx._get_level_number(level)
1905         variables = idx.levels[level_number]
1906         variable_dim = idx.names[level_number]
1907 
1908         # pull variables out of datarray
1909         data_dict = {}
1910         for k in variables:
1911             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)
1912 
1913         # unstacked dataset
1914         return Dataset(data_dict)
1915 
1916     def transpose(self, *dims: Hashable, transpose_coords: bool = None) -> "DataArray":
1917         """Return a new DataArray object with transposed dimensions.
1918 
1919         Parameters
1920         ----------
1921         *dims : hashable, optional
1922             By default, reverse the dimensions. Otherwise, reorder the
1923             dimensions to this order.
1924         transpose_coords : boolean, optional
1925             If True, also transpose the coordinates of this DataArray.
1926 
1927         Returns
1928         -------
1929         transposed : DataArray
1930             The returned DataArray's array is transposed.
1931 
1932         Notes
1933         -----
1934         This operation returns a view of this array's data. It is
1935         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
1936         -- the data will be fully loaded.
1937 
1938         See Also
1939         --------
1940         numpy.transpose
1941         Dataset.transpose
1942         """
1943         if dims:
1944             dims = tuple(utils.infix_dims(dims, self.dims))
1945         variable = self.variable.transpose(*dims)
1946         if transpose_coords:
1947             coords: Dict[Hashable, Variable] = {}
1948             for name, coord in self.coords.items():
1949                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
1950                 coords[name] = coord.variable.transpose(*coord_dims)
1951             return self._replace(variable, coords)
1952         else:
1953             if transpose_coords is None and any(self[c].ndim > 1 for c in self.coords):
1954                 warnings.warn(
1955                     "This DataArray contains multi-dimensional "
1956                     "coordinates. In the future, these coordinates "
1957                     "will be transposed as well unless you specify "
1958                     "transpose_coords=False.",
1959                     FutureWarning,
1960                     stacklevel=2,
1961                 )
1962             return self._replace(variable)
1963 
1964     @property
1965     def T(self) -> "DataArray":
1966         return self.transpose()
1967 
1968     def drop_vars(
1969         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
1970     ) -> "DataArray":
1971         """Drop variables from this DataArray.
1972 
1973         Parameters
1974         ----------
1975         names : hashable or iterable of hashables
1976             Name(s) of variables to drop.
1977         errors: {'raise', 'ignore'}, optional
1978             If 'raise' (default), raises a ValueError error if any of the variable
1979             passed are not in the dataset. If 'ignore', any given names that are in the
1980             DataArray are dropped and no error is raised.
1981 
1982         Returns
1983         -------
1984         dropped : Dataset
1985 
1986         """
1987         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
1988         return self._from_temp_dataset(ds)
1989 
1990     def drop(
1991         self,
1992         labels: Mapping = None,
1993         dim: Hashable = None,
1994         *,
1995         errors: str = "raise",
1996         **labels_kwargs,
1997     ) -> "DataArray":
1998         """Backward compatible method based on `drop_vars` and `drop_sel`
1999 
2000         Using either `drop_vars` or `drop_sel` is encouraged
2001 
2002         See Also
2003         --------
2004         DataArray.drop_vars
2005         DataArray.drop_sel
2006         """
2007         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2008         return self._from_temp_dataset(ds)
2009 
2010     def drop_sel(
2011         self,
2012         labels: Mapping[Hashable, Any] = None,
2013         *,
2014         errors: str = "raise",
2015         **labels_kwargs,
2016     ) -> "DataArray":
2017         """Drop index labels from this DataArray.
2018 
2019         Parameters
2020         ----------
2021         labels : Mapping[Hashable, Any]
2022             Index labels to drop
2023         errors: {'raise', 'ignore'}, optional
2024             If 'raise' (default), raises a ValueError error if
2025             any of the index labels passed are not
2026             in the dataset. If 'ignore', any given labels that are in the
2027             dataset are dropped and no error is raised.
2028         **labels_kwargs : {dim: label, ...}, optional
2029             The keyword arguments form of ``dim`` and ``labels``
2030 
2031         Returns
2032         -------
2033         dropped : DataArray
2034         """
2035         if labels_kwargs or isinstance(labels, dict):
2036             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2037 
2038         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2039         return self._from_temp_dataset(ds)
2040 
2041     def dropna(
2042         self, dim: Hashable, how: str = "any", thresh: int = None
2043     ) -> "DataArray":
2044         """Returns a new array with dropped labels for missing values along
2045         the provided dimension.
2046 
2047         Parameters
2048         ----------
2049         dim : hashable
2050             Dimension along which to drop missing values. Dropping along
2051             multiple dimensions simultaneously is not yet supported.
2052         how : {'any', 'all'}, optional
2053             * any : if any NA values are present, drop that label
2054             * all : if all values are NA, drop that label
2055         thresh : int, default None
2056             If supplied, require this many non-NA values.
2057 
2058         Returns
2059         -------
2060         DataArray
2061         """
2062         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2063         return self._from_temp_dataset(ds)
2064 
2065     def fillna(self, value: Any) -> "DataArray":
2066         """Fill missing values in this object.
2067 
2068         This operation follows the normal broadcasting and alignment rules that
2069         xarray uses for binary arithmetic, except the result is aligned to this
2070         object (``join='left'``) instead of aligned to the intersection of
2071         index coordinates (``join='inner'``).
2072 
2073         Parameters
2074         ----------
2075         value : scalar, ndarray or DataArray
2076             Used to fill all matching missing values in this array. If the
2077             argument is a DataArray, it is first aligned with (reindexed to)
2078             this array.
2079 
2080         Returns
2081         -------
2082         DataArray
2083         """
2084         if utils.is_dict_like(value):
2085             raise TypeError(
2086                 "cannot provide fill value as a dictionary with "
2087                 "fillna on a DataArray"
2088             )
2089         out = ops.fillna(self, value)
2090         return out
2091 
2092     def interpolate_na(
2093         self,
2094         dim: Hashable = None,
2095         method: str = "linear",
2096         limit: int = None,
2097         use_coordinate: Union[bool, str] = True,
2098         max_gap: Union[
2099             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2100         ] = None,
2101         **kwargs: Any,
2102     ) -> "DataArray":
2103         """Fill in NaNs by interpolating according to different methods.
2104 
2105         Parameters
2106         ----------
2107         dim : str
2108             Specifies the dimension along which to interpolate.
2109         method : str, optional
2110             String indicating which method to use for interpolation:
2111 
2112             - 'linear': linear interpolation (Default). Additional keyword
2113               arguments are passed to :py:func:`numpy.interp`
2114             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2115               are passed to :py:func:`scipy.interpolate.interp1d`. If
2116               ``method='polynomial'``, the ``order`` keyword argument must also be
2117               provided.
2118             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2119               respective :py:class:`scipy.interpolate` classes.
2120 
2121         use_coordinate : bool, str, default True
2122             Specifies which index to use as the x values in the interpolation
2123             formulated as `y = f(x)`. If False, values are treated as if
2124             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2125             used. If ``use_coordinate`` is a string, it specifies the name of a
2126             coordinate variariable to use as the index.
2127         limit : int, default None
2128             Maximum number of consecutive NaNs to fill. Must be greater than 0
2129             or None for no limit. This filling is done regardless of the size of
2130             the gap in the data. To only interpolate over gaps less than a given length,
2131             see ``max_gap``.
2132         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
2133             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2134             Use None for no limit. When interpolating along a datetime64 dimension
2135             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2136 
2137             - a string that is valid input for pandas.to_timedelta
2138             - a :py:class:`numpy.timedelta64` object
2139             - a :py:class:`pandas.Timedelta` object
2140             - a :py:class:`datetime.timedelta` object
2141 
2142             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2143             dimensions has not been implemented yet. Gap length is defined as the difference
2144             between coordinate values at the first data point after a gap and the last value
2145             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2146             between coordinate values at the first (last) valid data point and the first (last) NaN.
2147             For example, consider::
2148 
2149                 <xarray.DataArray (x: 9)>
2150                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2151                 Coordinates:
2152                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2153 
2154             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2155         kwargs : dict, optional
2156             parameters passed verbatim to the underlying interpolation function
2157 
2158         Returns
2159         -------
2160         interpolated: DataArray
2161             Filled in DataArray.
2162 
2163         See also
2164         --------
2165         numpy.interp
2166         scipy.interpolate
2167         """
2168         from .missing import interp_na
2169 
2170         return interp_na(
2171             self,
2172             dim=dim,
2173             method=method,
2174             limit=limit,
2175             use_coordinate=use_coordinate,
2176             max_gap=max_gap,
2177             **kwargs,
2178         )
2179 
2180     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2181         """Fill NaN values by propogating values forward
2182 
2183         *Requires bottleneck.*
2184 
2185         Parameters
2186         ----------
2187         dim : hashable
2188             Specifies the dimension along which to propagate values when
2189             filling.
2190         limit : int, default None
2191             The maximum number of consecutive NaN values to forward fill. In
2192             other words, if there is a gap with more than this number of
2193             consecutive NaNs, it will only be partially filled. Must be greater
2194             than 0 or None for no limit.
2195 
2196         Returns
2197         -------
2198         DataArray
2199         """
2200         from .missing import ffill
2201 
2202         return ffill(self, dim, limit=limit)
2203 
2204     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2205         """Fill NaN values by propogating values backward
2206 
2207         *Requires bottleneck.*
2208 
2209         Parameters
2210         ----------
2211         dim : str
2212             Specifies the dimension along which to propagate values when
2213             filling.
2214         limit : int, default None
2215             The maximum number of consecutive NaN values to backward fill. In
2216             other words, if there is a gap with more than this number of
2217             consecutive NaNs, it will only be partially filled. Must be greater
2218             than 0 or None for no limit.
2219 
2220         Returns
2221         -------
2222         DataArray
2223         """
2224         from .missing import bfill
2225 
2226         return bfill(self, dim, limit=limit)
2227 
2228     def combine_first(self, other: "DataArray") -> "DataArray":
2229         """Combine two DataArray objects, with union of coordinates.
2230 
2231         This operation follows the normal broadcasting and alignment rules of
2232         ``join='outer'``.  Default to non-null values of array calling the
2233         method.  Use np.nan to fill in vacant cells after alignment.
2234 
2235         Parameters
2236         ----------
2237         other : DataArray
2238             Used to fill all matching missing values in this array.
2239 
2240         Returns
2241         -------
2242         DataArray
2243         """
2244         return ops.fillna(self, other, join="outer")
2245 
2246     def reduce(
2247         self,
2248         func: Callable[..., Any],
2249         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2250         axis: Union[None, int, Sequence[int]] = None,
2251         keep_attrs: bool = None,
2252         keepdims: bool = False,
2253         **kwargs: Any,
2254     ) -> "DataArray":
2255         """Reduce this array by applying `func` along some dimension(s).
2256 
2257         Parameters
2258         ----------
2259         func : function
2260             Function which can be called in the form
2261             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2262             np.ndarray over an integer valued axis.
2263         dim : hashable or sequence of hashables, optional
2264             Dimension(s) over which to apply `func`.
2265         axis : int or sequence of int, optional
2266             Axis(es) over which to repeatedly apply `func`. Only one of the
2267             'dim' and 'axis' arguments can be supplied. If neither are
2268             supplied, then the reduction is calculated over the flattened array
2269             (by calling `f(x)` without an axis argument).
2270         keep_attrs : bool, optional
2271             If True, the variable's attributes (`attrs`) will be copied from
2272             the original object to the new one.  If False (default), the new
2273             object will be returned without attributes.
2274         keepdims : bool, default False
2275             If True, the dimensions which are reduced are left in the result
2276             as dimensions of size one. Coordinates that use these dimensions
2277             are removed.
2278         **kwargs : dict
2279             Additional keyword arguments passed on to `func`.
2280 
2281         Returns
2282         -------
2283         reduced : DataArray
2284             DataArray with this object's array replaced with an array with
2285             summarized data and the indicated dimension(s) removed.
2286         """
2287 
2288         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2289         return self._replace_maybe_drop_dims(var)
2290 
2291     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2292         """Convert this array into a pandas object with the same shape.
2293 
2294         The type of the returned object depends on the number of DataArray
2295         dimensions:
2296 
2297         * 0D -> `xarray.DataArray`
2298         * 1D -> `pandas.Series`
2299         * 2D -> `pandas.DataFrame`
2300 
2301         Only works for arrays with 2 or fewer dimensions.
2302 
2303         The DataArray constructor performs the inverse transformation.
2304         """
2305         # TODO: consolidate the info about pandas constructors and the
2306         # attributes that correspond to their indexes into a separate module?
2307         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2308         try:
2309             constructor = constructors[self.ndim]
2310         except KeyError:
2311             raise ValueError(
2312                 "cannot convert arrays with %s dimensions into "
2313                 "pandas objects" % self.ndim
2314             )
2315         indexes = [self.get_index(dim) for dim in self.dims]
2316         return constructor(self.values, *indexes)
2317 
2318     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:
2319         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2320 
2321         The DataFrame is indexed by the Cartesian product of index coordinates
2322         (in the form of a :py:class:`pandas.MultiIndex`).
2323 
2324         Other coordinates are included as columns in the DataFrame.
2325         """
2326         if name is None:
2327             name = self.name
2328         if name is None:
2329             raise ValueError(
2330                 "cannot convert an unnamed DataArray to a "
2331                 "DataFrame: use the ``name`` parameter"
2332             )
2333 
2334         dims = dict(zip(self.dims, self.shape))
2335         # By using a unique name, we can convert a DataArray into a DataFrame
2336         # even if it shares a name with one of its coordinates.
2337         # I would normally use unique_name = object() but that results in a
2338         # dataframe with columns in the wrong order, for reasons I have not
2339         # been able to debug (possibly a pandas bug?).
2340         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2341         ds = self._to_dataset_whole(name=unique_name)
2342         df = ds._to_dataframe(dims)
2343         df.columns = [name if c == unique_name else c for c in df.columns]
2344         return df
2345 
2346     def to_series(self) -> pd.Series:
2347         """Convert this array into a pandas.Series.
2348 
2349         The Series is indexed by the Cartesian product of index coordinates
2350         (in the form of a :py:class:`pandas.MultiIndex`).
2351         """
2352         index = self.coords.to_index()
2353         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2354 
2355     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2356         """Convert this array into a numpy.ma.MaskedArray
2357 
2358         Parameters
2359         ----------
2360         copy : bool
2361             If True (default) make a copy of the array in the result. If False,
2362             a MaskedArray view of DataArray.values is returned.
2363 
2364         Returns
2365         -------
2366         result : MaskedArray
2367             Masked where invalid values (nan or inf) occur.
2368         """
2369         values = self.values  # only compute lazy arrays once
2370         isnull = pd.isnull(values)
2371         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2372 
2373     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2374         """Write DataArray contents to a netCDF file.
2375 
2376         All parameters are passed directly to `xarray.Dataset.to_netcdf`.
2377 
2378         Notes
2379         -----
2380         Only xarray.Dataset objects can be written to netCDF files, so
2381         the xarray.DataArray is converted to a xarray.Dataset object
2382         containing a single variable. If the DataArray has no name, or if the
2383         name is the same as a co-ordinate name, then it is given the name
2384         '__xarray_dataarray_variable__'.
2385         """
2386         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2387 
2388         if self.name is None:
2389             # If no name is set then use a generic xarray name
2390             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2391         elif self.name in self.coords or self.name in self.dims:
2392             # The name is the same as one of the coords names, which netCDF
2393             # doesn't support, so rename it but keep track of the old name
2394             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2395             dataset.attrs[DATAARRAY_NAME] = self.name
2396         else:
2397             # No problems with the name - so we're fine!
2398             dataset = self.to_dataset()
2399 
2400         return dataset.to_netcdf(*args, **kwargs)
2401 
2402     def to_dict(self, data: bool = True) -> dict:
2403         """
2404         Convert this xarray.DataArray into a dictionary following xarray
2405         naming conventions.
2406 
2407         Converts all variables and attributes to native Python objects.
2408         Useful for converting to json. To avoid datetime incompatibility
2409         use decode_times=False kwarg in xarrray.open_dataset.
2410 
2411         Parameters
2412         ----------
2413         data : bool, optional
2414             Whether to include the actual data in the dictionary. When set to
2415             False, returns just the schema.
2416 
2417         See also
2418         --------
2419         DataArray.from_dict
2420         """
2421         d = self.variable.to_dict(data=data)
2422         d.update({"coords": {}, "name": self.name})
2423         for k in self.coords:
2424             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2425         return d
2426 
2427     @classmethod
2428     def from_dict(cls, d: dict) -> "DataArray":
2429         """
2430         Convert a dictionary into an xarray.DataArray
2431 
2432         Input dict can take several forms::
2433 
2434             d = {'dims': ('t'), 'data': x}
2435 
2436             d = {'coords': {'t': {'dims': 't', 'data': t,
2437                                   'attrs': {'units':'s'}}},
2438                  'attrs': {'title': 'air temperature'},
2439                  'dims': 't',
2440                  'data': x,
2441                  'name': 'a'}
2442 
2443         where 't' is the name of the dimesion, 'a' is the name of the array,
2444         and  x and t are lists, numpy.arrays, or pandas objects.
2445 
2446         Parameters
2447         ----------
2448         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}
2449 
2450         Returns
2451         -------
2452         obj : xarray.DataArray
2453 
2454         See also
2455         --------
2456         DataArray.to_dict
2457         Dataset.from_dict
2458         """
2459         coords = None
2460         if "coords" in d:
2461             try:
2462                 coords = {
2463                     k: (v["dims"], v["data"], v.get("attrs"))
2464                     for k, v in d["coords"].items()
2465                 }
2466             except KeyError as e:
2467                 raise ValueError(
2468                     "cannot convert dict when coords are missing the key "
2469                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2470                 )
2471         try:
2472             data = d["data"]
2473         except KeyError:
2474             raise ValueError("cannot convert dict without the key 'data''")
2475         else:
2476             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2477         return obj
2478 
2479     @classmethod
2480     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2481         """Convert a pandas.Series into an xarray.DataArray.
2482 
2483         If the series's index is a MultiIndex, it will be expanded into a
2484         tensor product of one-dimensional coordinates (filling in missing
2485         values with NaN). Thus this operation should be the inverse of the
2486         `to_series` method.
2487 
2488         If sparse=True, creates a sparse array instead of a dense NumPy array.
2489         Requires the pydata/sparse package.
2490 
2491         See also
2492         --------
2493         xarray.Dataset.from_dataframe
2494         """
2495         temp_name = "__temporary_name"
2496         df = pd.DataFrame({temp_name: series})
2497         ds = Dataset.from_dataframe(df, sparse=sparse)
2498         result = cast(DataArray, ds[temp_name])
2499         result.name = series.name
2500         return result
2501 
2502     def to_cdms2(self) -> "cdms2_Variable":
2503         """Convert this array into a cdms2.Variable
2504         """
2505         from ..convert import to_cdms2
2506 
2507         return to_cdms2(self)
2508 
2509     @classmethod
2510     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2511         """Convert a cdms2.Variable into an xarray.DataArray
2512         """
2513         from ..convert import from_cdms2
2514 
2515         return from_cdms2(variable)
2516 
2517     def to_iris(self) -> "iris_Cube":
2518         """Convert this array into a iris.cube.Cube
2519         """
2520         from ..convert import to_iris
2521 
2522         return to_iris(self)
2523 
2524     @classmethod
2525     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2526         """Convert a iris.cube.Cube into an xarray.DataArray
2527         """
2528         from ..convert import from_iris
2529 
2530         return from_iris(cube)
2531 
2532     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2533         """Helper function for equals, broadcast_equals, and identical
2534         """
2535 
2536         def compat(x, y):
2537             return getattr(x.variable, compat_str)(y.variable)
2538 
2539         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2540             self, other
2541         )
2542 
2543     def broadcast_equals(self, other: "DataArray") -> bool:
2544         """Two DataArrays are broadcast equal if they are equal after
2545         broadcasting them against each other such that they have the same
2546         dimensions.
2547 
2548         See Also
2549         --------
2550         DataArray.equals
2551         DataArray.identical
2552         """
2553         try:
2554             return self._all_compat(other, "broadcast_equals")
2555         except (TypeError, AttributeError):
2556             return False
2557 
2558     def equals(self, other: "DataArray") -> bool:
2559         """True if two DataArrays have the same dimensions, coordinates and
2560         values; otherwise False.
2561 
2562         DataArrays can still be equal (like pandas objects) if they have NaN
2563         values in the same locations.
2564 
2565         This method is necessary because `v1 == v2` for ``DataArray``
2566         does element-wise comparisons (like numpy.ndarrays).
2567 
2568         See Also
2569         --------
2570         DataArray.broadcast_equals
2571         DataArray.identical
2572         """
2573         try:
2574             return self._all_compat(other, "equals")
2575         except (TypeError, AttributeError):
2576             return False
2577 
2578     def identical(self, other: "DataArray") -> bool:
2579         """Like equals, but also checks the array name and attributes, and
2580         attributes on all coordinates.
2581 
2582         See Also
2583         --------
2584         DataArray.broadcast_equals
2585         DataArray.equal
2586         """
2587         try:
2588             return self.name == other.name and self._all_compat(other, "identical")
2589         except (TypeError, AttributeError):
2590             return False
2591 
2592     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2593         # use the same naming heuristics as pandas:
2594         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2595         other_name = getattr(other, "name", _default)
2596         if other_name is _default or other_name == self.name:
2597             return self.name
2598         else:
2599             return None
2600 
2601     def __array_wrap__(self, obj, context=None) -> "DataArray":
2602         new_var = self.variable.__array_wrap__(obj, context)
2603         return self._replace(new_var)
2604 
2605     def __matmul__(self, obj):
2606         return self.dot(obj)
2607 
2608     def __rmatmul__(self, other):
2609         # currently somewhat duplicative, as only other DataArrays are
2610         # compatible with matmul
2611         return computation.dot(other, self)
2612 
2613     @staticmethod
2614     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2615         @functools.wraps(f)
2616         def func(self, *args, **kwargs):
2617             with np.errstate(all="ignore"):
2618                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2619 
2620         return func
2621 
2622     @staticmethod
2623     def _binary_op(
2624         f: Callable[..., Any],
2625         reflexive: bool = False,
2626         join: str = None,  # see xarray.align
2627         **ignored_kwargs,
2628     ) -> Callable[..., "DataArray"]:
2629         @functools.wraps(f)
2630         def func(self, other):
2631             if isinstance(other, (Dataset, groupby.GroupBy)):
2632                 return NotImplemented
2633             if isinstance(other, DataArray):
2634                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2635                 self, other = align(self, other, join=align_type, copy=False)
2636             other_variable = getattr(other, "variable", other)
2637             other_coords = getattr(other, "coords", None)
2638 
2639             variable = (
2640                 f(self.variable, other_variable)
2641                 if not reflexive
2642                 else f(other_variable, self.variable)
2643             )
2644             coords, indexes = self.coords._merge_raw(other_coords)
2645             name = self._result_name(other)
2646 
2647             return self._replace(variable, coords, name, indexes=indexes)
2648 
2649         return func
2650 
2651     @staticmethod
2652     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
2653         @functools.wraps(f)
2654         def func(self, other):
2655             if isinstance(other, groupby.GroupBy):
2656                 raise TypeError(
2657                     "in-place operations between a DataArray and "
2658                     "a grouped object are not permitted"
2659                 )
2660             # n.b. we can't align other to self (with other.reindex_like(self))
2661             # because `other` may be converted into floats, which would cause
2662             # in-place arithmetic to fail unpredictably. Instead, we simply
2663             # don't support automatic alignment with in-place arithmetic.
2664             other_coords = getattr(other, "coords", None)
2665             other_variable = getattr(other, "variable", other)
2666             with self.coords._merge_inplace(other_coords):
2667                 f(self.variable, other_variable)
2668             return self
2669 
2670         return func
2671 
2672     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
2673         self.attrs = other.attrs
2674 
2675     @property
2676     def plot(self) -> _PlotMethods:
2677         """
2678         Access plotting functions for DataArray's
2679 
2680         >>> d = xr.DataArray([[1, 2], [3, 4]])
2681 
2682         For convenience just call this directly
2683 
2684         >>> d.plot()
2685 
2686         Or use it as a namespace to use xarray.plot functions as
2687         DataArray methods
2688 
2689         >>> d.plot.imshow()  # equivalent to xarray.plot.imshow(d)
2690 
2691         """
2692         return _PlotMethods(self)
2693 
2694     def _title_for_slice(self, truncate: int = 50) -> str:
2695         """
2696         If the dataarray has 1 dimensional coordinates or comes from a slice
2697         we can show that info in the title
2698 
2699         Parameters
2700         ----------
2701         truncate : integer
2702             maximum number of characters for title
2703 
2704         Returns
2705         -------
2706         title : string
2707             Can be used for plot titles
2708 
2709         """
2710         one_dims = []
2711         for dim, coord in self.coords.items():
2712             if coord.size == 1:
2713                 one_dims.append(
2714                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
2715                 )
2716 
2717         title = ", ".join(one_dims)
2718         if len(title) > truncate:
2719             title = title[: (truncate - 3)] + "..."
2720 
2721         return title
2722 
2723     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
2724         """Calculate the n-th order discrete difference along given axis.
2725 
2726         Parameters
2727         ----------
2728         dim : hashable
2729             Dimension over which to calculate the finite difference.
2730         n : int, optional
2731             The number of times values are differenced.
2732         label : hashable, optional
2733             The new coordinate in dimension ``dim`` will have the
2734             values of either the minuend's or subtrahend's coordinate
2735             for values 'upper' and 'lower', respectively.  Other
2736             values are not supported.
2737 
2738         Returns
2739         -------
2740         difference : same type as caller
2741             The n-th order finite difference of this object.
2742 
2743         .. note::
2744 
2745             `n` matches numpy's behavior and is different from pandas' first
2746             argument named `periods`.
2747 
2748 
2749         Examples
2750         --------
2751         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
2752         >>> arr.diff("x")
2753         <xarray.DataArray (x: 3)>
2754         array([0, 1, 0])
2755         Coordinates:
2756         * x        (x) int64 2 3 4
2757         >>> arr.diff("x", 2)
2758         <xarray.DataArray (x: 2)>
2759         array([ 1, -1])
2760         Coordinates:
2761         * x        (x) int64 3 4
2762 
2763         See Also
2764         --------
2765         DataArray.differentiate
2766         """
2767         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
2768         return self._from_temp_dataset(ds)
2769 
2770     def shift(
2771         self,
2772         shifts: Mapping[Hashable, int] = None,
2773         fill_value: Any = dtypes.NA,
2774         **shifts_kwargs: int,
2775     ) -> "DataArray":
2776         """Shift this array by an offset along one or more dimensions.
2777 
2778         Only the data is moved; coordinates stay in place. Values shifted from
2779         beyond array bounds are replaced by NaN. This is consistent with the
2780         behavior of ``shift`` in pandas.
2781 
2782         Parameters
2783         ----------
2784         shifts : Mapping with the form of {dim: offset}
2785             Integer offset to shift along each of the given dimensions.
2786             Positive offsets shift to the right; negative offsets shift to the
2787             left.
2788         fill_value: scalar, optional
2789             Value to use for newly missing values
2790         **shifts_kwargs:
2791             The keyword arguments form of ``shifts``.
2792             One of shifts or shifts_kwargs must be provided.
2793 
2794         Returns
2795         -------
2796         shifted : DataArray
2797             DataArray with the same coordinates and attributes but shifted
2798             data.
2799 
2800         See also
2801         --------
2802         roll
2803 
2804         Examples
2805         --------
2806 
2807         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2808         >>> arr.shift(x=1)
2809         <xarray.DataArray (x: 3)>
2810         array([ nan,   5.,   6.])
2811         Coordinates:
2812           * x        (x) int64 0 1 2
2813         """
2814         variable = self.variable.shift(
2815             shifts=shifts, fill_value=fill_value, **shifts_kwargs
2816         )
2817         return self._replace(variable=variable)
2818 
2819     def roll(
2820         self,
2821         shifts: Mapping[Hashable, int] = None,
2822         roll_coords: bool = None,
2823         **shifts_kwargs: int,
2824     ) -> "DataArray":
2825         """Roll this array by an offset along one or more dimensions.
2826 
2827         Unlike shift, roll may rotate all variables, including coordinates
2828         if specified. The direction of rotation is consistent with
2829         :py:func:`numpy.roll`.
2830 
2831         Parameters
2832         ----------
2833         shifts : Mapping with the form of {dim: offset}
2834             Integer offset to rotate each of the given dimensions.
2835             Positive offsets roll to the right; negative offsets roll to the
2836             left.
2837         roll_coords : bool
2838             Indicates whether to  roll the coordinates by the offset
2839             The current default of roll_coords (None, equivalent to True) is
2840             deprecated and will change to False in a future version.
2841             Explicitly pass roll_coords to silence the warning.
2842         **shifts_kwargs : The keyword arguments form of ``shifts``.
2843             One of shifts or shifts_kwargs must be provided.
2844 
2845         Returns
2846         -------
2847         rolled : DataArray
2848             DataArray with the same attributes but rolled data and coordinates.
2849 
2850         See also
2851         --------
2852         shift
2853 
2854         Examples
2855         --------
2856 
2857         >>> arr = xr.DataArray([5, 6, 7], dims="x")
2858         >>> arr.roll(x=1)
2859         <xarray.DataArray (x: 3)>
2860         array([7, 5, 6])
2861         Coordinates:
2862           * x        (x) int64 2 0 1
2863         """
2864         ds = self._to_temp_dataset().roll(
2865             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
2866         )
2867         return self._from_temp_dataset(ds)
2868 
2869     @property
2870     def real(self) -> "DataArray":
2871         return self._replace(self.variable.real)
2872 
2873     @property
2874     def imag(self) -> "DataArray":
2875         return self._replace(self.variable.imag)
2876 
2877     def dot(
2878         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
2879     ) -> "DataArray":
2880         """Perform dot product of two DataArrays along their shared dims.
2881 
2882         Equivalent to taking taking tensordot over all shared dims.
2883 
2884         Parameters
2885         ----------
2886         other : DataArray
2887             The other array with which the dot product is performed.
2888         dims: '...', hashable or sequence of hashables, optional
2889             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
2890             If not specified, then all the common dimensions are summed over.
2891 
2892         Returns
2893         -------
2894         result : DataArray
2895             Array resulting from the dot product over all shared dimensions.
2896 
2897         See also
2898         --------
2899         dot
2900         numpy.tensordot
2901 
2902         Examples
2903         --------
2904 
2905         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
2906         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
2907         >>> dm_vals = np.arange(4)
2908         >>> dm = xr.DataArray(dm_vals, dims=["z"])
2909 
2910         >>> dm.dims
2911         ('z')
2912 
2913         >>> da.dims
2914         ('x', 'y', 'z')
2915 
2916         >>> dot_result = da.dot(dm)
2917         >>> dot_result.dims
2918         ('x', 'y')
2919 
2920         """
2921         if isinstance(other, Dataset):
2922             raise NotImplementedError(
2923                 "dot products are not yet supported with Dataset objects."
2924             )
2925         if not isinstance(other, DataArray):
2926             raise TypeError("dot only operates on DataArrays.")
2927 
2928         return computation.dot(self, other, dims=dims)
2929 
2930     def sortby(
2931         self,
2932         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
2933         ascending: bool = True,
2934     ) -> "DataArray":
2935         """Sort object by labels or values (along an axis).
2936 
2937         Sorts the dataarray, either along specified dimensions,
2938         or according to values of 1-D dataarrays that share dimension
2939         with calling object.
2940 
2941         If the input variables are dataarrays, then the dataarrays are aligned
2942         (via left-join) to the calling object prior to sorting by cell values.
2943         NaNs are sorted to the end, following Numpy convention.
2944 
2945         If multiple sorts along the same dimension is
2946         given, numpy's lexsort is performed along that dimension:
2947         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
2948         and the FIRST key in the sequence is used as the primary sort key,
2949         followed by the 2nd key, etc.
2950 
2951         Parameters
2952         ----------
2953         variables: hashable, DataArray, or sequence of either
2954             1D DataArray objects or name(s) of 1D variable(s) in
2955             coords whose values are used to sort this array.
2956         ascending: boolean, optional
2957             Whether to sort by ascending or descending order.
2958 
2959         Returns
2960         -------
2961         sorted: DataArray
2962             A new dataarray where all the specified dims are sorted by dim
2963             labels.
2964 
2965         Examples
2966         --------
2967 
2968         >>> da = xr.DataArray(
2969         ...     np.random.rand(5),
2970         ...     coords=[pd.date_range("1/1/2000", periods=5)],
2971         ...     dims="time",
2972         ... )
2973         >>> da
2974         <xarray.DataArray (time: 5)>
2975         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])
2976         Coordinates:
2977           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...
2978 
2979         >>> da.sortby(da)
2980         <xarray.DataArray (time: 5)>
2981         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])
2982         Coordinates:
2983           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...
2984         """
2985         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
2986         return self._from_temp_dataset(ds)
2987 
2988     def quantile(
2989         self,
2990         q: Any,
2991         dim: Union[Hashable, Sequence[Hashable], None] = None,
2992         interpolation: str = "linear",
2993         keep_attrs: bool = None,
2994         skipna: bool = True,
2995     ) -> "DataArray":
2996         """Compute the qth quantile of the data along the specified dimension.
2997 
2998         Returns the qth quantiles(s) of the array elements.
2999 
3000         Parameters
3001         ----------
3002         q : float in range of [0,1] or array-like of floats
3003             Quantile to compute, which must be between 0 and 1 inclusive.
3004         dim : hashable or sequence of hashable, optional
3005             Dimension(s) over which to apply quantile.
3006         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
3007             This optional parameter specifies the interpolation method to
3008             use when the desired quantile lies between two data points
3009             ``i < j``:
3010 
3011                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3012                   the fractional part of the index surrounded by ``i`` and
3013                   ``j``.
3014                 - lower: ``i``.
3015                 - higher: ``j``.
3016                 - nearest: ``i`` or ``j``, whichever is nearest.
3017                 - midpoint: ``(i + j) / 2``.
3018         keep_attrs : bool, optional
3019             If True, the dataset's attributes (`attrs`) will be copied from
3020             the original object to the new one.  If False (default), the new
3021             object will be returned without attributes.
3022         skipna : bool, optional
3023             Whether to skip missing values when aggregating.
3024 
3025         Returns
3026         -------
3027         quantiles : DataArray
3028             If `q` is a single quantile, then the result
3029             is a scalar. If multiple percentiles are given, first axis of
3030             the result corresponds to the quantile and a quantile dimension
3031             is added to the return array. The other dimensions are the
3032             dimensions that remain after the reduction of the array.
3033 
3034         See Also
3035         --------
3036         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3037 
3038         Examples
3039         --------
3040 
3041         >>> da = xr.DataArray(
3042         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3043         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3044         ...     dims=("x", "y"),
3045         ... )
3046         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3047         <xarray.DataArray ()>
3048         array(0.7)
3049         Coordinates:
3050             quantile  float64 0.0
3051         >>> da.quantile(0, dim="x")
3052         <xarray.DataArray (y: 4)>
3053         array([0.7, 4.2, 2.6, 1.5])
3054         Coordinates:
3055           * y         (y) float64 1.0 1.5 2.0 2.5
3056             quantile  float64 0.0
3057         >>> da.quantile([0, 0.5, 1])
3058         <xarray.DataArray (quantile: 3)>
3059         array([0.7, 3.4, 9.4])
3060         Coordinates:
3061           * quantile  (quantile) float64 0.0 0.5 1.0
3062         >>> da.quantile([0, 0.5, 1], dim="x")
3063         <xarray.DataArray (quantile: 3, y: 4)>
3064         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3065                [3.6 , 5.75, 6.  , 1.7 ],
3066                [6.5 , 7.3 , 9.4 , 1.9 ]])
3067         Coordinates:
3068           * y         (y) float64 1.0 1.5 2.0 2.5
3069           * quantile  (quantile) float64 0.0 0.5 1.0
3070         """
3071 
3072         ds = self._to_temp_dataset().quantile(
3073             q,
3074             dim=dim,
3075             keep_attrs=keep_attrs,
3076             interpolation=interpolation,
3077             skipna=skipna,
3078         )
3079         return self._from_temp_dataset(ds)
3080 
3081     def rank(
3082         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3083     ) -> "DataArray":
3084         """Ranks the data.
3085 
3086         Equal values are assigned a rank that is the average of the ranks that
3087         would have been otherwise assigned to all of the values within that
3088         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3089 
3090         NaNs in the input array are returned as NaNs.
3091 
3092         The `bottleneck` library is required.
3093 
3094         Parameters
3095         ----------
3096         dim : hashable
3097             Dimension over which to compute rank.
3098         pct : bool, optional
3099             If True, compute percentage ranks, otherwise compute integer ranks.
3100         keep_attrs : bool, optional
3101             If True, the dataset's attributes (`attrs`) will be copied from
3102             the original object to the new one.  If False (default), the new
3103             object will be returned without attributes.
3104 
3105         Returns
3106         -------
3107         ranked : DataArray
3108             DataArray with the same coordinates and dtype 'float64'.
3109 
3110         Examples
3111         --------
3112 
3113         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3114         >>> arr.rank("x")
3115         <xarray.DataArray (x: 3)>
3116         array([ 1.,   2.,   3.])
3117         Dimensions without coordinates: x
3118         """
3119 
3120         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3121         return self._from_temp_dataset(ds)
3122 
3123     def differentiate(
3124         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3125     ) -> "DataArray":
3126         """ Differentiate the array with the second order accurate central
3127         differences.
3128 
3129         .. note::
3130             This feature is limited to simple cartesian geometry, i.e. coord
3131             must be one dimensional.
3132 
3133         Parameters
3134         ----------
3135         coord: hashable
3136             The coordinate to be used to compute the gradient.
3137         edge_order: 1 or 2. Default 1
3138             N-th order accurate differences at the boundaries.
3139         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
3140             'us', 'ns', 'ps', 'fs', 'as'}
3141             Unit to compute gradient. Only valid for datetime coordinate.
3142 
3143         Returns
3144         -------
3145         differentiated: DataArray
3146 
3147         See also
3148         --------
3149         numpy.gradient: corresponding numpy function
3150 
3151         Examples
3152         --------
3153 
3154         >>> da = xr.DataArray(
3155         ...     np.arange(12).reshape(4, 3),
3156         ...     dims=["x", "y"],
3157         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3158         ... )
3159         >>> da
3160         <xarray.DataArray (x: 4, y: 3)>
3161         array([[ 0,  1,  2],
3162                [ 3,  4,  5],
3163                [ 6,  7,  8],
3164                [ 9, 10, 11]])
3165         Coordinates:
3166           * x        (x) float64 0.0 0.1 1.1 1.2
3167         Dimensions without coordinates: y
3168         >>>
3169         >>> da.differentiate("x")
3170         <xarray.DataArray (x: 4, y: 3)>
3171         array([[30.      , 30.      , 30.      ],
3172                [27.545455, 27.545455, 27.545455],
3173                [27.545455, 27.545455, 27.545455],
3174                [30.      , 30.      , 30.      ]])
3175         Coordinates:
3176           * x        (x) float64 0.0 0.1 1.1 1.2
3177         Dimensions without coordinates: y
3178         """
3179         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3180         return self._from_temp_dataset(ds)
3181 
3182     def integrate(
3183         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
3184     ) -> "DataArray":
3185         """ integrate the array with the trapezoidal rule.
3186 
3187         .. note::
3188             This feature is limited to simple cartesian geometry, i.e. dim
3189             must be one dimensional.
3190 
3191         Parameters
3192         ----------
3193         dim: hashable, or a sequence of hashable
3194             Coordinate(s) used for the integration.
3195         datetime_unit: str, optional
3196             Can be used to specify the unit if datetime coordinate is used.
3197             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',
3198             'fs', 'as'}
3199 
3200         Returns
3201         -------
3202         integrated: DataArray
3203 
3204         See also
3205         --------
3206         numpy.trapz: corresponding numpy function
3207 
3208         Examples
3209         --------
3210 
3211         >>> da = xr.DataArray(
3212         ...     np.arange(12).reshape(4, 3),
3213         ...     dims=["x", "y"],
3214         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3215         ... )
3216         >>> da
3217         <xarray.DataArray (x: 4, y: 3)>
3218         array([[ 0,  1,  2],
3219                [ 3,  4,  5],
3220                [ 6,  7,  8],
3221                [ 9, 10, 11]])
3222         Coordinates:
3223           * x        (x) float64 0.0 0.1 1.1 1.2
3224         Dimensions without coordinates: y
3225         >>>
3226         >>> da.integrate("x")
3227         <xarray.DataArray (y: 3)>
3228         array([5.4, 6.6, 7.8])
3229         Dimensions without coordinates: y
3230         """
3231         ds = self._to_temp_dataset().integrate(dim, datetime_unit)
3232         return self._from_temp_dataset(ds)
3233 
3234     def unify_chunks(self) -> "DataArray":
3235         """ Unify chunk size along all chunked dimensions of this DataArray.
3236 
3237         Returns
3238         -------
3239 
3240         DataArray with consistent chunk sizes for all dask-array variables
3241 
3242         See Also
3243         --------
3244 
3245         dask.array.core.unify_chunks
3246         """
3247         ds = self._to_temp_dataset().unify_chunks()
3248         return self._from_temp_dataset(ds)
3249 
3250     def map_blocks(
3251         self,
3252         func: "Callable[..., T_DSorDA]",
3253         args: Sequence[Any] = (),
3254         kwargs: Mapping[str, Any] = None,
3255     ) -> "T_DSorDA":
3256         """
3257         Apply a function to each chunk of this DataArray. This method is experimental
3258         and its signature may change.
3259 
3260         Parameters
3261         ----------
3262         func: callable
3263             User-provided function that accepts a DataArray as its first parameter. The
3264             function will receive a subset of this DataArray, corresponding to one chunk
3265             along each chunked dimension. ``func`` will be executed as
3266             ``func(obj_subset, *args, **kwargs)``.
3267 
3268             The function will be first run on mocked-up data, that looks like this array
3269             but has sizes 0, to determine properties of the returned object such as
3270             dtype, variable names, new dimensions and new indexes (if any).
3271 
3272             This function must return either a single DataArray or a single Dataset.
3273 
3274             This function cannot change size of existing dimensions, or add new chunked
3275             dimensions.
3276         args: Sequence
3277             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
3278             objects, if any, will not be split by chunks. Passing dask collections is
3279             not allowed.
3280         kwargs: Mapping
3281             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3282             split by chunks. Passing dask collections is not allowed.
3283 
3284         Returns
3285         -------
3286         A single DataArray or Dataset with dask backend, reassembled from the outputs of
3287         the function.
3288 
3289         Notes
3290         -----
3291         This method is designed for when one needs to manipulate a whole xarray object
3292         within each chunk. In the more common case where one can work on numpy arrays,
3293         it is recommended to use apply_ufunc.
3294 
3295         If none of the variables in this DataArray is backed by dask, calling this
3296         method is equivalent to calling ``func(self, *args, **kwargs)``.
3297 
3298         See Also
3299         --------
3300         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
3301         xarray.Dataset.map_blocks
3302         """
3303         from .parallel import map_blocks
3304 
3305         return map_blocks(func, self, args, kwargs)
3306 
3307     def polyfit(
3308         self,
3309         dim: Hashable,
3310         deg: int,
3311         skipna: bool = None,
3312         rcond: float = None,
3313         w: Union[Hashable, Any] = None,
3314         full: bool = False,
3315         cov: bool = False,
3316     ):
3317         """
3318         Least squares polynomial fit.
3319 
3320         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3321         invalid values when `skipna = True`.
3322 
3323         Parameters
3324         ----------
3325         dim : hashable
3326             Coordinate along which to fit the polynomials.
3327         deg : int
3328             Degree of the fitting polynomial.
3329         skipna : bool, optional
3330             If True, removes all invalid values before fitting each 1D slices of the array.
3331             Default is True if data is stored in a dask.array or if there is any
3332             invalid values, False otherwise.
3333         rcond : float, optional
3334             Relative condition number to the fit.
3335         w : Union[Hashable, Any], optional
3336             Weights to apply to the y-coordinate of the sample points.
3337             Can be an array-like object or the name of a coordinate in the dataset.
3338         full : bool, optional
3339             Whether to return the residuals, matrix rank and singular values in addition
3340             to the coefficients.
3341         cov : Union[bool, str], optional
3342             Whether to return to the covariance matrix in addition to the coefficients.
3343             The matrix is not scaled if `cov='unscaled'`.
3344 
3345         Returns
3346         -------
3347         polyfit_results : Dataset
3348             A single dataset which contains:
3349 
3350             polyfit_coefficients
3351                 The coefficients of the best fit.
3352             polyfit_residuals
3353                 The residuals of the least-square computation (only included if `full=True`)
3354             [dim]_matrix_rank
3355                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3356             [dim]_singular_value
3357                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3358             polyfit_covariance
3359                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3360 
3361         See also
3362         --------
3363         numpy.polyfit
3364         """
3365         return self._to_temp_dataset().polyfit(
3366             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3367         )
3368 
3369     def pad(
3370         self,
3371         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3372         mode: str = "constant",
3373         stat_length: Union[
3374             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3375         ] = None,
3376         constant_values: Union[
3377             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3378         ] = None,
3379         end_values: Union[
3380             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3381         ] = None,
3382         reflect_type: str = None,
3383         **pad_width_kwargs: Any,
3384     ) -> "DataArray":
3385         """Pad this array along one or more dimensions.
3386 
3387         .. warning::
3388             This function is experimental and its behaviour is likely to change
3389             especially regarding padding of dimension coordinates (or IndexVariables).
3390 
3391         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3392         coordinates will be padded with the same mode, otherwise coordinates
3393         are padded using the "constant" mode with fill_value dtypes.NA.
3394 
3395         Parameters
3396         ----------
3397         pad_width : Mapping with the form of {dim: (pad_before, pad_after)}
3398             Number of values padded along each dimension.
3399             {dim: pad} is a shortcut for pad_before = pad_after = pad
3400         mode : str
3401             One of the following string values (taken from numpy docs)
3402 
3403             'constant' (default)
3404                 Pads with a constant value.
3405             'edge'
3406                 Pads with the edge values of array.
3407             'linear_ramp'
3408                 Pads with the linear ramp between end_value and the
3409                 array edge value.
3410             'maximum'
3411                 Pads with the maximum value of all or part of the
3412                 vector along each axis.
3413             'mean'
3414                 Pads with the mean value of all or part of the
3415                 vector along each axis.
3416             'median'
3417                 Pads with the median value of all or part of the
3418                 vector along each axis.
3419             'minimum'
3420                 Pads with the minimum value of all or part of the
3421                 vector along each axis.
3422             'reflect'
3423                 Pads with the reflection of the vector mirrored on
3424                 the first and last values of the vector along each
3425                 axis.
3426             'symmetric'
3427                 Pads with the reflection of the vector mirrored
3428                 along the edge of the array.
3429             'wrap'
3430                 Pads with the wrap of the vector along the axis.
3431                 The first values are used to pad the end and the
3432                 end values are used to pad the beginning.
3433         stat_length : int, tuple or mapping of the form {dim: tuple}
3434             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3435             values at edge of each axis used to calculate the statistic value.
3436             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3437             statistic lengths along each dimension.
3438             ((before, after),) yields same before and after statistic lengths
3439             for each dimension.
3440             (stat_length,) or int is a shortcut for before = after = statistic
3441             length for all axes.
3442             Default is ``None``, to use the entire axis.
3443         constant_values : scalar, tuple or mapping of the form {dim: tuple}
3444             Used in 'constant'.  The values to set the padded values for each
3445             axis.
3446             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3447             pad constants along each dimension.
3448             ``((before, after),)`` yields same before and after constants for each
3449             dimension.
3450             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3451             all dimensions.
3452             Default is 0.
3453         end_values : scalar, tuple or mapping of the form {dim: tuple}
3454             Used in 'linear_ramp'.  The values used for the ending value of the
3455             linear_ramp and that will form the edge of the padded array.
3456             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3457             end values along each dimension.
3458             ``((before, after),)`` yields same before and after end values for each
3459             axis.
3460             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3461             all axes.
3462             Default is 0.
3463         reflect_type : {'even', 'odd'}, optional
3464             Used in 'reflect', and 'symmetric'.  The 'even' style is the
3465             default with an unaltered reflection around the edge value.  For
3466             the 'odd' style, the extended part of the array is created by
3467             subtracting the reflected values from two times the edge value.
3468         **pad_width_kwargs:
3469             The keyword arguments form of ``pad_width``.
3470             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3471 
3472         Returns
3473         -------
3474         padded : DataArray
3475             DataArray with the padded coordinates and data.
3476 
3477         See also
3478         --------
3479         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3480 
3481         Notes
3482         -----
3483         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
3484         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
3485         specify ``constant_values=np.nan``
3486 
3487         Examples
3488         --------
3489 
3490         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0,1,2])])
3491         >>> arr.pad(x=(1,2), constant_values=0)
3492         <xarray.DataArray (x: 6)>
3493         array([0, 5, 6, 7, 0, 0])
3494         Coordinates:
3495           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3496 
3497         >>> da = xr.DataArray([[0,1,2,3], [10,11,12,13]],
3498                               dims=["x", "y"],
3499                               coords={"x": [0,1], "y": [10, 20 ,30, 40], "z": ("x", [100, 200])}
3500             )
3501         >>> da.pad(x=1)
3502         <xarray.DataArray (x: 4, y: 4)>
3503         array([[nan, nan, nan, nan],
3504                [ 0.,  1.,  2.,  3.],
3505                [10., 11., 12., 13.],
3506                [nan, nan, nan, nan]])
3507         Coordinates:
3508           * x        (x) float64 nan 0.0 1.0 nan
3509           * y        (y) int64 10 20 30 40
3510             z        (x) float64 nan 100.0 200.0 nan
3511         >>> da.pad(x=1, constant_values=np.nan)
3512         <xarray.DataArray (x: 4, y: 4)>
3513         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,
3514                 -9223372036854775808],
3515                [                   0,                    1,                    2,
3516                                    3],
3517                [                  10,                   11,                   12,
3518                                   13],
3519                [-9223372036854775808, -9223372036854775808, -9223372036854775808,
3520                 -9223372036854775808]])
3521         Coordinates:
3522           * x        (x) float64 nan 0.0 1.0 nan
3523           * y        (y) int64 10 20 30 40
3524             z        (x) float64 nan 100.0 200.0 nan
3525         """
3526         ds = self._to_temp_dataset().pad(
3527             pad_width=pad_width,
3528             mode=mode,
3529             stat_length=stat_length,
3530             constant_values=constant_values,
3531             end_values=end_values,
3532             reflect_type=reflect_type,
3533             **pad_width_kwargs,
3534         )
3535         return self._from_temp_dataset(ds)
3536 
3537     def idxmin(
3538         self,
3539         dim: Hashable = None,
3540         skipna: bool = None,
3541         fill_value: Any = dtypes.NA,
3542         keep_attrs: bool = None,
3543     ) -> "DataArray":
3544         """Return the coordinate label of the minimum value along a dimension.
3545 
3546         Returns a new `DataArray` named after the dimension with the values of
3547         the coordinate labels along that dimension corresponding to minimum
3548         values along that dimension.
3549 
3550         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3551         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3552 
3553         Parameters
3554         ----------
3555         dim : str, optional
3556             Dimension over which to apply `idxmin`.  This is optional for 1D
3557             arrays, but required for arrays with 2 or more dimensions.
3558         skipna : bool or None, default None
3559             If True, skip missing values (as marked by NaN). By default, only
3560             skips missing values for ``float``, ``complex``, and ``object``
3561             dtypes; other dtypes either do not have a sentinel missing value
3562             (``int``) or ``skipna=True`` has not been implemented
3563             (``datetime64`` or ``timedelta64``).
3564         fill_value : Any, default NaN
3565             Value to be filled in case all of the values along a dimension are
3566             null.  By default this is NaN.  The fill value and result are
3567             automatically converted to a compatible dtype if possible.
3568             Ignored if ``skipna`` is False.
3569         keep_attrs : bool, default False
3570             If True, the attributes (``attrs``) will be copied from the
3571             original object to the new one.  If False (default), the new object
3572             will be returned without attributes.
3573 
3574         Returns
3575         -------
3576         reduced : DataArray
3577             New `DataArray` object with `idxmin` applied to its data and the
3578             indicated dimension removed.
3579 
3580         See also
3581         --------
3582         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3583 
3584         Examples
3585         --------
3586 
3587         >>> array = xr.DataArray([0, 2, 1, 0, -2], dims="x",
3588         ...                      coords={"x": ['a', 'b', 'c', 'd', 'e']})
3589         >>> array.min()
3590         <xarray.DataArray ()>
3591         array(-2)
3592         >>> array.argmin()
3593         <xarray.DataArray ()>
3594         array(4)
3595         >>> array.idxmin()
3596         <xarray.DataArray 'x' ()>
3597         array('e', dtype='<U1')
3598 
3599         >>> array = xr.DataArray([[2.0, 1.0, 2.0, 0.0, -2.0],
3600         ...                       [-4.0, np.NaN, 2.0, np.NaN, -2.0],
3601         ...                       [np.NaN, np.NaN, 1., np.NaN, np.NaN]],
3602         ...                      dims=["y", "x"],
3603         ...                      coords={"y": [-1, 0, 1],
3604         ...                              "x": np.arange(5.)**2}
3605         ...                      )
3606         >>> array.min(dim="x")
3607         <xarray.DataArray (y: 3)>
3608         array([-2., -4.,  1.])
3609         Coordinates:
3610           * y        (y) int64 -1 0 1
3611         >>> array.argmin(dim="x")
3612         <xarray.DataArray (y: 3)>
3613         array([4, 0, 2])
3614         Coordinates:
3615           * y        (y) int64 -1 0 1
3616         >>> array.idxmin(dim="x")
3617         <xarray.DataArray 'x' (y: 3)>
3618         array([16.,  0.,  4.])
3619         Coordinates:
3620           * y        (y) int64 -1 0 1
3621         """
3622         return computation._calc_idxminmax(
3623             array=self,
3624             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
3625             dim=dim,
3626             skipna=skipna,
3627             fill_value=fill_value,
3628             keep_attrs=keep_attrs,
3629         )
3630 
3631     def idxmax(
3632         self,
3633         dim: Hashable = None,
3634         skipna: bool = None,
3635         fill_value: Any = dtypes.NA,
3636         keep_attrs: bool = None,
3637     ) -> "DataArray":
3638         """Return the coordinate label of the maximum value along a dimension.
3639 
3640         Returns a new `DataArray` named after the dimension with the values of
3641         the coordinate labels along that dimension corresponding to maximum
3642         values along that dimension.
3643 
3644         In comparison to :py:meth:`~DataArray.argmax`, this returns the
3645         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
3646 
3647         Parameters
3648         ----------
3649         dim : str, optional
3650             Dimension over which to apply `idxmax`.  This is optional for 1D
3651             arrays, but required for arrays with 2 or more dimensions.
3652         skipna : bool or None, default None
3653             If True, skip missing values (as marked by NaN). By default, only
3654             skips missing values for ``float``, ``complex``, and ``object``
3655             dtypes; other dtypes either do not have a sentinel missing value
3656             (``int``) or ``skipna=True`` has not been implemented
3657             (``datetime64`` or ``timedelta64``).
3658         fill_value : Any, default NaN
3659             Value to be filled in case all of the values along a dimension are
3660             null.  By default this is NaN.  The fill value and result are
3661             automatically converted to a compatible dtype if possible.
3662             Ignored if ``skipna`` is False.
3663         keep_attrs : bool, default False
3664             If True, the attributes (``attrs``) will be copied from the
3665             original object to the new one.  If False (default), the new object
3666             will be returned without attributes.
3667 
3668         Returns
3669         -------
3670         reduced : DataArray
3671             New `DataArray` object with `idxmax` applied to its data and the
3672             indicated dimension removed.
3673 
3674         See also
3675         --------
3676         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
3677 
3678         Examples
3679         --------
3680 
3681         >>> array = xr.DataArray([0, 2, 1, 0, -2], dims="x",
3682         ...                      coords={"x": ['a', 'b', 'c', 'd', 'e']})
3683         >>> array.max()
3684         <xarray.DataArray ()>
3685         array(2)
3686         >>> array.argmax()
3687         <xarray.DataArray ()>
3688         array(1)
3689         >>> array.idxmax()
3690         <xarray.DataArray 'x' ()>
3691         array('b', dtype='<U1')
3692 
3693         >>> array = xr.DataArray([[2.0, 1.0, 2.0, 0.0, -2.0],
3694         ...                       [-4.0, np.NaN, 2.0, np.NaN, -2.0],
3695         ...                       [np.NaN, np.NaN, 1., np.NaN, np.NaN]],
3696         ...                      dims=["y", "x"],
3697         ...                      coords={"y": [-1, 0, 1],
3698         ...                              "x": np.arange(5.)**2}
3699         ...                      )
3700         >>> array.max(dim="x")
3701         <xarray.DataArray (y: 3)>
3702         array([2., 2., 1.])
3703         Coordinates:
3704           * y        (y) int64 -1 0 1
3705         >>> array.argmax(dim="x")
3706         <xarray.DataArray (y: 3)>
3707         array([0, 2, 2])
3708         Coordinates:
3709           * y        (y) int64 -1 0 1
3710         >>> array.idxmax(dim="x")
3711         <xarray.DataArray 'x' (y: 3)>
3712         array([0., 4., 4.])
3713         Coordinates:
3714           * y        (y) int64 -1 0 1
3715         """
3716         return computation._calc_idxminmax(
3717             array=self,
3718             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
3719             dim=dim,
3720             skipna=skipna,
3721             fill_value=fill_value,
3722             keep_attrs=keep_attrs,
3723         )
3724 
3725     # this needs to be at the end, or mypy will confuse with `str`
3726     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
3727     str = property(StringAccessor)

</code><intended_behavior>Since the direct search for `__iadd__` yielded no results, it's plausible to infer that handling for the in-place addition operation (`+=`) might not be explicitly defined or is handled through more general mechanisms that do not specifically account for alignment issues. It may be necessary to implement or modify the `__iadd__` method (or the mechanism that indirectly handles `+=`) to ensure that it includes a step for aligning arrays before attempting the in-place merge or addition. This step should align the coordinates of the two `DataArray` objects, similarly to how coordinates would be aligned in an out-of-place addition, thereby ensuring that `A += B` is functionally equivalent to `A = A + B` with respect to coordinate alignment and merging. This could involve utilizing existing alignment mechanisms or developing new logic tailored for in-place operations that ensures compatibility between the array coordinates before proceeding with the operation.</intended_behavior>]
2025-10-15 21:39:25.350 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 21:39:25.350 | INFO     | Search-only: skipping patch selection
2025-10-15 21:39:25.436 | INFO     | Task pydata__xarray-3976 completed successfully.
2025-10-15 21:39:25.437 | INFO     | No patch generated. You can try running ACR again.
