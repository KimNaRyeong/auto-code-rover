2025-10-16 00:03:48.605 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-16 00:03:52.654 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=408, output_tokens=12, cost=0.004440
2025-10-16 00:03:52.655 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-16 00:03:52.659 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-16 00:03:57.437 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=717, output_tokens=183, cost=0.012660
2025-10-16 00:03:57.460 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-16 00:03:59.194 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=537, output_tokens=32, cost=0.006330
2025-10-16 00:03:59.195 | DEBUG    | Extracted a valid json.
2025-10-16 00:03:59.195 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_class(\"IterativeImputer\")", "search_class(\"SimpleImputer\")"],
    "bug_locations": []
}
2025-10-16 00:03:59.209 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-16 00:03:59.209 | DEBUG    | <Agent search> Analyzing search results.
2025-10-16 00:04:21.150 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1556, output_tokens=639, cost=0.034730
2025-10-16 00:04:21.165 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-16 00:04:31.966 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2411, output_tokens=362, cost=0.034970
2025-10-16 00:04:31.974 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-16 00:04:42.430 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=716, output_tokens=367, cost=0.018170
2025-10-16 00:04:42.431 | DEBUG    | Extracted a valid json.
2025-10-16 00:04:42.431 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/impute/_iterative.py",
            "class": "IterativeImputer",
            "method": "__init__",
            "intended_behavior": "The constructor of the `IterativeImputer` class should be modified to include a new parameter `fill_value`, which will be used when the `initial_strategy` is set to \"constant\". This parameter should allow any scalar value or `np.nan`, providing a default that maintains the current behavior when not explicitly set by the user. This change allows users to specify a custom initialization value for missing values, enhancing the flexibility of `IterativeImputer`. Existing functionality and default behavior of the class should remain unchanged when the `fill_value` parameter is not utilized (i.e., when `initial_strategy` is not \"constant\")."
        },
        {
            "file": "sklearn/impute/_iterative.py",
            "class": "IterativeImputer",
            "method": "_initial_imputation",
            "intended_behavior": "This method should be updated to utilize the `fill_value` parameter when performing the initial imputation with strategy \"constant\". The implementation should check if `initial_strategy` is \"constant\" and, if so, use the `fill_value` for replacing missing values. This adjustment ensures that `fill_value` directly influences the imputation process only in the context it's meant to (i.e., with \"constant\" strategy), without affecting other imputation strategies. The intended behavior is to provide a seamless way to customize initial missing values handling, which could be especially beneficial for datasets where a zero, the mean, or median value might not be the most appropriate initial guess."
        }
    ]
}
2025-10-16 00:04:42.437 | DEBUG    | Found override of __init__ in _BaseImputer
2025-10-16 00:04:42.437 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7fd54d722c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7fd45b357080>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7fd45d67e3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7fd45b357080>, <app.raw_tasks.RawSweTask object at 0x7fd45b3570b0>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7fd45d67e480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7fd45b28f440>
    └ <function run_task_in_subprocess at 0x7fd45d67e660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7fd45b28f440>
    │        │      └ <function run_raw_task at 0x7fd45d67e700>
    │        └ <function ProcessPoolExecutor.submit at 0x7fd54d336020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7fd54d335da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7fd54d335ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7fd54d335f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7fd54d4fcea0>
    └ <ForkProcess name='ForkProcess-365' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-365' parent=188003 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7fd54d56e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-365' parent=188003 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-365' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-365' parent=188003 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-365' parent=188003 started>
    │    └ <function Popen._launch at 0x7fd45b6737e0>
    └ <multiprocessing.popen_fork.Popen object at 0x7fd45c0ad6d0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7fd54d4fd8a0>
           └ <ForkProcess name='ForkProcess-365' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7fd54d4fce00>
    └ <ForkProcess name='ForkProcess-365' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-365' parent=188003 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7fd45bb80050>, <multiprocessing.queues.SimpleQueue object at 0x7fd45e5e9a...
    │    │        └ <ForkProcess name='ForkProcess-365' parent=188003 started>
    │    └ <function _process_worker at 0x7fd54d3351c0>
    └ <ForkProcess name='ForkProcess-365' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7fd45c963aa0>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>
        │         └ <function run_raw_task at 0x7fd45d67e700>
        └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output2/scikit-learn__scikit-learn-25232_2025-10-16_00-03-48'
             │            │    └ <function RawSweTask.to_task at 0x7fd45d67d6c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x7fd45c963aa0>
             └ <function do_inference at 0x7fd45d67e8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7fd45d8e0400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7fd45dd084a0>
       │                 │         │            └ SweTask(task_id='scikit-learn__scikit-learn-25232', problem_statement='IterativeImputer has no parameter "fill_value"\n### De...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7fd54c9f17f0>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output2/scikit-learn__scikit-learn-25232_2025-10-16_00-03-48/output_0')
       └ <function _run_one_task at 0x7fd45d8e18a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7fd45d995bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7fd45d827050>
                                  └ <app.manage.ProjectApiManager object at 0x7fd54c9f17f0>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'sklearn/impute/_iterative.py', 'class': 'IterativeImputer', 'method': '__init__', 'intended_behavior': 'The constru...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x7fd45d9958a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x7fd45d89a600>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x7fd45d827050>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ '__init__'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    │                      │   └ 'IterativeImputer'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    └ <function SearchBackend._get_inherited_methods at 0x7fd45d995800>
    └ <app.search.search_backend.SearchBackend object at 0x7fd45d89a600>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': '_BaseImputer', 'method_name': '__init__'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x7fd45d994f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x7fd45d89a600>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7fd45d89a600>, {'class_name': '_BaseImputer', 'method_name': '__init__'})
           └ <function SearchBackend.search_method_in_class at 0x7fd45d994ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-16 00:04:42.447 | DEBUG    | Bug location extracted successfully: [<file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>__init__</method>
<code>
294     def __init__(
295         self,
296         estimator=None,
297         *,
298         missing_values=np.nan,
299         sample_posterior=False,
300         max_iter=10,
301         tol=1e-3,
302         n_nearest_features=None,
303         initial_strategy="mean",
304         imputation_order="ascending",
305         skip_complete=False,
306         min_value=-np.inf,
307         max_value=np.inf,
308         verbose=0,
309         random_state=None,
310         add_indicator=False,
311         keep_empty_features=False,
312     ):
313         super().__init__(
314             missing_values=missing_values,
315             add_indicator=add_indicator,
316             keep_empty_features=keep_empty_features,
317         )
318 
319         self.estimator = estimator
320         self.sample_posterior = sample_posterior
321         self.max_iter = max_iter
322         self.tol = tol
323         self.n_nearest_features = n_nearest_features
324         self.initial_strategy = initial_strategy
325         self.imputation_order = imputation_order
326         self.skip_complete = skip_complete
327         self.min_value = min_value
328         self.max_value = max_value
329         self.verbose = verbose
330         self.random_state = random_state

</code><intended_behavior>The constructor of the `IterativeImputer` class should be modified to include a new parameter `fill_value`, which will be used when the `initial_strategy` is set to "constant". This parameter should allow any scalar value or `np.nan`, providing a default that maintains the current behavior when not explicitly set by the user. This change allows users to specify a custom initialization value for missing values, enhancing the flexibility of `IterativeImputer`. Existing functionality and default behavior of the class should remain unchanged when the `fill_value` parameter is not utilized (i.e., when `initial_strategy` is not "constant").</intended_behavior>, <file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>None</method>
<code>
54 class IterativeImputer(_BaseImputer):
55     """Multivariate imputer that estimates each feature from all the others.
56 
57     A strategy for imputing missing values by modeling each feature with
58     missing values as a function of other features in a round-robin fashion.
59 
60     Read more in the :ref:`User Guide <iterative_imputer>`.
61 
62     .. versionadded:: 0.21
63 
64     .. note::
65 
66       This estimator is still **experimental** for now: the predictions
67       and the API might change without any deprecation cycle. To use it,
68       you need to explicitly import `enable_iterative_imputer`::
69 
70         >>> # explicitly require this experimental feature
71         >>> from sklearn.experimental import enable_iterative_imputer  # noqa
72         >>> # now you can import normally from sklearn.impute
73         >>> from sklearn.impute import IterativeImputer
74 
75     Parameters
76     ----------
77     estimator : estimator object, default=BayesianRidge()
78         The estimator to use at each step of the round-robin imputation.
79         If `sample_posterior=True`, the estimator must support
80         `return_std` in its `predict` method.
81 
82     missing_values : int or np.nan, default=np.nan
83         The placeholder for the missing values. All occurrences of
84         `missing_values` will be imputed. For pandas' dataframes with
85         nullable integer dtypes with missing values, `missing_values`
86         should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.
87 
88     sample_posterior : bool, default=False
89         Whether to sample from the (Gaussian) predictive posterior of the
90         fitted estimator for each imputation. Estimator must support
91         `return_std` in its `predict` method if set to `True`. Set to
92         `True` if using `IterativeImputer` for multiple imputations.
93 
94     max_iter : int, default=10
95         Maximum number of imputation rounds to perform before returning the
96         imputations computed during the final round. A round is a single
97         imputation of each feature with missing values. The stopping criterion
98         is met once `max(abs(X_t - X_{t-1}))/max(abs(X[known_vals])) < tol`,
99         where `X_t` is `X` at iteration `t`. Note that early stopping is only
100         applied if `sample_posterior=False`.
101 
102     tol : float, default=1e-3
103         Tolerance of the stopping condition.
104 
105     n_nearest_features : int, default=None
106         Number of other features to use to estimate the missing values of
107         each feature column. Nearness between features is measured using
108         the absolute correlation coefficient between each feature pair (after
109         initial imputation). To ensure coverage of features throughout the
110         imputation process, the neighbor features are not necessarily nearest,
111         but are drawn with probability proportional to correlation for each
112         imputed target feature. Can provide significant speed-up when the
113         number of features is huge. If `None`, all features will be used.
114 
115     initial_strategy : {'mean', 'median', 'most_frequent', 'constant'}, \
116             default='mean'
117         Which strategy to use to initialize the missing values. Same as the
118         `strategy` parameter in :class:`~sklearn.impute.SimpleImputer`.
119 
120     imputation_order : {'ascending', 'descending', 'roman', 'arabic', \
121             'random'}, default='ascending'
122         The order in which the features will be imputed. Possible values:
123 
124         - `'ascending'`: From features with fewest missing values to most.
125         - `'descending'`: From features with most missing values to fewest.
126         - `'roman'`: Left to right.
127         - `'arabic'`: Right to left.
128         - `'random'`: A random order for each round.
129 
130     skip_complete : bool, default=False
131         If `True` then features with missing values during :meth:`transform`
132         which did not have any missing values during :meth:`fit` will be
133         imputed with the initial imputation method only. Set to `True` if you
134         have many features with no missing values at both :meth:`fit` and
135         :meth:`transform` time to save compute.
136 
137     min_value : float or array-like of shape (n_features,), default=-np.inf
138         Minimum possible imputed value. Broadcast to shape `(n_features,)` if
139         scalar. If array-like, expects shape `(n_features,)`, one min value for
140         each feature. The default is `-np.inf`.
141 
142         .. versionchanged:: 0.23
143            Added support for array-like.
144 
145     max_value : float or array-like of shape (n_features,), default=np.inf
146         Maximum possible imputed value. Broadcast to shape `(n_features,)` if
147         scalar. If array-like, expects shape `(n_features,)`, one max value for
148         each feature. The default is `np.inf`.
149 
150         .. versionchanged:: 0.23
151            Added support for array-like.
152 
153     verbose : int, default=0
154         Verbosity flag, controls the debug messages that are issued
155         as functions are evaluated. The higher, the more verbose. Can be 0, 1,
156         or 2.
157 
158     random_state : int, RandomState instance or None, default=None
159         The seed of the pseudo random number generator to use. Randomizes
160         selection of estimator features if `n_nearest_features` is not `None`,
161         the `imputation_order` if `random`, and the sampling from posterior if
162         `sample_posterior=True`. Use an integer for determinism.
163         See :term:`the Glossary <random_state>`.
164 
165     add_indicator : bool, default=False
166         If `True`, a :class:`MissingIndicator` transform will stack onto output
167         of the imputer's transform. This allows a predictive estimator
168         to account for missingness despite imputation. If a feature has no
169         missing values at fit/train time, the feature won't appear on
170         the missing indicator even if there are missing values at
171         transform/test time.
172 
173     keep_empty_features : bool, default=False
174         If True, features that consist exclusively of missing values when
175         `fit` is called are returned in results when `transform` is called.
176         The imputed value is always `0` except when
177         `initial_strategy="constant"` in which case `fill_value` will be
178         used instead.
179 
180         .. versionadded:: 1.2
181 
182     Attributes
183     ----------
184     initial_imputer_ : object of type :class:`~sklearn.impute.SimpleImputer`
185         Imputer used to initialize the missing values.
186 
187     imputation_sequence_ : list of tuples
188         Each tuple has `(feat_idx, neighbor_feat_idx, estimator)`, where
189         `feat_idx` is the current feature to be imputed,
190         `neighbor_feat_idx` is the array of other features used to impute the
191         current feature, and `estimator` is the trained estimator used for
192         the imputation. Length is `self.n_features_with_missing_ *
193         self.n_iter_`.
194 
195     n_iter_ : int
196         Number of iteration rounds that occurred. Will be less than
197         `self.max_iter` if early stopping criterion was reached.
198 
199     n_features_in_ : int
200         Number of features seen during :term:`fit`.
201 
202         .. versionadded:: 0.24
203 
204     feature_names_in_ : ndarray of shape (`n_features_in_`,)
205         Names of features seen during :term:`fit`. Defined only when `X`
206         has feature names that are all strings.
207 
208         .. versionadded:: 1.0
209 
210     n_features_with_missing_ : int
211         Number of features with missing values.
212 
213     indicator_ : :class:`~sklearn.impute.MissingIndicator`
214         Indicator used to add binary indicators for missing values.
215         `None` if `add_indicator=False`.
216 
217     random_state_ : RandomState instance
218         RandomState instance that is generated either from a seed, the random
219         number generator or by `np.random`.
220 
221     See Also
222     --------
223     SimpleImputer : Univariate imputer for completing missing values
224         with simple strategies.
225     KNNImputer : Multivariate imputer that estimates missing features using
226         nearest samples.
227 
228     Notes
229     -----
230     To support imputation in inductive mode we store each feature's estimator
231     during the :meth:`fit` phase, and predict without refitting (in order)
232     during the :meth:`transform` phase.
233 
234     Features which contain all missing values at :meth:`fit` are discarded upon
235     :meth:`transform`.
236 
237     Using defaults, the imputer scales in :math:`\\mathcal{O}(knp^3\\min(n,p))`
238     where :math:`k` = `max_iter`, :math:`n` the number of samples and
239     :math:`p` the number of features. It thus becomes prohibitively costly when
240     the number of features increases. Setting
241     `n_nearest_features << n_features`, `skip_complete=True` or increasing `tol`
242     can help to reduce its computational cost.
243 
244     Depending on the nature of missing values, simple imputers can be
245     preferable in a prediction context.
246 
247     References
248     ----------
249     .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice:
250         Multivariate Imputation by Chained Equations in R". Journal of
251         Statistical Software 45: 1-67.
252         <https://www.jstatsoft.org/article/view/v045i03>`_
253 
254     .. [2] `S. F. Buck, (1960). "A Method of Estimation of Missing Values in
255         Multivariate Data Suitable for use with an Electronic Computer".
256         Journal of the Royal Statistical Society 22(2): 302-306.
257         <https://www.jstor.org/stable/2984099>`_
258 
259     Examples
260     --------
261     >>> import numpy as np
262     >>> from sklearn.experimental import enable_iterative_imputer
263     >>> from sklearn.impute import IterativeImputer
264     >>> imp_mean = IterativeImputer(random_state=0)
265     >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
266     IterativeImputer(random_state=0)
267     >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
268     >>> imp_mean.transform(X)
269     array([[ 6.9584...,  2.       ,  3.        ],
270            [ 4.       ,  2.6000...,  6.        ],
271            [10.       ,  4.9999...,  9.        ]])
272     """
273 
274     _parameter_constraints: dict = {
275         **_BaseImputer._parameter_constraints,
276         "estimator": [None, HasMethods(["fit", "predict"])],
277         "sample_posterior": ["boolean"],
278         "max_iter": [Interval(Integral, 0, None, closed="left")],
279         "tol": [Interval(Real, 0, None, closed="left")],
280         "n_nearest_features": [None, Interval(Integral, 1, None, closed="left")],
281         "initial_strategy": [
282             StrOptions({"mean", "median", "most_frequent", "constant"})
283         ],
284         "imputation_order": [
285             StrOptions({"ascending", "descending", "roman", "arabic", "random"})
286         ],
287         "skip_complete": ["boolean"],
288         "min_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
289         "max_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
290         "verbose": ["verbose"],
291         "random_state": ["random_state"],
292     }
293 
294     def __init__(
295         self,
296         estimator=None,
297         *,
298         missing_values=np.nan,
299         sample_posterior=False,
300         max_iter=10,
301         tol=1e-3,
302         n_nearest_features=None,
303         initial_strategy="mean",
304         imputation_order="ascending",
305         skip_complete=False,
306         min_value=-np.inf,
307         max_value=np.inf,
308         verbose=0,
309         random_state=None,
310         add_indicator=False,
311         keep_empty_features=False,
312     ):
313         super().__init__(
314             missing_values=missing_values,
315             add_indicator=add_indicator,
316             keep_empty_features=keep_empty_features,
317         )
318 
319         self.estimator = estimator
320         self.sample_posterior = sample_posterior
321         self.max_iter = max_iter
322         self.tol = tol
323         self.n_nearest_features = n_nearest_features
324         self.initial_strategy = initial_strategy
325         self.imputation_order = imputation_order
326         self.skip_complete = skip_complete
327         self.min_value = min_value
328         self.max_value = max_value
329         self.verbose = verbose
330         self.random_state = random_state
331 
332     def _impute_one_feature(
333         self,
334         X_filled,
335         mask_missing_values,
336         feat_idx,
337         neighbor_feat_idx,
338         estimator=None,
339         fit_mode=True,
340     ):
341         """Impute a single feature from the others provided.
342 
343         This function predicts the missing values of one of the features using
344         the current estimates of all the other features. The `estimator` must
345         support `return_std=True` in its `predict` method for this function
346         to work.
347 
348         Parameters
349         ----------
350         X_filled : ndarray
351             Input data with the most recent imputations.
352 
353         mask_missing_values : ndarray
354             Input data's missing indicator matrix.
355 
356         feat_idx : int
357             Index of the feature currently being imputed.
358 
359         neighbor_feat_idx : ndarray
360             Indices of the features to be used in imputing `feat_idx`.
361 
362         estimator : object
363             The estimator to use at this step of the round-robin imputation.
364             If `sample_posterior=True`, the estimator must support
365             `return_std` in its `predict` method.
366             If None, it will be cloned from self._estimator.
367 
368         fit_mode : boolean, default=True
369             Whether to fit and predict with the estimator or just predict.
370 
371         Returns
372         -------
373         X_filled : ndarray
374             Input data with `X_filled[missing_row_mask, feat_idx]` updated.
375 
376         estimator : estimator with sklearn API
377             The fitted estimator used to impute
378             `X_filled[missing_row_mask, feat_idx]`.
379         """
380         if estimator is None and fit_mode is False:
381             raise ValueError(
382                 "If fit_mode is False, then an already-fitted "
383                 "estimator should be passed in."
384             )
385 
386         if estimator is None:
387             estimator = clone(self._estimator)
388 
389         missing_row_mask = mask_missing_values[:, feat_idx]
390         if fit_mode:
391             X_train = _safe_indexing(
392                 _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
393                 ~missing_row_mask,
394                 axis=0,
395             )
396             y_train = _safe_indexing(
397                 _safe_indexing(X_filled, feat_idx, axis=1),
398                 ~missing_row_mask,
399                 axis=0,
400             )
401             estimator.fit(X_train, y_train)
402 
403         # if no missing values, don't predict
404         if np.sum(missing_row_mask) == 0:
405             return X_filled, estimator
406 
407         # get posterior samples if there is at least one missing value
408         X_test = _safe_indexing(
409             _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
410             missing_row_mask,
411             axis=0,
412         )
413         if self.sample_posterior:
414             mus, sigmas = estimator.predict(X_test, return_std=True)
415             imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
416             # two types of problems: (1) non-positive sigmas
417             # (2) mus outside legal range of min_value and max_value
418             # (results in inf sample)
419             positive_sigmas = sigmas > 0
420             imputed_values[~positive_sigmas] = mus[~positive_sigmas]
421             mus_too_low = mus < self._min_value[feat_idx]
422             imputed_values[mus_too_low] = self._min_value[feat_idx]
423             mus_too_high = mus > self._max_value[feat_idx]
424             imputed_values[mus_too_high] = self._max_value[feat_idx]
425             # the rest can be sampled without statistical issues
426             inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high
427             mus = mus[inrange_mask]
428             sigmas = sigmas[inrange_mask]
429             a = (self._min_value[feat_idx] - mus) / sigmas
430             b = (self._max_value[feat_idx] - mus) / sigmas
431 
432             truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
433             imputed_values[inrange_mask] = truncated_normal.rvs(
434                 random_state=self.random_state_
435             )
436         else:
437             imputed_values = estimator.predict(X_test)
438             imputed_values = np.clip(
439                 imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]
440             )
441 
442         # update the feature
443         _safe_assign(
444             X_filled,
445             imputed_values,
446             row_indexer=missing_row_mask,
447             column_indexer=feat_idx,
448         )
449         return X_filled, estimator
450 
451     def _get_neighbor_feat_idx(self, n_features, feat_idx, abs_corr_mat):
452         """Get a list of other features to predict `feat_idx`.
453 
454         If `self.n_nearest_features` is less than or equal to the total
455         number of features, then use a probability proportional to the absolute
456         correlation between `feat_idx` and each other feature to randomly
457         choose a subsample of the other features (without replacement).
458 
459         Parameters
460         ----------
461         n_features : int
462             Number of features in `X`.
463 
464         feat_idx : int
465             Index of the feature currently being imputed.
466 
467         abs_corr_mat : ndarray, shape (n_features, n_features)
468             Absolute correlation matrix of `X`. The diagonal has been zeroed
469             out and each feature has been normalized to sum to 1. Can be None.
470 
471         Returns
472         -------
473         neighbor_feat_idx : array-like
474             The features to use to impute `feat_idx`.
475         """
476         if self.n_nearest_features is not None and self.n_nearest_features < n_features:
477             p = abs_corr_mat[:, feat_idx]
478             neighbor_feat_idx = self.random_state_.choice(
479                 np.arange(n_features), self.n_nearest_features, replace=False, p=p
480             )
481         else:
482             inds_left = np.arange(feat_idx)
483             inds_right = np.arange(feat_idx + 1, n_features)
484             neighbor_feat_idx = np.concatenate((inds_left, inds_right))
485         return neighbor_feat_idx
486 
487     def _get_ordered_idx(self, mask_missing_values):
488         """Decide in what order we will update the features.
489 
490         As a homage to the MICE R package, we will have 4 main options of
491         how to order the updates, and use a random order if anything else
492         is specified.
493 
494         Also, this function skips features which have no missing values.
495 
496         Parameters
497         ----------
498         mask_missing_values : array-like, shape (n_samples, n_features)
499             Input data's missing indicator matrix, where `n_samples` is the
500             number of samples and `n_features` is the number of features.
501 
502         Returns
503         -------
504         ordered_idx : ndarray, shape (n_features,)
505             The order in which to impute the features.
506         """
507         frac_of_missing_values = mask_missing_values.mean(axis=0)
508         if self.skip_complete:
509             missing_values_idx = np.flatnonzero(frac_of_missing_values)
510         else:
511             missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])
512         if self.imputation_order == "roman":
513             ordered_idx = missing_values_idx
514         elif self.imputation_order == "arabic":
515             ordered_idx = missing_values_idx[::-1]
516         elif self.imputation_order == "ascending":
517             n = len(frac_of_missing_values) - len(missing_values_idx)
518             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:]
519         elif self.imputation_order == "descending":
520             n = len(frac_of_missing_values) - len(missing_values_idx)
521             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:][::-1]
522         elif self.imputation_order == "random":
523             ordered_idx = missing_values_idx
524             self.random_state_.shuffle(ordered_idx)
525         return ordered_idx
526 
527     def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
528         """Get absolute correlation matrix between features.
529 
530         Parameters
531         ----------
532         X_filled : ndarray, shape (n_samples, n_features)
533             Input data with the most recent imputations.
534 
535         tolerance : float, default=1e-6
536             `abs_corr_mat` can have nans, which will be replaced
537             with `tolerance`.
538 
539         Returns
540         -------
541         abs_corr_mat : ndarray, shape (n_features, n_features)
542             Absolute correlation matrix of `X` at the beginning of the
543             current round. The diagonal has been zeroed out and each feature's
544             absolute correlations with all others have been normalized to sum
545             to 1.
546         """
547         n_features = X_filled.shape[1]
548         if self.n_nearest_features is None or self.n_nearest_features >= n_features:
549             return None
550         with np.errstate(invalid="ignore"):
551             # if a feature in the neighborhood has only a single value
552             # (e.g., categorical feature), the std. dev. will be null and
553             # np.corrcoef will raise a warning due to a division by zero
554             abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
555         # np.corrcoef is not defined for features with zero std
556         abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
557         # ensures exploration, i.e. at least some probability of sampling
558         np.clip(abs_corr_mat, tolerance, None, out=abs_corr_mat)
559         # features are not their own neighbors
560         np.fill_diagonal(abs_corr_mat, 0)
561         # needs to sum to 1 for np.random.choice sampling
562         abs_corr_mat = normalize(abs_corr_mat, norm="l1", axis=0, copy=False)
563         return abs_corr_mat
564 
565     def _initial_imputation(self, X, in_fit=False):
566         """Perform initial imputation for input `X`.
567 
568         Parameters
569         ----------
570         X : ndarray of shape (n_samples, n_features)
571             Input data, where `n_samples` is the number of samples and
572             `n_features` is the number of features.
573 
574         in_fit : bool, default=False
575             Whether function is called in :meth:`fit`.
576 
577         Returns
578         -------
579         Xt : ndarray of shape (n_samples, n_features)
580             Input data, where `n_samples` is the number of samples and
581             `n_features` is the number of features.
582 
583         X_filled : ndarray of shape (n_samples, n_features)
584             Input data with the most recent imputations.
585 
586         mask_missing_values : ndarray of shape (n_samples, n_features)
587             Input data's missing indicator matrix, where `n_samples` is the
588             number of samples and `n_features` is the number of features,
589             masked by non-missing features.
590 
591         X_missing_mask : ndarray, shape (n_samples, n_features)
592             Input data's mask matrix indicating missing datapoints, where
593             `n_samples` is the number of samples and `n_features` is the
594             number of features.
595         """
596         if is_scalar_nan(self.missing_values):
597             force_all_finite = "allow-nan"
598         else:
599             force_all_finite = True
600 
601         X = self._validate_data(
602             X,
603             dtype=FLOAT_DTYPES,
604             order="F",
605             reset=in_fit,
606             force_all_finite=force_all_finite,
607         )
608         _check_inputs_dtype(X, self.missing_values)
609 
610         X_missing_mask = _get_mask(X, self.missing_values)
611         mask_missing_values = X_missing_mask.copy()
612         if self.initial_imputer_ is None:
613             self.initial_imputer_ = SimpleImputer(
614                 missing_values=self.missing_values,
615                 strategy=self.initial_strategy,
616                 keep_empty_features=self.keep_empty_features,
617             )
618             X_filled = self.initial_imputer_.fit_transform(X)
619         else:
620             X_filled = self.initial_imputer_.transform(X)
621 
622         valid_mask = np.flatnonzero(
623             np.logical_not(np.isnan(self.initial_imputer_.statistics_))
624         )
625 
626         if not self.keep_empty_features:
627             # drop empty features
628             Xt = X[:, valid_mask]
629             mask_missing_values = mask_missing_values[:, valid_mask]
630         else:
631             # mark empty features as not missing and keep the original
632             # imputation
633             mask_missing_values[:, valid_mask] = True
634             Xt = X
635 
636         return Xt, X_filled, mask_missing_values, X_missing_mask
637 
638     @staticmethod
639     def _validate_limit(limit, limit_type, n_features):
640         """Validate the limits (min/max) of the feature values.
641 
642         Converts scalar min/max limits to vectors of shape `(n_features,)`.
643 
644         Parameters
645         ----------
646         limit: scalar or array-like
647             The user-specified limit (i.e, min_value or max_value).
648         limit_type: {'max', 'min'}
649             Type of limit to validate.
650         n_features: int
651             Number of features in the dataset.
652 
653         Returns
654         -------
655         limit: ndarray, shape(n_features,)
656             Array of limits, one for each feature.
657         """
658         limit_bound = np.inf if limit_type == "max" else -np.inf
659         limit = limit_bound if limit is None else limit
660         if np.isscalar(limit):
661             limit = np.full(n_features, limit)
662         limit = check_array(limit, force_all_finite=False, copy=False, ensure_2d=False)
663         if not limit.shape[0] == n_features:
664             raise ValueError(
665                 f"'{limit_type}_value' should be of "
666                 f"shape ({n_features},) when an array-like "
667                 f"is provided. Got {limit.shape}, instead."
668             )
669         return limit
670 
671     def fit_transform(self, X, y=None):
672         """Fit the imputer on `X` and return the transformed `X`.
673 
674         Parameters
675         ----------
676         X : array-like, shape (n_samples, n_features)
677             Input data, where `n_samples` is the number of samples and
678             `n_features` is the number of features.
679 
680         y : Ignored
681             Not used, present for API consistency by convention.
682 
683         Returns
684         -------
685         Xt : array-like, shape (n_samples, n_features)
686             The imputed input data.
687         """
688         self._validate_params()
689         self.random_state_ = getattr(
690             self, "random_state_", check_random_state(self.random_state)
691         )
692 
693         if self.estimator is None:
694             from ..linear_model import BayesianRidge
695 
696             self._estimator = BayesianRidge()
697         else:
698             self._estimator = clone(self.estimator)
699 
700         self.imputation_sequence_ = []
701 
702         self.initial_imputer_ = None
703 
704         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
705             X, in_fit=True
706         )
707 
708         super()._fit_indicator(complete_mask)
709         X_indicator = super()._transform_indicator(complete_mask)
710 
711         if self.max_iter == 0 or np.all(mask_missing_values):
712             self.n_iter_ = 0
713             return super()._concatenate_indicator(Xt, X_indicator)
714 
715         # Edge case: a single feature. We return the initial ...
716         if Xt.shape[1] == 1:
717             self.n_iter_ = 0
718             return super()._concatenate_indicator(Xt, X_indicator)
719 
720         self._min_value = self._validate_limit(self.min_value, "min", X.shape[1])
721         self._max_value = self._validate_limit(self.max_value, "max", X.shape[1])
722 
723         if not np.all(np.greater(self._max_value, self._min_value)):
724             raise ValueError("One (or more) features have min_value >= max_value.")
725 
726         # order in which to impute
727         # note this is probably too slow for large feature data (d > 100000)
728         # and a better way would be good.
729         # see: https://goo.gl/KyCNwj and subsequent comments
730         ordered_idx = self._get_ordered_idx(mask_missing_values)
731         self.n_features_with_missing_ = len(ordered_idx)
732 
733         abs_corr_mat = self._get_abs_corr_mat(Xt)
734 
735         n_samples, n_features = Xt.shape
736         if self.verbose > 0:
737             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
738         start_t = time()
739         if not self.sample_posterior:
740             Xt_previous = Xt.copy()
741             normalized_tol = self.tol * np.max(np.abs(X[~mask_missing_values]))
742         for self.n_iter_ in range(1, self.max_iter + 1):
743             if self.imputation_order == "random":
744                 ordered_idx = self._get_ordered_idx(mask_missing_values)
745 
746             for feat_idx in ordered_idx:
747                 neighbor_feat_idx = self._get_neighbor_feat_idx(
748                     n_features, feat_idx, abs_corr_mat
749                 )
750                 Xt, estimator = self._impute_one_feature(
751                     Xt,
752                     mask_missing_values,
753                     feat_idx,
754                     neighbor_feat_idx,
755                     estimator=None,
756                     fit_mode=True,
757                 )
758                 estimator_triplet = _ImputerTriplet(
759                     feat_idx, neighbor_feat_idx, estimator
760                 )
761                 self.imputation_sequence_.append(estimator_triplet)
762 
763             if self.verbose > 1:
764                 print(
765                     "[IterativeImputer] Ending imputation round "
766                     "%d/%d, elapsed time %0.2f"
767                     % (self.n_iter_, self.max_iter, time() - start_t)
768                 )
769 
770             if not self.sample_posterior:
771                 inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf, axis=None)
772                 if self.verbose > 0:
773                     print(
774                         "[IterativeImputer] Change: {}, scaled tolerance: {} ".format(
775                             inf_norm, normalized_tol
776                         )
777                     )
778                 if inf_norm < normalized_tol:
779                     if self.verbose > 0:
780                         print("[IterativeImputer] Early stopping criterion reached.")
781                     break
782                 Xt_previous = Xt.copy()
783         else:
784             if not self.sample_posterior:
785                 warnings.warn(
786                     "[IterativeImputer] Early stopping criterion not reached.",
787                     ConvergenceWarning,
788                 )
789         _assign_where(Xt, X, cond=~mask_missing_values)
790 
791         return super()._concatenate_indicator(Xt, X_indicator)
792 
793     def transform(self, X):
794         """Impute all missing values in `X`.
795 
796         Note that this is stochastic, and that if `random_state` is not fixed,
797         repeated calls, or permuted input, results will differ.
798 
799         Parameters
800         ----------
801         X : array-like of shape (n_samples, n_features)
802             The input data to complete.
803 
804         Returns
805         -------
806         Xt : array-like, shape (n_samples, n_features)
807              The imputed input data.
808         """
809         check_is_fitted(self)
810 
811         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
812             X, in_fit=False
813         )
814 
815         X_indicator = super()._transform_indicator(complete_mask)
816 
817         if self.n_iter_ == 0 or np.all(mask_missing_values):
818             return super()._concatenate_indicator(Xt, X_indicator)
819 
820         imputations_per_round = len(self.imputation_sequence_) // self.n_iter_
821         i_rnd = 0
822         if self.verbose > 0:
823             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
824         start_t = time()
825         for it, estimator_triplet in enumerate(self.imputation_sequence_):
826             Xt, _ = self._impute_one_feature(
827                 Xt,
828                 mask_missing_values,
829                 estimator_triplet.feat_idx,
830                 estimator_triplet.neighbor_feat_idx,
831                 estimator=estimator_triplet.estimator,
832                 fit_mode=False,
833             )
834             if not (it + 1) % imputations_per_round:
835                 if self.verbose > 1:
836                     print(
837                         "[IterativeImputer] Ending imputation round "
838                         "%d/%d, elapsed time %0.2f"
839                         % (i_rnd + 1, self.n_iter_, time() - start_t)
840                     )
841                 i_rnd += 1
842 
843         _assign_where(Xt, X, cond=~mask_missing_values)
844 
845         return super()._concatenate_indicator(Xt, X_indicator)
846 
847     def fit(self, X, y=None):
848         """Fit the imputer on `X` and return self.
849 
850         Parameters
851         ----------
852         X : array-like, shape (n_samples, n_features)
853             Input data, where `n_samples` is the number of samples and
854             `n_features` is the number of features.
855 
856         y : Ignored
857             Not used, present for API consistency by convention.
858 
859         Returns
860         -------
861         self : object
862             Fitted estimator.
863         """
864         self.fit_transform(X)
865         return self
866 
867     def get_feature_names_out(self, input_features=None):
868         """Get output feature names for transformation.
869 
870         Parameters
871         ----------
872         input_features : array-like of str or None, default=None
873             Input features.
874 
875             - If `input_features` is `None`, then `feature_names_in_` is
876               used as feature names in. If `feature_names_in_` is not defined,
877               then the following input feature names are generated:
878               `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
879             - If `input_features` is an array-like, then `input_features` must
880               match `feature_names_in_` if `feature_names_in_` is defined.
881 
882         Returns
883         -------
884         feature_names_out : ndarray of str objects
885             Transformed feature names.
886         """
887         input_features = _check_feature_names_in(self, input_features)
888         names = self.initial_imputer_.get_feature_names_out(input_features)
889         return self._concatenate_indicator_feature_names_out(names, input_features)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>_initial_imputation</method>
<code>
565     def _initial_imputation(self, X, in_fit=False):
566         """Perform initial imputation for input `X`.
567 
568         Parameters
569         ----------
570         X : ndarray of shape (n_samples, n_features)
571             Input data, where `n_samples` is the number of samples and
572             `n_features` is the number of features.
573 
574         in_fit : bool, default=False
575             Whether function is called in :meth:`fit`.
576 
577         Returns
578         -------
579         Xt : ndarray of shape (n_samples, n_features)
580             Input data, where `n_samples` is the number of samples and
581             `n_features` is the number of features.
582 
583         X_filled : ndarray of shape (n_samples, n_features)
584             Input data with the most recent imputations.
585 
586         mask_missing_values : ndarray of shape (n_samples, n_features)
587             Input data's missing indicator matrix, where `n_samples` is the
588             number of samples and `n_features` is the number of features,
589             masked by non-missing features.
590 
591         X_missing_mask : ndarray, shape (n_samples, n_features)
592             Input data's mask matrix indicating missing datapoints, where
593             `n_samples` is the number of samples and `n_features` is the
594             number of features.
595         """
596         if is_scalar_nan(self.missing_values):
597             force_all_finite = "allow-nan"
598         else:
599             force_all_finite = True
600 
601         X = self._validate_data(
602             X,
603             dtype=FLOAT_DTYPES,
604             order="F",
605             reset=in_fit,
606             force_all_finite=force_all_finite,
607         )
608         _check_inputs_dtype(X, self.missing_values)
609 
610         X_missing_mask = _get_mask(X, self.missing_values)
611         mask_missing_values = X_missing_mask.copy()
612         if self.initial_imputer_ is None:
613             self.initial_imputer_ = SimpleImputer(
614                 missing_values=self.missing_values,
615                 strategy=self.initial_strategy,
616                 keep_empty_features=self.keep_empty_features,
617             )
618             X_filled = self.initial_imputer_.fit_transform(X)
619         else:
620             X_filled = self.initial_imputer_.transform(X)
621 
622         valid_mask = np.flatnonzero(
623             np.logical_not(np.isnan(self.initial_imputer_.statistics_))
624         )
625 
626         if not self.keep_empty_features:
627             # drop empty features
628             Xt = X[:, valid_mask]
629             mask_missing_values = mask_missing_values[:, valid_mask]
630         else:
631             # mark empty features as not missing and keep the original
632             # imputation
633             mask_missing_values[:, valid_mask] = True
634             Xt = X
635 
636         return Xt, X_filled, mask_missing_values, X_missing_mask

</code><intended_behavior>This method should be updated to utilize the `fill_value` parameter when performing the initial imputation with strategy "constant". The implementation should check if `initial_strategy` is "constant" and, if so, use the `fill_value` for replacing missing values. This adjustment ensures that `fill_value` directly influences the imputation process only in the context it's meant to (i.e., with "constant" strategy), without affecting other imputation strategies. The intended behavior is to provide a seamless way to customize initial missing values handling, which could be especially beneficial for datasets where a zero, the mean, or median value might not be the most appropriate initial guess.</intended_behavior>, <file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>None</method>
<code>
54 class IterativeImputer(_BaseImputer):
55     """Multivariate imputer that estimates each feature from all the others.
56 
57     A strategy for imputing missing values by modeling each feature with
58     missing values as a function of other features in a round-robin fashion.
59 
60     Read more in the :ref:`User Guide <iterative_imputer>`.
61 
62     .. versionadded:: 0.21
63 
64     .. note::
65 
66       This estimator is still **experimental** for now: the predictions
67       and the API might change without any deprecation cycle. To use it,
68       you need to explicitly import `enable_iterative_imputer`::
69 
70         >>> # explicitly require this experimental feature
71         >>> from sklearn.experimental import enable_iterative_imputer  # noqa
72         >>> # now you can import normally from sklearn.impute
73         >>> from sklearn.impute import IterativeImputer
74 
75     Parameters
76     ----------
77     estimator : estimator object, default=BayesianRidge()
78         The estimator to use at each step of the round-robin imputation.
79         If `sample_posterior=True`, the estimator must support
80         `return_std` in its `predict` method.
81 
82     missing_values : int or np.nan, default=np.nan
83         The placeholder for the missing values. All occurrences of
84         `missing_values` will be imputed. For pandas' dataframes with
85         nullable integer dtypes with missing values, `missing_values`
86         should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.
87 
88     sample_posterior : bool, default=False
89         Whether to sample from the (Gaussian) predictive posterior of the
90         fitted estimator for each imputation. Estimator must support
91         `return_std` in its `predict` method if set to `True`. Set to
92         `True` if using `IterativeImputer` for multiple imputations.
93 
94     max_iter : int, default=10
95         Maximum number of imputation rounds to perform before returning the
96         imputations computed during the final round. A round is a single
97         imputation of each feature with missing values. The stopping criterion
98         is met once `max(abs(X_t - X_{t-1}))/max(abs(X[known_vals])) < tol`,
99         where `X_t` is `X` at iteration `t`. Note that early stopping is only
100         applied if `sample_posterior=False`.
101 
102     tol : float, default=1e-3
103         Tolerance of the stopping condition.
104 
105     n_nearest_features : int, default=None
106         Number of other features to use to estimate the missing values of
107         each feature column. Nearness between features is measured using
108         the absolute correlation coefficient between each feature pair (after
109         initial imputation). To ensure coverage of features throughout the
110         imputation process, the neighbor features are not necessarily nearest,
111         but are drawn with probability proportional to correlation for each
112         imputed target feature. Can provide significant speed-up when the
113         number of features is huge. If `None`, all features will be used.
114 
115     initial_strategy : {'mean', 'median', 'most_frequent', 'constant'}, \
116             default='mean'
117         Which strategy to use to initialize the missing values. Same as the
118         `strategy` parameter in :class:`~sklearn.impute.SimpleImputer`.
119 
120     imputation_order : {'ascending', 'descending', 'roman', 'arabic', \
121             'random'}, default='ascending'
122         The order in which the features will be imputed. Possible values:
123 
124         - `'ascending'`: From features with fewest missing values to most.
125         - `'descending'`: From features with most missing values to fewest.
126         - `'roman'`: Left to right.
127         - `'arabic'`: Right to left.
128         - `'random'`: A random order for each round.
129 
130     skip_complete : bool, default=False
131         If `True` then features with missing values during :meth:`transform`
132         which did not have any missing values during :meth:`fit` will be
133         imputed with the initial imputation method only. Set to `True` if you
134         have many features with no missing values at both :meth:`fit` and
135         :meth:`transform` time to save compute.
136 
137     min_value : float or array-like of shape (n_features,), default=-np.inf
138         Minimum possible imputed value. Broadcast to shape `(n_features,)` if
139         scalar. If array-like, expects shape `(n_features,)`, one min value for
140         each feature. The default is `-np.inf`.
141 
142         .. versionchanged:: 0.23
143            Added support for array-like.
144 
145     max_value : float or array-like of shape (n_features,), default=np.inf
146         Maximum possible imputed value. Broadcast to shape `(n_features,)` if
147         scalar. If array-like, expects shape `(n_features,)`, one max value for
148         each feature. The default is `np.inf`.
149 
150         .. versionchanged:: 0.23
151            Added support for array-like.
152 
153     verbose : int, default=0
154         Verbosity flag, controls the debug messages that are issued
155         as functions are evaluated. The higher, the more verbose. Can be 0, 1,
156         or 2.
157 
158     random_state : int, RandomState instance or None, default=None
159         The seed of the pseudo random number generator to use. Randomizes
160         selection of estimator features if `n_nearest_features` is not `None`,
161         the `imputation_order` if `random`, and the sampling from posterior if
162         `sample_posterior=True`. Use an integer for determinism.
163         See :term:`the Glossary <random_state>`.
164 
165     add_indicator : bool, default=False
166         If `True`, a :class:`MissingIndicator` transform will stack onto output
167         of the imputer's transform. This allows a predictive estimator
168         to account for missingness despite imputation. If a feature has no
169         missing values at fit/train time, the feature won't appear on
170         the missing indicator even if there are missing values at
171         transform/test time.
172 
173     keep_empty_features : bool, default=False
174         If True, features that consist exclusively of missing values when
175         `fit` is called are returned in results when `transform` is called.
176         The imputed value is always `0` except when
177         `initial_strategy="constant"` in which case `fill_value` will be
178         used instead.
179 
180         .. versionadded:: 1.2
181 
182     Attributes
183     ----------
184     initial_imputer_ : object of type :class:`~sklearn.impute.SimpleImputer`
185         Imputer used to initialize the missing values.
186 
187     imputation_sequence_ : list of tuples
188         Each tuple has `(feat_idx, neighbor_feat_idx, estimator)`, where
189         `feat_idx` is the current feature to be imputed,
190         `neighbor_feat_idx` is the array of other features used to impute the
191         current feature, and `estimator` is the trained estimator used for
192         the imputation. Length is `self.n_features_with_missing_ *
193         self.n_iter_`.
194 
195     n_iter_ : int
196         Number of iteration rounds that occurred. Will be less than
197         `self.max_iter` if early stopping criterion was reached.
198 
199     n_features_in_ : int
200         Number of features seen during :term:`fit`.
201 
202         .. versionadded:: 0.24
203 
204     feature_names_in_ : ndarray of shape (`n_features_in_`,)
205         Names of features seen during :term:`fit`. Defined only when `X`
206         has feature names that are all strings.
207 
208         .. versionadded:: 1.0
209 
210     n_features_with_missing_ : int
211         Number of features with missing values.
212 
213     indicator_ : :class:`~sklearn.impute.MissingIndicator`
214         Indicator used to add binary indicators for missing values.
215         `None` if `add_indicator=False`.
216 
217     random_state_ : RandomState instance
218         RandomState instance that is generated either from a seed, the random
219         number generator or by `np.random`.
220 
221     See Also
222     --------
223     SimpleImputer : Univariate imputer for completing missing values
224         with simple strategies.
225     KNNImputer : Multivariate imputer that estimates missing features using
226         nearest samples.
227 
228     Notes
229     -----
230     To support imputation in inductive mode we store each feature's estimator
231     during the :meth:`fit` phase, and predict without refitting (in order)
232     during the :meth:`transform` phase.
233 
234     Features which contain all missing values at :meth:`fit` are discarded upon
235     :meth:`transform`.
236 
237     Using defaults, the imputer scales in :math:`\\mathcal{O}(knp^3\\min(n,p))`
238     where :math:`k` = `max_iter`, :math:`n` the number of samples and
239     :math:`p` the number of features. It thus becomes prohibitively costly when
240     the number of features increases. Setting
241     `n_nearest_features << n_features`, `skip_complete=True` or increasing `tol`
242     can help to reduce its computational cost.
243 
244     Depending on the nature of missing values, simple imputers can be
245     preferable in a prediction context.
246 
247     References
248     ----------
249     .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice:
250         Multivariate Imputation by Chained Equations in R". Journal of
251         Statistical Software 45: 1-67.
252         <https://www.jstatsoft.org/article/view/v045i03>`_
253 
254     .. [2] `S. F. Buck, (1960). "A Method of Estimation of Missing Values in
255         Multivariate Data Suitable for use with an Electronic Computer".
256         Journal of the Royal Statistical Society 22(2): 302-306.
257         <https://www.jstor.org/stable/2984099>`_
258 
259     Examples
260     --------
261     >>> import numpy as np
262     >>> from sklearn.experimental import enable_iterative_imputer
263     >>> from sklearn.impute import IterativeImputer
264     >>> imp_mean = IterativeImputer(random_state=0)
265     >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
266     IterativeImputer(random_state=0)
267     >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
268     >>> imp_mean.transform(X)
269     array([[ 6.9584...,  2.       ,  3.        ],
270            [ 4.       ,  2.6000...,  6.        ],
271            [10.       ,  4.9999...,  9.        ]])
272     """
273 
274     _parameter_constraints: dict = {
275         **_BaseImputer._parameter_constraints,
276         "estimator": [None, HasMethods(["fit", "predict"])],
277         "sample_posterior": ["boolean"],
278         "max_iter": [Interval(Integral, 0, None, closed="left")],
279         "tol": [Interval(Real, 0, None, closed="left")],
280         "n_nearest_features": [None, Interval(Integral, 1, None, closed="left")],
281         "initial_strategy": [
282             StrOptions({"mean", "median", "most_frequent", "constant"})
283         ],
284         "imputation_order": [
285             StrOptions({"ascending", "descending", "roman", "arabic", "random"})
286         ],
287         "skip_complete": ["boolean"],
288         "min_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
289         "max_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
290         "verbose": ["verbose"],
291         "random_state": ["random_state"],
292     }
293 
294     def __init__(
295         self,
296         estimator=None,
297         *,
298         missing_values=np.nan,
299         sample_posterior=False,
300         max_iter=10,
301         tol=1e-3,
302         n_nearest_features=None,
303         initial_strategy="mean",
304         imputation_order="ascending",
305         skip_complete=False,
306         min_value=-np.inf,
307         max_value=np.inf,
308         verbose=0,
309         random_state=None,
310         add_indicator=False,
311         keep_empty_features=False,
312     ):
313         super().__init__(
314             missing_values=missing_values,
315             add_indicator=add_indicator,
316             keep_empty_features=keep_empty_features,
317         )
318 
319         self.estimator = estimator
320         self.sample_posterior = sample_posterior
321         self.max_iter = max_iter
322         self.tol = tol
323         self.n_nearest_features = n_nearest_features
324         self.initial_strategy = initial_strategy
325         self.imputation_order = imputation_order
326         self.skip_complete = skip_complete
327         self.min_value = min_value
328         self.max_value = max_value
329         self.verbose = verbose
330         self.random_state = random_state
331 
332     def _impute_one_feature(
333         self,
334         X_filled,
335         mask_missing_values,
336         feat_idx,
337         neighbor_feat_idx,
338         estimator=None,
339         fit_mode=True,
340     ):
341         """Impute a single feature from the others provided.
342 
343         This function predicts the missing values of one of the features using
344         the current estimates of all the other features. The `estimator` must
345         support `return_std=True` in its `predict` method for this function
346         to work.
347 
348         Parameters
349         ----------
350         X_filled : ndarray
351             Input data with the most recent imputations.
352 
353         mask_missing_values : ndarray
354             Input data's missing indicator matrix.
355 
356         feat_idx : int
357             Index of the feature currently being imputed.
358 
359         neighbor_feat_idx : ndarray
360             Indices of the features to be used in imputing `feat_idx`.
361 
362         estimator : object
363             The estimator to use at this step of the round-robin imputation.
364             If `sample_posterior=True`, the estimator must support
365             `return_std` in its `predict` method.
366             If None, it will be cloned from self._estimator.
367 
368         fit_mode : boolean, default=True
369             Whether to fit and predict with the estimator or just predict.
370 
371         Returns
372         -------
373         X_filled : ndarray
374             Input data with `X_filled[missing_row_mask, feat_idx]` updated.
375 
376         estimator : estimator with sklearn API
377             The fitted estimator used to impute
378             `X_filled[missing_row_mask, feat_idx]`.
379         """
380         if estimator is None and fit_mode is False:
381             raise ValueError(
382                 "If fit_mode is False, then an already-fitted "
383                 "estimator should be passed in."
384             )
385 
386         if estimator is None:
387             estimator = clone(self._estimator)
388 
389         missing_row_mask = mask_missing_values[:, feat_idx]
390         if fit_mode:
391             X_train = _safe_indexing(
392                 _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
393                 ~missing_row_mask,
394                 axis=0,
395             )
396             y_train = _safe_indexing(
397                 _safe_indexing(X_filled, feat_idx, axis=1),
398                 ~missing_row_mask,
399                 axis=0,
400             )
401             estimator.fit(X_train, y_train)
402 
403         # if no missing values, don't predict
404         if np.sum(missing_row_mask) == 0:
405             return X_filled, estimator
406 
407         # get posterior samples if there is at least one missing value
408         X_test = _safe_indexing(
409             _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
410             missing_row_mask,
411             axis=0,
412         )
413         if self.sample_posterior:
414             mus, sigmas = estimator.predict(X_test, return_std=True)
415             imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
416             # two types of problems: (1) non-positive sigmas
417             # (2) mus outside legal range of min_value and max_value
418             # (results in inf sample)
419             positive_sigmas = sigmas > 0
420             imputed_values[~positive_sigmas] = mus[~positive_sigmas]
421             mus_too_low = mus < self._min_value[feat_idx]
422             imputed_values[mus_too_low] = self._min_value[feat_idx]
423             mus_too_high = mus > self._max_value[feat_idx]
424             imputed_values[mus_too_high] = self._max_value[feat_idx]
425             # the rest can be sampled without statistical issues
426             inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high
427             mus = mus[inrange_mask]
428             sigmas = sigmas[inrange_mask]
429             a = (self._min_value[feat_idx] - mus) / sigmas
430             b = (self._max_value[feat_idx] - mus) / sigmas
431 
432             truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
433             imputed_values[inrange_mask] = truncated_normal.rvs(
434                 random_state=self.random_state_
435             )
436         else:
437             imputed_values = estimator.predict(X_test)
438             imputed_values = np.clip(
439                 imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]
440             )
441 
442         # update the feature
443         _safe_assign(
444             X_filled,
445             imputed_values,
446             row_indexer=missing_row_mask,
447             column_indexer=feat_idx,
448         )
449         return X_filled, estimator
450 
451     def _get_neighbor_feat_idx(self, n_features, feat_idx, abs_corr_mat):
452         """Get a list of other features to predict `feat_idx`.
453 
454         If `self.n_nearest_features` is less than or equal to the total
455         number of features, then use a probability proportional to the absolute
456         correlation between `feat_idx` and each other feature to randomly
457         choose a subsample of the other features (without replacement).
458 
459         Parameters
460         ----------
461         n_features : int
462             Number of features in `X`.
463 
464         feat_idx : int
465             Index of the feature currently being imputed.
466 
467         abs_corr_mat : ndarray, shape (n_features, n_features)
468             Absolute correlation matrix of `X`. The diagonal has been zeroed
469             out and each feature has been normalized to sum to 1. Can be None.
470 
471         Returns
472         -------
473         neighbor_feat_idx : array-like
474             The features to use to impute `feat_idx`.
475         """
476         if self.n_nearest_features is not None and self.n_nearest_features < n_features:
477             p = abs_corr_mat[:, feat_idx]
478             neighbor_feat_idx = self.random_state_.choice(
479                 np.arange(n_features), self.n_nearest_features, replace=False, p=p
480             )
481         else:
482             inds_left = np.arange(feat_idx)
483             inds_right = np.arange(feat_idx + 1, n_features)
484             neighbor_feat_idx = np.concatenate((inds_left, inds_right))
485         return neighbor_feat_idx
486 
487     def _get_ordered_idx(self, mask_missing_values):
488         """Decide in what order we will update the features.
489 
490         As a homage to the MICE R package, we will have 4 main options of
491         how to order the updates, and use a random order if anything else
492         is specified.
493 
494         Also, this function skips features which have no missing values.
495 
496         Parameters
497         ----------
498         mask_missing_values : array-like, shape (n_samples, n_features)
499             Input data's missing indicator matrix, where `n_samples` is the
500             number of samples and `n_features` is the number of features.
501 
502         Returns
503         -------
504         ordered_idx : ndarray, shape (n_features,)
505             The order in which to impute the features.
506         """
507         frac_of_missing_values = mask_missing_values.mean(axis=0)
508         if self.skip_complete:
509             missing_values_idx = np.flatnonzero(frac_of_missing_values)
510         else:
511             missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])
512         if self.imputation_order == "roman":
513             ordered_idx = missing_values_idx
514         elif self.imputation_order == "arabic":
515             ordered_idx = missing_values_idx[::-1]
516         elif self.imputation_order == "ascending":
517             n = len(frac_of_missing_values) - len(missing_values_idx)
518             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:]
519         elif self.imputation_order == "descending":
520             n = len(frac_of_missing_values) - len(missing_values_idx)
521             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:][::-1]
522         elif self.imputation_order == "random":
523             ordered_idx = missing_values_idx
524             self.random_state_.shuffle(ordered_idx)
525         return ordered_idx
526 
527     def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
528         """Get absolute correlation matrix between features.
529 
530         Parameters
531         ----------
532         X_filled : ndarray, shape (n_samples, n_features)
533             Input data with the most recent imputations.
534 
535         tolerance : float, default=1e-6
536             `abs_corr_mat` can have nans, which will be replaced
537             with `tolerance`.
538 
539         Returns
540         -------
541         abs_corr_mat : ndarray, shape (n_features, n_features)
542             Absolute correlation matrix of `X` at the beginning of the
543             current round. The diagonal has been zeroed out and each feature's
544             absolute correlations with all others have been normalized to sum
545             to 1.
546         """
547         n_features = X_filled.shape[1]
548         if self.n_nearest_features is None or self.n_nearest_features >= n_features:
549             return None
550         with np.errstate(invalid="ignore"):
551             # if a feature in the neighborhood has only a single value
552             # (e.g., categorical feature), the std. dev. will be null and
553             # np.corrcoef will raise a warning due to a division by zero
554             abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
555         # np.corrcoef is not defined for features with zero std
556         abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
557         # ensures exploration, i.e. at least some probability of sampling
558         np.clip(abs_corr_mat, tolerance, None, out=abs_corr_mat)
559         # features are not their own neighbors
560         np.fill_diagonal(abs_corr_mat, 0)
561         # needs to sum to 1 for np.random.choice sampling
562         abs_corr_mat = normalize(abs_corr_mat, norm="l1", axis=0, copy=False)
563         return abs_corr_mat
564 
565     def _initial_imputation(self, X, in_fit=False):
566         """Perform initial imputation for input `X`.
567 
568         Parameters
569         ----------
570         X : ndarray of shape (n_samples, n_features)
571             Input data, where `n_samples` is the number of samples and
572             `n_features` is the number of features.
573 
574         in_fit : bool, default=False
575             Whether function is called in :meth:`fit`.
576 
577         Returns
578         -------
579         Xt : ndarray of shape (n_samples, n_features)
580             Input data, where `n_samples` is the number of samples and
581             `n_features` is the number of features.
582 
583         X_filled : ndarray of shape (n_samples, n_features)
584             Input data with the most recent imputations.
585 
586         mask_missing_values : ndarray of shape (n_samples, n_features)
587             Input data's missing indicator matrix, where `n_samples` is the
588             number of samples and `n_features` is the number of features,
589             masked by non-missing features.
590 
591         X_missing_mask : ndarray, shape (n_samples, n_features)
592             Input data's mask matrix indicating missing datapoints, where
593             `n_samples` is the number of samples and `n_features` is the
594             number of features.
595         """
596         if is_scalar_nan(self.missing_values):
597             force_all_finite = "allow-nan"
598         else:
599             force_all_finite = True
600 
601         X = self._validate_data(
602             X,
603             dtype=FLOAT_DTYPES,
604             order="F",
605             reset=in_fit,
606             force_all_finite=force_all_finite,
607         )
608         _check_inputs_dtype(X, self.missing_values)
609 
610         X_missing_mask = _get_mask(X, self.missing_values)
611         mask_missing_values = X_missing_mask.copy()
612         if self.initial_imputer_ is None:
613             self.initial_imputer_ = SimpleImputer(
614                 missing_values=self.missing_values,
615                 strategy=self.initial_strategy,
616                 keep_empty_features=self.keep_empty_features,
617             )
618             X_filled = self.initial_imputer_.fit_transform(X)
619         else:
620             X_filled = self.initial_imputer_.transform(X)
621 
622         valid_mask = np.flatnonzero(
623             np.logical_not(np.isnan(self.initial_imputer_.statistics_))
624         )
625 
626         if not self.keep_empty_features:
627             # drop empty features
628             Xt = X[:, valid_mask]
629             mask_missing_values = mask_missing_values[:, valid_mask]
630         else:
631             # mark empty features as not missing and keep the original
632             # imputation
633             mask_missing_values[:, valid_mask] = True
634             Xt = X
635 
636         return Xt, X_filled, mask_missing_values, X_missing_mask
637 
638     @staticmethod
639     def _validate_limit(limit, limit_type, n_features):
640         """Validate the limits (min/max) of the feature values.
641 
642         Converts scalar min/max limits to vectors of shape `(n_features,)`.
643 
644         Parameters
645         ----------
646         limit: scalar or array-like
647             The user-specified limit (i.e, min_value or max_value).
648         limit_type: {'max', 'min'}
649             Type of limit to validate.
650         n_features: int
651             Number of features in the dataset.
652 
653         Returns
654         -------
655         limit: ndarray, shape(n_features,)
656             Array of limits, one for each feature.
657         """
658         limit_bound = np.inf if limit_type == "max" else -np.inf
659         limit = limit_bound if limit is None else limit
660         if np.isscalar(limit):
661             limit = np.full(n_features, limit)
662         limit = check_array(limit, force_all_finite=False, copy=False, ensure_2d=False)
663         if not limit.shape[0] == n_features:
664             raise ValueError(
665                 f"'{limit_type}_value' should be of "
666                 f"shape ({n_features},) when an array-like "
667                 f"is provided. Got {limit.shape}, instead."
668             )
669         return limit
670 
671     def fit_transform(self, X, y=None):
672         """Fit the imputer on `X` and return the transformed `X`.
673 
674         Parameters
675         ----------
676         X : array-like, shape (n_samples, n_features)
677             Input data, where `n_samples` is the number of samples and
678             `n_features` is the number of features.
679 
680         y : Ignored
681             Not used, present for API consistency by convention.
682 
683         Returns
684         -------
685         Xt : array-like, shape (n_samples, n_features)
686             The imputed input data.
687         """
688         self._validate_params()
689         self.random_state_ = getattr(
690             self, "random_state_", check_random_state(self.random_state)
691         )
692 
693         if self.estimator is None:
694             from ..linear_model import BayesianRidge
695 
696             self._estimator = BayesianRidge()
697         else:
698             self._estimator = clone(self.estimator)
699 
700         self.imputation_sequence_ = []
701 
702         self.initial_imputer_ = None
703 
704         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
705             X, in_fit=True
706         )
707 
708         super()._fit_indicator(complete_mask)
709         X_indicator = super()._transform_indicator(complete_mask)
710 
711         if self.max_iter == 0 or np.all(mask_missing_values):
712             self.n_iter_ = 0
713             return super()._concatenate_indicator(Xt, X_indicator)
714 
715         # Edge case: a single feature. We return the initial ...
716         if Xt.shape[1] == 1:
717             self.n_iter_ = 0
718             return super()._concatenate_indicator(Xt, X_indicator)
719 
720         self._min_value = self._validate_limit(self.min_value, "min", X.shape[1])
721         self._max_value = self._validate_limit(self.max_value, "max", X.shape[1])
722 
723         if not np.all(np.greater(self._max_value, self._min_value)):
724             raise ValueError("One (or more) features have min_value >= max_value.")
725 
726         # order in which to impute
727         # note this is probably too slow for large feature data (d > 100000)
728         # and a better way would be good.
729         # see: https://goo.gl/KyCNwj and subsequent comments
730         ordered_idx = self._get_ordered_idx(mask_missing_values)
731         self.n_features_with_missing_ = len(ordered_idx)
732 
733         abs_corr_mat = self._get_abs_corr_mat(Xt)
734 
735         n_samples, n_features = Xt.shape
736         if self.verbose > 0:
737             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
738         start_t = time()
739         if not self.sample_posterior:
740             Xt_previous = Xt.copy()
741             normalized_tol = self.tol * np.max(np.abs(X[~mask_missing_values]))
742         for self.n_iter_ in range(1, self.max_iter + 1):
743             if self.imputation_order == "random":
744                 ordered_idx = self._get_ordered_idx(mask_missing_values)
745 
746             for feat_idx in ordered_idx:
747                 neighbor_feat_idx = self._get_neighbor_feat_idx(
748                     n_features, feat_idx, abs_corr_mat
749                 )
750                 Xt, estimator = self._impute_one_feature(
751                     Xt,
752                     mask_missing_values,
753                     feat_idx,
754                     neighbor_feat_idx,
755                     estimator=None,
756                     fit_mode=True,
757                 )
758                 estimator_triplet = _ImputerTriplet(
759                     feat_idx, neighbor_feat_idx, estimator
760                 )
761                 self.imputation_sequence_.append(estimator_triplet)
762 
763             if self.verbose > 1:
764                 print(
765                     "[IterativeImputer] Ending imputation round "
766                     "%d/%d, elapsed time %0.2f"
767                     % (self.n_iter_, self.max_iter, time() - start_t)
768                 )
769 
770             if not self.sample_posterior:
771                 inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf, axis=None)
772                 if self.verbose > 0:
773                     print(
774                         "[IterativeImputer] Change: {}, scaled tolerance: {} ".format(
775                             inf_norm, normalized_tol
776                         )
777                     )
778                 if inf_norm < normalized_tol:
779                     if self.verbose > 0:
780                         print("[IterativeImputer] Early stopping criterion reached.")
781                     break
782                 Xt_previous = Xt.copy()
783         else:
784             if not self.sample_posterior:
785                 warnings.warn(
786                     "[IterativeImputer] Early stopping criterion not reached.",
787                     ConvergenceWarning,
788                 )
789         _assign_where(Xt, X, cond=~mask_missing_values)
790 
791         return super()._concatenate_indicator(Xt, X_indicator)
792 
793     def transform(self, X):
794         """Impute all missing values in `X`.
795 
796         Note that this is stochastic, and that if `random_state` is not fixed,
797         repeated calls, or permuted input, results will differ.
798 
799         Parameters
800         ----------
801         X : array-like of shape (n_samples, n_features)
802             The input data to complete.
803 
804         Returns
805         -------
806         Xt : array-like, shape (n_samples, n_features)
807              The imputed input data.
808         """
809         check_is_fitted(self)
810 
811         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
812             X, in_fit=False
813         )
814 
815         X_indicator = super()._transform_indicator(complete_mask)
816 
817         if self.n_iter_ == 0 or np.all(mask_missing_values):
818             return super()._concatenate_indicator(Xt, X_indicator)
819 
820         imputations_per_round = len(self.imputation_sequence_) // self.n_iter_
821         i_rnd = 0
822         if self.verbose > 0:
823             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
824         start_t = time()
825         for it, estimator_triplet in enumerate(self.imputation_sequence_):
826             Xt, _ = self._impute_one_feature(
827                 Xt,
828                 mask_missing_values,
829                 estimator_triplet.feat_idx,
830                 estimator_triplet.neighbor_feat_idx,
831                 estimator=estimator_triplet.estimator,
832                 fit_mode=False,
833             )
834             if not (it + 1) % imputations_per_round:
835                 if self.verbose > 1:
836                     print(
837                         "[IterativeImputer] Ending imputation round "
838                         "%d/%d, elapsed time %0.2f"
839                         % (i_rnd + 1, self.n_iter_, time() - start_t)
840                     )
841                 i_rnd += 1
842 
843         _assign_where(Xt, X, cond=~mask_missing_values)
844 
845         return super()._concatenate_indicator(Xt, X_indicator)
846 
847     def fit(self, X, y=None):
848         """Fit the imputer on `X` and return self.
849 
850         Parameters
851         ----------
852         X : array-like, shape (n_samples, n_features)
853             Input data, where `n_samples` is the number of samples and
854             `n_features` is the number of features.
855 
856         y : Ignored
857             Not used, present for API consistency by convention.
858 
859         Returns
860         -------
861         self : object
862             Fitted estimator.
863         """
864         self.fit_transform(X)
865         return self
866 
867     def get_feature_names_out(self, input_features=None):
868         """Get output feature names for transformation.
869 
870         Parameters
871         ----------
872         input_features : array-like of str or None, default=None
873             Input features.
874 
875             - If `input_features` is `None`, then `feature_names_in_` is
876               used as feature names in. If `feature_names_in_` is not defined,
877               then the following input feature names are generated:
878               `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
879             - If `input_features` is an array-like, then `input_features` must
880               match `feature_names_in_` if `feature_names_in_` is defined.
881 
882         Returns
883         -------
884         feature_names_out : ndarray of str objects
885             Transformed feature names.
886         """
887         input_features = _check_feature_names_in(self, input_features)
888         names = self.initial_imputer_.get_feature_names_out(input_features)
889         return self._concatenate_indicator_feature_names_out(names, input_features)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-16 00:04:42.447 | INFO     | Search completed. Bug locations: [<file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>__init__</method>
<code>
294     def __init__(
295         self,
296         estimator=None,
297         *,
298         missing_values=np.nan,
299         sample_posterior=False,
300         max_iter=10,
301         tol=1e-3,
302         n_nearest_features=None,
303         initial_strategy="mean",
304         imputation_order="ascending",
305         skip_complete=False,
306         min_value=-np.inf,
307         max_value=np.inf,
308         verbose=0,
309         random_state=None,
310         add_indicator=False,
311         keep_empty_features=False,
312     ):
313         super().__init__(
314             missing_values=missing_values,
315             add_indicator=add_indicator,
316             keep_empty_features=keep_empty_features,
317         )
318 
319         self.estimator = estimator
320         self.sample_posterior = sample_posterior
321         self.max_iter = max_iter
322         self.tol = tol
323         self.n_nearest_features = n_nearest_features
324         self.initial_strategy = initial_strategy
325         self.imputation_order = imputation_order
326         self.skip_complete = skip_complete
327         self.min_value = min_value
328         self.max_value = max_value
329         self.verbose = verbose
330         self.random_state = random_state

</code><intended_behavior>The constructor of the `IterativeImputer` class should be modified to include a new parameter `fill_value`, which will be used when the `initial_strategy` is set to "constant". This parameter should allow any scalar value or `np.nan`, providing a default that maintains the current behavior when not explicitly set by the user. This change allows users to specify a custom initialization value for missing values, enhancing the flexibility of `IterativeImputer`. Existing functionality and default behavior of the class should remain unchanged when the `fill_value` parameter is not utilized (i.e., when `initial_strategy` is not "constant").</intended_behavior>, <file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>None</method>
<code>
54 class IterativeImputer(_BaseImputer):
55     """Multivariate imputer that estimates each feature from all the others.
56 
57     A strategy for imputing missing values by modeling each feature with
58     missing values as a function of other features in a round-robin fashion.
59 
60     Read more in the :ref:`User Guide <iterative_imputer>`.
61 
62     .. versionadded:: 0.21
63 
64     .. note::
65 
66       This estimator is still **experimental** for now: the predictions
67       and the API might change without any deprecation cycle. To use it,
68       you need to explicitly import `enable_iterative_imputer`::
69 
70         >>> # explicitly require this experimental feature
71         >>> from sklearn.experimental import enable_iterative_imputer  # noqa
72         >>> # now you can import normally from sklearn.impute
73         >>> from sklearn.impute import IterativeImputer
74 
75     Parameters
76     ----------
77     estimator : estimator object, default=BayesianRidge()
78         The estimator to use at each step of the round-robin imputation.
79         If `sample_posterior=True`, the estimator must support
80         `return_std` in its `predict` method.
81 
82     missing_values : int or np.nan, default=np.nan
83         The placeholder for the missing values. All occurrences of
84         `missing_values` will be imputed. For pandas' dataframes with
85         nullable integer dtypes with missing values, `missing_values`
86         should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.
87 
88     sample_posterior : bool, default=False
89         Whether to sample from the (Gaussian) predictive posterior of the
90         fitted estimator for each imputation. Estimator must support
91         `return_std` in its `predict` method if set to `True`. Set to
92         `True` if using `IterativeImputer` for multiple imputations.
93 
94     max_iter : int, default=10
95         Maximum number of imputation rounds to perform before returning the
96         imputations computed during the final round. A round is a single
97         imputation of each feature with missing values. The stopping criterion
98         is met once `max(abs(X_t - X_{t-1}))/max(abs(X[known_vals])) < tol`,
99         where `X_t` is `X` at iteration `t`. Note that early stopping is only
100         applied if `sample_posterior=False`.
101 
102     tol : float, default=1e-3
103         Tolerance of the stopping condition.
104 
105     n_nearest_features : int, default=None
106         Number of other features to use to estimate the missing values of
107         each feature column. Nearness between features is measured using
108         the absolute correlation coefficient between each feature pair (after
109         initial imputation). To ensure coverage of features throughout the
110         imputation process, the neighbor features are not necessarily nearest,
111         but are drawn with probability proportional to correlation for each
112         imputed target feature. Can provide significant speed-up when the
113         number of features is huge. If `None`, all features will be used.
114 
115     initial_strategy : {'mean', 'median', 'most_frequent', 'constant'}, \
116             default='mean'
117         Which strategy to use to initialize the missing values. Same as the
118         `strategy` parameter in :class:`~sklearn.impute.SimpleImputer`.
119 
120     imputation_order : {'ascending', 'descending', 'roman', 'arabic', \
121             'random'}, default='ascending'
122         The order in which the features will be imputed. Possible values:
123 
124         - `'ascending'`: From features with fewest missing values to most.
125         - `'descending'`: From features with most missing values to fewest.
126         - `'roman'`: Left to right.
127         - `'arabic'`: Right to left.
128         - `'random'`: A random order for each round.
129 
130     skip_complete : bool, default=False
131         If `True` then features with missing values during :meth:`transform`
132         which did not have any missing values during :meth:`fit` will be
133         imputed with the initial imputation method only. Set to `True` if you
134         have many features with no missing values at both :meth:`fit` and
135         :meth:`transform` time to save compute.
136 
137     min_value : float or array-like of shape (n_features,), default=-np.inf
138         Minimum possible imputed value. Broadcast to shape `(n_features,)` if
139         scalar. If array-like, expects shape `(n_features,)`, one min value for
140         each feature. The default is `-np.inf`.
141 
142         .. versionchanged:: 0.23
143            Added support for array-like.
144 
145     max_value : float or array-like of shape (n_features,), default=np.inf
146         Maximum possible imputed value. Broadcast to shape `(n_features,)` if
147         scalar. If array-like, expects shape `(n_features,)`, one max value for
148         each feature. The default is `np.inf`.
149 
150         .. versionchanged:: 0.23
151            Added support for array-like.
152 
153     verbose : int, default=0
154         Verbosity flag, controls the debug messages that are issued
155         as functions are evaluated. The higher, the more verbose. Can be 0, 1,
156         or 2.
157 
158     random_state : int, RandomState instance or None, default=None
159         The seed of the pseudo random number generator to use. Randomizes
160         selection of estimator features if `n_nearest_features` is not `None`,
161         the `imputation_order` if `random`, and the sampling from posterior if
162         `sample_posterior=True`. Use an integer for determinism.
163         See :term:`the Glossary <random_state>`.
164 
165     add_indicator : bool, default=False
166         If `True`, a :class:`MissingIndicator` transform will stack onto output
167         of the imputer's transform. This allows a predictive estimator
168         to account for missingness despite imputation. If a feature has no
169         missing values at fit/train time, the feature won't appear on
170         the missing indicator even if there are missing values at
171         transform/test time.
172 
173     keep_empty_features : bool, default=False
174         If True, features that consist exclusively of missing values when
175         `fit` is called are returned in results when `transform` is called.
176         The imputed value is always `0` except when
177         `initial_strategy="constant"` in which case `fill_value` will be
178         used instead.
179 
180         .. versionadded:: 1.2
181 
182     Attributes
183     ----------
184     initial_imputer_ : object of type :class:`~sklearn.impute.SimpleImputer`
185         Imputer used to initialize the missing values.
186 
187     imputation_sequence_ : list of tuples
188         Each tuple has `(feat_idx, neighbor_feat_idx, estimator)`, where
189         `feat_idx` is the current feature to be imputed,
190         `neighbor_feat_idx` is the array of other features used to impute the
191         current feature, and `estimator` is the trained estimator used for
192         the imputation. Length is `self.n_features_with_missing_ *
193         self.n_iter_`.
194 
195     n_iter_ : int
196         Number of iteration rounds that occurred. Will be less than
197         `self.max_iter` if early stopping criterion was reached.
198 
199     n_features_in_ : int
200         Number of features seen during :term:`fit`.
201 
202         .. versionadded:: 0.24
203 
204     feature_names_in_ : ndarray of shape (`n_features_in_`,)
205         Names of features seen during :term:`fit`. Defined only when `X`
206         has feature names that are all strings.
207 
208         .. versionadded:: 1.0
209 
210     n_features_with_missing_ : int
211         Number of features with missing values.
212 
213     indicator_ : :class:`~sklearn.impute.MissingIndicator`
214         Indicator used to add binary indicators for missing values.
215         `None` if `add_indicator=False`.
216 
217     random_state_ : RandomState instance
218         RandomState instance that is generated either from a seed, the random
219         number generator or by `np.random`.
220 
221     See Also
222     --------
223     SimpleImputer : Univariate imputer for completing missing values
224         with simple strategies.
225     KNNImputer : Multivariate imputer that estimates missing features using
226         nearest samples.
227 
228     Notes
229     -----
230     To support imputation in inductive mode we store each feature's estimator
231     during the :meth:`fit` phase, and predict without refitting (in order)
232     during the :meth:`transform` phase.
233 
234     Features which contain all missing values at :meth:`fit` are discarded upon
235     :meth:`transform`.
236 
237     Using defaults, the imputer scales in :math:`\\mathcal{O}(knp^3\\min(n,p))`
238     where :math:`k` = `max_iter`, :math:`n` the number of samples and
239     :math:`p` the number of features. It thus becomes prohibitively costly when
240     the number of features increases. Setting
241     `n_nearest_features << n_features`, `skip_complete=True` or increasing `tol`
242     can help to reduce its computational cost.
243 
244     Depending on the nature of missing values, simple imputers can be
245     preferable in a prediction context.
246 
247     References
248     ----------
249     .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice:
250         Multivariate Imputation by Chained Equations in R". Journal of
251         Statistical Software 45: 1-67.
252         <https://www.jstatsoft.org/article/view/v045i03>`_
253 
254     .. [2] `S. F. Buck, (1960). "A Method of Estimation of Missing Values in
255         Multivariate Data Suitable for use with an Electronic Computer".
256         Journal of the Royal Statistical Society 22(2): 302-306.
257         <https://www.jstor.org/stable/2984099>`_
258 
259     Examples
260     --------
261     >>> import numpy as np
262     >>> from sklearn.experimental import enable_iterative_imputer
263     >>> from sklearn.impute import IterativeImputer
264     >>> imp_mean = IterativeImputer(random_state=0)
265     >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
266     IterativeImputer(random_state=0)
267     >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
268     >>> imp_mean.transform(X)
269     array([[ 6.9584...,  2.       ,  3.        ],
270            [ 4.       ,  2.6000...,  6.        ],
271            [10.       ,  4.9999...,  9.        ]])
272     """
273 
274     _parameter_constraints: dict = {
275         **_BaseImputer._parameter_constraints,
276         "estimator": [None, HasMethods(["fit", "predict"])],
277         "sample_posterior": ["boolean"],
278         "max_iter": [Interval(Integral, 0, None, closed="left")],
279         "tol": [Interval(Real, 0, None, closed="left")],
280         "n_nearest_features": [None, Interval(Integral, 1, None, closed="left")],
281         "initial_strategy": [
282             StrOptions({"mean", "median", "most_frequent", "constant"})
283         ],
284         "imputation_order": [
285             StrOptions({"ascending", "descending", "roman", "arabic", "random"})
286         ],
287         "skip_complete": ["boolean"],
288         "min_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
289         "max_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
290         "verbose": ["verbose"],
291         "random_state": ["random_state"],
292     }
293 
294     def __init__(
295         self,
296         estimator=None,
297         *,
298         missing_values=np.nan,
299         sample_posterior=False,
300         max_iter=10,
301         tol=1e-3,
302         n_nearest_features=None,
303         initial_strategy="mean",
304         imputation_order="ascending",
305         skip_complete=False,
306         min_value=-np.inf,
307         max_value=np.inf,
308         verbose=0,
309         random_state=None,
310         add_indicator=False,
311         keep_empty_features=False,
312     ):
313         super().__init__(
314             missing_values=missing_values,
315             add_indicator=add_indicator,
316             keep_empty_features=keep_empty_features,
317         )
318 
319         self.estimator = estimator
320         self.sample_posterior = sample_posterior
321         self.max_iter = max_iter
322         self.tol = tol
323         self.n_nearest_features = n_nearest_features
324         self.initial_strategy = initial_strategy
325         self.imputation_order = imputation_order
326         self.skip_complete = skip_complete
327         self.min_value = min_value
328         self.max_value = max_value
329         self.verbose = verbose
330         self.random_state = random_state
331 
332     def _impute_one_feature(
333         self,
334         X_filled,
335         mask_missing_values,
336         feat_idx,
337         neighbor_feat_idx,
338         estimator=None,
339         fit_mode=True,
340     ):
341         """Impute a single feature from the others provided.
342 
343         This function predicts the missing values of one of the features using
344         the current estimates of all the other features. The `estimator` must
345         support `return_std=True` in its `predict` method for this function
346         to work.
347 
348         Parameters
349         ----------
350         X_filled : ndarray
351             Input data with the most recent imputations.
352 
353         mask_missing_values : ndarray
354             Input data's missing indicator matrix.
355 
356         feat_idx : int
357             Index of the feature currently being imputed.
358 
359         neighbor_feat_idx : ndarray
360             Indices of the features to be used in imputing `feat_idx`.
361 
362         estimator : object
363             The estimator to use at this step of the round-robin imputation.
364             If `sample_posterior=True`, the estimator must support
365             `return_std` in its `predict` method.
366             If None, it will be cloned from self._estimator.
367 
368         fit_mode : boolean, default=True
369             Whether to fit and predict with the estimator or just predict.
370 
371         Returns
372         -------
373         X_filled : ndarray
374             Input data with `X_filled[missing_row_mask, feat_idx]` updated.
375 
376         estimator : estimator with sklearn API
377             The fitted estimator used to impute
378             `X_filled[missing_row_mask, feat_idx]`.
379         """
380         if estimator is None and fit_mode is False:
381             raise ValueError(
382                 "If fit_mode is False, then an already-fitted "
383                 "estimator should be passed in."
384             )
385 
386         if estimator is None:
387             estimator = clone(self._estimator)
388 
389         missing_row_mask = mask_missing_values[:, feat_idx]
390         if fit_mode:
391             X_train = _safe_indexing(
392                 _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
393                 ~missing_row_mask,
394                 axis=0,
395             )
396             y_train = _safe_indexing(
397                 _safe_indexing(X_filled, feat_idx, axis=1),
398                 ~missing_row_mask,
399                 axis=0,
400             )
401             estimator.fit(X_train, y_train)
402 
403         # if no missing values, don't predict
404         if np.sum(missing_row_mask) == 0:
405             return X_filled, estimator
406 
407         # get posterior samples if there is at least one missing value
408         X_test = _safe_indexing(
409             _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
410             missing_row_mask,
411             axis=0,
412         )
413         if self.sample_posterior:
414             mus, sigmas = estimator.predict(X_test, return_std=True)
415             imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
416             # two types of problems: (1) non-positive sigmas
417             # (2) mus outside legal range of min_value and max_value
418             # (results in inf sample)
419             positive_sigmas = sigmas > 0
420             imputed_values[~positive_sigmas] = mus[~positive_sigmas]
421             mus_too_low = mus < self._min_value[feat_idx]
422             imputed_values[mus_too_low] = self._min_value[feat_idx]
423             mus_too_high = mus > self._max_value[feat_idx]
424             imputed_values[mus_too_high] = self._max_value[feat_idx]
425             # the rest can be sampled without statistical issues
426             inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high
427             mus = mus[inrange_mask]
428             sigmas = sigmas[inrange_mask]
429             a = (self._min_value[feat_idx] - mus) / sigmas
430             b = (self._max_value[feat_idx] - mus) / sigmas
431 
432             truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
433             imputed_values[inrange_mask] = truncated_normal.rvs(
434                 random_state=self.random_state_
435             )
436         else:
437             imputed_values = estimator.predict(X_test)
438             imputed_values = np.clip(
439                 imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]
440             )
441 
442         # update the feature
443         _safe_assign(
444             X_filled,
445             imputed_values,
446             row_indexer=missing_row_mask,
447             column_indexer=feat_idx,
448         )
449         return X_filled, estimator
450 
451     def _get_neighbor_feat_idx(self, n_features, feat_idx, abs_corr_mat):
452         """Get a list of other features to predict `feat_idx`.
453 
454         If `self.n_nearest_features` is less than or equal to the total
455         number of features, then use a probability proportional to the absolute
456         correlation between `feat_idx` and each other feature to randomly
457         choose a subsample of the other features (without replacement).
458 
459         Parameters
460         ----------
461         n_features : int
462             Number of features in `X`.
463 
464         feat_idx : int
465             Index of the feature currently being imputed.
466 
467         abs_corr_mat : ndarray, shape (n_features, n_features)
468             Absolute correlation matrix of `X`. The diagonal has been zeroed
469             out and each feature has been normalized to sum to 1. Can be None.
470 
471         Returns
472         -------
473         neighbor_feat_idx : array-like
474             The features to use to impute `feat_idx`.
475         """
476         if self.n_nearest_features is not None and self.n_nearest_features < n_features:
477             p = abs_corr_mat[:, feat_idx]
478             neighbor_feat_idx = self.random_state_.choice(
479                 np.arange(n_features), self.n_nearest_features, replace=False, p=p
480             )
481         else:
482             inds_left = np.arange(feat_idx)
483             inds_right = np.arange(feat_idx + 1, n_features)
484             neighbor_feat_idx = np.concatenate((inds_left, inds_right))
485         return neighbor_feat_idx
486 
487     def _get_ordered_idx(self, mask_missing_values):
488         """Decide in what order we will update the features.
489 
490         As a homage to the MICE R package, we will have 4 main options of
491         how to order the updates, and use a random order if anything else
492         is specified.
493 
494         Also, this function skips features which have no missing values.
495 
496         Parameters
497         ----------
498         mask_missing_values : array-like, shape (n_samples, n_features)
499             Input data's missing indicator matrix, where `n_samples` is the
500             number of samples and `n_features` is the number of features.
501 
502         Returns
503         -------
504         ordered_idx : ndarray, shape (n_features,)
505             The order in which to impute the features.
506         """
507         frac_of_missing_values = mask_missing_values.mean(axis=0)
508         if self.skip_complete:
509             missing_values_idx = np.flatnonzero(frac_of_missing_values)
510         else:
511             missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])
512         if self.imputation_order == "roman":
513             ordered_idx = missing_values_idx
514         elif self.imputation_order == "arabic":
515             ordered_idx = missing_values_idx[::-1]
516         elif self.imputation_order == "ascending":
517             n = len(frac_of_missing_values) - len(missing_values_idx)
518             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:]
519         elif self.imputation_order == "descending":
520             n = len(frac_of_missing_values) - len(missing_values_idx)
521             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:][::-1]
522         elif self.imputation_order == "random":
523             ordered_idx = missing_values_idx
524             self.random_state_.shuffle(ordered_idx)
525         return ordered_idx
526 
527     def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
528         """Get absolute correlation matrix between features.
529 
530         Parameters
531         ----------
532         X_filled : ndarray, shape (n_samples, n_features)
533             Input data with the most recent imputations.
534 
535         tolerance : float, default=1e-6
536             `abs_corr_mat` can have nans, which will be replaced
537             with `tolerance`.
538 
539         Returns
540         -------
541         abs_corr_mat : ndarray, shape (n_features, n_features)
542             Absolute correlation matrix of `X` at the beginning of the
543             current round. The diagonal has been zeroed out and each feature's
544             absolute correlations with all others have been normalized to sum
545             to 1.
546         """
547         n_features = X_filled.shape[1]
548         if self.n_nearest_features is None or self.n_nearest_features >= n_features:
549             return None
550         with np.errstate(invalid="ignore"):
551             # if a feature in the neighborhood has only a single value
552             # (e.g., categorical feature), the std. dev. will be null and
553             # np.corrcoef will raise a warning due to a division by zero
554             abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
555         # np.corrcoef is not defined for features with zero std
556         abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
557         # ensures exploration, i.e. at least some probability of sampling
558         np.clip(abs_corr_mat, tolerance, None, out=abs_corr_mat)
559         # features are not their own neighbors
560         np.fill_diagonal(abs_corr_mat, 0)
561         # needs to sum to 1 for np.random.choice sampling
562         abs_corr_mat = normalize(abs_corr_mat, norm="l1", axis=0, copy=False)
563         return abs_corr_mat
564 
565     def _initial_imputation(self, X, in_fit=False):
566         """Perform initial imputation for input `X`.
567 
568         Parameters
569         ----------
570         X : ndarray of shape (n_samples, n_features)
571             Input data, where `n_samples` is the number of samples and
572             `n_features` is the number of features.
573 
574         in_fit : bool, default=False
575             Whether function is called in :meth:`fit`.
576 
577         Returns
578         -------
579         Xt : ndarray of shape (n_samples, n_features)
580             Input data, where `n_samples` is the number of samples and
581             `n_features` is the number of features.
582 
583         X_filled : ndarray of shape (n_samples, n_features)
584             Input data with the most recent imputations.
585 
586         mask_missing_values : ndarray of shape (n_samples, n_features)
587             Input data's missing indicator matrix, where `n_samples` is the
588             number of samples and `n_features` is the number of features,
589             masked by non-missing features.
590 
591         X_missing_mask : ndarray, shape (n_samples, n_features)
592             Input data's mask matrix indicating missing datapoints, where
593             `n_samples` is the number of samples and `n_features` is the
594             number of features.
595         """
596         if is_scalar_nan(self.missing_values):
597             force_all_finite = "allow-nan"
598         else:
599             force_all_finite = True
600 
601         X = self._validate_data(
602             X,
603             dtype=FLOAT_DTYPES,
604             order="F",
605             reset=in_fit,
606             force_all_finite=force_all_finite,
607         )
608         _check_inputs_dtype(X, self.missing_values)
609 
610         X_missing_mask = _get_mask(X, self.missing_values)
611         mask_missing_values = X_missing_mask.copy()
612         if self.initial_imputer_ is None:
613             self.initial_imputer_ = SimpleImputer(
614                 missing_values=self.missing_values,
615                 strategy=self.initial_strategy,
616                 keep_empty_features=self.keep_empty_features,
617             )
618             X_filled = self.initial_imputer_.fit_transform(X)
619         else:
620             X_filled = self.initial_imputer_.transform(X)
621 
622         valid_mask = np.flatnonzero(
623             np.logical_not(np.isnan(self.initial_imputer_.statistics_))
624         )
625 
626         if not self.keep_empty_features:
627             # drop empty features
628             Xt = X[:, valid_mask]
629             mask_missing_values = mask_missing_values[:, valid_mask]
630         else:
631             # mark empty features as not missing and keep the original
632             # imputation
633             mask_missing_values[:, valid_mask] = True
634             Xt = X
635 
636         return Xt, X_filled, mask_missing_values, X_missing_mask
637 
638     @staticmethod
639     def _validate_limit(limit, limit_type, n_features):
640         """Validate the limits (min/max) of the feature values.
641 
642         Converts scalar min/max limits to vectors of shape `(n_features,)`.
643 
644         Parameters
645         ----------
646         limit: scalar or array-like
647             The user-specified limit (i.e, min_value or max_value).
648         limit_type: {'max', 'min'}
649             Type of limit to validate.
650         n_features: int
651             Number of features in the dataset.
652 
653         Returns
654         -------
655         limit: ndarray, shape(n_features,)
656             Array of limits, one for each feature.
657         """
658         limit_bound = np.inf if limit_type == "max" else -np.inf
659         limit = limit_bound if limit is None else limit
660         if np.isscalar(limit):
661             limit = np.full(n_features, limit)
662         limit = check_array(limit, force_all_finite=False, copy=False, ensure_2d=False)
663         if not limit.shape[0] == n_features:
664             raise ValueError(
665                 f"'{limit_type}_value' should be of "
666                 f"shape ({n_features},) when an array-like "
667                 f"is provided. Got {limit.shape}, instead."
668             )
669         return limit
670 
671     def fit_transform(self, X, y=None):
672         """Fit the imputer on `X` and return the transformed `X`.
673 
674         Parameters
675         ----------
676         X : array-like, shape (n_samples, n_features)
677             Input data, where `n_samples` is the number of samples and
678             `n_features` is the number of features.
679 
680         y : Ignored
681             Not used, present for API consistency by convention.
682 
683         Returns
684         -------
685         Xt : array-like, shape (n_samples, n_features)
686             The imputed input data.
687         """
688         self._validate_params()
689         self.random_state_ = getattr(
690             self, "random_state_", check_random_state(self.random_state)
691         )
692 
693         if self.estimator is None:
694             from ..linear_model import BayesianRidge
695 
696             self._estimator = BayesianRidge()
697         else:
698             self._estimator = clone(self.estimator)
699 
700         self.imputation_sequence_ = []
701 
702         self.initial_imputer_ = None
703 
704         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
705             X, in_fit=True
706         )
707 
708         super()._fit_indicator(complete_mask)
709         X_indicator = super()._transform_indicator(complete_mask)
710 
711         if self.max_iter == 0 or np.all(mask_missing_values):
712             self.n_iter_ = 0
713             return super()._concatenate_indicator(Xt, X_indicator)
714 
715         # Edge case: a single feature. We return the initial ...
716         if Xt.shape[1] == 1:
717             self.n_iter_ = 0
718             return super()._concatenate_indicator(Xt, X_indicator)
719 
720         self._min_value = self._validate_limit(self.min_value, "min", X.shape[1])
721         self._max_value = self._validate_limit(self.max_value, "max", X.shape[1])
722 
723         if not np.all(np.greater(self._max_value, self._min_value)):
724             raise ValueError("One (or more) features have min_value >= max_value.")
725 
726         # order in which to impute
727         # note this is probably too slow for large feature data (d > 100000)
728         # and a better way would be good.
729         # see: https://goo.gl/KyCNwj and subsequent comments
730         ordered_idx = self._get_ordered_idx(mask_missing_values)
731         self.n_features_with_missing_ = len(ordered_idx)
732 
733         abs_corr_mat = self._get_abs_corr_mat(Xt)
734 
735         n_samples, n_features = Xt.shape
736         if self.verbose > 0:
737             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
738         start_t = time()
739         if not self.sample_posterior:
740             Xt_previous = Xt.copy()
741             normalized_tol = self.tol * np.max(np.abs(X[~mask_missing_values]))
742         for self.n_iter_ in range(1, self.max_iter + 1):
743             if self.imputation_order == "random":
744                 ordered_idx = self._get_ordered_idx(mask_missing_values)
745 
746             for feat_idx in ordered_idx:
747                 neighbor_feat_idx = self._get_neighbor_feat_idx(
748                     n_features, feat_idx, abs_corr_mat
749                 )
750                 Xt, estimator = self._impute_one_feature(
751                     Xt,
752                     mask_missing_values,
753                     feat_idx,
754                     neighbor_feat_idx,
755                     estimator=None,
756                     fit_mode=True,
757                 )
758                 estimator_triplet = _ImputerTriplet(
759                     feat_idx, neighbor_feat_idx, estimator
760                 )
761                 self.imputation_sequence_.append(estimator_triplet)
762 
763             if self.verbose > 1:
764                 print(
765                     "[IterativeImputer] Ending imputation round "
766                     "%d/%d, elapsed time %0.2f"
767                     % (self.n_iter_, self.max_iter, time() - start_t)
768                 )
769 
770             if not self.sample_posterior:
771                 inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf, axis=None)
772                 if self.verbose > 0:
773                     print(
774                         "[IterativeImputer] Change: {}, scaled tolerance: {} ".format(
775                             inf_norm, normalized_tol
776                         )
777                     )
778                 if inf_norm < normalized_tol:
779                     if self.verbose > 0:
780                         print("[IterativeImputer] Early stopping criterion reached.")
781                     break
782                 Xt_previous = Xt.copy()
783         else:
784             if not self.sample_posterior:
785                 warnings.warn(
786                     "[IterativeImputer] Early stopping criterion not reached.",
787                     ConvergenceWarning,
788                 )
789         _assign_where(Xt, X, cond=~mask_missing_values)
790 
791         return super()._concatenate_indicator(Xt, X_indicator)
792 
793     def transform(self, X):
794         """Impute all missing values in `X`.
795 
796         Note that this is stochastic, and that if `random_state` is not fixed,
797         repeated calls, or permuted input, results will differ.
798 
799         Parameters
800         ----------
801         X : array-like of shape (n_samples, n_features)
802             The input data to complete.
803 
804         Returns
805         -------
806         Xt : array-like, shape (n_samples, n_features)
807              The imputed input data.
808         """
809         check_is_fitted(self)
810 
811         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
812             X, in_fit=False
813         )
814 
815         X_indicator = super()._transform_indicator(complete_mask)
816 
817         if self.n_iter_ == 0 or np.all(mask_missing_values):
818             return super()._concatenate_indicator(Xt, X_indicator)
819 
820         imputations_per_round = len(self.imputation_sequence_) // self.n_iter_
821         i_rnd = 0
822         if self.verbose > 0:
823             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
824         start_t = time()
825         for it, estimator_triplet in enumerate(self.imputation_sequence_):
826             Xt, _ = self._impute_one_feature(
827                 Xt,
828                 mask_missing_values,
829                 estimator_triplet.feat_idx,
830                 estimator_triplet.neighbor_feat_idx,
831                 estimator=estimator_triplet.estimator,
832                 fit_mode=False,
833             )
834             if not (it + 1) % imputations_per_round:
835                 if self.verbose > 1:
836                     print(
837                         "[IterativeImputer] Ending imputation round "
838                         "%d/%d, elapsed time %0.2f"
839                         % (i_rnd + 1, self.n_iter_, time() - start_t)
840                     )
841                 i_rnd += 1
842 
843         _assign_where(Xt, X, cond=~mask_missing_values)
844 
845         return super()._concatenate_indicator(Xt, X_indicator)
846 
847     def fit(self, X, y=None):
848         """Fit the imputer on `X` and return self.
849 
850         Parameters
851         ----------
852         X : array-like, shape (n_samples, n_features)
853             Input data, where `n_samples` is the number of samples and
854             `n_features` is the number of features.
855 
856         y : Ignored
857             Not used, present for API consistency by convention.
858 
859         Returns
860         -------
861         self : object
862             Fitted estimator.
863         """
864         self.fit_transform(X)
865         return self
866 
867     def get_feature_names_out(self, input_features=None):
868         """Get output feature names for transformation.
869 
870         Parameters
871         ----------
872         input_features : array-like of str or None, default=None
873             Input features.
874 
875             - If `input_features` is `None`, then `feature_names_in_` is
876               used as feature names in. If `feature_names_in_` is not defined,
877               then the following input feature names are generated:
878               `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
879             - If `input_features` is an array-like, then `input_features` must
880               match `feature_names_in_` if `feature_names_in_` is defined.
881 
882         Returns
883         -------
884         feature_names_out : ndarray of str objects
885             Transformed feature names.
886         """
887         input_features = _check_feature_names_in(self, input_features)
888         names = self.initial_imputer_.get_feature_names_out(input_features)
889         return self._concatenate_indicator_feature_names_out(names, input_features)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>_initial_imputation</method>
<code>
565     def _initial_imputation(self, X, in_fit=False):
566         """Perform initial imputation for input `X`.
567 
568         Parameters
569         ----------
570         X : ndarray of shape (n_samples, n_features)
571             Input data, where `n_samples` is the number of samples and
572             `n_features` is the number of features.
573 
574         in_fit : bool, default=False
575             Whether function is called in :meth:`fit`.
576 
577         Returns
578         -------
579         Xt : ndarray of shape (n_samples, n_features)
580             Input data, where `n_samples` is the number of samples and
581             `n_features` is the number of features.
582 
583         X_filled : ndarray of shape (n_samples, n_features)
584             Input data with the most recent imputations.
585 
586         mask_missing_values : ndarray of shape (n_samples, n_features)
587             Input data's missing indicator matrix, where `n_samples` is the
588             number of samples and `n_features` is the number of features,
589             masked by non-missing features.
590 
591         X_missing_mask : ndarray, shape (n_samples, n_features)
592             Input data's mask matrix indicating missing datapoints, where
593             `n_samples` is the number of samples and `n_features` is the
594             number of features.
595         """
596         if is_scalar_nan(self.missing_values):
597             force_all_finite = "allow-nan"
598         else:
599             force_all_finite = True
600 
601         X = self._validate_data(
602             X,
603             dtype=FLOAT_DTYPES,
604             order="F",
605             reset=in_fit,
606             force_all_finite=force_all_finite,
607         )
608         _check_inputs_dtype(X, self.missing_values)
609 
610         X_missing_mask = _get_mask(X, self.missing_values)
611         mask_missing_values = X_missing_mask.copy()
612         if self.initial_imputer_ is None:
613             self.initial_imputer_ = SimpleImputer(
614                 missing_values=self.missing_values,
615                 strategy=self.initial_strategy,
616                 keep_empty_features=self.keep_empty_features,
617             )
618             X_filled = self.initial_imputer_.fit_transform(X)
619         else:
620             X_filled = self.initial_imputer_.transform(X)
621 
622         valid_mask = np.flatnonzero(
623             np.logical_not(np.isnan(self.initial_imputer_.statistics_))
624         )
625 
626         if not self.keep_empty_features:
627             # drop empty features
628             Xt = X[:, valid_mask]
629             mask_missing_values = mask_missing_values[:, valid_mask]
630         else:
631             # mark empty features as not missing and keep the original
632             # imputation
633             mask_missing_values[:, valid_mask] = True
634             Xt = X
635 
636         return Xt, X_filled, mask_missing_values, X_missing_mask

</code><intended_behavior>This method should be updated to utilize the `fill_value` parameter when performing the initial imputation with strategy "constant". The implementation should check if `initial_strategy` is "constant" and, if so, use the `fill_value` for replacing missing values. This adjustment ensures that `fill_value` directly influences the imputation process only in the context it's meant to (i.e., with "constant" strategy), without affecting other imputation strategies. The intended behavior is to provide a seamless way to customize initial missing values handling, which could be especially beneficial for datasets where a zero, the mean, or median value might not be the most appropriate initial guess.</intended_behavior>, <file>sklearn/impute/_iterative.py</file>
<class>IterativeImputer</class>
<method>None</method>
<code>
54 class IterativeImputer(_BaseImputer):
55     """Multivariate imputer that estimates each feature from all the others.
56 
57     A strategy for imputing missing values by modeling each feature with
58     missing values as a function of other features in a round-robin fashion.
59 
60     Read more in the :ref:`User Guide <iterative_imputer>`.
61 
62     .. versionadded:: 0.21
63 
64     .. note::
65 
66       This estimator is still **experimental** for now: the predictions
67       and the API might change without any deprecation cycle. To use it,
68       you need to explicitly import `enable_iterative_imputer`::
69 
70         >>> # explicitly require this experimental feature
71         >>> from sklearn.experimental import enable_iterative_imputer  # noqa
72         >>> # now you can import normally from sklearn.impute
73         >>> from sklearn.impute import IterativeImputer
74 
75     Parameters
76     ----------
77     estimator : estimator object, default=BayesianRidge()
78         The estimator to use at each step of the round-robin imputation.
79         If `sample_posterior=True`, the estimator must support
80         `return_std` in its `predict` method.
81 
82     missing_values : int or np.nan, default=np.nan
83         The placeholder for the missing values. All occurrences of
84         `missing_values` will be imputed. For pandas' dataframes with
85         nullable integer dtypes with missing values, `missing_values`
86         should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.
87 
88     sample_posterior : bool, default=False
89         Whether to sample from the (Gaussian) predictive posterior of the
90         fitted estimator for each imputation. Estimator must support
91         `return_std` in its `predict` method if set to `True`. Set to
92         `True` if using `IterativeImputer` for multiple imputations.
93 
94     max_iter : int, default=10
95         Maximum number of imputation rounds to perform before returning the
96         imputations computed during the final round. A round is a single
97         imputation of each feature with missing values. The stopping criterion
98         is met once `max(abs(X_t - X_{t-1}))/max(abs(X[known_vals])) < tol`,
99         where `X_t` is `X` at iteration `t`. Note that early stopping is only
100         applied if `sample_posterior=False`.
101 
102     tol : float, default=1e-3
103         Tolerance of the stopping condition.
104 
105     n_nearest_features : int, default=None
106         Number of other features to use to estimate the missing values of
107         each feature column. Nearness between features is measured using
108         the absolute correlation coefficient between each feature pair (after
109         initial imputation). To ensure coverage of features throughout the
110         imputation process, the neighbor features are not necessarily nearest,
111         but are drawn with probability proportional to correlation for each
112         imputed target feature. Can provide significant speed-up when the
113         number of features is huge. If `None`, all features will be used.
114 
115     initial_strategy : {'mean', 'median', 'most_frequent', 'constant'}, \
116             default='mean'
117         Which strategy to use to initialize the missing values. Same as the
118         `strategy` parameter in :class:`~sklearn.impute.SimpleImputer`.
119 
120     imputation_order : {'ascending', 'descending', 'roman', 'arabic', \
121             'random'}, default='ascending'
122         The order in which the features will be imputed. Possible values:
123 
124         - `'ascending'`: From features with fewest missing values to most.
125         - `'descending'`: From features with most missing values to fewest.
126         - `'roman'`: Left to right.
127         - `'arabic'`: Right to left.
128         - `'random'`: A random order for each round.
129 
130     skip_complete : bool, default=False
131         If `True` then features with missing values during :meth:`transform`
132         which did not have any missing values during :meth:`fit` will be
133         imputed with the initial imputation method only. Set to `True` if you
134         have many features with no missing values at both :meth:`fit` and
135         :meth:`transform` time to save compute.
136 
137     min_value : float or array-like of shape (n_features,), default=-np.inf
138         Minimum possible imputed value. Broadcast to shape `(n_features,)` if
139         scalar. If array-like, expects shape `(n_features,)`, one min value for
140         each feature. The default is `-np.inf`.
141 
142         .. versionchanged:: 0.23
143            Added support for array-like.
144 
145     max_value : float or array-like of shape (n_features,), default=np.inf
146         Maximum possible imputed value. Broadcast to shape `(n_features,)` if
147         scalar. If array-like, expects shape `(n_features,)`, one max value for
148         each feature. The default is `np.inf`.
149 
150         .. versionchanged:: 0.23
151            Added support for array-like.
152 
153     verbose : int, default=0
154         Verbosity flag, controls the debug messages that are issued
155         as functions are evaluated. The higher, the more verbose. Can be 0, 1,
156         or 2.
157 
158     random_state : int, RandomState instance or None, default=None
159         The seed of the pseudo random number generator to use. Randomizes
160         selection of estimator features if `n_nearest_features` is not `None`,
161         the `imputation_order` if `random`, and the sampling from posterior if
162         `sample_posterior=True`. Use an integer for determinism.
163         See :term:`the Glossary <random_state>`.
164 
165     add_indicator : bool, default=False
166         If `True`, a :class:`MissingIndicator` transform will stack onto output
167         of the imputer's transform. This allows a predictive estimator
168         to account for missingness despite imputation. If a feature has no
169         missing values at fit/train time, the feature won't appear on
170         the missing indicator even if there are missing values at
171         transform/test time.
172 
173     keep_empty_features : bool, default=False
174         If True, features that consist exclusively of missing values when
175         `fit` is called are returned in results when `transform` is called.
176         The imputed value is always `0` except when
177         `initial_strategy="constant"` in which case `fill_value` will be
178         used instead.
179 
180         .. versionadded:: 1.2
181 
182     Attributes
183     ----------
184     initial_imputer_ : object of type :class:`~sklearn.impute.SimpleImputer`
185         Imputer used to initialize the missing values.
186 
187     imputation_sequence_ : list of tuples
188         Each tuple has `(feat_idx, neighbor_feat_idx, estimator)`, where
189         `feat_idx` is the current feature to be imputed,
190         `neighbor_feat_idx` is the array of other features used to impute the
191         current feature, and `estimator` is the trained estimator used for
192         the imputation. Length is `self.n_features_with_missing_ *
193         self.n_iter_`.
194 
195     n_iter_ : int
196         Number of iteration rounds that occurred. Will be less than
197         `self.max_iter` if early stopping criterion was reached.
198 
199     n_features_in_ : int
200         Number of features seen during :term:`fit`.
201 
202         .. versionadded:: 0.24
203 
204     feature_names_in_ : ndarray of shape (`n_features_in_`,)
205         Names of features seen during :term:`fit`. Defined only when `X`
206         has feature names that are all strings.
207 
208         .. versionadded:: 1.0
209 
210     n_features_with_missing_ : int
211         Number of features with missing values.
212 
213     indicator_ : :class:`~sklearn.impute.MissingIndicator`
214         Indicator used to add binary indicators for missing values.
215         `None` if `add_indicator=False`.
216 
217     random_state_ : RandomState instance
218         RandomState instance that is generated either from a seed, the random
219         number generator or by `np.random`.
220 
221     See Also
222     --------
223     SimpleImputer : Univariate imputer for completing missing values
224         with simple strategies.
225     KNNImputer : Multivariate imputer that estimates missing features using
226         nearest samples.
227 
228     Notes
229     -----
230     To support imputation in inductive mode we store each feature's estimator
231     during the :meth:`fit` phase, and predict without refitting (in order)
232     during the :meth:`transform` phase.
233 
234     Features which contain all missing values at :meth:`fit` are discarded upon
235     :meth:`transform`.
236 
237     Using defaults, the imputer scales in :math:`\\mathcal{O}(knp^3\\min(n,p))`
238     where :math:`k` = `max_iter`, :math:`n` the number of samples and
239     :math:`p` the number of features. It thus becomes prohibitively costly when
240     the number of features increases. Setting
241     `n_nearest_features << n_features`, `skip_complete=True` or increasing `tol`
242     can help to reduce its computational cost.
243 
244     Depending on the nature of missing values, simple imputers can be
245     preferable in a prediction context.
246 
247     References
248     ----------
249     .. [1] `Stef van Buuren, Karin Groothuis-Oudshoorn (2011). "mice:
250         Multivariate Imputation by Chained Equations in R". Journal of
251         Statistical Software 45: 1-67.
252         <https://www.jstatsoft.org/article/view/v045i03>`_
253 
254     .. [2] `S. F. Buck, (1960). "A Method of Estimation of Missing Values in
255         Multivariate Data Suitable for use with an Electronic Computer".
256         Journal of the Royal Statistical Society 22(2): 302-306.
257         <https://www.jstor.org/stable/2984099>`_
258 
259     Examples
260     --------
261     >>> import numpy as np
262     >>> from sklearn.experimental import enable_iterative_imputer
263     >>> from sklearn.impute import IterativeImputer
264     >>> imp_mean = IterativeImputer(random_state=0)
265     >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
266     IterativeImputer(random_state=0)
267     >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
268     >>> imp_mean.transform(X)
269     array([[ 6.9584...,  2.       ,  3.        ],
270            [ 4.       ,  2.6000...,  6.        ],
271            [10.       ,  4.9999...,  9.        ]])
272     """
273 
274     _parameter_constraints: dict = {
275         **_BaseImputer._parameter_constraints,
276         "estimator": [None, HasMethods(["fit", "predict"])],
277         "sample_posterior": ["boolean"],
278         "max_iter": [Interval(Integral, 0, None, closed="left")],
279         "tol": [Interval(Real, 0, None, closed="left")],
280         "n_nearest_features": [None, Interval(Integral, 1, None, closed="left")],
281         "initial_strategy": [
282             StrOptions({"mean", "median", "most_frequent", "constant"})
283         ],
284         "imputation_order": [
285             StrOptions({"ascending", "descending", "roman", "arabic", "random"})
286         ],
287         "skip_complete": ["boolean"],
288         "min_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
289         "max_value": [None, Interval(Real, None, None, closed="both"), "array-like"],
290         "verbose": ["verbose"],
291         "random_state": ["random_state"],
292     }
293 
294     def __init__(
295         self,
296         estimator=None,
297         *,
298         missing_values=np.nan,
299         sample_posterior=False,
300         max_iter=10,
301         tol=1e-3,
302         n_nearest_features=None,
303         initial_strategy="mean",
304         imputation_order="ascending",
305         skip_complete=False,
306         min_value=-np.inf,
307         max_value=np.inf,
308         verbose=0,
309         random_state=None,
310         add_indicator=False,
311         keep_empty_features=False,
312     ):
313         super().__init__(
314             missing_values=missing_values,
315             add_indicator=add_indicator,
316             keep_empty_features=keep_empty_features,
317         )
318 
319         self.estimator = estimator
320         self.sample_posterior = sample_posterior
321         self.max_iter = max_iter
322         self.tol = tol
323         self.n_nearest_features = n_nearest_features
324         self.initial_strategy = initial_strategy
325         self.imputation_order = imputation_order
326         self.skip_complete = skip_complete
327         self.min_value = min_value
328         self.max_value = max_value
329         self.verbose = verbose
330         self.random_state = random_state
331 
332     def _impute_one_feature(
333         self,
334         X_filled,
335         mask_missing_values,
336         feat_idx,
337         neighbor_feat_idx,
338         estimator=None,
339         fit_mode=True,
340     ):
341         """Impute a single feature from the others provided.
342 
343         This function predicts the missing values of one of the features using
344         the current estimates of all the other features. The `estimator` must
345         support `return_std=True` in its `predict` method for this function
346         to work.
347 
348         Parameters
349         ----------
350         X_filled : ndarray
351             Input data with the most recent imputations.
352 
353         mask_missing_values : ndarray
354             Input data's missing indicator matrix.
355 
356         feat_idx : int
357             Index of the feature currently being imputed.
358 
359         neighbor_feat_idx : ndarray
360             Indices of the features to be used in imputing `feat_idx`.
361 
362         estimator : object
363             The estimator to use at this step of the round-robin imputation.
364             If `sample_posterior=True`, the estimator must support
365             `return_std` in its `predict` method.
366             If None, it will be cloned from self._estimator.
367 
368         fit_mode : boolean, default=True
369             Whether to fit and predict with the estimator or just predict.
370 
371         Returns
372         -------
373         X_filled : ndarray
374             Input data with `X_filled[missing_row_mask, feat_idx]` updated.
375 
376         estimator : estimator with sklearn API
377             The fitted estimator used to impute
378             `X_filled[missing_row_mask, feat_idx]`.
379         """
380         if estimator is None and fit_mode is False:
381             raise ValueError(
382                 "If fit_mode is False, then an already-fitted "
383                 "estimator should be passed in."
384             )
385 
386         if estimator is None:
387             estimator = clone(self._estimator)
388 
389         missing_row_mask = mask_missing_values[:, feat_idx]
390         if fit_mode:
391             X_train = _safe_indexing(
392                 _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
393                 ~missing_row_mask,
394                 axis=0,
395             )
396             y_train = _safe_indexing(
397                 _safe_indexing(X_filled, feat_idx, axis=1),
398                 ~missing_row_mask,
399                 axis=0,
400             )
401             estimator.fit(X_train, y_train)
402 
403         # if no missing values, don't predict
404         if np.sum(missing_row_mask) == 0:
405             return X_filled, estimator
406 
407         # get posterior samples if there is at least one missing value
408         X_test = _safe_indexing(
409             _safe_indexing(X_filled, neighbor_feat_idx, axis=1),
410             missing_row_mask,
411             axis=0,
412         )
413         if self.sample_posterior:
414             mus, sigmas = estimator.predict(X_test, return_std=True)
415             imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
416             # two types of problems: (1) non-positive sigmas
417             # (2) mus outside legal range of min_value and max_value
418             # (results in inf sample)
419             positive_sigmas = sigmas > 0
420             imputed_values[~positive_sigmas] = mus[~positive_sigmas]
421             mus_too_low = mus < self._min_value[feat_idx]
422             imputed_values[mus_too_low] = self._min_value[feat_idx]
423             mus_too_high = mus > self._max_value[feat_idx]
424             imputed_values[mus_too_high] = self._max_value[feat_idx]
425             # the rest can be sampled without statistical issues
426             inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high
427             mus = mus[inrange_mask]
428             sigmas = sigmas[inrange_mask]
429             a = (self._min_value[feat_idx] - mus) / sigmas
430             b = (self._max_value[feat_idx] - mus) / sigmas
431 
432             truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
433             imputed_values[inrange_mask] = truncated_normal.rvs(
434                 random_state=self.random_state_
435             )
436         else:
437             imputed_values = estimator.predict(X_test)
438             imputed_values = np.clip(
439                 imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]
440             )
441 
442         # update the feature
443         _safe_assign(
444             X_filled,
445             imputed_values,
446             row_indexer=missing_row_mask,
447             column_indexer=feat_idx,
448         )
449         return X_filled, estimator
450 
451     def _get_neighbor_feat_idx(self, n_features, feat_idx, abs_corr_mat):
452         """Get a list of other features to predict `feat_idx`.
453 
454         If `self.n_nearest_features` is less than or equal to the total
455         number of features, then use a probability proportional to the absolute
456         correlation between `feat_idx` and each other feature to randomly
457         choose a subsample of the other features (without replacement).
458 
459         Parameters
460         ----------
461         n_features : int
462             Number of features in `X`.
463 
464         feat_idx : int
465             Index of the feature currently being imputed.
466 
467         abs_corr_mat : ndarray, shape (n_features, n_features)
468             Absolute correlation matrix of `X`. The diagonal has been zeroed
469             out and each feature has been normalized to sum to 1. Can be None.
470 
471         Returns
472         -------
473         neighbor_feat_idx : array-like
474             The features to use to impute `feat_idx`.
475         """
476         if self.n_nearest_features is not None and self.n_nearest_features < n_features:
477             p = abs_corr_mat[:, feat_idx]
478             neighbor_feat_idx = self.random_state_.choice(
479                 np.arange(n_features), self.n_nearest_features, replace=False, p=p
480             )
481         else:
482             inds_left = np.arange(feat_idx)
483             inds_right = np.arange(feat_idx + 1, n_features)
484             neighbor_feat_idx = np.concatenate((inds_left, inds_right))
485         return neighbor_feat_idx
486 
487     def _get_ordered_idx(self, mask_missing_values):
488         """Decide in what order we will update the features.
489 
490         As a homage to the MICE R package, we will have 4 main options of
491         how to order the updates, and use a random order if anything else
492         is specified.
493 
494         Also, this function skips features which have no missing values.
495 
496         Parameters
497         ----------
498         mask_missing_values : array-like, shape (n_samples, n_features)
499             Input data's missing indicator matrix, where `n_samples` is the
500             number of samples and `n_features` is the number of features.
501 
502         Returns
503         -------
504         ordered_idx : ndarray, shape (n_features,)
505             The order in which to impute the features.
506         """
507         frac_of_missing_values = mask_missing_values.mean(axis=0)
508         if self.skip_complete:
509             missing_values_idx = np.flatnonzero(frac_of_missing_values)
510         else:
511             missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])
512         if self.imputation_order == "roman":
513             ordered_idx = missing_values_idx
514         elif self.imputation_order == "arabic":
515             ordered_idx = missing_values_idx[::-1]
516         elif self.imputation_order == "ascending":
517             n = len(frac_of_missing_values) - len(missing_values_idx)
518             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:]
519         elif self.imputation_order == "descending":
520             n = len(frac_of_missing_values) - len(missing_values_idx)
521             ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:][::-1]
522         elif self.imputation_order == "random":
523             ordered_idx = missing_values_idx
524             self.random_state_.shuffle(ordered_idx)
525         return ordered_idx
526 
527     def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
528         """Get absolute correlation matrix between features.
529 
530         Parameters
531         ----------
532         X_filled : ndarray, shape (n_samples, n_features)
533             Input data with the most recent imputations.
534 
535         tolerance : float, default=1e-6
536             `abs_corr_mat` can have nans, which will be replaced
537             with `tolerance`.
538 
539         Returns
540         -------
541         abs_corr_mat : ndarray, shape (n_features, n_features)
542             Absolute correlation matrix of `X` at the beginning of the
543             current round. The diagonal has been zeroed out and each feature's
544             absolute correlations with all others have been normalized to sum
545             to 1.
546         """
547         n_features = X_filled.shape[1]
548         if self.n_nearest_features is None or self.n_nearest_features >= n_features:
549             return None
550         with np.errstate(invalid="ignore"):
551             # if a feature in the neighborhood has only a single value
552             # (e.g., categorical feature), the std. dev. will be null and
553             # np.corrcoef will raise a warning due to a division by zero
554             abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
555         # np.corrcoef is not defined for features with zero std
556         abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
557         # ensures exploration, i.e. at least some probability of sampling
558         np.clip(abs_corr_mat, tolerance, None, out=abs_corr_mat)
559         # features are not their own neighbors
560         np.fill_diagonal(abs_corr_mat, 0)
561         # needs to sum to 1 for np.random.choice sampling
562         abs_corr_mat = normalize(abs_corr_mat, norm="l1", axis=0, copy=False)
563         return abs_corr_mat
564 
565     def _initial_imputation(self, X, in_fit=False):
566         """Perform initial imputation for input `X`.
567 
568         Parameters
569         ----------
570         X : ndarray of shape (n_samples, n_features)
571             Input data, where `n_samples` is the number of samples and
572             `n_features` is the number of features.
573 
574         in_fit : bool, default=False
575             Whether function is called in :meth:`fit`.
576 
577         Returns
578         -------
579         Xt : ndarray of shape (n_samples, n_features)
580             Input data, where `n_samples` is the number of samples and
581             `n_features` is the number of features.
582 
583         X_filled : ndarray of shape (n_samples, n_features)
584             Input data with the most recent imputations.
585 
586         mask_missing_values : ndarray of shape (n_samples, n_features)
587             Input data's missing indicator matrix, where `n_samples` is the
588             number of samples and `n_features` is the number of features,
589             masked by non-missing features.
590 
591         X_missing_mask : ndarray, shape (n_samples, n_features)
592             Input data's mask matrix indicating missing datapoints, where
593             `n_samples` is the number of samples and `n_features` is the
594             number of features.
595         """
596         if is_scalar_nan(self.missing_values):
597             force_all_finite = "allow-nan"
598         else:
599             force_all_finite = True
600 
601         X = self._validate_data(
602             X,
603             dtype=FLOAT_DTYPES,
604             order="F",
605             reset=in_fit,
606             force_all_finite=force_all_finite,
607         )
608         _check_inputs_dtype(X, self.missing_values)
609 
610         X_missing_mask = _get_mask(X, self.missing_values)
611         mask_missing_values = X_missing_mask.copy()
612         if self.initial_imputer_ is None:
613             self.initial_imputer_ = SimpleImputer(
614                 missing_values=self.missing_values,
615                 strategy=self.initial_strategy,
616                 keep_empty_features=self.keep_empty_features,
617             )
618             X_filled = self.initial_imputer_.fit_transform(X)
619         else:
620             X_filled = self.initial_imputer_.transform(X)
621 
622         valid_mask = np.flatnonzero(
623             np.logical_not(np.isnan(self.initial_imputer_.statistics_))
624         )
625 
626         if not self.keep_empty_features:
627             # drop empty features
628             Xt = X[:, valid_mask]
629             mask_missing_values = mask_missing_values[:, valid_mask]
630         else:
631             # mark empty features as not missing and keep the original
632             # imputation
633             mask_missing_values[:, valid_mask] = True
634             Xt = X
635 
636         return Xt, X_filled, mask_missing_values, X_missing_mask
637 
638     @staticmethod
639     def _validate_limit(limit, limit_type, n_features):
640         """Validate the limits (min/max) of the feature values.
641 
642         Converts scalar min/max limits to vectors of shape `(n_features,)`.
643 
644         Parameters
645         ----------
646         limit: scalar or array-like
647             The user-specified limit (i.e, min_value or max_value).
648         limit_type: {'max', 'min'}
649             Type of limit to validate.
650         n_features: int
651             Number of features in the dataset.
652 
653         Returns
654         -------
655         limit: ndarray, shape(n_features,)
656             Array of limits, one for each feature.
657         """
658         limit_bound = np.inf if limit_type == "max" else -np.inf
659         limit = limit_bound if limit is None else limit
660         if np.isscalar(limit):
661             limit = np.full(n_features, limit)
662         limit = check_array(limit, force_all_finite=False, copy=False, ensure_2d=False)
663         if not limit.shape[0] == n_features:
664             raise ValueError(
665                 f"'{limit_type}_value' should be of "
666                 f"shape ({n_features},) when an array-like "
667                 f"is provided. Got {limit.shape}, instead."
668             )
669         return limit
670 
671     def fit_transform(self, X, y=None):
672         """Fit the imputer on `X` and return the transformed `X`.
673 
674         Parameters
675         ----------
676         X : array-like, shape (n_samples, n_features)
677             Input data, where `n_samples` is the number of samples and
678             `n_features` is the number of features.
679 
680         y : Ignored
681             Not used, present for API consistency by convention.
682 
683         Returns
684         -------
685         Xt : array-like, shape (n_samples, n_features)
686             The imputed input data.
687         """
688         self._validate_params()
689         self.random_state_ = getattr(
690             self, "random_state_", check_random_state(self.random_state)
691         )
692 
693         if self.estimator is None:
694             from ..linear_model import BayesianRidge
695 
696             self._estimator = BayesianRidge()
697         else:
698             self._estimator = clone(self.estimator)
699 
700         self.imputation_sequence_ = []
701 
702         self.initial_imputer_ = None
703 
704         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
705             X, in_fit=True
706         )
707 
708         super()._fit_indicator(complete_mask)
709         X_indicator = super()._transform_indicator(complete_mask)
710 
711         if self.max_iter == 0 or np.all(mask_missing_values):
712             self.n_iter_ = 0
713             return super()._concatenate_indicator(Xt, X_indicator)
714 
715         # Edge case: a single feature. We return the initial ...
716         if Xt.shape[1] == 1:
717             self.n_iter_ = 0
718             return super()._concatenate_indicator(Xt, X_indicator)
719 
720         self._min_value = self._validate_limit(self.min_value, "min", X.shape[1])
721         self._max_value = self._validate_limit(self.max_value, "max", X.shape[1])
722 
723         if not np.all(np.greater(self._max_value, self._min_value)):
724             raise ValueError("One (or more) features have min_value >= max_value.")
725 
726         # order in which to impute
727         # note this is probably too slow for large feature data (d > 100000)
728         # and a better way would be good.
729         # see: https://goo.gl/KyCNwj and subsequent comments
730         ordered_idx = self._get_ordered_idx(mask_missing_values)
731         self.n_features_with_missing_ = len(ordered_idx)
732 
733         abs_corr_mat = self._get_abs_corr_mat(Xt)
734 
735         n_samples, n_features = Xt.shape
736         if self.verbose > 0:
737             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
738         start_t = time()
739         if not self.sample_posterior:
740             Xt_previous = Xt.copy()
741             normalized_tol = self.tol * np.max(np.abs(X[~mask_missing_values]))
742         for self.n_iter_ in range(1, self.max_iter + 1):
743             if self.imputation_order == "random":
744                 ordered_idx = self._get_ordered_idx(mask_missing_values)
745 
746             for feat_idx in ordered_idx:
747                 neighbor_feat_idx = self._get_neighbor_feat_idx(
748                     n_features, feat_idx, abs_corr_mat
749                 )
750                 Xt, estimator = self._impute_one_feature(
751                     Xt,
752                     mask_missing_values,
753                     feat_idx,
754                     neighbor_feat_idx,
755                     estimator=None,
756                     fit_mode=True,
757                 )
758                 estimator_triplet = _ImputerTriplet(
759                     feat_idx, neighbor_feat_idx, estimator
760                 )
761                 self.imputation_sequence_.append(estimator_triplet)
762 
763             if self.verbose > 1:
764                 print(
765                     "[IterativeImputer] Ending imputation round "
766                     "%d/%d, elapsed time %0.2f"
767                     % (self.n_iter_, self.max_iter, time() - start_t)
768                 )
769 
770             if not self.sample_posterior:
771                 inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf, axis=None)
772                 if self.verbose > 0:
773                     print(
774                         "[IterativeImputer] Change: {}, scaled tolerance: {} ".format(
775                             inf_norm, normalized_tol
776                         )
777                     )
778                 if inf_norm < normalized_tol:
779                     if self.verbose > 0:
780                         print("[IterativeImputer] Early stopping criterion reached.")
781                     break
782                 Xt_previous = Xt.copy()
783         else:
784             if not self.sample_posterior:
785                 warnings.warn(
786                     "[IterativeImputer] Early stopping criterion not reached.",
787                     ConvergenceWarning,
788                 )
789         _assign_where(Xt, X, cond=~mask_missing_values)
790 
791         return super()._concatenate_indicator(Xt, X_indicator)
792 
793     def transform(self, X):
794         """Impute all missing values in `X`.
795 
796         Note that this is stochastic, and that if `random_state` is not fixed,
797         repeated calls, or permuted input, results will differ.
798 
799         Parameters
800         ----------
801         X : array-like of shape (n_samples, n_features)
802             The input data to complete.
803 
804         Returns
805         -------
806         Xt : array-like, shape (n_samples, n_features)
807              The imputed input data.
808         """
809         check_is_fitted(self)
810 
811         X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
812             X, in_fit=False
813         )
814 
815         X_indicator = super()._transform_indicator(complete_mask)
816 
817         if self.n_iter_ == 0 or np.all(mask_missing_values):
818             return super()._concatenate_indicator(Xt, X_indicator)
819 
820         imputations_per_round = len(self.imputation_sequence_) // self.n_iter_
821         i_rnd = 0
822         if self.verbose > 0:
823             print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
824         start_t = time()
825         for it, estimator_triplet in enumerate(self.imputation_sequence_):
826             Xt, _ = self._impute_one_feature(
827                 Xt,
828                 mask_missing_values,
829                 estimator_triplet.feat_idx,
830                 estimator_triplet.neighbor_feat_idx,
831                 estimator=estimator_triplet.estimator,
832                 fit_mode=False,
833             )
834             if not (it + 1) % imputations_per_round:
835                 if self.verbose > 1:
836                     print(
837                         "[IterativeImputer] Ending imputation round "
838                         "%d/%d, elapsed time %0.2f"
839                         % (i_rnd + 1, self.n_iter_, time() - start_t)
840                     )
841                 i_rnd += 1
842 
843         _assign_where(Xt, X, cond=~mask_missing_values)
844 
845         return super()._concatenate_indicator(Xt, X_indicator)
846 
847     def fit(self, X, y=None):
848         """Fit the imputer on `X` and return self.
849 
850         Parameters
851         ----------
852         X : array-like, shape (n_samples, n_features)
853             Input data, where `n_samples` is the number of samples and
854             `n_features` is the number of features.
855 
856         y : Ignored
857             Not used, present for API consistency by convention.
858 
859         Returns
860         -------
861         self : object
862             Fitted estimator.
863         """
864         self.fit_transform(X)
865         return self
866 
867     def get_feature_names_out(self, input_features=None):
868         """Get output feature names for transformation.
869 
870         Parameters
871         ----------
872         input_features : array-like of str or None, default=None
873             Input features.
874 
875             - If `input_features` is `None`, then `feature_names_in_` is
876               used as feature names in. If `feature_names_in_` is not defined,
877               then the following input feature names are generated:
878               `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
879             - If `input_features` is an array-like, then `input_features` must
880               match `feature_names_in_` if `feature_names_in_` is defined.
881 
882         Returns
883         -------
884         feature_names_out : ndarray of str objects
885             Transformed feature names.
886         """
887         input_features = _check_feature_names_in(self, input_features)
888         names = self.initial_imputer_.get_feature_names_out(input_features)
889         return self._concatenate_indicator_feature_names_out(names, input_features)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-16 00:04:42.447 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-16 00:04:42.448 | INFO     | Search-only: skipping patch selection
2025-10-16 00:04:42.545 | INFO     | Task scikit-learn__scikit-learn-25232 completed successfully.
2025-10-16 00:04:42.546 | INFO     | No patch generated. You can try running ACR again.
