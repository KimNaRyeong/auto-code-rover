[
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 2001,
        "end": 2043,
        "class_name": "DataArray",
        "method_name": "rename",
        "code": "2001     def rename(\n2002         self,\n2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,\n2004         **names: Hashable,\n2005     ) -> DataArray:\n2006         \"\"\"Returns a new DataArray with renamed coordinates, dimensions or a new name.\n2007 \n2008         Parameters\n2009         ----------\n2010         new_name_or_name_dict : str or dict-like, optional\n2011             If the argument is dict-like, it used as a mapping from old\n2012             names to new names for coordinates or dimensions. Otherwise,\n2013             use the argument as the new name for this array.\n2014         **names : Hashable, optional\n2015             The keyword arguments form of a mapping from old names to\n2016             new names for coordinates or dimensions.\n2017             One of new_name_or_name_dict or names must be provided.\n2018 \n2019         Returns\n2020         -------\n2021         renamed : DataArray\n2022             Renamed array or array with renamed coordinates.\n2023 \n2024         See Also\n2025         --------\n2026         Dataset.rename\n2027         DataArray.swap_dims\n2028         \"\"\"\n2029         if new_name_or_name_dict is None and not names:\n2030             # change name to None?\n2031             return self._replace(name=None)\n2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n2033             # change dims/coords\n2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n2035             dataset = self._to_temp_dataset().rename(name_dict)\n2036             return self._from_temp_dataset(dataset)\n2037         if utils.hashable(new_name_or_name_dict) and names:\n2038             # change name + dims/coords\n2039             dataset = self._to_temp_dataset().rename(names)\n2040             dataarray = self._from_temp_dataset(dataset)\n2041             return dataarray._replace(name=new_name_or_name_dict)\n2042         # only change name\n2043         return self._replace(name=new_name_or_name_dict)\n",
        "intended_behavior": "This method, when renaming coordinates into a dimension coordinate, should either automatically create an index for the new dimension coordinate or raise a warning/error advising the use of `set_index` for such operations. This change ensures that all dimension coordinates have appropriate indexes for subsequent data selection operations. If the former approach (automatic index creation) is chosen, it should be implemented in a way that does not disrupt existing functionality for users who do not require an index."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 223,
        "end": 5777,
        "class_name": "DataArray",
        "method_name": null,
        "code": "223 class DataArray(\n224     AbstractArray, DataWithCoords, DataArrayArithmetic, DataArrayReductions\n225 ):\n226     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n227 \n228     DataArray provides a wrapper around numpy ndarrays that uses\n229     labeled dimensions and coordinates to support metadata aware\n230     operations. The API is similar to that for the pandas Series or\n231     DataFrame, but DataArray objects can have any number of dimensions,\n232     and their contents have fixed data types.\n233 \n234     Additional features over raw numpy arrays:\n235 \n236     - Apply operations over dimensions by name: ``x.sum('time')``.\n237     - Select or assign values by integer location (like numpy):\n238       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or\n239       ``x.sel(time='2014-01-01')``.\n240     - Mathematical operations (e.g., ``x - y``) vectorize across\n241       multiple dimensions (known in numpy as \"broadcasting\") based on\n242       dimension names, regardless of their original order.\n243     - Keep track of arbitrary metadata in the form of a Python\n244       dictionary: ``x.attrs``\n245     - Convert to a pandas Series: ``x.to_series()``.\n246 \n247     Getting items from or doing mathematical operations with a\n248     DataArray always returns another DataArray.\n249 \n250     Parameters\n251     ----------\n252     data : array_like\n253         Values for this array. Must be an ``numpy.ndarray``, ndarray\n254         like, or castable to an ``ndarray``. If a self-described xarray\n255         or pandas object, attempts are made to use this array's\n256         metadata to fill in other unspecified arguments. A view of the\n257         array's data is used instead of a copy if possible.\n258     coords : sequence or dict of array_like, optional\n259         Coordinates (tick labels) to use for indexing along each\n260         dimension. The following notations are accepted:\n261 \n262         - mapping {dimension name: array-like}\n263         - sequence of tuples that are valid arguments for\n264           ``xarray.Variable()``\n265           - (dims, data)\n266           - (dims, data, attrs)\n267           - (dims, data, attrs, encoding)\n268 \n269         Additionally, it is possible to define a coord whose name\n270         does not match the dimension name, or a coord based on multiple\n271         dimensions, with one of the following notations:\n272 \n273         - mapping {coord name: DataArray}\n274         - mapping {coord name: Variable}\n275         - mapping {coord name: (dimension name, array-like)}\n276         - mapping {coord name: (tuple of dimension names, array-like)}\n277 \n278     dims : Hashable or sequence of Hashable, optional\n279         Name(s) of the data dimension(s). Must be either a Hashable\n280         (only for 1D data) or a sequence of Hashables with length equal\n281         to the number of dimensions. If this argument is omitted,\n282         dimension names are taken from ``coords`` (if possible) and\n283         otherwise default to ``['dim_0', ... 'dim_n']``.\n284     name : str or None, optional\n285         Name of this array.\n286     attrs : dict_like or None, optional\n287         Attributes to assign to the new instance. By default, an empty\n288         attribute dictionary is initialized.\n289 \n290     Examples\n291     --------\n292     Create data:\n293 \n294     >>> np.random.seed(0)\n295     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n296     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n297     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n298     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n299     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n300 \n301     Initialize a dataarray with multiple dimensions:\n302 \n303     >>> da = xr.DataArray(\n304     ...     data=temperature,\n305     ...     dims=[\"x\", \"y\", \"time\"],\n306     ...     coords=dict(\n307     ...         lon=([\"x\", \"y\"], lon),\n308     ...         lat=([\"x\", \"y\"], lat),\n309     ...         time=time,\n310     ...         reference_time=reference_time,\n311     ...     ),\n312     ...     attrs=dict(\n313     ...         description=\"Ambient temperature.\",\n314     ...         units=\"degC\",\n315     ...     ),\n316     ... )\n317     >>> da\n318     <xarray.DataArray (x: 2, y: 2, time: 3)>\n319     array([[[29.11241877, 18.20125767, 22.82990387],\n320             [32.92714559, 29.94046392,  7.18177696]],\n321     <BLANKLINE>\n322            [[22.60070734, 13.78914233, 14.17424919],\n323             [18.28478802, 16.15234857, 26.63418806]]])\n324     Coordinates:\n325         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n326         lat             (x, y) float64 42.25 42.21 42.63 42.59\n327       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n328         reference_time  datetime64[ns] 2014-09-05\n329     Dimensions without coordinates: x, y\n330     Attributes:\n331         description:  Ambient temperature.\n332         units:        degC\n333 \n334     Find out where the coldest temperature was:\n335 \n336     >>> da.isel(da.argmin(...))\n337     <xarray.DataArray ()>\n338     array(7.18177696)\n339     Coordinates:\n340         lon             float64 -99.32\n341         lat             float64 42.21\n342         time            datetime64[ns] 2014-09-08\n343         reference_time  datetime64[ns] 2014-09-05\n344     Attributes:\n345         description:  Ambient temperature.\n346         units:        degC\n347     \"\"\"\n348 \n349     _cache: dict[str, Any]\n350     _coords: dict[Any, Variable]\n351     _close: Callable[[], None] | None\n352     _indexes: dict[Hashable, Index]\n353     _name: Hashable | None\n354     _variable: Variable\n355 \n356     __slots__ = (\n357         \"_cache\",\n358         \"_coords\",\n359         \"_close\",\n360         \"_indexes\",\n361         \"_name\",\n362         \"_variable\",\n363         \"__weakref__\",\n364     )\n365 \n366     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor[\"DataArray\"])\n367 \n368     def __init__(\n369         self,\n370         data: Any = dtypes.NA,\n371         coords: Sequence[Sequence[Any] | pd.Index | DataArray]\n372         | Mapping[Any, Any]\n373         | None = None,\n374         dims: Hashable | Sequence[Hashable] | None = None,\n375         name: Hashable = None,\n376         attrs: Mapping = None,\n377         # internal parameters\n378         indexes: dict[Hashable, Index] = None,\n379         fastpath: bool = False,\n380     ) -> None:\n381         if fastpath:\n382             variable = data\n383             assert dims is None\n384             assert attrs is None\n385             assert indexes is not None\n386         else:\n387             # TODO: (benbovy - explicit indexes) remove\n388             # once it becomes part of the public interface\n389             if indexes is not None:\n390                 raise ValueError(\"Providing explicit indexes is not supported yet\")\n391 \n392             # try to fill in arguments from data if they weren't supplied\n393             if coords is None:\n394 \n395                 if isinstance(data, DataArray):\n396                     coords = data.coords\n397                 elif isinstance(data, pd.Series):\n398                     coords = [data.index]\n399                 elif isinstance(data, pd.DataFrame):\n400                     coords = [data.index, data.columns]\n401                 elif isinstance(data, (pd.Index, IndexVariable)):\n402                     coords = [data]\n403 \n404             if dims is None:\n405                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n406             if name is None:\n407                 name = getattr(data, \"name\", None)\n408             if attrs is None and not isinstance(data, PANDAS_TYPES):\n409                 attrs = getattr(data, \"attrs\", None)\n410 \n411             data = _check_data_shape(data, coords, dims)\n412             data = as_compatible_data(data)\n413             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n414             variable = Variable(dims, data, attrs, fastpath=True)\n415             indexes, coords = _create_indexes_from_coords(coords)\n416 \n417         # These fully describe a DataArray\n418         self._variable = variable\n419         assert isinstance(coords, dict)\n420         self._coords = coords\n421         self._name = name\n422 \n423         # TODO(shoyer): document this argument, once it becomes part of the\n424         # public interface.\n425         self._indexes = indexes  # type: ignore[assignment]\n426 \n427         self._close = None\n428 \n429     @classmethod\n430     def _construct_direct(\n431         cls: type[T_DataArray],\n432         variable: Variable,\n433         coords: dict[Any, Variable],\n434         name: Hashable,\n435         indexes: dict[Hashable, Index],\n436     ) -> T_DataArray:\n437         \"\"\"Shortcut around __init__ for internal use when we want to skip\n438         costly validation\n439         \"\"\"\n440         obj = object.__new__(cls)\n441         obj._variable = variable\n442         obj._coords = coords\n443         obj._name = name\n444         obj._indexes = indexes\n445         obj._close = None\n446         return obj\n447 \n448     def _replace(\n449         self: T_DataArray,\n450         variable: Variable = None,\n451         coords=None,\n452         name: Hashable | None | Default = _default,\n453         indexes=None,\n454     ) -> T_DataArray:\n455         if variable is None:\n456             variable = self.variable\n457         if coords is None:\n458             coords = self._coords\n459         if indexes is None:\n460             indexes = self._indexes\n461         if name is _default:\n462             name = self.name\n463         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)\n464 \n465     def _replace_maybe_drop_dims(\n466         self: T_DataArray,\n467         variable: Variable,\n468         name: Hashable | None | Default = _default,\n469     ) -> T_DataArray:\n470         if variable.dims == self.dims and variable.shape == self.shape:\n471             coords = self._coords.copy()\n472             indexes = self._indexes\n473         elif variable.dims == self.dims:\n474             # Shape has changed (e.g. from reduce(..., keepdims=True)\n475             new_sizes = dict(zip(self.dims, variable.shape))\n476             coords = {\n477                 k: v\n478                 for k, v in self._coords.items()\n479                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n480             }\n481             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n482         else:\n483             allowed_dims = set(variable.dims)\n484             coords = {\n485                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n486             }\n487             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n488         return self._replace(variable, coords, name, indexes=indexes)\n489 \n490     def _overwrite_indexes(\n491         self: T_DataArray,\n492         indexes: Mapping[Any, Index],\n493         coords: Mapping[Any, Variable] = None,\n494         drop_coords: list[Hashable] = None,\n495         rename_dims: Mapping[Any, Any] = None,\n496     ) -> T_DataArray:\n497         \"\"\"Maybe replace indexes and their corresponding coordinates.\"\"\"\n498         if not indexes:\n499             return self\n500 \n501         if coords is None:\n502             coords = {}\n503         if drop_coords is None:\n504             drop_coords = []\n505 \n506         new_variable = self.variable.copy()\n507         new_coords = self._coords.copy()\n508         new_indexes = dict(self._indexes)\n509 \n510         for name in indexes:\n511             new_coords[name] = coords[name]\n512             new_indexes[name] = indexes[name]\n513 \n514         for name in drop_coords:\n515             new_coords.pop(name)\n516             new_indexes.pop(name)\n517 \n518         if rename_dims:\n519             new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]\n520 \n521         return self._replace(\n522             variable=new_variable, coords=new_coords, indexes=new_indexes\n523         )\n524 \n525     def _to_temp_dataset(self) -> Dataset:\n526         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n527 \n528     def _from_temp_dataset(\n529         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default\n530     ) -> T_DataArray:\n531         variable = dataset._variables.pop(_THIS_ARRAY)\n532         coords = dataset._variables\n533         indexes = dataset._indexes\n534         return self._replace(variable, coords, name, indexes=indexes)\n535 \n536     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n537         \"\"\"splits dataarray along dimension 'dim'\"\"\"\n538 \n539         def subset(dim, label):\n540             array = self.loc[{dim: label}]\n541             array.attrs = {}\n542             return as_variable(array)\n543 \n544         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n545         variables.update({k: v for k, v in self._coords.items() if k != dim})\n546         coord_names = set(self._coords) - {dim}\n547         indexes = filter_indexes_from_coords(self._indexes, coord_names)\n548         dataset = Dataset._construct_direct(\n549             variables, coord_names, indexes=indexes, attrs=self.attrs\n550         )\n551         return dataset\n552 \n553     def _to_dataset_whole(\n554         self, name: Hashable = None, shallow_copy: bool = True\n555     ) -> Dataset:\n556         if name is None:\n557             name = self.name\n558         if name is None:\n559             raise ValueError(\n560                 \"unable to convert unnamed DataArray to a \"\n561                 \"Dataset without providing an explicit name\"\n562             )\n563         if name in self.coords:\n564             raise ValueError(\n565                 \"cannot create a Dataset from a DataArray with \"\n566                 \"the same name as one of its coordinates\"\n567             )\n568         # use private APIs for speed: this is called by _to_temp_dataset(),\n569         # which is used in the guts of a lot of operations (e.g., reindex)\n570         variables = self._coords.copy()\n571         variables[name] = self.variable\n572         if shallow_copy:\n573             for k in variables:\n574                 variables[k] = variables[k].copy(deep=False)\n575         indexes = self._indexes\n576 \n577         coord_names = set(self._coords)\n578         return Dataset._construct_direct(variables, coord_names, indexes=indexes)\n579 \n580     def to_dataset(\n581         self,\n582         dim: Hashable = None,\n583         *,\n584         name: Hashable = None,\n585         promote_attrs: bool = False,\n586     ) -> Dataset:\n587         \"\"\"Convert a DataArray to a Dataset.\n588 \n589         Parameters\n590         ----------\n591         dim : Hashable, optional\n592             Name of the dimension on this array along which to split this array\n593             into separate variables. If not provided, this array is converted\n594             into a Dataset of one variable.\n595         name : Hashable, optional\n596             Name to substitute for this array's name. Only valid if ``dim`` is\n597             not provided.\n598         promote_attrs : bool, default: False\n599             Set to True to shallow copy attrs of DataArray to returned Dataset.\n600 \n601         Returns\n602         -------\n603         dataset : Dataset\n604         \"\"\"\n605         if dim is not None and dim not in self.dims:\n606             raise TypeError(\n607                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n608             )\n609 \n610         if dim is not None:\n611             if name is not None:\n612                 raise TypeError(\"cannot supply both dim and name arguments\")\n613             result = self._to_dataset_split(dim)\n614         else:\n615             result = self._to_dataset_whole(name)\n616 \n617         if promote_attrs:\n618             result.attrs = dict(self.attrs)\n619 \n620         return result\n621 \n622     @property\n623     def name(self) -> Hashable | None:\n624         \"\"\"The name of this array.\"\"\"\n625         return self._name\n626 \n627     @name.setter\n628     def name(self, value: Hashable | None) -> None:\n629         self._name = value\n630 \n631     @property\n632     def variable(self) -> Variable:\n633         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n634         return self._variable\n635 \n636     @property\n637     def dtype(self) -> np.dtype:\n638         return self.variable.dtype\n639 \n640     @property\n641     def shape(self) -> tuple[int, ...]:\n642         return self.variable.shape\n643 \n644     @property\n645     def size(self) -> int:\n646         return self.variable.size\n647 \n648     @property\n649     def nbytes(self) -> int:\n650         \"\"\"\n651         Total bytes consumed by the elements of this DataArray's data.\n652 \n653         If the backend data array does not include ``nbytes``, estimates\n654         the bytes consumed based on the ``size`` and ``dtype``.\n655         \"\"\"\n656         return self.variable.nbytes\n657 \n658     @property\n659     def ndim(self) -> int:\n660         return self.variable.ndim\n661 \n662     def __len__(self) -> int:\n663         return len(self.variable)\n664 \n665     @property\n666     def data(self) -> Any:\n667         \"\"\"\n668         The DataArray's data as an array. The underlying array type\n669         (e.g. dask, sparse, pint) is preserved.\n670 \n671         See Also\n672         --------\n673         DataArray.to_numpy\n674         DataArray.as_numpy\n675         DataArray.values\n676         \"\"\"\n677         return self.variable.data\n678 \n679     @data.setter\n680     def data(self, value: Any) -> None:\n681         self.variable.data = value\n682 \n683     @property\n684     def values(self) -> np.ndarray:\n685         \"\"\"\n686         The array's data as a numpy.ndarray.\n687 \n688         If the array's data is not a numpy.ndarray this will attempt to convert\n689         it naively using np.array(), which will raise an error if the array\n690         type does not support coercion like this (e.g. cupy).\n691         \"\"\"\n692         return self.variable.values\n693 \n694     @values.setter\n695     def values(self, value: Any) -> None:\n696         self.variable.values = value\n697 \n698     def to_numpy(self) -> np.ndarray:\n699         \"\"\"\n700         Coerces wrapped data to numpy and returns a numpy.ndarray.\n701 \n702         See Also\n703         --------\n704         DataArray.as_numpy : Same but returns the surrounding DataArray instead.\n705         Dataset.as_numpy\n706         DataArray.values\n707         DataArray.data\n708         \"\"\"\n709         return self.variable.to_numpy()\n710 \n711     def as_numpy(self: T_DataArray) -> T_DataArray:\n712         \"\"\"\n713         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.\n714 \n715         See Also\n716         --------\n717         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.\n718         Dataset.as_numpy : Converts all variables in a Dataset.\n719         DataArray.values\n720         DataArray.data\n721         \"\"\"\n722         coords = {k: v.as_numpy() for k, v in self._coords.items()}\n723         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)\n724 \n725     @property\n726     def _in_memory(self) -> bool:\n727         return self.variable._in_memory\n728 \n729     def to_index(self) -> pd.Index:\n730         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n731         arrays.\n732         \"\"\"\n733         return self.variable.to_index()\n734 \n735     @property\n736     def dims(self) -> tuple[Hashable, ...]:\n737         \"\"\"Tuple of dimension names associated with this array.\n738 \n739         Note that the type of this property is inconsistent with\n740         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n741         consistently named properties.\n742 \n743         See Also\n744         --------\n745         DataArray.sizes\n746         Dataset.dims\n747         \"\"\"\n748         return self.variable.dims\n749 \n750     @dims.setter\n751     def dims(self, value: Any) -> NoReturn:\n752         raise AttributeError(\n753             \"you cannot assign dims on a DataArray. Use \"\n754             \".rename() or .swap_dims() instead.\"\n755         )\n756 \n757     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n758         if utils.is_dict_like(key):\n759             return key\n760         key = indexing.expanded_indexer(key, self.ndim)\n761         return dict(zip(self.dims, key))\n762 \n763     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:\n764         from .dataset import _get_virtual_variable\n765 \n766         try:\n767             var = self._coords[key]\n768         except KeyError:\n769             dim_sizes = dict(zip(self.dims, self.shape))\n770             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)\n771 \n772         return self._replace_maybe_drop_dims(var, name=key)\n773 \n774     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:\n775         if isinstance(key, str):\n776             return self._getitem_coord(key)\n777         else:\n778             # xarray-style array indexing\n779             return self.isel(indexers=self._item_key_to_dict(key))\n780 \n781     def __setitem__(self, key: Any, value: Any) -> None:\n782         if isinstance(key, str):\n783             self.coords[key] = value\n784         else:\n785             # Coordinates in key, value and self[key] should be consistent.\n786             # TODO Coordinate consistency in key is checked here, but it\n787             # causes unnecessary indexing. It should be optimized.\n788             obj = self[key]\n789             if isinstance(value, DataArray):\n790                 assert_coordinate_consistent(value, obj.coords.variables)\n791             # DataArray key -> Variable key\n792             key = {\n793                 k: v.variable if isinstance(v, DataArray) else v\n794                 for k, v in self._item_key_to_dict(key).items()\n795             }\n796             self.variable[key] = value\n797 \n798     def __delitem__(self, key: Any) -> None:\n799         del self.coords[key]\n800 \n801     @property\n802     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n803         \"\"\"Places to look-up items for attribute-style access\"\"\"\n804         yield from self._item_sources\n805         yield self.attrs\n806 \n807     @property\n808     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n809         \"\"\"Places to look-up items for key-completion\"\"\"\n810         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n811 \n812         # virtual coordinates\n813         # uses empty dict -- everything here can already be found in self.coords.\n814         yield HybridMappingProxy(keys=self.dims, mapping={})\n815 \n816     def __contains__(self, key: Any) -> bool:\n817         return key in self.data\n818 \n819     @property\n820     def loc(self) -> _LocIndexer:\n821         \"\"\"Attribute for location based indexing like pandas.\"\"\"\n822         return _LocIndexer(self)\n823 \n824     @property\n825     # Key type needs to be `Any` because of mypy#4167\n826     def attrs(self) -> dict[Any, Any]:\n827         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n828         return self.variable.attrs\n829 \n830     @attrs.setter\n831     def attrs(self, value: Mapping[Any, Any]) -> None:\n832         # Disable type checking to work around mypy bug - see mypy#4167\n833         self.variable.attrs = value  # type: ignore[assignment]\n834 \n835     @property\n836     def encoding(self) -> dict[Hashable, Any]:\n837         \"\"\"Dictionary of format-specific settings for how this array should be\n838         serialized.\"\"\"\n839         return self.variable.encoding\n840 \n841     @encoding.setter\n842     def encoding(self, value: Mapping[Any, Any]) -> None:\n843         self.variable.encoding = value\n844 \n845     @property\n846     def indexes(self) -> Indexes:\n847         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n848 \n849         Raises an error if this Dataset has indexes that cannot be coerced\n850         to pandas.Index objects.\n851 \n852         See Also\n853         --------\n854         DataArray.xindexes\n855 \n856         \"\"\"\n857         return self.xindexes.to_pandas_indexes()\n858 \n859     @property\n860     def xindexes(self) -> Indexes:\n861         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n862         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})\n863 \n864     @property\n865     def coords(self) -> DataArrayCoordinates:\n866         \"\"\"Dictionary-like container of coordinate arrays.\"\"\"\n867         return DataArrayCoordinates(self)\n868 \n869     @overload\n870     def reset_coords(\n871         self: T_DataArray,\n872         names: Hashable | Iterable[Hashable] | None = None,\n873         drop: Literal[False] = False,\n874     ) -> Dataset:\n875         ...\n876 \n877     @overload\n878     def reset_coords(\n879         self: T_DataArray,\n880         names: Hashable | Iterable[Hashable] | None = None,\n881         *,\n882         drop: Literal[True],\n883     ) -> T_DataArray:\n884         ...\n885 \n886     def reset_coords(\n887         self: T_DataArray,\n888         names: Hashable | Iterable[Hashable] | None = None,\n889         drop: bool = False,\n890     ) -> T_DataArray | Dataset:\n891         \"\"\"Given names of coordinates, reset them to become variables.\n892 \n893         Parameters\n894         ----------\n895         names : Hashable or iterable of Hashable, optional\n896             Name(s) of non-index coordinates in this dataset to reset into\n897             variables. By default, all non-index coordinates are reset.\n898         drop : bool, default: False\n899             If True, remove coordinates instead of converting them into\n900             variables.\n901 \n902         Returns\n903         -------\n904         Dataset, or DataArray if ``drop == True``\n905         \"\"\"\n906         if names is None:\n907             names = set(self.coords) - set(self._indexes)\n908         dataset = self.coords.to_dataset().reset_coords(names, drop)\n909         if drop:\n910             return self._replace(coords=dataset._variables)\n911         if self.name is None:\n912             raise ValueError(\n913                 \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n914             )\n915         dataset[self.name] = self.variable\n916         return dataset\n917 \n918     def __dask_tokenize__(self):\n919         from dask.base import normalize_token\n920 \n921         return normalize_token((type(self), self._variable, self._coords, self._name))\n922 \n923     def __dask_graph__(self):\n924         return self._to_temp_dataset().__dask_graph__()\n925 \n926     def __dask_keys__(self):\n927         return self._to_temp_dataset().__dask_keys__()\n928 \n929     def __dask_layers__(self):\n930         return self._to_temp_dataset().__dask_layers__()\n931 \n932     @property\n933     def __dask_optimize__(self):\n934         return self._to_temp_dataset().__dask_optimize__\n935 \n936     @property\n937     def __dask_scheduler__(self):\n938         return self._to_temp_dataset().__dask_scheduler__\n939 \n940     def __dask_postcompute__(self):\n941         func, args = self._to_temp_dataset().__dask_postcompute__()\n942         return self._dask_finalize, (self.name, func) + args\n943 \n944     def __dask_postpersist__(self):\n945         func, args = self._to_temp_dataset().__dask_postpersist__()\n946         return self._dask_finalize, (self.name, func) + args\n947 \n948     @staticmethod\n949     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:\n950         ds = func(results, *args, **kwargs)\n951         variable = ds._variables.pop(_THIS_ARRAY)\n952         coords = ds._variables\n953         indexes = ds._indexes\n954         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n955 \n956     def load(self: T_DataArray, **kwargs) -> T_DataArray:\n957         \"\"\"Manually trigger loading of this array's data from disk or a\n958         remote source into memory and return this array.\n959 \n960         Normally, it should not be necessary to call this method in user code,\n961         because all xarray functions should either work on deferred data or\n962         load data automatically. However, this method can be necessary when\n963         working with many file objects on disk.\n964 \n965         Parameters\n966         ----------\n967         **kwargs : dict\n968             Additional keyword arguments passed on to ``dask.compute``.\n969 \n970         See Also\n971         --------\n972         dask.compute\n973         \"\"\"\n974         ds = self._to_temp_dataset().load(**kwargs)\n975         new = self._from_temp_dataset(ds)\n976         self._variable = new._variable\n977         self._coords = new._coords\n978         return self\n979 \n980     def compute(self: T_DataArray, **kwargs) -> T_DataArray:\n981         \"\"\"Manually trigger loading of this array's data from disk or a\n982         remote source into memory and return a new array. The original is\n983         left unaltered.\n984 \n985         Normally, it should not be necessary to call this method in user code,\n986         because all xarray functions should either work on deferred data or\n987         load data automatically. However, this method can be necessary when\n988         working with many file objects on disk.\n989 \n990         Parameters\n991         ----------\n992         **kwargs : dict\n993             Additional keyword arguments passed on to ``dask.compute``.\n994 \n995         See Also\n996         --------\n997         dask.compute\n998         \"\"\"\n999         new = self.copy(deep=False)\n1000         return new.load(**kwargs)\n1001 \n1002     def persist(self: T_DataArray, **kwargs) -> T_DataArray:\n1003         \"\"\"Trigger computation in constituent dask arrays\n1004 \n1005         This keeps them as dask arrays but encourages them to keep data in\n1006         memory.  This is particularly useful when on a distributed machine.\n1007         When on a single machine consider using ``.compute()`` instead.\n1008 \n1009         Parameters\n1010         ----------\n1011         **kwargs : dict\n1012             Additional keyword arguments passed on to ``dask.persist``.\n1013 \n1014         See Also\n1015         --------\n1016         dask.persist\n1017         \"\"\"\n1018         ds = self._to_temp_dataset().persist(**kwargs)\n1019         return self._from_temp_dataset(ds)\n1020 \n1021     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:\n1022         \"\"\"Returns a copy of this array.\n1023 \n1024         If `deep=True`, a deep copy is made of the data array.\n1025         Otherwise, a shallow copy is made, and the returned data array's\n1026         values are a new view of this data array's values.\n1027 \n1028         Use `data` to create a new object with the same structure as\n1029         original but entirely new data.\n1030 \n1031         Parameters\n1032         ----------\n1033         deep : bool, optional\n1034             Whether the data array and its coordinates are loaded into memory\n1035             and copied onto the new object. Default is True.\n1036         data : array_like, optional\n1037             Data to use in the new object. Must have same shape as original.\n1038             When `data` is used, `deep` is ignored for all data variables,\n1039             and only used for coords.\n1040 \n1041         Returns\n1042         -------\n1043         copy : DataArray\n1044             New object with dimensions, attributes, coordinates, name,\n1045             encoding, and optionally data copied from original.\n1046 \n1047         Examples\n1048         --------\n1049         Shallow versus deep copy\n1050 \n1051         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n1052         >>> array.copy()\n1053         <xarray.DataArray (x: 3)>\n1054         array([1, 2, 3])\n1055         Coordinates:\n1056           * x        (x) <U1 'a' 'b' 'c'\n1057         >>> array_0 = array.copy(deep=False)\n1058         >>> array_0[0] = 7\n1059         >>> array_0\n1060         <xarray.DataArray (x: 3)>\n1061         array([7, 2, 3])\n1062         Coordinates:\n1063           * x        (x) <U1 'a' 'b' 'c'\n1064         >>> array\n1065         <xarray.DataArray (x: 3)>\n1066         array([7, 2, 3])\n1067         Coordinates:\n1068           * x        (x) <U1 'a' 'b' 'c'\n1069 \n1070         Changing the data using the ``data`` argument maintains the\n1071         structure of the original object, but with the new data. Original\n1072         object is unaffected.\n1073 \n1074         >>> array.copy(data=[0.1, 0.2, 0.3])\n1075         <xarray.DataArray (x: 3)>\n1076         array([0.1, 0.2, 0.3])\n1077         Coordinates:\n1078           * x        (x) <U1 'a' 'b' 'c'\n1079         >>> array\n1080         <xarray.DataArray (x: 3)>\n1081         array([7, 2, 3])\n1082         Coordinates:\n1083           * x        (x) <U1 'a' 'b' 'c'\n1084 \n1085         See Also\n1086         --------\n1087         pandas.DataFrame.copy\n1088         \"\"\"\n1089         variable = self.variable.copy(deep=deep, data=data)\n1090         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1091 \n1092         coords = {}\n1093         for k, v in self._coords.items():\n1094             if k in index_vars:\n1095                 coords[k] = index_vars[k]\n1096             else:\n1097                 coords[k] = v.copy(deep=deep)\n1098 \n1099         return self._replace(variable, coords, indexes=indexes)\n1100 \n1101     def __copy__(self: T_DataArray) -> T_DataArray:\n1102         return self.copy(deep=False)\n1103 \n1104     def __deepcopy__(self: T_DataArray, memo=None) -> T_DataArray:\n1105         # memo does nothing but is required for compatibility with\n1106         # copy.deepcopy\n1107         return self.copy(deep=True)\n1108 \n1109     # mutable objects should not be Hashable\n1110     # https://github.com/python/mypy/issues/4266\n1111     __hash__ = None  # type: ignore[assignment]\n1112 \n1113     @property\n1114     def chunks(self) -> tuple[tuple[int, ...], ...] | None:\n1115         \"\"\"\n1116         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if\n1117         the underlying data is not a dask array.\n1118 \n1119         See Also\n1120         --------\n1121         DataArray.chunk\n1122         DataArray.chunksizes\n1123         xarray.unify_chunks\n1124         \"\"\"\n1125         return self.variable.chunks\n1126 \n1127     @property\n1128     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:\n1129         \"\"\"\n1130         Mapping from dimension names to block lengths for this dataarray's data, or None if\n1131         the underlying data is not a dask array.\n1132         Cannot be modified directly, but can be modified by calling .chunk().\n1133 \n1134         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes\n1135         instead of a tuple of chunk shapes.\n1136 \n1137         See Also\n1138         --------\n1139         DataArray.chunk\n1140         DataArray.chunks\n1141         xarray.unify_chunks\n1142         \"\"\"\n1143         all_variables = [self.variable] + [c.variable for c in self.coords.values()]\n1144         return get_chunksizes(all_variables)\n1145 \n1146     def chunk(\n1147         self: T_DataArray,\n1148         chunks: (\n1149             int\n1150             | Literal[\"auto\"]\n1151             | tuple[int, ...]\n1152             | tuple[tuple[int, ...], ...]\n1153             | Mapping[Any, None | int | tuple[int, ...]]\n1154         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n1155         name_prefix: str = \"xarray-\",\n1156         token: str | None = None,\n1157         lock: bool = False,\n1158         inline_array: bool = False,\n1159         **chunks_kwargs: Any,\n1160     ) -> T_DataArray:\n1161         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n1162 \n1163         If this variable is a non-dask array, it will be converted to dask\n1164         array. If it's a dask array, it will be rechunked to the given chunk\n1165         sizes.\n1166 \n1167         If neither chunks is not provided for one or more dimensions, chunk\n1168         sizes along that dimension will not be updated; non-dask arrays will be\n1169         converted into dask arrays with a single block.\n1170 \n1171         Parameters\n1172         ----------\n1173         chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n1174             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n1175             ``{\"x\": 5, \"y\": 5}``.\n1176         name_prefix : str, optional\n1177             Prefix for the name of the new dask array.\n1178         token : str, optional\n1179             Token uniquely identifying this array.\n1180         lock : optional\n1181             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1182             already as dask array.\n1183         inline_array: optional\n1184             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1185             already as dask array.\n1186         **chunks_kwargs : {dim: chunks, ...}, optional\n1187             The keyword arguments form of ``chunks``.\n1188             One of chunks or chunks_kwargs must be provided.\n1189 \n1190         Returns\n1191         -------\n1192         chunked : xarray.DataArray\n1193 \n1194         See Also\n1195         --------\n1196         DataArray.chunks\n1197         DataArray.chunksizes\n1198         xarray.unify_chunks\n1199         dask.array.from_array\n1200         \"\"\"\n1201         if chunks is None:\n1202             warnings.warn(\n1203                 \"None value for 'chunks' is deprecated. \"\n1204                 \"It will raise an error in the future. Use instead '{}'\",\n1205                 category=FutureWarning,\n1206             )\n1207             chunks = {}\n1208 \n1209         if isinstance(chunks, (float, str, int)):\n1210             # ignoring type; unclear why it won't accept a Literal into the value.\n1211             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore\n1212         elif isinstance(chunks, (tuple, list)):\n1213             chunks = dict(zip(self.dims, chunks))\n1214         else:\n1215             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n1216 \n1217         ds = self._to_temp_dataset().chunk(\n1218             chunks,\n1219             name_prefix=name_prefix,\n1220             token=token,\n1221             lock=lock,\n1222             inline_array=inline_array,\n1223         )\n1224         return self._from_temp_dataset(ds)\n1225 \n1226     def isel(\n1227         self: T_DataArray,\n1228         indexers: Mapping[Any, Any] | None = None,\n1229         drop: bool = False,\n1230         missing_dims: ErrorOptionsWithWarn = \"raise\",\n1231         **indexers_kwargs: Any,\n1232     ) -> T_DataArray:\n1233         \"\"\"Return a new DataArray whose data is given by selecting indexes\n1234         along the specified dimension(s).\n1235 \n1236         Parameters\n1237         ----------\n1238         indexers : dict, optional\n1239             A dict with keys matching dimensions and values given\n1240             by integers, slice objects or arrays.\n1241             indexer can be a integer, slice, array-like or DataArray.\n1242             If DataArrays are passed as indexers, xarray-style indexing will be\n1243             carried out. See :ref:`indexing` for the details.\n1244             One of indexers or indexers_kwargs must be provided.\n1245         drop : bool, default: False\n1246             If ``drop=True``, drop coordinates variables indexed by integers\n1247             instead of making them scalar.\n1248         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n1249             What to do if dimensions that should be selected from are not present in the\n1250             DataArray:\n1251             - \"raise\": raise an exception\n1252             - \"warn\": raise a warning, and ignore the missing dimensions\n1253             - \"ignore\": ignore the missing dimensions\n1254         **indexers_kwargs : {dim: indexer, ...}, optional\n1255             The keyword arguments form of ``indexers``.\n1256 \n1257         Returns\n1258         -------\n1259         indexed : xarray.DataArray\n1260 \n1261         See Also\n1262         --------\n1263         Dataset.isel\n1264         DataArray.sel\n1265 \n1266         Examples\n1267         --------\n1268         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"x\", \"y\"))\n1269         >>> da\n1270         <xarray.DataArray (x: 5, y: 5)>\n1271         array([[ 0,  1,  2,  3,  4],\n1272                [ 5,  6,  7,  8,  9],\n1273                [10, 11, 12, 13, 14],\n1274                [15, 16, 17, 18, 19],\n1275                [20, 21, 22, 23, 24]])\n1276         Dimensions without coordinates: x, y\n1277 \n1278         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1279         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1280         >>> da = da.isel(x=tgt_x, y=tgt_y)\n1281         >>> da\n1282         <xarray.DataArray (points: 5)>\n1283         array([ 0,  6, 12, 18, 24])\n1284         Dimensions without coordinates: points\n1285         \"\"\"\n1286 \n1287         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1288 \n1289         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1290             ds = self._to_temp_dataset()._isel_fancy(\n1291                 indexers, drop=drop, missing_dims=missing_dims\n1292             )\n1293             return self._from_temp_dataset(ds)\n1294 \n1295         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1296         # lists, or zero or one-dimensional np.ndarray's\n1297 \n1298         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1299         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n1300 \n1301         coords = {}\n1302         for coord_name, coord_value in self._coords.items():\n1303             if coord_name in index_variables:\n1304                 coord_value = index_variables[coord_name]\n1305             else:\n1306                 coord_indexers = {\n1307                     k: v for k, v in indexers.items() if k in coord_value.dims\n1308                 }\n1309                 if coord_indexers:\n1310                     coord_value = coord_value.isel(coord_indexers)\n1311                     if drop and coord_value.ndim == 0:\n1312                         continue\n1313             coords[coord_name] = coord_value\n1314 \n1315         return self._replace(variable=variable, coords=coords, indexes=indexes)\n1316 \n1317     def sel(\n1318         self: T_DataArray,\n1319         indexers: Mapping[Any, Any] = None,\n1320         method: str = None,\n1321         tolerance=None,\n1322         drop: bool = False,\n1323         **indexers_kwargs: Any,\n1324     ) -> T_DataArray:\n1325         \"\"\"Return a new DataArray whose data is given by selecting index\n1326         labels along the specified dimension(s).\n1327 \n1328         In contrast to `DataArray.isel`, indexers for this method should use\n1329         labels instead of integers.\n1330 \n1331         Under the hood, this method is powered by using pandas's powerful Index\n1332         objects. This makes label based indexing essentially just as fast as\n1333         using integer indexing.\n1334 \n1335         It also means this method uses pandas's (well documented) logic for\n1336         indexing. This means you can use string shortcuts for datetime indexes\n1337         (e.g., '2000-01' to select all values in January 2000). It also means\n1338         that slices are treated as inclusive of both the start and stop values,\n1339         unlike normal Python indexing.\n1340 \n1341         .. warning::\n1342 \n1343           Do not try to assign values when using any of the indexing methods\n1344           ``isel`` or ``sel``::\n1345 \n1346             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1347             # DO NOT do this\n1348             da.isel(x=[0, 1, 2])[1] = -1\n1349 \n1350           Assigning values with the chained indexing using ``.sel`` or\n1351           ``.isel`` fails silently.\n1352 \n1353         Parameters\n1354         ----------\n1355         indexers : dict, optional\n1356             A dict with keys matching dimensions and values given\n1357             by scalars, slices or arrays of tick labels. For dimensions with\n1358             multi-index, the indexer may also be a dict-like object with keys\n1359             matching index level names.\n1360             If DataArrays are passed as indexers, xarray-style indexing will be\n1361             carried out. See :ref:`indexing` for the details.\n1362             One of indexers or indexers_kwargs must be provided.\n1363         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1364             Method to use for inexact matches:\n1365 \n1366             - None (default): only exact matches\n1367             - pad / ffill: propagate last valid index value forward\n1368             - backfill / bfill: propagate next valid index value backward\n1369             - nearest: use nearest valid index value\n1370 \n1371         tolerance : optional\n1372             Maximum distance between original and new labels for inexact\n1373             matches. The values of the index at the matching locations must\n1374             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1375         drop : bool, optional\n1376             If ``drop=True``, drop coordinates variables in `indexers` instead\n1377             of making them scalar.\n1378         **indexers_kwargs : {dim: indexer, ...}, optional\n1379             The keyword arguments form of ``indexers``.\n1380             One of indexers or indexers_kwargs must be provided.\n1381 \n1382         Returns\n1383         -------\n1384         obj : DataArray\n1385             A new DataArray with the same contents as this DataArray, except the\n1386             data and each dimension is indexed by the appropriate indexers.\n1387             If indexer DataArrays have coordinates that do not conflict with\n1388             this object, then these coordinates will be attached.\n1389             In general, each array's data will be a view of the array's data\n1390             in this DataArray, unless vectorized indexing was triggered by using\n1391             an array indexer, in which case the data will be a copy.\n1392 \n1393         See Also\n1394         --------\n1395         Dataset.sel\n1396         DataArray.isel\n1397 \n1398         Examples\n1399         --------\n1400         >>> da = xr.DataArray(\n1401         ...     np.arange(25).reshape(5, 5),\n1402         ...     coords={\"x\": np.arange(5), \"y\": np.arange(5)},\n1403         ...     dims=(\"x\", \"y\"),\n1404         ... )\n1405         >>> da\n1406         <xarray.DataArray (x: 5, y: 5)>\n1407         array([[ 0,  1,  2,  3,  4],\n1408                [ 5,  6,  7,  8,  9],\n1409                [10, 11, 12, 13, 14],\n1410                [15, 16, 17, 18, 19],\n1411                [20, 21, 22, 23, 24]])\n1412         Coordinates:\n1413           * x        (x) int64 0 1 2 3 4\n1414           * y        (y) int64 0 1 2 3 4\n1415 \n1416         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1417         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1418         >>> da = da.sel(x=tgt_x, y=tgt_y, method=\"nearest\")\n1419         >>> da\n1420         <xarray.DataArray (points: 5)>\n1421         array([ 0,  6, 12, 18, 24])\n1422         Coordinates:\n1423             x        (points) int64 0 1 2 3 4\n1424             y        (points) int64 0 1 2 3 4\n1425         Dimensions without coordinates: points\n1426         \"\"\"\n1427         ds = self._to_temp_dataset().sel(\n1428             indexers=indexers,\n1429             drop=drop,\n1430             method=method,\n1431             tolerance=tolerance,\n1432             **indexers_kwargs,\n1433         )\n1434         return self._from_temp_dataset(ds)\n1435 \n1436     def head(\n1437         self: T_DataArray,\n1438         indexers: Mapping[Any, int] | int | None = None,\n1439         **indexers_kwargs: Any,\n1440     ) -> T_DataArray:\n1441         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1442         values along the specified dimension(s). Default `n` = 5\n1443 \n1444         See Also\n1445         --------\n1446         Dataset.head\n1447         DataArray.tail\n1448         DataArray.thin\n1449         \"\"\"\n1450         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1451         return self._from_temp_dataset(ds)\n1452 \n1453     def tail(\n1454         self: T_DataArray,\n1455         indexers: Mapping[Any, int] | int | None = None,\n1456         **indexers_kwargs: Any,\n1457     ) -> T_DataArray:\n1458         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1459         values along the specified dimension(s). Default `n` = 5\n1460 \n1461         See Also\n1462         --------\n1463         Dataset.tail\n1464         DataArray.head\n1465         DataArray.thin\n1466         \"\"\"\n1467         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1468         return self._from_temp_dataset(ds)\n1469 \n1470     def thin(\n1471         self: T_DataArray,\n1472         indexers: Mapping[Any, int] | int | None = None,\n1473         **indexers_kwargs: Any,\n1474     ) -> T_DataArray:\n1475         \"\"\"Return a new DataArray whose data is given by each `n` value\n1476         along the specified dimension(s).\n1477 \n1478         Examples\n1479         --------\n1480         >>> x_arr = np.arange(0, 26)\n1481         >>> x_arr\n1482         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n1483                17, 18, 19, 20, 21, 22, 23, 24, 25])\n1484         >>> x = xr.DataArray(\n1485         ...     np.reshape(x_arr, (2, 13)),\n1486         ...     dims=(\"x\", \"y\"),\n1487         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n1488         ... )\n1489         >>> x\n1490         <xarray.DataArray (x: 2, y: 13)>\n1491         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n1492                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])\n1493         Coordinates:\n1494           * x        (x) int64 0 1\n1495           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n1496 \n1497         >>>\n1498         >>> x.thin(3)\n1499         <xarray.DataArray (x: 1, y: 5)>\n1500         array([[ 0,  3,  6,  9, 12]])\n1501         Coordinates:\n1502           * x        (x) int64 0\n1503           * y        (y) int64 0 3 6 9 12\n1504         >>> x.thin({\"x\": 2, \"y\": 5})\n1505         <xarray.DataArray (x: 1, y: 3)>\n1506         array([[ 0,  5, 10]])\n1507         Coordinates:\n1508           * x        (x) int64 0\n1509           * y        (y) int64 0 5 10\n1510 \n1511         See Also\n1512         --------\n1513         Dataset.thin\n1514         DataArray.head\n1515         DataArray.tail\n1516         \"\"\"\n1517         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1518         return self._from_temp_dataset(ds)\n1519 \n1520     def broadcast_like(\n1521         self: T_DataArray,\n1522         other: DataArray | Dataset,\n1523         exclude: Iterable[Hashable] | None = None,\n1524     ) -> T_DataArray:\n1525         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1526 \n1527         This is equivalent to xr.broadcast(other, self)[1]\n1528 \n1529         xarray objects are broadcast against each other in arithmetic\n1530         operations, so this method is not be necessary for most uses.\n1531 \n1532         If no change is needed, the input data is returned to the output\n1533         without being copied.\n1534 \n1535         If new coords are added by the broadcast, their values are\n1536         NaN filled.\n1537 \n1538         Parameters\n1539         ----------\n1540         other : Dataset or DataArray\n1541             Object against which to broadcast this array.\n1542         exclude : iterable of Hashable, optional\n1543             Dimensions that must not be broadcasted\n1544 \n1545         Returns\n1546         -------\n1547         new_da : DataArray\n1548             The caller broadcasted against ``other``.\n1549 \n1550         Examples\n1551         --------\n1552         >>> arr1 = xr.DataArray(\n1553         ...     np.random.randn(2, 3),\n1554         ...     dims=(\"x\", \"y\"),\n1555         ...     coords={\"x\": [\"a\", \"b\"], \"y\": [\"a\", \"b\", \"c\"]},\n1556         ... )\n1557         >>> arr2 = xr.DataArray(\n1558         ...     np.random.randn(3, 2),\n1559         ...     dims=(\"x\", \"y\"),\n1560         ...     coords={\"x\": [\"a\", \"b\", \"c\"], \"y\": [\"a\", \"b\"]},\n1561         ... )\n1562         >>> arr1\n1563         <xarray.DataArray (x: 2, y: 3)>\n1564         array([[ 1.76405235,  0.40015721,  0.97873798],\n1565                [ 2.2408932 ,  1.86755799, -0.97727788]])\n1566         Coordinates:\n1567           * x        (x) <U1 'a' 'b'\n1568           * y        (y) <U1 'a' 'b' 'c'\n1569         >>> arr2\n1570         <xarray.DataArray (x: 3, y: 2)>\n1571         array([[ 0.95008842, -0.15135721],\n1572                [-0.10321885,  0.4105985 ],\n1573                [ 0.14404357,  1.45427351]])\n1574         Coordinates:\n1575           * x        (x) <U1 'a' 'b' 'c'\n1576           * y        (y) <U1 'a' 'b'\n1577         >>> arr1.broadcast_like(arr2)\n1578         <xarray.DataArray (x: 3, y: 3)>\n1579         array([[ 1.76405235,  0.40015721,  0.97873798],\n1580                [ 2.2408932 ,  1.86755799, -0.97727788],\n1581                [        nan,         nan,         nan]])\n1582         Coordinates:\n1583           * x        (x) <U1 'a' 'b' 'c'\n1584           * y        (y) <U1 'a' 'b' 'c'\n1585         \"\"\"\n1586         if exclude is None:\n1587             exclude = set()\n1588         else:\n1589             exclude = set(exclude)\n1590         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1591 \n1592         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1593 \n1594         return _broadcast_helper(\n1595             cast(\"T_DataArray\", args[1]), exclude, dims_map, common_coords\n1596         )\n1597 \n1598     def _reindex_callback(\n1599         self: T_DataArray,\n1600         aligner: alignment.Aligner,\n1601         dim_pos_indexers: dict[Hashable, Any],\n1602         variables: dict[Hashable, Variable],\n1603         indexes: dict[Hashable, Index],\n1604         fill_value: Any,\n1605         exclude_dims: frozenset[Hashable],\n1606         exclude_vars: frozenset[Hashable],\n1607     ) -> T_DataArray:\n1608         \"\"\"Callback called from ``Aligner`` to create a new reindexed DataArray.\"\"\"\n1609 \n1610         if isinstance(fill_value, dict):\n1611             fill_value = fill_value.copy()\n1612             sentinel = object()\n1613             value = fill_value.pop(self.name, sentinel)\n1614             if value is not sentinel:\n1615                 fill_value[_THIS_ARRAY] = value\n1616 \n1617         ds = self._to_temp_dataset()\n1618         reindexed = ds._reindex_callback(\n1619             aligner,\n1620             dim_pos_indexers,\n1621             variables,\n1622             indexes,\n1623             fill_value,\n1624             exclude_dims,\n1625             exclude_vars,\n1626         )\n1627         return self._from_temp_dataset(reindexed)\n1628 \n1629     def reindex_like(\n1630         self: T_DataArray,\n1631         other: DataArray | Dataset,\n1632         method: ReindexMethodOptions = None,\n1633         tolerance: int | float | Iterable[int | float] | None = None,\n1634         copy: bool = True,\n1635         fill_value=dtypes.NA,\n1636     ) -> T_DataArray:\n1637         \"\"\"Conform this object onto the indexes of another object, filling in\n1638         missing values with ``fill_value``. The default fill value is NaN.\n1639 \n1640         Parameters\n1641         ----------\n1642         other : Dataset or DataArray\n1643             Object with an 'indexes' attribute giving a mapping from dimension\n1644             names to pandas.Index objects, which provides coordinates upon\n1645             which to index the variables in this dataset. The indexes on this\n1646             other object need not be the same as the indexes on this\n1647             dataset. Any mis-matched index values will be filled in with\n1648             NaN, and any mis-matched dimension names will simply be ignored.\n1649         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1650             Method to use for filling index values from other not found on this\n1651             data array:\n1652 \n1653             - None (default): don't fill gaps\n1654             - pad / ffill: propagate last valid index value forward\n1655             - backfill / bfill: propagate next valid index value backward\n1656             - nearest: use nearest valid index value\n1657 \n1658         tolerance : optional\n1659             Maximum distance between original and new labels for inexact\n1660             matches. The values of the index at the matching locations must\n1661             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1662             Tolerance may be a scalar value, which applies the same tolerance\n1663             to all values, or list-like, which applies variable tolerance per\n1664             element. List-like must be the same size as the index and its dtype\n1665             must exactly match the index\u2019s type.\n1666         copy : bool, default: True\n1667             If ``copy=True``, data in the return value is always copied. If\n1668             ``copy=False`` and reindexing is unnecessary, or can be performed\n1669             with only slice operations, then the output may share memory with\n1670             the input. In either case, a new xarray object is always returned.\n1671         fill_value : scalar or dict-like, optional\n1672             Value to use for newly missing values. If a dict-like, maps\n1673             variable names (including coordinates) to fill values. Use this\n1674             data array's name to refer to the data array's values.\n1675 \n1676         Returns\n1677         -------\n1678         reindexed : DataArray\n1679             Another dataset array, with this array's data but coordinates from\n1680             the other object.\n1681 \n1682         See Also\n1683         --------\n1684         DataArray.reindex\n1685         align\n1686         \"\"\"\n1687         return alignment.reindex_like(\n1688             self,\n1689             other=other,\n1690             method=method,\n1691             tolerance=tolerance,\n1692             copy=copy,\n1693             fill_value=fill_value,\n1694         )\n1695 \n1696     def reindex(\n1697         self: T_DataArray,\n1698         indexers: Mapping[Any, Any] = None,\n1699         method: ReindexMethodOptions = None,\n1700         tolerance: float | Iterable[float] | None = None,\n1701         copy: bool = True,\n1702         fill_value=dtypes.NA,\n1703         **indexers_kwargs: Any,\n1704     ) -> T_DataArray:\n1705         \"\"\"Conform this object onto the indexes of another object, filling in\n1706         missing values with ``fill_value``. The default fill value is NaN.\n1707 \n1708         Parameters\n1709         ----------\n1710         indexers : dict, optional\n1711             Dictionary with keys given by dimension names and values given by\n1712             arrays of coordinates tick labels. Any mis-matched coordinate\n1713             values will be filled in with NaN, and any mis-matched dimension\n1714             names will simply be ignored.\n1715             One of indexers or indexers_kwargs must be provided.\n1716         copy : bool, optional\n1717             If ``copy=True``, data in the return value is always copied. If\n1718             ``copy=False`` and reindexing is unnecessary, or can be performed\n1719             with only slice operations, then the output may share memory with\n1720             the input. In either case, a new xarray object is always returned.\n1721         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1722             Method to use for filling index values in ``indexers`` not found on\n1723             this data array:\n1724 \n1725             - None (default): don't fill gaps\n1726             - pad / ffill: propagate last valid index value forward\n1727             - backfill / bfill: propagate next valid index value backward\n1728             - nearest: use nearest valid index value\n1729 \n1730         tolerance : float | Iterable[float] | None, default: None\n1731             Maximum distance between original and new labels for inexact\n1732             matches. The values of the index at the matching locations must\n1733             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1734             Tolerance may be a scalar value, which applies the same tolerance\n1735             to all values, or list-like, which applies variable tolerance per\n1736             element. List-like must be the same size as the index and its dtype\n1737             must exactly match the index\u2019s type.\n1738         fill_value : scalar or dict-like, optional\n1739             Value to use for newly missing values. If a dict-like, maps\n1740             variable names (including coordinates) to fill values. Use this\n1741             data array's name to refer to the data array's values.\n1742         **indexers_kwargs : {dim: indexer, ...}, optional\n1743             The keyword arguments form of ``indexers``.\n1744             One of indexers or indexers_kwargs must be provided.\n1745 \n1746         Returns\n1747         -------\n1748         reindexed : DataArray\n1749             Another dataset array, with this array's data but replaced\n1750             coordinates.\n1751 \n1752         Examples\n1753         --------\n1754         Reverse latitude:\n1755 \n1756         >>> da = xr.DataArray(\n1757         ...     np.arange(4),\n1758         ...     coords=[np.array([90, 89, 88, 87])],\n1759         ...     dims=\"lat\",\n1760         ... )\n1761         >>> da\n1762         <xarray.DataArray (lat: 4)>\n1763         array([0, 1, 2, 3])\n1764         Coordinates:\n1765           * lat      (lat) int64 90 89 88 87\n1766         >>> da.reindex(lat=da.lat[::-1])\n1767         <xarray.DataArray (lat: 4)>\n1768         array([3, 2, 1, 0])\n1769         Coordinates:\n1770           * lat      (lat) int64 87 88 89 90\n1771 \n1772         See Also\n1773         --------\n1774         DataArray.reindex_like\n1775         align\n1776         \"\"\"\n1777         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1778         return alignment.reindex(\n1779             self,\n1780             indexers=indexers,\n1781             method=method,\n1782             tolerance=tolerance,\n1783             copy=copy,\n1784             fill_value=fill_value,\n1785         )\n1786 \n1787     def interp(\n1788         self: T_DataArray,\n1789         coords: Mapping[Any, Any] | None = None,\n1790         method: InterpOptions = \"linear\",\n1791         assume_sorted: bool = False,\n1792         kwargs: Mapping[str, Any] | None = None,\n1793         **coords_kwargs: Any,\n1794     ) -> T_DataArray:\n1795         \"\"\"Interpolate a DataArray onto new coordinates\n1796 \n1797         Performs univariate or multivariate interpolation of a DataArray onto\n1798         new coordinates using scipy's interpolation routines. If interpolating\n1799         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n1800         called. When interpolating along multiple existing dimensions, an\n1801         attempt is made to decompose the interpolation into multiple\n1802         1-dimensional interpolations. If this is possible,\n1803         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n1804         :py:func:`scipy.interpolate.interpn` is called.\n1805 \n1806         Parameters\n1807         ----------\n1808         coords : dict, optional\n1809             Mapping from dimension names to the new coordinates.\n1810             New coordinate can be a scalar, array-like or DataArray.\n1811             If DataArrays are passed as new coordinates, their dimensions are\n1812             used for the broadcasting. Missing values are skipped.\n1813         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n1814             The method used to interpolate. The method should be supported by\n1815             the scipy interpolator:\n1816 \n1817             - ``interp1d``: {\"linear\", \"nearest\", \"zero\", \"slinear\",\n1818               \"quadratic\", \"cubic\", \"polynomial\"}\n1819             - ``interpn``: {\"linear\", \"nearest\"}\n1820 \n1821             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n1822             also be provided.\n1823         assume_sorted : bool, default: False\n1824             If False, values of x can be in any order and they are sorted\n1825             first. If True, x has to be an array of monotonically increasing\n1826             values.\n1827         kwargs : dict-like or None, default: None\n1828             Additional keyword arguments passed to scipy's interpolator. Valid\n1829             options and their behavior depend whether ``interp1d`` or\n1830             ``interpn`` is used.\n1831         **coords_kwargs : {dim: coordinate, ...}, optional\n1832             The keyword arguments form of ``coords``.\n1833             One of coords or coords_kwargs must be provided.\n1834 \n1835         Returns\n1836         -------\n1837         interpolated : DataArray\n1838             New dataarray on the new coordinates.\n1839 \n1840         Notes\n1841         -----\n1842         scipy is required.\n1843 \n1844         See Also\n1845         --------\n1846         scipy.interpolate.interp1d\n1847         scipy.interpolate.interpn\n1848 \n1849         Examples\n1850         --------\n1851         >>> da = xr.DataArray(\n1852         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n1853         ...     dims=(\"x\", \"y\"),\n1854         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n1855         ... )\n1856         >>> da\n1857         <xarray.DataArray (x: 3, y: 4)>\n1858         array([[ 1.,  4.,  2.,  9.],\n1859                [ 2.,  7.,  6., nan],\n1860                [ 6., nan,  5.,  8.]])\n1861         Coordinates:\n1862           * x        (x) int64 0 1 2\n1863           * y        (y) int64 10 12 14 16\n1864 \n1865         1D linear interpolation (the default):\n1866 \n1867         >>> da.interp(x=[0, 0.75, 1.25, 1.75])\n1868         <xarray.DataArray (x: 4, y: 4)>\n1869         array([[1.  , 4.  , 2.  ,  nan],\n1870                [1.75, 6.25, 5.  ,  nan],\n1871                [3.  ,  nan, 5.75,  nan],\n1872                [5.  ,  nan, 5.25,  nan]])\n1873         Coordinates:\n1874           * y        (y) int64 10 12 14 16\n1875           * x        (x) float64 0.0 0.75 1.25 1.75\n1876 \n1877         1D nearest interpolation:\n1878 \n1879         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n1880         <xarray.DataArray (x: 4, y: 4)>\n1881         array([[ 1.,  4.,  2.,  9.],\n1882                [ 2.,  7.,  6., nan],\n1883                [ 2.,  7.,  6., nan],\n1884                [ 6., nan,  5.,  8.]])\n1885         Coordinates:\n1886           * y        (y) int64 10 12 14 16\n1887           * x        (x) float64 0.0 0.75 1.25 1.75\n1888 \n1889         1D linear extrapolation:\n1890 \n1891         >>> da.interp(\n1892         ...     x=[1, 1.5, 2.5, 3.5],\n1893         ...     method=\"linear\",\n1894         ...     kwargs={\"fill_value\": \"extrapolate\"},\n1895         ... )\n1896         <xarray.DataArray (x: 4, y: 4)>\n1897         array([[ 2. ,  7. ,  6. ,  nan],\n1898                [ 4. ,  nan,  5.5,  nan],\n1899                [ 8. ,  nan,  4.5,  nan],\n1900                [12. ,  nan,  3.5,  nan]])\n1901         Coordinates:\n1902           * y        (y) int64 10 12 14 16\n1903           * x        (x) float64 1.0 1.5 2.5 3.5\n1904 \n1905         2D linear interpolation:\n1906 \n1907         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n1908         <xarray.DataArray (x: 4, y: 3)>\n1909         array([[2.5  , 3.   ,   nan],\n1910                [4.   , 5.625,   nan],\n1911                [  nan,   nan,   nan],\n1912                [  nan,   nan,   nan]])\n1913         Coordinates:\n1914           * x        (x) float64 0.0 0.75 1.25 1.75\n1915           * y        (y) int64 11 13 15\n1916         \"\"\"\n1917         if self.dtype.kind not in \"uifc\":\n1918             raise TypeError(\n1919                 \"interp only works for a numeric type array. \"\n1920                 \"Given {}.\".format(self.dtype)\n1921             )\n1922         ds = self._to_temp_dataset().interp(\n1923             coords,\n1924             method=method,\n1925             kwargs=kwargs,\n1926             assume_sorted=assume_sorted,\n1927             **coords_kwargs,\n1928         )\n1929         return self._from_temp_dataset(ds)\n1930 \n1931     def interp_like(\n1932         self: T_DataArray,\n1933         other: DataArray | Dataset,\n1934         method: InterpOptions = \"linear\",\n1935         assume_sorted: bool = False,\n1936         kwargs: Mapping[str, Any] | None = None,\n1937     ) -> T_DataArray:\n1938         \"\"\"Interpolate this object onto the coordinates of another object,\n1939         filling out of range values with NaN.\n1940 \n1941         If interpolating along a single existing dimension,\n1942         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n1943         along multiple existing dimensions, an attempt is made to decompose the\n1944         interpolation into multiple 1-dimensional interpolations. If this is\n1945         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n1946         :py:func:`scipy.interpolate.interpn` is called.\n1947 \n1948         Parameters\n1949         ----------\n1950         other : Dataset or DataArray\n1951             Object with an 'indexes' attribute giving a mapping from dimension\n1952             names to an 1d array-like, which provides coordinates upon\n1953             which to index the variables in this dataset. Missing values are skipped.\n1954         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n1955             The method used to interpolate. The method should be supported by\n1956             the scipy interpolator:\n1957 \n1958             - {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\",\n1959               \"polynomial\"} when ``interp1d`` is called.\n1960             - {\"linear\", \"nearest\"} when ``interpn`` is called.\n1961 \n1962             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n1963             also be provided.\n1964         assume_sorted : bool, default: False\n1965             If False, values of coordinates that are interpolated over can be\n1966             in any order and they are sorted first. If True, interpolated\n1967             coordinates are assumed to be an array of monotonically increasing\n1968             values.\n1969         kwargs : dict, optional\n1970             Additional keyword passed to scipy's interpolator.\n1971 \n1972         Returns\n1973         -------\n1974         interpolated : DataArray\n1975             Another dataarray by interpolating this dataarray's data along the\n1976             coordinates of the other object.\n1977 \n1978         Notes\n1979         -----\n1980         scipy is required.\n1981         If the dataarray has object-type coordinates, reindex is used for these\n1982         coordinates instead of the interpolation.\n1983 \n1984         See Also\n1985         --------\n1986         DataArray.interp\n1987         DataArray.reindex_like\n1988         \"\"\"\n1989         if self.dtype.kind not in \"uifc\":\n1990             raise TypeError(\n1991                 \"interp only works for a numeric type array. \"\n1992                 \"Given {}.\".format(self.dtype)\n1993             )\n1994         ds = self._to_temp_dataset().interp_like(\n1995             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1996         )\n1997         return self._from_temp_dataset(ds)\n1998 \n1999     # change type of self and return to T_DataArray once\n2000     # https://github.com/python/mypy/issues/12846 is resolved\n2001     def rename(\n2002         self,\n2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,\n2004         **names: Hashable,\n2005     ) -> DataArray:\n2006         \"\"\"Returns a new DataArray with renamed coordinates, dimensions or a new name.\n2007 \n2008         Parameters\n2009         ----------\n2010         new_name_or_name_dict : str or dict-like, optional\n2011             If the argument is dict-like, it used as a mapping from old\n2012             names to new names for coordinates or dimensions. Otherwise,\n2013             use the argument as the new name for this array.\n2014         **names : Hashable, optional\n2015             The keyword arguments form of a mapping from old names to\n2016             new names for coordinates or dimensions.\n2017             One of new_name_or_name_dict or names must be provided.\n2018 \n2019         Returns\n2020         -------\n2021         renamed : DataArray\n2022             Renamed array or array with renamed coordinates.\n2023 \n2024         See Also\n2025         --------\n2026         Dataset.rename\n2027         DataArray.swap_dims\n2028         \"\"\"\n2029         if new_name_or_name_dict is None and not names:\n2030             # change name to None?\n2031             return self._replace(name=None)\n2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n2033             # change dims/coords\n2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n2035             dataset = self._to_temp_dataset().rename(name_dict)\n2036             return self._from_temp_dataset(dataset)\n2037         if utils.hashable(new_name_or_name_dict) and names:\n2038             # change name + dims/coords\n2039             dataset = self._to_temp_dataset().rename(names)\n2040             dataarray = self._from_temp_dataset(dataset)\n2041             return dataarray._replace(name=new_name_or_name_dict)\n2042         # only change name\n2043         return self._replace(name=new_name_or_name_dict)\n2044 \n2045     def swap_dims(\n2046         self: T_DataArray,\n2047         dims_dict: Mapping[Any, Hashable] | None = None,\n2048         **dims_kwargs,\n2049     ) -> T_DataArray:\n2050         \"\"\"Returns a new DataArray with swapped dimensions.\n2051 \n2052         Parameters\n2053         ----------\n2054         dims_dict : dict-like\n2055             Dictionary whose keys are current dimension names and whose values\n2056             are new names.\n2057         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n2058             The keyword arguments form of ``dims_dict``.\n2059             One of dims_dict or dims_kwargs must be provided.\n2060 \n2061         Returns\n2062         -------\n2063         swapped : DataArray\n2064             DataArray with swapped dimensions.\n2065 \n2066         Examples\n2067         --------\n2068         >>> arr = xr.DataArray(\n2069         ...     data=[0, 1],\n2070         ...     dims=\"x\",\n2071         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n2072         ... )\n2073         >>> arr\n2074         <xarray.DataArray (x: 2)>\n2075         array([0, 1])\n2076         Coordinates:\n2077           * x        (x) <U1 'a' 'b'\n2078             y        (x) int64 0 1\n2079 \n2080         >>> arr.swap_dims({\"x\": \"y\"})\n2081         <xarray.DataArray (y: 2)>\n2082         array([0, 1])\n2083         Coordinates:\n2084             x        (y) <U1 'a' 'b'\n2085           * y        (y) int64 0 1\n2086 \n2087         >>> arr.swap_dims({\"x\": \"z\"})\n2088         <xarray.DataArray (z: 2)>\n2089         array([0, 1])\n2090         Coordinates:\n2091             x        (z) <U1 'a' 'b'\n2092             y        (z) int64 0 1\n2093         Dimensions without coordinates: z\n2094 \n2095         See Also\n2096         --------\n2097         DataArray.rename\n2098         Dataset.swap_dims\n2099         \"\"\"\n2100         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n2101         ds = self._to_temp_dataset().swap_dims(dims_dict)\n2102         return self._from_temp_dataset(ds)\n2103 \n2104     # change type of self and return to T_DataArray once\n2105     # https://github.com/python/mypy/issues/12846 is resolved\n2106     def expand_dims(\n2107         self,\n2108         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n2109         axis: None | int | Sequence[int] = None,\n2110         **dim_kwargs: Any,\n2111     ) -> DataArray:\n2112         \"\"\"Return a new object with an additional axis (or axes) inserted at\n2113         the corresponding position in the array shape. The new object is a\n2114         view into the underlying array, not a copy.\n2115 \n2116         If dim is already a scalar coordinate, it will be promoted to a 1D\n2117         coordinate consisting of a single value.\n2118 \n2119         Parameters\n2120         ----------\n2121         dim : Hashable, sequence of Hashable, dict, or None, optional\n2122             Dimensions to include on the new variable.\n2123             If provided as str or sequence of str, then dimensions are inserted\n2124             with length 1. If provided as a dict, then the keys are the new\n2125             dimensions and the values are either integers (giving the length of\n2126             the new dimensions) or sequence/ndarray (giving the coordinates of\n2127             the new dimensions).\n2128         axis : int, sequence of int, or None, default: None\n2129             Axis position(s) where new axis is to be inserted (position(s) on\n2130             the result array). If a sequence of integers is passed,\n2131             multiple axes are inserted. In this case, dim arguments should be\n2132             same length list. If axis=None is passed, all the axes will be\n2133             inserted to the start of the result array.\n2134         **dim_kwargs : int or sequence or ndarray\n2135             The keywords are arbitrary dimensions being inserted and the values\n2136             are either the lengths of the new dims (if int is given), or their\n2137             coordinates. Note, this is an alternative to passing a dict to the\n2138             dim kwarg and will only be used if dim is None.\n2139 \n2140         Returns\n2141         -------\n2142         expanded : DataArray\n2143             This object, but with additional dimension(s).\n2144 \n2145         See Also\n2146         --------\n2147         Dataset.expand_dims\n2148         \"\"\"\n2149         if isinstance(dim, int):\n2150             raise TypeError(\"dim should be Hashable or sequence/mapping of Hashables\")\n2151         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n2152             if len(dim) != len(set(dim)):\n2153                 raise ValueError(\"dims should not contain duplicate values.\")\n2154             dim = dict.fromkeys(dim, 1)\n2155         elif dim is not None and not isinstance(dim, Mapping):\n2156             dim = {cast(Hashable, dim): 1}\n2157 \n2158         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n2159         ds = self._to_temp_dataset().expand_dims(dim, axis)\n2160         return self._from_temp_dataset(ds)\n2161 \n2162     # change type of self and return to T_DataArray once\n2163     # https://github.com/python/mypy/issues/12846 is resolved\n2164     def set_index(\n2165         self,\n2166         indexes: Mapping[Any, Hashable | Sequence[Hashable]] = None,\n2167         append: bool = False,\n2168         **indexes_kwargs: Hashable | Sequence[Hashable],\n2169     ) -> DataArray:\n2170         \"\"\"Set DataArray (multi-)indexes using one or more existing\n2171         coordinates.\n2172 \n2173         Parameters\n2174         ----------\n2175         indexes : {dim: index, ...}\n2176             Mapping from names matching dimensions and values given\n2177             by (lists of) the names of existing coordinates or variables to set\n2178             as new (multi-)index.\n2179         append : bool, default: False\n2180             If True, append the supplied index(es) to the existing index(es).\n2181             Otherwise replace the existing index(es).\n2182         **indexes_kwargs : optional\n2183             The keyword arguments form of ``indexes``.\n2184             One of indexes or indexes_kwargs must be provided.\n2185 \n2186         Returns\n2187         -------\n2188         obj : DataArray\n2189             Another DataArray, with this data but replaced coordinates.\n2190 \n2191         Examples\n2192         --------\n2193         >>> arr = xr.DataArray(\n2194         ...     data=np.ones((2, 3)),\n2195         ...     dims=[\"x\", \"y\"],\n2196         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n2197         ... )\n2198         >>> arr\n2199         <xarray.DataArray (x: 2, y: 3)>\n2200         array([[1., 1., 1.],\n2201                [1., 1., 1.]])\n2202         Coordinates:\n2203           * x        (x) int64 0 1\n2204           * y        (y) int64 0 1 2\n2205             a        (x) int64 3 4\n2206         >>> arr.set_index(x=\"a\")\n2207         <xarray.DataArray (x: 2, y: 3)>\n2208         array([[1., 1., 1.],\n2209                [1., 1., 1.]])\n2210         Coordinates:\n2211           * x        (x) int64 3 4\n2212           * y        (y) int64 0 1 2\n2213 \n2214         See Also\n2215         --------\n2216         DataArray.reset_index\n2217         \"\"\"\n2218         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)\n2219         return self._from_temp_dataset(ds)\n2220 \n2221     # change type of self and return to T_DataArray once\n2222     # https://github.com/python/mypy/issues/12846 is resolved\n2223     def reset_index(\n2224         self,\n2225         dims_or_levels: Hashable | Sequence[Hashable],\n2226         drop: bool = False,\n2227     ) -> DataArray:\n2228         \"\"\"Reset the specified index(es) or multi-index level(s).\n2229 \n2230         Parameters\n2231         ----------\n2232         dims_or_levels : Hashable or sequence of Hashable\n2233             Name(s) of the dimension(s) and/or multi-index level(s) that will\n2234             be reset.\n2235         drop : bool, default: False\n2236             If True, remove the specified indexes and/or multi-index levels\n2237             instead of extracting them as new coordinates (default: False).\n2238 \n2239         Returns\n2240         -------\n2241         obj : DataArray\n2242             Another dataarray, with this dataarray's data but replaced\n2243             coordinates.\n2244 \n2245         See Also\n2246         --------\n2247         DataArray.set_index\n2248         \"\"\"\n2249         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)\n2250         return self._from_temp_dataset(ds)\n2251 \n2252     def reorder_levels(\n2253         self: T_DataArray,\n2254         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n2255         **dim_order_kwargs: Sequence[int | Hashable],\n2256     ) -> T_DataArray:\n2257         \"\"\"Rearrange index levels using input order.\n2258 \n2259         Parameters\n2260         ----------\n2261         dim_order dict-like of Hashable to int or Hashable: optional\n2262             Mapping from names matching dimensions and values given\n2263             by lists representing new level orders. Every given dimension\n2264             must have a multi-index.\n2265         **dim_order_kwargs : optional\n2266             The keyword arguments form of ``dim_order``.\n2267             One of dim_order or dim_order_kwargs must be provided.\n2268 \n2269         Returns\n2270         -------\n2271         obj : DataArray\n2272             Another dataarray, with this dataarray's data but replaced\n2273             coordinates.\n2274         \"\"\"\n2275         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)\n2276         return self._from_temp_dataset(ds)\n2277 \n2278     def stack(\n2279         self: T_DataArray,\n2280         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,\n2281         create_index: bool | None = True,\n2282         index_cls: type[Index] = PandasMultiIndex,\n2283         **dimensions_kwargs: Sequence[Hashable],\n2284     ) -> T_DataArray:\n2285         \"\"\"\n2286         Stack any number of existing dimensions into a single new dimension.\n2287 \n2288         New dimensions will be added at the end, and the corresponding\n2289         coordinate variables will be combined into a MultiIndex.\n2290 \n2291         Parameters\n2292         ----------\n2293         dimensions : mapping of Hashable to sequence of Hashable\n2294             Mapping of the form `new_name=(dim1, dim2, ...)`.\n2295             Names of new dimensions, and the existing dimensions that they\n2296             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n2297             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n2298             all dimensions.\n2299         create_index : bool or None, default: True\n2300             If True, create a multi-index for each of the stacked dimensions.\n2301             If False, don't create any index.\n2302             If None, create a multi-index only if exactly one single (1-d) coordinate\n2303             index is found for every dimension to stack.\n2304         index_cls: class, optional\n2305             Can be used to pass a custom multi-index type. Must be an Xarray index that\n2306             implements `.stack()`. By default, a pandas multi-index wrapper is used.\n2307         **dimensions_kwargs\n2308             The keyword arguments form of ``dimensions``.\n2309             One of dimensions or dimensions_kwargs must be provided.\n2310 \n2311         Returns\n2312         -------\n2313         stacked : DataArray\n2314             DataArray with stacked data.\n2315 \n2316         Examples\n2317         --------\n2318         >>> arr = xr.DataArray(\n2319         ...     np.arange(6).reshape(2, 3),\n2320         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2321         ... )\n2322         >>> arr\n2323         <xarray.DataArray (x: 2, y: 3)>\n2324         array([[0, 1, 2],\n2325                [3, 4, 5]])\n2326         Coordinates:\n2327           * x        (x) <U1 'a' 'b'\n2328           * y        (y) int64 0 1 2\n2329         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2330         >>> stacked.indexes[\"z\"]\n2331         MultiIndex([('a', 0),\n2332                     ('a', 1),\n2333                     ('a', 2),\n2334                     ('b', 0),\n2335                     ('b', 1),\n2336                     ('b', 2)],\n2337                    name='z')\n2338 \n2339         See Also\n2340         --------\n2341         DataArray.unstack\n2342         \"\"\"\n2343         ds = self._to_temp_dataset().stack(\n2344             dimensions,\n2345             create_index=create_index,\n2346             index_cls=index_cls,\n2347             **dimensions_kwargs,\n2348         )\n2349         return self._from_temp_dataset(ds)\n2350 \n2351     # change type of self and return to T_DataArray once\n2352     # https://github.com/python/mypy/issues/12846 is resolved\n2353     def unstack(\n2354         self,\n2355         dim: Hashable | Sequence[Hashable] | None = None,\n2356         fill_value: Any = dtypes.NA,\n2357         sparse: bool = False,\n2358     ) -> DataArray:\n2359         \"\"\"\n2360         Unstack existing dimensions corresponding to MultiIndexes into\n2361         multiple new dimensions.\n2362 \n2363         New dimensions will be added at the end.\n2364 \n2365         Parameters\n2366         ----------\n2367         dim : Hashable or sequence of Hashable, optional\n2368             Dimension(s) over which to unstack. By default unstacks all\n2369             MultiIndexes.\n2370         fill_value : scalar or dict-like, default: nan\n2371             Value to be filled. If a dict-like, maps variable names to\n2372             fill values. Use the data array's name to refer to its\n2373             name. If not provided or if the dict-like does not contain\n2374             all variables, the dtype's NA value will be used.\n2375         sparse : bool, default: False\n2376             Use sparse-array if True\n2377 \n2378         Returns\n2379         -------\n2380         unstacked : DataArray\n2381             Array with unstacked data.\n2382 \n2383         Examples\n2384         --------\n2385         >>> arr = xr.DataArray(\n2386         ...     np.arange(6).reshape(2, 3),\n2387         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2388         ... )\n2389         >>> arr\n2390         <xarray.DataArray (x: 2, y: 3)>\n2391         array([[0, 1, 2],\n2392                [3, 4, 5]])\n2393         Coordinates:\n2394           * x        (x) <U1 'a' 'b'\n2395           * y        (y) int64 0 1 2\n2396         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2397         >>> stacked.indexes[\"z\"]\n2398         MultiIndex([('a', 0),\n2399                     ('a', 1),\n2400                     ('a', 2),\n2401                     ('b', 0),\n2402                     ('b', 1),\n2403                     ('b', 2)],\n2404                    name='z')\n2405         >>> roundtripped = stacked.unstack()\n2406         >>> arr.identical(roundtripped)\n2407         True\n2408 \n2409         See Also\n2410         --------\n2411         DataArray.stack\n2412         \"\"\"\n2413         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n2414         return self._from_temp_dataset(ds)\n2415 \n2416     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:\n2417         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n2418         stacked coordinate.\n2419 \n2420         This is the inverse operation of Dataset.to_stacked_array.\n2421 \n2422         Parameters\n2423         ----------\n2424         dim : Hashable\n2425             Name of existing dimension to unstack\n2426         level : int or Hashable, default: 0\n2427             The MultiIndex level to expand to a dataset along. Can either be\n2428             the integer index of the level or its name.\n2429 \n2430         Returns\n2431         -------\n2432         unstacked: Dataset\n2433 \n2434         Examples\n2435         --------\n2436         >>> arr = xr.DataArray(\n2437         ...     np.arange(6).reshape(2, 3),\n2438         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2439         ... )\n2440         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n2441         >>> data\n2442         <xarray.Dataset>\n2443         Dimensions:  (x: 2, y: 3)\n2444         Coordinates:\n2445           * x        (x) <U1 'a' 'b'\n2446           * y        (y) int64 0 1 2\n2447         Data variables:\n2448             a        (x, y) int64 0 1 2 3 4 5\n2449             b        (x) int64 0 3\n2450         >>> stacked = data.to_stacked_array(\"z\", [\"x\"])\n2451         >>> stacked.indexes[\"z\"]\n2452         MultiIndex([('a', 0.0),\n2453                     ('a', 1.0),\n2454                     ('a', 2.0),\n2455                     ('b', nan)],\n2456                    name='z')\n2457         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n2458         >>> data.identical(roundtripped)\n2459         True\n2460 \n2461         See Also\n2462         --------\n2463         Dataset.to_stacked_array\n2464         \"\"\"\n2465         idx = self._indexes[dim].to_pandas_index()\n2466         if not isinstance(idx, pd.MultiIndex):\n2467             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n2468 \n2469         level_number = idx._get_level_number(level)\n2470         variables = idx.levels[level_number]\n2471         variable_dim = idx.names[level_number]\n2472 \n2473         # pull variables out of datarray\n2474         data_dict = {}\n2475         for k in variables:\n2476             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)\n2477 \n2478         # unstacked dataset\n2479         return Dataset(data_dict)\n2480 \n2481     def transpose(\n2482         self: T_DataArray,\n2483         *dims: Hashable,\n2484         transpose_coords: bool = True,\n2485         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2486     ) -> T_DataArray:\n2487         \"\"\"Return a new DataArray object with transposed dimensions.\n2488 \n2489         Parameters\n2490         ----------\n2491         *dims : Hashable, optional\n2492             By default, reverse the dimensions. Otherwise, reorder the\n2493             dimensions to this order.\n2494         transpose_coords : bool, default: True\n2495             If True, also transpose the coordinates of this DataArray.\n2496         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2497             What to do if dimensions that should be selected from are not present in the\n2498             DataArray:\n2499             - \"raise\": raise an exception\n2500             - \"warn\": raise a warning, and ignore the missing dimensions\n2501             - \"ignore\": ignore the missing dimensions\n2502 \n2503         Returns\n2504         -------\n2505         transposed : DataArray\n2506             The returned DataArray's array is transposed.\n2507 \n2508         Notes\n2509         -----\n2510         This operation returns a view of this array's data. It is\n2511         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n2512         -- the data will be fully loaded.\n2513 \n2514         See Also\n2515         --------\n2516         numpy.transpose\n2517         Dataset.transpose\n2518         \"\"\"\n2519         if dims:\n2520             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))\n2521         variable = self.variable.transpose(*dims)\n2522         if transpose_coords:\n2523             coords: dict[Hashable, Variable] = {}\n2524             for name, coord in self.coords.items():\n2525                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2526                 coords[name] = coord.variable.transpose(*coord_dims)\n2527             return self._replace(variable, coords)\n2528         else:\n2529             return self._replace(variable)\n2530 \n2531     @property\n2532     def T(self: T_DataArray) -> T_DataArray:\n2533         return self.transpose()\n2534 \n2535     # change type of self and return to T_DataArray once\n2536     # https://github.com/python/mypy/issues/12846 is resolved\n2537     def drop_vars(\n2538         self,\n2539         names: Hashable | Iterable[Hashable],\n2540         *,\n2541         errors: ErrorOptions = \"raise\",\n2542     ) -> DataArray:\n2543         \"\"\"Returns an array with dropped variables.\n2544 \n2545         Parameters\n2546         ----------\n2547         names : Hashable or iterable of Hashable\n2548             Name(s) of variables to drop.\n2549         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n2550             If 'raise', raises a ValueError error if any of the variable\n2551             passed are not in the dataset. If 'ignore', any given names that are in the\n2552             DataArray are dropped and no error is raised.\n2553 \n2554         Returns\n2555         -------\n2556         dropped : Dataset\n2557             New Dataset copied from `self` with variables removed.\n2558         \"\"\"\n2559         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n2560         return self._from_temp_dataset(ds)\n2561 \n2562     def drop(\n2563         self: T_DataArray,\n2564         labels: Mapping[Any, Any] | None = None,\n2565         dim: Hashable | None = None,\n2566         *,\n2567         errors: ErrorOptions = \"raise\",\n2568         **labels_kwargs,\n2569     ) -> T_DataArray:\n2570         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n2571 \n2572         Using either `drop_vars` or `drop_sel` is encouraged\n2573 \n2574         See Also\n2575         --------\n2576         DataArray.drop_vars\n2577         DataArray.drop_sel\n2578         \"\"\"\n2579         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)\n2580         return self._from_temp_dataset(ds)\n2581 \n2582     def drop_sel(\n2583         self: T_DataArray,\n2584         labels: Mapping[Any, Any] | None = None,\n2585         *,\n2586         errors: ErrorOptions = \"raise\",\n2587         **labels_kwargs,\n2588     ) -> T_DataArray:\n2589         \"\"\"Drop index labels from this DataArray.\n2590 \n2591         Parameters\n2592         ----------\n2593         labels : mapping of Hashable to Any\n2594             Index labels to drop\n2595         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n2596             If 'raise', raises a ValueError error if\n2597             any of the index labels passed are not\n2598             in the dataset. If 'ignore', any given labels that are in the\n2599             dataset are dropped and no error is raised.\n2600         **labels_kwargs : {dim: label, ...}, optional\n2601             The keyword arguments form of ``dim`` and ``labels``\n2602 \n2603         Returns\n2604         -------\n2605         dropped : DataArray\n2606         \"\"\"\n2607         if labels_kwargs or isinstance(labels, dict):\n2608             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n2609 \n2610         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n2611         return self._from_temp_dataset(ds)\n2612 \n2613     def drop_isel(\n2614         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs\n2615     ) -> T_DataArray:\n2616         \"\"\"Drop index positions from this DataArray.\n2617 \n2618         Parameters\n2619         ----------\n2620         indexers : mapping of Hashable to Any or None, default: None\n2621             Index locations to drop\n2622         **indexers_kwargs : {dim: position, ...}, optional\n2623             The keyword arguments form of ``dim`` and ``positions``\n2624 \n2625         Returns\n2626         -------\n2627         dropped : DataArray\n2628 \n2629         Raises\n2630         ------\n2631         IndexError\n2632         \"\"\"\n2633         dataset = self._to_temp_dataset()\n2634         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)\n2635         return self._from_temp_dataset(dataset)\n2636 \n2637     def dropna(\n2638         self: T_DataArray,\n2639         dim: Hashable,\n2640         how: Literal[\"any\", \"all\"] = \"any\",\n2641         thresh: int | None = None,\n2642     ) -> T_DataArray:\n2643         \"\"\"Returns a new array with dropped labels for missing values along\n2644         the provided dimension.\n2645 \n2646         Parameters\n2647         ----------\n2648         dim : Hashable\n2649             Dimension along which to drop missing values. Dropping along\n2650             multiple dimensions simultaneously is not yet supported.\n2651         how : {\"any\", \"all\"}, default: \"any\"\n2652             - any : if any NA values are present, drop that label\n2653             - all : if all values are NA, drop that label\n2654 \n2655         thresh : int or None, default: None\n2656             If supplied, require this many non-NA values.\n2657 \n2658         Returns\n2659         -------\n2660         dropped : DataArray\n2661         \"\"\"\n2662         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2663         return self._from_temp_dataset(ds)\n2664 \n2665     def fillna(self: T_DataArray, value: Any) -> T_DataArray:\n2666         \"\"\"Fill missing values in this object.\n2667 \n2668         This operation follows the normal broadcasting and alignment rules that\n2669         xarray uses for binary arithmetic, except the result is aligned to this\n2670         object (``join='left'``) instead of aligned to the intersection of\n2671         index coordinates (``join='inner'``).\n2672 \n2673         Parameters\n2674         ----------\n2675         value : scalar, ndarray or DataArray\n2676             Used to fill all matching missing values in this array. If the\n2677             argument is a DataArray, it is first aligned with (reindexed to)\n2678             this array.\n2679 \n2680         Returns\n2681         -------\n2682         filled : DataArray\n2683         \"\"\"\n2684         if utils.is_dict_like(value):\n2685             raise TypeError(\n2686                 \"cannot provide fill value as a dictionary with \"\n2687                 \"fillna on a DataArray\"\n2688             )\n2689         out = ops.fillna(self, value)\n2690         return out\n2691 \n2692     def interpolate_na(\n2693         self: T_DataArray,\n2694         dim: Hashable | None = None,\n2695         method: InterpOptions = \"linear\",\n2696         limit: int | None = None,\n2697         use_coordinate: bool | str = True,\n2698         max_gap: (\n2699             None\n2700             | int\n2701             | float\n2702             | str\n2703             | pd.Timedelta\n2704             | np.timedelta64\n2705             | datetime.timedelta\n2706         ) = None,\n2707         keep_attrs: bool | None = None,\n2708         **kwargs: Any,\n2709     ) -> T_DataArray:\n2710         \"\"\"Fill in NaNs by interpolating according to different methods.\n2711 \n2712         Parameters\n2713         ----------\n2714         dim : Hashable or None, optional\n2715             Specifies the dimension along which to interpolate.\n2716         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n2717             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n2718             String indicating which method to use for interpolation:\n2719 \n2720             - 'linear': linear interpolation. Additional keyword\n2721               arguments are passed to :py:func:`numpy.interp`\n2722             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2723               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2724               ``method='polynomial'``, the ``order`` keyword argument must also be\n2725               provided.\n2726             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2727               respective :py:class:`scipy.interpolate` classes.\n2728 \n2729         use_coordinate : bool or str, default: True\n2730             Specifies which index to use as the x values in the interpolation\n2731             formulated as `y = f(x)`. If False, values are treated as if\n2732             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2733             used. If ``use_coordinate`` is a string, it specifies the name of a\n2734             coordinate variariable to use as the index.\n2735         limit : int or None, default: None\n2736             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2737             or None for no limit. This filling is done regardless of the size of\n2738             the gap in the data. To only interpolate over gaps less than a given length,\n2739             see ``max_gap``.\n2740         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n2741             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2742             Use None for no limit. When interpolating along a datetime64 dimension\n2743             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2744 \n2745             - a string that is valid input for pandas.to_timedelta\n2746             - a :py:class:`numpy.timedelta64` object\n2747             - a :py:class:`pandas.Timedelta` object\n2748             - a :py:class:`datetime.timedelta` object\n2749 \n2750             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2751             dimensions has not been implemented yet. Gap length is defined as the difference\n2752             between coordinate values at the first data point after a gap and the last value\n2753             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2754             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2755             For example, consider::\n2756 \n2757                 <xarray.DataArray (x: 9)>\n2758                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2759                 Coordinates:\n2760                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2761 \n2762             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2763         keep_attrs : bool or None, default: None\n2764             If True, the dataarray's attributes (`attrs`) will be copied from\n2765             the original object to the new one.  If False, the new\n2766             object will be returned without attributes.\n2767         **kwargs : dict, optional\n2768             parameters passed verbatim to the underlying interpolation function\n2769 \n2770         Returns\n2771         -------\n2772         interpolated: DataArray\n2773             Filled in DataArray.\n2774 \n2775         See Also\n2776         --------\n2777         numpy.interp\n2778         scipy.interpolate\n2779 \n2780         Examples\n2781         --------\n2782         >>> da = xr.DataArray(\n2783         ...     [np.nan, 2, 3, np.nan, 0], dims=\"x\", coords={\"x\": [0, 1, 2, 3, 4]}\n2784         ... )\n2785         >>> da\n2786         <xarray.DataArray (x: 5)>\n2787         array([nan,  2.,  3., nan,  0.])\n2788         Coordinates:\n2789           * x        (x) int64 0 1 2 3 4\n2790 \n2791         >>> da.interpolate_na(dim=\"x\", method=\"linear\")\n2792         <xarray.DataArray (x: 5)>\n2793         array([nan, 2. , 3. , 1.5, 0. ])\n2794         Coordinates:\n2795           * x        (x) int64 0 1 2 3 4\n2796 \n2797         >>> da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n2798         <xarray.DataArray (x: 5)>\n2799         array([1. , 2. , 3. , 1.5, 0. ])\n2800         Coordinates:\n2801           * x        (x) int64 0 1 2 3 4\n2802         \"\"\"\n2803         from .missing import interp_na\n2804 \n2805         return interp_na(\n2806             self,\n2807             dim=dim,\n2808             method=method,\n2809             limit=limit,\n2810             use_coordinate=use_coordinate,\n2811             max_gap=max_gap,\n2812             keep_attrs=keep_attrs,\n2813             **kwargs,\n2814         )\n2815 \n2816     def ffill(\n2817         self: T_DataArray, dim: Hashable, limit: int | None = None\n2818     ) -> T_DataArray:\n2819         \"\"\"Fill NaN values by propagating values forward\n2820 \n2821         *Requires bottleneck.*\n2822 \n2823         Parameters\n2824         ----------\n2825         dim : Hashable\n2826             Specifies the dimension along which to propagate values when\n2827             filling.\n2828         limit : int or None, default: None\n2829             The maximum number of consecutive NaN values to forward fill. In\n2830             other words, if there is a gap with more than this number of\n2831             consecutive NaNs, it will only be partially filled. Must be greater\n2832             than 0 or None for no limit. Must be None or greater than or equal\n2833             to axis length if filling along chunked axes (dimensions).\n2834 \n2835         Returns\n2836         -------\n2837         filled : DataArray\n2838         \"\"\"\n2839         from .missing import ffill\n2840 \n2841         return ffill(self, dim, limit=limit)\n2842 \n2843     def bfill(\n2844         self: T_DataArray, dim: Hashable, limit: int | None = None\n2845     ) -> T_DataArray:\n2846         \"\"\"Fill NaN values by propagating values backward\n2847 \n2848         *Requires bottleneck.*\n2849 \n2850         Parameters\n2851         ----------\n2852         dim : str\n2853             Specifies the dimension along which to propagate values when\n2854             filling.\n2855         limit : int or None, default: None\n2856             The maximum number of consecutive NaN values to backward fill. In\n2857             other words, if there is a gap with more than this number of\n2858             consecutive NaNs, it will only be partially filled. Must be greater\n2859             than 0 or None for no limit. Must be None or greater than or equal\n2860             to axis length if filling along chunked axes (dimensions).\n2861 \n2862         Returns\n2863         -------\n2864         filled : DataArray\n2865         \"\"\"\n2866         from .missing import bfill\n2867 \n2868         return bfill(self, dim, limit=limit)\n2869 \n2870     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n2871         \"\"\"Combine two DataArray objects, with union of coordinates.\n2872 \n2873         This operation follows the normal broadcasting and alignment rules of\n2874         ``join='outer'``.  Default to non-null values of array calling the\n2875         method.  Use np.nan to fill in vacant cells after alignment.\n2876 \n2877         Parameters\n2878         ----------\n2879         other : DataArray\n2880             Used to fill all matching missing values in this array.\n2881 \n2882         Returns\n2883         -------\n2884         DataArray\n2885         \"\"\"\n2886         return ops.fillna(self, other, join=\"outer\")\n2887 \n2888     def reduce(\n2889         self: T_DataArray,\n2890         func: Callable[..., Any],\n2891         dim: None | Hashable | Iterable[Hashable] = None,\n2892         *,\n2893         axis: None | int | Sequence[int] = None,\n2894         keep_attrs: bool | None = None,\n2895         keepdims: bool = False,\n2896         **kwargs: Any,\n2897     ) -> T_DataArray:\n2898         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2899 \n2900         Parameters\n2901         ----------\n2902         func : callable\n2903             Function which can be called in the form\n2904             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2905             np.ndarray over an integer valued axis.\n2906         dim : Hashable or Iterable of Hashable, optional\n2907             Dimension(s) over which to apply `func`.\n2908         axis : int or sequence of int, optional\n2909             Axis(es) over which to repeatedly apply `func`. Only one of the\n2910             'dim' and 'axis' arguments can be supplied. If neither are\n2911             supplied, then the reduction is calculated over the flattened array\n2912             (by calling `f(x)` without an axis argument).\n2913         keep_attrs : bool or None, optional\n2914             If True, the variable's attributes (`attrs`) will be copied from\n2915             the original object to the new one.  If False (default), the new\n2916             object will be returned without attributes.\n2917         keepdims : bool, default: False\n2918             If True, the dimensions which are reduced are left in the result\n2919             as dimensions of size one. Coordinates that use these dimensions\n2920             are removed.\n2921         **kwargs : dict\n2922             Additional keyword arguments passed on to `func`.\n2923 \n2924         Returns\n2925         -------\n2926         reduced : DataArray\n2927             DataArray with this object's array replaced with an array with\n2928             summarized data and the indicated dimension(s) removed.\n2929         \"\"\"\n2930 \n2931         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2932         return self._replace_maybe_drop_dims(var)\n2933 \n2934     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:\n2935         \"\"\"Convert this array into a pandas object with the same shape.\n2936 \n2937         The type of the returned object depends on the number of DataArray\n2938         dimensions:\n2939 \n2940         * 0D -> `xarray.DataArray`\n2941         * 1D -> `pandas.Series`\n2942         * 2D -> `pandas.DataFrame`\n2943 \n2944         Only works for arrays with 2 or fewer dimensions.\n2945 \n2946         The DataArray constructor performs the inverse transformation.\n2947 \n2948         Returns\n2949         -------\n2950         result : DataArray | Series | DataFrame\n2951             DataArray, pandas Series or pandas DataFrame.\n2952         \"\"\"\n2953         # TODO: consolidate the info about pandas constructors and the\n2954         # attributes that correspond to their indexes into a separate module?\n2955         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2956         try:\n2957             constructor = constructors[self.ndim]\n2958         except KeyError:\n2959             raise ValueError(\n2960                 f\"Cannot convert arrays with {self.ndim} dimensions into \"\n2961                 \"pandas objects. Requires 2 or fewer dimensions.\"\n2962             )\n2963         indexes = [self.get_index(dim) for dim in self.dims]\n2964         return constructor(self.values, *indexes)\n2965 \n2966     def to_dataframe(\n2967         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None\n2968     ) -> pd.DataFrame:\n2969         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2970 \n2971         The DataFrame is indexed by the Cartesian product of index coordinates\n2972         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are\n2973         included as columns in the DataFrame.\n2974 \n2975         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which\n2976         doesn't rely on a MultiIndex to build the DataFrame.\n2977 \n2978         Parameters\n2979         ----------\n2980         name: Hashable or None, optional\n2981             Name to give to this array (required if unnamed).\n2982         dim_order: Sequence of Hashable or None, optional\n2983             Hierarchical dimension order for the resulting dataframe.\n2984             Array content is transposed to this order and then written out as flat\n2985             vectors in contiguous order, so the last dimension in this list\n2986             will be contiguous in the resulting DataFrame. This has a major\n2987             influence on which operations are efficient on the resulting\n2988             dataframe.\n2989 \n2990             If provided, must include all dimensions of this DataArray. By default,\n2991             dimensions are sorted according to the DataArray dimensions order.\n2992 \n2993         Returns\n2994         -------\n2995         result: DataFrame\n2996             DataArray as a pandas DataFrame.\n2997 \n2998         See also\n2999         --------\n3000         DataArray.to_pandas\n3001         DataArray.to_series\n3002         \"\"\"\n3003         if name is None:\n3004             name = self.name\n3005         if name is None:\n3006             raise ValueError(\n3007                 \"cannot convert an unnamed DataArray to a \"\n3008                 \"DataFrame: use the ``name`` parameter\"\n3009             )\n3010         if self.ndim == 0:\n3011             raise ValueError(\"cannot convert a scalar to a DataFrame\")\n3012 \n3013         # By using a unique name, we can convert a DataArray into a DataFrame\n3014         # even if it shares a name with one of its coordinates.\n3015         # I would normally use unique_name = object() but that results in a\n3016         # dataframe with columns in the wrong order, for reasons I have not\n3017         # been able to debug (possibly a pandas bug?).\n3018         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n3019         ds = self._to_dataset_whole(name=unique_name)\n3020 \n3021         if dim_order is None:\n3022             ordered_dims = dict(zip(self.dims, self.shape))\n3023         else:\n3024             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)\n3025 \n3026         df = ds._to_dataframe(ordered_dims)\n3027         df.columns = [name if c == unique_name else c for c in df.columns]\n3028         return df\n3029 \n3030     def to_series(self) -> pd.Series:\n3031         \"\"\"Convert this array into a pandas.Series.\n3032 \n3033         The Series is indexed by the Cartesian product of index coordinates\n3034         (in the form of a :py:class:`pandas.MultiIndex`).\n3035 \n3036         Returns\n3037         -------\n3038         result : Series\n3039             DataArray as a pandas Series.\n3040 \n3041         See also\n3042         --------\n3043         DataArray.to_pandas\n3044         DataArray.to_dataframe\n3045         \"\"\"\n3046         index = self.coords.to_index()\n3047         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n3048 \n3049     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n3050         \"\"\"Convert this array into a numpy.ma.MaskedArray\n3051 \n3052         Parameters\n3053         ----------\n3054         copy : bool, default: True\n3055             If True make a copy of the array in the result. If False,\n3056             a MaskedArray view of DataArray.values is returned.\n3057 \n3058         Returns\n3059         -------\n3060         result : MaskedArray\n3061             Masked where invalid values (nan or inf) occur.\n3062         \"\"\"\n3063         values = self.to_numpy()  # only compute lazy arrays once\n3064         isnull = pd.isnull(values)\n3065         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n3066 \n3067     # path=None writes to bytes\n3068     @overload\n3069     def to_netcdf(\n3070         self,\n3071         path: None = None,\n3072         mode: Literal[\"w\", \"a\"] = \"w\",\n3073         format: T_NetcdfTypes | None = None,\n3074         group: str | None = None,\n3075         engine: T_NetcdfEngine | None = None,\n3076         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3077         unlimited_dims: Iterable[Hashable] | None = None,\n3078         compute: bool = True,\n3079         invalid_netcdf: bool = False,\n3080     ) -> bytes:\n3081         ...\n3082 \n3083     # default return None\n3084     @overload\n3085     def to_netcdf(\n3086         self,\n3087         path: str | PathLike,\n3088         mode: Literal[\"w\", \"a\"] = \"w\",\n3089         format: T_NetcdfTypes | None = None,\n3090         group: str | None = None,\n3091         engine: T_NetcdfEngine | None = None,\n3092         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3093         unlimited_dims: Iterable[Hashable] | None = None,\n3094         compute: Literal[True] = True,\n3095         invalid_netcdf: bool = False,\n3096     ) -> None:\n3097         ...\n3098 \n3099     # compute=False returns dask.Delayed\n3100     @overload\n3101     def to_netcdf(\n3102         self,\n3103         path: str | PathLike,\n3104         mode: Literal[\"w\", \"a\"] = \"w\",\n3105         format: T_NetcdfTypes | None = None,\n3106         group: str | None = None,\n3107         engine: T_NetcdfEngine | None = None,\n3108         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3109         unlimited_dims: Iterable[Hashable] | None = None,\n3110         *,\n3111         compute: Literal[False],\n3112         invalid_netcdf: bool = False,\n3113     ) -> Delayed:\n3114         ...\n3115 \n3116     def to_netcdf(\n3117         self,\n3118         path: str | PathLike | None = None,\n3119         mode: Literal[\"w\", \"a\"] = \"w\",\n3120         format: T_NetcdfTypes | None = None,\n3121         group: str | None = None,\n3122         engine: T_NetcdfEngine | None = None,\n3123         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3124         unlimited_dims: Iterable[Hashable] | None = None,\n3125         compute: bool = True,\n3126         invalid_netcdf: bool = False,\n3127     ) -> bytes | Delayed | None:\n3128         \"\"\"Write dataset contents to a netCDF file.\n3129 \n3130         Parameters\n3131         ----------\n3132         path : str, path-like or None, optional\n3133             Path to which to save this dataset. File-like objects are only\n3134             supported by the scipy engine. If no path is provided, this\n3135             function returns the resulting netCDF file as bytes; in this case,\n3136             we need to use scipy, which does not support netCDF version 4 (the\n3137             default format becomes NETCDF3_64BIT).\n3138         mode : {\"w\", \"a\"}, default: \"w\"\n3139             Write ('w') or append ('a') mode. If mode='w', any existing file at\n3140             this location will be overwritten. If mode='a', existing variables\n3141             will be overwritten.\n3142         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n3143                   \"NETCDF3_CLASSIC\"}, optional\n3144             File format for the resulting netCDF file:\n3145 \n3146             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n3147               features.\n3148             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n3149               netCDF 3 compatible API features.\n3150             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n3151               which fully supports 2+ GB files, but is only compatible with\n3152               clients linked against netCDF version 3.6.0 or later.\n3153             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n3154               handle 2+ GB files very well.\n3155 \n3156             All formats are supported by the netCDF4-python library.\n3157             scipy.io.netcdf only supports the last two formats.\n3158 \n3159             The default format is NETCDF4 if you are saving a file to disk and\n3160             have the netCDF4-python library available. Otherwise, xarray falls\n3161             back to using scipy to write netCDF files and defaults to the\n3162             NETCDF3_64BIT format (scipy does not support netCDF4).\n3163         group : str, optional\n3164             Path to the netCDF4 group in the given file to open (only works for\n3165             format='NETCDF4'). The group(s) will be created if necessary.\n3166         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n3167             Engine to use when writing netCDF files. If not provided, the\n3168             default engine is chosen based on available dependencies, with a\n3169             preference for 'netcdf4' if writing to a file on disk.\n3170         encoding : dict, optional\n3171             Nested dictionary with variable names as keys and dictionaries of\n3172             variable specific encodings as values, e.g.,\n3173             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n3174             \"zlib\": True}, ...}``\n3175 \n3176             The `h5netcdf` engine supports both the NetCDF4-style compression\n3177             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n3178             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n3179             This allows using any compression plugin installed in the HDF5\n3180             library, e.g. LZF.\n3181 \n3182         unlimited_dims : iterable of Hashable, optional\n3183             Dimension(s) that should be serialized as unlimited dimensions.\n3184             By default, no dimensions are treated as unlimited dimensions.\n3185             Note that unlimited_dims may also be set via\n3186             ``dataset.encoding[\"unlimited_dims\"]``.\n3187         compute: bool, default: True\n3188             If true compute immediately, otherwise return a\n3189             ``dask.delayed.Delayed`` object that can be computed later.\n3190         invalid_netcdf: bool, default: False\n3191             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n3192             hdf5 files which are invalid netcdf as described in\n3193             https://github.com/h5netcdf/h5netcdf.\n3194 \n3195         Returns\n3196         -------\n3197         store: bytes or Delayed or None\n3198             * ``bytes`` if path is None\n3199             * ``dask.delayed.Delayed`` if compute is False\n3200             * None otherwise\n3201 \n3202         Notes\n3203         -----\n3204         Only xarray.Dataset objects can be written to netCDF files, so\n3205         the xarray.DataArray is converted to a xarray.Dataset object\n3206         containing a single variable. If the DataArray has no name, or if the\n3207         name is the same as a coordinate name, then it is given the name\n3208         ``\"__xarray_dataarray_variable__\"``.\n3209 \n3210         See Also\n3211         --------\n3212         Dataset.to_netcdf\n3213         \"\"\"\n3214         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf\n3215 \n3216         if self.name is None:\n3217             # If no name is set then use a generic xarray name\n3218             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3219         elif self.name in self.coords or self.name in self.dims:\n3220             # The name is the same as one of the coords names, which netCDF\n3221             # doesn't support, so rename it but keep track of the old name\n3222             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3223             dataset.attrs[DATAARRAY_NAME] = self.name\n3224         else:\n3225             # No problems with the name - so we're fine!\n3226             dataset = self.to_dataset()\n3227 \n3228         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n3229             dataset,\n3230             path,\n3231             mode=mode,\n3232             format=format,\n3233             group=group,\n3234             engine=engine,\n3235             encoding=encoding,\n3236             unlimited_dims=unlimited_dims,\n3237             compute=compute,\n3238             multifile=False,\n3239             invalid_netcdf=invalid_netcdf,\n3240         )\n3241 \n3242     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:\n3243         \"\"\"\n3244         Convert this xarray.DataArray into a dictionary following xarray\n3245         naming conventions.\n3246 \n3247         Converts all variables and attributes to native Python objects.\n3248         Useful for converting to json. To avoid datetime incompatibility\n3249         use decode_times=False kwarg in xarray.open_dataset.\n3250 \n3251         Parameters\n3252         ----------\n3253         data : bool, default: True\n3254             Whether to include the actual data in the dictionary. When set to\n3255             False, returns just the schema.\n3256         encoding : bool, default: False\n3257             Whether to include the Dataset's encoding in the dictionary.\n3258 \n3259         Returns\n3260         -------\n3261         dict: dict\n3262 \n3263         See Also\n3264         --------\n3265         DataArray.from_dict\n3266         Dataset.to_dict\n3267         \"\"\"\n3268         d = self.variable.to_dict(data=data)\n3269         d.update({\"coords\": {}, \"name\": self.name})\n3270         for k in self.coords:\n3271             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n3272         if encoding:\n3273             d[\"encoding\"] = dict(self.encoding)\n3274         return d\n3275 \n3276     @classmethod\n3277     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:\n3278         \"\"\"Convert a dictionary into an xarray.DataArray\n3279 \n3280         Parameters\n3281         ----------\n3282         d : dict\n3283             Mapping with a minimum structure of {\"dims\": [...], \"data\": [...]}\n3284 \n3285         Returns\n3286         -------\n3287         obj : xarray.DataArray\n3288 \n3289         See Also\n3290         --------\n3291         DataArray.to_dict\n3292         Dataset.from_dict\n3293 \n3294         Examples\n3295         --------\n3296         >>> d = {\"dims\": \"t\", \"data\": [1, 2, 3]}\n3297         >>> da = xr.DataArray.from_dict(d)\n3298         >>> da\n3299         <xarray.DataArray (t: 3)>\n3300         array([1, 2, 3])\n3301         Dimensions without coordinates: t\n3302 \n3303         >>> d = {\n3304         ...     \"coords\": {\n3305         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n3306         ...     },\n3307         ...     \"attrs\": {\"title\": \"air temperature\"},\n3308         ...     \"dims\": \"t\",\n3309         ...     \"data\": [10, 20, 30],\n3310         ...     \"name\": \"a\",\n3311         ... }\n3312         >>> da = xr.DataArray.from_dict(d)\n3313         >>> da\n3314         <xarray.DataArray 'a' (t: 3)>\n3315         array([10, 20, 30])\n3316         Coordinates:\n3317           * t        (t) int64 0 1 2\n3318         Attributes:\n3319             title:    air temperature\n3320         \"\"\"\n3321         coords = None\n3322         if \"coords\" in d:\n3323             try:\n3324                 coords = {\n3325                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n3326                     for k, v in d[\"coords\"].items()\n3327                 }\n3328             except KeyError as e:\n3329                 raise ValueError(\n3330                     \"cannot convert dict when coords are missing the key \"\n3331                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n3332                 )\n3333         try:\n3334             data = d[\"data\"]\n3335         except KeyError:\n3336             raise ValueError(\"cannot convert dict without the key 'data''\")\n3337         else:\n3338             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n3339 \n3340         obj.encoding.update(d.get(\"encoding\", {}))\n3341 \n3342         return obj\n3343 \n3344     @classmethod\n3345     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:\n3346         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n3347 \n3348         If the series's index is a MultiIndex, it will be expanded into a\n3349         tensor product of one-dimensional coordinates (filling in missing\n3350         values with NaN). Thus this operation should be the inverse of the\n3351         `to_series` method.\n3352 \n3353         Parameters\n3354         ----------\n3355         series : Series\n3356             Pandas Series object to convert.\n3357         sparse : bool, default: False\n3358             If sparse=True, creates a sparse array instead of a dense NumPy array.\n3359             Requires the pydata/sparse package.\n3360 \n3361         See Also\n3362         --------\n3363         DataArray.to_series\n3364         Dataset.from_dataframe\n3365         \"\"\"\n3366         temp_name = \"__temporary_name\"\n3367         df = pd.DataFrame({temp_name: series})\n3368         ds = Dataset.from_dataframe(df, sparse=sparse)\n3369         result = cast(DataArray, ds[temp_name])\n3370         result.name = series.name\n3371         return result\n3372 \n3373     def to_cdms2(self) -> cdms2_Variable:\n3374         \"\"\"Convert this array into a cdms2.Variable\"\"\"\n3375         from ..convert import to_cdms2\n3376 \n3377         return to_cdms2(self)\n3378 \n3379     @classmethod\n3380     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:\n3381         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\"\"\"\n3382         from ..convert import from_cdms2\n3383 \n3384         return from_cdms2(variable)\n3385 \n3386     def to_iris(self) -> iris_Cube:\n3387         \"\"\"Convert this array into a iris.cube.Cube\"\"\"\n3388         from ..convert import to_iris\n3389 \n3390         return to_iris(self)\n3391 \n3392     @classmethod\n3393     def from_iris(cls, cube: iris_Cube) -> DataArray:\n3394         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\"\"\"\n3395         from ..convert import from_iris\n3396 \n3397         return from_iris(cube)\n3398 \n3399     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:\n3400         \"\"\"Helper function for equals, broadcast_equals, and identical\"\"\"\n3401 \n3402         def compat(x, y):\n3403             return getattr(x.variable, compat_str)(y.variable)\n3404 \n3405         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n3406             self, other\n3407         )\n3408 \n3409     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:\n3410         \"\"\"Two DataArrays are broadcast equal if they are equal after\n3411         broadcasting them against each other such that they have the same\n3412         dimensions.\n3413 \n3414         Parameters\n3415         ----------\n3416         other : DataArray\n3417             DataArray to compare to.\n3418 \n3419         Returns\n3420         ----------\n3421         equal : bool\n3422             True if the two DataArrays are broadcast equal.\n3423 \n3424         See Also\n3425         --------\n3426         DataArray.equals\n3427         DataArray.identical\n3428         \"\"\"\n3429         try:\n3430             return self._all_compat(other, \"broadcast_equals\")\n3431         except (TypeError, AttributeError):\n3432             return False\n3433 \n3434     def equals(self: T_DataArray, other: T_DataArray) -> bool:\n3435         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n3436         values; otherwise False.\n3437 \n3438         DataArrays can still be equal (like pandas objects) if they have NaN\n3439         values in the same locations.\n3440 \n3441         This method is necessary because `v1 == v2` for ``DataArray``\n3442         does element-wise comparisons (like numpy.ndarrays).\n3443 \n3444         Parameters\n3445         ----------\n3446         other : DataArray\n3447             DataArray to compare to.\n3448 \n3449         Returns\n3450         ----------\n3451         equal : bool\n3452             True if the two DataArrays are equal.\n3453 \n3454         See Also\n3455         --------\n3456         DataArray.broadcast_equals\n3457         DataArray.identical\n3458         \"\"\"\n3459         try:\n3460             return self._all_compat(other, \"equals\")\n3461         except (TypeError, AttributeError):\n3462             return False\n3463 \n3464     def identical(self: T_DataArray, other: T_DataArray) -> bool:\n3465         \"\"\"Like equals, but also checks the array name and attributes, and\n3466         attributes on all coordinates.\n3467 \n3468         Parameters\n3469         ----------\n3470         other : DataArray\n3471             DataArray to compare to.\n3472 \n3473         Returns\n3474         ----------\n3475         equal : bool\n3476             True if the two DataArrays are identical.\n3477 \n3478         See Also\n3479         --------\n3480         DataArray.broadcast_equals\n3481         DataArray.equals\n3482         \"\"\"\n3483         try:\n3484             return self.name == other.name and self._all_compat(other, \"identical\")\n3485         except (TypeError, AttributeError):\n3486             return False\n3487 \n3488     def _result_name(self, other: Any = None) -> Hashable | None:\n3489         # use the same naming heuristics as pandas:\n3490         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n3491         other_name = getattr(other, \"name\", _default)\n3492         if other_name is _default or other_name == self.name:\n3493             return self.name\n3494         else:\n3495             return None\n3496 \n3497     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:\n3498         new_var = self.variable.__array_wrap__(obj, context)\n3499         return self._replace(new_var)\n3500 \n3501     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:\n3502         return self.dot(obj)\n3503 \n3504     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n3505         # currently somewhat duplicative, as only other DataArrays are\n3506         # compatible with matmul\n3507         return computation.dot(other, self)\n3508 \n3509     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:\n3510         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n3511         if keep_attrs is None:\n3512             keep_attrs = _get_keep_attrs(default=True)\n3513         with warnings.catch_warnings():\n3514             warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n3515             warnings.filterwarnings(\n3516                 \"ignore\", r\"Mean of empty slice\", category=RuntimeWarning\n3517             )\n3518             with np.errstate(all=\"ignore\"):\n3519                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n3520             if keep_attrs:\n3521                 da.attrs = self.attrs\n3522             return da\n3523 \n3524     def _binary_op(\n3525         self: T_DataArray,\n3526         other: Any,\n3527         f: Callable,\n3528         reflexive: bool = False,\n3529     ) -> T_DataArray:\n3530         from .groupby import GroupBy\n3531 \n3532         if isinstance(other, (Dataset, GroupBy)):\n3533             return NotImplemented\n3534         if isinstance(other, DataArray):\n3535             align_type = OPTIONS[\"arithmetic_join\"]\n3536             self, other = align(self, other, join=align_type, copy=False)  # type: ignore\n3537         other_variable = getattr(other, \"variable\", other)\n3538         other_coords = getattr(other, \"coords\", None)\n3539 \n3540         variable = (\n3541             f(self.variable, other_variable)\n3542             if not reflexive\n3543             else f(other_variable, self.variable)\n3544         )\n3545         coords, indexes = self.coords._merge_raw(other_coords, reflexive)\n3546         name = self._result_name(other)\n3547 \n3548         return self._replace(variable, coords, name, indexes=indexes)\n3549 \n3550     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:\n3551         from .groupby import GroupBy\n3552 \n3553         if isinstance(other, GroupBy):\n3554             raise TypeError(\n3555                 \"in-place operations between a DataArray and \"\n3556                 \"a grouped object are not permitted\"\n3557             )\n3558         # n.b. we can't align other to self (with other.reindex_like(self))\n3559         # because `other` may be converted into floats, which would cause\n3560         # in-place arithmetic to fail unpredictably. Instead, we simply\n3561         # don't support automatic alignment with in-place arithmetic.\n3562         other_coords = getattr(other, \"coords\", None)\n3563         other_variable = getattr(other, \"variable\", other)\n3564         try:\n3565             with self.coords._merge_inplace(other_coords):\n3566                 f(self.variable, other_variable)\n3567         except MergeError as exc:\n3568             raise MergeError(\n3569                 \"Automatic alignment is not supported for in-place operations.\\n\"\n3570                 \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n3571                 \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n3572             ) from exc\n3573         return self\n3574 \n3575     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:\n3576         self.attrs = other.attrs\n3577 \n3578     plot = utils.UncachedAccessor(_PlotMethods)\n3579 \n3580     def _title_for_slice(self, truncate: int = 50) -> str:\n3581         \"\"\"\n3582         If the dataarray has 1 dimensional coordinates or comes from a slice\n3583         we can show that info in the title\n3584 \n3585         Parameters\n3586         ----------\n3587         truncate : int, default: 50\n3588             maximum number of characters for title\n3589 \n3590         Returns\n3591         -------\n3592         title : string\n3593             Can be used for plot titles\n3594 \n3595         \"\"\"\n3596         one_dims = []\n3597         for dim, coord in self.coords.items():\n3598             if coord.size == 1:\n3599                 one_dims.append(\n3600                     \"{dim} = {v}{unit}\".format(\n3601                         dim=dim,\n3602                         v=format_item(coord.values),\n3603                         unit=_get_units_from_attrs(coord),\n3604                     )\n3605                 )\n3606 \n3607         title = \", \".join(one_dims)\n3608         if len(title) > truncate:\n3609             title = title[: (truncate - 3)] + \"...\"\n3610 \n3611         return title\n3612 \n3613     def diff(\n3614         self: T_DataArray,\n3615         dim: Hashable,\n3616         n: int = 1,\n3617         label: Literal[\"upper\", \"lower\"] = \"upper\",\n3618     ) -> T_DataArray:\n3619         \"\"\"Calculate the n-th order discrete difference along given axis.\n3620 \n3621         Parameters\n3622         ----------\n3623         dim : Hashable\n3624             Dimension over which to calculate the finite difference.\n3625         n : int, default: 1\n3626             The number of times values are differenced.\n3627         label : {\"upper\", \"lower\"}, default: \"upper\"\n3628             The new coordinate in dimension ``dim`` will have the\n3629             values of either the minuend's or subtrahend's coordinate\n3630             for values 'upper' and 'lower', respectively.\n3631 \n3632         Returns\n3633         -------\n3634         difference : DataArray\n3635             The n-th order finite difference of this object.\n3636 \n3637         Notes\n3638         -----\n3639         `n` matches numpy's behavior and is different from pandas' first argument named\n3640         `periods`.\n3641 \n3642         Examples\n3643         --------\n3644         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n3645         >>> arr.diff(\"x\")\n3646         <xarray.DataArray (x: 3)>\n3647         array([0, 1, 0])\n3648         Coordinates:\n3649           * x        (x) int64 2 3 4\n3650         >>> arr.diff(\"x\", 2)\n3651         <xarray.DataArray (x: 2)>\n3652         array([ 1, -1])\n3653         Coordinates:\n3654           * x        (x) int64 3 4\n3655 \n3656         See Also\n3657         --------\n3658         DataArray.differentiate\n3659         \"\"\"\n3660         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n3661         return self._from_temp_dataset(ds)\n3662 \n3663     def shift(\n3664         self: T_DataArray,\n3665         shifts: Mapping[Any, int] | None = None,\n3666         fill_value: Any = dtypes.NA,\n3667         **shifts_kwargs: int,\n3668     ) -> T_DataArray:\n3669         \"\"\"Shift this DataArray by an offset along one or more dimensions.\n3670 \n3671         Only the data is moved; coordinates stay in place. This is consistent\n3672         with the behavior of ``shift`` in pandas.\n3673 \n3674         Values shifted from beyond array bounds will appear at one end of\n3675         each dimension, which are filled according to `fill_value`. For periodic\n3676         offsets instead see `roll`.\n3677 \n3678         Parameters\n3679         ----------\n3680         shifts : mapping of Hashable to int or None, optional\n3681             Integer offset to shift along each of the given dimensions.\n3682             Positive offsets shift to the right; negative offsets shift to the\n3683             left.\n3684         fill_value : scalar, optional\n3685             Value to use for newly missing values\n3686         **shifts_kwargs\n3687             The keyword arguments form of ``shifts``.\n3688             One of shifts or shifts_kwargs must be provided.\n3689 \n3690         Returns\n3691         -------\n3692         shifted : DataArray\n3693             DataArray with the same coordinates and attributes but shifted\n3694             data.\n3695 \n3696         See Also\n3697         --------\n3698         roll\n3699 \n3700         Examples\n3701         --------\n3702         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3703         >>> arr.shift(x=1)\n3704         <xarray.DataArray (x: 3)>\n3705         array([nan,  5.,  6.])\n3706         Dimensions without coordinates: x\n3707         \"\"\"\n3708         variable = self.variable.shift(\n3709             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n3710         )\n3711         return self._replace(variable=variable)\n3712 \n3713     def roll(\n3714         self: T_DataArray,\n3715         shifts: Mapping[Hashable, int] | None = None,\n3716         roll_coords: bool = False,\n3717         **shifts_kwargs: int,\n3718     ) -> T_DataArray:\n3719         \"\"\"Roll this array by an offset along one or more dimensions.\n3720 \n3721         Unlike shift, roll treats the given dimensions as periodic, so will not\n3722         create any missing values to be filled.\n3723 \n3724         Unlike shift, roll may rotate all variables, including coordinates\n3725         if specified. The direction of rotation is consistent with\n3726         :py:func:`numpy.roll`.\n3727 \n3728         Parameters\n3729         ----------\n3730         shifts : mapping of Hashable to int, optional\n3731             Integer offset to rotate each of the given dimensions.\n3732             Positive offsets roll to the right; negative offsets roll to the\n3733             left.\n3734         roll_coords : bool, default: False\n3735             Indicates whether to roll the coordinates by the offset too.\n3736         **shifts_kwargs : {dim: offset, ...}, optional\n3737             The keyword arguments form of ``shifts``.\n3738             One of shifts or shifts_kwargs must be provided.\n3739 \n3740         Returns\n3741         -------\n3742         rolled : DataArray\n3743             DataArray with the same attributes but rolled data and coordinates.\n3744 \n3745         See Also\n3746         --------\n3747         shift\n3748 \n3749         Examples\n3750         --------\n3751         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3752         >>> arr.roll(x=1)\n3753         <xarray.DataArray (x: 3)>\n3754         array([7, 5, 6])\n3755         Dimensions without coordinates: x\n3756         \"\"\"\n3757         ds = self._to_temp_dataset().roll(\n3758             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n3759         )\n3760         return self._from_temp_dataset(ds)\n3761 \n3762     @property\n3763     def real(self: T_DataArray) -> T_DataArray:\n3764         return self._replace(self.variable.real)\n3765 \n3766     @property\n3767     def imag(self: T_DataArray) -> T_DataArray:\n3768         return self._replace(self.variable.imag)\n3769 \n3770     def dot(\n3771         self: T_DataArray,\n3772         other: T_DataArray,\n3773         dims: str | Iterable[Hashable] | Ellipsis | None = None,\n3774     ) -> T_DataArray:\n3775         \"\"\"Perform dot product of two DataArrays along their shared dims.\n3776 \n3777         Equivalent to taking taking tensordot over all shared dims.\n3778 \n3779         Parameters\n3780         ----------\n3781         other : DataArray\n3782             The other array with which the dot product is performed.\n3783         dims : ..., str or Iterable of Hashable, optional\n3784             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.\n3785             If not specified, then all the common dimensions are summed over.\n3786 \n3787         Returns\n3788         -------\n3789         result : DataArray\n3790             Array resulting from the dot product over all shared dimensions.\n3791 \n3792         See Also\n3793         --------\n3794         dot\n3795         numpy.tensordot\n3796 \n3797         Examples\n3798         --------\n3799         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n3800         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n3801         >>> dm_vals = np.arange(4)\n3802         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n3803 \n3804         >>> dm.dims\n3805         ('z',)\n3806 \n3807         >>> da.dims\n3808         ('x', 'y', 'z')\n3809 \n3810         >>> dot_result = da.dot(dm)\n3811         >>> dot_result.dims\n3812         ('x', 'y')\n3813 \n3814         \"\"\"\n3815         if isinstance(other, Dataset):\n3816             raise NotImplementedError(\n3817                 \"dot products are not yet supported with Dataset objects.\"\n3818             )\n3819         if not isinstance(other, DataArray):\n3820             raise TypeError(\"dot only operates on DataArrays.\")\n3821 \n3822         return computation.dot(self, other, dims=dims)\n3823 \n3824     # change type of self and return to T_DataArray once\n3825     # https://github.com/python/mypy/issues/12846 is resolved\n3826     def sortby(\n3827         self,\n3828         variables: Hashable | DataArray | Sequence[Hashable | DataArray],\n3829         ascending: bool = True,\n3830     ) -> DataArray:\n3831         \"\"\"Sort object by labels or values (along an axis).\n3832 \n3833         Sorts the dataarray, either along specified dimensions,\n3834         or according to values of 1-D dataarrays that share dimension\n3835         with calling object.\n3836 \n3837         If the input variables are dataarrays, then the dataarrays are aligned\n3838         (via left-join) to the calling object prior to sorting by cell values.\n3839         NaNs are sorted to the end, following Numpy convention.\n3840 \n3841         If multiple sorts along the same dimension is\n3842         given, numpy's lexsort is performed along that dimension:\n3843         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n3844         and the FIRST key in the sequence is used as the primary sort key,\n3845         followed by the 2nd key, etc.\n3846 \n3847         Parameters\n3848         ----------\n3849         variables : Hashable, DataArray, or sequence of Hashable or DataArray\n3850             1D DataArray objects or name(s) of 1D variable(s) in\n3851             coords whose values are used to sort this array.\n3852         ascending : bool, default: True\n3853             Whether to sort by ascending or descending order.\n3854 \n3855         Returns\n3856         -------\n3857         sorted : DataArray\n3858             A new dataarray where all the specified dims are sorted by dim\n3859             labels.\n3860 \n3861         See Also\n3862         --------\n3863         Dataset.sortby\n3864         numpy.sort\n3865         pandas.sort_values\n3866         pandas.sort_index\n3867 \n3868         Examples\n3869         --------\n3870         >>> da = xr.DataArray(\n3871         ...     np.random.rand(5),\n3872         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n3873         ...     dims=\"time\",\n3874         ... )\n3875         >>> da\n3876         <xarray.DataArray (time: 5)>\n3877         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])\n3878         Coordinates:\n3879           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05\n3880 \n3881         >>> da.sortby(da)\n3882         <xarray.DataArray (time: 5)>\n3883         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])\n3884         Coordinates:\n3885           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02\n3886         \"\"\"\n3887         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n3888         return self._from_temp_dataset(ds)\n3889 \n3890     def quantile(\n3891         self: T_DataArray,\n3892         q: ArrayLike,\n3893         dim: str | Iterable[Hashable] | None = None,\n3894         method: QUANTILE_METHODS = \"linear\",\n3895         keep_attrs: bool | None = None,\n3896         skipna: bool | None = None,\n3897         interpolation: QUANTILE_METHODS = None,\n3898     ) -> T_DataArray:\n3899         \"\"\"Compute the qth quantile of the data along the specified dimension.\n3900 \n3901         Returns the qth quantiles(s) of the array elements.\n3902 \n3903         Parameters\n3904         ----------\n3905         q : float or array-like of float\n3906             Quantile to compute, which must be between 0 and 1 inclusive.\n3907         dim : str or Iterable of Hashable, optional\n3908             Dimension(s) over which to apply quantile.\n3909         method : str, default: \"linear\"\n3910             This optional parameter specifies the interpolation method to use when the\n3911             desired quantile lies between two data points. The options sorted by their R\n3912             type as summarized in the H&F paper [1]_ are:\n3913 \n3914                 1. \"inverted_cdf\" (*)\n3915                 2. \"averaged_inverted_cdf\" (*)\n3916                 3. \"closest_observation\" (*)\n3917                 4. \"interpolated_inverted_cdf\" (*)\n3918                 5. \"hazen\" (*)\n3919                 6. \"weibull\" (*)\n3920                 7. \"linear\"  (default)\n3921                 8. \"median_unbiased\" (*)\n3922                 9. \"normal_unbiased\" (*)\n3923 \n3924             The first three methods are discontiuous. The following discontinuous\n3925             variations of the default \"linear\" (7.) option are also available:\n3926 \n3927                 * \"lower\"\n3928                 * \"higher\"\n3929                 * \"midpoint\"\n3930                 * \"nearest\"\n3931 \n3932             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n3933             was previously called \"interpolation\", renamed in accordance with numpy\n3934             version 1.22.0.\n3935 \n3936             (*) These methods require numpy version 1.22 or newer.\n3937 \n3938         keep_attrs : bool or None, optional\n3939             If True, the dataset's attributes (`attrs`) will be copied from\n3940             the original object to the new one.  If False (default), the new\n3941             object will be returned without attributes.\n3942         skipna : bool or None, optional\n3943             If True, skip missing values (as marked by NaN). By default, only\n3944             skips missing values for float dtypes; other dtypes either do not\n3945             have a sentinel missing value (int) or skipna=True has not been\n3946             implemented (object, datetime64 or timedelta64).\n3947 \n3948         Returns\n3949         -------\n3950         quantiles : DataArray\n3951             If `q` is a single quantile, then the result\n3952             is a scalar. If multiple percentiles are given, first axis of\n3953             the result corresponds to the quantile and a quantile dimension\n3954             is added to the return array. The other dimensions are the\n3955             dimensions that remain after the reduction of the array.\n3956 \n3957         See Also\n3958         --------\n3959         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n3960 \n3961         Examples\n3962         --------\n3963         >>> da = xr.DataArray(\n3964         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n3965         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n3966         ...     dims=(\"x\", \"y\"),\n3967         ... )\n3968         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3969         <xarray.DataArray ()>\n3970         array(0.7)\n3971         Coordinates:\n3972             quantile  float64 0.0\n3973         >>> da.quantile(0, dim=\"x\")\n3974         <xarray.DataArray (y: 4)>\n3975         array([0.7, 4.2, 2.6, 1.5])\n3976         Coordinates:\n3977           * y         (y) float64 1.0 1.5 2.0 2.5\n3978             quantile  float64 0.0\n3979         >>> da.quantile([0, 0.5, 1])\n3980         <xarray.DataArray (quantile: 3)>\n3981         array([0.7, 3.4, 9.4])\n3982         Coordinates:\n3983           * quantile  (quantile) float64 0.0 0.5 1.0\n3984         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3985         <xarray.DataArray (quantile: 3, y: 4)>\n3986         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3987                [3.6 , 5.75, 6.  , 1.7 ],\n3988                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3989         Coordinates:\n3990           * y         (y) float64 1.0 1.5 2.0 2.5\n3991           * quantile  (quantile) float64 0.0 0.5 1.0\n3992 \n3993         References\n3994         ----------\n3995         .. [1] R. J. Hyndman and Y. Fan,\n3996            \"Sample quantiles in statistical packages,\"\n3997            The American Statistician, 50(4), pp. 361-365, 1996\n3998         \"\"\"\n3999 \n4000         ds = self._to_temp_dataset().quantile(\n4001             q,\n4002             dim=dim,\n4003             keep_attrs=keep_attrs,\n4004             method=method,\n4005             skipna=skipna,\n4006             interpolation=interpolation,\n4007         )\n4008         return self._from_temp_dataset(ds)\n4009 \n4010     def rank(\n4011         self: T_DataArray,\n4012         dim: Hashable,\n4013         pct: bool = False,\n4014         keep_attrs: bool | None = None,\n4015     ) -> T_DataArray:\n4016         \"\"\"Ranks the data.\n4017 \n4018         Equal values are assigned a rank that is the average of the ranks that\n4019         would have been otherwise assigned to all of the values within that\n4020         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n4021 \n4022         NaNs in the input array are returned as NaNs.\n4023 \n4024         The `bottleneck` library is required.\n4025 \n4026         Parameters\n4027         ----------\n4028         dim : Hashable\n4029             Dimension over which to compute rank.\n4030         pct : bool, default: False\n4031             If True, compute percentage ranks, otherwise compute integer ranks.\n4032         keep_attrs : bool or None, optional\n4033             If True, the dataset's attributes (`attrs`) will be copied from\n4034             the original object to the new one.  If False (default), the new\n4035             object will be returned without attributes.\n4036 \n4037         Returns\n4038         -------\n4039         ranked : DataArray\n4040             DataArray with the same coordinates and dtype 'float64'.\n4041 \n4042         Examples\n4043         --------\n4044         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n4045         >>> arr.rank(\"x\")\n4046         <xarray.DataArray (x: 3)>\n4047         array([1., 2., 3.])\n4048         Dimensions without coordinates: x\n4049         \"\"\"\n4050 \n4051         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n4052         return self._from_temp_dataset(ds)\n4053 \n4054     def differentiate(\n4055         self: T_DataArray,\n4056         coord: Hashable,\n4057         edge_order: Literal[1, 2] = 1,\n4058         datetime_unit: DatetimeUnitOptions = None,\n4059     ) -> T_DataArray:\n4060         \"\"\" Differentiate the array with the second order accurate central\n4061         differences.\n4062 \n4063         .. note::\n4064             This feature is limited to simple cartesian geometry, i.e. coord\n4065             must be one dimensional.\n4066 \n4067         Parameters\n4068         ----------\n4069         coord : Hashable\n4070             The coordinate to be used to compute the gradient.\n4071         edge_order : {1, 2}, default: 1\n4072             N-th order accurate differences at the boundaries.\n4073         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n4074                          \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, optional\n4075             Unit to compute gradient. Only valid for datetime coordinate.\n4076 \n4077         Returns\n4078         -------\n4079         differentiated: DataArray\n4080 \n4081         See also\n4082         --------\n4083         numpy.gradient: corresponding numpy function\n4084 \n4085         Examples\n4086         --------\n4087 \n4088         >>> da = xr.DataArray(\n4089         ...     np.arange(12).reshape(4, 3),\n4090         ...     dims=[\"x\", \"y\"],\n4091         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n4092         ... )\n4093         >>> da\n4094         <xarray.DataArray (x: 4, y: 3)>\n4095         array([[ 0,  1,  2],\n4096                [ 3,  4,  5],\n4097                [ 6,  7,  8],\n4098                [ 9, 10, 11]])\n4099         Coordinates:\n4100           * x        (x) float64 0.0 0.1 1.1 1.2\n4101         Dimensions without coordinates: y\n4102         >>>\n4103         >>> da.differentiate(\"x\")\n4104         <xarray.DataArray (x: 4, y: 3)>\n4105         array([[30.        , 30.        , 30.        ],\n4106                [27.54545455, 27.54545455, 27.54545455],\n4107                [27.54545455, 27.54545455, 27.54545455],\n4108                [30.        , 30.        , 30.        ]])\n4109         Coordinates:\n4110           * x        (x) float64 0.0 0.1 1.1 1.2\n4111         Dimensions without coordinates: y\n4112         \"\"\"\n4113         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n4114         return self._from_temp_dataset(ds)\n4115 \n4116     # change type of self and return to T_DataArray once\n4117     # https://github.com/python/mypy/issues/12846 is resolved\n4118     def integrate(\n4119         self,\n4120         coord: Hashable | Sequence[Hashable] = None,\n4121         datetime_unit: DatetimeUnitOptions = None,\n4122     ) -> DataArray:\n4123         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n4124 \n4125         .. note::\n4126             This feature is limited to simple cartesian geometry, i.e. coord\n4127             must be one dimensional.\n4128 \n4129         Parameters\n4130         ----------\n4131         coord : Hashable, or sequence of Hashable\n4132             Coordinate(s) used for the integration.\n4133         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n4134                         'ps', 'fs', 'as', None}, optional\n4135             Specify the unit if a datetime coordinate is used.\n4136 \n4137         Returns\n4138         -------\n4139         integrated : DataArray\n4140 \n4141         See also\n4142         --------\n4143         Dataset.integrate\n4144         numpy.trapz : corresponding numpy function\n4145 \n4146         Examples\n4147         --------\n4148 \n4149         >>> da = xr.DataArray(\n4150         ...     np.arange(12).reshape(4, 3),\n4151         ...     dims=[\"x\", \"y\"],\n4152         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n4153         ... )\n4154         >>> da\n4155         <xarray.DataArray (x: 4, y: 3)>\n4156         array([[ 0,  1,  2],\n4157                [ 3,  4,  5],\n4158                [ 6,  7,  8],\n4159                [ 9, 10, 11]])\n4160         Coordinates:\n4161           * x        (x) float64 0.0 0.1 1.1 1.2\n4162         Dimensions without coordinates: y\n4163         >>>\n4164         >>> da.integrate(\"x\")\n4165         <xarray.DataArray (y: 3)>\n4166         array([5.4, 6.6, 7.8])\n4167         Dimensions without coordinates: y\n4168         \"\"\"\n4169         ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n4170         return self._from_temp_dataset(ds)\n4171 \n4172     # change type of self and return to T_DataArray once\n4173     # https://github.com/python/mypy/issues/12846 is resolved\n4174     def cumulative_integrate(\n4175         self,\n4176         coord: Hashable | Sequence[Hashable] = None,\n4177         datetime_unit: DatetimeUnitOptions = None,\n4178     ) -> DataArray:\n4179         \"\"\"Integrate cumulatively along the given coordinate using the trapezoidal rule.\n4180 \n4181         .. note::\n4182             This feature is limited to simple cartesian geometry, i.e. coord\n4183             must be one dimensional.\n4184 \n4185             The first entry of the cumulative integral is always 0, in order to keep the\n4186             length of the dimension unchanged between input and output.\n4187 \n4188         Parameters\n4189         ----------\n4190         coord : Hashable, or sequence of Hashable\n4191             Coordinate(s) used for the integration.\n4192         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n4193                         'ps', 'fs', 'as', None}, optional\n4194             Specify the unit if a datetime coordinate is used.\n4195 \n4196         Returns\n4197         -------\n4198         integrated : DataArray\n4199 \n4200         See also\n4201         --------\n4202         Dataset.cumulative_integrate\n4203         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n4204 \n4205         Examples\n4206         --------\n4207 \n4208         >>> da = xr.DataArray(\n4209         ...     np.arange(12).reshape(4, 3),\n4210         ...     dims=[\"x\", \"y\"],\n4211         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n4212         ... )\n4213         >>> da\n4214         <xarray.DataArray (x: 4, y: 3)>\n4215         array([[ 0,  1,  2],\n4216                [ 3,  4,  5],\n4217                [ 6,  7,  8],\n4218                [ 9, 10, 11]])\n4219         Coordinates:\n4220           * x        (x) float64 0.0 0.1 1.1 1.2\n4221         Dimensions without coordinates: y\n4222         >>>\n4223         >>> da.cumulative_integrate(\"x\")\n4224         <xarray.DataArray (x: 4, y: 3)>\n4225         array([[0.  , 0.  , 0.  ],\n4226                [0.15, 0.25, 0.35],\n4227                [4.65, 5.75, 6.85],\n4228                [5.4 , 6.6 , 7.8 ]])\n4229         Coordinates:\n4230           * x        (x) float64 0.0 0.1 1.1 1.2\n4231         Dimensions without coordinates: y\n4232         \"\"\"\n4233         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)\n4234         return self._from_temp_dataset(ds)\n4235 \n4236     def unify_chunks(self) -> DataArray:\n4237         \"\"\"Unify chunk size along all chunked dimensions of this DataArray.\n4238 \n4239         Returns\n4240         -------\n4241         DataArray with consistent chunk sizes for all dask-array variables\n4242 \n4243         See Also\n4244         --------\n4245         dask.array.core.unify_chunks\n4246         \"\"\"\n4247 \n4248         return unify_chunks(self)[0]\n4249 \n4250     def map_blocks(\n4251         self,\n4252         func: Callable[..., T_Xarray],\n4253         args: Sequence[Any] = (),\n4254         kwargs: Mapping[str, Any] | None = None,\n4255         template: DataArray | Dataset | None = None,\n4256     ) -> T_Xarray:\n4257         \"\"\"\n4258         Apply a function to each block of this DataArray.\n4259 \n4260         .. warning::\n4261             This method is experimental and its signature may change.\n4262 \n4263         Parameters\n4264         ----------\n4265         func : callable\n4266             User-provided function that accepts a DataArray as its first\n4267             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n4268             corresponding to one chunk along each chunked dimension. ``func`` will be\n4269             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n4270 \n4271             This function must return either a single DataArray or a single Dataset.\n4272 \n4273             This function cannot add a new chunked dimension.\n4274         args : sequence\n4275             Passed to func after unpacking and subsetting any xarray objects by blocks.\n4276             xarray objects in args must be aligned with this object, otherwise an error is raised.\n4277         kwargs : mapping\n4278             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n4279             subset to blocks. Passing dask collections in kwargs is not allowed.\n4280         template : DataArray or Dataset, optional\n4281             xarray object representing the final result after compute is called. If not provided,\n4282             the function will be first run on mocked-up data, that looks like this object but\n4283             has sizes 0, to determine properties of the returned object such as dtype,\n4284             variable names, attributes, new dimensions and new indexes (if any).\n4285             ``template`` must be provided if the function changes the size of existing dimensions.\n4286             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n4287             ``attrs`` set by ``func`` will be ignored.\n4288 \n4289         Returns\n4290         -------\n4291         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n4292         function.\n4293 \n4294         Notes\n4295         -----\n4296         This function is designed for when ``func`` needs to manipulate a whole xarray object\n4297         subset to each block. Each block is loaded into memory. In the more common case where\n4298         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n4299 \n4300         If none of the variables in this object is backed by dask arrays, calling this function is\n4301         equivalent to calling ``func(obj, *args, **kwargs)``.\n4302 \n4303         See Also\n4304         --------\n4305         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n4306         xarray.DataArray.map_blocks\n4307 \n4308         Examples\n4309         --------\n4310         Calculate an anomaly from climatology using ``.groupby()``. Using\n4311         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n4312         its indices, and its methods like ``.groupby()``.\n4313 \n4314         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n4315         ...     gb = da.groupby(groupby_type)\n4316         ...     clim = gb.mean(dim=\"time\")\n4317         ...     return gb - clim\n4318         ...\n4319         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n4320         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n4321         >>> np.random.seed(123)\n4322         >>> array = xr.DataArray(\n4323         ...     np.random.rand(len(time)),\n4324         ...     dims=[\"time\"],\n4325         ...     coords={\"time\": time, \"month\": month},\n4326         ... ).chunk()\n4327         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n4328         <xarray.DataArray (time: 24)>\n4329         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n4330                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n4331                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n4332                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n4333                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n4334         Coordinates:\n4335           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n4336             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n4337 \n4338         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n4339         to the function being applied in ``xr.map_blocks()``:\n4340 \n4341         >>> array.map_blocks(\n4342         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array\n4343         ... )  # doctest: +ELLIPSIS\n4344         <xarray.DataArray (time: 24)>\n4345         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>\n4346         Coordinates:\n4347           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n4348             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n4349         \"\"\"\n4350         from .parallel import map_blocks\n4351 \n4352         return map_blocks(func, self, args, kwargs, template)\n4353 \n4354     def polyfit(\n4355         self,\n4356         dim: Hashable,\n4357         deg: int,\n4358         skipna: bool | None = None,\n4359         rcond: float | None = None,\n4360         w: Hashable | Any | None = None,\n4361         full: bool = False,\n4362         cov: bool | Literal[\"unscaled\"] = False,\n4363     ) -> Dataset:\n4364         \"\"\"\n4365         Least squares polynomial fit.\n4366 \n4367         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n4368         invalid values when `skipna = True`.\n4369 \n4370         Parameters\n4371         ----------\n4372         dim : Hashable\n4373             Coordinate along which to fit the polynomials.\n4374         deg : int\n4375             Degree of the fitting polynomial.\n4376         skipna : bool or None, optional\n4377             If True, removes all invalid values before fitting each 1D slices of the array.\n4378             Default is True if data is stored in a dask.array or if there is any\n4379             invalid values, False otherwise.\n4380         rcond : float or None, optional\n4381             Relative condition number to the fit.\n4382         w : Hashable, array-like or None, optional\n4383             Weights to apply to the y-coordinate of the sample points.\n4384             Can be an array-like object or the name of a coordinate in the dataset.\n4385         full : bool, default: False\n4386             Whether to return the residuals, matrix rank and singular values in addition\n4387             to the coefficients.\n4388         cov : bool or \"unscaled\", default: False\n4389             Whether to return to the covariance matrix in addition to the coefficients.\n4390             The matrix is not scaled if `cov='unscaled'`.\n4391 \n4392         Returns\n4393         -------\n4394         polyfit_results : Dataset\n4395             A single dataset which contains:\n4396 \n4397             polyfit_coefficients\n4398                 The coefficients of the best fit.\n4399             polyfit_residuals\n4400                 The residuals of the least-square computation (only included if `full=True`).\n4401                 When the matrix rank is deficient, np.nan is returned.\n4402             [dim]_matrix_rank\n4403                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n4404             [dim]_singular_value\n4405                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n4406             polyfit_covariance\n4407                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n4408 \n4409         See Also\n4410         --------\n4411         numpy.polyfit\n4412         numpy.polyval\n4413         xarray.polyval\n4414         \"\"\"\n4415         return self._to_temp_dataset().polyfit(\n4416             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n4417         )\n4418 \n4419     def pad(\n4420         self: T_DataArray,\n4421         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,\n4422         mode: PadModeOptions = \"constant\",\n4423         stat_length: int\n4424         | tuple[int, int]\n4425         | Mapping[Any, tuple[int, int]]\n4426         | None = None,\n4427         constant_values: float\n4428         | tuple[float, float]\n4429         | Mapping[Any, tuple[float, float]]\n4430         | None = None,\n4431         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n4432         reflect_type: PadReflectOptions = None,\n4433         **pad_width_kwargs: Any,\n4434     ) -> T_DataArray:\n4435         \"\"\"Pad this array along one or more dimensions.\n4436 \n4437         .. warning::\n4438             This function is experimental and its behaviour is likely to change\n4439             especially regarding padding of dimension coordinates (or IndexVariables).\n4440 \n4441         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n4442         coordinates will be padded with the same mode, otherwise coordinates\n4443         are padded using the \"constant\" mode with fill_value dtypes.NA.\n4444 \n4445         Parameters\n4446         ----------\n4447         pad_width : mapping of Hashable to tuple of int\n4448             Mapping with the form of {dim: (pad_before, pad_after)}\n4449             describing the number of values padded along each dimension.\n4450             {dim: pad} is a shortcut for pad_before = pad_after = pad\n4451         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n4452             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n4453             How to pad the DataArray (taken from numpy docs):\n4454 \n4455             - \"constant\": Pads with a constant value.\n4456             - \"edge\": Pads with the edge values of array.\n4457             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n4458               array edge value.\n4459             - \"maximum\": Pads with the maximum value of all or part of the\n4460               vector along each axis.\n4461             - \"mean\": Pads with the mean value of all or part of the\n4462               vector along each axis.\n4463             - \"median\": Pads with the median value of all or part of the\n4464               vector along each axis.\n4465             - \"minimum\": Pads with the minimum value of all or part of the\n4466               vector along each axis.\n4467             - \"reflect\": Pads with the reflection of the vector mirrored on\n4468               the first and last values of the vector along each axis.\n4469             - \"symmetric\": Pads with the reflection of the vector mirrored\n4470               along the edge of the array.\n4471             - \"wrap\": Pads with the wrap of the vector along the axis.\n4472               The first values are used to pad the end and the\n4473               end values are used to pad the beginning.\n4474 \n4475         stat_length : int, tuple or mapping of Hashable to tuple, default: None\n4476             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n4477             values at edge of each axis used to calculate the statistic value.\n4478             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n4479             statistic lengths along each dimension.\n4480             ((before, after),) yields same before and after statistic lengths\n4481             for each dimension.\n4482             (stat_length,) or int is a shortcut for before = after = statistic\n4483             length for all axes.\n4484             Default is ``None``, to use the entire axis.\n4485         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n4486             Used in 'constant'.  The values to set the padded values for each\n4487             axis.\n4488             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n4489             pad constants along each dimension.\n4490             ``((before, after),)`` yields same before and after constants for each\n4491             dimension.\n4492             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n4493             all dimensions.\n4494             Default is 0.\n4495         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n4496             Used in 'linear_ramp'.  The values used for the ending value of the\n4497             linear_ramp and that will form the edge of the padded array.\n4498             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n4499             end values along each dimension.\n4500             ``((before, after),)`` yields same before and after end values for each\n4501             axis.\n4502             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n4503             all axes.\n4504             Default is 0.\n4505         reflect_type : {\"even\", \"odd\", None}, optional\n4506             Used in \"reflect\", and \"symmetric\". The \"even\" style is the\n4507             default with an unaltered reflection around the edge value. For\n4508             the \"odd\" style, the extended part of the array is created by\n4509             subtracting the reflected values from two times the edge value.\n4510         **pad_width_kwargs\n4511             The keyword arguments form of ``pad_width``.\n4512             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n4513 \n4514         Returns\n4515         -------\n4516         padded : DataArray\n4517             DataArray with the padded coordinates and data.\n4518 \n4519         See Also\n4520         --------\n4521         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n4522 \n4523         Notes\n4524         -----\n4525         For ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n4526         promoted to ``float`` and padded with ``np.nan``.\n4527 \n4528         Padding coordinates will drop their corresponding index (if any) and will reset default\n4529         indexes for dimension coordinates.\n4530 \n4531         Examples\n4532         --------\n4533         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n4534         >>> arr.pad(x=(1, 2), constant_values=0)\n4535         <xarray.DataArray (x: 6)>\n4536         array([0, 5, 6, 7, 0, 0])\n4537         Coordinates:\n4538           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n4539 \n4540         >>> da = xr.DataArray(\n4541         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n4542         ...     dims=[\"x\", \"y\"],\n4543         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n4544         ... )\n4545         >>> da.pad(x=1)\n4546         <xarray.DataArray (x: 4, y: 4)>\n4547         array([[nan, nan, nan, nan],\n4548                [ 0.,  1.,  2.,  3.],\n4549                [10., 11., 12., 13.],\n4550                [nan, nan, nan, nan]])\n4551         Coordinates:\n4552           * x        (x) float64 nan 0.0 1.0 nan\n4553           * y        (y) int64 10 20 30 40\n4554             z        (x) float64 nan 100.0 200.0 nan\n4555 \n4556         Careful, ``constant_values`` are coerced to the data type of the array which may\n4557         lead to a loss of precision:\n4558 \n4559         >>> da.pad(x=1, constant_values=1.23456789)\n4560         <xarray.DataArray (x: 4, y: 4)>\n4561         array([[ 1,  1,  1,  1],\n4562                [ 0,  1,  2,  3],\n4563                [10, 11, 12, 13],\n4564                [ 1,  1,  1,  1]])\n4565         Coordinates:\n4566           * x        (x) float64 nan 0.0 1.0 nan\n4567           * y        (y) int64 10 20 30 40\n4568             z        (x) float64 nan 100.0 200.0 nan\n4569         \"\"\"\n4570         ds = self._to_temp_dataset().pad(\n4571             pad_width=pad_width,\n4572             mode=mode,\n4573             stat_length=stat_length,\n4574             constant_values=constant_values,\n4575             end_values=end_values,\n4576             reflect_type=reflect_type,\n4577             **pad_width_kwargs,\n4578         )\n4579         return self._from_temp_dataset(ds)\n4580 \n4581     def idxmin(\n4582         self,\n4583         dim: Hashable | None = None,\n4584         skipna: bool | None = None,\n4585         fill_value: Any = dtypes.NA,\n4586         keep_attrs: bool | None = None,\n4587     ) -> DataArray:\n4588         \"\"\"Return the coordinate label of the minimum value along a dimension.\n4589 \n4590         Returns a new `DataArray` named after the dimension with the values of\n4591         the coordinate labels along that dimension corresponding to minimum\n4592         values along that dimension.\n4593 \n4594         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n4595         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n4596 \n4597         Parameters\n4598         ----------\n4599         dim : str, optional\n4600             Dimension over which to apply `idxmin`.  This is optional for 1D\n4601             arrays, but required for arrays with 2 or more dimensions.\n4602         skipna : bool or None, default: None\n4603             If True, skip missing values (as marked by NaN). By default, only\n4604             skips missing values for ``float``, ``complex``, and ``object``\n4605             dtypes; other dtypes either do not have a sentinel missing value\n4606             (``int``) or ``skipna=True`` has not been implemented\n4607             (``datetime64`` or ``timedelta64``).\n4608         fill_value : Any, default: NaN\n4609             Value to be filled in case all of the values along a dimension are\n4610             null.  By default this is NaN.  The fill value and result are\n4611             automatically converted to a compatible dtype if possible.\n4612             Ignored if ``skipna`` is False.\n4613         keep_attrs : bool or None, optional\n4614             If True, the attributes (``attrs``) will be copied from the\n4615             original object to the new one. If False, the new object\n4616             will be returned without attributes.\n4617 \n4618         Returns\n4619         -------\n4620         reduced : DataArray\n4621             New `DataArray` object with `idxmin` applied to its data and the\n4622             indicated dimension removed.\n4623 \n4624         See Also\n4625         --------\n4626         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n4627 \n4628         Examples\n4629         --------\n4630         >>> array = xr.DataArray(\n4631         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n4632         ... )\n4633         >>> array.min()\n4634         <xarray.DataArray ()>\n4635         array(-2)\n4636         >>> array.argmin()\n4637         <xarray.DataArray ()>\n4638         array(4)\n4639         >>> array.idxmin()\n4640         <xarray.DataArray 'x' ()>\n4641         array('e', dtype='<U1')\n4642 \n4643         >>> array = xr.DataArray(\n4644         ...     [\n4645         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n4646         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n4647         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n4648         ...     ],\n4649         ...     dims=[\"y\", \"x\"],\n4650         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n4651         ... )\n4652         >>> array.min(dim=\"x\")\n4653         <xarray.DataArray (y: 3)>\n4654         array([-2., -4.,  1.])\n4655         Coordinates:\n4656           * y        (y) int64 -1 0 1\n4657         >>> array.argmin(dim=\"x\")\n4658         <xarray.DataArray (y: 3)>\n4659         array([4, 0, 2])\n4660         Coordinates:\n4661           * y        (y) int64 -1 0 1\n4662         >>> array.idxmin(dim=\"x\")\n4663         <xarray.DataArray 'x' (y: 3)>\n4664         array([16.,  0.,  4.])\n4665         Coordinates:\n4666           * y        (y) int64 -1 0 1\n4667         \"\"\"\n4668         return computation._calc_idxminmax(\n4669             array=self,\n4670             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n4671             dim=dim,\n4672             skipna=skipna,\n4673             fill_value=fill_value,\n4674             keep_attrs=keep_attrs,\n4675         )\n4676 \n4677     def idxmax(\n4678         self,\n4679         dim: Hashable = None,\n4680         skipna: bool | None = None,\n4681         fill_value: Any = dtypes.NA,\n4682         keep_attrs: bool | None = None,\n4683     ) -> DataArray:\n4684         \"\"\"Return the coordinate label of the maximum value along a dimension.\n4685 \n4686         Returns a new `DataArray` named after the dimension with the values of\n4687         the coordinate labels along that dimension corresponding to maximum\n4688         values along that dimension.\n4689 \n4690         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n4691         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n4692 \n4693         Parameters\n4694         ----------\n4695         dim : Hashable, optional\n4696             Dimension over which to apply `idxmax`.  This is optional for 1D\n4697             arrays, but required for arrays with 2 or more dimensions.\n4698         skipna : bool or None, default: None\n4699             If True, skip missing values (as marked by NaN). By default, only\n4700             skips missing values for ``float``, ``complex``, and ``object``\n4701             dtypes; other dtypes either do not have a sentinel missing value\n4702             (``int``) or ``skipna=True`` has not been implemented\n4703             (``datetime64`` or ``timedelta64``).\n4704         fill_value : Any, default: NaN\n4705             Value to be filled in case all of the values along a dimension are\n4706             null.  By default this is NaN.  The fill value and result are\n4707             automatically converted to a compatible dtype if possible.\n4708             Ignored if ``skipna`` is False.\n4709         keep_attrs : bool or None, optional\n4710             If True, the attributes (``attrs``) will be copied from the\n4711             original object to the new one. If False, the new object\n4712             will be returned without attributes.\n4713 \n4714         Returns\n4715         -------\n4716         reduced : DataArray\n4717             New `DataArray` object with `idxmax` applied to its data and the\n4718             indicated dimension removed.\n4719 \n4720         See Also\n4721         --------\n4722         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n4723 \n4724         Examples\n4725         --------\n4726         >>> array = xr.DataArray(\n4727         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n4728         ... )\n4729         >>> array.max()\n4730         <xarray.DataArray ()>\n4731         array(2)\n4732         >>> array.argmax()\n4733         <xarray.DataArray ()>\n4734         array(1)\n4735         >>> array.idxmax()\n4736         <xarray.DataArray 'x' ()>\n4737         array('b', dtype='<U1')\n4738 \n4739         >>> array = xr.DataArray(\n4740         ...     [\n4741         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n4742         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n4743         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n4744         ...     ],\n4745         ...     dims=[\"y\", \"x\"],\n4746         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n4747         ... )\n4748         >>> array.max(dim=\"x\")\n4749         <xarray.DataArray (y: 3)>\n4750         array([2., 2., 1.])\n4751         Coordinates:\n4752           * y        (y) int64 -1 0 1\n4753         >>> array.argmax(dim=\"x\")\n4754         <xarray.DataArray (y: 3)>\n4755         array([0, 2, 2])\n4756         Coordinates:\n4757           * y        (y) int64 -1 0 1\n4758         >>> array.idxmax(dim=\"x\")\n4759         <xarray.DataArray 'x' (y: 3)>\n4760         array([0., 4., 4.])\n4761         Coordinates:\n4762           * y        (y) int64 -1 0 1\n4763         \"\"\"\n4764         return computation._calc_idxminmax(\n4765             array=self,\n4766             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n4767             dim=dim,\n4768             skipna=skipna,\n4769             fill_value=fill_value,\n4770             keep_attrs=keep_attrs,\n4771         )\n4772 \n4773     # change type of self and return to T_DataArray once\n4774     # https://github.com/python/mypy/issues/12846 is resolved\n4775     def argmin(\n4776         self,\n4777         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,\n4778         axis: int | None = None,\n4779         keep_attrs: bool | None = None,\n4780         skipna: bool | None = None,\n4781     ) -> DataArray | dict[Hashable, DataArray]:\n4782         \"\"\"Index or indices of the minimum of the DataArray over one or more dimensions.\n4783 \n4784         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n4785         which can be passed directly to isel(). If a single str is passed to 'dim' then\n4786         returns a DataArray with dtype int.\n4787 \n4788         If there are multiple minima, the indices of the first one found will be\n4789         returned.\n4790 \n4791         Parameters\n4792         ----------\n4793         dim : Hashable, sequence of Hashable, None or ..., optional\n4794             The dimensions over which to find the minimum. By default, finds minimum over\n4795             all dimensions - for now returning an int for backward compatibility, but\n4796             this is deprecated, in future will return a dict with indices for all\n4797             dimensions; to return a dict with all dimensions now, pass '...'.\n4798         axis : int or None, optional\n4799             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n4800             can be supplied.\n4801         keep_attrs : bool or None, optional\n4802             If True, the attributes (`attrs`) will be copied from the original\n4803             object to the new one. If False, the new object will be\n4804             returned without attributes.\n4805         skipna : bool or None, optional\n4806             If True, skip missing values (as marked by NaN). By default, only\n4807             skips missing values for float dtypes; other dtypes either do not\n4808             have a sentinel missing value (int) or skipna=True has not been\n4809             implemented (object, datetime64 or timedelta64).\n4810 \n4811         Returns\n4812         -------\n4813         result : DataArray or dict of DataArray\n4814 \n4815         See Also\n4816         --------\n4817         Variable.argmin, DataArray.idxmin\n4818 \n4819         Examples\n4820         --------\n4821         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n4822         >>> array.min()\n4823         <xarray.DataArray ()>\n4824         array(-1)\n4825         >>> array.argmin()\n4826         <xarray.DataArray ()>\n4827         array(2)\n4828         >>> array.argmin(...)\n4829         {'x': <xarray.DataArray ()>\n4830         array(2)}\n4831         >>> array.isel(array.argmin(...))\n4832         <xarray.DataArray ()>\n4833         array(-1)\n4834 \n4835         >>> array = xr.DataArray(\n4836         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],\n4837         ...     dims=(\"x\", \"y\", \"z\"),\n4838         ... )\n4839         >>> array.min(dim=\"x\")\n4840         <xarray.DataArray (y: 3, z: 3)>\n4841         array([[ 1,  2,  1],\n4842                [ 2, -5,  1],\n4843                [ 2,  1,  1]])\n4844         Dimensions without coordinates: y, z\n4845         >>> array.argmin(dim=\"x\")\n4846         <xarray.DataArray (y: 3, z: 3)>\n4847         array([[1, 0, 0],\n4848                [1, 1, 1],\n4849                [0, 0, 1]])\n4850         Dimensions without coordinates: y, z\n4851         >>> array.argmin(dim=[\"x\"])\n4852         {'x': <xarray.DataArray (y: 3, z: 3)>\n4853         array([[1, 0, 0],\n4854                [1, 1, 1],\n4855                [0, 0, 1]])\n4856         Dimensions without coordinates: y, z}\n4857         >>> array.min(dim=(\"x\", \"z\"))\n4858         <xarray.DataArray (y: 3)>\n4859         array([ 1, -5,  1])\n4860         Dimensions without coordinates: y\n4861         >>> array.argmin(dim=[\"x\", \"z\"])\n4862         {'x': <xarray.DataArray (y: 3)>\n4863         array([0, 1, 0])\n4864         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n4865         array([2, 1, 1])\n4866         Dimensions without coordinates: y}\n4867         >>> array.isel(array.argmin(dim=[\"x\", \"z\"]))\n4868         <xarray.DataArray (y: 3)>\n4869         array([ 1, -5,  1])\n4870         Dimensions without coordinates: y\n4871         \"\"\"\n4872         result = self.variable.argmin(dim, axis, keep_attrs, skipna)\n4873         if isinstance(result, dict):\n4874             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n4875         else:\n4876             return self._replace_maybe_drop_dims(result)\n4877 \n4878     # change type of self and return to T_DataArray once\n4879     # https://github.com/python/mypy/issues/12846 is resolved\n4880     def argmax(\n4881         self,\n4882         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,\n4883         axis: int | None = None,\n4884         keep_attrs: bool | None = None,\n4885         skipna: bool | None = None,\n4886     ) -> DataArray | dict[Hashable, DataArray]:\n4887         \"\"\"Index or indices of the maximum of the DataArray over one or more dimensions.\n4888 \n4889         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n4890         which can be passed directly to isel(). If a single str is passed to 'dim' then\n4891         returns a DataArray with dtype int.\n4892 \n4893         If there are multiple maxima, the indices of the first one found will be\n4894         returned.\n4895 \n4896         Parameters\n4897         ----------\n4898         dim : Hashable, sequence of Hashable, None or ..., optional\n4899             The dimensions over which to find the maximum. By default, finds maximum over\n4900             all dimensions - for now returning an int for backward compatibility, but\n4901             this is deprecated, in future will return a dict with indices for all\n4902             dimensions; to return a dict with all dimensions now, pass '...'.\n4903         axis : int or None, optional\n4904             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments\n4905             can be supplied.\n4906         keep_attrs : bool or None, optional\n4907             If True, the attributes (`attrs`) will be copied from the original\n4908             object to the new one. If False, the new object will be\n4909             returned without attributes.\n4910         skipna : bool or None, optional\n4911             If True, skip missing values (as marked by NaN). By default, only\n4912             skips missing values for float dtypes; other dtypes either do not\n4913             have a sentinel missing value (int) or skipna=True has not been\n4914             implemented (object, datetime64 or timedelta64).\n4915 \n4916         Returns\n4917         -------\n4918         result : DataArray or dict of DataArray\n4919 \n4920         See Also\n4921         --------\n4922         Variable.argmax, DataArray.idxmax\n4923 \n4924         Examples\n4925         --------\n4926         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n4927         >>> array.max()\n4928         <xarray.DataArray ()>\n4929         array(3)\n4930         >>> array.argmax()\n4931         <xarray.DataArray ()>\n4932         array(3)\n4933         >>> array.argmax(...)\n4934         {'x': <xarray.DataArray ()>\n4935         array(3)}\n4936         >>> array.isel(array.argmax(...))\n4937         <xarray.DataArray ()>\n4938         array(3)\n4939 \n4940         >>> array = xr.DataArray(\n4941         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],\n4942         ...     dims=(\"x\", \"y\", \"z\"),\n4943         ... )\n4944         >>> array.max(dim=\"x\")\n4945         <xarray.DataArray (y: 3, z: 3)>\n4946         array([[3, 3, 2],\n4947                [3, 5, 2],\n4948                [2, 3, 3]])\n4949         Dimensions without coordinates: y, z\n4950         >>> array.argmax(dim=\"x\")\n4951         <xarray.DataArray (y: 3, z: 3)>\n4952         array([[0, 1, 1],\n4953                [0, 1, 0],\n4954                [0, 1, 0]])\n4955         Dimensions without coordinates: y, z\n4956         >>> array.argmax(dim=[\"x\"])\n4957         {'x': <xarray.DataArray (y: 3, z: 3)>\n4958         array([[0, 1, 1],\n4959                [0, 1, 0],\n4960                [0, 1, 0]])\n4961         Dimensions without coordinates: y, z}\n4962         >>> array.max(dim=(\"x\", \"z\"))\n4963         <xarray.DataArray (y: 3)>\n4964         array([3, 5, 3])\n4965         Dimensions without coordinates: y\n4966         >>> array.argmax(dim=[\"x\", \"z\"])\n4967         {'x': <xarray.DataArray (y: 3)>\n4968         array([0, 1, 0])\n4969         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n4970         array([0, 1, 2])\n4971         Dimensions without coordinates: y}\n4972         >>> array.isel(array.argmax(dim=[\"x\", \"z\"]))\n4973         <xarray.DataArray (y: 3)>\n4974         array([3, 5, 3])\n4975         Dimensions without coordinates: y\n4976         \"\"\"\n4977         result = self.variable.argmax(dim, axis, keep_attrs, skipna)\n4978         if isinstance(result, dict):\n4979             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n4980         else:\n4981             return self._replace_maybe_drop_dims(result)\n4982 \n4983     def query(\n4984         self,\n4985         queries: Mapping[Any, Any] | None = None,\n4986         parser: QueryParserOptions = \"pandas\",\n4987         engine: QueryEngineOptions = None,\n4988         missing_dims: ErrorOptionsWithWarn = \"raise\",\n4989         **queries_kwargs: Any,\n4990     ) -> DataArray:\n4991         \"\"\"Return a new data array indexed along the specified\n4992         dimension(s), where the indexers are given as strings containing\n4993         Python expressions to be evaluated against the values in the array.\n4994 \n4995         Parameters\n4996         ----------\n4997         queries : dict-like or None, optional\n4998             A dict-like with keys matching dimensions and values given by strings\n4999             containing Python expressions to be evaluated against the data variables\n5000             in the dataset. The expressions will be evaluated using the pandas\n5001             eval() function, and can contain any valid Python expressions but cannot\n5002             contain any Python statements.\n5003         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n5004             The parser to use to construct the syntax tree from the expression.\n5005             The default of 'pandas' parses code slightly different than standard\n5006             Python. Alternatively, you can parse an expression using the 'python'\n5007             parser to retain strict Python semantics.\n5008         engine : {\"python\", \"numexpr\", None}, default: None\n5009             The engine used to evaluate the expression. Supported engines are:\n5010 \n5011             - None: tries to use numexpr, falls back to python\n5012             - \"numexpr\": evaluates expressions using numexpr\n5013             - \"python\": performs operations as if you had eval\u2019d in top level python\n5014 \n5015         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n5016             What to do if dimensions that should be selected from are not present in the\n5017             DataArray:\n5018 \n5019             - \"raise\": raise an exception\n5020             - \"warn\": raise a warning, and ignore the missing dimensions\n5021             - \"ignore\": ignore the missing dimensions\n5022 \n5023         **queries_kwargs : {dim: query, ...}, optional\n5024             The keyword arguments form of ``queries``.\n5025             One of queries or queries_kwargs must be provided.\n5026 \n5027         Returns\n5028         -------\n5029         obj : DataArray\n5030             A new DataArray with the same contents as this dataset, indexed by\n5031             the results of the appropriate queries.\n5032 \n5033         See Also\n5034         --------\n5035         DataArray.isel\n5036         Dataset.query\n5037         pandas.eval\n5038 \n5039         Examples\n5040         --------\n5041         >>> da = xr.DataArray(np.arange(0, 5, 1), dims=\"x\", name=\"a\")\n5042         >>> da\n5043         <xarray.DataArray 'a' (x: 5)>\n5044         array([0, 1, 2, 3, 4])\n5045         Dimensions without coordinates: x\n5046         >>> da.query(x=\"a > 2\")\n5047         <xarray.DataArray 'a' (x: 2)>\n5048         array([3, 4])\n5049         Dimensions without coordinates: x\n5050         \"\"\"\n5051 \n5052         ds = self._to_dataset_whole(shallow_copy=True)\n5053         ds = ds.query(\n5054             queries=queries,\n5055             parser=parser,\n5056             engine=engine,\n5057             missing_dims=missing_dims,\n5058             **queries_kwargs,\n5059         )\n5060         return ds[self.name]\n5061 \n5062     def curvefit(\n5063         self,\n5064         coords: str | DataArray | Iterable[str | DataArray],\n5065         func: Callable[..., Any],\n5066         reduce_dims: Hashable | Iterable[Hashable] | None = None,\n5067         skipna: bool = True,\n5068         p0: dict[str, Any] | None = None,\n5069         bounds: dict[str, Any] | None = None,\n5070         param_names: Sequence[str] | None = None,\n5071         kwargs: dict[str, Any] | None = None,\n5072     ) -> Dataset:\n5073         \"\"\"\n5074         Curve fitting optimization for arbitrary functions.\n5075 \n5076         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n5077 \n5078         Parameters\n5079         ----------\n5080         coords : Hashable, DataArray, or sequence of DataArray or Hashable\n5081             Independent coordinate(s) over which to perform the curve fitting. Must share\n5082             at least one dimension with the calling object. When fitting multi-dimensional\n5083             functions, supply `coords` as a sequence in the same order as arguments in\n5084             `func`. To fit along existing dimensions of the calling object, `coords` can\n5085             also be specified as a str or sequence of strs.\n5086         func : callable\n5087             User specified function in the form `f(x, *params)` which returns a numpy\n5088             array of length `len(x)`. `params` are the fittable parameters which are optimized\n5089             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n5090             coordinates, e.g. `f((x0, x1), *params)`.\n5091         reduce_dims : Hashable or sequence of Hashable\n5092             Additional dimension(s) over which to aggregate while fitting. For example,\n5093             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n5094             aggregate all lat and lon points and fit the specified function along the\n5095             time dimension.\n5096         skipna : bool, default: True\n5097             Whether to skip missing values when fitting. Default is True.\n5098         p0 : dict-like or None, optional\n5099             Optional dictionary of parameter names to initial guesses passed to the\n5100             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n5101             be assigned initial values following the default scipy behavior.\n5102         bounds : dict-like or None, optional\n5103             Optional dictionary of parameter names to bounding values passed to the\n5104             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n5105             will be unbounded following the default scipy behavior.\n5106         param_names : sequence of Hashable or None, optional\n5107             Sequence of names for the fittable parameters of `func`. If not supplied,\n5108             this will be automatically determined by arguments of `func`. `param_names`\n5109             should be manually supplied when fitting a function that takes a variable\n5110             number of parameters.\n5111         **kwargs : optional\n5112             Additional keyword arguments to passed to scipy curve_fit.\n5113 \n5114         Returns\n5115         -------\n5116         curvefit_results : Dataset\n5117             A single dataset which contains:\n5118 \n5119             [var]_curvefit_coefficients\n5120                 The coefficients of the best fit.\n5121             [var]_curvefit_covariance\n5122                 The covariance matrix of the coefficient estimates.\n5123 \n5124         See Also\n5125         --------\n5126         DataArray.polyfit\n5127         scipy.optimize.curve_fit\n5128         \"\"\"\n5129         return self._to_temp_dataset().curvefit(\n5130             coords,\n5131             func,\n5132             reduce_dims=reduce_dims,\n5133             skipna=skipna,\n5134             p0=p0,\n5135             bounds=bounds,\n5136             param_names=param_names,\n5137             kwargs=kwargs,\n5138         )\n5139 \n5140     def drop_duplicates(\n5141         self: T_DataArray,\n5142         dim: Hashable | Iterable[Hashable],\n5143         keep: Literal[\"first\", \"last\", False] = \"first\",\n5144     ) -> T_DataArray:\n5145         \"\"\"Returns a new DataArray with duplicate dimension values removed.\n5146 \n5147         Parameters\n5148         ----------\n5149         dim : dimension label or labels\n5150             Pass `...` to drop duplicates along all dimensions.\n5151         keep : {\"first\", \"last\", False}, default: \"first\"\n5152             Determines which duplicates (if any) to keep.\n5153 \n5154             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n5155             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n5156             - False : Drop all duplicates.\n5157 \n5158         Returns\n5159         -------\n5160         DataArray\n5161 \n5162         See Also\n5163         --------\n5164         Dataset.drop_duplicates\n5165         \"\"\"\n5166         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)\n5167         return self._from_temp_dataset(deduplicated)\n5168 \n5169     def convert_calendar(\n5170         self,\n5171         calendar: str,\n5172         dim: str = \"time\",\n5173         align_on: str | None = None,\n5174         missing: Any | None = None,\n5175         use_cftime: bool | None = None,\n5176     ) -> DataArray:\n5177         \"\"\"Convert the DataArray to another calendar.\n5178 \n5179         Only converts the individual timestamps, does not modify any data except\n5180         in dropping invalid/surplus dates or inserting missing dates.\n5181 \n5182         If the source and target calendars are either no_leap, all_leap or a\n5183         standard type, only the type of the time array is modified.\n5184         When converting to a leap year from a non-leap year, the 29th of February\n5185         is removed from the array. In the other direction the 29th of February\n5186         will be missing in the output, unless `missing` is specified,\n5187         in which case that value is inserted.\n5188 \n5189         For conversions involving `360_day` calendars, see Notes.\n5190 \n5191         This method is safe to use with sub-daily data as it doesn't touch the\n5192         time part of the timestamps.\n5193 \n5194         Parameters\n5195         ---------\n5196         calendar : str\n5197             The target calendar name.\n5198         dim : str\n5199             Name of the time coordinate.\n5200         align_on : {None, 'date', 'year'}\n5201             Must be specified when either source or target is a `360_day` calendar,\n5202            ignored otherwise. See Notes.\n5203         missing : Optional[any]\n5204             By default, i.e. if the value is None, this method will simply attempt\n5205             to convert the dates in the source calendar to the same dates in the\n5206             target calendar, and drop any of those that are not possible to\n5207             represent.  If a value is provided, a new time coordinate will be\n5208             created in the target calendar with the same frequency as the original\n5209             time coordinate; for any dates that are not present in the source, the\n5210             data will be filled with this value.  Note that using this mode requires\n5211             that the source data have an inferable frequency; for more information\n5212             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n5213             target calendar combinations, this could result in many missing values, see notes.\n5214         use_cftime : boolean, optional\n5215             Whether to use cftime objects in the output, only used if `calendar`\n5216             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n5217             If True, the new time axis uses cftime objects.\n5218             If None (default), it uses :py:class:`numpy.datetime64` values if the\n5219             date range permits it, and :py:class:`cftime.datetime` objects if not.\n5220             If False, it uses :py:class:`numpy.datetime64`  or fails.\n5221 \n5222         Returns\n5223         -------\n5224         DataArray\n5225             Copy of the dataarray with the time coordinate converted to the\n5226             target calendar. If 'missing' was None (default), invalid dates in\n5227             the new calendar are dropped, but missing dates are not inserted.\n5228             If `missing` was given, the new data is reindexed to have a time axis\n5229             with the same frequency as the source, but in the new calendar; any\n5230             missing datapoints are filled with `missing`.\n5231 \n5232         Notes\n5233         -----\n5234         Passing a value to `missing` is only usable if the source's time coordinate as an\n5235         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n5236         if the target coordinate, generated from this frequency, has dates equivalent to the\n5237         source. It is usually **not** appropriate to use this mode with:\n5238 \n5239         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n5240         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n5241             or 'mH' where 24 % m != 0).\n5242 \n5243         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n5244         be specified and two options are offered.\n5245 \n5246         - \"year\"\n5247             The dates are translated according to their relative position in the year,\n5248             ignoring their original month and day information, meaning that the\n5249             missing/surplus days are added/removed at regular intervals.\n5250 \n5251             From a `360_day` to a standard calendar, the output will be missing the\n5252             following dates (day of year in parentheses):\n5253 \n5254             To a leap year:\n5255                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n5256                 September 31st (275) and November 30th (335).\n5257             To a non-leap year:\n5258                 February 6th (36), April 19th (109), July 2nd (183),\n5259                 September 12th (255), November 25th (329).\n5260 \n5261             From a standard calendar to a `\"360_day\"`, the following dates in the\n5262             source array will be dropped:\n5263 \n5264             From a leap year:\n5265                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n5266                 September 31st (275), December 1st (336)\n5267             From a non-leap year:\n5268                 February 6th (37), April 20th (110), July 2nd (183),\n5269                 September 13th (256), November 25th (329)\n5270 \n5271             This option is best used on daily and subdaily data.\n5272 \n5273         - \"date\"\n5274             The month/day information is conserved and invalid dates are dropped\n5275             from the output. This means that when converting from a `\"360_day\"` to a\n5276             standard calendar, all 31st (Jan, March, May, July, August, October and\n5277             December) will be missing as there is no equivalent dates in the\n5278             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n5279             will be dropped as there are no equivalent dates in a standard calendar.\n5280 \n5281             This option is best used with data on a frequency coarser than daily.\n5282         \"\"\"\n5283         return convert_calendar(\n5284             self,\n5285             calendar,\n5286             dim=dim,\n5287             align_on=align_on,\n5288             missing=missing,\n5289             use_cftime=use_cftime,\n5290         )\n5291 \n5292     def interp_calendar(\n5293         self,\n5294         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n5295         dim: str = \"time\",\n5296     ) -> DataArray:\n5297         \"\"\"Interpolates the DataArray to another calendar based on decimal year measure.\n5298 \n5299         Each timestamp in `source` and `target` are first converted to their decimal\n5300         year equivalent then `source` is interpolated on the target coordinate.\n5301         The decimal year of a timestamp is its year plus its sub-year component\n5302         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n5303         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n5304 \n5305         This method should only be used when the time (HH:MM:SS) information of\n5306         time coordinate is not important.\n5307 \n5308         Parameters\n5309         ----------\n5310         target: DataArray or DatetimeIndex or CFTimeIndex\n5311             The target time coordinate of a valid dtype\n5312             (np.datetime64 or cftime objects)\n5313         dim : str\n5314             The time coordinate name.\n5315 \n5316         Return\n5317         ------\n5318         DataArray\n5319             The source interpolated on the decimal years of target,\n5320         \"\"\"\n5321         return interp_calendar(self, target, dim=dim)\n5322 \n5323     def groupby(\n5324         self,\n5325         group: Hashable | DataArray | IndexVariable,\n5326         squeeze: bool = True,\n5327         restore_coord_dims: bool = False,\n5328     ) -> DataArrayGroupBy:\n5329         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n5330 \n5331         Parameters\n5332         ----------\n5333         group : Hashable, DataArray or IndexVariable\n5334             Array whose unique values should be used to group this array. If a\n5335             Hashable, must be the name of a coordinate contained in this dataarray.\n5336         squeeze : bool, default: True\n5337             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n5338             controls whether the subarrays have a dimension of length 1 along\n5339             that dimension or if the dimension is squeezed out.\n5340         restore_coord_dims : bool, default: False\n5341             If True, also restore the dimension order of multi-dimensional\n5342             coordinates.\n5343 \n5344         Returns\n5345         -------\n5346         grouped : DataArrayGroupBy\n5347             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n5348             iterated over in the form of `(unique_value, grouped_array)` pairs.\n5349 \n5350         Examples\n5351         --------\n5352         Calculate daily anomalies for daily data:\n5353 \n5354         >>> da = xr.DataArray(\n5355         ...     np.linspace(0, 1826, num=1827),\n5356         ...     coords=[pd.date_range(\"1/1/2000\", \"31/12/2004\", freq=\"D\")],\n5357         ...     dims=\"time\",\n5358         ... )\n5359         >>> da\n5360         <xarray.DataArray (time: 1827)>\n5361         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,\n5362                1.826e+03])\n5363         Coordinates:\n5364           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n5365         >>> da.groupby(\"time.dayofyear\") - da.groupby(\"time.dayofyear\").mean(\"time\")\n5366         <xarray.DataArray (time: 1827)>\n5367         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n5368         Coordinates:\n5369           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n5370             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366\n5371 \n5372         See Also\n5373         --------\n5374         DataArray.groupby_bins\n5375         Dataset.groupby\n5376         core.groupby.DataArrayGroupBy\n5377         pandas.DataFrame.groupby\n5378         \"\"\"\n5379         from .groupby import DataArrayGroupBy\n5380 \n5381         # While we don't generally check the type of every arg, passing\n5382         # multiple dimensions as multiple arguments is common enough, and the\n5383         # consequences hidden enough (strings evaluate as true) to warrant\n5384         # checking here.\n5385         # A future version could make squeeze kwarg only, but would face\n5386         # backward-compat issues.\n5387         if not isinstance(squeeze, bool):\n5388             raise TypeError(\n5389                 f\"`squeeze` must be True or False, but {squeeze} was supplied\"\n5390             )\n5391 \n5392         return DataArrayGroupBy(\n5393             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims\n5394         )\n5395 \n5396     def groupby_bins(\n5397         self,\n5398         group: Hashable | DataArray | IndexVariable,\n5399         bins: ArrayLike,\n5400         right: bool = True,\n5401         labels: ArrayLike | Literal[False] | None = None,\n5402         precision: int = 3,\n5403         include_lowest: bool = False,\n5404         squeeze: bool = True,\n5405         restore_coord_dims: bool = False,\n5406     ) -> DataArrayGroupBy:\n5407         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n5408 \n5409         Rather than using all unique values of `group`, the values are discretized\n5410         first by applying `pandas.cut` [1]_ to `group`.\n5411 \n5412         Parameters\n5413         ----------\n5414         group : Hashable, DataArray or IndexVariable\n5415             Array whose binned values should be used to group this array. If a\n5416             Hashable, must be the name of a coordinate contained in this dataarray.\n5417         bins : int or array-like\n5418             If bins is an int, it defines the number of equal-width bins in the\n5419             range of x. However, in this case, the range of x is extended by .1%\n5420             on each side to include the min or max values of x. If bins is a\n5421             sequence it defines the bin edges allowing for non-uniform bin\n5422             width. No extension of the range of x is done in this case.\n5423         right : bool, default: True\n5424             Indicates whether the bins include the rightmost edge or not. If\n5425             right == True (the default), then the bins [1,2,3,4] indicate\n5426             (1,2], (2,3], (3,4].\n5427         labels : array-like, False or None, default: None\n5428             Used as labels for the resulting bins. Must be of the same length as\n5429             the resulting bins. If False, string bin labels are assigned by\n5430             `pandas.cut`.\n5431         precision : int, default: 3\n5432             The precision at which to store and display the bins labels.\n5433         include_lowest : bool, default: False\n5434             Whether the first interval should be left-inclusive or not.\n5435         squeeze : bool, default: True\n5436             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n5437             controls whether the subarrays have a dimension of length 1 along\n5438             that dimension or if the dimension is squeezed out.\n5439         restore_coord_dims : bool, default: False\n5440             If True, also restore the dimension order of multi-dimensional\n5441             coordinates.\n5442 \n5443         Returns\n5444         -------\n5445         grouped : DataArrayGroupBy\n5446             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n5447             iterated over in the form of `(unique_value, grouped_array)` pairs.\n5448             The name of the group has the added suffix `_bins` in order to\n5449             distinguish it from the original variable.\n5450 \n5451         See Also\n5452         --------\n5453         DataArray.groupby\n5454         Dataset.groupby_bins\n5455         core.groupby.DataArrayGroupBy\n5456         pandas.DataFrame.groupby\n5457 \n5458         References\n5459         ----------\n5460         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n5461         \"\"\"\n5462         from .groupby import DataArrayGroupBy\n5463 \n5464         return DataArrayGroupBy(\n5465             self,\n5466             group,\n5467             squeeze=squeeze,\n5468             bins=bins,\n5469             restore_coord_dims=restore_coord_dims,\n5470             cut_kwargs={\n5471                 \"right\": right,\n5472                 \"labels\": labels,\n5473                 \"precision\": precision,\n5474                 \"include_lowest\": include_lowest,\n5475             },\n5476         )\n5477 \n5478     def weighted(self, weights: DataArray) -> DataArrayWeighted:\n5479         \"\"\"\n5480         Weighted DataArray operations.\n5481 \n5482         Parameters\n5483         ----------\n5484         weights : DataArray\n5485             An array of weights associated with the values in this Dataset.\n5486             Each value in the data contributes to the reduction operation\n5487             according to its associated weight.\n5488 \n5489         Notes\n5490         -----\n5491         ``weights`` must be a DataArray and cannot contain missing values.\n5492         Missing values can be replaced by ``weights.fillna(0)``.\n5493 \n5494         Returns\n5495         -------\n5496         core.weighted.DataArrayWeighted\n5497 \n5498         See Also\n5499         --------\n5500         Dataset.weighted\n5501         \"\"\"\n5502         from .weighted import DataArrayWeighted\n5503 \n5504         return DataArrayWeighted(self, weights)\n5505 \n5506     def rolling(\n5507         self,\n5508         dim: Mapping[Any, int] | None = None,\n5509         min_periods: int | None = None,\n5510         center: bool | Mapping[Any, bool] = False,\n5511         **window_kwargs: int,\n5512     ) -> DataArrayRolling:\n5513         \"\"\"\n5514         Rolling window object for DataArrays.\n5515 \n5516         Parameters\n5517         ----------\n5518         dim : dict, optional\n5519             Mapping from the dimension name to create the rolling iterator\n5520             along (e.g. `time`) to its moving window size.\n5521         min_periods : int or None, default: None\n5522             Minimum number of observations in window required to have a value\n5523             (otherwise result is NA). The default, None, is equivalent to\n5524             setting min_periods equal to the size of the window.\n5525         center : bool or Mapping to int, default: False\n5526             Set the labels at the center of the window.\n5527         **window_kwargs : optional\n5528             The keyword arguments form of ``dim``.\n5529             One of dim or window_kwargs must be provided.\n5530 \n5531         Returns\n5532         -------\n5533         core.rolling.DataArrayRolling\n5534 \n5535         Examples\n5536         --------\n5537         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n5538 \n5539         >>> da = xr.DataArray(\n5540         ...     np.linspace(0, 11, num=12),\n5541         ...     coords=[\n5542         ...         pd.date_range(\n5543         ...             \"1999-12-15\",\n5544         ...             periods=12,\n5545         ...             freq=pd.DateOffset(months=1),\n5546         ...         )\n5547         ...     ],\n5548         ...     dims=\"time\",\n5549         ... )\n5550         >>> da\n5551         <xarray.DataArray (time: 12)>\n5552         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n5553         Coordinates:\n5554           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n5555         >>> da.rolling(time=3, center=True).mean()\n5556         <xarray.DataArray (time: 12)>\n5557         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n5558         Coordinates:\n5559           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n5560 \n5561         Remove the NaNs using ``dropna()``:\n5562 \n5563         >>> da.rolling(time=3, center=True).mean().dropna(\"time\")\n5564         <xarray.DataArray (time: 10)>\n5565         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n5566         Coordinates:\n5567           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15\n5568 \n5569         See Also\n5570         --------\n5571         core.rolling.DataArrayRolling\n5572         Dataset.rolling\n5573         \"\"\"\n5574         from .rolling import DataArrayRolling\n5575 \n5576         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n5577         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)\n5578 \n5579     def coarsen(\n5580         self,\n5581         dim: Mapping[Any, int] | None = None,\n5582         boundary: CoarsenBoundaryOptions = \"exact\",\n5583         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n5584         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n5585         **window_kwargs: int,\n5586     ) -> DataArrayCoarsen:\n5587         \"\"\"\n5588         Coarsen object for DataArrays.\n5589 \n5590         Parameters\n5591         ----------\n5592         dim : mapping of hashable to int, optional\n5593             Mapping from the dimension name to the window size.\n5594         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n5595             If 'exact', a ValueError will be raised if dimension size is not a\n5596             multiple of the window size. If 'trim', the excess entries are\n5597             dropped. If 'pad', NA will be padded.\n5598         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n5599         coord_func : str or mapping of hashable to str, default: \"mean\"\n5600             function (name) that is applied to the coordinates,\n5601             or a mapping from coordinate name to function (name).\n5602 \n5603         Returns\n5604         -------\n5605         core.rolling.DataArrayCoarsen\n5606 \n5607         Examples\n5608         --------\n5609         Coarsen the long time series by averaging over every four days.\n5610 \n5611         >>> da = xr.DataArray(\n5612         ...     np.linspace(0, 364, num=364),\n5613         ...     dims=\"time\",\n5614         ...     coords={\"time\": pd.date_range(\"1999-12-15\", periods=364)},\n5615         ... )\n5616         >>> da  # +doctest: ELLIPSIS\n5617         <xarray.DataArray (time: 364)>\n5618         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,\n5619                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,\n5620                  8.02203857,   9.02479339,  10.02754821,  11.03030303,\n5621         ...\n5622                356.98071625, 357.98347107, 358.9862259 , 359.98898072,\n5623                360.99173554, 361.99449036, 362.99724518, 364.        ])\n5624         Coordinates:\n5625           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n5626         >>> da.coarsen(time=3, boundary=\"trim\").mean()  # +doctest: ELLIPSIS\n5627         <xarray.DataArray (time: 121)>\n5628         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,\n5629                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,\n5630                 25.06887052,  28.07713499,  31.08539945,  34.09366391,\n5631         ...\n5632                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,\n5633                361.99449036])\n5634         Coordinates:\n5635           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n5636         >>>\n5637 \n5638         See Also\n5639         --------\n5640         core.rolling.DataArrayCoarsen\n5641         Dataset.coarsen\n5642         \"\"\"\n5643         from .rolling import DataArrayCoarsen\n5644 \n5645         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n5646         return DataArrayCoarsen(\n5647             self,\n5648             dim,\n5649             boundary=boundary,\n5650             side=side,\n5651             coord_func=coord_func,\n5652         )\n5653 \n5654     def resample(\n5655         self,\n5656         indexer: Mapping[Any, str] | None = None,\n5657         skipna: bool | None = None,\n5658         closed: SideOptions | None = None,\n5659         label: SideOptions | None = None,\n5660         base: int = 0,\n5661         keep_attrs: bool | None = None,\n5662         loffset: datetime.timedelta | str | None = None,\n5663         restore_coord_dims: bool | None = None,\n5664         **indexer_kwargs: str,\n5665     ) -> DataArrayResample:\n5666         \"\"\"Returns a Resample object for performing resampling operations.\n5667 \n5668         Handles both downsampling and upsampling. The resampled\n5669         dimension must be a datetime-like coordinate. If any intervals\n5670         contain no values from the original object, they will be given\n5671         the value ``NaN``.\n5672 \n5673         Parameters\n5674         ----------\n5675         indexer : Mapping of Hashable to str, optional\n5676             Mapping from the dimension name to resample frequency [1]_. The\n5677             dimension must be datetime-like.\n5678         skipna : bool, optional\n5679             Whether to skip missing values when aggregating in downsampling.\n5680         closed : {\"left\", \"right\"}, optional\n5681             Side of each interval to treat as closed.\n5682         label : {\"left\", \"right\"}, optional\n5683             Side of each interval to use for labeling.\n5684         base : int, default = 0\n5685             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n5686             aggregated intervals. For example, for \"24H\" frequency, base could\n5687             range from 0 through 23.\n5688         loffset : timedelta or str, optional\n5689             Offset used to adjust the resampled time labels. Some pandas date\n5690             offset strings are supported.\n5691         restore_coord_dims : bool, optional\n5692             If True, also restore the dimension order of multi-dimensional\n5693             coordinates.\n5694         **indexer_kwargs : str\n5695             The keyword arguments form of ``indexer``.\n5696             One of indexer or indexer_kwargs must be provided.\n5697 \n5698         Returns\n5699         -------\n5700         resampled : core.resample.DataArrayResample\n5701             This object resampled.\n5702 \n5703         Examples\n5704         --------\n5705         Downsample monthly time-series data to seasonal data:\n5706 \n5707         >>> da = xr.DataArray(\n5708         ...     np.linspace(0, 11, num=12),\n5709         ...     coords=[\n5710         ...         pd.date_range(\n5711         ...             \"1999-12-15\",\n5712         ...             periods=12,\n5713         ...             freq=pd.DateOffset(months=1),\n5714         ...         )\n5715         ...     ],\n5716         ...     dims=\"time\",\n5717         ... )\n5718         >>> da\n5719         <xarray.DataArray (time: 12)>\n5720         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n5721         Coordinates:\n5722           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n5723         >>> da.resample(time=\"QS-DEC\").mean()\n5724         <xarray.DataArray (time: 4)>\n5725         array([ 1.,  4.,  7., 10.])\n5726         Coordinates:\n5727           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n5728 \n5729         Upsample monthly time-series data to daily data:\n5730 \n5731         >>> da.resample(time=\"1D\").interpolate(\"linear\")  # +doctest: ELLIPSIS\n5732         <xarray.DataArray (time: 337)>\n5733         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,\n5734                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,\n5735                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,\n5736         ...\n5737                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,\n5738                10.96774194, 11.        ])\n5739         Coordinates:\n5740           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n5741 \n5742         Limit scope of upsampling method\n5743 \n5744         >>> da.resample(time=\"1D\").nearest(tolerance=\"1D\")\n5745         <xarray.DataArray (time: 337)>\n5746         array([ 0.,  0., nan, ..., nan, 11., 11.])\n5747         Coordinates:\n5748           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n5749 \n5750         See Also\n5751         --------\n5752         Dataset.resample\n5753         pandas.Series.resample\n5754         pandas.DataFrame.resample\n5755 \n5756         References\n5757         ----------\n5758         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n5759         \"\"\"\n5760         from .resample import DataArrayResample\n5761 \n5762         return self._resample(\n5763             resample_cls=DataArrayResample,\n5764             indexer=indexer,\n5765             skipna=skipna,\n5766             closed=closed,\n5767             label=label,\n5768             base=base,\n5769             keep_attrs=keep_attrs,\n5770             loffset=loffset,\n5771             restore_coord_dims=restore_coord_dims,\n5772             **indexer_kwargs,\n5773         )\n5774 \n5775     # this needs to be at the end, or mypy will confuse with `str`\n5776     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n5777     str = utils.UncachedAccessor(StringAccessor[\"DataArray\"])\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataset.py",
        "start": 3651,
        "end": 3687,
        "class_name": "Dataset",
        "method_name": "rename_vars",
        "code": "3651     def rename_vars(\n3652         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable\n3653     ) -> T_Dataset:\n3654         \"\"\"Returns a new object with renamed variables including coordinates\n3655 \n3656         Parameters\n3657         ----------\n3658         name_dict : dict-like, optional\n3659             Dictionary whose keys are current variable or coordinate names and\n3660             whose values are the desired names.\n3661         **names : optional\n3662             Keyword form of ``name_dict``.\n3663             One of name_dict or names must be provided.\n3664 \n3665         Returns\n3666         -------\n3667         renamed : Dataset\n3668             Dataset with renamed variables including coordinates\n3669 \n3670         See Also\n3671         --------\n3672         Dataset.swap_dims\n3673         Dataset.rename\n3674         Dataset.rename_dims\n3675         DataArray.rename\n3676         \"\"\"\n3677         name_dict = either_dict_or_kwargs(name_dict, names, \"rename_vars\")\n3678         for k in name_dict:\n3679             if k not in self:\n3680                 raise ValueError(\n3681                     f\"cannot rename {k!r} because it is not a \"\n3682                     \"variable or coordinate in this dataset\"\n3683                 )\n3684         variables, coord_names, dims, indexes = self._rename_all(\n3685             name_dict=name_dict, dims_dict={}\n3686         )\n3687         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n",
        "intended_behavior": "Similar to `DataArray.rename`, `rename_vars` should handle the renaming of variables to dimension coordinates by ensuring that any such new dimension coordinate has an associated index. This could be achieved by automatic index creation or by warning/error pointing users towards `set_index`. The solution implemented should preserve the method's original functionality while enhancing it to cover the use case presented by the issue."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataset.py",
        "start": 430,
        "end": 8890,
        "class_name": "Dataset",
        "method_name": null,
        "code": "430 class Dataset(\n431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, \"DataArray\"]\n432 ):\n433     \"\"\"A multi-dimensional, in memory, array database.\n434 \n435     A dataset resembles an in-memory representation of a NetCDF file,\n436     and consists of variables, coordinates and attributes which\n437     together form a self describing dataset.\n438 \n439     Dataset implements the mapping interface with keys given by variable\n440     names and values given by DataArray objects for each variable name.\n441 \n442     One dimensional variables with name equal to their dimension are\n443     index coordinates used for label based indexing.\n444 \n445     To load data from a file or file-like object, use the `open_dataset`\n446     function.\n447 \n448     Parameters\n449     ----------\n450     data_vars : dict-like, optional\n451         A mapping from variable names to :py:class:`~xarray.DataArray`\n452         objects, :py:class:`~xarray.Variable` objects or to tuples of\n453         the form ``(dims, data[, attrs])`` which can be used as\n454         arguments to create a new ``Variable``. Each dimension must\n455         have the same length in all variables in which it appears.\n456 \n457         The following notations are accepted:\n458 \n459         - mapping {var name: DataArray}\n460         - mapping {var name: Variable}\n461         - mapping {var name: (dimension name, array-like)}\n462         - mapping {var name: (tuple of dimension names, array-like)}\n463         - mapping {dimension name: array-like}\n464           (it will be automatically moved to coords, see below)\n465 \n466         Each dimension must have the same length in all variables in\n467         which it appears.\n468     coords : dict-like, optional\n469         Another mapping in similar form as the `data_vars` argument,\n470         except the each item is saved on the dataset as a \"coordinate\".\n471         These variables have an associated meaning: they describe\n472         constant/fixed/independent quantities, unlike the\n473         varying/measured/dependent quantities that belong in\n474         `variables`. Coordinates values may be given by 1-dimensional\n475         arrays or scalars, in which case `dims` do not need to be\n476         supplied: 1D arrays will be assumed to give index values along\n477         the dimension with the same name.\n478 \n479         The following notations are accepted:\n480 \n481         - mapping {coord name: DataArray}\n482         - mapping {coord name: Variable}\n483         - mapping {coord name: (dimension name, array-like)}\n484         - mapping {coord name: (tuple of dimension names, array-like)}\n485         - mapping {dimension name: array-like}\n486           (the dimension name is implicitly set to be the same as the\n487           coord name)\n488 \n489         The last notation implies that the coord name is the same as\n490         the dimension name.\n491 \n492     attrs : dict-like, optional\n493         Global attributes to save on this dataset.\n494 \n495     Examples\n496     --------\n497     Create data:\n498 \n499     >>> np.random.seed(0)\n500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n501     >>> precipitation = 10 * np.random.rand(2, 2, 3)\n502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n504     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n505     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n506 \n507     Initialize a dataset with multiple dimensions:\n508 \n509     >>> ds = xr.Dataset(\n510     ...     data_vars=dict(\n511     ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n512     ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n513     ...     ),\n514     ...     coords=dict(\n515     ...         lon=([\"x\", \"y\"], lon),\n516     ...         lat=([\"x\", \"y\"], lat),\n517     ...         time=time,\n518     ...         reference_time=reference_time,\n519     ...     ),\n520     ...     attrs=dict(description=\"Weather related data.\"),\n521     ... )\n522     >>> ds\n523     <xarray.Dataset>\n524     Dimensions:         (x: 2, y: 2, time: 3)\n525     Coordinates:\n526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n527         lat             (x, y) float64 42.25 42.21 42.63 42.59\n528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n529         reference_time  datetime64[ns] 2014-09-05\n530     Dimensions without coordinates: x, y\n531     Data variables:\n532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n534     Attributes:\n535         description:  Weather related data.\n536 \n537     Find out where the coldest temperature was and what values the\n538     other variables had:\n539 \n540     >>> ds.isel(ds.temperature.argmin(...))\n541     <xarray.Dataset>\n542     Dimensions:         ()\n543     Coordinates:\n544         lon             float64 -99.32\n545         lat             float64 42.21\n546         time            datetime64[ns] 2014-09-08\n547         reference_time  datetime64[ns] 2014-09-05\n548     Data variables:\n549         temperature     float64 7.182\n550         precipitation   float64 8.326\n551     Attributes:\n552         description:  Weather related data.\n553     \"\"\"\n554 \n555     _attrs: dict[Hashable, Any] | None\n556     _cache: dict[str, Any]\n557     _coord_names: set[Hashable]\n558     _dims: dict[Hashable, int]\n559     _encoding: dict[Hashable, Any] | None\n560     _close: Callable[[], None] | None\n561     _indexes: dict[Hashable, Index]\n562     _variables: dict[Hashable, Variable]\n563 \n564     __slots__ = (\n565         \"_attrs\",\n566         \"_cache\",\n567         \"_coord_names\",\n568         \"_dims\",\n569         \"_encoding\",\n570         \"_close\",\n571         \"_indexes\",\n572         \"_variables\",\n573         \"__weakref__\",\n574     )\n575 \n576     def __init__(\n577         self,\n578         # could make a VariableArgs to use more generally, and refine these\n579         # categories\n580         data_vars: Mapping[Any, Any] | None = None,\n581         coords: Mapping[Any, Any] | None = None,\n582         attrs: Mapping[Any, Any] | None = None,\n583     ) -> None:\n584         # TODO(shoyer): expose indexes as a public argument in __init__\n585 \n586         if data_vars is None:\n587             data_vars = {}\n588         if coords is None:\n589             coords = {}\n590 \n591         both_data_and_coords = set(data_vars) & set(coords)\n592         if both_data_and_coords:\n593             raise ValueError(\n594                 f\"variables {both_data_and_coords!r} are found in both data_vars and coords\"\n595             )\n596 \n597         if isinstance(coords, Dataset):\n598             coords = coords.variables\n599 \n600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n601             data_vars, coords, compat=\"broadcast_equals\"\n602         )\n603 \n604         self._attrs = dict(attrs) if attrs is not None else None\n605         self._close = None\n606         self._encoding = None\n607         self._variables = variables\n608         self._coord_names = coord_names\n609         self._dims = dims\n610         self._indexes = indexes\n611 \n612     @classmethod\n613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:\n614         \"\"\"Create a new dataset from the contents of a backends.*DataStore\n615         object\n616         \"\"\"\n617         variables, attributes = store.load()\n618         if decoder:\n619             variables, attributes = decoder(variables, attributes)\n620         obj = cls(variables, attrs=attributes)\n621         obj.set_close(store.close)\n622         return obj\n623 \n624     @property\n625     def variables(self) -> Frozen[Hashable, Variable]:\n626         \"\"\"Low level interface to Dataset contents as dict of Variable objects.\n627 \n628         This ordered dictionary is frozen to prevent mutation that could\n629         violate Dataset invariants. It contains all variable objects\n630         constituting the Dataset, including both data variables and\n631         coordinates.\n632         \"\"\"\n633         return Frozen(self._variables)\n634 \n635     @property\n636     def attrs(self) -> dict[Hashable, Any]:\n637         \"\"\"Dictionary of global attributes on this dataset\"\"\"\n638         if self._attrs is None:\n639             self._attrs = {}\n640         return self._attrs\n641 \n642     @attrs.setter\n643     def attrs(self, value: Mapping[Any, Any]) -> None:\n644         self._attrs = dict(value)\n645 \n646     @property\n647     def encoding(self) -> dict[Hashable, Any]:\n648         \"\"\"Dictionary of global encoding attributes on this dataset\"\"\"\n649         if self._encoding is None:\n650             self._encoding = {}\n651         return self._encoding\n652 \n653     @encoding.setter\n654     def encoding(self, value: Mapping[Any, Any]) -> None:\n655         self._encoding = dict(value)\n656 \n657     @property\n658     def dims(self) -> Frozen[Hashable, int]:\n659         \"\"\"Mapping from dimension names to lengths.\n660 \n661         Cannot be modified directly, but is updated when adding new variables.\n662 \n663         Note that type of this object differs from `DataArray.dims`.\n664         See `Dataset.sizes` and `DataArray.sizes` for consistently named\n665         properties.\n666 \n667         See Also\n668         --------\n669         Dataset.sizes\n670         DataArray.dims\n671         \"\"\"\n672         return Frozen(self._dims)\n673 \n674     @property\n675     def sizes(self) -> Frozen[Hashable, int]:\n676         \"\"\"Mapping from dimension names to lengths.\n677 \n678         Cannot be modified directly, but is updated when adding new variables.\n679 \n680         This is an alias for `Dataset.dims` provided for the benefit of\n681         consistency with `DataArray.sizes`.\n682 \n683         See Also\n684         --------\n685         DataArray.sizes\n686         \"\"\"\n687         return self.dims\n688 \n689     @property\n690     def dtypes(self) -> Frozen[Hashable, np.dtype]:\n691         \"\"\"Mapping from data variable names to dtypes.\n692 \n693         Cannot be modified directly, but is updated when adding new variables.\n694 \n695         See Also\n696         --------\n697         DataArray.dtype\n698         \"\"\"\n699         return Frozen(\n700             {\n701                 n: v.dtype\n702                 for n, v in self._variables.items()\n703                 if n not in self._coord_names\n704             }\n705         )\n706 \n707     def load(self: T_Dataset, **kwargs) -> T_Dataset:\n708         \"\"\"Manually trigger loading and/or computation of this dataset's data\n709         from disk or a remote source into memory and return this dataset.\n710         Unlike compute, the original dataset is modified and returned.\n711 \n712         Normally, it should not be necessary to call this method in user code,\n713         because all xarray functions should either work on deferred data or\n714         load data automatically. However, this method can be necessary when\n715         working with many file objects on disk.\n716 \n717         Parameters\n718         ----------\n719         **kwargs : dict\n720             Additional keyword arguments passed on to ``dask.compute``.\n721 \n722         See Also\n723         --------\n724         dask.compute\n725         \"\"\"\n726         # access .data to coerce everything to numpy or dask arrays\n727         lazy_data = {\n728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n729         }\n730         if lazy_data:\n731             import dask.array as da\n732 \n733             # evaluate all the dask arrays simultaneously\n734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n735 \n736             for k, data in zip(lazy_data, evaluated_data):\n737                 self.variables[k].data = data\n738 \n739         # load everything else sequentially\n740         for k, v in self.variables.items():\n741             if k not in lazy_data:\n742                 v.load()\n743 \n744         return self\n745 \n746     def __dask_tokenize__(self):\n747         from dask.base import normalize_token\n748 \n749         return normalize_token(\n750             (type(self), self._variables, self._coord_names, self._attrs)\n751         )\n752 \n753     def __dask_graph__(self):\n754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}\n755         graphs = {k: v for k, v in graphs.items() if v is not None}\n756         if not graphs:\n757             return None\n758         else:\n759             try:\n760                 from dask.highlevelgraph import HighLevelGraph\n761 \n762                 return HighLevelGraph.merge(*graphs.values())\n763             except ImportError:\n764                 from dask import sharedict\n765 \n766                 return sharedict.merge(*graphs.values())\n767 \n768     def __dask_keys__(self):\n769         import dask\n770 \n771         return [\n772             v.__dask_keys__()\n773             for v in self.variables.values()\n774             if dask.is_dask_collection(v)\n775         ]\n776 \n777     def __dask_layers__(self):\n778         import dask\n779 \n780         return sum(\n781             (\n782                 v.__dask_layers__()\n783                 for v in self.variables.values()\n784                 if dask.is_dask_collection(v)\n785             ),\n786             (),\n787         )\n788 \n789     @property\n790     def __dask_optimize__(self):\n791         import dask.array as da\n792 \n793         return da.Array.__dask_optimize__\n794 \n795     @property\n796     def __dask_scheduler__(self):\n797         import dask.array as da\n798 \n799         return da.Array.__dask_scheduler__\n800 \n801     def __dask_postcompute__(self):\n802         return self._dask_postcompute, ()\n803 \n804     def __dask_postpersist__(self):\n805         return self._dask_postpersist, ()\n806 \n807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:\n808         import dask\n809 \n810         variables = {}\n811         results_iter = iter(results)\n812 \n813         for k, v in self._variables.items():\n814             if dask.is_dask_collection(v):\n815                 rebuild, args = v.__dask_postcompute__()\n816                 v = rebuild(next(results_iter), *args)\n817             variables[k] = v\n818 \n819         return type(self)._construct_direct(\n820             variables,\n821             self._coord_names,\n822             self._dims,\n823             self._attrs,\n824             self._indexes,\n825             self._encoding,\n826             self._close,\n827         )\n828 \n829     def _dask_postpersist(\n830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None\n831     ) -> T_Dataset:\n832         from dask import is_dask_collection\n833         from dask.highlevelgraph import HighLevelGraph\n834         from dask.optimization import cull\n835 \n836         variables = {}\n837 \n838         for k, v in self._variables.items():\n839             if not is_dask_collection(v):\n840                 variables[k] = v\n841                 continue\n842 \n843             if isinstance(dsk, HighLevelGraph):\n844                 # dask >= 2021.3\n845                 # __dask_postpersist__() was called by dask.highlevelgraph.\n846                 # Don't use dsk.cull(), as we need to prevent partial layers:\n847                 # https://github.com/dask/dask/issues/7137\n848                 layers = v.__dask_layers__()\n849                 if rename:\n850                     layers = [rename.get(k, k) for k in layers]\n851                 dsk2 = dsk.cull_layers(layers)\n852             elif rename:  # pragma: nocover\n853                 # At the moment of writing, this is only for forward compatibility.\n854                 # replace_name_in_key requires dask >= 2021.3.\n855                 from dask.base import flatten, replace_name_in_key\n856 \n857                 keys = [\n858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())\n859                 ]\n860                 dsk2, _ = cull(dsk, keys)\n861             else:\n862                 # __dask_postpersist__() was called by dask.optimize or dask.persist\n863                 dsk2, _ = cull(dsk, v.__dask_keys__())\n864 \n865             rebuild, args = v.__dask_postpersist__()\n866             # rename was added in dask 2021.3\n867             kwargs = {\"rename\": rename} if rename else {}\n868             variables[k] = rebuild(dsk2, *args, **kwargs)\n869 \n870         return type(self)._construct_direct(\n871             variables,\n872             self._coord_names,\n873             self._dims,\n874             self._attrs,\n875             self._indexes,\n876             self._encoding,\n877             self._close,\n878         )\n879 \n880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:\n881         \"\"\"Manually trigger loading and/or computation of this dataset's data\n882         from disk or a remote source into memory and return a new dataset.\n883         Unlike load, the original dataset is left unaltered.\n884 \n885         Normally, it should not be necessary to call this method in user code,\n886         because all xarray functions should either work on deferred data or\n887         load data automatically. However, this method can be necessary when\n888         working with many file objects on disk.\n889 \n890         Parameters\n891         ----------\n892         **kwargs : dict\n893             Additional keyword arguments passed on to ``dask.compute``.\n894 \n895         See Also\n896         --------\n897         dask.compute\n898         \"\"\"\n899         new = self.copy(deep=False)\n900         return new.load(**kwargs)\n901 \n902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:\n903         \"\"\"Persist all Dask arrays in memory\"\"\"\n904         # access .data to coerce everything to numpy or dask arrays\n905         lazy_data = {\n906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n907         }\n908         if lazy_data:\n909             import dask\n910 \n911             # evaluate all the dask arrays simultaneously\n912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n913 \n914             for k, data in zip(lazy_data, evaluated_data):\n915                 self.variables[k].data = data\n916 \n917         return self\n918 \n919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:\n920         \"\"\"Trigger computation, keeping data as dask arrays\n921 \n922         This operation can be used to trigger computation on underlying dask\n923         arrays, similar to ``.compute()`` or ``.load()``.  However this\n924         operation keeps the data as dask arrays. This is particularly useful\n925         when using the dask.distributed scheduler and you want to load a large\n926         amount of data into distributed memory.\n927 \n928         Parameters\n929         ----------\n930         **kwargs : dict\n931             Additional keyword arguments passed on to ``dask.persist``.\n932 \n933         See Also\n934         --------\n935         dask.persist\n936         \"\"\"\n937         new = self.copy(deep=False)\n938         return new._persist_inplace(**kwargs)\n939 \n940     @classmethod\n941     def _construct_direct(\n942         cls: type[T_Dataset],\n943         variables: dict[Any, Variable],\n944         coord_names: set[Hashable],\n945         dims: dict[Any, int] | None = None,\n946         attrs: dict | None = None,\n947         indexes: dict[Any, Index] | None = None,\n948         encoding: dict | None = None,\n949         close: Callable[[], None] | None = None,\n950     ) -> T_Dataset:\n951         \"\"\"Shortcut around __init__ for internal use when we want to skip\n952         costly validation\n953         \"\"\"\n954         if dims is None:\n955             dims = calculate_dimensions(variables)\n956         if indexes is None:\n957             indexes = {}\n958         obj = object.__new__(cls)\n959         obj._variables = variables\n960         obj._coord_names = coord_names\n961         obj._dims = dims\n962         obj._indexes = indexes\n963         obj._attrs = attrs\n964         obj._close = close\n965         obj._encoding = encoding\n966         return obj\n967 \n968     def _replace(\n969         self: T_Dataset,\n970         variables: dict[Hashable, Variable] = None,\n971         coord_names: set[Hashable] | None = None,\n972         dims: dict[Any, int] | None = None,\n973         attrs: dict[Hashable, Any] | None | Default = _default,\n974         indexes: dict[Hashable, Index] | None = None,\n975         encoding: dict | None | Default = _default,\n976         inplace: bool = False,\n977     ) -> T_Dataset:\n978         \"\"\"Fastpath constructor for internal use.\n979 \n980         Returns an object with optionally with replaced attributes.\n981 \n982         Explicitly passed arguments are *not* copied when placed on the new\n983         dataset. It is up to the caller to ensure that they have the right type\n984         and are not used elsewhere.\n985         \"\"\"\n986         if inplace:\n987             if variables is not None:\n988                 self._variables = variables\n989             if coord_names is not None:\n990                 self._coord_names = coord_names\n991             if dims is not None:\n992                 self._dims = dims\n993             if attrs is not _default:\n994                 self._attrs = attrs\n995             if indexes is not None:\n996                 self._indexes = indexes\n997             if encoding is not _default:\n998                 self._encoding = encoding\n999             obj = self\n1000         else:\n1001             if variables is None:\n1002                 variables = self._variables.copy()\n1003             if coord_names is None:\n1004                 coord_names = self._coord_names.copy()\n1005             if dims is None:\n1006                 dims = self._dims.copy()\n1007             if attrs is _default:\n1008                 attrs = copy.copy(self._attrs)\n1009             if indexes is None:\n1010                 indexes = self._indexes.copy()\n1011             if encoding is _default:\n1012                 encoding = copy.copy(self._encoding)\n1013             obj = self._construct_direct(\n1014                 variables, coord_names, dims, attrs, indexes, encoding\n1015             )\n1016         return obj\n1017 \n1018     def _replace_with_new_dims(\n1019         self: T_Dataset,\n1020         variables: dict[Hashable, Variable],\n1021         coord_names: set | None = None,\n1022         attrs: dict[Hashable, Any] | None | Default = _default,\n1023         indexes: dict[Hashable, Index] | None = None,\n1024         inplace: bool = False,\n1025     ) -> T_Dataset:\n1026         \"\"\"Replace variables with recalculated dimensions.\"\"\"\n1027         dims = calculate_dimensions(variables)\n1028         return self._replace(\n1029             variables, coord_names, dims, attrs, indexes, inplace=inplace\n1030         )\n1031 \n1032     def _replace_vars_and_dims(\n1033         self: T_Dataset,\n1034         variables: dict[Hashable, Variable],\n1035         coord_names: set | None = None,\n1036         dims: dict[Hashable, int] | None = None,\n1037         attrs: dict[Hashable, Any] | None | Default = _default,\n1038         inplace: bool = False,\n1039     ) -> T_Dataset:\n1040         \"\"\"Deprecated version of _replace_with_new_dims().\n1041 \n1042         Unlike _replace_with_new_dims(), this method always recalculates\n1043         indexes from variables.\n1044         \"\"\"\n1045         if dims is None:\n1046             dims = calculate_dimensions(variables)\n1047         return self._replace(\n1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace\n1049         )\n1050 \n1051     def _overwrite_indexes(\n1052         self: T_Dataset,\n1053         indexes: Mapping[Hashable, Index],\n1054         variables: Mapping[Hashable, Variable] | None = None,\n1055         drop_variables: list[Hashable] | None = None,\n1056         drop_indexes: list[Hashable] | None = None,\n1057         rename_dims: Mapping[Hashable, Hashable] | None = None,\n1058     ) -> T_Dataset:\n1059         \"\"\"Maybe replace indexes.\n1060 \n1061         This function may do a lot more depending on index query\n1062         results.\n1063 \n1064         \"\"\"\n1065         if not indexes:\n1066             return self\n1067 \n1068         if variables is None:\n1069             variables = {}\n1070         if drop_variables is None:\n1071             drop_variables = []\n1072         if drop_indexes is None:\n1073             drop_indexes = []\n1074 \n1075         new_variables = self._variables.copy()\n1076         new_coord_names = self._coord_names.copy()\n1077         new_indexes = dict(self._indexes)\n1078 \n1079         index_variables = {}\n1080         no_index_variables = {}\n1081         for name, var in variables.items():\n1082             old_var = self._variables.get(name)\n1083             if old_var is not None:\n1084                 var.attrs.update(old_var.attrs)\n1085                 var.encoding.update(old_var.encoding)\n1086             if name in indexes:\n1087                 index_variables[name] = var\n1088             else:\n1089                 no_index_variables[name] = var\n1090 \n1091         for name in indexes:\n1092             new_indexes[name] = indexes[name]\n1093 \n1094         for name, var in index_variables.items():\n1095             new_coord_names.add(name)\n1096             new_variables[name] = var\n1097 \n1098         # append no-index variables at the end\n1099         for k in no_index_variables:\n1100             new_variables.pop(k)\n1101         new_variables.update(no_index_variables)\n1102 \n1103         for name in drop_indexes:\n1104             new_indexes.pop(name)\n1105 \n1106         for name in drop_variables:\n1107             new_variables.pop(name)\n1108             new_indexes.pop(name, None)\n1109             new_coord_names.remove(name)\n1110 \n1111         replaced = self._replace(\n1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes\n1113         )\n1114 \n1115         if rename_dims:\n1116             # skip rename indexes: they should already have the right name(s)\n1117             dims = replaced._rename_dims(rename_dims)\n1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)\n1119             return replaced._replace(\n1120                 variables=new_variables, coord_names=new_coord_names, dims=dims\n1121             )\n1122         else:\n1123             return replaced\n1124 \n1125     def copy(\n1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None\n1127     ) -> T_Dataset:\n1128         \"\"\"Returns a copy of this dataset.\n1129 \n1130         If `deep=True`, a deep copy is made of each of the component variables.\n1131         Otherwise, a shallow copy of each of the component variable is made, so\n1132         that the underlying memory region of the new dataset is the same as in\n1133         the original dataset.\n1134 \n1135         Use `data` to create a new object with the same structure as\n1136         original but entirely new data.\n1137 \n1138         Parameters\n1139         ----------\n1140         deep : bool, default: False\n1141             Whether each component variable is loaded into memory and copied onto\n1142             the new object. Default is False.\n1143         data : dict-like or None, optional\n1144             Data to use in the new object. Each item in `data` must have same\n1145             shape as corresponding data variable in original. When `data` is\n1146             used, `deep` is ignored for the data variables and only used for\n1147             coords.\n1148 \n1149         Returns\n1150         -------\n1151         object : Dataset\n1152             New object with dimensions, attributes, coordinates, name, encoding,\n1153             and optionally data copied from original.\n1154 \n1155         Examples\n1156         --------\n1157         Shallow copy versus deep copy\n1158 \n1159         >>> da = xr.DataArray(np.random.randn(2, 3))\n1160         >>> ds = xr.Dataset(\n1161         ...     {\"foo\": da, \"bar\": (\"x\", [-1, 2])},\n1162         ...     coords={\"x\": [\"one\", \"two\"]},\n1163         ... )\n1164         >>> ds.copy()\n1165         <xarray.Dataset>\n1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1167         Coordinates:\n1168           * x        (x) <U3 'one' 'two'\n1169         Dimensions without coordinates: dim_0, dim_1\n1170         Data variables:\n1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n1172             bar      (x) int64 -1 2\n1173 \n1174         >>> ds_0 = ds.copy(deep=False)\n1175         >>> ds_0[\"foo\"][0, 0] = 7\n1176         >>> ds_0\n1177         <xarray.Dataset>\n1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1179         Coordinates:\n1180           * x        (x) <U3 'one' 'two'\n1181         Dimensions without coordinates: dim_0, dim_1\n1182         Data variables:\n1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1184             bar      (x) int64 -1 2\n1185 \n1186         >>> ds\n1187         <xarray.Dataset>\n1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1189         Coordinates:\n1190           * x        (x) <U3 'one' 'two'\n1191         Dimensions without coordinates: dim_0, dim_1\n1192         Data variables:\n1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1194             bar      (x) int64 -1 2\n1195 \n1196         Changing the data using the ``data`` argument maintains the\n1197         structure of the original object, but with the new data. Original\n1198         object is unaffected.\n1199 \n1200         >>> ds.copy(data={\"foo\": np.arange(6).reshape(2, 3), \"bar\": [\"a\", \"b\"]})\n1201         <xarray.Dataset>\n1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1203         Coordinates:\n1204           * x        (x) <U3 'one' 'two'\n1205         Dimensions without coordinates: dim_0, dim_1\n1206         Data variables:\n1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n1208             bar      (x) <U1 'a' 'b'\n1209 \n1210         >>> ds\n1211         <xarray.Dataset>\n1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1213         Coordinates:\n1214           * x        (x) <U3 'one' 'two'\n1215         Dimensions without coordinates: dim_0, dim_1\n1216         Data variables:\n1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1218             bar      (x) int64 -1 2\n1219 \n1220         See Also\n1221         --------\n1222         pandas.DataFrame.copy\n1223         \"\"\"\n1224         if data is None:\n1225             data = {}\n1226         elif not utils.is_dict_like(data):\n1227             raise ValueError(\"Data must be dict-like\")\n1228 \n1229         if data:\n1230             var_keys = set(self.data_vars.keys())\n1231             data_keys = set(data.keys())\n1232             keys_not_in_vars = data_keys - var_keys\n1233             if keys_not_in_vars:\n1234                 raise ValueError(\n1235                     \"Data must only contain variables in original \"\n1236                     \"dataset. Extra variables: {}\".format(keys_not_in_vars)\n1237                 )\n1238             keys_missing_from_data = var_keys - data_keys\n1239             if keys_missing_from_data:\n1240                 raise ValueError(\n1241                     \"Data must contain all variables in original \"\n1242                     \"dataset. Data is missing {}\".format(keys_missing_from_data)\n1243                 )\n1244 \n1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1246 \n1247         variables = {}\n1248         for k, v in self._variables.items():\n1249             if k in index_vars:\n1250                 variables[k] = index_vars[k]\n1251             else:\n1252                 variables[k] = v.copy(deep=deep, data=data.get(k))\n1253 \n1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n1255 \n1256         return self._replace(variables, indexes=indexes, attrs=attrs)\n1257 \n1258     def as_numpy(self: T_Dataset) -> T_Dataset:\n1259         \"\"\"\n1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.\n1261 \n1262         See also\n1263         --------\n1264         DataArray.as_numpy\n1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.\n1266         \"\"\"\n1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}\n1268         return self._replace(variables=numpy_variables)\n1269 \n1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:\n1271         \"\"\"Create a new Dataset with the listed variables from this dataset and\n1272         the all relevant coordinates. Skips all validation.\n1273         \"\"\"\n1274         variables: dict[Hashable, Variable] = {}\n1275         coord_names = set()\n1276         indexes: dict[Hashable, Index] = {}\n1277 \n1278         for name in names:\n1279             try:\n1280                 variables[name] = self._variables[name]\n1281             except KeyError:\n1282                 ref_name, var_name, var = _get_virtual_variable(\n1283                     self._variables, name, self.dims\n1284                 )\n1285                 variables[var_name] = var\n1286                 if ref_name in self._coord_names or ref_name in self.dims:\n1287                     coord_names.add(var_name)\n1288                 if (var_name,) == var.dims:\n1289                     index, index_vars = create_default_index_implicit(var, names)\n1290                     indexes.update({k: index for k in index_vars})\n1291                     variables.update(index_vars)\n1292                     coord_names.update(index_vars)\n1293 \n1294         needed_dims: OrderedSet[Hashable] = OrderedSet()\n1295         for v in variables.values():\n1296             needed_dims.update(v.dims)\n1297 \n1298         dims = {k: self.dims[k] for k in needed_dims}\n1299 \n1300         # preserves ordering of coordinates\n1301         for k in self._variables:\n1302             if k not in self._coord_names:\n1303                 continue\n1304 \n1305             if set(self.variables[k].dims) <= needed_dims:\n1306                 variables[k] = self._variables[k]\n1307                 coord_names.add(k)\n1308 \n1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))\n1310 \n1311         return self._replace(variables, coord_names, dims, indexes=indexes)\n1312 \n1313     def _construct_dataarray(self, name: Hashable) -> DataArray:\n1314         \"\"\"Construct a DataArray by indexing this dataset\"\"\"\n1315         from .dataarray import DataArray\n1316 \n1317         try:\n1318             variable = self._variables[name]\n1319         except KeyError:\n1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)\n1321 \n1322         needed_dims = set(variable.dims)\n1323 \n1324         coords: dict[Hashable, Variable] = {}\n1325         # preserve ordering\n1326         for k in self._variables:\n1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:\n1328                 coords[k] = self.variables[k]\n1329 \n1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))\n1331 \n1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1333 \n1334     def __copy__(self: T_Dataset) -> T_Dataset:\n1335         return self.copy(deep=False)\n1336 \n1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:\n1338         # memo does nothing but is required for compatibility with\n1339         # copy.deepcopy\n1340         return self.copy(deep=True)\n1341 \n1342     @property\n1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1344         \"\"\"Places to look-up items for attribute-style access\"\"\"\n1345         yield from self._item_sources\n1346         yield self.attrs\n1347 \n1348     @property\n1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1350         \"\"\"Places to look-up items for key-completion\"\"\"\n1351         yield self.data_vars\n1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)\n1353 \n1354         # virtual coordinates\n1355         yield HybridMappingProxy(keys=self.dims, mapping=self)\n1356 \n1357     def __contains__(self, key: object) -> bool:\n1358         \"\"\"The 'in' operator will return true or false depending on whether\n1359         'key' is an array in the dataset or not.\n1360         \"\"\"\n1361         return key in self._variables\n1362 \n1363     def __len__(self) -> int:\n1364         return len(self.data_vars)\n1365 \n1366     def __bool__(self) -> bool:\n1367         return bool(self.data_vars)\n1368 \n1369     def __iter__(self) -> Iterator[Hashable]:\n1370         return iter(self.data_vars)\n1371 \n1372     def __array__(self, dtype=None):\n1373         raise TypeError(\n1374             \"cannot directly convert an xarray.Dataset into a \"\n1375             \"numpy array. Instead, create an xarray.DataArray \"\n1376             \"first, either with indexing on the Dataset or by \"\n1377             \"invoking the `to_array()` method.\"\n1378         )\n1379 \n1380     @property\n1381     def nbytes(self) -> int:\n1382         \"\"\"\n1383         Total bytes consumed by the data arrays of all variables in this dataset.\n1384 \n1385         If the backend array for any variable does not include ``nbytes``, estimates\n1386         the total bytes for that array based on the ``size`` and ``dtype``.\n1387         \"\"\"\n1388         return sum(v.nbytes for v in self.variables.values())\n1389 \n1390     @property\n1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:\n1392         \"\"\"Attribute for location based indexing. Only supports __getitem__,\n1393         and only when the key is a dict of the form {dim: labels}.\n1394         \"\"\"\n1395         return _LocIndexer(self)\n1396 \n1397     @overload\n1398     def __getitem__(self, key: Hashable) -> DataArray:\n1399         ...\n1400 \n1401     # Mapping is Iterable\n1402     @overload\n1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:\n1404         ...\n1405 \n1406     def __getitem__(\n1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]\n1408     ) -> T_Dataset | DataArray:\n1409         \"\"\"Access variables or coordinates of this dataset as a\n1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.\n1411 \n1412         Indexing with a list of names will return a new ``Dataset`` object.\n1413         \"\"\"\n1414         if utils.is_dict_like(key):\n1415             return self.isel(**key)\n1416         if utils.hashable(key):\n1417             return self._construct_dataarray(key)\n1418         if utils.iterable_of_hashable(key):\n1419             return self._copy_listed(key)\n1420         raise ValueError(f\"Unsupported key-type {type(key)}\")\n1421 \n1422     def __setitem__(\n1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any\n1424     ) -> None:\n1425         \"\"\"Add an array to this dataset.\n1426         Multiple arrays can be added at the same time, in which case each of\n1427         the following operations is applied to the respective value.\n1428 \n1429         If key is dict-like, update all variables in the dataset\n1430         one by one with the given value at the given location.\n1431         If the given value is also a dataset, select corresponding variables\n1432         in the given value and in the dataset to be changed.\n1433 \n1434         If value is a `\n1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it\n1436         to `key` and merge the contents of the resulting dataset into this\n1437         dataset.\n1438 \n1439         If value is a `Variable` object (or tuple of form\n1440         ``(dims, data[, attrs])``), add it to this dataset as a new\n1441         variable.\n1442         \"\"\"\n1443         from .dataarray import DataArray\n1444 \n1445         if utils.is_dict_like(key):\n1446             # check for consistency and convert value to dataset\n1447             value = self._setitem_check(key, value)\n1448             # loop over dataset variables and set new values\n1449             processed = []\n1450             for name, var in self.items():\n1451                 try:\n1452                     var[key] = value[name]\n1453                     processed.append(name)\n1454                 except Exception as e:\n1455                     if processed:\n1456                         raise RuntimeError(\n1457                             \"An error occurred while setting values of the\"\n1458                             f\" variable '{name}'. The following variables have\"\n1459                             f\" been successfully updated:\\n{processed}\"\n1460                         ) from e\n1461                     else:\n1462                         raise e\n1463 \n1464         elif utils.hashable(key):\n1465             if isinstance(value, Dataset):\n1466                 raise TypeError(\n1467                     \"Cannot assign a Dataset to a single key - only a DataArray or Variable \"\n1468                     \"object can be stored under a single key.\"\n1469                 )\n1470             self.update({key: value})\n1471 \n1472         elif utils.iterable_of_hashable(key):\n1473             keylist = list(key)\n1474             if len(keylist) == 0:\n1475                 raise ValueError(\"Empty list of variables to be set\")\n1476             if len(keylist) == 1:\n1477                 self.update({keylist[0]: value})\n1478             else:\n1479                 if len(keylist) != len(value):\n1480                     raise ValueError(\n1481                         f\"Different lengths of variables to be set \"\n1482                         f\"({len(keylist)}) and data used as input for \"\n1483                         f\"setting ({len(value)})\"\n1484                     )\n1485                 if isinstance(value, Dataset):\n1486                     self.update(dict(zip(keylist, value.data_vars.values())))\n1487                 elif isinstance(value, DataArray):\n1488                     raise ValueError(\"Cannot assign single DataArray to multiple keys\")\n1489                 else:\n1490                     self.update(dict(zip(keylist, value)))\n1491 \n1492         else:\n1493             raise ValueError(f\"Unsupported key-type {type(key)}\")\n1494 \n1495     def _setitem_check(self, key, value):\n1496         \"\"\"Consistency check for __setitem__\n1497 \n1498         When assigning values to a subset of a Dataset, do consistency check beforehand\n1499         to avoid leaving the dataset in a partially updated state when an error occurs.\n1500         \"\"\"\n1501         from .alignment import align\n1502         from .dataarray import DataArray\n1503 \n1504         if isinstance(value, Dataset):\n1505             missing_vars = [\n1506                 name for name in value.data_vars if name not in self.data_vars\n1507             ]\n1508             if missing_vars:\n1509                 raise ValueError(\n1510                     f\"Variables {missing_vars} in new values\"\n1511                     f\" not available in original dataset:\\n{self}\"\n1512                 )\n1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):\n1514             raise TypeError(\n1515                 \"Dataset assignment only accepts DataArrays, Datasets, and scalars.\"\n1516             )\n1517 \n1518         new_value = Dataset()\n1519         for name, var in self.items():\n1520             # test indexing\n1521             try:\n1522                 var_k = var[key]\n1523             except Exception as e:\n1524                 raise ValueError(\n1525                     f\"Variable '{name}': indexer {key} not available\"\n1526                 ) from e\n1527 \n1528             if isinstance(value, Dataset):\n1529                 val = value[name]\n1530             else:\n1531                 val = value\n1532 \n1533             if isinstance(val, DataArray):\n1534                 # check consistency of dimensions\n1535                 for dim in val.dims:\n1536                     if dim not in var_k.dims:\n1537                         raise KeyError(\n1538                             f\"Variable '{name}': dimension '{dim}' appears in new values \"\n1539                             f\"but not in the indexed original data\"\n1540                         )\n1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)\n1542                 if dims != val.dims:\n1543                     raise ValueError(\n1544                         f\"Variable '{name}': dimension order differs between\"\n1545                         f\" original and new data:\\n{dims}\\nvs.\\n{val.dims}\"\n1546                     )\n1547             else:\n1548                 val = np.array(val)\n1549 \n1550             # type conversion\n1551             new_value[name] = val.astype(var_k.dtype, copy=False)\n1552 \n1553         # check consistency of dimension sizes and dimension coordinates\n1554         if isinstance(value, DataArray) or isinstance(value, Dataset):\n1555             align(self[key], value, join=\"exact\", copy=False)\n1556 \n1557         return new_value\n1558 \n1559     def __delitem__(self, key: Hashable) -> None:\n1560         \"\"\"Remove a variable from this dataset.\"\"\"\n1561         assert_no_index_corrupted(self.xindexes, {key})\n1562 \n1563         if key in self._indexes:\n1564             del self._indexes[key]\n1565         del self._variables[key]\n1566         self._coord_names.discard(key)\n1567         self._dims = calculate_dimensions(self._variables)\n1568 \n1569     # mutable objects should not be hashable\n1570     # https://github.com/python/mypy/issues/4266\n1571     __hash__ = None  # type: ignore[assignment]\n1572 \n1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:\n1574         \"\"\"Helper function for equals and identical\"\"\"\n1575 \n1576         # some stores (e.g., scipy) do not seem to preserve order, so don't\n1577         # require matching order for equality\n1578         def compat(x: Variable, y: Variable) -> bool:\n1579             return getattr(x, compat_str)(y)\n1580 \n1581         return self._coord_names == other._coord_names and utils.dict_equiv(\n1582             self._variables, other._variables, compat=compat\n1583         )\n1584 \n1585     def broadcast_equals(self, other: Dataset) -> bool:\n1586         \"\"\"Two Datasets are broadcast equal if they are equal after\n1587         broadcasting all variables against each other.\n1588 \n1589         For example, variables that are scalar in one dataset but non-scalar in\n1590         the other dataset can still be broadcast equal if the the non-scalar\n1591         variable is a constant.\n1592 \n1593         See Also\n1594         --------\n1595         Dataset.equals\n1596         Dataset.identical\n1597         \"\"\"\n1598         try:\n1599             return self._all_compat(other, \"broadcast_equals\")\n1600         except (TypeError, AttributeError):\n1601             return False\n1602 \n1603     def equals(self, other: Dataset) -> bool:\n1604         \"\"\"Two Datasets are equal if they have matching variables and\n1605         coordinates, all of which are equal.\n1606 \n1607         Datasets can still be equal (like pandas objects) if they have NaN\n1608         values in the same locations.\n1609 \n1610         This method is necessary because `v1 == v2` for ``Dataset``\n1611         does element-wise comparisons (like numpy.ndarrays).\n1612 \n1613         See Also\n1614         --------\n1615         Dataset.broadcast_equals\n1616         Dataset.identical\n1617         \"\"\"\n1618         try:\n1619             return self._all_compat(other, \"equals\")\n1620         except (TypeError, AttributeError):\n1621             return False\n1622 \n1623     def identical(self, other: Dataset) -> bool:\n1624         \"\"\"Like equals, but also checks all dataset attributes and the\n1625         attributes on all variables and coordinates.\n1626 \n1627         See Also\n1628         --------\n1629         Dataset.broadcast_equals\n1630         Dataset.equals\n1631         \"\"\"\n1632         try:\n1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(\n1634                 other, \"identical\"\n1635             )\n1636         except (TypeError, AttributeError):\n1637             return False\n1638 \n1639     @property\n1640     def indexes(self) -> Indexes[pd.Index]:\n1641         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n1642 \n1643         Raises an error if this Dataset has indexes that cannot be coerced\n1644         to pandas.Index objects.\n1645 \n1646         See Also\n1647         --------\n1648         Dataset.xindexes\n1649 \n1650         \"\"\"\n1651         return self.xindexes.to_pandas_indexes()\n1652 \n1653     @property\n1654     def xindexes(self) -> Indexes[Index]:\n1655         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})\n1657 \n1658     @property\n1659     def coords(self) -> DatasetCoordinates:\n1660         \"\"\"Dictionary of xarray.DataArray objects corresponding to coordinate\n1661         variables\n1662         \"\"\"\n1663         return DatasetCoordinates(self)\n1664 \n1665     @property\n1666     def data_vars(self) -> DataVariables:\n1667         \"\"\"Dictionary of DataArray objects corresponding to data variables\"\"\"\n1668         return DataVariables(self)\n1669 \n1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:\n1671         \"\"\"Given names of one or more variables, set them as coordinates\n1672 \n1673         Parameters\n1674         ----------\n1675         names : hashable or iterable of hashable\n1676             Name(s) of variables in this dataset to convert into coordinates.\n1677 \n1678         Returns\n1679         -------\n1680         Dataset\n1681 \n1682         See Also\n1683         --------\n1684         Dataset.swap_dims\n1685         \"\"\"\n1686         # TODO: allow inserting new coordinates with this method, like\n1687         # DataFrame.set_index?\n1688         # nb. check in self._variables, not self.data_vars to insure that the\n1689         # operation is idempotent\n1690         if isinstance(names, str) or not isinstance(names, Iterable):\n1691             names = [names]\n1692         else:\n1693             names = list(names)\n1694         self._assert_all_in_dataset(names)\n1695         obj = self.copy()\n1696         obj._coord_names.update(names)\n1697         return obj\n1698 \n1699     def reset_coords(\n1700         self: T_Dataset,\n1701         names: Hashable | Iterable[Hashable] | None = None,\n1702         drop: bool = False,\n1703     ) -> T_Dataset:\n1704         \"\"\"Given names of coordinates, reset them to become variables\n1705 \n1706         Parameters\n1707         ----------\n1708         names : hashable or iterable of hashable, optional\n1709             Name(s) of non-index coordinates in this dataset to reset into\n1710             variables. By default, all non-index coordinates are reset.\n1711         drop : bool, default: False\n1712             If True, remove coordinates instead of converting them into\n1713             variables.\n1714 \n1715         Returns\n1716         -------\n1717         Dataset\n1718         \"\"\"\n1719         if names is None:\n1720             names = self._coord_names - set(self._indexes)\n1721         else:\n1722             if isinstance(names, str) or not isinstance(names, Iterable):\n1723                 names = [names]\n1724             else:\n1725                 names = list(names)\n1726             self._assert_all_in_dataset(names)\n1727             bad_coords = set(names) & set(self._indexes)\n1728             if bad_coords:\n1729                 raise ValueError(\n1730                     f\"cannot remove index coordinates with reset_coords: {bad_coords}\"\n1731                 )\n1732         obj = self.copy()\n1733         obj._coord_names.difference_update(names)\n1734         if drop:\n1735             for name in names:\n1736                 del obj._variables[name]\n1737         return obj\n1738 \n1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:\n1740         \"\"\"Store dataset contents to a backends.*DataStore object.\"\"\"\n1741         from ..backends.api import dump_to_store\n1742 \n1743         # TODO: rename and/or cleanup this method to make it more consistent\n1744         # with to_netcdf()\n1745         dump_to_store(self, store, **kwargs)\n1746 \n1747     # path=None writes to bytes\n1748     @overload\n1749     def to_netcdf(\n1750         self,\n1751         path: None = None,\n1752         mode: Literal[\"w\", \"a\"] = \"w\",\n1753         format: T_NetcdfTypes | None = None,\n1754         group: str | None = None,\n1755         engine: T_NetcdfEngine | None = None,\n1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n1757         unlimited_dims: Iterable[Hashable] | None = None,\n1758         compute: bool = True,\n1759         invalid_netcdf: bool = False,\n1760     ) -> bytes:\n1761         ...\n1762 \n1763     # default return None\n1764     @overload\n1765     def to_netcdf(\n1766         self,\n1767         path: str | PathLike,\n1768         mode: Literal[\"w\", \"a\"] = \"w\",\n1769         format: T_NetcdfTypes | None = None,\n1770         group: str | None = None,\n1771         engine: T_NetcdfEngine | None = None,\n1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n1773         unlimited_dims: Iterable[Hashable] | None = None,\n1774         compute: Literal[True] = True,\n1775         invalid_netcdf: bool = False,\n1776     ) -> None:\n1777         ...\n1778 \n1779     # compute=False returns dask.Delayed\n1780     @overload\n1781     def to_netcdf(\n1782         self,\n1783         path: str | PathLike,\n1784         mode: Literal[\"w\", \"a\"] = \"w\",\n1785         format: T_NetcdfTypes | None = None,\n1786         group: str | None = None,\n1787         engine: T_NetcdfEngine | None = None,\n1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n1789         unlimited_dims: Iterable[Hashable] | None = None,\n1790         *,\n1791         compute: Literal[False],\n1792         invalid_netcdf: bool = False,\n1793     ) -> Delayed:\n1794         ...\n1795 \n1796     def to_netcdf(\n1797         self,\n1798         path: str | PathLike | None = None,\n1799         mode: Literal[\"w\", \"a\"] = \"w\",\n1800         format: T_NetcdfTypes | None = None,\n1801         group: str | None = None,\n1802         engine: T_NetcdfEngine | None = None,\n1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n1804         unlimited_dims: Iterable[Hashable] | None = None,\n1805         compute: bool = True,\n1806         invalid_netcdf: bool = False,\n1807     ) -> bytes | Delayed | None:\n1808         \"\"\"Write dataset contents to a netCDF file.\n1809 \n1810         Parameters\n1811         ----------\n1812         path : str, path-like or file-like, optional\n1813             Path to which to save this dataset. File-like objects are only\n1814             supported by the scipy engine. If no path is provided, this\n1815             function returns the resulting netCDF file as bytes; in this case,\n1816             we need to use scipy, which does not support netCDF version 4 (the\n1817             default format becomes NETCDF3_64BIT).\n1818         mode : {\"w\", \"a\"}, default: \"w\"\n1819             Write ('w') or append ('a') mode. If mode='w', any existing file at\n1820             this location will be overwritten. If mode='a', existing variables\n1821             will be overwritten.\n1822         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n1823                   \"NETCDF3_CLASSIC\"}, optional\n1824             File format for the resulting netCDF file:\n1825 \n1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n1827               features.\n1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n1829               netCDF 3 compatible API features.\n1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n1831               which fully supports 2+ GB files, but is only compatible with\n1832               clients linked against netCDF version 3.6.0 or later.\n1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n1834               handle 2+ GB files very well.\n1835 \n1836             All formats are supported by the netCDF4-python library.\n1837             scipy.io.netcdf only supports the last two formats.\n1838 \n1839             The default format is NETCDF4 if you are saving a file to disk and\n1840             have the netCDF4-python library available. Otherwise, xarray falls\n1841             back to using scipy to write netCDF files and defaults to the\n1842             NETCDF3_64BIT format (scipy does not support netCDF4).\n1843         group : str, optional\n1844             Path to the netCDF4 group in the given file to open (only works for\n1845             format='NETCDF4'). The group(s) will be created if necessary.\n1846         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n1847             Engine to use when writing netCDF files. If not provided, the\n1848             default engine is chosen based on available dependencies, with a\n1849             preference for 'netcdf4' if writing to a file on disk.\n1850         encoding : dict, optional\n1851             Nested dictionary with variable names as keys and dictionaries of\n1852             variable specific encodings as values, e.g.,\n1853             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n1854             \"zlib\": True}, ...}``\n1855 \n1856             The `h5netcdf` engine supports both the NetCDF4-style compression\n1857             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n1858             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n1859             This allows using any compression plugin installed in the HDF5\n1860             library, e.g. LZF.\n1861 \n1862         unlimited_dims : iterable of hashable, optional\n1863             Dimension(s) that should be serialized as unlimited dimensions.\n1864             By default, no dimensions are treated as unlimited dimensions.\n1865             Note that unlimited_dims may also be set via\n1866             ``dataset.encoding[\"unlimited_dims\"]``.\n1867         compute: bool, default: True\n1868             If true compute immediately, otherwise return a\n1869             ``dask.delayed.Delayed`` object that can be computed later.\n1870         invalid_netcdf: bool, default: False\n1871             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n1872             hdf5 files which are invalid netcdf as described in\n1873             https://github.com/h5netcdf/h5netcdf.\n1874 \n1875         Returns\n1876         -------\n1877             * ``bytes`` if path is None\n1878             * ``dask.delayed.Delayed`` if compute is False\n1879             * None otherwise\n1880 \n1881         See Also\n1882         --------\n1883         DataArray.to_netcdf\n1884         \"\"\"\n1885         if encoding is None:\n1886             encoding = {}\n1887         from ..backends.api import to_netcdf\n1888 \n1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n1890             self,\n1891             path,\n1892             mode=mode,\n1893             format=format,\n1894             group=group,\n1895             engine=engine,\n1896             encoding=encoding,\n1897             unlimited_dims=unlimited_dims,\n1898             compute=compute,\n1899             multifile=False,\n1900             invalid_netcdf=invalid_netcdf,\n1901         )\n1902 \n1903     # compute=True (default) returns ZarrStore\n1904     @overload\n1905     def to_zarr(\n1906         self,\n1907         store: MutableMapping | str | PathLike[str] | None = None,\n1908         chunk_store: MutableMapping | str | PathLike | None = None,\n1909         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1910         synchronizer=None,\n1911         group: str | None = None,\n1912         encoding: Mapping | None = None,\n1913         compute: Literal[True] = True,\n1914         consolidated: bool | None = None,\n1915         append_dim: Hashable | None = None,\n1916         region: Mapping[str, slice] | None = None,\n1917         safe_chunks: bool = True,\n1918         storage_options: dict[str, str] | None = None,\n1919     ) -> ZarrStore:\n1920         ...\n1921 \n1922     # compute=False returns dask.Delayed\n1923     @overload\n1924     def to_zarr(\n1925         self,\n1926         store: MutableMapping | str | PathLike[str] | None = None,\n1927         chunk_store: MutableMapping | str | PathLike | None = None,\n1928         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1929         synchronizer=None,\n1930         group: str | None = None,\n1931         encoding: Mapping | None = None,\n1932         *,\n1933         compute: Literal[False],\n1934         consolidated: bool | None = None,\n1935         append_dim: Hashable | None = None,\n1936         region: Mapping[str, slice] | None = None,\n1937         safe_chunks: bool = True,\n1938         storage_options: dict[str, str] | None = None,\n1939     ) -> Delayed:\n1940         ...\n1941 \n1942     def to_zarr(\n1943         self,\n1944         store: MutableMapping | str | PathLike[str] | None = None,\n1945         chunk_store: MutableMapping | str | PathLike | None = None,\n1946         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1947         synchronizer=None,\n1948         group: str | None = None,\n1949         encoding: Mapping | None = None,\n1950         compute: bool = True,\n1951         consolidated: bool | None = None,\n1952         append_dim: Hashable | None = None,\n1953         region: Mapping[str, slice] | None = None,\n1954         safe_chunks: bool = True,\n1955         storage_options: dict[str, str] | None = None,\n1956     ) -> ZarrStore | Delayed:\n1957         \"\"\"Write dataset contents to a zarr group.\n1958 \n1959         Zarr chunks are determined in the following way:\n1960 \n1961         - From the ``chunks`` attribute in each variable's ``encoding``\n1962           (can be set via `Dataset.chunk`).\n1963         - If the variable is a Dask array, from the dask chunks\n1964         - If neither Dask chunks nor encoding chunks are present, chunks will\n1965           be determined automatically by Zarr\n1966         - If both Dask chunks and encoding chunks are present, encoding chunks\n1967           will be used, provided that there is a many-to-one relationship between\n1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and\n1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.\n1970           This restriction ensures that no synchronization / locks are required\n1971           when writing. To disable this restriction, use ``safe_chunks=False``.\n1972 \n1973         Parameters\n1974         ----------\n1975         store : MutableMapping, str or path-like, optional\n1976             Store or path to directory in local or remote file system.\n1977         chunk_store : MutableMapping, str or path-like, optional\n1978             Store or path to directory in local or remote file system only for Zarr\n1979             array chunks. Requires zarr-python v2.4.0 or later.\n1980         mode : {\"w\", \"w-\", \"a\", \"r+\", None}, optional\n1981             Persistence mode: \"w\" means create (overwrite if exists);\n1982             \"w-\" means create (fail if exists);\n1983             \"a\" means override existing variables (create if does not exist);\n1984             \"r+\" means modify existing array *values* only (raise an error if\n1985             any metadata or shapes would change).\n1986             The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n1987             \"r+\" if ``region`` is set and ``w-`` otherwise.\n1988         synchronizer : object, optional\n1989             Zarr array synchronizer.\n1990         group : str, optional\n1991             Group path. (a.k.a. `path` in zarr terminology.)\n1992         encoding : dict, optional\n1993             Nested dictionary with variable names as keys and dictionaries of\n1994             variable specific encodings as values, e.g.,\n1995             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n1996         compute : bool, optional\n1997             If True write array data immediately, otherwise return a\n1998             ``dask.delayed.Delayed`` object that can be computed to write\n1999             array data later. Metadata is always updated eagerly.\n2000         consolidated : bool, optional\n2001             If True, apply zarr's `consolidate_metadata` function to the store\n2002             after writing metadata and read existing stores with consolidated\n2003             metadata; if False, do not. The default (`consolidated=None`) means\n2004             write consolidated metadata and attempt to read consolidated\n2005             metadata for existing stores (falling back to non-consolidated).\n2006         append_dim : hashable, optional\n2007             If set, the dimension along which the data will be appended. All\n2008             other dimensions on overridden variables must remain the same size.\n2009         region : dict, optional\n2010             Optional mapping from dimension names to integer slices along\n2011             dataset dimensions to indicate the region of existing zarr array(s)\n2012             in which to write this dataset's data. For example,\n2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n2014             that values should be written to the region ``0:1000`` along ``x``\n2015             and ``10000:11000`` along ``y``.\n2016 \n2017             Two restrictions apply to the use of ``region``:\n2018 \n2019             - If ``region`` is set, _all_ variables in a dataset must have at\n2020               least one dimension in common with the region. Other variables\n2021               should be written in a separate call to ``to_zarr()``.\n2022             - Dimensions cannot be included in both ``region`` and\n2023               ``append_dim`` at the same time. To create empty arrays to fill\n2024               in with ``region``, use a separate call to ``to_zarr()`` with\n2025               ``compute=False``. See \"Appending to existing Zarr stores\" in\n2026               the reference documentation for full details.\n2027         safe_chunks : bool, optional\n2028             If True, only allow writes to when there is a many-to-one relationship\n2029             between Zarr chunks (specified in encoding) and Dask chunks.\n2030             Set False to override this restriction; however, data may become corrupted\n2031             if Zarr arrays are written in parallel. This option may be useful in combination\n2032             with ``compute=False`` to initialize a Zarr from an existing\n2033             Dataset with arbitrary chunk structure.\n2034         storage_options : dict, optional\n2035             Any additional parameters for the storage backend (ignored for local\n2036             paths).\n2037 \n2038         Returns\n2039         -------\n2040             * ``dask.delayed.Delayed`` if compute is False\n2041             * ZarrStore otherwise\n2042 \n2043         References\n2044         ----------\n2045         https://zarr.readthedocs.io/\n2046 \n2047         Notes\n2048         -----\n2049         Zarr chunking behavior:\n2050             If chunks are found in the encoding argument or attribute\n2051             corresponding to any DataArray, those chunks are used.\n2052             If a DataArray is a dask array, it is written with those chunks.\n2053             If not other chunks are found, Zarr uses its own heuristics to\n2054             choose automatic chunk sizes.\n2055 \n2056         encoding:\n2057             The encoding attribute (if exists) of the DataArray(s) will be\n2058             used. Override any existing encodings by providing the ``encoding`` kwarg.\n2059 \n2060         See Also\n2061         --------\n2062         :ref:`io.zarr`\n2063             The I/O user guide, with more details and examples.\n2064         \"\"\"\n2065         from ..backends.api import to_zarr\n2066 \n2067         return to_zarr(  # type: ignore\n2068             self,\n2069             store=store,\n2070             chunk_store=chunk_store,\n2071             storage_options=storage_options,\n2072             mode=mode,\n2073             synchronizer=synchronizer,\n2074             group=group,\n2075             encoding=encoding,\n2076             compute=compute,\n2077             consolidated=consolidated,\n2078             append_dim=append_dim,\n2079             region=region,\n2080             safe_chunks=safe_chunks,\n2081         )\n2082 \n2083     def __repr__(self) -> str:\n2084         return formatting.dataset_repr(self)\n2085 \n2086     def _repr_html_(self) -> str:\n2087         if OPTIONS[\"display_style\"] == \"text\":\n2088             return f\"<pre>{escape(repr(self))}</pre>\"\n2089         return formatting_html.dataset_repr(self)\n2090 \n2091     def info(self, buf: IO | None = None) -> None:\n2092         \"\"\"\n2093         Concise summary of a Dataset variables and attributes.\n2094 \n2095         Parameters\n2096         ----------\n2097         buf : file-like, default: sys.stdout\n2098             writable buffer\n2099 \n2100         See Also\n2101         --------\n2102         pandas.DataFrame.assign\n2103         ncdump : netCDF's ncdump\n2104         \"\"\"\n2105         if buf is None:  # pragma: no cover\n2106             buf = sys.stdout\n2107 \n2108         lines = []\n2109         lines.append(\"xarray.Dataset {\")\n2110         lines.append(\"dimensions:\")\n2111         for name, size in self.dims.items():\n2112             lines.append(f\"\\t{name} = {size} ;\")\n2113         lines.append(\"\\nvariables:\")\n2114         for name, da in self.variables.items():\n2115             dims = \", \".join(map(str, da.dims))\n2116             lines.append(f\"\\t{da.dtype} {name}({dims}) ;\")\n2117             for k, v in da.attrs.items():\n2118                 lines.append(f\"\\t\\t{name}:{k} = {v} ;\")\n2119         lines.append(\"\\n// global attributes:\")\n2120         for k, v in self.attrs.items():\n2121             lines.append(f\"\\t:{k} = {v} ;\")\n2122         lines.append(\"}\")\n2123 \n2124         buf.write(\"\\n\".join(lines))\n2125 \n2126     @property\n2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:\n2128         \"\"\"\n2129         Mapping from dimension names to block lengths for this dataset's data, or None if\n2130         the underlying data is not a dask array.\n2131         Cannot be modified directly, but can be modified by calling .chunk().\n2132 \n2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.\n2134 \n2135         See Also\n2136         --------\n2137         Dataset.chunk\n2138         Dataset.chunksizes\n2139         xarray.unify_chunks\n2140         \"\"\"\n2141         return get_chunksizes(self.variables.values())\n2142 \n2143     @property\n2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:\n2145         \"\"\"\n2146         Mapping from dimension names to block lengths for this dataset's data, or None if\n2147         the underlying data is not a dask array.\n2148         Cannot be modified directly, but can be modified by calling .chunk().\n2149 \n2150         Same as Dataset.chunks.\n2151 \n2152         See Also\n2153         --------\n2154         Dataset.chunk\n2155         Dataset.chunks\n2156         xarray.unify_chunks\n2157         \"\"\"\n2158         return get_chunksizes(self.variables.values())\n2159 \n2160     def chunk(\n2161         self: T_Dataset,\n2162         chunks: (\n2163             int | Literal[\"auto\"] | Mapping[Any, None | int | str | tuple[int, ...]]\n2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n2165         name_prefix: str = \"xarray-\",\n2166         token: str | None = None,\n2167         lock: bool = False,\n2168         inline_array: bool = False,\n2169         **chunks_kwargs: Any,\n2170     ) -> T_Dataset:\n2171         \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n2172         chunks.\n2173 \n2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask\n2175         arrays will be rechunked to the given chunk sizes.\n2176 \n2177         If neither chunks is not provided for one or more dimensions, chunk\n2178         sizes along that dimension will not be updated; non-dask arrays will be\n2179         converted into dask arrays with a single block.\n2180 \n2181         Parameters\n2182         ----------\n2183         chunks : int, tuple of int, \"auto\" or mapping of hashable to int, optional\n2184             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, or\n2185             ``{\"x\": 5, \"y\": 5}``.\n2186         name_prefix : str, default: \"xarray-\"\n2187             Prefix for the name of any new dask arrays.\n2188         token : str, optional\n2189             Token uniquely identifying this dataset.\n2190         lock : bool, default: False\n2191             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2192             already as dask array.\n2193         inline_array: bool, default: False\n2194             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2195             already as dask array.\n2196         **chunks_kwargs : {dim: chunks, ...}, optional\n2197             The keyword arguments form of ``chunks``.\n2198             One of chunks or chunks_kwargs must be provided\n2199 \n2200         Returns\n2201         -------\n2202         chunked : xarray.Dataset\n2203 \n2204         See Also\n2205         --------\n2206         Dataset.chunks\n2207         Dataset.chunksizes\n2208         xarray.unify_chunks\n2209         dask.array.from_array\n2210         \"\"\"\n2211         if chunks is None and chunks_kwargs is None:\n2212             warnings.warn(\n2213                 \"None value for 'chunks' is deprecated. \"\n2214                 \"It will raise an error in the future. Use instead '{}'\",\n2215                 category=FutureWarning,\n2216             )\n2217             chunks = {}\n2218 \n2219         if isinstance(chunks, (Number, str, int)):\n2220             chunks = dict.fromkeys(self.dims, chunks)\n2221         else:\n2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n2223 \n2224         bad_dims = chunks.keys() - self.dims.keys()\n2225         if bad_dims:\n2226             raise ValueError(\n2227                 f\"some chunks keys are not dimensions on this object: {bad_dims}\"\n2228             )\n2229 \n2230         variables = {\n2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)\n2232             for k, v in self.variables.items()\n2233         }\n2234         return self._replace(variables)\n2235 \n2236     def _validate_indexers(\n2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = \"raise\"\n2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:\n2239         \"\"\"Here we make sure\n2240         + indexer has a valid keys\n2241         + indexer is in a valid data type\n2242         + string indexers are cast to the appropriate date type if the\n2243           associated index is a DatetimeIndex or CFTimeIndex\n2244         \"\"\"\n2245         from ..coding.cftimeindex import CFTimeIndex\n2246         from .dataarray import DataArray\n2247 \n2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2249 \n2250         # all indexers should be int, slice, np.ndarrays, or Variable\n2251         for k, v in indexers.items():\n2252             if isinstance(v, (int, slice, Variable)):\n2253                 yield k, v\n2254             elif isinstance(v, DataArray):\n2255                 yield k, v.variable\n2256             elif isinstance(v, tuple):\n2257                 yield k, as_variable(v)\n2258             elif isinstance(v, Dataset):\n2259                 raise TypeError(\"cannot use a Dataset as an indexer\")\n2260             elif isinstance(v, Sequence) and len(v) == 0:\n2261                 yield k, np.empty((0,), dtype=\"int64\")\n2262             else:\n2263                 v = np.asarray(v)\n2264 \n2265                 if v.dtype.kind in \"US\":\n2266                     index = self._indexes[k].to_pandas_index()\n2267                     if isinstance(index, pd.DatetimeIndex):\n2268                         v = v.astype(\"datetime64[ns]\")\n2269                     elif isinstance(index, CFTimeIndex):\n2270                         v = _parse_array_of_cftime_strings(v, index.date_type)\n2271 \n2272                 if v.ndim > 1:\n2273                     raise IndexError(\n2274                         \"Unlabeled multi-dimensional array cannot be \"\n2275                         \"used for indexing: {}\".format(k)\n2276                     )\n2277                 yield k, v\n2278 \n2279     def _validate_interp_indexers(\n2280         self, indexers: Mapping[Any, Any]\n2281     ) -> Iterator[tuple[Hashable, Variable]]:\n2282         \"\"\"Variant of _validate_indexers to be used for interpolation\"\"\"\n2283         for k, v in self._validate_indexers(indexers):\n2284             if isinstance(v, Variable):\n2285                 if v.ndim == 1:\n2286                     yield k, v.to_index_variable()\n2287                 else:\n2288                     yield k, v\n2289             elif isinstance(v, int):\n2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)\n2291             elif isinstance(v, np.ndarray):\n2292                 if v.ndim == 0:\n2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)\n2294                 elif v.ndim == 1:\n2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)\n2296                 else:\n2297                     raise AssertionError()  # Already tested by _validate_indexers\n2298             else:\n2299                 raise TypeError(type(v))\n2300 \n2301     def _get_indexers_coords_and_indexes(self, indexers):\n2302         \"\"\"Extract coordinates and indexes from indexers.\n2303 \n2304         Only coordinate with a name different from any of self.variables will\n2305         be attached.\n2306         \"\"\"\n2307         from .dataarray import DataArray\n2308 \n2309         coords_list = []\n2310         for k, v in indexers.items():\n2311             if isinstance(v, DataArray):\n2312                 if v.dtype.kind == \"b\":\n2313                     if v.ndim != 1:  # we only support 1-d boolean array\n2314                         raise ValueError(\n2315                             \"{:d}d-boolean array is used for indexing along \"\n2316                             \"dimension {!r}, but only 1d boolean arrays are \"\n2317                             \"supported.\".format(v.ndim, k)\n2318                         )\n2319                     # Make sure in case of boolean DataArray, its\n2320                     # coordinate also should be indexed.\n2321                     v_coords = v[v.values.nonzero()[0]].coords\n2322                 else:\n2323                     v_coords = v.coords\n2324                 coords_list.append(v_coords)\n2325 \n2326         # we don't need to call align() explicitly or check indexes for\n2327         # alignment, because merge_variables already checks for exact alignment\n2328         # between dimension coordinates\n2329         coords, indexes = merge_coordinates_without_align(coords_list)\n2330         assert_coordinate_consistent(self, coords)\n2331 \n2332         # silently drop the conflicted variables.\n2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n2334         attached_indexes = {\n2335             k: v for k, v in indexes.items() if k not in self._variables\n2336         }\n2337         return attached_coords, attached_indexes\n2338 \n2339     def isel(\n2340         self: T_Dataset,\n2341         indexers: Mapping[Any, Any] | None = None,\n2342         drop: bool = False,\n2343         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2344         **indexers_kwargs: Any,\n2345     ) -> T_Dataset:\n2346         \"\"\"Returns a new dataset with each array indexed along the specified\n2347         dimension(s).\n2348 \n2349         This method selects values from each array using its `__getitem__`\n2350         method, except this method does not require knowing the order of\n2351         each array's dimensions.\n2352 \n2353         Parameters\n2354         ----------\n2355         indexers : dict, optional\n2356             A dict with keys matching dimensions and values given\n2357             by integers, slice objects or arrays.\n2358             indexer can be a integer, slice, array-like or DataArray.\n2359             If DataArrays are passed as indexers, xarray-style indexing will be\n2360             carried out. See :ref:`indexing` for the details.\n2361             One of indexers or indexers_kwargs must be provided.\n2362         drop : bool, default: False\n2363             If ``drop=True``, drop coordinates variables indexed by integers\n2364             instead of making them scalar.\n2365         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2366             What to do if dimensions that should be selected from are not present in the\n2367             Dataset:\n2368             - \"raise\": raise an exception\n2369             - \"warn\": raise a warning, and ignore the missing dimensions\n2370             - \"ignore\": ignore the missing dimensions\n2371 \n2372         **indexers_kwargs : {dim: indexer, ...}, optional\n2373             The keyword arguments form of ``indexers``.\n2374             One of indexers or indexers_kwargs must be provided.\n2375 \n2376         Returns\n2377         -------\n2378         obj : Dataset\n2379             A new Dataset with the same contents as this dataset, except each\n2380             array and dimension is indexed by the appropriate indexers.\n2381             If indexer DataArrays have coordinates that do not conflict with\n2382             this object, then these coordinates will be attached.\n2383             In general, each array's data will be a view of the array's data\n2384             in this dataset, unless vectorized indexing was triggered by using\n2385             an array indexer, in which case the data will be a copy.\n2386 \n2387         See Also\n2388         --------\n2389         Dataset.sel\n2390         DataArray.isel\n2391         \"\"\"\n2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n2395 \n2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n2397         # lists, or zero or one-dimensional np.ndarray's\n2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2399 \n2400         variables = {}\n2401         dims: dict[Hashable, int] = {}\n2402         coord_names = self._coord_names.copy()\n2403 \n2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n2405 \n2406         for name, var in self._variables.items():\n2407             # preserve variable order\n2408             if name in index_variables:\n2409                 var = index_variables[name]\n2410             else:\n2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}\n2412                 if var_indexers:\n2413                     var = var.isel(var_indexers)\n2414                     if drop and var.ndim == 0 and name in coord_names:\n2415                         coord_names.remove(name)\n2416                         continue\n2417             variables[name] = var\n2418             dims.update(zip(var.dims, var.shape))\n2419 \n2420         return self._construct_direct(\n2421             variables=variables,\n2422             coord_names=coord_names,\n2423             dims=dims,\n2424             attrs=self._attrs,\n2425             indexes=indexes,\n2426             encoding=self._encoding,\n2427             close=self._close,\n2428         )\n2429 \n2430     def _isel_fancy(\n2431         self: T_Dataset,\n2432         indexers: Mapping[Any, Any],\n2433         *,\n2434         drop: bool,\n2435         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2436     ) -> T_Dataset:\n2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))\n2438 \n2439         variables: dict[Hashable, Variable] = {}\n2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)\n2441 \n2442         for name, var in self.variables.items():\n2443             if name in index_variables:\n2444                 new_var = index_variables[name]\n2445             else:\n2446                 var_indexers = {\n2447                     k: v for k, v in valid_indexers.items() if k in var.dims\n2448                 }\n2449                 if var_indexers:\n2450                     new_var = var.isel(indexers=var_indexers)\n2451                     # drop scalar coordinates\n2452                     # https://github.com/pydata/xarray/issues/6554\n2453                     if name in self.coords and drop and new_var.ndim == 0:\n2454                         continue\n2455                 else:\n2456                     new_var = var.copy(deep=False)\n2457                 if name not in indexes:\n2458                     new_var = new_var.to_base_variable()\n2459             variables[name] = new_var\n2460 \n2461         coord_names = self._coord_names & variables.keys()\n2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)\n2463 \n2464         # Extract coordinates from indexers\n2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n2466         variables.update(coord_vars)\n2467         indexes.update(new_indexes)\n2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2470 \n2471     def sel(\n2472         self: T_Dataset,\n2473         indexers: Mapping[Any, Any] = None,\n2474         method: str = None,\n2475         tolerance: int | float | Iterable[int | float] | None = None,\n2476         drop: bool = False,\n2477         **indexers_kwargs: Any,\n2478     ) -> T_Dataset:\n2479         \"\"\"Returns a new dataset with each array indexed by tick labels\n2480         along the specified dimension(s).\n2481 \n2482         In contrast to `Dataset.isel`, indexers for this method should use\n2483         labels instead of integers.\n2484 \n2485         Under the hood, this method is powered by using pandas's powerful Index\n2486         objects. This makes label based indexing essentially just as fast as\n2487         using integer indexing.\n2488 \n2489         It also means this method uses pandas's (well documented) logic for\n2490         indexing. This means you can use string shortcuts for datetime indexes\n2491         (e.g., '2000-01' to select all values in January 2000). It also means\n2492         that slices are treated as inclusive of both the start and stop values,\n2493         unlike normal Python indexing.\n2494 \n2495         Parameters\n2496         ----------\n2497         indexers : dict, optional\n2498             A dict with keys matching dimensions and values given\n2499             by scalars, slices or arrays of tick labels. For dimensions with\n2500             multi-index, the indexer may also be a dict-like object with keys\n2501             matching index level names.\n2502             If DataArrays are passed as indexers, xarray-style indexing will be\n2503             carried out. See :ref:`indexing` for the details.\n2504             One of indexers or indexers_kwargs must be provided.\n2505         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n2506             Method to use for inexact matches:\n2507 \n2508             * None (default): only exact matches\n2509             * pad / ffill: propagate last valid index value forward\n2510             * backfill / bfill: propagate next valid index value backward\n2511             * nearest: use nearest valid index value\n2512         tolerance : optional\n2513             Maximum distance between original and new labels for inexact\n2514             matches. The values of the index at the matching locations must\n2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2516         drop : bool, optional\n2517             If ``drop=True``, drop coordinates variables in `indexers` instead\n2518             of making them scalar.\n2519         **indexers_kwargs : {dim: indexer, ...}, optional\n2520             The keyword arguments form of ``indexers``.\n2521             One of indexers or indexers_kwargs must be provided.\n2522 \n2523         Returns\n2524         -------\n2525         obj : Dataset\n2526             A new Dataset with the same contents as this dataset, except each\n2527             variable and dimension is indexed by the appropriate indexers.\n2528             If indexer DataArrays have coordinates that do not conflict with\n2529             this object, then these coordinates will be attached.\n2530             In general, each array's data will be a view of the array's data\n2531             in this dataset, unless vectorized indexing was triggered by using\n2532             an array indexer, in which case the data will be a copy.\n2533 \n2534         See Also\n2535         --------\n2536         Dataset.isel\n2537         DataArray.sel\n2538         \"\"\"\n2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"sel\")\n2540         query_results = map_index_queries(\n2541             self, indexers=indexers, method=method, tolerance=tolerance\n2542         )\n2543 \n2544         if drop:\n2545             no_scalar_variables = {}\n2546             for k, v in query_results.variables.items():\n2547                 if v.dims:\n2548                     no_scalar_variables[k] = v\n2549                 else:\n2550                     if k in self._coord_names:\n2551                         query_results.drop_coords.append(k)\n2552             query_results.variables = no_scalar_variables\n2553 \n2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)\n2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])\n2556 \n2557     def head(\n2558         self: T_Dataset,\n2559         indexers: Mapping[Any, int] | int | None = None,\n2560         **indexers_kwargs: Any,\n2561     ) -> T_Dataset:\n2562         \"\"\"Returns a new dataset with the first `n` values of each array\n2563         for the specified dimension(s).\n2564 \n2565         Parameters\n2566         ----------\n2567         indexers : dict or int, default: 5\n2568             A dict with keys matching dimensions and integer values `n`\n2569             or a single integer `n` applied over all dimensions.\n2570             One of indexers or indexers_kwargs must be provided.\n2571         **indexers_kwargs : {dim: n, ...}, optional\n2572             The keyword arguments form of ``indexers``.\n2573             One of indexers or indexers_kwargs must be provided.\n2574 \n2575         See Also\n2576         --------\n2577         Dataset.tail\n2578         Dataset.thin\n2579         DataArray.head\n2580         \"\"\"\n2581         if not indexers_kwargs:\n2582             if indexers is None:\n2583                 indexers = 5\n2584             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2585                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2586         if isinstance(indexers, int):\n2587             indexers = {dim: indexers for dim in self.dims}\n2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"head\")\n2589         for k, v in indexers.items():\n2590             if not isinstance(v, int):\n2591                 raise TypeError(\n2592                     \"expected integer type indexer for \"\n2593                     f\"dimension {k!r}, found {type(v)!r}\"\n2594                 )\n2595             elif v < 0:\n2596                 raise ValueError(\n2597                     \"expected positive integer as indexer \"\n2598                     f\"for dimension {k!r}, found {v}\"\n2599                 )\n2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}\n2601         return self.isel(indexers_slices)\n2602 \n2603     def tail(\n2604         self: T_Dataset,\n2605         indexers: Mapping[Any, int] | int | None = None,\n2606         **indexers_kwargs: Any,\n2607     ) -> T_Dataset:\n2608         \"\"\"Returns a new dataset with the last `n` values of each array\n2609         for the specified dimension(s).\n2610 \n2611         Parameters\n2612         ----------\n2613         indexers : dict or int, default: 5\n2614             A dict with keys matching dimensions and integer values `n`\n2615             or a single integer `n` applied over all dimensions.\n2616             One of indexers or indexers_kwargs must be provided.\n2617         **indexers_kwargs : {dim: n, ...}, optional\n2618             The keyword arguments form of ``indexers``.\n2619             One of indexers or indexers_kwargs must be provided.\n2620 \n2621         See Also\n2622         --------\n2623         Dataset.head\n2624         Dataset.thin\n2625         DataArray.tail\n2626         \"\"\"\n2627         if not indexers_kwargs:\n2628             if indexers is None:\n2629                 indexers = 5\n2630             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2631                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2632         if isinstance(indexers, int):\n2633             indexers = {dim: indexers for dim in self.dims}\n2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"tail\")\n2635         for k, v in indexers.items():\n2636             if not isinstance(v, int):\n2637                 raise TypeError(\n2638                     \"expected integer type indexer for \"\n2639                     f\"dimension {k!r}, found {type(v)!r}\"\n2640                 )\n2641             elif v < 0:\n2642                 raise ValueError(\n2643                     \"expected positive integer as indexer \"\n2644                     f\"for dimension {k!r}, found {v}\"\n2645                 )\n2646         indexers_slices = {\n2647             k: slice(-val, None) if val != 0 else slice(val)\n2648             for k, val in indexers.items()\n2649         }\n2650         return self.isel(indexers_slices)\n2651 \n2652     def thin(\n2653         self: T_Dataset,\n2654         indexers: Mapping[Any, int] | int | None = None,\n2655         **indexers_kwargs: Any,\n2656     ) -> T_Dataset:\n2657         \"\"\"Returns a new dataset with each array indexed along every `n`-th\n2658         value for the specified dimension(s)\n2659 \n2660         Parameters\n2661         ----------\n2662         indexers : dict or int\n2663             A dict with keys matching dimensions and integer values `n`\n2664             or a single integer `n` applied over all dimensions.\n2665             One of indexers or indexers_kwargs must be provided.\n2666         **indexers_kwargs : {dim: n, ...}, optional\n2667             The keyword arguments form of ``indexers``.\n2668             One of indexers or indexers_kwargs must be provided.\n2669 \n2670         Examples\n2671         --------\n2672         >>> x_arr = np.arange(0, 26)\n2673         >>> x_arr\n2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n2675                17, 18, 19, 20, 21, 22, 23, 24, 25])\n2676         >>> x = xr.DataArray(\n2677         ...     np.reshape(x_arr, (2, 13)),\n2678         ...     dims=(\"x\", \"y\"),\n2679         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n2680         ... )\n2681         >>> x_ds = xr.Dataset({\"foo\": x})\n2682         >>> x_ds\n2683         <xarray.Dataset>\n2684         Dimensions:  (x: 2, y: 13)\n2685         Coordinates:\n2686           * x        (x) int64 0 1\n2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n2688         Data variables:\n2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25\n2690 \n2691         >>> x_ds.thin(3)\n2692         <xarray.Dataset>\n2693         Dimensions:  (x: 1, y: 5)\n2694         Coordinates:\n2695           * x        (x) int64 0\n2696           * y        (y) int64 0 3 6 9 12\n2697         Data variables:\n2698             foo      (x, y) int64 0 3 6 9 12\n2699         >>> x.thin({\"x\": 2, \"y\": 5})\n2700         <xarray.DataArray (x: 1, y: 3)>\n2701         array([[ 0,  5, 10]])\n2702         Coordinates:\n2703           * x        (x) int64 0\n2704           * y        (y) int64 0 5 10\n2705 \n2706         See Also\n2707         --------\n2708         Dataset.head\n2709         Dataset.tail\n2710         DataArray.thin\n2711         \"\"\"\n2712         if (\n2713             not indexers_kwargs\n2714             and not isinstance(indexers, int)\n2715             and not is_dict_like(indexers)\n2716         ):\n2717             raise TypeError(\"indexers must be either dict-like or a single integer\")\n2718         if isinstance(indexers, int):\n2719             indexers = {dim: indexers for dim in self.dims}\n2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"thin\")\n2721         for k, v in indexers.items():\n2722             if not isinstance(v, int):\n2723                 raise TypeError(\n2724                     \"expected integer type indexer for \"\n2725                     f\"dimension {k!r}, found {type(v)!r}\"\n2726                 )\n2727             elif v < 0:\n2728                 raise ValueError(\n2729                     \"expected positive integer as indexer \"\n2730                     f\"for dimension {k!r}, found {v}\"\n2731                 )\n2732             elif v == 0:\n2733                 raise ValueError(\"step cannot be zero\")\n2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}\n2735         return self.isel(indexers_slices)\n2736 \n2737     def broadcast_like(\n2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None\n2739     ) -> T_Dataset:\n2740         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n2741         This is equivalent to xr.broadcast(other, self)[1]\n2742 \n2743         Parameters\n2744         ----------\n2745         other : Dataset or DataArray\n2746             Object against which to broadcast this array.\n2747         exclude : iterable of hashable, optional\n2748             Dimensions that must not be broadcasted\n2749 \n2750         \"\"\"\n2751         if exclude is None:\n2752             exclude = set()\n2753         else:\n2754             exclude = set(exclude)\n2755         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n2756 \n2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n2758 \n2759         return _broadcast_helper(\n2760             cast(\"T_Dataset\", args[1]), exclude, dims_map, common_coords\n2761         )\n2762 \n2763     def _reindex_callback(\n2764         self,\n2765         aligner: alignment.Aligner,\n2766         dim_pos_indexers: dict[Hashable, Any],\n2767         variables: dict[Hashable, Variable],\n2768         indexes: dict[Hashable, Index],\n2769         fill_value: Any,\n2770         exclude_dims: frozenset[Hashable],\n2771         exclude_vars: frozenset[Hashable],\n2772     ) -> Dataset:\n2773         \"\"\"Callback called from ``Aligner`` to create a new reindexed Dataset.\"\"\"\n2774 \n2775         new_variables = variables.copy()\n2776         new_indexes = indexes.copy()\n2777 \n2778         # re-assign variable metadata\n2779         for name, new_var in new_variables.items():\n2780             var = self._variables.get(name)\n2781             if var is not None:\n2782                 new_var.attrs = var.attrs\n2783                 new_var.encoding = var.encoding\n2784 \n2785         # pass through indexes from excluded dimensions\n2786         # no extra check needed for multi-coordinate indexes, potential conflicts\n2787         # should already have been detected when aligning the indexes\n2788         for name, idx in self._indexes.items():\n2789             var = self._variables[name]\n2790             if set(var.dims) <= exclude_dims:\n2791                 new_indexes[name] = idx\n2792                 new_variables[name] = var\n2793 \n2794         if not dim_pos_indexers:\n2795             # fast path for no reindexing necessary\n2796             if set(new_indexes) - set(self._indexes):\n2797                 # this only adds new indexes and their coordinate variables\n2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)\n2799             else:\n2800                 reindexed = self.copy(deep=aligner.copy)\n2801         else:\n2802             to_reindex = {\n2803                 k: v\n2804                 for k, v in self.variables.items()\n2805                 if k not in variables and k not in exclude_vars\n2806             }\n2807             reindexed_vars = alignment.reindex_variables(\n2808                 to_reindex,\n2809                 dim_pos_indexers,\n2810                 copy=aligner.copy,\n2811                 fill_value=fill_value,\n2812                 sparse=aligner.sparse,\n2813             )\n2814             new_variables.update(reindexed_vars)\n2815             new_coord_names = self._coord_names | set(new_indexes)\n2816             reindexed = self._replace_with_new_dims(\n2817                 new_variables, new_coord_names, indexes=new_indexes\n2818             )\n2819 \n2820         return reindexed\n2821 \n2822     def reindex_like(\n2823         self: T_Dataset,\n2824         other: Dataset | DataArray,\n2825         method: ReindexMethodOptions = None,\n2826         tolerance: int | float | Iterable[int | float] | None = None,\n2827         copy: bool = True,\n2828         fill_value: Any = xrdtypes.NA,\n2829     ) -> T_Dataset:\n2830         \"\"\"Conform this object onto the indexes of another object, filling in\n2831         missing values with ``fill_value``. The default fill value is NaN.\n2832 \n2833         Parameters\n2834         ----------\n2835         other : Dataset or DataArray\n2836             Object with an 'indexes' attribute giving a mapping from dimension\n2837             names to pandas.Index objects, which provides coordinates upon\n2838             which to index the variables in this dataset. The indexes on this\n2839             other object need not be the same as the indexes on this\n2840             dataset. Any mis-matched index values will be filled in with\n2841             NaN, and any mis-matched dimension names will simply be ignored.\n2842         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n2843             Method to use for filling index values from other not found in this\n2844             dataset:\n2845 \n2846             - None (default): don't fill gaps\n2847             - \"pad\" / \"ffill\": propagate last valid index value forward\n2848             - \"backfill\" / \"bfill\": propagate next valid index value backward\n2849             - \"nearest\": use nearest valid index value\n2850 \n2851         tolerance : optional\n2852             Maximum distance between original and new labels for inexact\n2853             matches. The values of the index at the matching locations must\n2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2855             Tolerance may be a scalar value, which applies the same tolerance\n2856             to all values, or list-like, which applies variable tolerance per\n2857             element. List-like must be the same size as the index and its dtype\n2858             must exactly match the index\u2019s type.\n2859         copy : bool, default: True\n2860             If ``copy=True``, data in the return value is always copied. If\n2861             ``copy=False`` and reindexing is unnecessary, or can be performed\n2862             with only slice operations, then the output may share memory with\n2863             the input. In either case, a new xarray object is always returned.\n2864         fill_value : scalar or dict-like, optional\n2865             Value to use for newly missing values. If a dict-like maps\n2866             variable names to fill values.\n2867 \n2868         Returns\n2869         -------\n2870         reindexed : Dataset\n2871             Another dataset, with this dataset's data but coordinates from the\n2872             other object.\n2873 \n2874         See Also\n2875         --------\n2876         Dataset.reindex\n2877         align\n2878         \"\"\"\n2879         return alignment.reindex_like(\n2880             self,\n2881             other=other,\n2882             method=method,\n2883             tolerance=tolerance,\n2884             copy=copy,\n2885             fill_value=fill_value,\n2886         )\n2887 \n2888     def reindex(\n2889         self: T_Dataset,\n2890         indexers: Mapping[Any, Any] | None = None,\n2891         method: ReindexMethodOptions = None,\n2892         tolerance: int | float | Iterable[int | float] | None = None,\n2893         copy: bool = True,\n2894         fill_value: Any = xrdtypes.NA,\n2895         **indexers_kwargs: Any,\n2896     ) -> T_Dataset:\n2897         \"\"\"Conform this object onto a new set of indexes, filling in\n2898         missing values with ``fill_value``. The default fill value is NaN.\n2899 \n2900         Parameters\n2901         ----------\n2902         indexers : dict, optional\n2903             Dictionary with keys given by dimension names and values given by\n2904             arrays of coordinates tick labels. Any mis-matched coordinate\n2905             values will be filled in with NaN, and any mis-matched dimension\n2906             names will simply be ignored.\n2907             One of indexers or indexers_kwargs must be provided.\n2908         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n2909             Method to use for filling index values in ``indexers`` not found in\n2910             this dataset:\n2911 \n2912             - None (default): don't fill gaps\n2913             - \"pad\" / \"ffill\": propagate last valid index value forward\n2914             - \"backfill\" / \"bfill\": propagate next valid index value backward\n2915             - \"nearest\": use nearest valid index value\n2916 \n2917         tolerance : optional\n2918             Maximum distance between original and new labels for inexact\n2919             matches. The values of the index at the matching locations must\n2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2921             Tolerance may be a scalar value, which applies the same tolerance\n2922             to all values, or list-like, which applies variable tolerance per\n2923             element. List-like must be the same size as the index and its dtype\n2924             must exactly match the index\u2019s type.\n2925         copy : bool, default: True\n2926             If ``copy=True``, data in the return value is always copied. If\n2927             ``copy=False`` and reindexing is unnecessary, or can be performed\n2928             with only slice operations, then the output may share memory with\n2929             the input. In either case, a new xarray object is always returned.\n2930         fill_value : scalar or dict-like, optional\n2931             Value to use for newly missing values. If a dict-like,\n2932             maps variable names (including coordinates) to fill values.\n2933         sparse : bool, default: False\n2934             use sparse-array.\n2935         **indexers_kwargs : {dim: indexer, ...}, optional\n2936             Keyword arguments in the same form as ``indexers``.\n2937             One of indexers or indexers_kwargs must be provided.\n2938 \n2939         Returns\n2940         -------\n2941         reindexed : Dataset\n2942             Another dataset, with this dataset's data but replaced coordinates.\n2943 \n2944         See Also\n2945         --------\n2946         Dataset.reindex_like\n2947         align\n2948         pandas.Index.get_indexer\n2949 \n2950         Examples\n2951         --------\n2952         Create a dataset with some fictional data.\n2953 \n2954         >>> x = xr.Dataset(\n2955         ...     {\n2956         ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n2957         ...         \"pressure\": (\"station\", 500 * np.random.rand(4)),\n2958         ...     },\n2959         ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]},\n2960         ... )\n2961         >>> x\n2962         <xarray.Dataset>\n2963         Dimensions:      (station: 4)\n2964         Coordinates:\n2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n2966         Data variables:\n2967             temperature  (station) float64 10.98 14.3 12.06 10.9\n2968             pressure     (station) float64 211.8 322.9 218.8 445.9\n2969         >>> x.indexes\n2970         Indexes:\n2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n2972 \n2973         Create a new index and reindex the dataset. By default values in the new index that\n2974         do not have corresponding records in the dataset are assigned `NaN`.\n2975 \n2976         >>> new_index = [\"boston\", \"austin\", \"seattle\", \"lincoln\"]\n2977         >>> x.reindex({\"station\": new_index})\n2978         <xarray.Dataset>\n2979         Dimensions:      (station: 4)\n2980         Coordinates:\n2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n2982         Data variables:\n2983             temperature  (station) float64 10.98 nan 12.06 nan\n2984             pressure     (station) float64 211.8 nan 218.8 nan\n2985 \n2986         We can fill in the missing values by passing a value to the keyword `fill_value`.\n2987 \n2988         >>> x.reindex({\"station\": new_index}, fill_value=0)\n2989         <xarray.Dataset>\n2990         Dimensions:      (station: 4)\n2991         Coordinates:\n2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n2993         Data variables:\n2994             temperature  (station) float64 10.98 0.0 12.06 0.0\n2995             pressure     (station) float64 211.8 0.0 218.8 0.0\n2996 \n2997         We can also use different fill values for each variable.\n2998 \n2999         >>> x.reindex(\n3000         ...     {\"station\": new_index}, fill_value={\"temperature\": 0, \"pressure\": 100}\n3001         ... )\n3002         <xarray.Dataset>\n3003         Dimensions:      (station: 4)\n3004         Coordinates:\n3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3006         Data variables:\n3007             temperature  (station) float64 10.98 0.0 12.06 0.0\n3008             pressure     (station) float64 211.8 100.0 218.8 100.0\n3009 \n3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments\n3011         to the keyword method to fill the `NaN` values.\n3012 \n3013         >>> x.reindex({\"station\": new_index}, method=\"nearest\")\n3014         Traceback (most recent call last):\n3015         ...\n3016             raise ValueError('index must be monotonic increasing or decreasing')\n3017         ValueError: index must be monotonic increasing or decreasing\n3018 \n3019         To further illustrate the filling functionality in reindex, we will create a\n3020         dataset with a monotonically increasing index (for example, a sequence of dates).\n3021 \n3022         >>> x2 = xr.Dataset(\n3023         ...     {\n3024         ...         \"temperature\": (\n3025         ...             \"time\",\n3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n3027         ...         ),\n3028         ...         \"pressure\": (\"time\", 500 * np.random.rand(6)),\n3029         ...     },\n3030         ...     coords={\"time\": pd.date_range(\"01/01/2019\", periods=6, freq=\"D\")},\n3031         ... )\n3032         >>> x2\n3033         <xarray.Dataset>\n3034         Dimensions:      (time: 6)\n3035         Coordinates:\n3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n3037         Data variables:\n3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8\n3040 \n3041         Suppose we decide to expand the dataset to cover a wider date range.\n3042 \n3043         >>> time_index2 = pd.date_range(\"12/29/2018\", periods=10, freq=\"D\")\n3044         >>> x2.reindex({\"time\": time_index2})\n3045         <xarray.Dataset>\n3046         Dimensions:      (time: 10)\n3047         Coordinates:\n3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n3049         Data variables:\n3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan\n3052 \n3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n3055 \n3056         For example, to back-propagate the last valid value to fill the `NaN` values,\n3057         pass `bfill` as an argument to the `method` keyword.\n3058 \n3059         >>> x3 = x2.reindex({\"time\": time_index2}, method=\"bfill\")\n3060         >>> x3\n3061         <xarray.Dataset>\n3062         Dimensions:      (time: 10)\n3063         Coordinates:\n3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n3065         Data variables:\n3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan\n3068 \n3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n3070         will not be filled by any of the value propagation schemes.\n3071 \n3072         >>> x2.where(x2.temperature.isnull(), drop=True)\n3073         <xarray.Dataset>\n3074         Dimensions:      (time: 1)\n3075         Coordinates:\n3076           * time         (time) datetime64[ns] 2019-01-03\n3077         Data variables:\n3078             temperature  (time) float64 nan\n3079             pressure     (time) float64 395.9\n3080         >>> x3.where(x3.temperature.isnull(), drop=True)\n3081         <xarray.Dataset>\n3082         Dimensions:      (time: 2)\n3083         Coordinates:\n3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n3085         Data variables:\n3086             temperature  (time) float64 nan nan\n3087             pressure     (time) float64 395.9 nan\n3088 \n3089         This is because filling while reindexing does not look at dataset values, but only compares\n3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the\n3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.\n3092 \n3093         \"\"\"\n3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n3095         return alignment.reindex(\n3096             self,\n3097             indexers=indexers,\n3098             method=method,\n3099             tolerance=tolerance,\n3100             copy=copy,\n3101             fill_value=fill_value,\n3102         )\n3103 \n3104     def _reindex(\n3105         self: T_Dataset,\n3106         indexers: Mapping[Any, Any] = None,\n3107         method: str = None,\n3108         tolerance: int | float | Iterable[int | float] | None = None,\n3109         copy: bool = True,\n3110         fill_value: Any = xrdtypes.NA,\n3111         sparse: bool = False,\n3112         **indexers_kwargs: Any,\n3113     ) -> T_Dataset:\n3114         \"\"\"\n3115         Same as reindex but supports sparse option.\n3116         \"\"\"\n3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n3118         return alignment.reindex(\n3119             self,\n3120             indexers=indexers,\n3121             method=method,\n3122             tolerance=tolerance,\n3123             copy=copy,\n3124             fill_value=fill_value,\n3125             sparse=sparse,\n3126         )\n3127 \n3128     def interp(\n3129         self: T_Dataset,\n3130         coords: Mapping[Any, Any] | None = None,\n3131         method: InterpOptions = \"linear\",\n3132         assume_sorted: bool = False,\n3133         kwargs: Mapping[str, Any] = None,\n3134         method_non_numeric: str = \"nearest\",\n3135         **coords_kwargs: Any,\n3136     ) -> T_Dataset:\n3137         \"\"\"Interpolate a Dataset onto new coordinates\n3138 \n3139         Performs univariate or multivariate interpolation of a Dataset onto\n3140         new coordinates using scipy's interpolation routines. If interpolating\n3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n3142         called.  When interpolating along multiple existing dimensions, an\n3143         attempt is made to decompose the interpolation into multiple\n3144         1-dimensional interpolations. If this is possible,\n3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n3146         :py:func:`scipy.interpolate.interpn` is called.\n3147 \n3148         Parameters\n3149         ----------\n3150         coords : dict, optional\n3151             Mapping from dimension names to the new coordinates.\n3152             New coordinate can be a scalar, array-like or DataArray.\n3153             If DataArrays are passed as new coordinates, their dimensions are\n3154             used for the broadcasting. Missing values are skipped.\n3155         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3156             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3157             String indicating which method to use for interpolation:\n3158 \n3159             - 'linear': linear interpolation. Additional keyword\n3160               arguments are passed to :py:func:`numpy.interp`\n3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3162               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3163               ``method='polynomial'``, the ``order`` keyword argument must also be\n3164               provided.\n3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3166               respective :py:class:`scipy.interpolate` classes.\n3167 \n3168         assume_sorted : bool, default: False\n3169             If False, values of coordinates that are interpolated over can be\n3170             in any order and they are sorted first. If True, interpolated\n3171             coordinates are assumed to be an array of monotonically increasing\n3172             values.\n3173         kwargs : dict, optional\n3174             Additional keyword arguments passed to scipy's interpolator. Valid\n3175             options and their behavior depend whether ``interp1d`` or\n3176             ``interpn`` is used.\n3177         method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n3179             ``\"nearest\"`` is used by default.\n3180         **coords_kwargs : {dim: coordinate, ...}, optional\n3181             The keyword arguments form of ``coords``.\n3182             One of coords or coords_kwargs must be provided.\n3183 \n3184         Returns\n3185         -------\n3186         interpolated : Dataset\n3187             New dataset on the new coordinates.\n3188 \n3189         Notes\n3190         -----\n3191         scipy is required.\n3192 \n3193         See Also\n3194         --------\n3195         scipy.interpolate.interp1d\n3196         scipy.interpolate.interpn\n3197 \n3198         Examples\n3199         --------\n3200         >>> ds = xr.Dataset(\n3201         ...     data_vars={\n3202         ...         \"a\": (\"x\", [5, 7, 4]),\n3203         ...         \"b\": (\n3204         ...             (\"x\", \"y\"),\n3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n3206         ...         ),\n3207         ...     },\n3208         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n3209         ... )\n3210         >>> ds\n3211         <xarray.Dataset>\n3212         Dimensions:  (x: 3, y: 4)\n3213         Coordinates:\n3214           * x        (x) int64 0 1 2\n3215           * y        (y) int64 10 12 14 16\n3216         Data variables:\n3217             a        (x) int64 5 7 4\n3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0\n3219 \n3220         1D interpolation with the default method (linear):\n3221 \n3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])\n3223         <xarray.Dataset>\n3224         Dimensions:  (x: 4, y: 4)\n3225         Coordinates:\n3226           * y        (y) int64 10 12 14 16\n3227           * x        (x) float64 0.0 0.75 1.25 1.75\n3228         Data variables:\n3229             a        (x) float64 5.0 6.5 6.25 4.75\n3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan\n3231 \n3232         1D interpolation with a different method:\n3233 \n3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n3235         <xarray.Dataset>\n3236         Dimensions:  (x: 4, y: 4)\n3237         Coordinates:\n3238           * y        (y) int64 10 12 14 16\n3239           * x        (x) float64 0.0 0.75 1.25 1.75\n3240         Data variables:\n3241             a        (x) float64 5.0 7.0 7.0 4.0\n3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0\n3243 \n3244         1D extrapolation:\n3245 \n3246         >>> ds.interp(\n3247         ...     x=[1, 1.5, 2.5, 3.5],\n3248         ...     method=\"linear\",\n3249         ...     kwargs={\"fill_value\": \"extrapolate\"},\n3250         ... )\n3251         <xarray.Dataset>\n3252         Dimensions:  (x: 4, y: 4)\n3253         Coordinates:\n3254           * y        (y) int64 10 12 14 16\n3255           * x        (x) float64 1.0 1.5 2.5 3.5\n3256         Data variables:\n3257             a        (x) float64 7.0 5.5 2.5 -0.5\n3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan\n3259 \n3260         2D interpolation:\n3261 \n3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n3263         <xarray.Dataset>\n3264         Dimensions:  (x: 4, y: 3)\n3265         Coordinates:\n3266           * x        (x) float64 0.0 0.75 1.25 1.75\n3267           * y        (y) int64 11 13 15\n3268         Data variables:\n3269             a        (x) float64 5.0 6.5 6.25 4.75\n3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan\n3271         \"\"\"\n3272         from . import missing\n3273 \n3274         if kwargs is None:\n3275             kwargs = {}\n3276 \n3277         coords = either_dict_or_kwargs(coords, coords_kwargs, \"interp\")\n3278         indexers = dict(self._validate_interp_indexers(coords))\n3279 \n3280         if coords:\n3281             # This avoids broadcasting over coordinates that are both in\n3282             # the original array AND in the indexing array. It essentially\n3283             # forces interpolation along the shared coordinates.\n3284             sdims = (\n3285                 set(self.dims)\n3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])\n3287                 .difference(coords.keys())\n3288             )\n3289             indexers.update({d: self.variables[d] for d in sdims})\n3290 \n3291         obj = self if assume_sorted else self.sortby([k for k in coords])\n3292 \n3293         def maybe_variable(obj, k):\n3294             # workaround to get variable for dimension without coordinate.\n3295             try:\n3296                 return obj._variables[k]\n3297             except KeyError:\n3298                 return as_variable((k, range(obj.dims[k])))\n3299 \n3300         def _validate_interp_indexer(x, new_x):\n3301             # In the case of datetimes, the restrictions placed on indexers\n3302             # used with interp are stronger than those which are placed on\n3303             # isel, so we need an additional check after _validate_indexers.\n3304             if _contains_datetime_like_objects(\n3305                 x\n3306             ) and not _contains_datetime_like_objects(new_x):\n3307                 raise TypeError(\n3308                     \"When interpolating over a datetime-like \"\n3309                     \"coordinate, the coordinates to \"\n3310                     \"interpolate to must be either datetime \"\n3311                     \"strings or datetimes. \"\n3312                     \"Instead got\\n{}\".format(new_x)\n3313                 )\n3314             return x, new_x\n3315 \n3316         validated_indexers = {\n3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)\n3318             for k, v in indexers.items()\n3319         }\n3320 \n3321         # optimization: subset to coordinate range of the target index\n3322         if method in [\"linear\", \"nearest\"]:\n3323             for k, v in validated_indexers.items():\n3324                 obj, newidx = missing._localize(obj, {k: v})\n3325                 validated_indexers[k] = newidx[k]\n3326 \n3327         # optimization: create dask coordinate arrays once per Dataset\n3328         # rather than once per Variable when dask.array.unify_chunks is called later\n3329         # GH4739\n3330         if obj.__dask_graph__():\n3331             dask_indexers = {\n3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())\n3333                 for k, (index, dest) in validated_indexers.items()\n3334             }\n3335 \n3336         variables: dict[Hashable, Variable] = {}\n3337         reindex: bool = False\n3338         for name, var in obj._variables.items():\n3339             if name in indexers:\n3340                 continue\n3341 \n3342             if is_duck_dask_array(var.data):\n3343                 use_indexers = dask_indexers\n3344             else:\n3345                 use_indexers = validated_indexers\n3346 \n3347             dtype_kind = var.dtype.kind\n3348             if dtype_kind in \"uifc\":\n3349                 # For normal number types do the interpolation:\n3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}\n3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)\n3352             elif dtype_kind in \"ObU\" and (use_indexers.keys() & var.dims):\n3353                 # For types that we do not understand do stepwise\n3354                 # interpolation to avoid modifying the elements.\n3355                 # reindex the variable instead because it supports\n3356                 # booleans and objects and retains the dtype but inside\n3357                 # this loop there might be some duplicate code that slows it\n3358                 # down, therefore collect these signals and run it later:\n3359                 reindex = True\n3360             elif all(d not in indexers for d in var.dims):\n3361                 # For anything else we can only keep variables if they\n3362                 # are not dependent on any coords that are being\n3363                 # interpolated along:\n3364                 variables[name] = var\n3365 \n3366         if reindex:\n3367             reindex_indexers = {\n3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)\n3369             }\n3370             reindexed = alignment.reindex(\n3371                 obj,\n3372                 indexers=reindex_indexers,\n3373                 method=method_non_numeric,\n3374                 exclude_vars=variables.keys(),\n3375             )\n3376             indexes = dict(reindexed._indexes)\n3377             variables.update(reindexed.variables)\n3378         else:\n3379             # Get the indexes that are not being interpolated along\n3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}\n3381 \n3382         # Get the coords that also exist in the variables:\n3383         coord_names = obj._coord_names & variables.keys()\n3384         selected = self._replace_with_new_dims(\n3385             variables.copy(), coord_names, indexes=indexes\n3386         )\n3387 \n3388         # Attach indexer as coordinate\n3389         for k, v in indexers.items():\n3390             assert isinstance(v, Variable)\n3391             if v.dims == (k,):\n3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)\n3393                 index_vars = index.create_variables({k: v})\n3394                 indexes[k] = index\n3395                 variables.update(index_vars)\n3396             else:\n3397                 variables[k] = v\n3398 \n3399         # Extract coordinates from indexers\n3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)\n3401         variables.update(coord_vars)\n3402         indexes.update(new_indexes)\n3403 \n3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()\n3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3406 \n3407     def interp_like(\n3408         self,\n3409         other: Dataset | DataArray,\n3410         method: InterpOptions = \"linear\",\n3411         assume_sorted: bool = False,\n3412         kwargs: Mapping[str, Any] | None = None,\n3413         method_non_numeric: str = \"nearest\",\n3414     ) -> Dataset:\n3415         \"\"\"Interpolate this object onto the coordinates of another object,\n3416         filling the out of range values with NaN.\n3417 \n3418         If interpolating along a single existing dimension,\n3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n3420         along multiple existing dimensions, an attempt is made to decompose the\n3421         interpolation into multiple 1-dimensional interpolations. If this is\n3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n3423         :py:func:`scipy.interpolate.interpn` is called.\n3424 \n3425         Parameters\n3426         ----------\n3427         other : Dataset or DataArray\n3428             Object with an 'indexes' attribute giving a mapping from dimension\n3429             names to an 1d array-like, which provides coordinates upon\n3430             which to index the variables in this dataset. Missing values are skipped.\n3431         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3432             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3433             String indicating which method to use for interpolation:\n3434 \n3435             - 'linear': linear interpolation. Additional keyword\n3436               arguments are passed to :py:func:`numpy.interp`\n3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3438               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3439               ``method='polynomial'``, the ``order`` keyword argument must also be\n3440               provided.\n3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3442               respective :py:class:`scipy.interpolate` classes.\n3443 \n3444         assume_sorted : bool, default: False\n3445             If False, values of coordinates that are interpolated over can be\n3446             in any order and they are sorted first. If True, interpolated\n3447             coordinates are assumed to be an array of monotonically increasing\n3448             values.\n3449         kwargs : dict, optional\n3450             Additional keyword passed to scipy's interpolator.\n3451         method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n3453             ``\"nearest\"`` is used by default.\n3454 \n3455         Returns\n3456         -------\n3457         interpolated : Dataset\n3458             Another dataset by interpolating this dataset's data along the\n3459             coordinates of the other object.\n3460 \n3461         Notes\n3462         -----\n3463         scipy is required.\n3464         If the dataset has object-type coordinates, reindex is used for these\n3465         coordinates instead of the interpolation.\n3466 \n3467         See Also\n3468         --------\n3469         Dataset.interp\n3470         Dataset.reindex_like\n3471         \"\"\"\n3472         if kwargs is None:\n3473             kwargs = {}\n3474 \n3475         # pick only dimension coordinates with a single index\n3476         coords = {}\n3477         other_indexes = other.xindexes\n3478         for dim in self.dims:\n3479             other_dim_coords = other_indexes.get_all_coords(dim, errors=\"ignore\")\n3480             if len(other_dim_coords) == 1:\n3481                 coords[dim] = other_dim_coords[dim]\n3482 \n3483         numeric_coords: dict[Hashable, pd.Index] = {}\n3484         object_coords: dict[Hashable, pd.Index] = {}\n3485         for k, v in coords.items():\n3486             if v.dtype.kind in \"uifcMm\":\n3487                 numeric_coords[k] = v\n3488             else:\n3489                 object_coords[k] = v\n3490 \n3491         ds = self\n3492         if object_coords:\n3493             # We do not support interpolation along object coordinate.\n3494             # reindex instead.\n3495             ds = self.reindex(object_coords)\n3496         return ds.interp(\n3497             coords=numeric_coords,\n3498             method=method,\n3499             assume_sorted=assume_sorted,\n3500             kwargs=kwargs,\n3501             method_non_numeric=method_non_numeric,\n3502         )\n3503 \n3504     # Helper methods for rename()\n3505     def _rename_vars(\n3506         self, name_dict, dims_dict\n3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:\n3508         variables = {}\n3509         coord_names = set()\n3510         for k, v in self.variables.items():\n3511             var = v.copy(deep=False)\n3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3513             name = name_dict.get(k, k)\n3514             if name in variables:\n3515                 raise ValueError(f\"the new name {name!r} conflicts\")\n3516             variables[name] = var\n3517             if k in self._coord_names:\n3518                 coord_names.add(name)\n3519         return variables, coord_names\n3520 \n3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:\n3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}\n3523 \n3524     def _rename_indexes(\n3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]\n3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n3527         if not self._indexes:\n3528             return {}, {}\n3529 \n3530         indexes = {}\n3531         variables = {}\n3532 \n3533         for index, coord_names in self.xindexes.group_by_index():\n3534             new_index = index.rename(name_dict, dims_dict)\n3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]\n3536             indexes.update({k: new_index for k in new_coord_names})\n3537             new_index_vars = new_index.create_variables(\n3538                 {\n3539                     new: self._variables[old]\n3540                     for old, new in zip(coord_names, new_coord_names)\n3541                 }\n3542             )\n3543             variables.update(new_index_vars)\n3544 \n3545         return indexes, variables\n3546 \n3547     def _rename_all(\n3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]\n3549     ) -> tuple[\n3550         dict[Hashable, Variable],\n3551         set[Hashable],\n3552         dict[Hashable, int],\n3553         dict[Hashable, Index],\n3554     ]:\n3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)\n3556         dims = self._rename_dims(dims_dict)\n3557 \n3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)\n3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}\n3560 \n3561         return variables, coord_names, dims, indexes\n3562 \n3563     def rename(\n3564         self: T_Dataset,\n3565         name_dict: Mapping[Any, Hashable] | None = None,\n3566         **names: Hashable,\n3567     ) -> T_Dataset:\n3568         \"\"\"Returns a new object with renamed variables, coordinates and dimensions.\n3569 \n3570         Parameters\n3571         ----------\n3572         name_dict : dict-like, optional\n3573             Dictionary whose keys are current variable, coordinate or dimension names and\n3574             whose values are the desired names.\n3575         **names : optional\n3576             Keyword form of ``name_dict``.\n3577             One of name_dict or names must be provided.\n3578 \n3579         Returns\n3580         -------\n3581         renamed : Dataset\n3582             Dataset with renamed variables, coordinates and dimensions.\n3583 \n3584         See Also\n3585         --------\n3586         Dataset.swap_dims\n3587         Dataset.rename_vars\n3588         Dataset.rename_dims\n3589         DataArray.rename\n3590         \"\"\"\n3591         name_dict = either_dict_or_kwargs(name_dict, names, \"rename\")\n3592         for k in name_dict.keys():\n3593             if k not in self and k not in self.dims:\n3594                 raise ValueError(\n3595                     f\"cannot rename {k!r} because it is not a \"\n3596                     \"variable or dimension in this dataset\"\n3597                 )\n3598 \n3599         variables, coord_names, dims, indexes = self._rename_all(\n3600             name_dict=name_dict, dims_dict=name_dict\n3601         )\n3602         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3603 \n3604     def rename_dims(\n3605         self: T_Dataset,\n3606         dims_dict: Mapping[Any, Hashable] | None = None,\n3607         **dims: Hashable,\n3608     ) -> T_Dataset:\n3609         \"\"\"Returns a new object with renamed dimensions only.\n3610 \n3611         Parameters\n3612         ----------\n3613         dims_dict : dict-like, optional\n3614             Dictionary whose keys are current dimension names and\n3615             whose values are the desired names. The desired names must\n3616             not be the name of an existing dimension or Variable in the Dataset.\n3617         **dims : optional\n3618             Keyword form of ``dims_dict``.\n3619             One of dims_dict or dims must be provided.\n3620 \n3621         Returns\n3622         -------\n3623         renamed : Dataset\n3624             Dataset with renamed dimensions.\n3625 \n3626         See Also\n3627         --------\n3628         Dataset.swap_dims\n3629         Dataset.rename\n3630         Dataset.rename_vars\n3631         DataArray.rename\n3632         \"\"\"\n3633         dims_dict = either_dict_or_kwargs(dims_dict, dims, \"rename_dims\")\n3634         for k, v in dims_dict.items():\n3635             if k not in self.dims:\n3636                 raise ValueError(\n3637                     f\"cannot rename {k!r} because it is not a \"\n3638                     \"dimension in this dataset\"\n3639                 )\n3640             if v in self.dims or v in self:\n3641                 raise ValueError(\n3642                     f\"Cannot rename {k} to {v} because {v} already exists. \"\n3643                     \"Try using swap_dims instead.\"\n3644                 )\n3645 \n3646         variables, coord_names, sizes, indexes = self._rename_all(\n3647             name_dict={}, dims_dict=dims_dict\n3648         )\n3649         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)\n3650 \n3651     def rename_vars(\n3652         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable\n3653     ) -> T_Dataset:\n3654         \"\"\"Returns a new object with renamed variables including coordinates\n3655 \n3656         Parameters\n3657         ----------\n3658         name_dict : dict-like, optional\n3659             Dictionary whose keys are current variable or coordinate names and\n3660             whose values are the desired names.\n3661         **names : optional\n3662             Keyword form of ``name_dict``.\n3663             One of name_dict or names must be provided.\n3664 \n3665         Returns\n3666         -------\n3667         renamed : Dataset\n3668             Dataset with renamed variables including coordinates\n3669 \n3670         See Also\n3671         --------\n3672         Dataset.swap_dims\n3673         Dataset.rename\n3674         Dataset.rename_dims\n3675         DataArray.rename\n3676         \"\"\"\n3677         name_dict = either_dict_or_kwargs(name_dict, names, \"rename_vars\")\n3678         for k in name_dict:\n3679             if k not in self:\n3680                 raise ValueError(\n3681                     f\"cannot rename {k!r} because it is not a \"\n3682                     \"variable or coordinate in this dataset\"\n3683                 )\n3684         variables, coord_names, dims, indexes = self._rename_all(\n3685             name_dict=name_dict, dims_dict={}\n3686         )\n3687         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3688 \n3689     def swap_dims(\n3690         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs\n3691     ) -> T_Dataset:\n3692         \"\"\"Returns a new object with swapped dimensions.\n3693 \n3694         Parameters\n3695         ----------\n3696         dims_dict : dict-like\n3697             Dictionary whose keys are current dimension names and whose values\n3698             are new names.\n3699         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n3700             The keyword arguments form of ``dims_dict``.\n3701             One of dims_dict or dims_kwargs must be provided.\n3702 \n3703         Returns\n3704         -------\n3705         swapped : Dataset\n3706             Dataset with swapped dimensions.\n3707 \n3708         Examples\n3709         --------\n3710         >>> ds = xr.Dataset(\n3711         ...     data_vars={\"a\": (\"x\", [5, 7]), \"b\": (\"x\", [0.1, 2.4])},\n3712         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n3713         ... )\n3714         >>> ds\n3715         <xarray.Dataset>\n3716         Dimensions:  (x: 2)\n3717         Coordinates:\n3718           * x        (x) <U1 'a' 'b'\n3719             y        (x) int64 0 1\n3720         Data variables:\n3721             a        (x) int64 5 7\n3722             b        (x) float64 0.1 2.4\n3723 \n3724         >>> ds.swap_dims({\"x\": \"y\"})\n3725         <xarray.Dataset>\n3726         Dimensions:  (y: 2)\n3727         Coordinates:\n3728             x        (y) <U1 'a' 'b'\n3729           * y        (y) int64 0 1\n3730         Data variables:\n3731             a        (y) int64 5 7\n3732             b        (y) float64 0.1 2.4\n3733 \n3734         >>> ds.swap_dims({\"x\": \"z\"})\n3735         <xarray.Dataset>\n3736         Dimensions:  (z: 2)\n3737         Coordinates:\n3738             x        (z) <U1 'a' 'b'\n3739             y        (z) int64 0 1\n3740         Dimensions without coordinates: z\n3741         Data variables:\n3742             a        (z) int64 5 7\n3743             b        (z) float64 0.1 2.4\n3744 \n3745         See Also\n3746         --------\n3747         Dataset.rename\n3748         DataArray.swap_dims\n3749         \"\"\"\n3750         # TODO: deprecate this method in favor of a (less confusing)\n3751         # rename_dims() method that only renames dimensions.\n3752 \n3753         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n3754         for k, v in dims_dict.items():\n3755             if k not in self.dims:\n3756                 raise ValueError(\n3757                     f\"cannot swap from dimension {k!r} because it is \"\n3758                     \"not an existing dimension\"\n3759                 )\n3760             if v in self.variables and self.variables[v].dims != (k,):\n3761                 raise ValueError(\n3762                     f\"replacement dimension {v!r} is not a 1D \"\n3763                     f\"variable along the old dimension {k!r}\"\n3764                 )\n3765 \n3766         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}\n3767 \n3768         coord_names = self._coord_names.copy()\n3769         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})\n3770 \n3771         variables: dict[Hashable, Variable] = {}\n3772         indexes: dict[Hashable, Index] = {}\n3773         for k, v in self.variables.items():\n3774             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3775             var: Variable\n3776             if k in result_dims:\n3777                 var = v.to_index_variable()\n3778                 var.dims = dims\n3779                 if k in self._indexes:\n3780                     indexes[k] = self._indexes[k]\n3781                     variables[k] = var\n3782                 else:\n3783                     index, index_vars = create_default_index_implicit(var)\n3784                     indexes.update({name: index for name in index_vars})\n3785                     variables.update(index_vars)\n3786                     coord_names.update(index_vars)\n3787             else:\n3788                 var = v.to_base_variable()\n3789                 var.dims = dims\n3790                 variables[k] = var\n3791 \n3792         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3793 \n3794     # change type of self and return to T_Dataset once\n3795     # https://github.com/python/mypy/issues/12846 is resolved\n3796     def expand_dims(\n3797         self,\n3798         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n3799         axis: None | int | Sequence[int] = None,\n3800         **dim_kwargs: Any,\n3801     ) -> Dataset:\n3802         \"\"\"Return a new object with an additional axis (or axes) inserted at\n3803         the corresponding position in the array shape.  The new object is a\n3804         view into the underlying array, not a copy.\n3805 \n3806         If dim is already a scalar coordinate, it will be promoted to a 1D\n3807         coordinate consisting of a single value.\n3808 \n3809         Parameters\n3810         ----------\n3811         dim : hashable, sequence of hashable, mapping, or None\n3812             Dimensions to include on the new variable. If provided as hashable\n3813             or sequence of hashable, then dimensions are inserted with length\n3814             1. If provided as a mapping, then the keys are the new dimensions\n3815             and the values are either integers (giving the length of the new\n3816             dimensions) or array-like (giving the coordinates of the new\n3817             dimensions).\n3818         axis : int, sequence of int, or None, default: None\n3819             Axis position(s) where new axis is to be inserted (position(s) on\n3820             the result array). If a sequence of integers is passed,\n3821             multiple axes are inserted. In this case, dim arguments should be\n3822             same length list. If axis=None is passed, all the axes will be\n3823             inserted to the start of the result array.\n3824         **dim_kwargs : int or sequence or ndarray\n3825             The keywords are arbitrary dimensions being inserted and the values\n3826             are either the lengths of the new dims (if int is given), or their\n3827             coordinates. Note, this is an alternative to passing a dict to the\n3828             dim kwarg and will only be used if dim is None.\n3829 \n3830         Returns\n3831         -------\n3832         expanded : Dataset\n3833             This object, but with additional dimension(s).\n3834 \n3835         See Also\n3836         --------\n3837         DataArray.expand_dims\n3838         \"\"\"\n3839         if dim is None:\n3840             pass\n3841         elif isinstance(dim, Mapping):\n3842             # We're later going to modify dim in place; don't tamper with\n3843             # the input\n3844             dim = dict(dim)\n3845         elif isinstance(dim, int):\n3846             raise TypeError(\n3847                 \"dim should be hashable or sequence of hashables or mapping\"\n3848             )\n3849         elif isinstance(dim, str) or not isinstance(dim, Sequence):\n3850             dim = {dim: 1}\n3851         elif isinstance(dim, Sequence):\n3852             if len(dim) != len(set(dim)):\n3853                 raise ValueError(\"dims should not contain duplicate values.\")\n3854             dim = {d: 1 for d in dim}\n3855 \n3856         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n3857         assert isinstance(dim, MutableMapping)\n3858 \n3859         if axis is None:\n3860             axis = list(range(len(dim)))\n3861         elif not isinstance(axis, Sequence):\n3862             axis = [axis]\n3863 \n3864         if len(dim) != len(axis):\n3865             raise ValueError(\"lengths of dim and axis should be identical.\")\n3866         for d in dim:\n3867             if d in self.dims:\n3868                 raise ValueError(f\"Dimension {d} already exists.\")\n3869             if d in self._variables and not utils.is_scalar(self._variables[d]):\n3870                 raise ValueError(\n3871                     \"{dim} already exists as coordinate or\"\n3872                     \" variable name.\".format(dim=d)\n3873                 )\n3874 \n3875         variables: dict[Hashable, Variable] = {}\n3876         indexes: dict[Hashable, Index] = dict(self._indexes)\n3877         coord_names = self._coord_names.copy()\n3878         # If dim is a dict, then ensure that the values are either integers\n3879         # or iterables.\n3880         for k, v in dim.items():\n3881             if hasattr(v, \"__iter__\"):\n3882                 # If the value for the new dimension is an iterable, then\n3883                 # save the coordinates to the variables dict, and set the\n3884                 # value within the dim dict to the length of the iterable\n3885                 # for later use.\n3886                 index = PandasIndex(v, k)\n3887                 indexes[k] = index\n3888                 variables.update(index.create_variables())\n3889                 coord_names.add(k)\n3890                 dim[k] = variables[k].size\n3891             elif isinstance(v, int):\n3892                 pass  # Do nothing if the dimensions value is just an int\n3893             else:\n3894                 raise TypeError(\n3895                     \"The value of new dimension {k} must be \"\n3896                     \"an iterable or an int\".format(k=k)\n3897                 )\n3898 \n3899         for k, v in self._variables.items():\n3900             if k not in dim:\n3901                 if k in coord_names:  # Do not change coordinates\n3902                     variables[k] = v\n3903                 else:\n3904                     result_ndim = len(v.dims) + len(axis)\n3905                     for a in axis:\n3906                         if a < -result_ndim or result_ndim - 1 < a:\n3907                             raise IndexError(\n3908                                 f\"Axis {a} of variable {k} is out of bounds of the \"\n3909                                 f\"expanded dimension size {result_ndim}\"\n3910                             )\n3911 \n3912                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n3913                     if len(axis_pos) != len(set(axis_pos)):\n3914                         raise ValueError(\"axis should not contain duplicate values\")\n3915                     # We need to sort them to make sure `axis` equals to the\n3916                     # axis positions of the result array.\n3917                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n3918 \n3919                     all_dims = list(zip(v.dims, v.shape))\n3920                     for d, c in zip_axis_dim:\n3921                         all_dims.insert(d, c)\n3922                     variables[k] = v.set_dims(dict(all_dims))\n3923             else:\n3924                 if k not in variables:\n3925                     # If dims includes a label of a non-dimension coordinate,\n3926                     # it will be promoted to a 1D coordinate with a single value.\n3927                     index, index_vars = create_default_index_implicit(v.set_dims(k))\n3928                     indexes[k] = index\n3929                     variables.update(index_vars)\n3930 \n3931         return self._replace_with_new_dims(\n3932             variables, coord_names=coord_names, indexes=indexes\n3933         )\n3934 \n3935     # change type of self and return to T_Dataset once\n3936     # https://github.com/python/mypy/issues/12846 is resolved\n3937     def set_index(\n3938         self,\n3939         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,\n3940         append: bool = False,\n3941         **indexes_kwargs: Hashable | Sequence[Hashable],\n3942     ) -> Dataset:\n3943         \"\"\"Set Dataset (multi-)indexes using one or more existing coordinates\n3944         or variables.\n3945 \n3946         Parameters\n3947         ----------\n3948         indexes : {dim: index, ...}\n3949             Mapping from names matching dimensions and values given\n3950             by (lists of) the names of existing coordinates or variables to set\n3951             as new (multi-)index.\n3952         append : bool, default: False\n3953             If True, append the supplied index(es) to the existing index(es).\n3954             Otherwise replace the existing index(es) (default).\n3955         **indexes_kwargs : optional\n3956             The keyword arguments form of ``indexes``.\n3957             One of indexes or indexes_kwargs must be provided.\n3958 \n3959         Returns\n3960         -------\n3961         obj : Dataset\n3962             Another dataset, with this dataset's data but replaced coordinates.\n3963 \n3964         Examples\n3965         --------\n3966         >>> arr = xr.DataArray(\n3967         ...     data=np.ones((2, 3)),\n3968         ...     dims=[\"x\", \"y\"],\n3969         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n3970         ... )\n3971         >>> ds = xr.Dataset({\"v\": arr})\n3972         >>> ds\n3973         <xarray.Dataset>\n3974         Dimensions:  (x: 2, y: 3)\n3975         Coordinates:\n3976           * x        (x) int64 0 1\n3977           * y        (y) int64 0 1 2\n3978             a        (x) int64 3 4\n3979         Data variables:\n3980             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n3981         >>> ds.set_index(x=\"a\")\n3982         <xarray.Dataset>\n3983         Dimensions:  (x: 2, y: 3)\n3984         Coordinates:\n3985           * x        (x) int64 3 4\n3986           * y        (y) int64 0 1 2\n3987         Data variables:\n3988             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n3989 \n3990         See Also\n3991         --------\n3992         Dataset.reset_index\n3993         Dataset.swap_dims\n3994         \"\"\"\n3995         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, \"set_index\")\n3996 \n3997         new_indexes: dict[Hashable, Index] = {}\n3998         new_variables: dict[Hashable, IndexVariable] = {}\n3999         maybe_drop_indexes: list[Hashable] = []\n4000         drop_variables: list[Hashable] = []\n4001         replace_dims: dict[Hashable, Hashable] = {}\n4002 \n4003         for dim, _var_names in dim_coords.items():\n4004             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):\n4005                 var_names = [_var_names]\n4006             else:\n4007                 var_names = list(_var_names)\n4008 \n4009             invalid_vars = set(var_names) - set(self._variables)\n4010             if invalid_vars:\n4011                 raise ValueError(\n4012                     \", \".join([str(v) for v in invalid_vars])\n4013                     + \" variable(s) do not exist\"\n4014                 )\n4015 \n4016             current_coord_names = self.xindexes.get_all_coords(dim, errors=\"ignore\")\n4017 \n4018             # drop any pre-existing index involved\n4019             maybe_drop_indexes += list(current_coord_names) + var_names\n4020             for k in var_names:\n4021                 maybe_drop_indexes += list(\n4022                     self.xindexes.get_all_coords(k, errors=\"ignore\")\n4023                 )\n4024 \n4025             drop_variables += var_names\n4026 \n4027             if len(var_names) == 1 and (not append or dim not in self._indexes):\n4028                 var_name = var_names[0]\n4029                 var = self._variables[var_name]\n4030                 if var.dims != (dim,):\n4031                     raise ValueError(\n4032                         f\"dimension mismatch: try setting an index for dimension {dim!r} with \"\n4033                         f\"variable {var_name!r} that has dimensions {var.dims}\"\n4034                     )\n4035                 idx = PandasIndex.from_variables({dim: var})\n4036                 idx_vars = idx.create_variables({var_name: var})\n4037             else:\n4038                 if append:\n4039                     current_variables = {\n4040                         k: self._variables[k] for k in current_coord_names\n4041                     }\n4042                 else:\n4043                     current_variables = {}\n4044                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(\n4045                     dim,\n4046                     current_variables,\n4047                     {k: self._variables[k] for k in var_names},\n4048                 )\n4049                 for n in idx.index.names:\n4050                     replace_dims[n] = dim\n4051 \n4052             new_indexes.update({k: idx for k in idx_vars})\n4053             new_variables.update(idx_vars)\n4054 \n4055         indexes_: dict[Any, Index] = {\n4056             k: v for k, v in self._indexes.items() if k not in maybe_drop_indexes\n4057         }\n4058         indexes_.update(new_indexes)\n4059 \n4060         variables = {\n4061             k: v for k, v in self._variables.items() if k not in drop_variables\n4062         }\n4063         variables.update(new_variables)\n4064 \n4065         # update dimensions if necessary, GH: 3512\n4066         for k, v in variables.items():\n4067             if any(d in replace_dims for d in v.dims):\n4068                 new_dims = [replace_dims.get(d, d) for d in v.dims]\n4069                 variables[k] = v._replace(dims=new_dims)\n4070 \n4071         coord_names = self._coord_names - set(drop_variables) | set(new_variables)\n4072 \n4073         return self._replace_with_new_dims(\n4074             variables, coord_names=coord_names, indexes=indexes_\n4075         )\n4076 \n4077     def reset_index(\n4078         self: T_Dataset,\n4079         dims_or_levels: Hashable | Sequence[Hashable],\n4080         drop: bool = False,\n4081     ) -> T_Dataset:\n4082         \"\"\"Reset the specified index(es) or multi-index level(s).\n4083 \n4084         Parameters\n4085         ----------\n4086         dims_or_levels : Hashable or Sequence of Hashable\n4087             Name(s) of the dimension(s) and/or multi-index level(s) that will\n4088             be reset.\n4089         drop : bool, default: False\n4090             If True, remove the specified indexes and/or multi-index levels\n4091             instead of extracting them as new coordinates (default: False).\n4092 \n4093         Returns\n4094         -------\n4095         obj : Dataset\n4096             Another dataset, with this dataset's data but replaced coordinates.\n4097 \n4098         See Also\n4099         --------\n4100         Dataset.set_index\n4101         \"\"\"\n4102         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):\n4103             dims_or_levels = [dims_or_levels]\n4104 \n4105         invalid_coords = set(dims_or_levels) - set(self._indexes)\n4106         if invalid_coords:\n4107             raise ValueError(\n4108                 f\"{tuple(invalid_coords)} are not coordinates with an index\"\n4109             )\n4110 \n4111         drop_indexes: list[Hashable] = []\n4112         drop_variables: list[Hashable] = []\n4113         replaced_indexes: list[PandasMultiIndex] = []\n4114         new_indexes: dict[Hashable, Index] = {}\n4115         new_variables: dict[Hashable, IndexVariable] = {}\n4116 \n4117         for name in dims_or_levels:\n4118             index = self._indexes[name]\n4119             drop_indexes += list(self.xindexes.get_all_coords(name))\n4120 \n4121             if isinstance(index, PandasMultiIndex) and name not in self.dims:\n4122                 # special case for pd.MultiIndex (name is an index level):\n4123                 # replace by a new index with dropped level(s) instead of just drop the index\n4124                 if index not in replaced_indexes:\n4125                     level_names = index.index.names\n4126                     level_vars = {\n4127                         k: self._variables[k]\n4128                         for k in level_names\n4129                         if k not in dims_or_levels\n4130                     }\n4131                     if level_vars:\n4132                         idx = index.keep_levels(level_vars)\n4133                         idx_vars = idx.create_variables(level_vars)\n4134                         new_indexes.update({k: idx for k in idx_vars})\n4135                         new_variables.update(idx_vars)\n4136                 replaced_indexes.append(index)\n4137 \n4138             if drop:\n4139                 drop_variables.append(name)\n4140 \n4141         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n4142         indexes.update(new_indexes)\n4143 \n4144         variables = {\n4145             k: v for k, v in self._variables.items() if k not in drop_variables\n4146         }\n4147         variables.update(new_variables)\n4148 \n4149         coord_names = set(new_variables) | self._coord_names\n4150 \n4151         return self._replace(variables, coord_names=coord_names, indexes=indexes)\n4152 \n4153     def reorder_levels(\n4154         self: T_Dataset,\n4155         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n4156         **dim_order_kwargs: Sequence[int | Hashable],\n4157     ) -> T_Dataset:\n4158         \"\"\"Rearrange index levels using input order.\n4159 \n4160         Parameters\n4161         ----------\n4162         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional\n4163             Mapping from names matching dimensions and values given\n4164             by lists representing new level orders. Every given dimension\n4165             must have a multi-index.\n4166         **dim_order_kwargs : Sequence of int or Hashable, optional\n4167             The keyword arguments form of ``dim_order``.\n4168             One of dim_order or dim_order_kwargs must be provided.\n4169 \n4170         Returns\n4171         -------\n4172         obj : Dataset\n4173             Another dataset, with this dataset's data but replaced\n4174             coordinates.\n4175         \"\"\"\n4176         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n4177         variables = self._variables.copy()\n4178         indexes = dict(self._indexes)\n4179         new_indexes: dict[Hashable, Index] = {}\n4180         new_variables: dict[Hashable, IndexVariable] = {}\n4181 \n4182         for dim, order in dim_order.items():\n4183             index = self._indexes[dim]\n4184 \n4185             if not isinstance(index, PandasMultiIndex):\n4186                 raise ValueError(f\"coordinate {dim} has no MultiIndex\")\n4187 \n4188             level_vars = {k: self._variables[k] for k in order}\n4189             idx = index.reorder_levels(level_vars)\n4190             idx_vars = idx.create_variables(level_vars)\n4191             new_indexes.update({k: idx for k in idx_vars})\n4192             new_variables.update(idx_vars)\n4193 \n4194         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}\n4195         indexes.update(new_indexes)\n4196 \n4197         variables = {k: v for k, v in self._variables.items() if k not in new_variables}\n4198         variables.update(new_variables)\n4199 \n4200         return self._replace(variables, indexes=indexes)\n4201 \n4202     def _get_stack_index(\n4203         self,\n4204         dim,\n4205         multi=False,\n4206         create_index=False,\n4207     ) -> tuple[Index | None, dict[Hashable, Variable]]:\n4208         \"\"\"Used by stack and unstack to get one pandas (multi-)index among\n4209         the indexed coordinates along dimension `dim`.\n4210 \n4211         If exactly one index is found, return it with its corresponding\n4212         coordinate variables(s), otherwise return None and an empty dict.\n4213 \n4214         If `create_index=True`, create a new index if none is found or raise\n4215         an error if multiple indexes are found.\n4216 \n4217         \"\"\"\n4218         stack_index: Index | None = None\n4219         stack_coords: dict[Hashable, Variable] = {}\n4220 \n4221         for name, index in self._indexes.items():\n4222             var = self._variables[name]\n4223             if (\n4224                 var.ndim == 1\n4225                 and var.dims[0] == dim\n4226                 and (\n4227                     # stack: must be a single coordinate index\n4228                     not multi\n4229                     and not self.xindexes.is_multi(name)\n4230                     # unstack: must be an index that implements .unstack\n4231                     or multi\n4232                     and type(index).unstack is not Index.unstack\n4233                 )\n4234             ):\n4235                 if stack_index is not None and index is not stack_index:\n4236                     # more than one index found, stop\n4237                     if create_index:\n4238                         raise ValueError(\n4239                             f\"cannot stack dimension {dim!r} with `create_index=True` \"\n4240                             \"and with more than one index found along that dimension\"\n4241                         )\n4242                     return None, {}\n4243                 stack_index = index\n4244                 stack_coords[name] = var\n4245 \n4246         if create_index and stack_index is None:\n4247             if dim in self._variables:\n4248                 var = self._variables[dim]\n4249             else:\n4250                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)\n4251             # dummy index (only `stack_coords` will be used to construct the multi-index)\n4252             stack_index = PandasIndex([0], dim)\n4253             stack_coords = {dim: var}\n4254 \n4255         return stack_index, stack_coords\n4256 \n4257     def _stack_once(\n4258         self: T_Dataset,\n4259         dims: Sequence[Hashable | Ellipsis],\n4260         new_dim: Hashable,\n4261         index_cls: type[Index],\n4262         create_index: bool | None = True,\n4263     ) -> T_Dataset:\n4264         if dims == ...:\n4265             raise ValueError(\"Please use [...] for dims, rather than just ...\")\n4266         if ... in dims:\n4267             dims = list(infix_dims(dims, self.dims))\n4268 \n4269         new_variables: dict[Hashable, Variable] = {}\n4270         stacked_var_names: list[Hashable] = []\n4271         drop_indexes: list[Hashable] = []\n4272 \n4273         for name, var in self.variables.items():\n4274             if any(d in var.dims for d in dims):\n4275                 add_dims = [d for d in dims if d not in var.dims]\n4276                 vdims = list(var.dims) + add_dims\n4277                 shape = [self.dims[d] for d in vdims]\n4278                 exp_var = var.set_dims(vdims, shape)\n4279                 stacked_var = exp_var.stack(**{new_dim: dims})\n4280                 new_variables[name] = stacked_var\n4281                 stacked_var_names.append(name)\n4282             else:\n4283                 new_variables[name] = var.copy(deep=False)\n4284 \n4285         # drop indexes of stacked coordinates (if any)\n4286         for name in stacked_var_names:\n4287             drop_indexes += list(self.xindexes.get_all_coords(name, errors=\"ignore\"))\n4288 \n4289         new_indexes = {}\n4290         new_coord_names = set(self._coord_names)\n4291         if create_index or create_index is None:\n4292             product_vars: dict[Any, Variable] = {}\n4293             for dim in dims:\n4294                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)\n4295                 if idx is not None:\n4296                     product_vars.update(idx_vars)\n4297 \n4298             if len(product_vars) == len(dims):\n4299                 idx = index_cls.stack(product_vars, new_dim)\n4300                 new_indexes[new_dim] = idx\n4301                 new_indexes.update({k: idx for k in product_vars})\n4302                 idx_vars = idx.create_variables(product_vars)\n4303                 # keep consistent multi-index coordinate order\n4304                 for k in idx_vars:\n4305                     new_variables.pop(k, None)\n4306                 new_variables.update(idx_vars)\n4307                 new_coord_names.update(idx_vars)\n4308 \n4309         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n4310         indexes.update(new_indexes)\n4311 \n4312         return self._replace_with_new_dims(\n4313             new_variables, coord_names=new_coord_names, indexes=indexes\n4314         )\n4315 \n4316     def stack(\n4317         self: T_Dataset,\n4318         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,\n4319         create_index: bool | None = True,\n4320         index_cls: type[Index] = PandasMultiIndex,\n4321         **dimensions_kwargs: Sequence[Hashable | Ellipsis],\n4322     ) -> T_Dataset:\n4323         \"\"\"\n4324         Stack any number of existing dimensions into a single new dimension.\n4325 \n4326         New dimensions will be added at the end, and by default the corresponding\n4327         coordinate variables will be combined into a MultiIndex.\n4328 \n4329         Parameters\n4330         ----------\n4331         dimensions : mapping of hashable to sequence of hashable\n4332             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new\n4333             dimensions, and the existing dimensions that they replace. An\n4334             ellipsis (`...`) will be replaced by all unlisted dimensions.\n4335             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n4336             all dimensions.\n4337         create_index : bool or None, default: True\n4338 \n4339             - True: create a multi-index for each of the stacked dimensions.\n4340             - False: don't create any index.\n4341             - None. create a multi-index only if exactly one single (1-d) coordinate\n4342               index is found for every dimension to stack.\n4343 \n4344         index_cls: Index-class, default: PandasMultiIndex\n4345             Can be used to pass a custom multi-index type (must be an Xarray index that\n4346             implements `.stack()`). By default, a pandas multi-index wrapper is used.\n4347         **dimensions_kwargs\n4348             The keyword arguments form of ``dimensions``.\n4349             One of dimensions or dimensions_kwargs must be provided.\n4350 \n4351         Returns\n4352         -------\n4353         stacked : Dataset\n4354             Dataset with stacked data.\n4355 \n4356         See Also\n4357         --------\n4358         Dataset.unstack\n4359         \"\"\"\n4360         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n4361         result = self\n4362         for new_dim, dims in dimensions.items():\n4363             result = result._stack_once(dims, new_dim, index_cls, create_index)\n4364         return result\n4365 \n4366     def to_stacked_array(\n4367         self,\n4368         new_dim: Hashable,\n4369         sample_dims: Collection[Hashable],\n4370         variable_dim: Hashable = \"variable\",\n4371         name: Hashable | None = None,\n4372     ) -> DataArray:\n4373         \"\"\"Combine variables of differing dimensionality into a DataArray\n4374         without broadcasting.\n4375 \n4376         This method is similar to Dataset.to_array but does not broadcast the\n4377         variables.\n4378 \n4379         Parameters\n4380         ----------\n4381         new_dim : hashable\n4382             Name of the new stacked coordinate\n4383         sample_dims : Collection of hashables\n4384             List of dimensions that **will not** be stacked. Each array in the\n4385             dataset must share these dimensions. For machine learning\n4386             applications, these define the dimensions over which samples are\n4387             drawn.\n4388         variable_dim : hashable, default: \"variable\"\n4389             Name of the level in the stacked coordinate which corresponds to\n4390             the variables.\n4391         name : hashable, optional\n4392             Name of the new data array.\n4393 \n4394         Returns\n4395         -------\n4396         stacked : DataArray\n4397             DataArray with the specified dimensions and data variables\n4398             stacked together. The stacked coordinate is named ``new_dim``\n4399             and represented by a MultiIndex object with a level containing the\n4400             data variable names. The name of this level is controlled using\n4401             the ``variable_dim`` argument.\n4402 \n4403         See Also\n4404         --------\n4405         Dataset.to_array\n4406         Dataset.stack\n4407         DataArray.to_unstacked_dataset\n4408 \n4409         Examples\n4410         --------\n4411         >>> data = xr.Dataset(\n4412         ...     data_vars={\n4413         ...         \"a\": ((\"x\", \"y\"), [[0, 1, 2], [3, 4, 5]]),\n4414         ...         \"b\": (\"x\", [6, 7]),\n4415         ...     },\n4416         ...     coords={\"y\": [\"u\", \"v\", \"w\"]},\n4417         ... )\n4418 \n4419         >>> data\n4420         <xarray.Dataset>\n4421         Dimensions:  (x: 2, y: 3)\n4422         Coordinates:\n4423           * y        (y) <U1 'u' 'v' 'w'\n4424         Dimensions without coordinates: x\n4425         Data variables:\n4426             a        (x, y) int64 0 1 2 3 4 5\n4427             b        (x) int64 6 7\n4428 \n4429         >>> data.to_stacked_array(\"z\", sample_dims=[\"x\"])\n4430         <xarray.DataArray 'a' (x: 2, z: 4)>\n4431         array([[0, 1, 2, 6],\n4432                [3, 4, 5, 7]])\n4433         Coordinates:\n4434           * z         (z) object MultiIndex\n4435           * variable  (z) object 'a' 'a' 'a' 'b'\n4436           * y         (z) object 'u' 'v' 'w' nan\n4437         Dimensions without coordinates: x\n4438 \n4439         \"\"\"\n4440         from .concat import concat\n4441 \n4442         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n4443 \n4444         for variable in self:\n4445             dims = self[variable].dims\n4446             dims_include_sample_dims = set(sample_dims) <= set(dims)\n4447             if not dims_include_sample_dims:\n4448                 raise ValueError(\n4449                     \"All variables in the dataset must contain the \"\n4450                     \"dimensions {}.\".format(dims)\n4451                 )\n4452 \n4453         def ensure_stackable(val):\n4454             assign_coords = {variable_dim: val.name}\n4455             for dim in stacking_dims:\n4456                 if dim not in val.dims:\n4457                     assign_coords[dim] = None\n4458 \n4459             expand_dims = set(stacking_dims).difference(set(val.dims))\n4460             expand_dims.add(variable_dim)\n4461             # must be list for .expand_dims\n4462             expand_dims = list(expand_dims)\n4463 \n4464             return (\n4465                 val.assign_coords(**assign_coords)\n4466                 .expand_dims(expand_dims)\n4467                 .stack({new_dim: (variable_dim,) + stacking_dims})\n4468             )\n4469 \n4470         # concatenate the arrays\n4471         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n4472         data_array = concat(stackable_vars, dim=new_dim)\n4473 \n4474         if name is not None:\n4475             data_array.name = name\n4476 \n4477         return data_array\n4478 \n4479     def _unstack_once(\n4480         self: T_Dataset,\n4481         dim: Hashable,\n4482         index_and_vars: tuple[Index, dict[Hashable, Variable]],\n4483         fill_value,\n4484         sparse: bool = False,\n4485     ) -> T_Dataset:\n4486         index, index_vars = index_and_vars\n4487         variables: dict[Hashable, Variable] = {}\n4488         indexes = {k: v for k, v in self._indexes.items() if k != dim}\n4489 \n4490         new_indexes, clean_index = index.unstack()\n4491         indexes.update(new_indexes)\n4492 \n4493         for name, idx in new_indexes.items():\n4494             variables.update(idx.create_variables(index_vars))\n4495 \n4496         for name, var in self.variables.items():\n4497             if name not in index_vars:\n4498                 if dim in var.dims:\n4499                     if isinstance(fill_value, Mapping):\n4500                         fill_value_ = fill_value[name]\n4501                     else:\n4502                         fill_value_ = fill_value\n4503 \n4504                     variables[name] = var._unstack_once(\n4505                         index=clean_index,\n4506                         dim=dim,\n4507                         fill_value=fill_value_,\n4508                         sparse=sparse,\n4509                     )\n4510                 else:\n4511                     variables[name] = var\n4512 \n4513         coord_names = set(self._coord_names) - {dim} | set(new_indexes)\n4514 \n4515         return self._replace_with_new_dims(\n4516             variables, coord_names=coord_names, indexes=indexes\n4517         )\n4518 \n4519     def _unstack_full_reindex(\n4520         self: T_Dataset,\n4521         dim: Hashable,\n4522         index_and_vars: tuple[Index, dict[Hashable, Variable]],\n4523         fill_value,\n4524         sparse: bool,\n4525     ) -> T_Dataset:\n4526         index, index_vars = index_and_vars\n4527         variables: dict[Hashable, Variable] = {}\n4528         indexes = {k: v for k, v in self._indexes.items() if k != dim}\n4529 \n4530         new_indexes, clean_index = index.unstack()\n4531         indexes.update(new_indexes)\n4532 \n4533         new_index_variables = {}\n4534         for name, idx in new_indexes.items():\n4535             new_index_variables.update(idx.create_variables(index_vars))\n4536 \n4537         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}\n4538         variables.update(new_index_variables)\n4539 \n4540         # take a shortcut in case the MultiIndex was not modified.\n4541         full_idx = pd.MultiIndex.from_product(\n4542             clean_index.levels, names=clean_index.names\n4543         )\n4544         if clean_index.equals(full_idx):\n4545             obj = self\n4546         else:\n4547             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex\n4548             xr_full_idx = PandasMultiIndex(full_idx, dim)\n4549             indexers = Indexes(\n4550                 {k: xr_full_idx for k in index_vars},\n4551                 xr_full_idx.create_variables(index_vars),\n4552             )\n4553             obj = self._reindex(\n4554                 indexers, copy=False, fill_value=fill_value, sparse=sparse\n4555             )\n4556 \n4557         for name, var in obj.variables.items():\n4558             if name not in index_vars:\n4559                 if dim in var.dims:\n4560                     variables[name] = var.unstack({dim: new_dim_sizes})\n4561                 else:\n4562                     variables[name] = var\n4563 \n4564         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)\n4565 \n4566         return self._replace_with_new_dims(\n4567             variables, coord_names=coord_names, indexes=indexes\n4568         )\n4569 \n4570     def unstack(\n4571         self: T_Dataset,\n4572         dim: Hashable | Iterable[Hashable] | None = None,\n4573         fill_value: Any = xrdtypes.NA,\n4574         sparse: bool = False,\n4575     ) -> T_Dataset:\n4576         \"\"\"\n4577         Unstack existing dimensions corresponding to MultiIndexes into\n4578         multiple new dimensions.\n4579 \n4580         New dimensions will be added at the end.\n4581 \n4582         Parameters\n4583         ----------\n4584         dim : hashable or iterable of hashable, optional\n4585             Dimension(s) over which to unstack. By default unstacks all\n4586             MultiIndexes.\n4587         fill_value : scalar or dict-like, default: nan\n4588             value to be filled. If a dict-like, maps variable names to\n4589             fill values. If not provided or if the dict-like does not\n4590             contain all variables, the dtype's NA value will be used.\n4591         sparse : bool, default: False\n4592             use sparse-array if True\n4593 \n4594         Returns\n4595         -------\n4596         unstacked : Dataset\n4597             Dataset with unstacked data.\n4598 \n4599         See Also\n4600         --------\n4601         Dataset.stack\n4602         \"\"\"\n4603 \n4604         if dim is None:\n4605             dims = list(self.dims)\n4606         else:\n4607             if isinstance(dim, str) or not isinstance(dim, Iterable):\n4608                 dims = [dim]\n4609             else:\n4610                 dims = list(dim)\n4611 \n4612             missing_dims = [d for d in dims if d not in self.dims]\n4613             if missing_dims:\n4614                 raise ValueError(\n4615                     f\"Dataset does not contain the dimensions: {missing_dims}\"\n4616                 )\n4617 \n4618         # each specified dimension must have exactly one multi-index\n4619         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}\n4620         for d in dims:\n4621             idx, idx_vars = self._get_stack_index(d, multi=True)\n4622             if idx is not None:\n4623                 stacked_indexes[d] = idx, idx_vars\n4624 \n4625         if dim is None:\n4626             dims = list(stacked_indexes)\n4627         else:\n4628             non_multi_dims = set(dims) - set(stacked_indexes)\n4629             if non_multi_dims:\n4630                 raise ValueError(\n4631                     \"cannot unstack dimensions that do not \"\n4632                     f\"have exactly one multi-index: {tuple(non_multi_dims)}\"\n4633                 )\n4634 \n4635         result = self.copy(deep=False)\n4636 \n4637         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,\n4638         # so we can't just access self.variables[v].data for every variable.\n4639         # We only check the non-index variables.\n4640         # https://github.com/pydata/xarray/issues/5902\n4641         nonindexes = [\n4642             self.variables[k] for k in set(self.variables) - set(self._indexes)\n4643         ]\n4644         # Notes for each of these cases:\n4645         # 1. Dask arrays don't support assignment by index, which the fast unstack\n4646         #    function requires.\n4647         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125\n4648         # 2. Sparse doesn't currently support (though we could special-case it)\n4649         #    https://github.com/pydata/sparse/issues/422\n4650         # 3. pint requires checking if it's a NumPy array until\n4651         #    https://github.com/pydata/xarray/pull/4751 is resolved,\n4652         #    Once that is resolved, explicitly exclude pint arrays.\n4653         #    pint doesn't implement `np.full_like` in a way that's\n4654         #    currently compatible.\n4655         needs_full_reindex = any(\n4656             is_duck_dask_array(v.data)\n4657             or isinstance(v.data, sparse_array_type)\n4658             or not isinstance(v.data, np.ndarray)\n4659             for v in nonindexes\n4660         )\n4661 \n4662         for dim in dims:\n4663             if needs_full_reindex:\n4664                 result = result._unstack_full_reindex(\n4665                     dim, stacked_indexes[dim], fill_value, sparse\n4666                 )\n4667             else:\n4668                 result = result._unstack_once(\n4669                     dim, stacked_indexes[dim], fill_value, sparse\n4670                 )\n4671         return result\n4672 \n4673     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:\n4674         \"\"\"Update this dataset's variables with those from another dataset.\n4675 \n4676         Just like :py:meth:`dict.update` this is a in-place operation.\n4677         For a non-inplace version, see :py:meth:`Dataset.merge`.\n4678 \n4679         Parameters\n4680         ----------\n4681         other : Dataset or mapping\n4682             Variables with which to update this dataset. One of:\n4683 \n4684             - Dataset\n4685             - mapping {var name: DataArray}\n4686             - mapping {var name: Variable}\n4687             - mapping {var name: (dimension name, array-like)}\n4688             - mapping {var name: (tuple of dimension names, array-like)}\n4689 \n4690         Returns\n4691         -------\n4692         updated : Dataset\n4693             Updated dataset. Note that since the update is in-place this is the input\n4694             dataset.\n4695 \n4696             It is deprecated since version 0.17 and scheduled to be removed in 0.21.\n4697 \n4698         Raises\n4699         ------\n4700         ValueError\n4701             If any dimensions would have inconsistent sizes in the updated\n4702             dataset.\n4703 \n4704         See Also\n4705         --------\n4706         Dataset.assign\n4707         Dataset.merge\n4708         \"\"\"\n4709         merge_result = dataset_update_method(self, other)\n4710         return self._replace(inplace=True, **merge_result._asdict())\n4711 \n4712     def merge(\n4713         self: T_Dataset,\n4714         other: CoercibleMapping | DataArray,\n4715         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),\n4716         compat: CompatOptions = \"no_conflicts\",\n4717         join: JoinOptions = \"outer\",\n4718         fill_value: Any = xrdtypes.NA,\n4719         combine_attrs: CombineAttrsOptions = \"override\",\n4720     ) -> T_Dataset:\n4721         \"\"\"Merge the arrays of two datasets into a single dataset.\n4722 \n4723         This method generally does not allow for overriding data, with the\n4724         exception of attributes, which are ignored on the second dataset.\n4725         Variables with the same name are checked for conflicts via the equals\n4726         or identical methods.\n4727 \n4728         Parameters\n4729         ----------\n4730         other : Dataset or mapping\n4731             Dataset or variables to merge with this dataset.\n4732         overwrite_vars : hashable or iterable of hashable, optional\n4733             If provided, update variables of these name(s) without checking for\n4734             conflicts in this dataset.\n4735         compat : {\"identical\", \"equals\", \"broadcast_equals\", \\\n4736                   \"no_conflicts\", \"override\", \"minimal\"}, default: \"no_conflicts\"\n4737             String indicating how to compare variables of the same name for\n4738             potential conflicts:\n4739 \n4740             - 'identical': all values, dimensions and attributes must be the\n4741               same.\n4742             - 'equals': all values and dimensions must be the same.\n4743             - 'broadcast_equals': all values must be equal when variables are\n4744               broadcast against each other to ensure common dimensions.\n4745             - 'no_conflicts': only values which are not null in both datasets\n4746               must be equal. The returned dataset then contains the combination\n4747               of all non-null values.\n4748             - 'override': skip comparing and pick variable from first dataset\n4749             - 'minimal': drop conflicting coordinates\n4750 \n4751         join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\", \"override\"}, \\\n4752                default: \"outer\"\n4753             Method for joining ``self`` and ``other`` along shared dimensions:\n4754 \n4755             - 'outer': use the union of the indexes\n4756             - 'inner': use the intersection of the indexes\n4757             - 'left': use indexes from ``self``\n4758             - 'right': use indexes from ``other``\n4759             - 'exact': error instead of aligning non-equal indexes\n4760             - 'override': use indexes from ``self`` that are the same size\n4761               as those of ``other`` in that dimension\n4762 \n4763         fill_value : scalar or dict-like, optional\n4764             Value to use for newly missing values. If a dict-like, maps\n4765             variable names (including coordinates) to fill values.\n4766         combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"drop_conflicts\", \\\n4767                          \"override\"} or callable, default: \"override\"\n4768             A callable or a string indicating how to combine attrs of the objects being\n4769             merged:\n4770 \n4771             - \"drop\": empty attrs on returned Dataset.\n4772             - \"identical\": all attrs must be the same on every object.\n4773             - \"no_conflicts\": attrs from all objects are combined, any that have\n4774               the same name must also have the same value.\n4775             - \"drop_conflicts\": attrs from all objects are combined, any that have\n4776               the same name but different values are dropped.\n4777             - \"override\": skip comparing and copy attrs from the first dataset to\n4778               the result.\n4779 \n4780             If a callable, it must expect a sequence of ``attrs`` dicts and a context object\n4781             as its only parameters.\n4782 \n4783         Returns\n4784         -------\n4785         merged : Dataset\n4786             Merged dataset.\n4787 \n4788         Raises\n4789         ------\n4790         MergeError\n4791             If any variables conflict (see ``compat``).\n4792 \n4793         See Also\n4794         --------\n4795         Dataset.update\n4796         \"\"\"\n4797         from .dataarray import DataArray\n4798 \n4799         other = other.to_dataset() if isinstance(other, DataArray) else other\n4800         merge_result = dataset_merge_method(\n4801             self,\n4802             other,\n4803             overwrite_vars=overwrite_vars,\n4804             compat=compat,\n4805             join=join,\n4806             fill_value=fill_value,\n4807             combine_attrs=combine_attrs,\n4808         )\n4809         return self._replace(**merge_result._asdict())\n4810 \n4811     def _assert_all_in_dataset(\n4812         self, names: Iterable[Hashable], virtual_okay: bool = False\n4813     ) -> None:\n4814         bad_names = set(names) - set(self._variables)\n4815         if virtual_okay:\n4816             bad_names -= self.virtual_variables\n4817         if bad_names:\n4818             raise ValueError(\n4819                 \"One or more of the specified variables \"\n4820                 \"cannot be found in this dataset\"\n4821             )\n4822 \n4823     def drop_vars(\n4824         self: T_Dataset,\n4825         names: Hashable | Iterable[Hashable],\n4826         *,\n4827         errors: ErrorOptions = \"raise\",\n4828     ) -> T_Dataset:\n4829         \"\"\"Drop variables from this dataset.\n4830 \n4831         Parameters\n4832         ----------\n4833         names : hashable or iterable of hashable\n4834             Name(s) of variables to drop.\n4835         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n4836             If 'raise', raises a ValueError error if any of the variable\n4837             passed are not in the dataset. If 'ignore', any given names that are in the\n4838             dataset are dropped and no error is raised.\n4839 \n4840         Returns\n4841         -------\n4842         dropped : Dataset\n4843 \n4844         \"\"\"\n4845         # the Iterable check is required for mypy\n4846         if is_scalar(names) or not isinstance(names, Iterable):\n4847             names = {names}\n4848         else:\n4849             names = set(names)\n4850         if errors == \"raise\":\n4851             self._assert_all_in_dataset(names)\n4852 \n4853         # GH6505\n4854         other_names = set()\n4855         for var in names:\n4856             maybe_midx = self._indexes.get(var, None)\n4857             if isinstance(maybe_midx, PandasMultiIndex):\n4858                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])\n4859                 idx_other_names = idx_coord_names - set(names)\n4860                 other_names.update(idx_other_names)\n4861         if other_names:\n4862             names |= set(other_names)\n4863             warnings.warn(\n4864                 f\"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. \"\n4865                 f\"Please also drop the following variables: {other_names!r} to avoid an error in the future.\",\n4866                 DeprecationWarning,\n4867                 stacklevel=2,\n4868             )\n4869 \n4870         assert_no_index_corrupted(self.xindexes, names)\n4871 \n4872         variables = {k: v for k, v in self._variables.items() if k not in names}\n4873         coord_names = {k for k in self._coord_names if k in variables}\n4874         indexes = {k: v for k, v in self._indexes.items() if k not in names}\n4875         return self._replace_with_new_dims(\n4876             variables, coord_names=coord_names, indexes=indexes\n4877         )\n4878 \n4879     def drop(\n4880         self: T_Dataset,\n4881         labels=None,\n4882         dim=None,\n4883         *,\n4884         errors: ErrorOptions = \"raise\",\n4885         **labels_kwargs,\n4886     ) -> T_Dataset:\n4887         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n4888 \n4889         Using either `drop_vars` or `drop_sel` is encouraged\n4890 \n4891         See Also\n4892         --------\n4893         Dataset.drop_vars\n4894         Dataset.drop_sel\n4895         \"\"\"\n4896         if errors not in [\"raise\", \"ignore\"]:\n4897             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n4898 \n4899         if is_dict_like(labels) and not isinstance(labels, dict):\n4900             warnings.warn(\n4901                 \"dropping coordinates using `drop` is be deprecated; use drop_vars.\",\n4902                 FutureWarning,\n4903                 stacklevel=2,\n4904             )\n4905             return self.drop_vars(labels, errors=errors)\n4906 \n4907         if labels_kwargs or isinstance(labels, dict):\n4908             if dim is not None:\n4909                 raise ValueError(\"cannot specify dim and dict-like arguments.\")\n4910             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n4911 \n4912         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n4913             warnings.warn(\n4914                 \"dropping variables using `drop` will be deprecated; using drop_vars is encouraged.\",\n4915                 PendingDeprecationWarning,\n4916                 stacklevel=2,\n4917             )\n4918             return self.drop_vars(labels, errors=errors)\n4919         if dim is not None:\n4920             warnings.warn(\n4921                 \"dropping labels using list-like labels is deprecated; using \"\n4922                 \"dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).\",\n4923                 DeprecationWarning,\n4924                 stacklevel=2,\n4925             )\n4926             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n4927 \n4928         warnings.warn(\n4929             \"dropping labels using `drop` will be deprecated; using drop_sel is encouraged.\",\n4930             PendingDeprecationWarning,\n4931             stacklevel=2,\n4932         )\n4933         return self.drop_sel(labels, errors=errors)\n4934 \n4935     def drop_sel(\n4936         self: T_Dataset, labels=None, *, errors: ErrorOptions = \"raise\", **labels_kwargs\n4937     ) -> T_Dataset:\n4938         \"\"\"Drop index labels from this dataset.\n4939 \n4940         Parameters\n4941         ----------\n4942         labels : mapping of hashable to Any\n4943             Index labels to drop\n4944         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n4945             If 'raise', raises a ValueError error if\n4946             any of the index labels passed are not\n4947             in the dataset. If 'ignore', any given labels that are in the\n4948             dataset are dropped and no error is raised.\n4949         **labels_kwargs : {dim: label, ...}, optional\n4950             The keyword arguments form of ``dim`` and ``labels``\n4951 \n4952         Returns\n4953         -------\n4954         dropped : Dataset\n4955 \n4956         Examples\n4957         --------\n4958         >>> data = np.arange(6).reshape(2, 3)\n4959         >>> labels = [\"a\", \"b\", \"c\"]\n4960         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n4961         >>> ds\n4962         <xarray.Dataset>\n4963         Dimensions:  (x: 2, y: 3)\n4964         Coordinates:\n4965           * y        (y) <U1 'a' 'b' 'c'\n4966         Dimensions without coordinates: x\n4967         Data variables:\n4968             A        (x, y) int64 0 1 2 3 4 5\n4969         >>> ds.drop_sel(y=[\"a\", \"c\"])\n4970         <xarray.Dataset>\n4971         Dimensions:  (x: 2, y: 1)\n4972         Coordinates:\n4973           * y        (y) <U1 'b'\n4974         Dimensions without coordinates: x\n4975         Data variables:\n4976             A        (x, y) int64 1 4\n4977         >>> ds.drop_sel(y=\"b\")\n4978         <xarray.Dataset>\n4979         Dimensions:  (x: 2, y: 2)\n4980         Coordinates:\n4981           * y        (y) <U1 'a' 'c'\n4982         Dimensions without coordinates: x\n4983         Data variables:\n4984             A        (x, y) int64 0 2 3 5\n4985         \"\"\"\n4986         if errors not in [\"raise\", \"ignore\"]:\n4987             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n4988 \n4989         labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop_sel\")\n4990 \n4991         ds = self\n4992         for dim, labels_for_dim in labels.items():\n4993             # Don't cast to set, as it would harm performance when labels\n4994             # is a large numpy array\n4995             if utils.is_scalar(labels_for_dim):\n4996                 labels_for_dim = [labels_for_dim]\n4997             labels_for_dim = np.asarray(labels_for_dim)\n4998             try:\n4999                 index = self.get_index(dim)\n5000             except KeyError:\n5001                 raise ValueError(f\"dimension {dim!r} does not have coordinate labels\")\n5002             new_index = index.drop(labels_for_dim, errors=errors)\n5003             ds = ds.loc[{dim: new_index}]\n5004         return ds\n5005 \n5006     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:\n5007         \"\"\"Drop index positions from this Dataset.\n5008 \n5009         Parameters\n5010         ----------\n5011         indexers : mapping of hashable to Any\n5012             Index locations to drop\n5013         **indexers_kwargs : {dim: position, ...}, optional\n5014             The keyword arguments form of ``dim`` and ``positions``\n5015 \n5016         Returns\n5017         -------\n5018         dropped : Dataset\n5019 \n5020         Raises\n5021         ------\n5022         IndexError\n5023 \n5024         Examples\n5025         --------\n5026         >>> data = np.arange(6).reshape(2, 3)\n5027         >>> labels = [\"a\", \"b\", \"c\"]\n5028         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n5029         >>> ds\n5030         <xarray.Dataset>\n5031         Dimensions:  (x: 2, y: 3)\n5032         Coordinates:\n5033           * y        (y) <U1 'a' 'b' 'c'\n5034         Dimensions without coordinates: x\n5035         Data variables:\n5036             A        (x, y) int64 0 1 2 3 4 5\n5037         >>> ds.drop_isel(y=[0, 2])\n5038         <xarray.Dataset>\n5039         Dimensions:  (x: 2, y: 1)\n5040         Coordinates:\n5041           * y        (y) <U1 'b'\n5042         Dimensions without coordinates: x\n5043         Data variables:\n5044             A        (x, y) int64 1 4\n5045         >>> ds.drop_isel(y=1)\n5046         <xarray.Dataset>\n5047         Dimensions:  (x: 2, y: 2)\n5048         Coordinates:\n5049           * y        (y) <U1 'a' 'c'\n5050         Dimensions without coordinates: x\n5051         Data variables:\n5052             A        (x, y) int64 0 2 3 5\n5053         \"\"\"\n5054 \n5055         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"drop_isel\")\n5056 \n5057         ds = self\n5058         dimension_index = {}\n5059         for dim, pos_for_dim in indexers.items():\n5060             # Don't cast to set, as it would harm performance when labels\n5061             # is a large numpy array\n5062             if utils.is_scalar(pos_for_dim):\n5063                 pos_for_dim = [pos_for_dim]\n5064             pos_for_dim = np.asarray(pos_for_dim)\n5065             index = self.get_index(dim)\n5066             new_index = index.delete(pos_for_dim)\n5067             dimension_index[dim] = new_index\n5068         ds = ds.loc[dimension_index]\n5069         return ds\n5070 \n5071     def drop_dims(\n5072         self: T_Dataset,\n5073         drop_dims: Hashable | Iterable[Hashable],\n5074         *,\n5075         errors: ErrorOptions = \"raise\",\n5076     ) -> T_Dataset:\n5077         \"\"\"Drop dimensions and associated variables from this dataset.\n5078 \n5079         Parameters\n5080         ----------\n5081         drop_dims : hashable or iterable of hashable\n5082             Dimension or dimensions to drop.\n5083         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5084             If 'raise', raises a ValueError error if any of the\n5085             dimensions passed are not in the dataset. If 'ignore', any given\n5086             dimensions that are in the dataset are dropped and no error is raised.\n5087 \n5088         Returns\n5089         -------\n5090         obj : Dataset\n5091             The dataset without the given dimensions (or any variables\n5092             containing those dimensions).\n5093         \"\"\"\n5094         if errors not in [\"raise\", \"ignore\"]:\n5095             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5096 \n5097         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):\n5098             drop_dims = {drop_dims}\n5099         else:\n5100             drop_dims = set(drop_dims)\n5101 \n5102         if errors == \"raise\":\n5103             missing_dims = drop_dims - set(self.dims)\n5104             if missing_dims:\n5105                 raise ValueError(\n5106                     f\"Dataset does not contain the dimensions: {missing_dims}\"\n5107                 )\n5108 \n5109         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}\n5110         return self.drop_vars(drop_vars)\n5111 \n5112     def transpose(\n5113         self: T_Dataset,\n5114         *dims: Hashable,\n5115         missing_dims: ErrorOptionsWithWarn = \"raise\",\n5116     ) -> T_Dataset:\n5117         \"\"\"Return a new Dataset object with all array dimensions transposed.\n5118 \n5119         Although the order of dimensions on each array will change, the dataset\n5120         dimensions themselves will remain in fixed (sorted) order.\n5121 \n5122         Parameters\n5123         ----------\n5124         *dims : hashable, optional\n5125             By default, reverse the dimensions on each array. Otherwise,\n5126             reorder the dimensions to this order.\n5127         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n5128             What to do if dimensions that should be selected from are not present in the\n5129             Dataset:\n5130             - \"raise\": raise an exception\n5131             - \"warn\": raise a warning, and ignore the missing dimensions\n5132             - \"ignore\": ignore the missing dimensions\n5133 \n5134         Returns\n5135         -------\n5136         transposed : Dataset\n5137             Each array in the dataset (including) coordinates will be\n5138             transposed to the given order.\n5139 \n5140         Notes\n5141         -----\n5142         This operation returns a view of each array's data. It is\n5143         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n5144         -- the data will be fully loaded into memory.\n5145 \n5146         See Also\n5147         --------\n5148         numpy.transpose\n5149         DataArray.transpose\n5150         \"\"\"\n5151         # Use infix_dims to check once for missing dimensions\n5152         if len(dims) != 0:\n5153             _ = list(infix_dims(dims, self.dims, missing_dims))\n5154 \n5155         ds = self.copy()\n5156         for name, var in self._variables.items():\n5157             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))\n5158             ds._variables[name] = var.transpose(*var_dims)\n5159         return ds\n5160 \n5161     def dropna(\n5162         self: T_Dataset,\n5163         dim: Hashable,\n5164         how: Literal[\"any\", \"all\"] = \"any\",\n5165         thresh: int | None = None,\n5166         subset: Iterable[Hashable] | None = None,\n5167     ) -> T_Dataset:\n5168         \"\"\"Returns a new dataset with dropped labels for missing values along\n5169         the provided dimension.\n5170 \n5171         Parameters\n5172         ----------\n5173         dim : hashable\n5174             Dimension along which to drop missing values. Dropping along\n5175             multiple dimensions simultaneously is not yet supported.\n5176         how : {\"any\", \"all\"}, default: \"any\"\n5177             - any : if any NA values are present, drop that label\n5178             - all : if all values are NA, drop that label\n5179 \n5180         thresh : int or None, optional\n5181             If supplied, require this many non-NA values.\n5182         subset : iterable of hashable or None, optional\n5183             Which variables to check for missing values. By default, all\n5184             variables in the dataset are checked.\n5185 \n5186         Returns\n5187         -------\n5188         Dataset\n5189         \"\"\"\n5190         # TODO: consider supporting multiple dimensions? Or not, given that\n5191         # there are some ugly edge cases, e.g., pandas's dropna differs\n5192         # depending on the order of the supplied axes.\n5193 \n5194         if dim not in self.dims:\n5195             raise ValueError(f\"{dim} must be a single dataset dimension\")\n5196 \n5197         if subset is None:\n5198             subset = iter(self.data_vars)\n5199 \n5200         count = np.zeros(self.dims[dim], dtype=np.int64)\n5201         size = np.int_(0)  # for type checking\n5202 \n5203         for k in subset:\n5204             array = self._variables[k]\n5205             if dim in array.dims:\n5206                 dims = [d for d in array.dims if d != dim]\n5207                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]\n5208                 size += math.prod([self.dims[d] for d in dims])\n5209 \n5210         if thresh is not None:\n5211             mask = count >= thresh\n5212         elif how == \"any\":\n5213             mask = count == size\n5214         elif how == \"all\":\n5215             mask = count > 0\n5216         elif how is not None:\n5217             raise ValueError(f\"invalid how option: {how}\")\n5218         else:\n5219             raise TypeError(\"must specify how or thresh\")\n5220 \n5221         return self.isel({dim: mask})\n5222 \n5223     def fillna(self: T_Dataset, value: Any) -> T_Dataset:\n5224         \"\"\"Fill missing values in this object.\n5225 \n5226         This operation follows the normal broadcasting and alignment rules that\n5227         xarray uses for binary arithmetic, except the result is aligned to this\n5228         object (``join='left'``) instead of aligned to the intersection of\n5229         index coordinates (``join='inner'``).\n5230 \n5231         Parameters\n5232         ----------\n5233         value : scalar, ndarray, DataArray, dict or Dataset\n5234             Used to fill all matching missing values in this dataset's data\n5235             variables. Scalars, ndarrays or DataArrays arguments are used to\n5236             fill all data with aligned coordinates (for DataArrays).\n5237             Dictionaries or datasets match data variables and then align\n5238             coordinates if necessary.\n5239 \n5240         Returns\n5241         -------\n5242         Dataset\n5243 \n5244         Examples\n5245         --------\n5246         >>> ds = xr.Dataset(\n5247         ...     {\n5248         ...         \"A\": (\"x\", [np.nan, 2, np.nan, 0]),\n5249         ...         \"B\": (\"x\", [3, 4, np.nan, 1]),\n5250         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5]),\n5251         ...         \"D\": (\"x\", [np.nan, 3, np.nan, 4]),\n5252         ...     },\n5253         ...     coords={\"x\": [0, 1, 2, 3]},\n5254         ... )\n5255         >>> ds\n5256         <xarray.Dataset>\n5257         Dimensions:  (x: 4)\n5258         Coordinates:\n5259           * x        (x) int64 0 1 2 3\n5260         Data variables:\n5261             A        (x) float64 nan 2.0 nan 0.0\n5262             B        (x) float64 3.0 4.0 nan 1.0\n5263             C        (x) float64 nan nan nan 5.0\n5264             D        (x) float64 nan 3.0 nan 4.0\n5265 \n5266         Replace all `NaN` values with 0s.\n5267 \n5268         >>> ds.fillna(0)\n5269         <xarray.Dataset>\n5270         Dimensions:  (x: 4)\n5271         Coordinates:\n5272           * x        (x) int64 0 1 2 3\n5273         Data variables:\n5274             A        (x) float64 0.0 2.0 0.0 0.0\n5275             B        (x) float64 3.0 4.0 0.0 1.0\n5276             C        (x) float64 0.0 0.0 0.0 5.0\n5277             D        (x) float64 0.0 3.0 0.0 4.0\n5278 \n5279         Replace all `NaN` elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.\n5280 \n5281         >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n5282         >>> ds.fillna(value=values)\n5283         <xarray.Dataset>\n5284         Dimensions:  (x: 4)\n5285         Coordinates:\n5286           * x        (x) int64 0 1 2 3\n5287         Data variables:\n5288             A        (x) float64 0.0 2.0 0.0 0.0\n5289             B        (x) float64 3.0 4.0 1.0 1.0\n5290             C        (x) float64 2.0 2.0 2.0 5.0\n5291             D        (x) float64 3.0 3.0 3.0 4.0\n5292         \"\"\"\n5293         if utils.is_dict_like(value):\n5294             value_keys = getattr(value, \"data_vars\", value).keys()\n5295             if not set(value_keys) <= set(self.data_vars.keys()):\n5296                 raise ValueError(\n5297                     \"all variables in the argument to `fillna` \"\n5298                     \"must be contained in the original dataset\"\n5299                 )\n5300         out = ops.fillna(self, value)\n5301         return out\n5302 \n5303     def interpolate_na(\n5304         self: T_Dataset,\n5305         dim: Hashable | None = None,\n5306         method: InterpOptions = \"linear\",\n5307         limit: int = None,\n5308         use_coordinate: bool | Hashable = True,\n5309         max_gap: (\n5310             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta\n5311         ) = None,\n5312         **kwargs: Any,\n5313     ) -> T_Dataset:\n5314         \"\"\"Fill in NaNs by interpolating according to different methods.\n5315 \n5316         Parameters\n5317         ----------\n5318         dim : Hashable or None, optional\n5319             Specifies the dimension along which to interpolate.\n5320         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n5321             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n5322             String indicating which method to use for interpolation:\n5323 \n5324             - 'linear': linear interpolation. Additional keyword\n5325               arguments are passed to :py:func:`numpy.interp`\n5326             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n5327               are passed to :py:func:`scipy.interpolate.interp1d`. If\n5328               ``method='polynomial'``, the ``order`` keyword argument must also be\n5329               provided.\n5330             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n5331               respective :py:class:`scipy.interpolate` classes.\n5332 \n5333         use_coordinate : bool or Hashable, default: True\n5334             Specifies which index to use as the x values in the interpolation\n5335             formulated as `y = f(x)`. If False, values are treated as if\n5336             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n5337             used. If ``use_coordinate`` is a string, it specifies the name of a\n5338             coordinate variariable to use as the index.\n5339         limit : int, default: None\n5340             Maximum number of consecutive NaNs to fill. Must be greater than 0\n5341             or None for no limit. This filling is done regardless of the size of\n5342             the gap in the data. To only interpolate over gaps less than a given length,\n5343             see ``max_gap``.\n5344         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n5345             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n5346             Use None for no limit. When interpolating along a datetime64 dimension\n5347             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n5348 \n5349             - a string that is valid input for pandas.to_timedelta\n5350             - a :py:class:`numpy.timedelta64` object\n5351             - a :py:class:`pandas.Timedelta` object\n5352             - a :py:class:`datetime.timedelta` object\n5353 \n5354             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n5355             dimensions has not been implemented yet. Gap length is defined as the difference\n5356             between coordinate values at the first data point after a gap and the last value\n5357             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n5358             between coordinate values at the first (last) valid data point and the first (last) NaN.\n5359             For example, consider::\n5360 \n5361                 <xarray.DataArray (x: 9)>\n5362                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n5363                 Coordinates:\n5364                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n5365 \n5366             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n5367         **kwargs : dict, optional\n5368             parameters passed verbatim to the underlying interpolation function\n5369 \n5370         Returns\n5371         -------\n5372         interpolated: Dataset\n5373             Filled in Dataset.\n5374 \n5375         See Also\n5376         --------\n5377         numpy.interp\n5378         scipy.interpolate\n5379 \n5380         Examples\n5381         --------\n5382         >>> ds = xr.Dataset(\n5383         ...     {\n5384         ...         \"A\": (\"x\", [np.nan, 2, 3, np.nan, 0]),\n5385         ...         \"B\": (\"x\", [3, 4, np.nan, 1, 7]),\n5386         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5, 0]),\n5387         ...         \"D\": (\"x\", [np.nan, 3, np.nan, -1, 4]),\n5388         ...     },\n5389         ...     coords={\"x\": [0, 1, 2, 3, 4]},\n5390         ... )\n5391         >>> ds\n5392         <xarray.Dataset>\n5393         Dimensions:  (x: 5)\n5394         Coordinates:\n5395           * x        (x) int64 0 1 2 3 4\n5396         Data variables:\n5397             A        (x) float64 nan 2.0 3.0 nan 0.0\n5398             B        (x) float64 3.0 4.0 nan 1.0 7.0\n5399             C        (x) float64 nan nan nan 5.0 0.0\n5400             D        (x) float64 nan 3.0 nan -1.0 4.0\n5401 \n5402         >>> ds.interpolate_na(dim=\"x\", method=\"linear\")\n5403         <xarray.Dataset>\n5404         Dimensions:  (x: 5)\n5405         Coordinates:\n5406           * x        (x) int64 0 1 2 3 4\n5407         Data variables:\n5408             A        (x) float64 nan 2.0 3.0 1.5 0.0\n5409             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n5410             C        (x) float64 nan nan nan 5.0 0.0\n5411             D        (x) float64 nan 3.0 1.0 -1.0 4.0\n5412 \n5413         >>> ds.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n5414         <xarray.Dataset>\n5415         Dimensions:  (x: 5)\n5416         Coordinates:\n5417           * x        (x) int64 0 1 2 3 4\n5418         Data variables:\n5419             A        (x) float64 1.0 2.0 3.0 1.5 0.0\n5420             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n5421             C        (x) float64 20.0 15.0 10.0 5.0 0.0\n5422             D        (x) float64 5.0 3.0 1.0 -1.0 4.0\n5423         \"\"\"\n5424         from .missing import _apply_over_vars_with_dim, interp_na\n5425 \n5426         new = _apply_over_vars_with_dim(\n5427             interp_na,\n5428             self,\n5429             dim=dim,\n5430             method=method,\n5431             limit=limit,\n5432             use_coordinate=use_coordinate,\n5433             max_gap=max_gap,\n5434             **kwargs,\n5435         )\n5436         return new\n5437 \n5438     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:\n5439         \"\"\"Fill NaN values by propagating values forward\n5440 \n5441         *Requires bottleneck.*\n5442 \n5443         Parameters\n5444         ----------\n5445         dim : Hashable\n5446             Specifies the dimension along which to propagate values when\n5447             filling.\n5448         limit : int or None, optional\n5449             The maximum number of consecutive NaN values to forward fill. In\n5450             other words, if there is a gap with more than this number of\n5451             consecutive NaNs, it will only be partially filled. Must be greater\n5452             than 0 or None for no limit. Must be None or greater than or equal\n5453             to axis length if filling along chunked axes (dimensions).\n5454 \n5455         Returns\n5456         -------\n5457         Dataset\n5458         \"\"\"\n5459         from .missing import _apply_over_vars_with_dim, ffill\n5460 \n5461         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n5462         return new\n5463 \n5464     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:\n5465         \"\"\"Fill NaN values by propagating values backward\n5466 \n5467         *Requires bottleneck.*\n5468 \n5469         Parameters\n5470         ----------\n5471         dim : Hashable\n5472             Specifies the dimension along which to propagate values when\n5473             filling.\n5474         limit : int or None, optional\n5475             The maximum number of consecutive NaN values to backward fill. In\n5476             other words, if there is a gap with more than this number of\n5477             consecutive NaNs, it will only be partially filled. Must be greater\n5478             than 0 or None for no limit. Must be None or greater than or equal\n5479             to axis length if filling along chunked axes (dimensions).\n5480 \n5481         Returns\n5482         -------\n5483         Dataset\n5484         \"\"\"\n5485         from .missing import _apply_over_vars_with_dim, bfill\n5486 \n5487         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n5488         return new\n5489 \n5490     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:\n5491         \"\"\"Combine two Datasets, default to data_vars of self.\n5492 \n5493         The new coordinates follow the normal broadcasting and alignment rules\n5494         of ``join='outer'``.  Vacant cells in the expanded coordinates are\n5495         filled with np.nan.\n5496 \n5497         Parameters\n5498         ----------\n5499         other : Dataset\n5500             Used to fill all matching missing values in this array.\n5501 \n5502         Returns\n5503         -------\n5504         Dataset\n5505         \"\"\"\n5506         out = ops.fillna(self, other, join=\"outer\", dataset_join=\"outer\")\n5507         return out\n5508 \n5509     def reduce(\n5510         self: T_Dataset,\n5511         func: Callable,\n5512         dim: Hashable | Iterable[Hashable] = None,\n5513         *,\n5514         keep_attrs: bool | None = None,\n5515         keepdims: bool = False,\n5516         numeric_only: bool = False,\n5517         **kwargs: Any,\n5518     ) -> T_Dataset:\n5519         \"\"\"Reduce this dataset by applying `func` along some dimension(s).\n5520 \n5521         Parameters\n5522         ----------\n5523         func : callable\n5524             Function which can be called in the form\n5525             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n5526             np.ndarray over an integer valued axis.\n5527         dim : str or sequence of str, optional\n5528             Dimension(s) over which to apply `func`.  By default `func` is\n5529             applied over all dimensions.\n5530         keep_attrs : bool or None, optional\n5531             If True, the dataset's attributes (`attrs`) will be copied from\n5532             the original object to the new one.  If False (default), the new\n5533             object will be returned without attributes.\n5534         keepdims : bool, default: False\n5535             If True, the dimensions which are reduced are left in the result\n5536             as dimensions of size one. Coordinates that use these dimensions\n5537             are removed.\n5538         numeric_only : bool, default: False\n5539             If True, only apply ``func`` to variables with a numeric dtype.\n5540         **kwargs : Any\n5541             Additional keyword arguments passed on to ``func``.\n5542 \n5543         Returns\n5544         -------\n5545         reduced : Dataset\n5546             Dataset with this object's DataArrays replaced with new DataArrays\n5547             of summarized data and the indicated dimension(s) removed.\n5548         \"\"\"\n5549         if kwargs.get(\"axis\", None) is not None:\n5550             raise ValueError(\n5551                 \"passing 'axis' to Dataset reduce methods is ambiguous.\"\n5552                 \" Please use 'dim' instead.\"\n5553             )\n5554 \n5555         if dim is None or dim is ...:\n5556             dims = set(self.dims)\n5557         elif isinstance(dim, str) or not isinstance(dim, Iterable):\n5558             dims = {dim}\n5559         else:\n5560             dims = set(dim)\n5561 \n5562         missing_dimensions = [d for d in dims if d not in self.dims]\n5563         if missing_dimensions:\n5564             raise ValueError(\n5565                 f\"Dataset does not contain the dimensions: {missing_dimensions}\"\n5566             )\n5567 \n5568         if keep_attrs is None:\n5569             keep_attrs = _get_keep_attrs(default=False)\n5570 \n5571         variables: dict[Hashable, Variable] = {}\n5572         for name, var in self._variables.items():\n5573             reduce_dims = [d for d in var.dims if d in dims]\n5574             if name in self.coords:\n5575                 if not reduce_dims:\n5576                     variables[name] = var\n5577             else:\n5578                 if (\n5579                     # Some reduction functions (e.g. std, var) need to run on variables\n5580                     # that don't have the reduce dims: PR5393\n5581                     not reduce_dims\n5582                     or not numeric_only\n5583                     or np.issubdtype(var.dtype, np.number)\n5584                     or (var.dtype == np.bool_)\n5585                 ):\n5586                     reduce_maybe_single: Hashable | None | list[Hashable]\n5587                     if len(reduce_dims) == 1:\n5588                         # unpack dimensions for the benefit of functions\n5589                         # like np.argmin which can't handle tuple arguments\n5590                         (reduce_maybe_single,) = reduce_dims\n5591                     elif len(reduce_dims) == var.ndim:\n5592                         # prefer to aggregate over axis=None rather than\n5593                         # axis=(0, 1) if they will be equivalent, because\n5594                         # the former is often more efficient\n5595                         reduce_maybe_single = None\n5596                     else:\n5597                         reduce_maybe_single = reduce_dims\n5598                     variables[name] = var.reduce(\n5599                         func,\n5600                         dim=reduce_maybe_single,\n5601                         keep_attrs=keep_attrs,\n5602                         keepdims=keepdims,\n5603                         **kwargs,\n5604                     )\n5605 \n5606         coord_names = {k for k in self.coords if k in variables}\n5607         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n5608         attrs = self.attrs if keep_attrs else None\n5609         return self._replace_with_new_dims(\n5610             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n5611         )\n5612 \n5613     def map(\n5614         self: T_Dataset,\n5615         func: Callable,\n5616         keep_attrs: bool | None = None,\n5617         args: Iterable[Any] = (),\n5618         **kwargs: Any,\n5619     ) -> T_Dataset:\n5620         \"\"\"Apply a function to each data variable in this dataset\n5621 \n5622         Parameters\n5623         ----------\n5624         func : callable\n5625             Function which can be called in the form `func(x, *args, **kwargs)`\n5626             to transform each DataArray `x` in this dataset into another\n5627             DataArray.\n5628         keep_attrs : bool or None, optional\n5629             If True, both the dataset's and variables' attributes (`attrs`) will be\n5630             copied from the original objects to the new ones. If False, the new dataset\n5631             and variables will be returned without copying the attributes.\n5632         args : iterable, optional\n5633             Positional arguments passed on to `func`.\n5634         **kwargs : Any\n5635             Keyword arguments passed on to `func`.\n5636 \n5637         Returns\n5638         -------\n5639         applied : Dataset\n5640             Resulting dataset from applying ``func`` to each data variable.\n5641 \n5642         Examples\n5643         --------\n5644         >>> da = xr.DataArray(np.random.randn(2, 3))\n5645         >>> ds = xr.Dataset({\"foo\": da, \"bar\": (\"x\", [-1, 2])})\n5646         >>> ds\n5647         <xarray.Dataset>\n5648         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n5649         Dimensions without coordinates: dim_0, dim_1, x\n5650         Data variables:\n5651             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n5652             bar      (x) int64 -1 2\n5653         >>> ds.map(np.fabs)\n5654         <xarray.Dataset>\n5655         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n5656         Dimensions without coordinates: dim_0, dim_1, x\n5657         Data variables:\n5658             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773\n5659             bar      (x) float64 1.0 2.0\n5660         \"\"\"\n5661         if keep_attrs is None:\n5662             keep_attrs = _get_keep_attrs(default=False)\n5663         variables = {\n5664             k: maybe_wrap_array(v, func(v, *args, **kwargs))\n5665             for k, v in self.data_vars.items()\n5666         }\n5667         if keep_attrs:\n5668             for k, v in variables.items():\n5669                 v._copy_attrs_from(self.data_vars[k])\n5670         attrs = self.attrs if keep_attrs else None\n5671         return type(self)(variables, attrs=attrs)\n5672 \n5673     def apply(\n5674         self: T_Dataset,\n5675         func: Callable,\n5676         keep_attrs: bool | None = None,\n5677         args: Iterable[Any] = (),\n5678         **kwargs: Any,\n5679     ) -> T_Dataset:\n5680         \"\"\"\n5681         Backward compatible implementation of ``map``\n5682 \n5683         See Also\n5684         --------\n5685         Dataset.map\n5686         \"\"\"\n5687         warnings.warn(\n5688             \"Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged\",\n5689             PendingDeprecationWarning,\n5690             stacklevel=2,\n5691         )\n5692         return self.map(func, keep_attrs, args, **kwargs)\n5693 \n5694     def assign(\n5695         self: T_Dataset,\n5696         variables: Mapping[Any, Any] | None = None,\n5697         **variables_kwargs: Any,\n5698     ) -> T_Dataset:\n5699         \"\"\"Assign new data variables to a Dataset, returning a new object\n5700         with all the original variables in addition to the new ones.\n5701 \n5702         Parameters\n5703         ----------\n5704         variables : mapping of hashable to Any\n5705             Mapping from variables names to the new values. If the new values\n5706             are callable, they are computed on the Dataset and assigned to new\n5707             data variables. If the values are not callable, (e.g. a DataArray,\n5708             scalar, or array), they are simply assigned.\n5709         **variables_kwargs\n5710             The keyword arguments form of ``variables``.\n5711             One of variables or variables_kwargs must be provided.\n5712 \n5713         Returns\n5714         -------\n5715         ds : Dataset\n5716             A new Dataset with the new variables in addition to all the\n5717             existing variables.\n5718 \n5719         Notes\n5720         -----\n5721         Since ``kwargs`` is a dictionary, the order of your arguments may not\n5722         be preserved, and so the order of the new variables is not well\n5723         defined. Assigning multiple variables within the same ``assign`` is\n5724         possible, but you cannot reference other variables created within the\n5725         same ``assign`` call.\n5726 \n5727         See Also\n5728         --------\n5729         pandas.DataFrame.assign\n5730 \n5731         Examples\n5732         --------\n5733         >>> x = xr.Dataset(\n5734         ...     {\n5735         ...         \"temperature_c\": (\n5736         ...             (\"lat\", \"lon\"),\n5737         ...             20 * np.random.rand(4).reshape(2, 2),\n5738         ...         ),\n5739         ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n5740         ...     },\n5741         ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n5742         ... )\n5743         >>> x\n5744         <xarray.Dataset>\n5745         Dimensions:        (lat: 2, lon: 2)\n5746         Coordinates:\n5747           * lat            (lat) int64 10 20\n5748           * lon            (lon) int64 150 160\n5749         Data variables:\n5750             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n5751             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n5752 \n5753         Where the value is a callable, evaluated on dataset:\n5754 \n5755         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n5756         <xarray.Dataset>\n5757         Dimensions:        (lat: 2, lon: 2)\n5758         Coordinates:\n5759           * lat            (lat) int64 10 20\n5760           * lon            (lon) int64 150 160\n5761         Data variables:\n5762             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n5763             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n5764             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n5765 \n5766         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n5767 \n5768         >>> x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)\n5769         <xarray.Dataset>\n5770         Dimensions:        (lat: 2, lon: 2)\n5771         Coordinates:\n5772           * lat            (lat) int64 10 20\n5773           * lon            (lon) int64 150 160\n5774         Data variables:\n5775             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n5776             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n5777             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n5778 \n5779         \"\"\"\n5780         variables = either_dict_or_kwargs(variables, variables_kwargs, \"assign\")\n5781         data = self.copy()\n5782         # do all calculations first...\n5783         results: CoercibleMapping = data._calc_assign_results(variables)\n5784         data.coords._maybe_drop_multiindex_coords(set(results.keys()))\n5785         # ... and then assign\n5786         data.update(results)\n5787         return data\n5788 \n5789     def to_array(\n5790         self, dim: Hashable = \"variable\", name: Hashable | None = None\n5791     ) -> DataArray:\n5792         \"\"\"Convert this dataset into an xarray.DataArray\n5793 \n5794         The data variables of this dataset will be broadcast against each other\n5795         and stacked along the first axis of the new array. All coordinates of\n5796         this dataset will remain coordinates.\n5797 \n5798         Parameters\n5799         ----------\n5800         dim : Hashable, default: \"variable\"\n5801             Name of the new dimension.\n5802         name : Hashable or None, optional\n5803             Name of the new data array.\n5804 \n5805         Returns\n5806         -------\n5807         array : xarray.DataArray\n5808         \"\"\"\n5809         from .dataarray import DataArray\n5810 \n5811         data_vars = [self.variables[k] for k in self.data_vars]\n5812         broadcast_vars = broadcast_variables(*data_vars)\n5813         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n5814 \n5815         dims = (dim,) + broadcast_vars[0].dims\n5816         variable = Variable(dims, data, self.attrs, fastpath=True)\n5817 \n5818         coords = {k: v.variable for k, v in self.coords.items()}\n5819         indexes = filter_indexes_from_coords(self._indexes, set(coords))\n5820         new_dim_index = PandasIndex(list(self.data_vars), dim)\n5821         indexes[dim] = new_dim_index\n5822         coords.update(new_dim_index.create_variables())\n5823 \n5824         return DataArray._construct_direct(variable, coords, name, indexes)\n5825 \n5826     def _normalize_dim_order(\n5827         self, dim_order: Sequence[Hashable] | None = None\n5828     ) -> dict[Hashable, int]:\n5829         \"\"\"\n5830         Check the validity of the provided dimensions if any and return the mapping\n5831         between dimension name and their size.\n5832 \n5833         Parameters\n5834         ----------\n5835         dim_order: Sequence of Hashable or None, optional\n5836             Dimension order to validate (default to the alphabetical order if None).\n5837 \n5838         Returns\n5839         -------\n5840         result : dict[Hashable, int]\n5841             Validated dimensions mapping.\n5842 \n5843         \"\"\"\n5844         if dim_order is None:\n5845             dim_order = list(self.dims)\n5846         elif set(dim_order) != set(self.dims):\n5847             raise ValueError(\n5848                 \"dim_order {} does not match the set of dimensions of this \"\n5849                 \"Dataset: {}\".format(dim_order, list(self.dims))\n5850             )\n5851 \n5852         ordered_dims = {k: self.dims[k] for k in dim_order}\n5853 \n5854         return ordered_dims\n5855 \n5856     def to_pandas(self) -> pd.Series | pd.DataFrame:\n5857         \"\"\"Convert this dataset into a pandas object without changing the number of dimensions.\n5858 \n5859         The type of the returned object depends on the number of Dataset\n5860         dimensions:\n5861 \n5862         * 0D -> `pandas.Series`\n5863         * 1D -> `pandas.DataFrame`\n5864 \n5865         Only works for Datasets with 1 or fewer dimensions.\n5866         \"\"\"\n5867         if len(self.dims) == 0:\n5868             return pd.Series({k: v.item() for k, v in self.items()})\n5869         if len(self.dims) == 1:\n5870             return self.to_dataframe()\n5871         raise ValueError(\n5872             \"cannot convert Datasets with %s dimensions into \"\n5873             \"pandas objects without changing the number of dimensions. \"\n5874             \"Please use Dataset.to_dataframe() instead.\" % len(self.dims)\n5875         )\n5876 \n5877     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):\n5878         columns = [k for k in self.variables if k not in self.dims]\n5879         data = [\n5880             self._variables[k].set_dims(ordered_dims).values.reshape(-1)\n5881             for k in columns\n5882         ]\n5883         index = self.coords.to_index([*ordered_dims])\n5884         return pd.DataFrame(dict(zip(columns, data)), index=index)\n5885 \n5886     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:\n5887         \"\"\"Convert this dataset into a pandas.DataFrame.\n5888 \n5889         Non-index variables in this dataset form the columns of the\n5890         DataFrame. The DataFrame is indexed by the Cartesian product of\n5891         this dataset's indices.\n5892 \n5893         Parameters\n5894         ----------\n5895         dim_order: Sequence of Hashable or None, optional\n5896             Hierarchical dimension order for the resulting dataframe. All\n5897             arrays are transposed to this order and then written out as flat\n5898             vectors in contiguous order, so the last dimension in this list\n5899             will be contiguous in the resulting DataFrame. This has a major\n5900             influence on which operations are efficient on the resulting\n5901             dataframe.\n5902 \n5903             If provided, must include all dimensions of this dataset. By\n5904             default, dimensions are sorted alphabetically.\n5905 \n5906         Returns\n5907         -------\n5908         result : DataFrame\n5909             Dataset as a pandas DataFrame.\n5910 \n5911         \"\"\"\n5912 \n5913         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n5914 \n5915         return self._to_dataframe(ordered_dims=ordered_dims)\n5916 \n5917     def _set_sparse_data_from_dataframe(\n5918         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple\n5919     ) -> None:\n5920         from sparse import COO\n5921 \n5922         if isinstance(idx, pd.MultiIndex):\n5923             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)\n5924             is_sorted = idx.is_monotonic_increasing\n5925             shape = tuple(lev.size for lev in idx.levels)\n5926         else:\n5927             coords = np.arange(idx.size).reshape(1, -1)\n5928             is_sorted = True\n5929             shape = (idx.size,)\n5930 \n5931         for name, values in arrays:\n5932             # In virtually all real use cases, the sparse array will now have\n5933             # missing values and needs a fill_value. For consistency, don't\n5934             # special case the rare exceptions (e.g., dtype=int without a\n5935             # MultiIndex).\n5936             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n5937             values = np.asarray(values, dtype=dtype)\n5938 \n5939             data = COO(\n5940                 coords,\n5941                 values,\n5942                 shape,\n5943                 has_duplicates=False,\n5944                 sorted=is_sorted,\n5945                 fill_value=fill_value,\n5946             )\n5947             self[name] = (dims, data)\n5948 \n5949     def _set_numpy_data_from_dataframe(\n5950         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple\n5951     ) -> None:\n5952         if not isinstance(idx, pd.MultiIndex):\n5953             for name, values in arrays:\n5954                 self[name] = (dims, values)\n5955             return\n5956 \n5957         # NB: similar, more general logic, now exists in\n5958         # variable.unstack_once; we could consider combining them at some\n5959         # point.\n5960 \n5961         shape = tuple(lev.size for lev in idx.levels)\n5962         indexer = tuple(idx.codes)\n5963 \n5964         # We already verified that the MultiIndex has all unique values, so\n5965         # there are missing values if and only if the size of output arrays is\n5966         # larger that the index.\n5967         missing_values = math.prod(shape) > idx.shape[0]\n5968 \n5969         for name, values in arrays:\n5970             # NumPy indexing is much faster than using DataFrame.reindex() to\n5971             # fill in missing values:\n5972             # https://stackoverflow.com/a/35049899/809705\n5973             if missing_values:\n5974                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n5975                 data = np.full(shape, fill_value, dtype)\n5976             else:\n5977                 # If there are no missing values, keep the existing dtype\n5978                 # instead of promoting to support NA, e.g., keep integer\n5979                 # columns as integers.\n5980                 # TODO: consider removing this special case, which doesn't\n5981                 # exist for sparse=True.\n5982                 data = np.zeros(shape, values.dtype)\n5983             data[indexer] = values\n5984             self[name] = (dims, data)\n5985 \n5986     @classmethod\n5987     def from_dataframe(\n5988         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False\n5989     ) -> T_Dataset:\n5990         \"\"\"Convert a pandas.DataFrame into an xarray.Dataset\n5991 \n5992         Each column will be converted into an independent variable in the\n5993         Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n5994         into a tensor product of one-dimensional indices (filling in missing\n5995         values with NaN). This method will produce a Dataset very similar to\n5996         that on which the 'to_dataframe' method was called, except with\n5997         possibly redundant dimensions (since all dataset variables will have\n5998         the same dimensionality)\n5999 \n6000         Parameters\n6001         ----------\n6002         dataframe : DataFrame\n6003             DataFrame from which to copy data and indices.\n6004         sparse : bool, default: False\n6005             If true, create a sparse arrays instead of dense numpy arrays. This\n6006             can potentially save a large amount of memory if the DataFrame has\n6007             a MultiIndex. Requires the sparse package (sparse.pydata.org).\n6008 \n6009         Returns\n6010         -------\n6011         New Dataset.\n6012 \n6013         See Also\n6014         --------\n6015         xarray.DataArray.from_series\n6016         pandas.DataFrame.to_xarray\n6017         \"\"\"\n6018         # TODO: Add an option to remove dimensions along which the variables\n6019         # are constant, to enable consistent serialization to/from a dataframe,\n6020         # even if some variables have different dimensionality.\n6021 \n6022         if not dataframe.columns.is_unique:\n6023             raise ValueError(\"cannot convert DataFrame with non-unique columns\")\n6024 \n6025         idx = remove_unused_levels_categories(dataframe.index)\n6026 \n6027         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:\n6028             raise ValueError(\n6029                 \"cannot convert a DataFrame with a non-unique MultiIndex into xarray\"\n6030             )\n6031 \n6032         # Cast to a NumPy array first, in case the Series is a pandas Extension\n6033         # array (which doesn't have a valid NumPy dtype)\n6034         # TODO: allow users to control how this casting happens, e.g., by\n6035         # forwarding arguments to pandas.Series.to_numpy?\n6036         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]\n6037 \n6038         indexes: dict[Hashable, Index] = {}\n6039         index_vars: dict[Hashable, Variable] = {}\n6040 \n6041         if isinstance(idx, pd.MultiIndex):\n6042             dims = tuple(\n6043                 name if name is not None else \"level_%i\" % n\n6044                 for n, name in enumerate(idx.names)\n6045             )\n6046             for dim, lev in zip(dims, idx.levels):\n6047                 xr_idx = PandasIndex(lev, dim)\n6048                 indexes[dim] = xr_idx\n6049                 index_vars.update(xr_idx.create_variables())\n6050         else:\n6051             index_name = idx.name if idx.name is not None else \"index\"\n6052             dims = (index_name,)\n6053             xr_idx = PandasIndex(idx, index_name)\n6054             indexes[index_name] = xr_idx\n6055             index_vars.update(xr_idx.create_variables())\n6056 \n6057         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)\n6058 \n6059         if sparse:\n6060             obj._set_sparse_data_from_dataframe(idx, arrays, dims)\n6061         else:\n6062             obj._set_numpy_data_from_dataframe(idx, arrays, dims)\n6063         return obj\n6064 \n6065     def to_dask_dataframe(\n6066         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False\n6067     ) -> DaskDataFrame:\n6068         \"\"\"\n6069         Convert this dataset into a dask.dataframe.DataFrame.\n6070 \n6071         The dimensions, coordinates and data variables in this dataset form\n6072         the columns of the DataFrame.\n6073 \n6074         Parameters\n6075         ----------\n6076         dim_order : list, optional\n6077             Hierarchical dimension order for the resulting dataframe. All\n6078             arrays are transposed to this order and then written out as flat\n6079             vectors in contiguous order, so the last dimension in this list\n6080             will be contiguous in the resulting DataFrame. This has a major\n6081             influence on which operations are efficient on the resulting dask\n6082             dataframe.\n6083 \n6084             If provided, must include all dimensions of this dataset. By\n6085             default, dimensions are sorted alphabetically.\n6086         set_index : bool, default: False\n6087             If set_index=True, the dask DataFrame is indexed by this dataset's\n6088             coordinate. Since dask DataFrames do not support multi-indexes,\n6089             set_index only works if the dataset only contains one dimension.\n6090 \n6091         Returns\n6092         -------\n6093         dask.dataframe.DataFrame\n6094         \"\"\"\n6095 \n6096         import dask.array as da\n6097         import dask.dataframe as dd\n6098 \n6099         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n6100 \n6101         columns = list(ordered_dims)\n6102         columns.extend(k for k in self.coords if k not in self.dims)\n6103         columns.extend(self.data_vars)\n6104 \n6105         series_list = []\n6106         for name in columns:\n6107             try:\n6108                 var = self.variables[name]\n6109             except KeyError:\n6110                 # dimension without a matching coordinate\n6111                 size = self.dims[name]\n6112                 data = da.arange(size, chunks=size, dtype=np.int64)\n6113                 var = Variable((name,), data)\n6114 \n6115             # IndexVariable objects have a dummy .chunk() method\n6116             if isinstance(var, IndexVariable):\n6117                 var = var.to_base_variable()\n6118 \n6119             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n6120             series = dd.from_array(dask_array.reshape(-1), columns=[name])\n6121             series_list.append(series)\n6122 \n6123         df = dd.concat(series_list, axis=1)\n6124 \n6125         if set_index:\n6126             dim_order = [*ordered_dims]\n6127 \n6128             if len(dim_order) == 1:\n6129                 (dim,) = dim_order\n6130                 df = df.set_index(dim)\n6131             else:\n6132                 # triggers an error about multi-indexes, even if only one\n6133                 # dimension is passed\n6134                 df = df.set_index(dim_order)\n6135 \n6136         return df\n6137 \n6138     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:\n6139         \"\"\"\n6140         Convert this dataset to a dictionary following xarray naming\n6141         conventions.\n6142 \n6143         Converts all variables and attributes to native Python objects\n6144         Useful for converting to json. To avoid datetime incompatibility\n6145         use decode_times=False kwarg in xarrray.open_dataset.\n6146 \n6147         Parameters\n6148         ----------\n6149         data : bool, default: True\n6150             Whether to include the actual data in the dictionary. When set to\n6151             False, returns just the schema.\n6152         encoding : bool, default: False\n6153             Whether to include the Dataset's encoding in the dictionary.\n6154 \n6155         Returns\n6156         -------\n6157         d : dict\n6158             Dict with keys: \"coords\", \"attrs\", \"dims\", \"data_vars\" and optionally\n6159             \"encoding\".\n6160 \n6161         See Also\n6162         --------\n6163         Dataset.from_dict\n6164         DataArray.to_dict\n6165         \"\"\"\n6166         d: dict = {\n6167             \"coords\": {},\n6168             \"attrs\": decode_numpy_dict_values(self.attrs),\n6169             \"dims\": dict(self.dims),\n6170             \"data_vars\": {},\n6171         }\n6172         for k in self.coords:\n6173             d[\"coords\"].update(\n6174                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}\n6175             )\n6176         for k in self.data_vars:\n6177             d[\"data_vars\"].update(\n6178                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}\n6179             )\n6180         if encoding:\n6181             d[\"encoding\"] = dict(self.encoding)\n6182         return d\n6183 \n6184     @classmethod\n6185     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:\n6186         \"\"\"Convert a dictionary into an xarray.Dataset.\n6187 \n6188         Parameters\n6189         ----------\n6190         d : dict-like\n6191             Mapping with a minimum structure of\n6192                 ``{\"var_0\": {\"dims\": [..], \"data\": [..]}, \\\n6193                             ...}``\n6194 \n6195         Returns\n6196         -------\n6197         obj : Dataset\n6198 \n6199         See also\n6200         --------\n6201         Dataset.to_dict\n6202         DataArray.from_dict\n6203 \n6204         Examples\n6205         --------\n6206         >>> d = {\n6207         ...     \"t\": {\"dims\": (\"t\"), \"data\": [0, 1, 2]},\n6208         ...     \"a\": {\"dims\": (\"t\"), \"data\": [\"a\", \"b\", \"c\"]},\n6209         ...     \"b\": {\"dims\": (\"t\"), \"data\": [10, 20, 30]},\n6210         ... }\n6211         >>> ds = xr.Dataset.from_dict(d)\n6212         >>> ds\n6213         <xarray.Dataset>\n6214         Dimensions:  (t: 3)\n6215         Coordinates:\n6216           * t        (t) int64 0 1 2\n6217         Data variables:\n6218             a        (t) <U1 'a' 'b' 'c'\n6219             b        (t) int64 10 20 30\n6220 \n6221         >>> d = {\n6222         ...     \"coords\": {\n6223         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n6224         ...     },\n6225         ...     \"attrs\": {\"title\": \"air temperature\"},\n6226         ...     \"dims\": \"t\",\n6227         ...     \"data_vars\": {\n6228         ...         \"a\": {\"dims\": \"t\", \"data\": [10, 20, 30]},\n6229         ...         \"b\": {\"dims\": \"t\", \"data\": [\"a\", \"b\", \"c\"]},\n6230         ...     },\n6231         ... }\n6232         >>> ds = xr.Dataset.from_dict(d)\n6233         >>> ds\n6234         <xarray.Dataset>\n6235         Dimensions:  (t: 3)\n6236         Coordinates:\n6237           * t        (t) int64 0 1 2\n6238         Data variables:\n6239             a        (t) int64 10 20 30\n6240             b        (t) <U1 'a' 'b' 'c'\n6241         Attributes:\n6242             title:    air temperature\n6243 \n6244         \"\"\"\n6245 \n6246         variables: Iterable[tuple[Hashable, Any]]\n6247         if not {\"coords\", \"data_vars\"}.issubset(set(d)):\n6248             variables = d.items()\n6249         else:\n6250             import itertools\n6251 \n6252             variables = itertools.chain(\n6253                 d.get(\"coords\", {}).items(), d.get(\"data_vars\", {}).items()\n6254             )\n6255         try:\n6256             variable_dict = {\n6257                 k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\")) for k, v in variables\n6258             }\n6259         except KeyError as e:\n6260             raise ValueError(\n6261                 \"cannot convert dict without the key \"\n6262                 \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n6263             )\n6264         obj = cls(variable_dict)\n6265 \n6266         # what if coords aren't dims?\n6267         coords = set(d.get(\"coords\", {})) - set(d.get(\"dims\", {}))\n6268         obj = obj.set_coords(coords)\n6269 \n6270         obj.attrs.update(d.get(\"attrs\", {}))\n6271         obj.encoding.update(d.get(\"encoding\", {}))\n6272 \n6273         return obj\n6274 \n6275     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:\n6276         variables = {}\n6277         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n6278         if keep_attrs is None:\n6279             keep_attrs = _get_keep_attrs(default=True)\n6280         for k, v in self._variables.items():\n6281             if k in self._coord_names:\n6282                 variables[k] = v\n6283             else:\n6284                 variables[k] = f(v, *args, **kwargs)\n6285                 if keep_attrs:\n6286                     variables[k].attrs = v._attrs\n6287         attrs = self._attrs if keep_attrs else None\n6288         return self._replace_with_new_dims(variables, attrs=attrs)\n6289 \n6290     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:\n6291         from .dataarray import DataArray\n6292         from .groupby import GroupBy\n6293 \n6294         if isinstance(other, GroupBy):\n6295             return NotImplemented\n6296         align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n6297         if isinstance(other, (DataArray, Dataset)):\n6298             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]\n6299         g = f if not reflexive else lambda x, y: f(y, x)\n6300         ds = self._calculate_binary_op(g, other, join=align_type)\n6301         return ds\n6302 \n6303     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:\n6304         from .dataarray import DataArray\n6305         from .groupby import GroupBy\n6306 \n6307         if isinstance(other, GroupBy):\n6308             raise TypeError(\n6309                 \"in-place operations between a Dataset and \"\n6310                 \"a grouped object are not permitted\"\n6311             )\n6312         # we don't actually modify arrays in-place with in-place Dataset\n6313         # arithmetic -- this lets us automatically align things\n6314         if isinstance(other, (DataArray, Dataset)):\n6315             other = other.reindex_like(self, copy=False)\n6316         g = ops.inplace_to_noninplace_op(f)\n6317         ds = self._calculate_binary_op(g, other, inplace=True)\n6318         self._replace_with_new_dims(\n6319             ds._variables,\n6320             ds._coord_names,\n6321             attrs=ds._attrs,\n6322             indexes=ds._indexes,\n6323             inplace=True,\n6324         )\n6325         return self\n6326 \n6327     def _calculate_binary_op(\n6328         self, f, other, join=\"inner\", inplace: bool = False\n6329     ) -> Dataset:\n6330         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n6331             if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n6332                 raise ValueError(\n6333                     \"datasets must have the same data variables \"\n6334                     f\"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}\"\n6335                 )\n6336 \n6337             dest_vars = {}\n6338 \n6339             for k in lhs_data_vars:\n6340                 if k in rhs_data_vars:\n6341                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n6342                 elif join in [\"left\", \"outer\"]:\n6343                     dest_vars[k] = f(lhs_vars[k], np.nan)\n6344             for k in rhs_data_vars:\n6345                 if k not in dest_vars and join in [\"right\", \"outer\"]:\n6346                     dest_vars[k] = f(rhs_vars[k], np.nan)\n6347             return dest_vars\n6348 \n6349         if utils.is_dict_like(other) and not isinstance(other, Dataset):\n6350             # can't use our shortcut of doing the binary operation with\n6351             # Variable objects, so apply over our data vars instead.\n6352             new_data_vars = apply_over_both(\n6353                 self.data_vars, other, self.data_vars, other\n6354             )\n6355             return type(self)(new_data_vars)\n6356 \n6357         other_coords: Coordinates | None = getattr(other, \"coords\", None)\n6358         ds = self.coords.merge(other_coords)\n6359 \n6360         if isinstance(other, Dataset):\n6361             new_vars = apply_over_both(\n6362                 self.data_vars, other.data_vars, self.variables, other.variables\n6363             )\n6364         else:\n6365             other_variable = getattr(other, \"variable\", other)\n6366             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n6367         ds._variables.update(new_vars)\n6368         ds._dims = calculate_dimensions(ds._variables)\n6369         return ds\n6370 \n6371     def _copy_attrs_from(self, other):\n6372         self.attrs = other.attrs\n6373         for v in other.variables:\n6374             if v in self.variables:\n6375                 self.variables[v].attrs = other.variables[v].attrs\n6376 \n6377     def diff(\n6378         self: T_Dataset,\n6379         dim: Hashable,\n6380         n: int = 1,\n6381         label: Literal[\"upper\", \"lower\"] = \"upper\",\n6382     ) -> T_Dataset:\n6383         \"\"\"Calculate the n-th order discrete difference along given axis.\n6384 \n6385         Parameters\n6386         ----------\n6387         dim : Hashable\n6388             Dimension over which to calculate the finite difference.\n6389         n : int, default: 1\n6390             The number of times values are differenced.\n6391         label : {\"upper\", \"lower\"}, default: \"upper\"\n6392             The new coordinate in dimension ``dim`` will have the\n6393             values of either the minuend's or subtrahend's coordinate\n6394             for values 'upper' and 'lower', respectively.\n6395 \n6396         Returns\n6397         -------\n6398         difference : Dataset\n6399             The n-th order finite difference of this object.\n6400 \n6401         Notes\n6402         -----\n6403         `n` matches numpy's behavior and is different from pandas' first argument named\n6404         `periods`.\n6405 \n6406         Examples\n6407         --------\n6408         >>> ds = xr.Dataset({\"foo\": (\"x\", [5, 5, 6, 6])})\n6409         >>> ds.diff(\"x\")\n6410         <xarray.Dataset>\n6411         Dimensions:  (x: 3)\n6412         Dimensions without coordinates: x\n6413         Data variables:\n6414             foo      (x) int64 0 1 0\n6415         >>> ds.diff(\"x\", 2)\n6416         <xarray.Dataset>\n6417         Dimensions:  (x: 2)\n6418         Dimensions without coordinates: x\n6419         Data variables:\n6420             foo      (x) int64 1 -1\n6421 \n6422         See Also\n6423         --------\n6424         Dataset.differentiate\n6425         \"\"\"\n6426         if n == 0:\n6427             return self\n6428         if n < 0:\n6429             raise ValueError(f\"order `n` must be non-negative but got {n}\")\n6430 \n6431         # prepare slices\n6432         slice_start = {dim: slice(None, -1)}\n6433         slice_end = {dim: slice(1, None)}\n6434 \n6435         # prepare new coordinate\n6436         if label == \"upper\":\n6437             slice_new = slice_end\n6438         elif label == \"lower\":\n6439             slice_new = slice_start\n6440         else:\n6441             raise ValueError(\"The 'label' argument has to be either 'upper' or 'lower'\")\n6442 \n6443         indexes, index_vars = isel_indexes(self.xindexes, slice_new)\n6444         variables = {}\n6445 \n6446         for name, var in self.variables.items():\n6447             if name in index_vars:\n6448                 variables[name] = index_vars[name]\n6449             elif dim in var.dims:\n6450                 if name in self.data_vars:\n6451                     variables[name] = var.isel(slice_end) - var.isel(slice_start)\n6452                 else:\n6453                     variables[name] = var.isel(slice_new)\n6454             else:\n6455                 variables[name] = var\n6456 \n6457         difference = self._replace_with_new_dims(variables, indexes=indexes)\n6458 \n6459         if n > 1:\n6460             return difference.diff(dim, n - 1)\n6461         else:\n6462             return difference\n6463 \n6464     def shift(\n6465         self: T_Dataset,\n6466         shifts: Mapping[Any, int] | None = None,\n6467         fill_value: Any = xrdtypes.NA,\n6468         **shifts_kwargs: int,\n6469     ) -> T_Dataset:\n6470 \n6471         \"\"\"Shift this dataset by an offset along one or more dimensions.\n6472 \n6473         Only data variables are moved; coordinates stay in place. This is\n6474         consistent with the behavior of ``shift`` in pandas.\n6475 \n6476         Values shifted from beyond array bounds will appear at one end of\n6477         each dimension, which are filled according to `fill_value`. For periodic\n6478         offsets instead see `roll`.\n6479 \n6480         Parameters\n6481         ----------\n6482         shifts : mapping of hashable to int\n6483             Integer offset to shift along each of the given dimensions.\n6484             Positive offsets shift to the right; negative offsets shift to the\n6485             left.\n6486         fill_value : scalar or dict-like, optional\n6487             Value to use for newly missing values. If a dict-like, maps\n6488             variable names (including coordinates) to fill values.\n6489         **shifts_kwargs\n6490             The keyword arguments form of ``shifts``.\n6491             One of shifts or shifts_kwargs must be provided.\n6492 \n6493         Returns\n6494         -------\n6495         shifted : Dataset\n6496             Dataset with the same coordinates and attributes but shifted data\n6497             variables.\n6498 \n6499         See Also\n6500         --------\n6501         roll\n6502 \n6503         Examples\n6504         --------\n6505         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n6506         >>> ds.shift(x=2)\n6507         <xarray.Dataset>\n6508         Dimensions:  (x: 5)\n6509         Dimensions without coordinates: x\n6510         Data variables:\n6511             foo      (x) object nan nan 'a' 'b' 'c'\n6512         \"\"\"\n6513         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n6514         invalid = [k for k in shifts if k not in self.dims]\n6515         if invalid:\n6516             raise ValueError(f\"dimensions {invalid!r} do not exist\")\n6517 \n6518         variables = {}\n6519         for name, var in self.variables.items():\n6520             if name in self.data_vars:\n6521                 fill_value_ = (\n6522                     fill_value.get(name, xrdtypes.NA)\n6523                     if isinstance(fill_value, dict)\n6524                     else fill_value\n6525                 )\n6526 \n6527                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}\n6528                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)\n6529             else:\n6530                 variables[name] = var\n6531 \n6532         return self._replace(variables)\n6533 \n6534     def roll(\n6535         self: T_Dataset,\n6536         shifts: Mapping[Any, int] | None = None,\n6537         roll_coords: bool = False,\n6538         **shifts_kwargs: int,\n6539     ) -> T_Dataset:\n6540         \"\"\"Roll this dataset by an offset along one or more dimensions.\n6541 \n6542         Unlike shift, roll treats the given dimensions as periodic, so will not\n6543         create any missing values to be filled.\n6544 \n6545         Also unlike shift, roll may rotate all variables, including coordinates\n6546         if specified. The direction of rotation is consistent with\n6547         :py:func:`numpy.roll`.\n6548 \n6549         Parameters\n6550         ----------\n6551         shifts : mapping of hashable to int, optional\n6552             A dict with keys matching dimensions and values given\n6553             by integers to rotate each of the given dimensions. Positive\n6554             offsets roll to the right; negative offsets roll to the left.\n6555         roll_coords : bool, default: False\n6556             Indicates whether to roll the coordinates by the offset too.\n6557         **shifts_kwargs : {dim: offset, ...}, optional\n6558             The keyword arguments form of ``shifts``.\n6559             One of shifts or shifts_kwargs must be provided.\n6560 \n6561         Returns\n6562         -------\n6563         rolled : Dataset\n6564             Dataset with the same attributes but rolled data and coordinates.\n6565 \n6566         See Also\n6567         --------\n6568         shift\n6569 \n6570         Examples\n6571         --------\n6572         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))}, coords={\"x\": np.arange(5)})\n6573         >>> ds.roll(x=2)\n6574         <xarray.Dataset>\n6575         Dimensions:  (x: 5)\n6576         Coordinates:\n6577           * x        (x) int64 0 1 2 3 4\n6578         Data variables:\n6579             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n6580 \n6581         >>> ds.roll(x=2, roll_coords=True)\n6582         <xarray.Dataset>\n6583         Dimensions:  (x: 5)\n6584         Coordinates:\n6585           * x        (x) int64 3 4 0 1 2\n6586         Data variables:\n6587             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n6588 \n6589         \"\"\"\n6590         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n6591         invalid = [k for k in shifts if k not in self.dims]\n6592         if invalid:\n6593             raise ValueError(f\"dimensions {invalid!r} do not exist\")\n6594 \n6595         unrolled_vars: tuple[Hashable, ...]\n6596 \n6597         if roll_coords:\n6598             indexes, index_vars = roll_indexes(self.xindexes, shifts)\n6599             unrolled_vars = ()\n6600         else:\n6601             indexes = dict(self._indexes)\n6602             index_vars = dict(self.xindexes.variables)\n6603             unrolled_vars = tuple(self.coords)\n6604 \n6605         variables = {}\n6606         for k, var in self.variables.items():\n6607             if k in index_vars:\n6608                 variables[k] = index_vars[k]\n6609             elif k not in unrolled_vars:\n6610                 variables[k] = var.roll(\n6611                     shifts={k: s for k, s in shifts.items() if k in var.dims}\n6612                 )\n6613             else:\n6614                 variables[k] = var\n6615 \n6616         return self._replace(variables, indexes=indexes)\n6617 \n6618     def sortby(\n6619         self: T_Dataset,\n6620         variables: Hashable | DataArray | list[Hashable | DataArray],\n6621         ascending: bool = True,\n6622     ) -> T_Dataset:\n6623         \"\"\"\n6624         Sort object by labels or values (along an axis).\n6625 \n6626         Sorts the dataset, either along specified dimensions,\n6627         or according to values of 1-D dataarrays that share dimension\n6628         with calling object.\n6629 \n6630         If the input variables are dataarrays, then the dataarrays are aligned\n6631         (via left-join) to the calling object prior to sorting by cell values.\n6632         NaNs are sorted to the end, following Numpy convention.\n6633 \n6634         If multiple sorts along the same dimension is\n6635         given, numpy's lexsort is performed along that dimension:\n6636         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n6637         and the FIRST key in the sequence is used as the primary sort key,\n6638         followed by the 2nd key, etc.\n6639 \n6640         Parameters\n6641         ----------\n6642         variables : Hashable, DataArray, or list of hashable or DataArray\n6643             1D DataArray objects or name(s) of 1D variable(s) in\n6644             coords/data_vars whose values are used to sort the dataset.\n6645         ascending : bool, default: True\n6646             Whether to sort by ascending or descending order.\n6647 \n6648         Returns\n6649         -------\n6650         sorted : Dataset\n6651             A new dataset where all the specified dims are sorted by dim\n6652             labels.\n6653 \n6654         See Also\n6655         --------\n6656         DataArray.sortby\n6657         numpy.sort\n6658         pandas.sort_values\n6659         pandas.sort_index\n6660 \n6661         Examples\n6662         --------\n6663         >>> ds = xr.Dataset(\n6664         ...     {\n6665         ...         \"A\": ((\"x\", \"y\"), [[1, 2], [3, 4]]),\n6666         ...         \"B\": ((\"x\", \"y\"), [[5, 6], [7, 8]]),\n6667         ...     },\n6668         ...     coords={\"x\": [\"b\", \"a\"], \"y\": [1, 0]},\n6669         ... )\n6670         >>> ds.sortby(\"x\")\n6671         <xarray.Dataset>\n6672         Dimensions:  (x: 2, y: 2)\n6673         Coordinates:\n6674           * x        (x) <U1 'a' 'b'\n6675           * y        (y) int64 1 0\n6676         Data variables:\n6677             A        (x, y) int64 3 4 1 2\n6678             B        (x, y) int64 7 8 5 6\n6679         \"\"\"\n6680         from .dataarray import DataArray\n6681 \n6682         if not isinstance(variables, list):\n6683             variables = [variables]\n6684         else:\n6685             variables = variables\n6686         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]\n6687         aligned_vars = align(self, *arrays, join=\"left\")  # type: ignore[type-var]\n6688         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]\n6689         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]\n6690         vars_by_dim = defaultdict(list)\n6691         for data_array in aligned_other_vars:\n6692             if data_array.ndim != 1:\n6693                 raise ValueError(\"Input DataArray is not 1-D.\")\n6694             (key,) = data_array.dims\n6695             vars_by_dim[key].append(data_array)\n6696 \n6697         indices = {}\n6698         for key, arrays in vars_by_dim.items():\n6699             order = np.lexsort(tuple(reversed(arrays)))\n6700             indices[key] = order if ascending else order[::-1]\n6701         return aligned_self.isel(indices)\n6702 \n6703     def quantile(\n6704         self: T_Dataset,\n6705         q: ArrayLike,\n6706         dim: str | Iterable[Hashable] | None = None,\n6707         method: QUANTILE_METHODS = \"linear\",\n6708         numeric_only: bool = False,\n6709         keep_attrs: bool = None,\n6710         skipna: bool = None,\n6711         interpolation: QUANTILE_METHODS = None,\n6712     ) -> T_Dataset:\n6713         \"\"\"Compute the qth quantile of the data along the specified dimension.\n6714 \n6715         Returns the qth quantiles(s) of the array elements for each variable\n6716         in the Dataset.\n6717 \n6718         Parameters\n6719         ----------\n6720         q : float or array-like of float\n6721             Quantile to compute, which must be between 0 and 1 inclusive.\n6722         dim : str or Iterable of Hashable, optional\n6723             Dimension(s) over which to apply quantile.\n6724         method : str, default: \"linear\"\n6725             This optional parameter specifies the interpolation method to use when the\n6726             desired quantile lies between two data points. The options sorted by their R\n6727             type as summarized in the H&F paper [1]_ are:\n6728 \n6729                 1. \"inverted_cdf\" (*)\n6730                 2. \"averaged_inverted_cdf\" (*)\n6731                 3. \"closest_observation\" (*)\n6732                 4. \"interpolated_inverted_cdf\" (*)\n6733                 5. \"hazen\" (*)\n6734                 6. \"weibull\" (*)\n6735                 7. \"linear\"  (default)\n6736                 8. \"median_unbiased\" (*)\n6737                 9. \"normal_unbiased\" (*)\n6738 \n6739             The first three methods are discontiuous.  The following discontinuous\n6740             variations of the default \"linear\" (7.) option are also available:\n6741 \n6742                 * \"lower\"\n6743                 * \"higher\"\n6744                 * \"midpoint\"\n6745                 * \"nearest\"\n6746 \n6747             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n6748             was previously called \"interpolation\", renamed in accordance with numpy\n6749             version 1.22.0.\n6750 \n6751             (*) These methods require numpy version 1.22 or newer.\n6752 \n6753         keep_attrs : bool, optional\n6754             If True, the dataset's attributes (`attrs`) will be copied from\n6755             the original object to the new one.  If False (default), the new\n6756             object will be returned without attributes.\n6757         numeric_only : bool, optional\n6758             If True, only apply ``func`` to variables with a numeric dtype.\n6759         skipna : bool, optional\n6760             If True, skip missing values (as marked by NaN). By default, only\n6761             skips missing values for float dtypes; other dtypes either do not\n6762             have a sentinel missing value (int) or skipna=True has not been\n6763             implemented (object, datetime64 or timedelta64).\n6764 \n6765         Returns\n6766         -------\n6767         quantiles : Dataset\n6768             If `q` is a single quantile, then the result is a scalar for each\n6769             variable in data_vars. If multiple percentiles are given, first\n6770             axis of the result corresponds to the quantile and a quantile\n6771             dimension is added to the return Dataset. The other dimensions are\n6772             the dimensions that remain after the reduction of the array.\n6773 \n6774         See Also\n6775         --------\n6776         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n6777 \n6778         Examples\n6779         --------\n6780         >>> ds = xr.Dataset(\n6781         ...     {\"a\": ((\"x\", \"y\"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n6782         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n6783         ... )\n6784         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n6785         <xarray.Dataset>\n6786         Dimensions:   ()\n6787         Coordinates:\n6788             quantile  float64 0.0\n6789         Data variables:\n6790             a         float64 0.7\n6791         >>> ds.quantile(0, dim=\"x\")\n6792         <xarray.Dataset>\n6793         Dimensions:   (y: 4)\n6794         Coordinates:\n6795           * y         (y) float64 1.0 1.5 2.0 2.5\n6796             quantile  float64 0.0\n6797         Data variables:\n6798             a         (y) float64 0.7 4.2 2.6 1.5\n6799         >>> ds.quantile([0, 0.5, 1])\n6800         <xarray.Dataset>\n6801         Dimensions:   (quantile: 3)\n6802         Coordinates:\n6803           * quantile  (quantile) float64 0.0 0.5 1.0\n6804         Data variables:\n6805             a         (quantile) float64 0.7 3.4 9.4\n6806         >>> ds.quantile([0, 0.5, 1], dim=\"x\")\n6807         <xarray.Dataset>\n6808         Dimensions:   (quantile: 3, y: 4)\n6809         Coordinates:\n6810           * y         (y) float64 1.0 1.5 2.0 2.5\n6811           * quantile  (quantile) float64 0.0 0.5 1.0\n6812         Data variables:\n6813             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n6814 \n6815         References\n6816         ----------\n6817         .. [1] R. J. Hyndman and Y. Fan,\n6818            \"Sample quantiles in statistical packages,\"\n6819            The American Statistician, 50(4), pp. 361-365, 1996\n6820         \"\"\"\n6821 \n6822         # interpolation renamed to method in version 0.21.0\n6823         # check here and in variable to avoid repeated warnings\n6824         if interpolation is not None:\n6825             warnings.warn(\n6826                 \"The `interpolation` argument to quantile was renamed to `method`.\",\n6827                 FutureWarning,\n6828             )\n6829 \n6830             if method != \"linear\":\n6831                 raise TypeError(\"Cannot pass interpolation and method keywords!\")\n6832 \n6833             method = interpolation\n6834 \n6835         dims: set[Hashable]\n6836         if isinstance(dim, str):\n6837             dims = {dim}\n6838         elif dim is None or dim is ...:\n6839             dims = set(self.dims)\n6840         else:\n6841             dims = set(dim)\n6842 \n6843         _assert_empty(\n6844             tuple(d for d in dims if d not in self.dims),\n6845             \"Dataset does not contain the dimensions: %s\",\n6846         )\n6847 \n6848         q = np.asarray(q, dtype=np.float64)\n6849 \n6850         variables = {}\n6851         for name, var in self.variables.items():\n6852             reduce_dims = [d for d in var.dims if d in dims]\n6853             if reduce_dims or not var.dims:\n6854                 if name not in self.coords:\n6855                     if (\n6856                         not numeric_only\n6857                         or np.issubdtype(var.dtype, np.number)\n6858                         or var.dtype == np.bool_\n6859                     ):\n6860                         variables[name] = var.quantile(\n6861                             q,\n6862                             dim=reduce_dims,\n6863                             method=method,\n6864                             keep_attrs=keep_attrs,\n6865                             skipna=skipna,\n6866                         )\n6867 \n6868             else:\n6869                 variables[name] = var\n6870 \n6871         # construct the new dataset\n6872         coord_names = {k for k in self.coords if k in variables}\n6873         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n6874         if keep_attrs is None:\n6875             keep_attrs = _get_keep_attrs(default=False)\n6876         attrs = self.attrs if keep_attrs else None\n6877         new = self._replace_with_new_dims(\n6878             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n6879         )\n6880         return new.assign_coords(quantile=q)\n6881 \n6882     def rank(\n6883         self: T_Dataset,\n6884         dim: Hashable,\n6885         pct: bool = False,\n6886         keep_attrs: bool | None = None,\n6887     ) -> T_Dataset:\n6888         \"\"\"Ranks the data.\n6889 \n6890         Equal values are assigned a rank that is the average of the ranks that\n6891         would have been otherwise assigned to all of the values within\n6892         that set.\n6893         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n6894 \n6895         NaNs in the input array are returned as NaNs.\n6896 \n6897         The `bottleneck` library is required.\n6898 \n6899         Parameters\n6900         ----------\n6901         dim : Hashable\n6902             Dimension over which to compute rank.\n6903         pct : bool, default: False\n6904             If True, compute percentage ranks, otherwise compute integer ranks.\n6905         keep_attrs : bool or None, optional\n6906             If True, the dataset's attributes (`attrs`) will be copied from\n6907             the original object to the new one.  If False, the new\n6908             object will be returned without attributes.\n6909 \n6910         Returns\n6911         -------\n6912         ranked : Dataset\n6913             Variables that do not depend on `dim` are dropped.\n6914         \"\"\"\n6915         if not OPTIONS[\"use_bottleneck\"]:\n6916             raise RuntimeError(\n6917                 \"rank requires bottleneck to be enabled.\"\n6918                 \" Call `xr.set_options(use_bottleneck=True)` to enable it.\"\n6919             )\n6920 \n6921         if dim not in self.dims:\n6922             raise ValueError(f\"Dataset does not contain the dimension: {dim}\")\n6923 \n6924         variables = {}\n6925         for name, var in self.variables.items():\n6926             if name in self.data_vars:\n6927                 if dim in var.dims:\n6928                     variables[name] = var.rank(dim, pct=pct)\n6929             else:\n6930                 variables[name] = var\n6931 \n6932         coord_names = set(self.coords)\n6933         if keep_attrs is None:\n6934             keep_attrs = _get_keep_attrs(default=False)\n6935         attrs = self.attrs if keep_attrs else None\n6936         return self._replace(variables, coord_names, attrs=attrs)\n6937 \n6938     def differentiate(\n6939         self: T_Dataset,\n6940         coord: Hashable,\n6941         edge_order: Literal[1, 2] = 1,\n6942         datetime_unit: DatetimeUnitOptions | None = None,\n6943     ) -> T_Dataset:\n6944         \"\"\" Differentiate with the second order accurate central\n6945         differences.\n6946 \n6947         .. note::\n6948             This feature is limited to simple cartesian geometry, i.e. coord\n6949             must be one dimensional.\n6950 \n6951         Parameters\n6952         ----------\n6953         coord : Hashable\n6954             The coordinate to be used to compute the gradient.\n6955         edge_order : {1, 2}, default: 1\n6956             N-th order accurate differences at the boundaries.\n6957         datetime_unit : None or {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n6958             \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, default: None\n6959             Unit to compute gradient. Only valid for datetime coordinate.\n6960 \n6961         Returns\n6962         -------\n6963         differentiated: Dataset\n6964 \n6965         See also\n6966         --------\n6967         numpy.gradient: corresponding numpy function\n6968         \"\"\"\n6969         from .variable import Variable\n6970 \n6971         if coord not in self.variables and coord not in self.dims:\n6972             raise ValueError(f\"Coordinate {coord} does not exist.\")\n6973 \n6974         coord_var = self[coord].variable\n6975         if coord_var.ndim != 1:\n6976             raise ValueError(\n6977                 \"Coordinate {} must be 1 dimensional but is {}\"\n6978                 \" dimensional\".format(coord, coord_var.ndim)\n6979             )\n6980 \n6981         dim = coord_var.dims[0]\n6982         if _contains_datetime_like_objects(coord_var):\n6983             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n6984                 datetime_unit = cast(\n6985                     \"DatetimeUnitOptions\", np.datetime_data(coord_var.dtype)[0]\n6986                 )\n6987             elif datetime_unit is None:\n6988                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n6989             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n6990 \n6991         variables = {}\n6992         for k, v in self.variables.items():\n6993             if k in self.data_vars and dim in v.dims and k not in self.coords:\n6994                 if _contains_datetime_like_objects(v):\n6995                     v = v._to_numeric(datetime_unit=datetime_unit)\n6996                 grad = duck_array_ops.gradient(\n6997                     v.data,\n6998                     coord_var.data,\n6999                     edge_order=edge_order,\n7000                     axis=v.get_axis_num(dim),\n7001                 )\n7002                 variables[k] = Variable(v.dims, grad)\n7003             else:\n7004                 variables[k] = v\n7005         return self._replace(variables)\n7006 \n7007     def integrate(\n7008         self: T_Dataset,\n7009         coord: Hashable | Sequence[Hashable],\n7010         datetime_unit: DatetimeUnitOptions = None,\n7011     ) -> T_Dataset:\n7012         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n7013 \n7014         .. note::\n7015             This feature is limited to simple cartesian geometry, i.e. coord\n7016             must be one dimensional.\n7017 \n7018         Parameters\n7019         ----------\n7020         coord : hashable, or sequence of hashable\n7021             Coordinate(s) used for the integration.\n7022         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n7023                         'ps', 'fs', 'as', None}, optional\n7024             Specify the unit if datetime coordinate is used.\n7025 \n7026         Returns\n7027         -------\n7028         integrated : Dataset\n7029 \n7030         See also\n7031         --------\n7032         DataArray.integrate\n7033         numpy.trapz : corresponding numpy function\n7034 \n7035         Examples\n7036         --------\n7037         >>> ds = xr.Dataset(\n7038         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n7039         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n7040         ... )\n7041         >>> ds\n7042         <xarray.Dataset>\n7043         Dimensions:  (x: 4)\n7044         Coordinates:\n7045           * x        (x) int64 0 1 2 3\n7046             y        (x) int64 1 7 3 5\n7047         Data variables:\n7048             a        (x) int64 5 5 6 6\n7049             b        (x) int64 1 2 1 0\n7050         >>> ds.integrate(\"x\")\n7051         <xarray.Dataset>\n7052         Dimensions:  ()\n7053         Data variables:\n7054             a        float64 16.5\n7055             b        float64 3.5\n7056         >>> ds.integrate(\"y\")\n7057         <xarray.Dataset>\n7058         Dimensions:  ()\n7059         Data variables:\n7060             a        float64 20.0\n7061             b        float64 4.0\n7062         \"\"\"\n7063         if not isinstance(coord, (list, tuple)):\n7064             coord = (coord,)\n7065         result = self\n7066         for c in coord:\n7067             result = result._integrate_one(c, datetime_unit=datetime_unit)\n7068         return result\n7069 \n7070     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):\n7071         from .variable import Variable\n7072 \n7073         if coord not in self.variables and coord not in self.dims:\n7074             raise ValueError(f\"Coordinate {coord} does not exist.\")\n7075 \n7076         coord_var = self[coord].variable\n7077         if coord_var.ndim != 1:\n7078             raise ValueError(\n7079                 \"Coordinate {} must be 1 dimensional but is {}\"\n7080                 \" dimensional\".format(coord, coord_var.ndim)\n7081             )\n7082 \n7083         dim = coord_var.dims[0]\n7084         if _contains_datetime_like_objects(coord_var):\n7085             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n7086                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n7087             elif datetime_unit is None:\n7088                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n7089             coord_var = coord_var._replace(\n7090                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)\n7091             )\n7092 \n7093         variables = {}\n7094         coord_names = set()\n7095         for k, v in self.variables.items():\n7096             if k in self.coords:\n7097                 if dim not in v.dims or cumulative:\n7098                     variables[k] = v\n7099                     coord_names.add(k)\n7100             else:\n7101                 if k in self.data_vars and dim in v.dims:\n7102                     if _contains_datetime_like_objects(v):\n7103                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)\n7104                     if cumulative:\n7105                         integ = duck_array_ops.cumulative_trapezoid(\n7106                             v.data, coord_var.data, axis=v.get_axis_num(dim)\n7107                         )\n7108                         v_dims = v.dims\n7109                     else:\n7110                         integ = duck_array_ops.trapz(\n7111                             v.data, coord_var.data, axis=v.get_axis_num(dim)\n7112                         )\n7113                         v_dims = list(v.dims)\n7114                         v_dims.remove(dim)\n7115                     variables[k] = Variable(v_dims, integ)\n7116                 else:\n7117                     variables[k] = v\n7118         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n7119         return self._replace_with_new_dims(\n7120             variables, coord_names=coord_names, indexes=indexes\n7121         )\n7122 \n7123     def cumulative_integrate(\n7124         self: T_Dataset,\n7125         coord: Hashable | Sequence[Hashable],\n7126         datetime_unit: DatetimeUnitOptions = None,\n7127     ) -> T_Dataset:\n7128         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n7129 \n7130         .. note::\n7131             This feature is limited to simple cartesian geometry, i.e. coord\n7132             must be one dimensional.\n7133 \n7134             The first entry of the cumulative integral of each variable is always 0, in\n7135             order to keep the length of the dimension unchanged between input and\n7136             output.\n7137 \n7138         Parameters\n7139         ----------\n7140         coord : hashable, or sequence of hashable\n7141             Coordinate(s) used for the integration.\n7142         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n7143                         'ps', 'fs', 'as', None}, optional\n7144             Specify the unit if datetime coordinate is used.\n7145 \n7146         Returns\n7147         -------\n7148         integrated : Dataset\n7149 \n7150         See also\n7151         --------\n7152         DataArray.cumulative_integrate\n7153         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n7154 \n7155         Examples\n7156         --------\n7157         >>> ds = xr.Dataset(\n7158         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n7159         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n7160         ... )\n7161         >>> ds\n7162         <xarray.Dataset>\n7163         Dimensions:  (x: 4)\n7164         Coordinates:\n7165           * x        (x) int64 0 1 2 3\n7166             y        (x) int64 1 7 3 5\n7167         Data variables:\n7168             a        (x) int64 5 5 6 6\n7169             b        (x) int64 1 2 1 0\n7170         >>> ds.cumulative_integrate(\"x\")\n7171         <xarray.Dataset>\n7172         Dimensions:  (x: 4)\n7173         Coordinates:\n7174           * x        (x) int64 0 1 2 3\n7175             y        (x) int64 1 7 3 5\n7176         Data variables:\n7177             a        (x) float64 0.0 5.0 10.5 16.5\n7178             b        (x) float64 0.0 1.5 3.0 3.5\n7179         >>> ds.cumulative_integrate(\"y\")\n7180         <xarray.Dataset>\n7181         Dimensions:  (x: 4)\n7182         Coordinates:\n7183           * x        (x) int64 0 1 2 3\n7184             y        (x) int64 1 7 3 5\n7185         Data variables:\n7186             a        (x) float64 0.0 30.0 8.0 20.0\n7187             b        (x) float64 0.0 9.0 3.0 4.0\n7188         \"\"\"\n7189         if not isinstance(coord, (list, tuple)):\n7190             coord = (coord,)\n7191         result = self\n7192         for c in coord:\n7193             result = result._integrate_one(\n7194                 c, datetime_unit=datetime_unit, cumulative=True\n7195             )\n7196         return result\n7197 \n7198     @property\n7199     def real(self: T_Dataset) -> T_Dataset:\n7200         return self.map(lambda x: x.real, keep_attrs=True)\n7201 \n7202     @property\n7203     def imag(self: T_Dataset) -> T_Dataset:\n7204         return self.map(lambda x: x.imag, keep_attrs=True)\n7205 \n7206     plot = utils.UncachedAccessor(_Dataset_PlotMethods)\n7207 \n7208     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:\n7209         \"\"\"Returns a ``Dataset`` with variables that match specific conditions.\n7210 \n7211         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n7212         containing only the variables for which all the filter tests pass.\n7213         These tests are either ``key=value`` for which the attribute ``key``\n7214         has the exact value ``value`` or the callable passed into\n7215         ``key=callable`` returns True. The callable will be passed a single\n7216         value, either the value of the attribute ``key`` or ``None`` if the\n7217         DataArray does not have an attribute with the name ``key``.\n7218 \n7219         Parameters\n7220         ----------\n7221         **kwargs\n7222             key : str\n7223                 Attribute name.\n7224             value : callable or obj\n7225                 If value is a callable, it should return a boolean in the form\n7226                 of bool = func(attr) where attr is da.attrs[key].\n7227                 Otherwise, value will be compared to the each\n7228                 DataArray's attrs[key].\n7229 \n7230         Returns\n7231         -------\n7232         new : Dataset\n7233             New dataset with variables filtered by attribute.\n7234 \n7235         Examples\n7236         --------\n7237         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n7238         >>> precip = 10 * np.random.rand(2, 2, 3)\n7239         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n7240         >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n7241         >>> dims = [\"x\", \"y\", \"time\"]\n7242         >>> temp_attr = dict(standard_name=\"air_potential_temperature\")\n7243         >>> precip_attr = dict(standard_name=\"convective_precipitation_flux\")\n7244 \n7245         >>> ds = xr.Dataset(\n7246         ...     dict(\n7247         ...         temperature=(dims, temp, temp_attr),\n7248         ...         precipitation=(dims, precip, precip_attr),\n7249         ...     ),\n7250         ...     coords=dict(\n7251         ...         lon=([\"x\", \"y\"], lon),\n7252         ...         lat=([\"x\", \"y\"], lat),\n7253         ...         time=pd.date_range(\"2014-09-06\", periods=3),\n7254         ...         reference_time=pd.Timestamp(\"2014-09-05\"),\n7255         ...     ),\n7256         ... )\n7257 \n7258         Get variables matching a specific standard_name:\n7259 \n7260         >>> ds.filter_by_attrs(standard_name=\"convective_precipitation_flux\")\n7261         <xarray.Dataset>\n7262         Dimensions:         (x: 2, y: 2, time: 3)\n7263         Coordinates:\n7264             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n7265             lat             (x, y) float64 42.25 42.21 42.63 42.59\n7266           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n7267             reference_time  datetime64[ns] 2014-09-05\n7268         Dimensions without coordinates: x, y\n7269         Data variables:\n7270             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n7271 \n7272         Get all variables that have a standard_name attribute:\n7273 \n7274         >>> standard_name = lambda v: v is not None\n7275         >>> ds.filter_by_attrs(standard_name=standard_name)\n7276         <xarray.Dataset>\n7277         Dimensions:         (x: 2, y: 2, time: 3)\n7278         Coordinates:\n7279             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n7280             lat             (x, y) float64 42.25 42.21 42.63 42.59\n7281           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n7282             reference_time  datetime64[ns] 2014-09-05\n7283         Dimensions without coordinates: x, y\n7284         Data variables:\n7285             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n7286             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n7287 \n7288         \"\"\"\n7289         selection = []\n7290         for var_name, variable in self.variables.items():\n7291             has_value_flag = False\n7292             for attr_name, pattern in kwargs.items():\n7293                 attr_value = variable.attrs.get(attr_name)\n7294                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:\n7295                     has_value_flag = True\n7296                 else:\n7297                     has_value_flag = False\n7298                     break\n7299             if has_value_flag is True:\n7300                 selection.append(var_name)\n7301         return self[selection]\n7302 \n7303     def unify_chunks(self: T_Dataset) -> T_Dataset:\n7304         \"\"\"Unify chunk size along all chunked dimensions of this Dataset.\n7305 \n7306         Returns\n7307         -------\n7308         Dataset with consistent chunk sizes for all dask-array variables\n7309 \n7310         See Also\n7311         --------\n7312         dask.array.core.unify_chunks\n7313         \"\"\"\n7314 \n7315         return unify_chunks(self)[0]\n7316 \n7317     def map_blocks(\n7318         self,\n7319         func: Callable[..., T_Xarray],\n7320         args: Sequence[Any] = (),\n7321         kwargs: Mapping[str, Any] | None = None,\n7322         template: DataArray | Dataset | None = None,\n7323     ) -> T_Xarray:\n7324         \"\"\"\n7325         Apply a function to each block of this Dataset.\n7326 \n7327         .. warning::\n7328             This method is experimental and its signature may change.\n7329 \n7330         Parameters\n7331         ----------\n7332         func : callable\n7333             User-provided function that accepts a Dataset as its first\n7334             parameter. The function will receive a subset or 'block' of this Dataset (see below),\n7335             corresponding to one chunk along each chunked dimension. ``func`` will be\n7336             executed as ``func(subset_dataset, *subset_args, **kwargs)``.\n7337 \n7338             This function must return either a single DataArray or a single Dataset.\n7339 \n7340             This function cannot add a new chunked dimension.\n7341         args : sequence\n7342             Passed to func after unpacking and subsetting any xarray objects by blocks.\n7343             xarray objects in args must be aligned with obj, otherwise an error is raised.\n7344         kwargs : Mapping or None\n7345             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n7346             subset to blocks. Passing dask collections in kwargs is not allowed.\n7347         template : DataArray, Dataset or None, optional\n7348             xarray object representing the final result after compute is called. If not provided,\n7349             the function will be first run on mocked-up data, that looks like this object but\n7350             has sizes 0, to determine properties of the returned object such as dtype,\n7351             variable names, attributes, new dimensions and new indexes (if any).\n7352             ``template`` must be provided if the function changes the size of existing dimensions.\n7353             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n7354             ``attrs`` set by ``func`` will be ignored.\n7355 \n7356         Returns\n7357         -------\n7358         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n7359         function.\n7360 \n7361         Notes\n7362         -----\n7363         This function is designed for when ``func`` needs to manipulate a whole xarray object\n7364         subset to each block. Each block is loaded into memory. In the more common case where\n7365         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n7366 \n7367         If none of the variables in this object is backed by dask arrays, calling this function is\n7368         equivalent to calling ``func(obj, *args, **kwargs)``.\n7369 \n7370         See Also\n7371         --------\n7372         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n7373         xarray.DataArray.map_blocks\n7374 \n7375         Examples\n7376         --------\n7377         Calculate an anomaly from climatology using ``.groupby()``. Using\n7378         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n7379         its indices, and its methods like ``.groupby()``.\n7380 \n7381         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n7382         ...     gb = da.groupby(groupby_type)\n7383         ...     clim = gb.mean(dim=\"time\")\n7384         ...     return gb - clim\n7385         ...\n7386         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n7387         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n7388         >>> np.random.seed(123)\n7389         >>> array = xr.DataArray(\n7390         ...     np.random.rand(len(time)),\n7391         ...     dims=[\"time\"],\n7392         ...     coords={\"time\": time, \"month\": month},\n7393         ... ).chunk()\n7394         >>> ds = xr.Dataset({\"a\": array})\n7395         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()\n7396         <xarray.Dataset>\n7397         Dimensions:  (time: 24)\n7398         Coordinates:\n7399           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n7400             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n7401         Data variables:\n7402             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901\n7403 \n7404         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n7405         to the function being applied in ``xr.map_blocks()``:\n7406 \n7407         >>> ds.map_blocks(\n7408         ...     calculate_anomaly,\n7409         ...     kwargs={\"groupby_type\": \"time.year\"},\n7410         ...     template=ds,\n7411         ... )\n7412         <xarray.Dataset>\n7413         Dimensions:  (time: 24)\n7414         Coordinates:\n7415           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n7416             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n7417         Data variables:\n7418             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>\n7419         \"\"\"\n7420         from .parallel import map_blocks\n7421 \n7422         return map_blocks(func, self, args, kwargs, template)\n7423 \n7424     def polyfit(\n7425         self: T_Dataset,\n7426         dim: Hashable,\n7427         deg: int,\n7428         skipna: bool | None = None,\n7429         rcond: float | None = None,\n7430         w: Hashable | Any = None,\n7431         full: bool = False,\n7432         cov: bool | Literal[\"unscaled\"] = False,\n7433     ) -> T_Dataset:\n7434         \"\"\"\n7435         Least squares polynomial fit.\n7436 \n7437         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n7438         invalid values when `skipna = True`.\n7439 \n7440         Parameters\n7441         ----------\n7442         dim : hashable\n7443             Coordinate along which to fit the polynomials.\n7444         deg : int\n7445             Degree of the fitting polynomial.\n7446         skipna : bool or None, optional\n7447             If True, removes all invalid values before fitting each 1D slices of the array.\n7448             Default is True if data is stored in a dask.array or if there is any\n7449             invalid values, False otherwise.\n7450         rcond : float or None, optional\n7451             Relative condition number to the fit.\n7452         w : hashable or Any, optional\n7453             Weights to apply to the y-coordinate of the sample points.\n7454             Can be an array-like object or the name of a coordinate in the dataset.\n7455         full : bool, default: False\n7456             Whether to return the residuals, matrix rank and singular values in addition\n7457             to the coefficients.\n7458         cov : bool or \"unscaled\", default: False\n7459             Whether to return to the covariance matrix in addition to the coefficients.\n7460             The matrix is not scaled if `cov='unscaled'`.\n7461 \n7462         Returns\n7463         -------\n7464         polyfit_results : Dataset\n7465             A single dataset which contains (for each \"var\" in the input dataset):\n7466 \n7467             [var]_polyfit_coefficients\n7468                 The coefficients of the best fit for each variable in this dataset.\n7469             [var]_polyfit_residuals\n7470                 The residuals of the least-square computation for each variable (only included if `full=True`)\n7471                 When the matrix rank is deficient, np.nan is returned.\n7472             [dim]_matrix_rank\n7473                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n7474                 The rank is computed ignoring the NaN values that might be skipped.\n7475             [dim]_singular_values\n7476                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n7477             [var]_polyfit_covariance\n7478                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n7479 \n7480         Warns\n7481         -----\n7482         RankWarning\n7483             The rank of the coefficient matrix in the least-squares fit is deficient.\n7484             The warning is not raised with in-memory (not dask) data and `full=True`.\n7485 \n7486         See Also\n7487         --------\n7488         numpy.polyfit\n7489         numpy.polyval\n7490         xarray.polyval\n7491         \"\"\"\n7492         from .dataarray import DataArray\n7493 \n7494         variables = {}\n7495         skipna_da = skipna\n7496 \n7497         x = get_clean_interp_index(self, dim, strict=False)\n7498         xname = f\"{self[dim].name}_\"\n7499         order = int(deg) + 1\n7500         lhs = np.vander(x, order)\n7501 \n7502         if rcond is None:\n7503             rcond = (\n7504                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]\n7505             )\n7506 \n7507         # Weights:\n7508         if w is not None:\n7509             if isinstance(w, Hashable):\n7510                 w = self.coords[w]\n7511             w = np.asarray(w)\n7512             if w.ndim != 1:\n7513                 raise TypeError(\"Expected a 1-d array for weights.\")\n7514             if w.shape[0] != lhs.shape[0]:\n7515                 raise TypeError(f\"Expected w and {dim} to have the same length\")\n7516             lhs *= w[:, np.newaxis]\n7517 \n7518         # Scaling\n7519         scale = np.sqrt((lhs * lhs).sum(axis=0))\n7520         lhs /= scale\n7521 \n7522         degree_dim = utils.get_temp_dimname(self.dims, \"degree\")\n7523 \n7524         rank = np.linalg.matrix_rank(lhs)\n7525 \n7526         if full:\n7527             rank = DataArray(rank, name=xname + \"matrix_rank\")\n7528             variables[rank.name] = rank\n7529             _sing = np.linalg.svd(lhs, compute_uv=False)\n7530             sing = DataArray(\n7531                 _sing,\n7532                 dims=(degree_dim,),\n7533                 coords={degree_dim: np.arange(rank - 1, -1, -1)},\n7534                 name=xname + \"singular_values\",\n7535             )\n7536             variables[sing.name] = sing\n7537 \n7538         for name, da in self.data_vars.items():\n7539             if dim not in da.dims:\n7540                 continue\n7541 \n7542             if is_duck_dask_array(da.data) and (\n7543                 rank != order or full or skipna is None\n7544             ):\n7545                 # Current algorithm with dask and skipna=False neither supports\n7546                 # deficient ranks nor does it output the \"full\" info (issue dask/dask#6516)\n7547                 skipna_da = True\n7548             elif skipna is None:\n7549                 skipna_da = bool(np.any(da.isnull()))\n7550 \n7551             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]\n7552             stacked_coords: dict[Hashable, DataArray] = {}\n7553             if dims_to_stack:\n7554                 stacked_dim = utils.get_temp_dimname(dims_to_stack, \"stacked\")\n7555                 rhs = da.transpose(dim, *dims_to_stack).stack(\n7556                     {stacked_dim: dims_to_stack}\n7557                 )\n7558                 stacked_coords = {stacked_dim: rhs[stacked_dim]}\n7559                 scale_da = scale[:, np.newaxis]\n7560             else:\n7561                 rhs = da\n7562                 scale_da = scale\n7563 \n7564             if w is not None:\n7565                 rhs *= w[:, np.newaxis]\n7566 \n7567             with warnings.catch_warnings():\n7568                 if full:  # Copy np.polyfit behavior\n7569                     warnings.simplefilter(\"ignore\", np.RankWarning)\n7570                 else:  # Raise only once per variable\n7571                     warnings.simplefilter(\"once\", np.RankWarning)\n7572 \n7573                 coeffs, residuals = duck_array_ops.least_squares(\n7574                     lhs, rhs.data, rcond=rcond, skipna=skipna_da\n7575                 )\n7576 \n7577             if isinstance(name, str):\n7578                 name = f\"{name}_\"\n7579             else:\n7580                 # Thus a ReprObject => polyfit was called on a DataArray\n7581                 name = \"\"\n7582 \n7583             coeffs = DataArray(\n7584                 coeffs / scale_da,\n7585                 dims=[degree_dim] + list(stacked_coords.keys()),\n7586                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},\n7587                 name=name + \"polyfit_coefficients\",\n7588             )\n7589             if dims_to_stack:\n7590                 coeffs = coeffs.unstack(stacked_dim)\n7591             variables[coeffs.name] = coeffs\n7592 \n7593             if full or (cov is True):\n7594                 residuals = DataArray(\n7595                     residuals if dims_to_stack else residuals.squeeze(),\n7596                     dims=list(stacked_coords.keys()),\n7597                     coords=stacked_coords,\n7598                     name=name + \"polyfit_residuals\",\n7599                 )\n7600                 if dims_to_stack:\n7601                     residuals = residuals.unstack(stacked_dim)\n7602                 variables[residuals.name] = residuals\n7603 \n7604             if cov:\n7605                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))\n7606                 Vbase /= np.outer(scale, scale)\n7607                 if cov == \"unscaled\":\n7608                     fac = 1\n7609                 else:\n7610                     if x.shape[0] <= order:\n7611                         raise ValueError(\n7612                             \"The number of data points must exceed order to scale the covariance matrix.\"\n7613                         )\n7614                     fac = residuals / (x.shape[0] - order)\n7615                 covariance = DataArray(Vbase, dims=(\"cov_i\", \"cov_j\")) * fac\n7616                 variables[name + \"polyfit_covariance\"] = covariance\n7617 \n7618         return type(self)(data_vars=variables, attrs=self.attrs.copy())\n7619 \n7620     def pad(\n7621         self: T_Dataset,\n7622         pad_width: Mapping[Any, int | tuple[int, int]] = None,\n7623         mode: PadModeOptions = \"constant\",\n7624         stat_length: int\n7625         | tuple[int, int]\n7626         | Mapping[Any, tuple[int, int]]\n7627         | None = None,\n7628         constant_values: (\n7629             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None\n7630         ) = None,\n7631         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n7632         reflect_type: PadReflectOptions = None,\n7633         **pad_width_kwargs: Any,\n7634     ) -> T_Dataset:\n7635         \"\"\"Pad this dataset along one or more dimensions.\n7636 \n7637         .. warning::\n7638             This function is experimental and its behaviour is likely to change\n7639             especially regarding padding of dimension coordinates (or IndexVariables).\n7640 \n7641         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n7642         coordinates will be padded with the same mode, otherwise coordinates\n7643         are padded using the \"constant\" mode with fill_value dtypes.NA.\n7644 \n7645         Parameters\n7646         ----------\n7647         pad_width : mapping of hashable to tuple of int\n7648             Mapping with the form of {dim: (pad_before, pad_after)}\n7649             describing the number of values padded along each dimension.\n7650             {dim: pad} is a shortcut for pad_before = pad_after = pad\n7651         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n7652             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n7653             How to pad the DataArray (taken from numpy docs):\n7654 \n7655             - \"constant\": Pads with a constant value.\n7656             - \"edge\": Pads with the edge values of array.\n7657             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n7658               array edge value.\n7659             - \"maximum\": Pads with the maximum value of all or part of the\n7660               vector along each axis.\n7661             - \"mean\": Pads with the mean value of all or part of the\n7662               vector along each axis.\n7663             - \"median\": Pads with the median value of all or part of the\n7664               vector along each axis.\n7665             - \"minimum\": Pads with the minimum value of all or part of the\n7666               vector along each axis.\n7667             - \"reflect\": Pads with the reflection of the vector mirrored on\n7668               the first and last values of the vector along each axis.\n7669             - \"symmetric\": Pads with the reflection of the vector mirrored\n7670               along the edge of the array.\n7671             - \"wrap\": Pads with the wrap of the vector along the axis.\n7672               The first values are used to pad the end and the\n7673               end values are used to pad the beginning.\n7674 \n7675         stat_length : int, tuple or mapping of hashable to tuple, default: None\n7676             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n7677             values at edge of each axis used to calculate the statistic value.\n7678             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n7679             statistic lengths along each dimension.\n7680             ((before, after),) yields same before and after statistic lengths\n7681             for each dimension.\n7682             (stat_length,) or int is a shortcut for before = after = statistic\n7683             length for all axes.\n7684             Default is ``None``, to use the entire axis.\n7685         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0\n7686             Used in 'constant'.  The values to set the padded values for each\n7687             axis.\n7688             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n7689             pad constants along each dimension.\n7690             ``((before, after),)`` yields same before and after constants for each\n7691             dimension.\n7692             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n7693             all dimensions.\n7694             Default is 0.\n7695         end_values : scalar, tuple or mapping of hashable to tuple, default: 0\n7696             Used in 'linear_ramp'.  The values used for the ending value of the\n7697             linear_ramp and that will form the edge of the padded array.\n7698             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n7699             end values along each dimension.\n7700             ``((before, after),)`` yields same before and after end values for each\n7701             axis.\n7702             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n7703             all axes.\n7704             Default is 0.\n7705         reflect_type : {\"even\", \"odd\", None}, optional\n7706             Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n7707             default with an unaltered reflection around the edge value.  For\n7708             the \"odd\" style, the extended part of the array is created by\n7709             subtracting the reflected values from two times the edge value.\n7710         **pad_width_kwargs\n7711             The keyword arguments form of ``pad_width``.\n7712             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n7713 \n7714         Returns\n7715         -------\n7716         padded : Dataset\n7717             Dataset with the padded coordinates and data.\n7718 \n7719         See Also\n7720         --------\n7721         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad\n7722 \n7723         Notes\n7724         -----\n7725         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n7726         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n7727         specify ``constant_values=np.nan``\n7728 \n7729         Padding coordinates will drop their corresponding index (if any) and will reset default\n7730         indexes for dimension coordinates.\n7731 \n7732         Examples\n7733         --------\n7734         >>> ds = xr.Dataset({\"foo\": (\"x\", range(5))})\n7735         >>> ds.pad(x=(1, 2))\n7736         <xarray.Dataset>\n7737         Dimensions:  (x: 8)\n7738         Dimensions without coordinates: x\n7739         Data variables:\n7740             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan\n7741         \"\"\"\n7742         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n7743 \n7744         if mode in (\"edge\", \"reflect\", \"symmetric\", \"wrap\"):\n7745             coord_pad_mode = mode\n7746             coord_pad_options = {\n7747                 \"stat_length\": stat_length,\n7748                 \"constant_values\": constant_values,\n7749                 \"end_values\": end_values,\n7750                 \"reflect_type\": reflect_type,\n7751             }\n7752         else:\n7753             coord_pad_mode = \"constant\"\n7754             coord_pad_options = {}\n7755 \n7756         variables = {}\n7757 \n7758         # keep indexes that won't be affected by pad and drop all other indexes\n7759         xindexes = self.xindexes\n7760         pad_dims = set(pad_width)\n7761         indexes = {}\n7762         for k, idx in xindexes.items():\n7763             if not pad_dims.intersection(xindexes.get_all_dims(k)):\n7764                 indexes[k] = idx\n7765 \n7766         for name, var in self.variables.items():\n7767             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}\n7768             if not var_pad_width:\n7769                 variables[name] = var\n7770             elif name in self.data_vars:\n7771                 variables[name] = var.pad(\n7772                     pad_width=var_pad_width,\n7773                     mode=mode,\n7774                     stat_length=stat_length,\n7775                     constant_values=constant_values,\n7776                     end_values=end_values,\n7777                     reflect_type=reflect_type,\n7778                 )\n7779             else:\n7780                 variables[name] = var.pad(\n7781                     pad_width=var_pad_width,\n7782                     mode=coord_pad_mode,\n7783                     **coord_pad_options,  # type: ignore[arg-type]\n7784                 )\n7785                 # reset default index of dimension coordinates\n7786                 if (name,) == var.dims:\n7787                     dim_var = {name: variables[name]}\n7788                     index = PandasIndex.from_variables(dim_var)\n7789                     index_vars = index.create_variables(dim_var)\n7790                     indexes[name] = index\n7791                     variables[name] = index_vars[name]\n7792 \n7793         return self._replace_with_new_dims(variables, indexes=indexes)\n7794 \n7795     def idxmin(\n7796         self: T_Dataset,\n7797         dim: Hashable | None = None,\n7798         skipna: bool | None = None,\n7799         fill_value: Any = xrdtypes.NA,\n7800         keep_attrs: bool | None = None,\n7801     ) -> T_Dataset:\n7802         \"\"\"Return the coordinate label of the minimum value along a dimension.\n7803 \n7804         Returns a new `Dataset` named after the dimension with the values of\n7805         the coordinate labels along that dimension corresponding to minimum\n7806         values along that dimension.\n7807 \n7808         In comparison to :py:meth:`~Dataset.argmin`, this returns the\n7809         coordinate label while :py:meth:`~Dataset.argmin` returns the index.\n7810 \n7811         Parameters\n7812         ----------\n7813         dim : Hashable, optional\n7814             Dimension over which to apply `idxmin`.  This is optional for 1D\n7815             variables, but required for variables with 2 or more dimensions.\n7816         skipna : bool or None, optional\n7817             If True, skip missing values (as marked by NaN). By default, only\n7818             skips missing values for ``float``, ``complex``, and ``object``\n7819             dtypes; other dtypes either do not have a sentinel missing value\n7820             (``int``) or ``skipna=True`` has not been implemented\n7821             (``datetime64`` or ``timedelta64``).\n7822         fill_value : Any, default: NaN\n7823             Value to be filled in case all of the values along a dimension are\n7824             null.  By default this is NaN.  The fill value and result are\n7825             automatically converted to a compatible dtype if possible.\n7826             Ignored if ``skipna`` is False.\n7827         keep_attrs : bool or None, optional\n7828             If True, the attributes (``attrs``) will be copied from the\n7829             original object to the new one. If False, the new object\n7830             will be returned without attributes.\n7831 \n7832         Returns\n7833         -------\n7834         reduced : Dataset\n7835             New `Dataset` object with `idxmin` applied to its data and the\n7836             indicated dimension removed.\n7837 \n7838         See Also\n7839         --------\n7840         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin\n7841 \n7842         Examples\n7843         --------\n7844         >>> array1 = xr.DataArray(\n7845         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n7846         ... )\n7847         >>> array2 = xr.DataArray(\n7848         ...     [\n7849         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n7850         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n7851         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n7852         ...     ],\n7853         ...     dims=[\"y\", \"x\"],\n7854         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n7855         ... )\n7856         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n7857         >>> ds.min(dim=\"x\")\n7858         <xarray.Dataset>\n7859         Dimensions:  (y: 3)\n7860         Coordinates:\n7861           * y        (y) int64 -1 0 1\n7862         Data variables:\n7863             int      int64 -2\n7864             float    (y) float64 -2.0 -4.0 1.0\n7865         >>> ds.argmin(dim=\"x\")\n7866         <xarray.Dataset>\n7867         Dimensions:  (y: 3)\n7868         Coordinates:\n7869           * y        (y) int64 -1 0 1\n7870         Data variables:\n7871             int      int64 4\n7872             float    (y) int64 4 0 2\n7873         >>> ds.idxmin(dim=\"x\")\n7874         <xarray.Dataset>\n7875         Dimensions:  (y: 3)\n7876         Coordinates:\n7877           * y        (y) int64 -1 0 1\n7878         Data variables:\n7879             int      <U1 'e'\n7880             float    (y) object 'e' 'a' 'c'\n7881         \"\"\"\n7882         return self.map(\n7883             methodcaller(\n7884                 \"idxmin\",\n7885                 dim=dim,\n7886                 skipna=skipna,\n7887                 fill_value=fill_value,\n7888                 keep_attrs=keep_attrs,\n7889             )\n7890         )\n7891 \n7892     def idxmax(\n7893         self: T_Dataset,\n7894         dim: Hashable | None = None,\n7895         skipna: bool | None = None,\n7896         fill_value: Any = xrdtypes.NA,\n7897         keep_attrs: bool | None = None,\n7898     ) -> T_Dataset:\n7899         \"\"\"Return the coordinate label of the maximum value along a dimension.\n7900 \n7901         Returns a new `Dataset` named after the dimension with the values of\n7902         the coordinate labels along that dimension corresponding to maximum\n7903         values along that dimension.\n7904 \n7905         In comparison to :py:meth:`~Dataset.argmax`, this returns the\n7906         coordinate label while :py:meth:`~Dataset.argmax` returns the index.\n7907 \n7908         Parameters\n7909         ----------\n7910         dim : str, optional\n7911             Dimension over which to apply `idxmax`.  This is optional for 1D\n7912             variables, but required for variables with 2 or more dimensions.\n7913         skipna : bool or None, optional\n7914             If True, skip missing values (as marked by NaN). By default, only\n7915             skips missing values for ``float``, ``complex``, and ``object``\n7916             dtypes; other dtypes either do not have a sentinel missing value\n7917             (``int``) or ``skipna=True`` has not been implemented\n7918             (``datetime64`` or ``timedelta64``).\n7919         fill_value : Any, default: NaN\n7920             Value to be filled in case all of the values along a dimension are\n7921             null.  By default this is NaN.  The fill value and result are\n7922             automatically converted to a compatible dtype if possible.\n7923             Ignored if ``skipna`` is False.\n7924         keep_attrs : bool or None, optional\n7925             If True, the attributes (``attrs``) will be copied from the\n7926             original object to the new one. If False, the new object\n7927             will be returned without attributes.\n7928 \n7929         Returns\n7930         -------\n7931         reduced : Dataset\n7932             New `Dataset` object with `idxmax` applied to its data and the\n7933             indicated dimension removed.\n7934 \n7935         See Also\n7936         --------\n7937         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax\n7938 \n7939         Examples\n7940         --------\n7941         >>> array1 = xr.DataArray(\n7942         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n7943         ... )\n7944         >>> array2 = xr.DataArray(\n7945         ...     [\n7946         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n7947         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n7948         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n7949         ...     ],\n7950         ...     dims=[\"y\", \"x\"],\n7951         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n7952         ... )\n7953         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n7954         >>> ds.max(dim=\"x\")\n7955         <xarray.Dataset>\n7956         Dimensions:  (y: 3)\n7957         Coordinates:\n7958           * y        (y) int64 -1 0 1\n7959         Data variables:\n7960             int      int64 2\n7961             float    (y) float64 2.0 2.0 1.0\n7962         >>> ds.argmax(dim=\"x\")\n7963         <xarray.Dataset>\n7964         Dimensions:  (y: 3)\n7965         Coordinates:\n7966           * y        (y) int64 -1 0 1\n7967         Data variables:\n7968             int      int64 1\n7969             float    (y) int64 0 2 2\n7970         >>> ds.idxmax(dim=\"x\")\n7971         <xarray.Dataset>\n7972         Dimensions:  (y: 3)\n7973         Coordinates:\n7974           * y        (y) int64 -1 0 1\n7975         Data variables:\n7976             int      <U1 'b'\n7977             float    (y) object 'a' 'c' 'c'\n7978         \"\"\"\n7979         return self.map(\n7980             methodcaller(\n7981                 \"idxmax\",\n7982                 dim=dim,\n7983                 skipna=skipna,\n7984                 fill_value=fill_value,\n7985                 keep_attrs=keep_attrs,\n7986             )\n7987         )\n7988 \n7989     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:\n7990         \"\"\"Indices of the minima of the member variables.\n7991 \n7992         If there are multiple minima, the indices of the first one found will be\n7993         returned.\n7994 \n7995         Parameters\n7996         ----------\n7997         dim : Hashable, optional\n7998             The dimension over which to find the minimum. By default, finds minimum over\n7999             all dimensions - for now returning an int for backward compatibility, but\n8000             this is deprecated, in future will be an error, since DataArray.argmin will\n8001             return a dict with indices for all dimensions, which does not make sense for\n8002             a Dataset.\n8003         keep_attrs : bool, optional\n8004             If True, the attributes (`attrs`) will be copied from the original\n8005             object to the new one.  If False (default), the new object will be\n8006             returned without attributes.\n8007         skipna : bool, optional\n8008             If True, skip missing values (as marked by NaN). By default, only\n8009             skips missing values for float dtypes; other dtypes either do not\n8010             have a sentinel missing value (int) or skipna=True has not been\n8011             implemented (object, datetime64 or timedelta64).\n8012 \n8013         Returns\n8014         -------\n8015         result : Dataset\n8016 \n8017         See Also\n8018         --------\n8019         DataArray.argmin\n8020         \"\"\"\n8021         if dim is None:\n8022             warnings.warn(\n8023                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() without \"\n8024                 \"dim changes to return a dict of indices of each dimension, for \"\n8025                 \"consistency it will be an error to call Dataset.argmin() with no argument,\"\n8026                 \"since we don't return a dict of Datasets.\",\n8027                 DeprecationWarning,\n8028                 stacklevel=2,\n8029             )\n8030         if (\n8031             dim is None\n8032             or (not isinstance(dim, Sequence) and dim is not ...)\n8033             or isinstance(dim, str)\n8034         ):\n8035             # Return int index if single dimension is passed, and is not part of a\n8036             # sequence\n8037             argmin_func = getattr(duck_array_ops, \"argmin\")\n8038             return self.reduce(argmin_func, dim=dim, **kwargs)\n8039         else:\n8040             raise ValueError(\n8041                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n8042                 \"dicts cannot be contained in a Dataset, so cannot call \"\n8043                 \"Dataset.argmin() with a sequence or ... for dim\"\n8044             )\n8045 \n8046     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:\n8047         \"\"\"Indices of the maxima of the member variables.\n8048 \n8049         If there are multiple maxima, the indices of the first one found will be\n8050         returned.\n8051 \n8052         Parameters\n8053         ----------\n8054         dim : str, optional\n8055             The dimension over which to find the maximum. By default, finds maximum over\n8056             all dimensions - for now returning an int for backward compatibility, but\n8057             this is deprecated, in future will be an error, since DataArray.argmax will\n8058             return a dict with indices for all dimensions, which does not make sense for\n8059             a Dataset.\n8060         keep_attrs : bool, optional\n8061             If True, the attributes (`attrs`) will be copied from the original\n8062             object to the new one.  If False (default), the new object will be\n8063             returned without attributes.\n8064         skipna : bool, optional\n8065             If True, skip missing values (as marked by NaN). By default, only\n8066             skips missing values for float dtypes; other dtypes either do not\n8067             have a sentinel missing value (int) or skipna=True has not been\n8068             implemented (object, datetime64 or timedelta64).\n8069 \n8070         Returns\n8071         -------\n8072         result : Dataset\n8073 \n8074         See Also\n8075         --------\n8076         DataArray.argmax\n8077 \n8078         \"\"\"\n8079         if dim is None:\n8080             warnings.warn(\n8081                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() without \"\n8082                 \"dim changes to return a dict of indices of each dimension, for \"\n8083                 \"consistency it will be an error to call Dataset.argmin() with no argument,\"\n8084                 \"since we don't return a dict of Datasets.\",\n8085                 DeprecationWarning,\n8086                 stacklevel=2,\n8087             )\n8088         if (\n8089             dim is None\n8090             or (not isinstance(dim, Sequence) and dim is not ...)\n8091             or isinstance(dim, str)\n8092         ):\n8093             # Return int index if single dimension is passed, and is not part of a\n8094             # sequence\n8095             argmax_func = getattr(duck_array_ops, \"argmax\")\n8096             return self.reduce(argmax_func, dim=dim, **kwargs)\n8097         else:\n8098             raise ValueError(\n8099                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n8100                 \"dicts cannot be contained in a Dataset, so cannot call \"\n8101                 \"Dataset.argmin() with a sequence or ... for dim\"\n8102             )\n8103 \n8104     def query(\n8105         self: T_Dataset,\n8106         queries: Mapping[Any, Any] | None = None,\n8107         parser: QueryParserOptions = \"pandas\",\n8108         engine: QueryEngineOptions = None,\n8109         missing_dims: ErrorOptionsWithWarn = \"raise\",\n8110         **queries_kwargs: Any,\n8111     ) -> T_Dataset:\n8112         \"\"\"Return a new dataset with each array indexed along the specified\n8113         dimension(s), where the indexers are given as strings containing\n8114         Python expressions to be evaluated against the data variables in the\n8115         dataset.\n8116 \n8117         Parameters\n8118         ----------\n8119         queries : dict-like, optional\n8120             A dict-like with keys matching dimensions and values given by strings\n8121             containing Python expressions to be evaluated against the data variables\n8122             in the dataset. The expressions will be evaluated using the pandas\n8123             eval() function, and can contain any valid Python expressions but cannot\n8124             contain any Python statements.\n8125         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n8126             The parser to use to construct the syntax tree from the expression.\n8127             The default of 'pandas' parses code slightly different than standard\n8128             Python. Alternatively, you can parse an expression using the 'python'\n8129             parser to retain strict Python semantics.\n8130         engine : {\"python\", \"numexpr\", None}, default: None\n8131             The engine used to evaluate the expression. Supported engines are:\n8132 \n8133             - None: tries to use numexpr, falls back to python\n8134             - \"numexpr\": evaluates expressions using numexpr\n8135             - \"python\": performs operations as if you had eval\u2019d in top level python\n8136 \n8137         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n8138             What to do if dimensions that should be selected from are not present in the\n8139             Dataset:\n8140 \n8141             - \"raise\": raise an exception\n8142             - \"warn\": raise a warning, and ignore the missing dimensions\n8143             - \"ignore\": ignore the missing dimensions\n8144 \n8145         **queries_kwargs : {dim: query, ...}, optional\n8146             The keyword arguments form of ``queries``.\n8147             One of queries or queries_kwargs must be provided.\n8148 \n8149         Returns\n8150         -------\n8151         obj : Dataset\n8152             A new Dataset with the same contents as this dataset, except each\n8153             array and dimension is indexed by the results of the appropriate\n8154             queries.\n8155 \n8156         See Also\n8157         --------\n8158         Dataset.isel\n8159         pandas.eval\n8160 \n8161         Examples\n8162         --------\n8163         >>> a = np.arange(0, 5, 1)\n8164         >>> b = np.linspace(0, 1, 5)\n8165         >>> ds = xr.Dataset({\"a\": (\"x\", a), \"b\": (\"x\", b)})\n8166         >>> ds\n8167         <xarray.Dataset>\n8168         Dimensions:  (x: 5)\n8169         Dimensions without coordinates: x\n8170         Data variables:\n8171             a        (x) int64 0 1 2 3 4\n8172             b        (x) float64 0.0 0.25 0.5 0.75 1.0\n8173         >>> ds.query(x=\"a > 2\")\n8174         <xarray.Dataset>\n8175         Dimensions:  (x: 2)\n8176         Dimensions without coordinates: x\n8177         Data variables:\n8178             a        (x) int64 3 4\n8179             b        (x) float64 0.75 1.0\n8180         \"\"\"\n8181 \n8182         # allow queries to be given either as a dict or as kwargs\n8183         queries = either_dict_or_kwargs(queries, queries_kwargs, \"query\")\n8184 \n8185         # check queries\n8186         for dim, expr in queries.items():\n8187             if not isinstance(expr, str):\n8188                 msg = f\"expr for dim {dim} must be a string to be evaluated, {type(expr)} given\"\n8189                 raise ValueError(msg)\n8190 \n8191         # evaluate the queries to create the indexers\n8192         indexers = {\n8193             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)\n8194             for dim, expr in queries.items()\n8195         }\n8196 \n8197         # apply the selection\n8198         return self.isel(indexers, missing_dims=missing_dims)\n8199 \n8200     def curvefit(\n8201         self: T_Dataset,\n8202         coords: str | DataArray | Iterable[str | DataArray],\n8203         func: Callable[..., Any],\n8204         reduce_dims: Hashable | Iterable[Hashable] | None = None,\n8205         skipna: bool = True,\n8206         p0: dict[str, Any] | None = None,\n8207         bounds: dict[str, Any] | None = None,\n8208         param_names: Sequence[str] | None = None,\n8209         kwargs: dict[str, Any] | None = None,\n8210     ) -> T_Dataset:\n8211         \"\"\"\n8212         Curve fitting optimization for arbitrary functions.\n8213 \n8214         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n8215 \n8216         Parameters\n8217         ----------\n8218         coords : hashable, DataArray, or sequence of hashable or DataArray\n8219             Independent coordinate(s) over which to perform the curve fitting. Must share\n8220             at least one dimension with the calling object. When fitting multi-dimensional\n8221             functions, supply `coords` as a sequence in the same order as arguments in\n8222             `func`. To fit along existing dimensions of the calling object, `coords` can\n8223             also be specified as a str or sequence of strs.\n8224         func : callable\n8225             User specified function in the form `f(x, *params)` which returns a numpy\n8226             array of length `len(x)`. `params` are the fittable parameters which are optimized\n8227             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n8228             coordinates, e.g. `f((x0, x1), *params)`.\n8229         reduce_dims : hashable or sequence of hashable\n8230             Additional dimension(s) over which to aggregate while fitting. For example,\n8231             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n8232             aggregate all lat and lon points and fit the specified function along the\n8233             time dimension.\n8234         skipna : bool, default: True\n8235             Whether to skip missing values when fitting. Default is True.\n8236         p0 : dict-like, optional\n8237             Optional dictionary of parameter names to initial guesses passed to the\n8238             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n8239             be assigned initial values following the default scipy behavior.\n8240         bounds : dict-like, optional\n8241             Optional dictionary of parameter names to bounding values passed to the\n8242             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n8243             will be unbounded following the default scipy behavior.\n8244         param_names : sequence of hashable, optional\n8245             Sequence of names for the fittable parameters of `func`. If not supplied,\n8246             this will be automatically determined by arguments of `func`. `param_names`\n8247             should be manually supplied when fitting a function that takes a variable\n8248             number of parameters.\n8249         **kwargs : optional\n8250             Additional keyword arguments to passed to scipy curve_fit.\n8251 \n8252         Returns\n8253         -------\n8254         curvefit_results : Dataset\n8255             A single dataset which contains:\n8256 \n8257             [var]_curvefit_coefficients\n8258                 The coefficients of the best fit.\n8259             [var]_curvefit_covariance\n8260                 The covariance matrix of the coefficient estimates.\n8261 \n8262         See Also\n8263         --------\n8264         Dataset.polyfit\n8265         scipy.optimize.curve_fit\n8266         \"\"\"\n8267         from scipy.optimize import curve_fit\n8268 \n8269         from .alignment import broadcast\n8270         from .computation import apply_ufunc\n8271         from .dataarray import _THIS_ARRAY, DataArray\n8272 \n8273         if p0 is None:\n8274             p0 = {}\n8275         if bounds is None:\n8276             bounds = {}\n8277         if kwargs is None:\n8278             kwargs = {}\n8279 \n8280         if not reduce_dims:\n8281             reduce_dims_ = []\n8282         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):\n8283             reduce_dims_ = [reduce_dims]\n8284         else:\n8285             reduce_dims_ = list(reduce_dims)\n8286 \n8287         if (\n8288             isinstance(coords, str)\n8289             or isinstance(coords, DataArray)\n8290             or not isinstance(coords, Iterable)\n8291         ):\n8292             coords = [coords]\n8293         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]\n8294 \n8295         # Determine whether any coords are dims on self\n8296         for coord in coords_:\n8297             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]\n8298         reduce_dims_ = list(set(reduce_dims_))\n8299         preserved_dims = list(set(self.dims) - set(reduce_dims_))\n8300         if not reduce_dims_:\n8301             raise ValueError(\n8302                 \"No arguments to `coords` were identified as a dimension on the calling \"\n8303                 \"object, and no dims were supplied to `reduce_dims`. This would result \"\n8304                 \"in fitting on scalar data.\"\n8305             )\n8306 \n8307         # Broadcast all coords with each other\n8308         coords_ = broadcast(*coords_)\n8309         coords_ = [\n8310             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_\n8311         ]\n8312 \n8313         params, func_args = _get_func_args(func, param_names)\n8314         param_defaults, bounds_defaults = _initialize_curvefit_params(\n8315             params, p0, bounds, func_args\n8316         )\n8317         n_params = len(params)\n8318         kwargs.setdefault(\"p0\", [param_defaults[p] for p in params])\n8319         kwargs.setdefault(\n8320             \"bounds\",\n8321             [\n8322                 [bounds_defaults[p][0] for p in params],\n8323                 [bounds_defaults[p][1] for p in params],\n8324             ],\n8325         )\n8326 \n8327         def _wrapper(Y, *coords_, **kwargs):\n8328             # Wrap curve_fit with raveled coordinates and pointwise NaN handling\n8329             x = np.vstack([c.ravel() for c in coords_])\n8330             y = Y.ravel()\n8331             if skipna:\n8332                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)\n8333                 x = x[:, mask]\n8334                 y = y[mask]\n8335                 if not len(y):\n8336                     popt = np.full([n_params], np.nan)\n8337                     pcov = np.full([n_params, n_params], np.nan)\n8338                     return popt, pcov\n8339             x = np.squeeze(x)\n8340             popt, pcov = curve_fit(func, x, y, **kwargs)\n8341             return popt, pcov\n8342 \n8343         result = type(self)()\n8344         for name, da in self.data_vars.items():\n8345             if name is _THIS_ARRAY:\n8346                 name = \"\"\n8347             else:\n8348                 name = f\"{str(name)}_\"\n8349 \n8350             popt, pcov = apply_ufunc(\n8351                 _wrapper,\n8352                 da,\n8353                 *coords_,\n8354                 vectorize=True,\n8355                 dask=\"parallelized\",\n8356                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],\n8357                 output_core_dims=[[\"param\"], [\"cov_i\", \"cov_j\"]],\n8358                 dask_gufunc_kwargs={\n8359                     \"output_sizes\": {\n8360                         \"param\": n_params,\n8361                         \"cov_i\": n_params,\n8362                         \"cov_j\": n_params,\n8363                     },\n8364                 },\n8365                 output_dtypes=(np.float64, np.float64),\n8366                 exclude_dims=set(reduce_dims_),\n8367                 kwargs=kwargs,\n8368             )\n8369             result[name + \"curvefit_coefficients\"] = popt\n8370             result[name + \"curvefit_covariance\"] = pcov\n8371 \n8372         result = result.assign_coords(\n8373             {\"param\": params, \"cov_i\": params, \"cov_j\": params}\n8374         )\n8375         result.attrs = self.attrs.copy()\n8376 \n8377         return result\n8378 \n8379     def drop_duplicates(\n8380         self: T_Dataset,\n8381         dim: Hashable | Iterable[Hashable],\n8382         keep: Literal[\"first\", \"last\", False] = \"first\",\n8383     ) -> T_Dataset:\n8384         \"\"\"Returns a new Dataset with duplicate dimension values removed.\n8385 \n8386         Parameters\n8387         ----------\n8388         dim : dimension label or labels\n8389             Pass `...` to drop duplicates along all dimensions.\n8390         keep : {\"first\", \"last\", False}, default: \"first\"\n8391             Determines which duplicates (if any) to keep.\n8392             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n8393             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n8394             - False : Drop all duplicates.\n8395 \n8396         Returns\n8397         -------\n8398         Dataset\n8399 \n8400         See Also\n8401         --------\n8402         DataArray.drop_duplicates\n8403         \"\"\"\n8404         if isinstance(dim, str):\n8405             dims: Iterable = (dim,)\n8406         elif dim is ...:\n8407             dims = self.dims\n8408         elif not isinstance(dim, Iterable):\n8409             dims = [dim]\n8410         else:\n8411             dims = dim\n8412 \n8413         missing_dims = set(dims) - set(self.dims)\n8414         if missing_dims:\n8415             raise ValueError(f\"'{missing_dims}' not found in dimensions\")\n8416 \n8417         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}\n8418         return self.isel(indexes)\n8419 \n8420     def convert_calendar(\n8421         self: T_Dataset,\n8422         calendar: CFCalendar,\n8423         dim: Hashable = \"time\",\n8424         align_on: Literal[\"date\", \"year\", None] = None,\n8425         missing: Any | None = None,\n8426         use_cftime: bool | None = None,\n8427     ) -> T_Dataset:\n8428         \"\"\"Convert the Dataset to another calendar.\n8429 \n8430         Only converts the individual timestamps, does not modify any data except\n8431         in dropping invalid/surplus dates or inserting missing dates.\n8432 \n8433         If the source and target calendars are either no_leap, all_leap or a\n8434         standard type, only the type of the time array is modified.\n8435         When converting to a leap year from a non-leap year, the 29th of February\n8436         is removed from the array. In the other direction the 29th of February\n8437         will be missing in the output, unless `missing` is specified,\n8438         in which case that value is inserted.\n8439 \n8440         For conversions involving `360_day` calendars, see Notes.\n8441 \n8442         This method is safe to use with sub-daily data as it doesn't touch the\n8443         time part of the timestamps.\n8444 \n8445         Parameters\n8446         ---------\n8447         calendar : str\n8448             The target calendar name.\n8449         dim : Hashable, default: \"time\"\n8450             Name of the time coordinate.\n8451         align_on : {None, 'date', 'year'}, optional\n8452             Must be specified when either source or target is a `360_day` calendar,\n8453             ignored otherwise. See Notes.\n8454         missing : Any or None, optional\n8455             By default, i.e. if the value is None, this method will simply attempt\n8456             to convert the dates in the source calendar to the same dates in the\n8457             target calendar, and drop any of those that are not possible to\n8458             represent.  If a value is provided, a new time coordinate will be\n8459             created in the target calendar with the same frequency as the original\n8460             time coordinate; for any dates that are not present in the source, the\n8461             data will be filled with this value.  Note that using this mode requires\n8462             that the source data have an inferable frequency; for more information\n8463             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n8464             target calendar combinations, this could result in many missing values, see notes.\n8465         use_cftime : bool or None, optional\n8466             Whether to use cftime objects in the output, only used if `calendar`\n8467             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n8468             If True, the new time axis uses cftime objects.\n8469             If None (default), it uses :py:class:`numpy.datetime64` values if the\n8470             date range permits it, and :py:class:`cftime.datetime` objects if not.\n8471             If False, it uses :py:class:`numpy.datetime64`  or fails.\n8472 \n8473         Returns\n8474         -------\n8475         Dataset\n8476             Copy of the dataarray with the time coordinate converted to the\n8477             target calendar. If 'missing' was None (default), invalid dates in\n8478             the new calendar are dropped, but missing dates are not inserted.\n8479             If `missing` was given, the new data is reindexed to have a time axis\n8480             with the same frequency as the source, but in the new calendar; any\n8481             missing datapoints are filled with `missing`.\n8482 \n8483         Notes\n8484         -----\n8485         Passing a value to `missing` is only usable if the source's time coordinate as an\n8486         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n8487         if the target coordinate, generated from this frequency, has dates equivalent to the\n8488         source. It is usually **not** appropriate to use this mode with:\n8489 \n8490         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n8491         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n8492             or 'mH' where 24 % m != 0).\n8493 \n8494         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n8495         be specified and two options are offered.\n8496 \n8497         - \"year\"\n8498             The dates are translated according to their relative position in the year,\n8499             ignoring their original month and day information, meaning that the\n8500             missing/surplus days are added/removed at regular intervals.\n8501 \n8502             From a `360_day` to a standard calendar, the output will be missing the\n8503             following dates (day of year in parentheses):\n8504 \n8505             To a leap year:\n8506                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n8507                 September 31st (275) and November 30th (335).\n8508             To a non-leap year:\n8509                 February 6th (36), April 19th (109), July 2nd (183),\n8510                 September 12th (255), November 25th (329).\n8511 \n8512             From a standard calendar to a `\"360_day\"`, the following dates in the\n8513             source array will be dropped:\n8514 \n8515             From a leap year:\n8516                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n8517                 September 31st (275), December 1st (336)\n8518             From a non-leap year:\n8519                 February 6th (37), April 20th (110), July 2nd (183),\n8520                 September 13th (256), November 25th (329)\n8521 \n8522             This option is best used on daily and subdaily data.\n8523 \n8524         - \"date\"\n8525             The month/day information is conserved and invalid dates are dropped\n8526             from the output. This means that when converting from a `\"360_day\"` to a\n8527             standard calendar, all 31st (Jan, March, May, July, August, October and\n8528             December) will be missing as there is no equivalent dates in the\n8529             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n8530             will be dropped as there are no equivalent dates in a standard calendar.\n8531 \n8532             This option is best used with data on a frequency coarser than daily.\n8533         \"\"\"\n8534         return convert_calendar(\n8535             self,\n8536             calendar,\n8537             dim=dim,\n8538             align_on=align_on,\n8539             missing=missing,\n8540             use_cftime=use_cftime,\n8541         )\n8542 \n8543     def interp_calendar(\n8544         self: T_Dataset,\n8545         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n8546         dim: Hashable = \"time\",\n8547     ) -> T_Dataset:\n8548         \"\"\"Interpolates the Dataset to another calendar based on decimal year measure.\n8549 \n8550         Each timestamp in `source` and `target` are first converted to their decimal\n8551         year equivalent then `source` is interpolated on the target coordinate.\n8552         The decimal year of a timestamp is its year plus its sub-year component\n8553         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n8554         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n8555 \n8556         This method should only be used when the time (HH:MM:SS) information of\n8557         time coordinate is not important.\n8558 \n8559         Parameters\n8560         ----------\n8561         target: DataArray or DatetimeIndex or CFTimeIndex\n8562             The target time coordinate of a valid dtype\n8563             (np.datetime64 or cftime objects)\n8564         dim : Hashable, default: \"time\"\n8565             The time coordinate name.\n8566 \n8567         Return\n8568         ------\n8569         DataArray\n8570             The source interpolated on the decimal years of target,\n8571         \"\"\"\n8572         return interp_calendar(self, target, dim=dim)\n8573 \n8574     def groupby(\n8575         self,\n8576         group: Hashable | DataArray | IndexVariable,\n8577         squeeze: bool = True,\n8578         restore_coord_dims: bool = False,\n8579     ) -> DatasetGroupBy:\n8580         \"\"\"Returns a DatasetGroupBy object for performing grouped operations.\n8581 \n8582         Parameters\n8583         ----------\n8584         group : Hashable, DataArray or IndexVariable\n8585             Array whose unique values should be used to group this array. If a\n8586             string, must be the name of a variable contained in this dataset.\n8587         squeeze : bool, default: True\n8588             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n8589             controls whether the subarrays have a dimension of length 1 along\n8590             that dimension or if the dimension is squeezed out.\n8591         restore_coord_dims : bool, default: False\n8592             If True, also restore the dimension order of multi-dimensional\n8593             coordinates.\n8594 \n8595         Returns\n8596         -------\n8597         grouped : DatasetGroupBy\n8598             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n8599             iterated over in the form of `(unique_value, grouped_array)` pairs.\n8600 \n8601         See Also\n8602         --------\n8603         Dataset.groupby_bins\n8604         DataArray.groupby\n8605         core.groupby.DatasetGroupBy\n8606         pandas.DataFrame.groupby\n8607         \"\"\"\n8608         from .groupby import DatasetGroupBy\n8609 \n8610         # While we don't generally check the type of every arg, passing\n8611         # multiple dimensions as multiple arguments is common enough, and the\n8612         # consequences hidden enough (strings evaluate as true) to warrant\n8613         # checking here.\n8614         # A future version could make squeeze kwarg only, but would face\n8615         # backward-compat issues.\n8616         if not isinstance(squeeze, bool):\n8617             raise TypeError(\n8618                 f\"`squeeze` must be True or False, but {squeeze} was supplied\"\n8619             )\n8620 \n8621         return DatasetGroupBy(\n8622             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims\n8623         )\n8624 \n8625     def groupby_bins(\n8626         self,\n8627         group: Hashable | DataArray | IndexVariable,\n8628         bins: ArrayLike,\n8629         right: bool = True,\n8630         labels: ArrayLike | None = None,\n8631         precision: int = 3,\n8632         include_lowest: bool = False,\n8633         squeeze: bool = True,\n8634         restore_coord_dims: bool = False,\n8635     ) -> DatasetGroupBy:\n8636         \"\"\"Returns a DatasetGroupBy object for performing grouped operations.\n8637 \n8638         Rather than using all unique values of `group`, the values are discretized\n8639         first by applying `pandas.cut` [1]_ to `group`.\n8640 \n8641         Parameters\n8642         ----------\n8643         group : Hashable, DataArray or IndexVariable\n8644             Array whose binned values should be used to group this array. If a\n8645             string, must be the name of a variable contained in this dataset.\n8646         bins : int or array-like\n8647             If bins is an int, it defines the number of equal-width bins in the\n8648             range of x. However, in this case, the range of x is extended by .1%\n8649             on each side to include the min or max values of x. If bins is a\n8650             sequence it defines the bin edges allowing for non-uniform bin\n8651             width. No extension of the range of x is done in this case.\n8652         right : bool, default: True\n8653             Indicates whether the bins include the rightmost edge or not. If\n8654             right == True (the default), then the bins [1,2,3,4] indicate\n8655             (1,2], (2,3], (3,4].\n8656         labels : array-like or bool, default: None\n8657             Used as labels for the resulting bins. Must be of the same length as\n8658             the resulting bins. If False, string bin labels are assigned by\n8659             `pandas.cut`.\n8660         precision : int, default: 3\n8661             The precision at which to store and display the bins labels.\n8662         include_lowest : bool, default: False\n8663             Whether the first interval should be left-inclusive or not.\n8664         squeeze : bool, default: True\n8665             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n8666             controls whether the subarrays have a dimension of length 1 along\n8667             that dimension or if the dimension is squeezed out.\n8668         restore_coord_dims : bool, default: False\n8669             If True, also restore the dimension order of multi-dimensional\n8670             coordinates.\n8671 \n8672         Returns\n8673         -------\n8674         grouped : DatasetGroupBy\n8675             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n8676             iterated over in the form of `(unique_value, grouped_array)` pairs.\n8677             The name of the group has the added suffix `_bins` in order to\n8678             distinguish it from the original variable.\n8679 \n8680         See Also\n8681         --------\n8682         Dataset.groupby\n8683         DataArray.groupby_bins\n8684         core.groupby.DatasetGroupBy\n8685         pandas.DataFrame.groupby\n8686 \n8687         References\n8688         ----------\n8689         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n8690         \"\"\"\n8691         from .groupby import DatasetGroupBy\n8692 \n8693         return DatasetGroupBy(\n8694             self,\n8695             group,\n8696             squeeze=squeeze,\n8697             bins=bins,\n8698             restore_coord_dims=restore_coord_dims,\n8699             cut_kwargs={\n8700                 \"right\": right,\n8701                 \"labels\": labels,\n8702                 \"precision\": precision,\n8703                 \"include_lowest\": include_lowest,\n8704             },\n8705         )\n8706 \n8707     def weighted(self, weights: DataArray) -> DatasetWeighted:\n8708         \"\"\"\n8709         Weighted Dataset operations.\n8710 \n8711         Parameters\n8712         ----------\n8713         weights : DataArray\n8714             An array of weights associated with the values in this Dataset.\n8715             Each value in the data contributes to the reduction operation\n8716             according to its associated weight.\n8717 \n8718         Notes\n8719         -----\n8720         ``weights`` must be a DataArray and cannot contain missing values.\n8721         Missing values can be replaced by ``weights.fillna(0)``.\n8722 \n8723         Returns\n8724         -------\n8725         core.weighted.DatasetWeighted\n8726 \n8727         See Also\n8728         --------\n8729         DataArray.weighted\n8730         \"\"\"\n8731         from .weighted import DatasetWeighted\n8732 \n8733         return DatasetWeighted(self, weights)\n8734 \n8735     def rolling(\n8736         self,\n8737         dim: Mapping[Any, int] | None = None,\n8738         min_periods: int | None = None,\n8739         center: bool | Mapping[Any, bool] = False,\n8740         **window_kwargs: int,\n8741     ) -> DatasetRolling:\n8742         \"\"\"\n8743         Rolling window object for Datasets.\n8744 \n8745         Parameters\n8746         ----------\n8747         dim : dict, optional\n8748             Mapping from the dimension name to create the rolling iterator\n8749             along (e.g. `time`) to its moving window size.\n8750         min_periods : int or None, default: None\n8751             Minimum number of observations in window required to have a value\n8752             (otherwise result is NA). The default, None, is equivalent to\n8753             setting min_periods equal to the size of the window.\n8754         center : bool or Mapping to int, default: False\n8755             Set the labels at the center of the window.\n8756         **window_kwargs : optional\n8757             The keyword arguments form of ``dim``.\n8758             One of dim or window_kwargs must be provided.\n8759 \n8760         Returns\n8761         -------\n8762         core.rolling.DatasetRolling\n8763 \n8764         See Also\n8765         --------\n8766         core.rolling.DatasetRolling\n8767         DataArray.rolling\n8768         \"\"\"\n8769         from .rolling import DatasetRolling\n8770 \n8771         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n8772         return DatasetRolling(self, dim, min_periods=min_periods, center=center)\n8773 \n8774     def coarsen(\n8775         self,\n8776         dim: Mapping[Any, int] | None = None,\n8777         boundary: CoarsenBoundaryOptions = \"exact\",\n8778         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n8779         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n8780         **window_kwargs: int,\n8781     ) -> DatasetCoarsen:\n8782         \"\"\"\n8783         Coarsen object for Datasets.\n8784 \n8785         Parameters\n8786         ----------\n8787         dim : mapping of hashable to int, optional\n8788             Mapping from the dimension name to the window size.\n8789         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n8790             If 'exact', a ValueError will be raised if dimension size is not a\n8791             multiple of the window size. If 'trim', the excess entries are\n8792             dropped. If 'pad', NA will be padded.\n8793         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n8794         coord_func : str or mapping of hashable to str, default: \"mean\"\n8795             function (name) that is applied to the coordinates,\n8796             or a mapping from coordinate name to function (name).\n8797 \n8798         Returns\n8799         -------\n8800         core.rolling.DatasetCoarsen\n8801 \n8802         See Also\n8803         --------\n8804         core.rolling.DatasetCoarsen\n8805         DataArray.coarsen\n8806         \"\"\"\n8807         from .rolling import DatasetCoarsen\n8808 \n8809         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n8810         return DatasetCoarsen(\n8811             self,\n8812             dim,\n8813             boundary=boundary,\n8814             side=side,\n8815             coord_func=coord_func,\n8816         )\n8817 \n8818     def resample(\n8819         self,\n8820         indexer: Mapping[Any, str] | None = None,\n8821         skipna: bool | None = None,\n8822         closed: SideOptions | None = None,\n8823         label: SideOptions | None = None,\n8824         base: int = 0,\n8825         keep_attrs: bool | None = None,\n8826         loffset: datetime.timedelta | str | None = None,\n8827         restore_coord_dims: bool | None = None,\n8828         **indexer_kwargs: str,\n8829     ) -> DatasetResample:\n8830         \"\"\"Returns a Resample object for performing resampling operations.\n8831 \n8832         Handles both downsampling and upsampling. The resampled\n8833         dimension must be a datetime-like coordinate. If any intervals\n8834         contain no values from the original object, they will be given\n8835         the value ``NaN``.\n8836 \n8837         Parameters\n8838         ----------\n8839         indexer : Mapping of Hashable to str, optional\n8840             Mapping from the dimension name to resample frequency [1]_. The\n8841             dimension must be datetime-like.\n8842         skipna : bool, optional\n8843             Whether to skip missing values when aggregating in downsampling.\n8844         closed : {\"left\", \"right\"}, optional\n8845             Side of each interval to treat as closed.\n8846         label : {\"left\", \"right\"}, optional\n8847             Side of each interval to use for labeling.\n8848         base : int, default = 0\n8849             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n8850             aggregated intervals. For example, for \"24H\" frequency, base could\n8851             range from 0 through 23.\n8852         loffset : timedelta or str, optional\n8853             Offset used to adjust the resampled time labels. Some pandas date\n8854             offset strings are supported.\n8855         restore_coord_dims : bool, optional\n8856             If True, also restore the dimension order of multi-dimensional\n8857             coordinates.\n8858         **indexer_kwargs : str\n8859             The keyword arguments form of ``indexer``.\n8860             One of indexer or indexer_kwargs must be provided.\n8861 \n8862         Returns\n8863         -------\n8864         resampled : core.resample.DataArrayResample\n8865             This object resampled.\n8866 \n8867         See Also\n8868         --------\n8869         DataArray.resample\n8870         pandas.Series.resample\n8871         pandas.DataFrame.resample\n8872 \n8873         References\n8874         ----------\n8875         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n8876         \"\"\"\n8877         from .resample import DatasetResample\n8878 \n8879         return self._resample(\n8880             resample_cls=DatasetResample,\n8881             indexer=indexer,\n8882             skipna=skipna,\n8883             closed=closed,\n8884             label=label,\n8885             base=base,\n8886             keep_attrs=keep_attrs,\n8887             loffset=loffset,\n8888             restore_coord_dims=restore_coord_dims,\n8889             **indexer_kwargs,\n8890         )\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]