2025-10-15 20:12:56.110 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 20:12:58.720 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=764, output_tokens=12, cost=0.008000
2025-10-15 20:12:58.721 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 20:12:58.724 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 20:13:06.262 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1052, output_tokens=333, cost=0.020510
2025-10-15 20:13:06.272 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 20:13:08.367 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=687, output_tokens=67, cost=0.008880
2025-10-15 20:13:08.368 | DEBUG    | Extracted a valid json.
2025-10-15 20:13:08.368 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Query\")",
        "search_method_in_class(\"combine\", \"Query\")",
        "search_code(\"assert set(change_map.keys()).intersection(set(change_map.values())) == set()\")",
        "search_class(\"Invoice\")"
    ],
    "bug_locations": []
}
2025-10-15 20:13:08.456 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 20:13:08.456 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 20:13:28.190 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3865, output_tokens=766, cost=0.061630
2025-10-15 20:13:28.205 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 20:13:39.406 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4847, output_tokens=413, cost=0.060860
2025-10-15 20:13:39.416 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 20:13:47.689 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=767, output_tokens=312, cost=0.017030
2025-10-15 20:13:47.690 | DEBUG    | Extracted a valid json.
2025-10-15 20:13:47.690 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/sql/query.py",
            "class": "Query",
            "method": "combine",
            "intended_behavior": "The `combine` method should ensure stability and predictability in how queries are merged, regardless of the order in which dict or set operations occur. This might involve introducing order-independent checks or validations to prevent the creation of circular references in `change_map` or handling Python's non-deterministic collections in a manner that does not affect the query's outcome. Additionally, improving error handling around assertions or conditions that lead to the observed non-deterministic crashes could be part of the solution. Ensuring consistent behavior across executions with the same input should be a key aim."
        },
        {
            "file": "django/db/models/sql/query.py",
            "class": "Query",
            "method": "change_aliases",
            "intended_behavior": "In the `change_aliases` method, the critical assertion that checks for circular references in `change_map` should be preserved, but the analysis and manipulation of `change_map` may need enhancement. Specifically, the method should implement a more robust mechanism for managing alias changes that inherently avoids the possibility of creating circular references, regardless of the complexity of the query structures or the interactions of combined queries. This might require reevaluating the logic that leads to alias modifications, ensuring that alias changes do not introduce logical inconsistencies or dependencies that break this assertion."
        }
    ]
}
2025-10-15 20:13:47.714 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>combine</method>
<code>
519     def combine(self, rhs, connector):
520         """
521         Merge the 'rhs' query into the current one (with any 'rhs' effects
522         being applied *after* (that is, "to the right of") anything in the
523         current query. 'rhs' is not modified during a call to this function.
524 
525         The 'connector' parameter describes how to connect filters from the
526         'rhs' query.
527         """
528         assert self.model == rhs.model, \
529             "Cannot combine queries on two different base models."
530         assert self.can_filter(), \
531             "Cannot combine queries once a slice has been taken."
532         assert self.distinct == rhs.distinct, \
533             "Cannot combine a unique query with a non-unique query."
534         assert self.distinct_fields == rhs.distinct_fields, \
535             "Cannot combine queries with different distinct fields."
536 
537         # Work out how to relabel the rhs aliases, if necessary.
538         change_map = {}
539         conjunction = (connector == AND)
540 
541         # Determine which existing joins can be reused. When combining the
542         # query with AND we must recreate all joins for m2m filters. When
543         # combining with OR we can reuse joins. The reason is that in AND
544         # case a single row can't fulfill a condition like:
545         #     revrel__col=1 & revrel__col=2
546         # But, there might be two different related rows matching this
547         # condition. In OR case a single True is enough, so single row is
548         # enough, too.
549         #
550         # Note that we will be creating duplicate joins for non-m2m joins in
551         # the AND case. The results will be correct but this creates too many
552         # joins. This is something that could be fixed later on.
553         reuse = set() if conjunction else set(self.tables)
554         # Base table must be present in the query - this is the same
555         # table on both sides.
556         self.get_initial_alias()
557         joinpromoter = JoinPromoter(connector, 2, False)
558         joinpromoter.add_votes(
559             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
560         rhs_votes = set()
561         # Now, add the joins from rhs query into the new query (skipping base
562         # table).
563         for alias in rhs.tables[1:]:
564             join = rhs.alias_map[alias]
565             # If the left side of the join was already relabeled, use the
566             # updated alias.
567             join = join.relabeled_clone(change_map)
568             new_alias = self.join(join, reuse=reuse)
569             if join.join_type == INNER:
570                 rhs_votes.add(new_alias)
571             # We can't reuse the same join again in the query. If we have two
572             # distinct joins for the same connection in rhs query, then the
573             # combined query must have two joins, too.
574             reuse.discard(new_alias)
575             if alias != new_alias:
576                 change_map[alias] = new_alias
577             if not rhs.alias_refcount[alias]:
578                 # The alias was unused in the rhs query. Unref it so that it
579                 # will be unused in the new query, too. We have to add and
580                 # unref the alias so that join promotion has information of
581                 # the join type for the unused alias.
582                 self.unref_alias(new_alias)
583         joinpromoter.add_votes(rhs_votes)
584         joinpromoter.update_join_types(self)
585 
586         # Now relabel a copy of the rhs where-clause and add it to the current
587         # one.
588         w = rhs.where.clone()
589         w.relabel_aliases(change_map)
590         self.where.add(w, connector)
591 
592         # Selection columns and extra extensions are those provided by 'rhs'.
593         self.select = []
594         for col in rhs.select:
595             self.add_select(col.relabeled_clone(change_map))
596 
597         if connector == OR:
598             # It would be nice to be able to handle this, but the queries don't
599             # really make sense (or return consistent value sets). Not worth
600             # the extra complexity when you can write a real query instead.
601             if self._extra and rhs._extra:
602                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
603         self.extra.update(rhs.extra)
604         extra_select_mask = set()
605         if self.extra_select_mask is not None:
606             extra_select_mask.update(self.extra_select_mask)
607         if rhs.extra_select_mask is not None:
608             extra_select_mask.update(rhs.extra_select_mask)
609         if extra_select_mask:
610             self.set_extra_mask(extra_select_mask)
611         self.extra_tables += rhs.extra_tables
612 
613         # Ordering uses the 'rhs' ordering, unless it has none, in which case
614         # the current ordering is used.
615         self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by
616         self.extra_order_by = rhs.extra_order_by or self.extra_order_by

</code><intended_behavior>The `combine` method should ensure stability and predictability in how queries are merged, regardless of the order in which dict or set operations occur. This might involve introducing order-independent checks or validations to prevent the creation of circular references in `change_map` or handling Python's non-deterministic collections in a manner that does not affect the query's outcome. Additionally, improving error handling around assertions or conditions that lead to the observed non-deterministic crashes could be part of the solution. Ensuring consistent behavior across executions with the same input should be a key aim.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
117 class Query(object):
118     """
119     A single SQL query.
120     """
121 
122     alias_prefix = 'T'
123     subq_aliases = frozenset([alias_prefix])
124     query_terms = QUERY_TERMS
125 
126     compiler = 'SQLCompiler'
127 
128     def __init__(self, model, where=WhereNode):
129         self.model = model
130         self.alias_refcount = {}
131         # alias_map is the most important data structure regarding joins.
132         # It's used for recording which joins exist in the query and what
133         # types they are. The key is the alias of the joined table (possibly
134         # the table name) and the value is a Join-like object (see
135         # sql.datastructures.Join for more information).
136         self.alias_map = {}
137         # Sometimes the query contains references to aliases in outer queries (as
138         # a result of split_exclude). Correct alias quoting needs to know these
139         # aliases too.
140         self.external_aliases = set()
141         self.table_map = {}     # Maps table names to list of aliases.
142         self.default_cols = True
143         self.default_ordering = True
144         self.standard_ordering = True
145         self.used_aliases = set()
146         self.filter_is_sticky = False
147         self.subquery = False
148 
149         # SQL-related attributes
150         # Select and related select clauses are expressions to use in the
151         # SELECT clause of the query.
152         # The select is used for cases where we want to set up the select
153         # clause to contain other than default fields (values(), subqueries...)
154         # Note that annotations go to annotations dictionary.
155         self.select = []
156         self.tables = []    # Aliases in the order they are created.
157         self.where = where()
158         self.where_class = where
159         # The group_by attribute can have one of the following forms:
160         #  - None: no group by at all in the query
161         #  - A list of expressions: group by (at least) those expressions.
162         #    String refs are also allowed for now.
163         #  - True: group by all select fields of the model
164         # See compiler.get_group_by() for details.
165         self.group_by = None
166         self.order_by = []
167         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
168         self.distinct = False
169         self.distinct_fields = []
170         self.select_for_update = False
171         self.select_for_update_nowait = False
172         self.select_for_update_skip_locked = False
173 
174         self.select_related = False
175         # Arbitrary limit for select_related to prevents infinite recursion.
176         self.max_depth = 5
177 
178         # Holds the selects defined by a call to values() or values_list()
179         # excluding annotation_select and extra_select.
180         self.values_select = []
181 
182         # SQL annotation-related attributes
183         # The _annotations will be an OrderedDict when used. Due to the cost
184         # of creating OrderedDict this attribute is created lazily (in
185         # self.annotations property).
186         self._annotations = None  # Maps alias -> Annotation Expression
187         self.annotation_select_mask = None
188         self._annotation_select_cache = None
189 
190         # Set combination attributes
191         self.combinator = None
192         self.combinator_all = False
193         self.combined_queries = ()
194 
195         # These are for extensions. The contents are more or less appended
196         # verbatim to the appropriate clause.
197         # The _extra attribute is an OrderedDict, lazily created similarly to
198         # .annotations
199         self._extra = None  # Maps col_alias -> (col_sql, params).
200         self.extra_select_mask = None
201         self._extra_select_cache = None
202 
203         self.extra_tables = ()
204         self.extra_order_by = ()
205 
206         # A tuple that is a set of model field names and either True, if these
207         # are the fields to defer, or False if these are the only fields to
208         # load.
209         self.deferred_loading = (set(), True)
210 
211         self.context = {}
212 
213     @property
214     def extra(self):
215         if self._extra is None:
216             self._extra = OrderedDict()
217         return self._extra
218 
219     @property
220     def annotations(self):
221         if self._annotations is None:
222             self._annotations = OrderedDict()
223         return self._annotations
224 
225     def __str__(self):
226         """
227         Returns the query as a string of SQL with the parameter values
228         substituted in (use sql_with_params() to see the unsubstituted string).
229 
230         Parameter values won't necessarily be quoted correctly, since that is
231         done by the database interface at execution time.
232         """
233         sql, params = self.sql_with_params()
234         return sql % params
235 
236     def sql_with_params(self):
237         """
238         Returns the query as an SQL string and the parameters that will be
239         substituted into the query.
240         """
241         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
242 
243     def __deepcopy__(self, memo):
244         result = self.clone(memo=memo)
245         memo[id(self)] = result
246         return result
247 
248     def _prepare(self, field):
249         return self
250 
251     def get_compiler(self, using=None, connection=None):
252         if using is None and connection is None:
253             raise ValueError("Need either using or connection")
254         if using:
255             connection = connections[using]
256         return connection.ops.compiler(self.compiler)(self, connection, using)
257 
258     def get_meta(self):
259         """
260         Returns the Options instance (the model._meta) from which to start
261         processing. Normally, this is self.model._meta, but it can be changed
262         by subclasses.
263         """
264         return self.model._meta
265 
266     def clone(self, klass=None, memo=None, **kwargs):
267         """
268         Creates a copy of the current instance. The 'kwargs' parameter can be
269         used by clients to update attributes after copying has taken place.
270         """
271         obj = Empty()
272         obj.__class__ = klass or self.__class__
273         obj.model = self.model
274         obj.alias_refcount = self.alias_refcount.copy()
275         obj.alias_map = self.alias_map.copy()
276         obj.external_aliases = self.external_aliases.copy()
277         obj.table_map = self.table_map.copy()
278         obj.default_cols = self.default_cols
279         obj.default_ordering = self.default_ordering
280         obj.standard_ordering = self.standard_ordering
281         obj.select = self.select[:]
282         obj.tables = self.tables[:]
283         obj.where = self.where.clone()
284         obj.where_class = self.where_class
285         if self.group_by is None:
286             obj.group_by = None
287         elif self.group_by is True:
288             obj.group_by = True
289         else:
290             obj.group_by = self.group_by[:]
291         obj.order_by = self.order_by[:]
292         obj.low_mark, obj.high_mark = self.low_mark, self.high_mark
293         obj.distinct = self.distinct
294         obj.distinct_fields = self.distinct_fields[:]
295         obj.select_for_update = self.select_for_update
296         obj.select_for_update_nowait = self.select_for_update_nowait
297         obj.select_for_update_skip_locked = self.select_for_update_skip_locked
298         obj.select_related = self.select_related
299         obj.values_select = self.values_select[:]
300         obj._annotations = self._annotations.copy() if self._annotations is not None else None
301         if self.annotation_select_mask is None:
302             obj.annotation_select_mask = None
303         else:
304             obj.annotation_select_mask = self.annotation_select_mask.copy()
305         # _annotation_select_cache cannot be copied, as doing so breaks the
306         # (necessary) state in which both annotations and
307         # _annotation_select_cache point to the same underlying objects.
308         # It will get re-populated in the cloned queryset the next time it's
309         # used.
310         obj._annotation_select_cache = None
311         obj.max_depth = self.max_depth
312         obj.combinator = self.combinator
313         obj.combinator_all = self.combinator_all
314         obj.combined_queries = self.combined_queries
315         obj._extra = self._extra.copy() if self._extra is not None else None
316         if self.extra_select_mask is None:
317             obj.extra_select_mask = None
318         else:
319             obj.extra_select_mask = self.extra_select_mask.copy()
320         if self._extra_select_cache is None:
321             obj._extra_select_cache = None
322         else:
323             obj._extra_select_cache = self._extra_select_cache.copy()
324         obj.extra_tables = self.extra_tables
325         obj.extra_order_by = self.extra_order_by
326         obj.deferred_loading = copy.copy(self.deferred_loading[0]), self.deferred_loading[1]
327         if self.filter_is_sticky and self.used_aliases:
328             obj.used_aliases = self.used_aliases.copy()
329         else:
330             obj.used_aliases = set()
331         obj.filter_is_sticky = False
332         obj.subquery = self.subquery
333         if 'alias_prefix' in self.__dict__:
334             obj.alias_prefix = self.alias_prefix
335         if 'subq_aliases' in self.__dict__:
336             obj.subq_aliases = self.subq_aliases.copy()
337 
338         obj.__dict__.update(kwargs)
339         if hasattr(obj, '_setup_query'):
340             obj._setup_query()
341         obj.context = self.context.copy()
342         obj._forced_pk = getattr(self, '_forced_pk', False)
343         return obj
344 
345     def add_context(self, key, value):
346         self.context[key] = value
347 
348     def get_context(self, key, default=None):
349         return self.context.get(key, default)
350 
351     def relabeled_clone(self, change_map):
352         clone = self.clone()
353         clone.change_aliases(change_map)
354         return clone
355 
356     def rewrite_cols(self, annotation, col_cnt):
357         # We must make sure the inner query has the referred columns in it.
358         # If we are aggregating over an annotation, then Django uses Ref()
359         # instances to note this. However, if we are annotating over a column
360         # of a related model, then it might be that column isn't part of the
361         # SELECT clause of the inner query, and we must manually make sure
362         # the column is selected. An example case is:
363         #    .aggregate(Sum('author__awards'))
364         # Resolving this expression results in a join to author, but there
365         # is no guarantee the awards column of author is in the select clause
366         # of the query. Thus we must manually add the column to the inner
367         # query.
368         orig_exprs = annotation.get_source_expressions()
369         new_exprs = []
370         for expr in orig_exprs:
371             # FIXME: These conditions are fairly arbitrary. Identify a better
372             # method of having expressions decide which code path they should
373             # take.
374             if isinstance(expr, Ref):
375                 # Its already a Ref to subquery (see resolve_ref() for
376                 # details)
377                 new_exprs.append(expr)
378             elif isinstance(expr, (WhereNode, Lookup)):
379                 # Decompose the subexpressions further. The code here is
380                 # copied from the else clause, but this condition must appear
381                 # before the contains_aggregate/is_summary condition below.
382                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
383                 new_exprs.append(new_expr)
384             elif isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
385                 # Reference to column. Make sure the referenced column
386                 # is selected.
387                 col_cnt += 1
388                 col_alias = '__col%d' % col_cnt
389                 self.annotations[col_alias] = expr
390                 self.append_annotation_mask([col_alias])
391                 new_exprs.append(Ref(col_alias, expr))
392             else:
393                 # Some other expression not referencing database values
394                 # directly. Its subexpression might contain Cols.
395                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
396                 new_exprs.append(new_expr)
397         annotation.set_source_expressions(new_exprs)
398         return annotation, col_cnt
399 
400     def get_aggregation(self, using, added_aggregate_names):
401         """
402         Returns the dictionary with the values of the existing aggregations.
403         """
404         if not self.annotation_select:
405             return {}
406         has_limit = self.low_mark != 0 or self.high_mark is not None
407         has_existing_annotations = any(
408             annotation for alias, annotation
409             in self.annotations.items()
410             if alias not in added_aggregate_names
411         )
412         # Decide if we need to use a subquery.
413         #
414         # Existing annotations would cause incorrect results as get_aggregation()
415         # must produce just one result and thus must not use GROUP BY. But we
416         # aren't smart enough to remove the existing annotations from the
417         # query, so those would force us to use GROUP BY.
418         #
419         # If the query has limit or distinct, or uses set operations, then
420         # those operations must be done in a subquery so that the query
421         # aggregates on the limit and/or distinct results instead of applying
422         # the distinct and limit after the aggregation.
423         if (isinstance(self.group_by, list) or has_limit or has_existing_annotations or
424                 self.distinct or self.combinator):
425             from django.db.models.sql.subqueries import AggregateQuery
426             outer_query = AggregateQuery(self.model)
427             inner_query = self.clone()
428             inner_query.select_for_update = False
429             inner_query.select_related = False
430             if not has_limit and not self.distinct_fields:
431                 # Queries with distinct_fields need ordering and when a limit
432                 # is applied we must take the slice from the ordered query.
433                 # Otherwise no need for ordering.
434                 inner_query.clear_ordering(True)
435             if not inner_query.distinct:
436                 # If the inner query uses default select and it has some
437                 # aggregate annotations, then we must make sure the inner
438                 # query is grouped by the main model's primary key. However,
439                 # clearing the select clause can alter results if distinct is
440                 # used.
441                 if inner_query.default_cols and has_existing_annotations:
442                     inner_query.group_by = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
443                 inner_query.default_cols = False
444 
445             relabels = {t: 'subquery' for t in inner_query.tables}
446             relabels[None] = 'subquery'
447             # Remove any aggregates marked for reduction from the subquery
448             # and move them to the outer AggregateQuery.
449             col_cnt = 0
450             for alias, expression in list(inner_query.annotation_select.items()):
451                 if expression.is_summary:
452                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
453                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
454                     del inner_query.annotations[alias]
455                 # Make sure the annotation_select wont use cached results.
456                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
457             if inner_query.select == [] and not inner_query.default_cols and not inner_query.annotation_select_mask:
458                 # In case of Model.objects[0:3].count(), there would be no
459                 # field selected in the inner query, yet we must use a subquery.
460                 # So, make sure at least one field is selected.
461                 inner_query.select = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
462             try:
463                 outer_query.add_subquery(inner_query, using)
464             except EmptyResultSet:
465                 return {
466                     alias: None
467                     for alias in outer_query.annotation_select
468                 }
469         else:
470             outer_query = self
471             self.select = []
472             self.default_cols = False
473             self._extra = {}
474 
475         outer_query.clear_ordering(True)
476         outer_query.clear_limits()
477         outer_query.select_for_update = False
478         outer_query.select_related = False
479         compiler = outer_query.get_compiler(using)
480         result = compiler.execute_sql(SINGLE)
481         if result is None:
482             result = [None for q in outer_query.annotation_select.items()]
483 
484         converters = compiler.get_converters(outer_query.annotation_select.values())
485         result = compiler.apply_converters(result, converters)
486 
487         return {
488             alias: val
489             for (alias, annotation), val
490             in zip(outer_query.annotation_select.items(), result)
491         }
492 
493     def get_count(self, using):
494         """
495         Performs a COUNT() query using the current filter constraints.
496         """
497         obj = self.clone()
498         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
499         number = obj.get_aggregation(using, ['__count'])['__count']
500         if number is None:
501             number = 0
502         return number
503 
504     def has_filters(self):
505         return self.where
506 
507     def has_results(self, using):
508         q = self.clone()
509         if not q.distinct:
510             if q.group_by is True:
511                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
512                 q.set_group_by()
513             q.clear_select_clause()
514         q.clear_ordering(True)
515         q.set_limits(high=1)
516         compiler = q.get_compiler(using=using)
517         return compiler.has_results()
518 
519     def combine(self, rhs, connector):
520         """
521         Merge the 'rhs' query into the current one (with any 'rhs' effects
522         being applied *after* (that is, "to the right of") anything in the
523         current query. 'rhs' is not modified during a call to this function.
524 
525         The 'connector' parameter describes how to connect filters from the
526         'rhs' query.
527         """
528         assert self.model == rhs.model, \
529             "Cannot combine queries on two different base models."
530         assert self.can_filter(), \
531             "Cannot combine queries once a slice has been taken."
532         assert self.distinct == rhs.distinct, \
533             "Cannot combine a unique query with a non-unique query."
534         assert self.distinct_fields == rhs.distinct_fields, \
535             "Cannot combine queries with different distinct fields."
536 
537         # Work out how to relabel the rhs aliases, if necessary.
538         change_map = {}
539         conjunction = (connector == AND)
540 
541         # Determine which existing joins can be reused. When combining the
542         # query with AND we must recreate all joins for m2m filters. When
543         # combining with OR we can reuse joins. The reason is that in AND
544         # case a single row can't fulfill a condition like:
545         #     revrel__col=1 & revrel__col=2
546         # But, there might be two different related rows matching this
547         # condition. In OR case a single True is enough, so single row is
548         # enough, too.
549         #
550         # Note that we will be creating duplicate joins for non-m2m joins in
551         # the AND case. The results will be correct but this creates too many
552         # joins. This is something that could be fixed later on.
553         reuse = set() if conjunction else set(self.tables)
554         # Base table must be present in the query - this is the same
555         # table on both sides.
556         self.get_initial_alias()
557         joinpromoter = JoinPromoter(connector, 2, False)
558         joinpromoter.add_votes(
559             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
560         rhs_votes = set()
561         # Now, add the joins from rhs query into the new query (skipping base
562         # table).
563         for alias in rhs.tables[1:]:
564             join = rhs.alias_map[alias]
565             # If the left side of the join was already relabeled, use the
566             # updated alias.
567             join = join.relabeled_clone(change_map)
568             new_alias = self.join(join, reuse=reuse)
569             if join.join_type == INNER:
570                 rhs_votes.add(new_alias)
571             # We can't reuse the same join again in the query. If we have two
572             # distinct joins for the same connection in rhs query, then the
573             # combined query must have two joins, too.
574             reuse.discard(new_alias)
575             if alias != new_alias:
576                 change_map[alias] = new_alias
577             if not rhs.alias_refcount[alias]:
578                 # The alias was unused in the rhs query. Unref it so that it
579                 # will be unused in the new query, too. We have to add and
580                 # unref the alias so that join promotion has information of
581                 # the join type for the unused alias.
582                 self.unref_alias(new_alias)
583         joinpromoter.add_votes(rhs_votes)
584         joinpromoter.update_join_types(self)
585 
586         # Now relabel a copy of the rhs where-clause and add it to the current
587         # one.
588         w = rhs.where.clone()
589         w.relabel_aliases(change_map)
590         self.where.add(w, connector)
591 
592         # Selection columns and extra extensions are those provided by 'rhs'.
593         self.select = []
594         for col in rhs.select:
595             self.add_select(col.relabeled_clone(change_map))
596 
597         if connector == OR:
598             # It would be nice to be able to handle this, but the queries don't
599             # really make sense (or return consistent value sets). Not worth
600             # the extra complexity when you can write a real query instead.
601             if self._extra and rhs._extra:
602                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
603         self.extra.update(rhs.extra)
604         extra_select_mask = set()
605         if self.extra_select_mask is not None:
606             extra_select_mask.update(self.extra_select_mask)
607         if rhs.extra_select_mask is not None:
608             extra_select_mask.update(rhs.extra_select_mask)
609         if extra_select_mask:
610             self.set_extra_mask(extra_select_mask)
611         self.extra_tables += rhs.extra_tables
612 
613         # Ordering uses the 'rhs' ordering, unless it has none, in which case
614         # the current ordering is used.
615         self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by
616         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
617 
618     def deferred_to_data(self, target, callback):
619         """
620         Converts the self.deferred_loading data structure to an alternate data
621         structure, describing the field that *will* be loaded. This is used to
622         compute the columns to select from the database and also by the
623         QuerySet class to work out which fields are being initialized on each
624         model. Models that have all their fields included aren't mentioned in
625         the result, only those that have field restrictions in place.
626 
627         The "target" parameter is the instance that is populated (in place).
628         The "callback" is a function that is called whenever a (model, field)
629         pair need to be added to "target". It accepts three parameters:
630         "target", and the model and list of fields being added for that model.
631         """
632         field_names, defer = self.deferred_loading
633         if not field_names:
634             return
635         orig_opts = self.get_meta()
636         seen = {}
637         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
638         for field_name in field_names:
639             parts = field_name.split(LOOKUP_SEP)
640             cur_model = self.model._meta.concrete_model
641             opts = orig_opts
642             for name in parts[:-1]:
643                 old_model = cur_model
644                 source = opts.get_field(name)
645                 if is_reverse_o2o(source):
646                     cur_model = source.related_model
647                 else:
648                     cur_model = source.remote_field.model
649                 opts = cur_model._meta
650                 # Even if we're "just passing through" this model, we must add
651                 # both the current model's pk and the related reference field
652                 # (if it's not a reverse relation) to the things we select.
653                 if not is_reverse_o2o(source):
654                     must_include[old_model].add(source)
655                 add_to_dict(must_include, cur_model, opts.pk)
656             field = opts.get_field(parts[-1])
657             is_reverse_object = field.auto_created and not field.concrete
658             model = field.related_model if is_reverse_object else field.model
659             model = model._meta.concrete_model
660             if model == opts.model:
661                 model = cur_model
662             if not is_reverse_o2o(field):
663                 add_to_dict(seen, model, field)
664 
665         if defer:
666             # We need to load all fields for each model, except those that
667             # appear in "seen" (for all models that appear in "seen"). The only
668             # slight complexity here is handling fields that exist on parent
669             # models.
670             workset = {}
671             for model, values in six.iteritems(seen):
672                 for field in model._meta.fields:
673                     if field in values:
674                         continue
675                     m = field.model._meta.concrete_model
676                     add_to_dict(workset, m, field)
677             for model, values in six.iteritems(must_include):
678                 # If we haven't included a model in workset, we don't add the
679                 # corresponding must_include fields for that model, since an
680                 # empty set means "include all fields". That's why there's no
681                 # "else" branch here.
682                 if model in workset:
683                     workset[model].update(values)
684             for model, values in six.iteritems(workset):
685                 callback(target, model, values)
686         else:
687             for model, values in six.iteritems(must_include):
688                 if model in seen:
689                     seen[model].update(values)
690                 else:
691                     # As we've passed through this model, but not explicitly
692                     # included any fields, we have to make sure it's mentioned
693                     # so that only the "must include" fields are pulled in.
694                     seen[model] = values
695             # Now ensure that every model in the inheritance chain is mentioned
696             # in the parent list. Again, it must be mentioned to ensure that
697             # only "must include" fields are pulled in.
698             for model in orig_opts.get_parent_list():
699                 if model not in seen:
700                     seen[model] = set()
701             for model, values in six.iteritems(seen):
702                 callback(target, model, values)
703 
704     def table_alias(self, table_name, create=False):
705         """
706         Returns a table alias for the given table_name and whether this is a
707         new alias or not.
708 
709         If 'create' is true, a new alias is always created. Otherwise, the
710         most recently created alias for the table (if one exists) is reused.
711         """
712         alias_list = self.table_map.get(table_name)
713         if not create and alias_list:
714             alias = alias_list[0]
715             self.alias_refcount[alias] += 1
716             return alias, False
717 
718         # Create a new alias for this table.
719         if alias_list:
720             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
721             alias_list.append(alias)
722         else:
723             # The first occurrence of a table uses the table name directly.
724             alias = table_name
725             self.table_map[alias] = [alias]
726         self.alias_refcount[alias] = 1
727         self.tables.append(alias)
728         return alias, True
729 
730     def ref_alias(self, alias):
731         """ Increases the reference count for this alias. """
732         self.alias_refcount[alias] += 1
733 
734     def unref_alias(self, alias, amount=1):
735         """ Decreases the reference count for this alias. """
736         self.alias_refcount[alias] -= amount
737 
738     def promote_joins(self, aliases):
739         """
740         Promotes recursively the join type of given aliases and its children to
741         an outer join. If 'unconditional' is False, the join is only promoted if
742         it is nullable or the parent join is an outer join.
743 
744         The children promotion is done to avoid join chains that contain a LOUTER
745         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
746         then we must also promote b->c automatically, or otherwise the promotion
747         of a->b doesn't actually change anything in the query results.
748         """
749         aliases = list(aliases)
750         while aliases:
751             alias = aliases.pop(0)
752             if self.alias_map[alias].join_type is None:
753                 # This is the base table (first FROM entry) - this table
754                 # isn't really joined at all in the query, so we should not
755                 # alter its join type.
756                 continue
757             # Only the first alias (skipped above) should have None join_type
758             assert self.alias_map[alias].join_type is not None
759             parent_alias = self.alias_map[alias].parent_alias
760             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
761             already_louter = self.alias_map[alias].join_type == LOUTER
762             if ((self.alias_map[alias].nullable or parent_louter) and
763                     not already_louter):
764                 self.alias_map[alias] = self.alias_map[alias].promote()
765                 # Join type of 'alias' changed, so re-examine all aliases that
766                 # refer to this one.
767                 aliases.extend(
768                     join for join in self.alias_map.keys()
769                     if self.alias_map[join].parent_alias == alias and join not in aliases
770                 )
771 
772     def demote_joins(self, aliases):
773         """
774         Change join type from LOUTER to INNER for all joins in aliases.
775 
776         Similarly to promote_joins(), this method must ensure no join chains
777         containing first an outer, then an inner join are generated. If we
778         are demoting b->c join in chain a LOUTER b LOUTER c then we must
779         demote a->b automatically, or otherwise the demotion of b->c doesn't
780         actually change anything in the query results. .
781         """
782         aliases = list(aliases)
783         while aliases:
784             alias = aliases.pop(0)
785             if self.alias_map[alias].join_type == LOUTER:
786                 self.alias_map[alias] = self.alias_map[alias].demote()
787                 parent_alias = self.alias_map[alias].parent_alias
788                 if self.alias_map[parent_alias].join_type == INNER:
789                     aliases.append(parent_alias)
790 
791     def reset_refcounts(self, to_counts):
792         """
793         This method will reset reference counts for aliases so that they match
794         the value passed in :param to_counts:.
795         """
796         for alias, cur_refcount in self.alias_refcount.copy().items():
797             unref_amount = cur_refcount - to_counts.get(alias, 0)
798             self.unref_alias(alias, unref_amount)
799 
800     def change_aliases(self, change_map):
801         """
802         Changes the aliases in change_map (which maps old-alias -> new-alias),
803         relabelling any references to them in select columns and the where
804         clause.
805         """
806         assert set(change_map.keys()).intersection(set(change_map.values())) == set()
807 
808         # 1. Update references in "select" (normal columns plus aliases),
809         # "group by" and "where".
810         self.where.relabel_aliases(change_map)
811         if isinstance(self.group_by, list):
812             self.group_by = [col.relabeled_clone(change_map) for col in self.group_by]
813         self.select = [col.relabeled_clone(change_map) for col in self.select]
814         if self._annotations:
815             self._annotations = OrderedDict(
816                 (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items())
817 
818         # 2. Rename the alias in the internal table/alias datastructures.
819         for old_alias, new_alias in six.iteritems(change_map):
820             if old_alias not in self.alias_map:
821                 continue
822             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
823             self.alias_map[new_alias] = alias_data
824             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
825             del self.alias_refcount[old_alias]
826             del self.alias_map[old_alias]
827 
828             table_aliases = self.table_map[alias_data.table_name]
829             for pos, alias in enumerate(table_aliases):
830                 if alias == old_alias:
831                     table_aliases[pos] = new_alias
832                     break
833         self.external_aliases = {change_map.get(alias, alias)
834                                  for alias in self.external_aliases}
835 
836     def bump_prefix(self, outer_query):
837         """
838         Changes the alias prefix to the next letter in the alphabet in a way
839         that the outer query's aliases and this query's aliases will not
840         conflict. Even tables that previously had no alias will get an alias
841         after this call.
842         """
843         def prefix_gen():
844             """
845             Generates a sequence of characters in alphabetical order:
846                 -> 'A', 'B', 'C', ...
847 
848             When the alphabet is finished, the sequence will continue with the
849             Cartesian product:
850                 -> 'AA', 'AB', 'AC', ...
851             """
852             alphabet = ascii_uppercase
853             prefix = chr(ord(self.alias_prefix) + 1)
854             yield prefix
855             for n in count(1):
856                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
857                 for s in product(seq, repeat=n):
858                     yield ''.join(s)
859                 prefix = None
860 
861         if self.alias_prefix != outer_query.alias_prefix:
862             # No clashes between self and outer query should be possible.
863             return
864 
865         local_recursion_limit = 127  # explicitly avoid infinite loop
866         for pos, prefix in enumerate(prefix_gen()):
867             if prefix not in self.subq_aliases:
868                 self.alias_prefix = prefix
869                 break
870             if pos > local_recursion_limit:
871                 raise RuntimeError(
872                     'Maximum recursion depth exceeded: too many subqueries.'
873                 )
874         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
875         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
876         change_map = OrderedDict()
877         for pos, alias in enumerate(self.tables):
878             new_alias = '%s%d' % (self.alias_prefix, pos)
879             change_map[alias] = new_alias
880             self.tables[pos] = new_alias
881         self.change_aliases(change_map)
882 
883     def get_initial_alias(self):
884         """
885         Returns the first alias for this query, after increasing its reference
886         count.
887         """
888         if self.tables:
889             alias = self.tables[0]
890             self.ref_alias(alias)
891         else:
892             alias = self.join(BaseTable(self.get_meta().db_table, None))
893         return alias
894 
895     def count_active_tables(self):
896         """
897         Returns the number of tables in this query with a non-zero reference
898         count. Note that after execution, the reference counts are zeroed, so
899         tables added in compiler will not be seen by this method.
900         """
901         return len([1 for count in self.alias_refcount.values() if count])
902 
903     def join(self, join, reuse=None):
904         """
905         Return an alias for the 'join', either reusing an existing alias for
906         that join or creating a new one. 'join' is either a
907         sql.datastructures.BaseTable or Join.
908 
909         The 'reuse' parameter can be either None which means all joins are
910         reusable, or it can be a set containing the aliases that can be reused.
911 
912         A join is always created as LOUTER if the lhs alias is LOUTER to make
913         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
914         joins are created as LOUTER if the join is nullable.
915         """
916         reuse = [a for a, j in self.alias_map.items()
917                  if (reuse is None or a in reuse) and j == join]
918         if reuse:
919             self.ref_alias(reuse[0])
920             return reuse[0]
921 
922         # No reuse is possible, so we need a new alias.
923         alias, _ = self.table_alias(join.table_name, create=True)
924         if join.join_type:
925             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
926                 join_type = LOUTER
927             else:
928                 join_type = INNER
929             join.join_type = join_type
930         join.table_alias = alias
931         self.alias_map[alias] = join
932         return alias
933 
934     def join_parent_model(self, opts, model, alias, seen):
935         """
936         Makes sure the given 'model' is joined in the query. If 'model' isn't
937         a parent of 'opts' or if it is None this method is a no-op.
938 
939         The 'alias' is the root alias for starting the join, 'seen' is a dict
940         of model -> alias of existing joins. It must also contain a mapping
941         of None -> some alias. This will be returned in the no-op case.
942         """
943         if model in seen:
944             return seen[model]
945         chain = opts.get_base_chain(model)
946         if not chain:
947             return alias
948         curr_opts = opts
949         for int_model in chain:
950             if int_model in seen:
951                 curr_opts = int_model._meta
952                 alias = seen[int_model]
953                 continue
954             # Proxy model have elements in base chain
955             # with no parents, assign the new options
956             # object and skip to the next base in that
957             # case
958             if not curr_opts.parents[int_model]:
959                 curr_opts = int_model._meta
960                 continue
961             link_field = curr_opts.get_ancestor_link(int_model)
962             _, _, _, joins, _ = self.setup_joins(
963                 [link_field.name], curr_opts, alias)
964             curr_opts = int_model._meta
965             alias = seen[int_model] = joins[-1]
966         return alias or seen[None]
967 
968     def add_annotation(self, annotation, alias, is_summary=False):
969         """
970         Adds a single annotation expression to the Query
971         """
972         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
973                                                    summarize=is_summary)
974         self.append_annotation_mask([alias])
975         self.annotations[alias] = annotation
976 
977     def _prepare_as_filter_value(self):
978         return self.clone()
979 
980     def prepare_lookup_value(self, value, lookups, can_reuse, allow_joins=True):
981         # Default lookup if none given is exact.
982         used_joins = []
983         if len(lookups) == 0:
984             lookups = ['exact']
985         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
986         # uses of None as a query value.
987         if value is None:
988             if lookups[-1] not in ('exact', 'iexact'):
989                 raise ValueError("Cannot use None as a query value")
990             return True, ['isnull'], used_joins
991         elif hasattr(value, 'resolve_expression'):
992             pre_joins = self.alias_refcount.copy()
993             value = value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
994             used_joins = [k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)]
995         elif isinstance(value, (list, tuple)):
996             # The items of the iterable may be expressions and therefore need
997             # to be resolved independently.
998             processed_values = []
999             used_joins = set()
1000             for sub_value in value:
1001                 if hasattr(sub_value, 'resolve_expression'):
1002                     pre_joins = self.alias_refcount.copy()
1003                     processed_values.append(
1004                         sub_value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
1005                     )
1006                     # The used_joins for a tuple of expressions is the union of
1007                     # the used_joins for the individual expressions.
1008                     used_joins |= set(k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0))
1009         # Subqueries need to use a different set of aliases than the
1010         # outer query. Call bump_prefix to change aliases of the inner
1011         # query (the value).
1012         if hasattr(value, '_prepare_as_filter_value'):
1013             value = value._prepare_as_filter_value()
1014             value.bump_prefix(self)
1015         # For Oracle '' is equivalent to null. The check needs to be done
1016         # at this stage because join promotion can't be done at compiler
1017         # stage. Using DEFAULT_DB_ALIAS isn't nice, but it is the best we
1018         # can do here. Similar thing is done in is_nullable(), too.
1019         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1020                 lookups[-1] == 'exact' and value == ''):
1021             value = True
1022             lookups[-1] = 'isnull'
1023         return value, lookups, used_joins
1024 
1025     def solve_lookup_type(self, lookup):
1026         """
1027         Solve the lookup type from the lookup (eg: 'foobar__id__icontains')
1028         """
1029         lookup_splitted = lookup.split(LOOKUP_SEP)
1030         if self._annotations:
1031             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1032             if expression:
1033                 return expression_lookups, (), expression
1034         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1035         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1036         if len(lookup_parts) == 0:
1037             lookup_parts = ['exact']
1038         elif len(lookup_parts) > 1:
1039             if not field_parts:
1040                 raise FieldError(
1041                     'Invalid lookup "%s" for model %s".' %
1042                     (lookup, self.get_meta().model.__name__))
1043         return lookup_parts, field_parts, False
1044 
1045     def check_query_object_type(self, value, opts, field):
1046         """
1047         Checks whether the object passed while querying is of the correct type.
1048         If not, it raises a ValueError specifying the wrong object.
1049         """
1050         if hasattr(value, '_meta'):
1051             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1052                 raise ValueError(
1053                     'Cannot query "%s": Must be "%s" instance.' %
1054                     (value, opts.object_name))
1055 
1056     def check_related_objects(self, field, value, opts):
1057         """
1058         Checks the type of object passed to query relations.
1059         """
1060         if field.is_relation:
1061             # Check that the field and the queryset use the same model in a
1062             # query like .filter(author=Author.objects.all()). For example, the
1063             # opts would be Author's (from the author field) and value.model
1064             # would be Author.objects.all() queryset's .model (Author also).
1065             # The field is the related field on the lhs side.
1066             # If _forced_pk isn't set, this isn't a queryset query or values()
1067             # or values_list() was specified by the developer in which case
1068             # that choice is trusted.
1069             if (getattr(value, '_forced_pk', False) and
1070                     not check_rel_lookup_compatibility(value.model, opts, field)):
1071                 raise ValueError(
1072                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1073                     (value.model._meta.object_name, opts.object_name)
1074                 )
1075             elif hasattr(value, '_meta'):
1076                 self.check_query_object_type(value, opts, field)
1077             elif hasattr(value, '__iter__'):
1078                 for v in value:
1079                     self.check_query_object_type(v, opts, field)
1080 
1081     def build_lookup(self, lookups, lhs, rhs):
1082         """
1083         Tries to extract transforms and lookup from given lhs.
1084 
1085         The lhs value is something that works like SQLExpression.
1086         The rhs value is what the lookup is going to compare against.
1087         The lookups is a list of names to extract using get_lookup()
1088         and get_transform().
1089         """
1090         lookups = lookups[:]
1091         while lookups:
1092             name = lookups[0]
1093             # If there is just one part left, try first get_lookup() so
1094             # that if the lhs supports both transform and lookup for the
1095             # name, then lookup will be picked.
1096             if len(lookups) == 1:
1097                 final_lookup = lhs.get_lookup(name)
1098                 if not final_lookup:
1099                     # We didn't find a lookup. We are going to interpret
1100                     # the name as transform, and do an Exact lookup against
1101                     # it.
1102                     lhs = self.try_transform(lhs, name, lookups)
1103                     final_lookup = lhs.get_lookup('exact')
1104                 return final_lookup(lhs, rhs)
1105             lhs = self.try_transform(lhs, name, lookups)
1106             lookups = lookups[1:]
1107 
1108     def try_transform(self, lhs, name, rest_of_lookups):
1109         """
1110         Helper method for build_lookup. Tries to fetch and initialize
1111         a transform for name parameter from lhs.
1112         """
1113         transform_class = lhs.get_transform(name)
1114         if transform_class:
1115             return transform_class(lhs)
1116         else:
1117             raise FieldError(
1118                 "Unsupported lookup '%s' for %s or join on the field not "
1119                 "permitted." %
1120                 (name, lhs.output_field.__class__.__name__))
1121 
1122     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1123                      can_reuse=None, connector=AND, allow_joins=True, split_subq=True):
1124         """
1125         Builds a WhereNode for a single filter clause, but doesn't add it
1126         to this Query. Query.add_q() will then add this filter to the where
1127         Node.
1128 
1129         The 'branch_negated' tells us if the current branch contains any
1130         negations. This will be used to determine if subqueries are needed.
1131 
1132         The 'current_negated' is used to determine if the current filter is
1133         negated or not and this will be used to determine if IS NULL filtering
1134         is needed.
1135 
1136         The difference between current_netageted and branch_negated is that
1137         branch_negated is set on first negation, but current_negated is
1138         flipped for each negation.
1139 
1140         Note that add_filter will not do any negating itself, that is done
1141         upper in the code by add_q().
1142 
1143         The 'can_reuse' is a set of reusable joins for multijoins.
1144 
1145         The method will create a filter clause that can be added to the current
1146         query. However, if the filter isn't added to the query then the caller
1147         is responsible for unreffing the joins used.
1148         """
1149         if isinstance(filter_expr, dict):
1150             raise FieldError("Cannot parse keyword query as dict")
1151         arg, value = filter_expr
1152         if not arg:
1153             raise FieldError("Cannot parse keyword query %r" % arg)
1154         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1155         if not allow_joins and len(parts) > 1:
1156             raise FieldError("Joined field references are not permitted in this query")
1157 
1158         # Work out the lookup type and remove it from the end of 'parts',
1159         # if necessary.
1160         value, lookups, used_joins = self.prepare_lookup_value(value, lookups, can_reuse, allow_joins)
1161 
1162         clause = self.where_class()
1163         if reffed_expression:
1164             condition = self.build_lookup(lookups, reffed_expression, value)
1165             clause.add(condition, AND)
1166             return clause, []
1167 
1168         opts = self.get_meta()
1169         alias = self.get_initial_alias()
1170         allow_many = not branch_negated or not split_subq
1171 
1172         try:
1173             field, sources, opts, join_list, path = self.setup_joins(
1174                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many)
1175 
1176             # Prevent iterator from being consumed by check_related_objects()
1177             if isinstance(value, Iterator):
1178                 value = list(value)
1179             self.check_related_objects(field, value, opts)
1180 
1181             # split_exclude() needs to know which joins were generated for the
1182             # lookup parts
1183             self._lookup_joins = join_list
1184         except MultiJoin as e:
1185             return self.split_exclude(filter_expr, LOOKUP_SEP.join(parts[:e.level]),
1186                                       can_reuse, e.names_with_path)
1187 
1188         if can_reuse is not None:
1189             can_reuse.update(join_list)
1190         used_joins = set(used_joins).union(set(join_list))
1191         targets, alias, join_list = self.trim_joins(sources, join_list, path)
1192 
1193         if field.is_relation:
1194             # No support for transforms for relational fields
1195             num_lookups = len(lookups)
1196             if num_lookups > 1:
1197                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1198             assert num_lookups > 0  # Likely a bug in Django if this fails.
1199             lookup_class = field.get_lookup(lookups[0])
1200             if lookup_class is None:
1201                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1202             if len(targets) == 1:
1203                 lhs = targets[0].get_col(alias, field)
1204             else:
1205                 lhs = MultiColSource(alias, targets, sources, field)
1206             condition = lookup_class(lhs, value)
1207             lookup_type = lookup_class.lookup_name
1208         else:
1209             col = targets[0].get_col(alias, field)
1210             condition = self.build_lookup(lookups, col, value)
1211             lookup_type = condition.lookup_name
1212 
1213         clause.add(condition, AND)
1214 
1215         require_outer = lookup_type == 'isnull' and value is True and not current_negated
1216         if current_negated and (lookup_type != 'isnull' or value is False):
1217             require_outer = True
1218             if (lookup_type != 'isnull' and (
1219                     self.is_nullable(targets[0]) or
1220                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1221                 # The condition added here will be SQL like this:
1222                 # NOT (col IS NOT NULL), where the first NOT is added in
1223                 # upper layers of code. The reason for addition is that if col
1224                 # is null, then col != someval will result in SQL "unknown"
1225                 # which isn't the same as in Python. The Python None handling
1226                 # is wanted, and it can be gotten by
1227                 # (col IS NULL OR col != someval)
1228                 #   <=>
1229                 # NOT (col IS NOT NULL AND col = someval).
1230                 lookup_class = targets[0].get_lookup('isnull')
1231                 clause.add(lookup_class(targets[0].get_col(alias, sources[0]), False), AND)
1232         return clause, used_joins if not require_outer else ()
1233 
1234     def add_filter(self, filter_clause):
1235         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1236 
1237     def add_q(self, q_object):
1238         """
1239         A preprocessor for the internal _add_q(). Responsible for doing final
1240         join promotion.
1241         """
1242         # For join promotion this case is doing an AND for the added q_object
1243         # and existing conditions. So, any existing inner join forces the join
1244         # type to remain inner. Existing outer joins can however be demoted.
1245         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1246         # rel_a doesn't produce any rows, then the whole condition must fail.
1247         # So, demotion is OK.
1248         existing_inner = set(
1249             (a for a in self.alias_map if self.alias_map[a].join_type == INNER))
1250         clause, _ = self._add_q(q_object, self.used_aliases)
1251         if clause:
1252             self.where.add(clause, AND)
1253         self.demote_joins(existing_inner)
1254 
1255     def _add_q(self, q_object, used_aliases, branch_negated=False,
1256                current_negated=False, allow_joins=True, split_subq=True):
1257         """
1258         Adds a Q-object to the current filter.
1259         """
1260         connector = q_object.connector
1261         current_negated = current_negated ^ q_object.negated
1262         branch_negated = branch_negated or q_object.negated
1263         target_clause = self.where_class(connector=connector,
1264                                          negated=q_object.negated)
1265         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1266         for child in q_object.children:
1267             if isinstance(child, Node):
1268                 child_clause, needed_inner = self._add_q(
1269                     child, used_aliases, branch_negated,
1270                     current_negated, allow_joins, split_subq)
1271                 joinpromoter.add_votes(needed_inner)
1272             else:
1273                 child_clause, needed_inner = self.build_filter(
1274                     child, can_reuse=used_aliases, branch_negated=branch_negated,
1275                     current_negated=current_negated, connector=connector,
1276                     allow_joins=allow_joins, split_subq=split_subq,
1277                 )
1278                 joinpromoter.add_votes(needed_inner)
1279             if child_clause:
1280                 target_clause.add(child_clause, connector)
1281         needed_inner = joinpromoter.update_join_types(self)
1282         return target_clause, needed_inner
1283 
1284     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1285         """
1286         Walks the list of names and turns them into PathInfo tuples. Note that
1287         a single name in 'names' can generate multiple PathInfos (m2m for
1288         example).
1289 
1290         'names' is the path of names to travel, 'opts' is the model Options we
1291         start the name resolving from, 'allow_many' is as for setup_joins().
1292         If fail_on_missing is set to True, then a name that can't be resolved
1293         will generate a FieldError.
1294 
1295         Returns a list of PathInfo tuples. In addition returns the final field
1296         (the last used join field), and target (which is a field guaranteed to
1297         contain the same value as the final field). Finally, the method returns
1298         those names that weren't found (which are likely transforms and the
1299         final lookup).
1300         """
1301         path, names_with_path = [], []
1302         for pos, name in enumerate(names):
1303             cur_names_with_path = (name, [])
1304             if name == 'pk':
1305                 name = opts.pk.name
1306 
1307             field = None
1308             try:
1309                 field = opts.get_field(name)
1310             except FieldDoesNotExist:
1311                 if name in self.annotation_select:
1312                     field = self.annotation_select[name].output_field
1313                 elif pos == 0:
1314                     for rel in opts.related_objects:
1315                         if (name == rel.related_model._meta.model_name and
1316                                 rel.related_name == rel.related_model._meta.default_related_name):
1317                             related_name = rel.related_name
1318                             field = opts.get_field(related_name)
1319                             warnings.warn(
1320                                 "Query lookup '%s' is deprecated in favor of "
1321                                 "Meta.default_related_name '%s'."
1322                                 % (name, related_name),
1323                                 RemovedInDjango20Warning, 2
1324                             )
1325                             break
1326 
1327             if field is not None:
1328                 # Fields that contain one-to-many relations with a generic
1329                 # model (like a GenericForeignKey) cannot generate reverse
1330                 # relations and therefore cannot be used for reverse querying.
1331                 if field.is_relation and not field.related_model:
1332                     raise FieldError(
1333                         "Field %r does not generate an automatic reverse "
1334                         "relation and therefore cannot be used for reverse "
1335                         "querying. If it is a GenericForeignKey, consider "
1336                         "adding a GenericRelation." % name
1337                     )
1338                 try:
1339                     model = field.model._meta.concrete_model
1340                 except AttributeError:
1341                     # QuerySet.annotate() may introduce fields that aren't
1342                     # attached to a model.
1343                     model = None
1344             else:
1345                 # We didn't find the current field, so move position back
1346                 # one step.
1347                 pos -= 1
1348                 if pos == -1 or fail_on_missing:
1349                     field_names = list(get_field_names_from_opts(opts))
1350                     available = sorted(field_names + list(self.annotation_select))
1351                     raise FieldError("Cannot resolve keyword '%s' into field. "
1352                                      "Choices are: %s" % (name, ", ".join(available)))
1353                 break
1354             # Check if we need any joins for concrete inheritance cases (the
1355             # field lives in parent, but we are currently in one of its
1356             # children)
1357             if model is not opts.model:
1358                 path_to_parent = opts.get_path_to_parent(model)
1359                 if path_to_parent:
1360                     path.extend(path_to_parent)
1361                     cur_names_with_path[1].extend(path_to_parent)
1362                     opts = path_to_parent[-1].to_opts
1363             if hasattr(field, 'get_path_info'):
1364                 pathinfos = field.get_path_info()
1365                 if not allow_many:
1366                     for inner_pos, p in enumerate(pathinfos):
1367                         if p.m2m:
1368                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1369                             names_with_path.append(cur_names_with_path)
1370                             raise MultiJoin(pos + 1, names_with_path)
1371                 last = pathinfos[-1]
1372                 path.extend(pathinfos)
1373                 final_field = last.join_field
1374                 opts = last.to_opts
1375                 targets = last.target_fields
1376                 cur_names_with_path[1].extend(pathinfos)
1377                 names_with_path.append(cur_names_with_path)
1378             else:
1379                 # Local non-relational field.
1380                 final_field = field
1381                 targets = (field,)
1382                 if fail_on_missing and pos + 1 != len(names):
1383                     raise FieldError(
1384                         "Cannot resolve keyword %r into field. Join on '%s'"
1385                         " not permitted." % (names[pos + 1], name))
1386                 break
1387         return path, final_field, targets, names[pos + 1:]
1388 
1389     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True):
1390         """
1391         Compute the necessary table joins for the passage through the fields
1392         given in 'names'. 'opts' is the Options class for the current model
1393         (which gives the table we are starting from), 'alias' is the alias for
1394         the table to start the joining from.
1395 
1396         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1397         can be None in which case all joins are reusable or a set of aliases
1398         that can be reused. Note that non-reverse foreign keys are always
1399         reusable when using setup_joins().
1400 
1401         If 'allow_many' is False, then any reverse foreign key seen will
1402         generate a MultiJoin exception.
1403 
1404         Returns the final field involved in the joins, the target field (used
1405         for any 'where' constraint), the final 'opts' value, the joins and the
1406         field path travelled to generate the joins.
1407 
1408         The target field is the field containing the concrete value. Final
1409         field can be something different, for example foreign key pointing to
1410         that value. Final field is needed for example in some value
1411         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1412         key field for example).
1413         """
1414         joins = [alias]
1415         # First, generate the path for the names
1416         path, final_field, targets, rest = self.names_to_path(
1417             names, opts, allow_many, fail_on_missing=True)
1418 
1419         # Then, add the path to the query's joins. Note that we can't trim
1420         # joins at this stage - we will need the information about join type
1421         # of the trimmed joins.
1422         for join in path:
1423             opts = join.to_opts
1424             if join.direct:
1425                 nullable = self.is_nullable(join.join_field)
1426             else:
1427                 nullable = True
1428             connection = Join(opts.db_table, alias, None, INNER, join.join_field, nullable)
1429             reuse = can_reuse if join.m2m else None
1430             alias = self.join(connection, reuse=reuse)
1431             joins.append(alias)
1432         return final_field, targets, opts, joins, path
1433 
1434     def trim_joins(self, targets, joins, path):
1435         """
1436         The 'target' parameter is the final field being joined to, 'joins'
1437         is the full list of join aliases. The 'path' contain the PathInfos
1438         used to create the joins.
1439 
1440         Returns the final target field and table alias and the new active
1441         joins.
1442 
1443         We will always trim any direct join if we have the target column
1444         available already in the previous table. Reverse joins can't be
1445         trimmed as we don't know if there is anything on the other side of
1446         the join.
1447         """
1448         joins = joins[:]
1449         for pos, info in enumerate(reversed(path)):
1450             if len(joins) == 1 or not info.direct:
1451                 break
1452             join_targets = set(t.column for t in info.join_field.foreign_related_fields)
1453             cur_targets = set(t.column for t in targets)
1454             if not cur_targets.issubset(join_targets):
1455                 break
1456             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1457             targets = tuple(targets_dict[t.column] for t in targets)
1458             self.unref_alias(joins.pop())
1459         return targets, joins[-1], joins
1460 
1461     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1462         if not allow_joins and LOOKUP_SEP in name:
1463             raise FieldError("Joined field references are not permitted in this query")
1464         if name in self.annotations:
1465             if summarize:
1466                 # Summarize currently means we are doing an aggregate() query
1467                 # which is executed as a wrapped subquery if any of the
1468                 # aggregate() elements reference an existing annotation. In
1469                 # that case we need to return a Ref to the subquery's annotation.
1470                 return Ref(name, self.annotation_select[name])
1471             else:
1472                 return self.annotation_select[name]
1473         else:
1474             field_list = name.split(LOOKUP_SEP)
1475             field, sources, opts, join_list, path = self.setup_joins(
1476                 field_list, self.get_meta(),
1477                 self.get_initial_alias(), reuse)
1478             targets, _, join_list = self.trim_joins(sources, join_list, path)
1479             if len(targets) > 1:
1480                 raise FieldError("Referencing multicolumn fields with F() objects "
1481                                  "isn't supported")
1482             if reuse is not None:
1483                 reuse.update(join_list)
1484             col = targets[0].get_col(join_list[-1], sources[0])
1485             return col
1486 
1487     def split_exclude(self, filter_expr, prefix, can_reuse, names_with_path):
1488         """
1489         When doing an exclude against any kind of N-to-many relation, we need
1490         to use a subquery. This method constructs the nested query, given the
1491         original exclude filter (filter_expr) and the portion up to the first
1492         N-to-many relation field.
1493 
1494         As an example we could have original filter ~Q(child__name='foo').
1495         We would get here with filter_expr = child__name, prefix = child and
1496         can_reuse is a set of joins usable for filters in the original query.
1497 
1498         We will turn this into equivalent of:
1499             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1500                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1501 
1502         It might be worth it to consider using WHERE NOT EXISTS as that has
1503         saner null handling, and is easier for the backend's optimizer to
1504         handle.
1505         """
1506         # Generate the inner query.
1507         query = Query(self.model)
1508         query.add_filter(filter_expr)
1509         query.clear_ordering(True)
1510         # Try to have as simple as possible subquery -> trim leading joins from
1511         # the subquery.
1512         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1513 
1514         # Add extra check to make sure the selected field will not be null
1515         # since we are adding an IN <subquery> clause. This prevents the
1516         # database from tripping over IN (...,NULL,...) selects and returning
1517         # nothing
1518         col = query.select[0]
1519         select_field = col.target
1520         alias = col.alias
1521         if self.is_nullable(select_field):
1522             lookup_class = select_field.get_lookup('isnull')
1523             lookup = lookup_class(select_field.get_col(alias), False)
1524             query.where.add(lookup, AND)
1525         if alias in can_reuse:
1526             pk = select_field.model._meta.pk
1527             # Need to add a restriction so that outer query's filters are in effect for
1528             # the subquery, too.
1529             query.bump_prefix(self)
1530             lookup_class = select_field.get_lookup('exact')
1531             # Note that the query.select[0].alias is different from alias
1532             # due to bump_prefix above.
1533             lookup = lookup_class(pk.get_col(query.select[0].alias),
1534                                   pk.get_col(alias))
1535             query.where.add(lookup, AND)
1536             query.external_aliases.add(alias)
1537 
1538         condition, needed_inner = self.build_filter(
1539             ('%s__in' % trimmed_prefix, query),
1540             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1541         if contains_louter:
1542             or_null_condition, _ = self.build_filter(
1543                 ('%s__isnull' % trimmed_prefix, True),
1544                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1545             condition.add(or_null_condition, OR)
1546             # Note that the end result will be:
1547             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1548             # This might look crazy but due to how IN works, this seems to be
1549             # correct. If the IS NOT NULL check is removed then outercol NOT
1550             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1551             # outercol IS NULL we will not match the row.
1552         return condition, needed_inner
1553 
1554     def set_empty(self):
1555         self.where.add(NothingNode(), AND)
1556 
1557     def is_empty(self):
1558         return any(isinstance(c, NothingNode) for c in self.where.children)
1559 
1560     def set_limits(self, low=None, high=None):
1561         """
1562         Adjusts the limits on the rows retrieved. We use low/high to set these,
1563         as it makes it more Pythonic to read and write. When the SQL query is
1564         created, they are converted to the appropriate offset and limit values.
1565 
1566         Any limits passed in here are applied relative to the existing
1567         constraints. So low is added to the current low value and both will be
1568         clamped to any existing high value.
1569         """
1570         if high is not None:
1571             if self.high_mark is not None:
1572                 self.high_mark = min(self.high_mark, self.low_mark + high)
1573             else:
1574                 self.high_mark = self.low_mark + high
1575         if low is not None:
1576             if self.high_mark is not None:
1577                 self.low_mark = min(self.high_mark, self.low_mark + low)
1578             else:
1579                 self.low_mark = self.low_mark + low
1580 
1581         if self.low_mark == self.high_mark:
1582             self.set_empty()
1583 
1584     def clear_limits(self):
1585         """
1586         Clears any existing limits.
1587         """
1588         self.low_mark, self.high_mark = 0, None
1589 
1590     def can_filter(self):
1591         """
1592         Returns True if adding filters to this instance is still possible.
1593 
1594         Typically, this means no limits or offsets have been put on the results.
1595         """
1596         return not self.low_mark and self.high_mark is None
1597 
1598     def clear_select_clause(self):
1599         """
1600         Removes all fields from SELECT clause.
1601         """
1602         self.select = []
1603         self.default_cols = False
1604         self.select_related = False
1605         self.set_extra_mask(())
1606         self.set_annotation_mask(())
1607 
1608     def clear_select_fields(self):
1609         """
1610         Clears the list of fields to select (but not extra_select columns).
1611         Some queryset types completely replace any existing list of select
1612         columns.
1613         """
1614         self.select = []
1615         self.values_select = []
1616 
1617     def add_select(self, col):
1618         self.default_cols = False
1619         self.select.append(col)
1620 
1621     def set_select(self, cols):
1622         self.default_cols = False
1623         self.select = cols
1624 
1625     def add_distinct_fields(self, *field_names):
1626         """
1627         Adds and resolves the given fields to the query's "distinct on" clause.
1628         """
1629         self.distinct_fields = field_names
1630         self.distinct = True
1631 
1632     def add_fields(self, field_names, allow_m2m=True):
1633         """
1634         Adds the given (model) fields to the select set. The field names are
1635         added in the order specified.
1636         """
1637         alias = self.get_initial_alias()
1638         opts = self.get_meta()
1639 
1640         try:
1641             for name in field_names:
1642                 # Join promotion note - we must not remove any rows here, so
1643                 # if there is no existing joins, use outer join.
1644                 _, targets, _, joins, path = self.setup_joins(
1645                     name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1646                 targets, final_alias, joins = self.trim_joins(targets, joins, path)
1647                 for target in targets:
1648                     self.add_select(target.get_col(final_alias))
1649         except MultiJoin:
1650             raise FieldError("Invalid field name: '%s'" % name)
1651         except FieldError:
1652             if LOOKUP_SEP in name:
1653                 # For lookups spanning over relationships, show the error
1654                 # from the model on which the lookup failed.
1655                 raise
1656             else:
1657                 names = sorted(list(get_field_names_from_opts(opts)) + list(self.extra) + list(self.annotation_select))
1658                 raise FieldError("Cannot resolve keyword %r into field. "
1659                                  "Choices are: %s" % (name, ", ".join(names)))
1660 
1661     def add_ordering(self, *ordering):
1662         """
1663         Adds items from the 'ordering' sequence to the query's "order by"
1664         clause. These items are either field names (not column names) --
1665         possibly with a direction prefix ('-' or '?') -- or OrderBy
1666         expressions.
1667 
1668         If 'ordering' is empty, all ordering is cleared from the query.
1669         """
1670         errors = []
1671         for item in ordering:
1672             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
1673                 errors.append(item)
1674             if getattr(item, 'contains_aggregate', False):
1675                 raise FieldError(
1676                     'Using an aggregate in order_by() without also including '
1677                     'it in annotate() is not allowed: %s' % item
1678                 )
1679         if errors:
1680             raise FieldError('Invalid order_by arguments: %s' % errors)
1681         if ordering:
1682             self.order_by.extend(ordering)
1683         else:
1684             self.default_ordering = False
1685 
1686     def clear_ordering(self, force_empty):
1687         """
1688         Removes any ordering settings. If 'force_empty' is True, there will be
1689         no ordering in the resulting query (not even the model's default).
1690         """
1691         self.order_by = []
1692         self.extra_order_by = ()
1693         if force_empty:
1694             self.default_ordering = False
1695 
1696     def set_group_by(self):
1697         """
1698         Expands the GROUP BY clause required by the query.
1699 
1700         This will usually be the set of all non-aggregate fields in the
1701         return data. If the database backend supports grouping by the
1702         primary key, and the query would be equivalent, the optimization
1703         will be made automatically.
1704         """
1705         self.group_by = []
1706 
1707         for col in self.select:
1708             self.group_by.append(col)
1709 
1710         if self.annotation_select:
1711             for alias, annotation in six.iteritems(self.annotation_select):
1712                 for col in annotation.get_group_by_cols():
1713                     self.group_by.append(col)
1714 
1715     def add_select_related(self, fields):
1716         """
1717         Sets up the select_related data structure so that we only select
1718         certain related models (as opposed to all models, when
1719         self.select_related=True).
1720         """
1721         if isinstance(self.select_related, bool):
1722             field_dict = {}
1723         else:
1724             field_dict = self.select_related
1725         for field in fields:
1726             d = field_dict
1727             for part in field.split(LOOKUP_SEP):
1728                 d = d.setdefault(part, {})
1729         self.select_related = field_dict
1730 
1731     def add_extra(self, select, select_params, where, params, tables, order_by):
1732         """
1733         Adds data to the various extra_* attributes for user-created additions
1734         to the query.
1735         """
1736         if select:
1737             # We need to pair any placeholder markers in the 'select'
1738             # dictionary with their parameters in 'select_params' so that
1739             # subsequent updates to the select dictionary also adjust the
1740             # parameters appropriately.
1741             select_pairs = OrderedDict()
1742             if select_params:
1743                 param_iter = iter(select_params)
1744             else:
1745                 param_iter = iter([])
1746             for name, entry in select.items():
1747                 entry = force_text(entry)
1748                 entry_params = []
1749                 pos = entry.find("%s")
1750                 while pos != -1:
1751                     if pos == 0 or entry[pos - 1] != '%':
1752                         entry_params.append(next(param_iter))
1753                     pos = entry.find("%s", pos + 2)
1754                 select_pairs[name] = (entry, entry_params)
1755             # This is order preserving, since self.extra_select is an OrderedDict.
1756             self.extra.update(select_pairs)
1757         if where or params:
1758             self.where.add(ExtraWhere(where, params), AND)
1759         if tables:
1760             self.extra_tables += tuple(tables)
1761         if order_by:
1762             self.extra_order_by = order_by
1763 
1764     def clear_deferred_loading(self):
1765         """
1766         Remove any fields from the deferred loading set.
1767         """
1768         self.deferred_loading = (set(), True)
1769 
1770     def add_deferred_loading(self, field_names):
1771         """
1772         Add the given list of model field names to the set of fields to
1773         exclude from loading from the database when automatic column selection
1774         is done. The new field names are added to any existing field names that
1775         are deferred (or removed from any existing field names that are marked
1776         as the only ones for immediate loading).
1777         """
1778         # Fields on related models are stored in the literal double-underscore
1779         # format, so that we can use a set datastructure. We do the foo__bar
1780         # splitting and handling when computing the SQL column names (as part of
1781         # get_columns()).
1782         existing, defer = self.deferred_loading
1783         if defer:
1784             # Add to existing deferred names.
1785             self.deferred_loading = existing.union(field_names), True
1786         else:
1787             # Remove names from the set of any existing "immediate load" names.
1788             self.deferred_loading = existing.difference(field_names), False
1789 
1790     def add_immediate_loading(self, field_names):
1791         """
1792         Add the given list of model field names to the set of fields to
1793         retrieve when the SQL is executed ("immediate loading" fields). The
1794         field names replace any existing immediate loading field names. If
1795         there are field names already specified for deferred loading, those
1796         names are removed from the new field_names before storing the new names
1797         for immediate loading. (That is, immediate loading overrides any
1798         existing immediate values, but respects existing deferrals.)
1799         """
1800         existing, defer = self.deferred_loading
1801         field_names = set(field_names)
1802         if 'pk' in field_names:
1803             field_names.remove('pk')
1804             field_names.add(self.get_meta().pk.name)
1805 
1806         if defer:
1807             # Remove any existing deferred names from the current set before
1808             # setting the new names.
1809             self.deferred_loading = field_names.difference(existing), False
1810         else:
1811             # Replace any existing "immediate load" field names.
1812             self.deferred_loading = field_names, False
1813 
1814     def get_loaded_field_names(self):
1815         """
1816         If any fields are marked to be deferred, returns a dictionary mapping
1817         models to a set of names in those fields that will be loaded. If a
1818         model is not in the returned dictionary, none of its fields are
1819         deferred.
1820 
1821         If no fields are marked for deferral, returns an empty dictionary.
1822         """
1823         # We cache this because we call this function multiple times
1824         # (compiler.fill_related_selections, query.iterator)
1825         try:
1826             return self._loaded_field_names_cache
1827         except AttributeError:
1828             collection = {}
1829             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
1830             self._loaded_field_names_cache = collection
1831             return collection
1832 
1833     def get_loaded_field_names_cb(self, target, model, fields):
1834         """
1835         Callback used by get_deferred_field_names().
1836         """
1837         target[model] = {f.attname for f in fields}
1838 
1839     def set_annotation_mask(self, names):
1840         "Set the mask of annotations that will actually be returned by the SELECT"
1841         if names is None:
1842             self.annotation_select_mask = None
1843         else:
1844             self.annotation_select_mask = set(names)
1845         self._annotation_select_cache = None
1846 
1847     def append_annotation_mask(self, names):
1848         if self.annotation_select_mask is not None:
1849             self.set_annotation_mask(set(names).union(self.annotation_select_mask))
1850 
1851     def set_extra_mask(self, names):
1852         """
1853         Set the mask of extra select items that will be returned by SELECT,
1854         we don't actually remove them from the Query since they might be used
1855         later
1856         """
1857         if names is None:
1858             self.extra_select_mask = None
1859         else:
1860             self.extra_select_mask = set(names)
1861         self._extra_select_cache = None
1862 
1863     def set_values(self, fields):
1864         self.select_related = False
1865         self.clear_deferred_loading()
1866         self.clear_select_fields()
1867 
1868         if self.group_by is True:
1869             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
1870             self.set_group_by()
1871             self.clear_select_fields()
1872 
1873         if fields:
1874             field_names = []
1875             extra_names = []
1876             annotation_names = []
1877             if not self._extra and not self._annotations:
1878                 # Shortcut - if there are no extra or annotations, then
1879                 # the values() clause must be just field names.
1880                 field_names = list(fields)
1881             else:
1882                 self.default_cols = False
1883                 for f in fields:
1884                     if f in self.extra_select:
1885                         extra_names.append(f)
1886                     elif f in self.annotation_select:
1887                         annotation_names.append(f)
1888                     else:
1889                         field_names.append(f)
1890             self.set_extra_mask(extra_names)
1891             self.set_annotation_mask(annotation_names)
1892         else:
1893             field_names = [f.attname for f in self.model._meta.concrete_fields]
1894 
1895         self.values_select = field_names
1896         self.add_fields(field_names, True)
1897 
1898     @property
1899     def annotation_select(self):
1900         """The OrderedDict of aggregate columns that are not masked, and should
1901         be used in the SELECT clause.
1902 
1903         This result is cached for optimization purposes.
1904         """
1905         if self._annotation_select_cache is not None:
1906             return self._annotation_select_cache
1907         elif not self._annotations:
1908             return {}
1909         elif self.annotation_select_mask is not None:
1910             self._annotation_select_cache = OrderedDict(
1911                 (k, v) for k, v in self.annotations.items()
1912                 if k in self.annotation_select_mask
1913             )
1914             return self._annotation_select_cache
1915         else:
1916             return self.annotations
1917 
1918     @property
1919     def extra_select(self):
1920         if self._extra_select_cache is not None:
1921             return self._extra_select_cache
1922         if not self._extra:
1923             return {}
1924         elif self.extra_select_mask is not None:
1925             self._extra_select_cache = OrderedDict(
1926                 (k, v) for k, v in self.extra.items()
1927                 if k in self.extra_select_mask
1928             )
1929             return self._extra_select_cache
1930         else:
1931             return self.extra
1932 
1933     def trim_start(self, names_with_path):
1934         """
1935         Trims joins from the start of the join path. The candidates for trim
1936         are the PathInfos in names_with_path structure that are m2m joins.
1937 
1938         Also sets the select column so the start matches the join.
1939 
1940         This method is meant to be used for generating the subquery joins &
1941         cols in split_exclude().
1942 
1943         Returns a lookup usable for doing outerq.filter(lookup=self). Returns
1944         also if the joins in the prefix contain a LEFT OUTER join.
1945         _"""
1946         all_paths = []
1947         for _, paths in names_with_path:
1948             all_paths.extend(paths)
1949         contains_louter = False
1950         # Trim and operate only on tables that were generated for
1951         # the lookup part of the query. That is, avoid trimming
1952         # joins generated for F() expressions.
1953         lookup_tables = [t for t in self.tables if t in self._lookup_joins or t == self.tables[0]]
1954         for trimmed_paths, path in enumerate(all_paths):
1955             if path.m2m:
1956                 break
1957             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
1958                 contains_louter = True
1959             alias = lookup_tables[trimmed_paths]
1960             self.unref_alias(alias)
1961         # The path.join_field is a Rel, lets get the other side's field
1962         join_field = path.join_field.field
1963         # Build the filter prefix.
1964         paths_in_prefix = trimmed_paths
1965         trimmed_prefix = []
1966         for name, path in names_with_path:
1967             if paths_in_prefix - len(path) < 0:
1968                 break
1969             trimmed_prefix.append(name)
1970             paths_in_prefix -= len(path)
1971         trimmed_prefix.append(
1972             join_field.foreign_related_fields[0].name)
1973         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
1974         # Lets still see if we can trim the first join from the inner query
1975         # (that is, self). We can't do this for LEFT JOINs because we would
1976         # miss those rows that have nothing on the outer side.
1977         if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type != LOUTER:
1978             select_fields = [r[0] for r in join_field.related_fields]
1979             select_alias = lookup_tables[trimmed_paths + 1]
1980             self.unref_alias(lookup_tables[trimmed_paths])
1981             extra_restriction = join_field.get_extra_restriction(
1982                 self.where_class, None, lookup_tables[trimmed_paths + 1])
1983             if extra_restriction:
1984                 self.where.add(extra_restriction, AND)
1985         else:
1986             # TODO: It might be possible to trim more joins from the start of the
1987             # inner query if it happens to have a longer join chain containing the
1988             # values in select_fields. Lets punt this one for now.
1989             select_fields = [r[1] for r in join_field.related_fields]
1990             select_alias = lookup_tables[trimmed_paths]
1991         # The found starting point is likely a Join instead of a BaseTable reference.
1992         # But the first entry in the query's FROM clause must not be a JOIN.
1993         for table in self.tables:
1994             if self.alias_refcount[table] > 0:
1995                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
1996                 break
1997         self.set_select([f.get_col(select_alias) for f in select_fields])
1998         return trimmed_prefix, contains_louter
1999 
2000     def is_nullable(self, field):
2001         """
2002         A helper to check if the given field should be treated as nullable.
2003 
2004         Some backends treat '' as null and Django treats such fields as
2005         nullable for those backends. In such situations field.null can be
2006         False even if we should treat the field as nullable.
2007         """
2008         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2009         # (nor should it have) knowledge of which connection is going to be
2010         # used. The proper fix would be to defer all decisions where
2011         # is_nullable() is needed to the compiler stage, but that is not easy
2012         # to do currently.
2013         if connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and field.empty_strings_allowed:
2014             return True
2015         else:
2016             return field.null
2017 
2018     def as_subquery_filter(self, db):
2019         self._db = db
2020         self.subquery = True
2021         # It's safe to drop ordering if the queryset isn't using slicing,
2022         # distinct(*fields) or select_for_update().
2023         if (self.low_mark == 0 and self.high_mark is None and
2024                 not self.distinct_fields and
2025                 not self.select_for_update):
2026             self.clear_ordering(True)
2027         return self

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>change_aliases</method>
<code>
800     def change_aliases(self, change_map):
801         """
802         Changes the aliases in change_map (which maps old-alias -> new-alias),
803         relabelling any references to them in select columns and the where
804         clause.
805         """
806         assert set(change_map.keys()).intersection(set(change_map.values())) == set()
807 
808         # 1. Update references in "select" (normal columns plus aliases),
809         # "group by" and "where".
810         self.where.relabel_aliases(change_map)
811         if isinstance(self.group_by, list):
812             self.group_by = [col.relabeled_clone(change_map) for col in self.group_by]
813         self.select = [col.relabeled_clone(change_map) for col in self.select]
814         if self._annotations:
815             self._annotations = OrderedDict(
816                 (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items())
817 
818         # 2. Rename the alias in the internal table/alias datastructures.
819         for old_alias, new_alias in six.iteritems(change_map):
820             if old_alias not in self.alias_map:
821                 continue
822             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
823             self.alias_map[new_alias] = alias_data
824             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
825             del self.alias_refcount[old_alias]
826             del self.alias_map[old_alias]
827 
828             table_aliases = self.table_map[alias_data.table_name]
829             for pos, alias in enumerate(table_aliases):
830                 if alias == old_alias:
831                     table_aliases[pos] = new_alias
832                     break
833         self.external_aliases = {change_map.get(alias, alias)
834                                  for alias in self.external_aliases}

</code><intended_behavior>In the `change_aliases` method, the critical assertion that checks for circular references in `change_map` should be preserved, but the analysis and manipulation of `change_map` may need enhancement. Specifically, the method should implement a more robust mechanism for managing alias changes that inherently avoids the possibility of creating circular references, regardless of the complexity of the query structures or the interactions of combined queries. This might require reevaluating the logic that leads to alias modifications, ensuring that alias changes do not introduce logical inconsistencies or dependencies that break this assertion.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
117 class Query(object):
118     """
119     A single SQL query.
120     """
121 
122     alias_prefix = 'T'
123     subq_aliases = frozenset([alias_prefix])
124     query_terms = QUERY_TERMS
125 
126     compiler = 'SQLCompiler'
127 
128     def __init__(self, model, where=WhereNode):
129         self.model = model
130         self.alias_refcount = {}
131         # alias_map is the most important data structure regarding joins.
132         # It's used for recording which joins exist in the query and what
133         # types they are. The key is the alias of the joined table (possibly
134         # the table name) and the value is a Join-like object (see
135         # sql.datastructures.Join for more information).
136         self.alias_map = {}
137         # Sometimes the query contains references to aliases in outer queries (as
138         # a result of split_exclude). Correct alias quoting needs to know these
139         # aliases too.
140         self.external_aliases = set()
141         self.table_map = {}     # Maps table names to list of aliases.
142         self.default_cols = True
143         self.default_ordering = True
144         self.standard_ordering = True
145         self.used_aliases = set()
146         self.filter_is_sticky = False
147         self.subquery = False
148 
149         # SQL-related attributes
150         # Select and related select clauses are expressions to use in the
151         # SELECT clause of the query.
152         # The select is used for cases where we want to set up the select
153         # clause to contain other than default fields (values(), subqueries...)
154         # Note that annotations go to annotations dictionary.
155         self.select = []
156         self.tables = []    # Aliases in the order they are created.
157         self.where = where()
158         self.where_class = where
159         # The group_by attribute can have one of the following forms:
160         #  - None: no group by at all in the query
161         #  - A list of expressions: group by (at least) those expressions.
162         #    String refs are also allowed for now.
163         #  - True: group by all select fields of the model
164         # See compiler.get_group_by() for details.
165         self.group_by = None
166         self.order_by = []
167         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
168         self.distinct = False
169         self.distinct_fields = []
170         self.select_for_update = False
171         self.select_for_update_nowait = False
172         self.select_for_update_skip_locked = False
173 
174         self.select_related = False
175         # Arbitrary limit for select_related to prevents infinite recursion.
176         self.max_depth = 5
177 
178         # Holds the selects defined by a call to values() or values_list()
179         # excluding annotation_select and extra_select.
180         self.values_select = []
181 
182         # SQL annotation-related attributes
183         # The _annotations will be an OrderedDict when used. Due to the cost
184         # of creating OrderedDict this attribute is created lazily (in
185         # self.annotations property).
186         self._annotations = None  # Maps alias -> Annotation Expression
187         self.annotation_select_mask = None
188         self._annotation_select_cache = None
189 
190         # Set combination attributes
191         self.combinator = None
192         self.combinator_all = False
193         self.combined_queries = ()
194 
195         # These are for extensions. The contents are more or less appended
196         # verbatim to the appropriate clause.
197         # The _extra attribute is an OrderedDict, lazily created similarly to
198         # .annotations
199         self._extra = None  # Maps col_alias -> (col_sql, params).
200         self.extra_select_mask = None
201         self._extra_select_cache = None
202 
203         self.extra_tables = ()
204         self.extra_order_by = ()
205 
206         # A tuple that is a set of model field names and either True, if these
207         # are the fields to defer, or False if these are the only fields to
208         # load.
209         self.deferred_loading = (set(), True)
210 
211         self.context = {}
212 
213     @property
214     def extra(self):
215         if self._extra is None:
216             self._extra = OrderedDict()
217         return self._extra
218 
219     @property
220     def annotations(self):
221         if self._annotations is None:
222             self._annotations = OrderedDict()
223         return self._annotations
224 
225     def __str__(self):
226         """
227         Returns the query as a string of SQL with the parameter values
228         substituted in (use sql_with_params() to see the unsubstituted string).
229 
230         Parameter values won't necessarily be quoted correctly, since that is
231         done by the database interface at execution time.
232         """
233         sql, params = self.sql_with_params()
234         return sql % params
235 
236     def sql_with_params(self):
237         """
238         Returns the query as an SQL string and the parameters that will be
239         substituted into the query.
240         """
241         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
242 
243     def __deepcopy__(self, memo):
244         result = self.clone(memo=memo)
245         memo[id(self)] = result
246         return result
247 
248     def _prepare(self, field):
249         return self
250 
251     def get_compiler(self, using=None, connection=None):
252         if using is None and connection is None:
253             raise ValueError("Need either using or connection")
254         if using:
255             connection = connections[using]
256         return connection.ops.compiler(self.compiler)(self, connection, using)
257 
258     def get_meta(self):
259         """
260         Returns the Options instance (the model._meta) from which to start
261         processing. Normally, this is self.model._meta, but it can be changed
262         by subclasses.
263         """
264         return self.model._meta
265 
266     def clone(self, klass=None, memo=None, **kwargs):
267         """
268         Creates a copy of the current instance. The 'kwargs' parameter can be
269         used by clients to update attributes after copying has taken place.
270         """
271         obj = Empty()
272         obj.__class__ = klass or self.__class__
273         obj.model = self.model
274         obj.alias_refcount = self.alias_refcount.copy()
275         obj.alias_map = self.alias_map.copy()
276         obj.external_aliases = self.external_aliases.copy()
277         obj.table_map = self.table_map.copy()
278         obj.default_cols = self.default_cols
279         obj.default_ordering = self.default_ordering
280         obj.standard_ordering = self.standard_ordering
281         obj.select = self.select[:]
282         obj.tables = self.tables[:]
283         obj.where = self.where.clone()
284         obj.where_class = self.where_class
285         if self.group_by is None:
286             obj.group_by = None
287         elif self.group_by is True:
288             obj.group_by = True
289         else:
290             obj.group_by = self.group_by[:]
291         obj.order_by = self.order_by[:]
292         obj.low_mark, obj.high_mark = self.low_mark, self.high_mark
293         obj.distinct = self.distinct
294         obj.distinct_fields = self.distinct_fields[:]
295         obj.select_for_update = self.select_for_update
296         obj.select_for_update_nowait = self.select_for_update_nowait
297         obj.select_for_update_skip_locked = self.select_for_update_skip_locked
298         obj.select_related = self.select_related
299         obj.values_select = self.values_select[:]
300         obj._annotations = self._annotations.copy() if self._annotations is not None else None
301         if self.annotation_select_mask is None:
302             obj.annotation_select_mask = None
303         else:
304             obj.annotation_select_mask = self.annotation_select_mask.copy()
305         # _annotation_select_cache cannot be copied, as doing so breaks the
306         # (necessary) state in which both annotations and
307         # _annotation_select_cache point to the same underlying objects.
308         # It will get re-populated in the cloned queryset the next time it's
309         # used.
310         obj._annotation_select_cache = None
311         obj.max_depth = self.max_depth
312         obj.combinator = self.combinator
313         obj.combinator_all = self.combinator_all
314         obj.combined_queries = self.combined_queries
315         obj._extra = self._extra.copy() if self._extra is not None else None
316         if self.extra_select_mask is None:
317             obj.extra_select_mask = None
318         else:
319             obj.extra_select_mask = self.extra_select_mask.copy()
320         if self._extra_select_cache is None:
321             obj._extra_select_cache = None
322         else:
323             obj._extra_select_cache = self._extra_select_cache.copy()
324         obj.extra_tables = self.extra_tables
325         obj.extra_order_by = self.extra_order_by
326         obj.deferred_loading = copy.copy(self.deferred_loading[0]), self.deferred_loading[1]
327         if self.filter_is_sticky and self.used_aliases:
328             obj.used_aliases = self.used_aliases.copy()
329         else:
330             obj.used_aliases = set()
331         obj.filter_is_sticky = False
332         obj.subquery = self.subquery
333         if 'alias_prefix' in self.__dict__:
334             obj.alias_prefix = self.alias_prefix
335         if 'subq_aliases' in self.__dict__:
336             obj.subq_aliases = self.subq_aliases.copy()
337 
338         obj.__dict__.update(kwargs)
339         if hasattr(obj, '_setup_query'):
340             obj._setup_query()
341         obj.context = self.context.copy()
342         obj._forced_pk = getattr(self, '_forced_pk', False)
343         return obj
344 
345     def add_context(self, key, value):
346         self.context[key] = value
347 
348     def get_context(self, key, default=None):
349         return self.context.get(key, default)
350 
351     def relabeled_clone(self, change_map):
352         clone = self.clone()
353         clone.change_aliases(change_map)
354         return clone
355 
356     def rewrite_cols(self, annotation, col_cnt):
357         # We must make sure the inner query has the referred columns in it.
358         # If we are aggregating over an annotation, then Django uses Ref()
359         # instances to note this. However, if we are annotating over a column
360         # of a related model, then it might be that column isn't part of the
361         # SELECT clause of the inner query, and we must manually make sure
362         # the column is selected. An example case is:
363         #    .aggregate(Sum('author__awards'))
364         # Resolving this expression results in a join to author, but there
365         # is no guarantee the awards column of author is in the select clause
366         # of the query. Thus we must manually add the column to the inner
367         # query.
368         orig_exprs = annotation.get_source_expressions()
369         new_exprs = []
370         for expr in orig_exprs:
371             # FIXME: These conditions are fairly arbitrary. Identify a better
372             # method of having expressions decide which code path they should
373             # take.
374             if isinstance(expr, Ref):
375                 # Its already a Ref to subquery (see resolve_ref() for
376                 # details)
377                 new_exprs.append(expr)
378             elif isinstance(expr, (WhereNode, Lookup)):
379                 # Decompose the subexpressions further. The code here is
380                 # copied from the else clause, but this condition must appear
381                 # before the contains_aggregate/is_summary condition below.
382                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
383                 new_exprs.append(new_expr)
384             elif isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
385                 # Reference to column. Make sure the referenced column
386                 # is selected.
387                 col_cnt += 1
388                 col_alias = '__col%d' % col_cnt
389                 self.annotations[col_alias] = expr
390                 self.append_annotation_mask([col_alias])
391                 new_exprs.append(Ref(col_alias, expr))
392             else:
393                 # Some other expression not referencing database values
394                 # directly. Its subexpression might contain Cols.
395                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
396                 new_exprs.append(new_expr)
397         annotation.set_source_expressions(new_exprs)
398         return annotation, col_cnt
399 
400     def get_aggregation(self, using, added_aggregate_names):
401         """
402         Returns the dictionary with the values of the existing aggregations.
403         """
404         if not self.annotation_select:
405             return {}
406         has_limit = self.low_mark != 0 or self.high_mark is not None
407         has_existing_annotations = any(
408             annotation for alias, annotation
409             in self.annotations.items()
410             if alias not in added_aggregate_names
411         )
412         # Decide if we need to use a subquery.
413         #
414         # Existing annotations would cause incorrect results as get_aggregation()
415         # must produce just one result and thus must not use GROUP BY. But we
416         # aren't smart enough to remove the existing annotations from the
417         # query, so those would force us to use GROUP BY.
418         #
419         # If the query has limit or distinct, or uses set operations, then
420         # those operations must be done in a subquery so that the query
421         # aggregates on the limit and/or distinct results instead of applying
422         # the distinct and limit after the aggregation.
423         if (isinstance(self.group_by, list) or has_limit or has_existing_annotations or
424                 self.distinct or self.combinator):
425             from django.db.models.sql.subqueries import AggregateQuery
426             outer_query = AggregateQuery(self.model)
427             inner_query = self.clone()
428             inner_query.select_for_update = False
429             inner_query.select_related = False
430             if not has_limit and not self.distinct_fields:
431                 # Queries with distinct_fields need ordering and when a limit
432                 # is applied we must take the slice from the ordered query.
433                 # Otherwise no need for ordering.
434                 inner_query.clear_ordering(True)
435             if not inner_query.distinct:
436                 # If the inner query uses default select and it has some
437                 # aggregate annotations, then we must make sure the inner
438                 # query is grouped by the main model's primary key. However,
439                 # clearing the select clause can alter results if distinct is
440                 # used.
441                 if inner_query.default_cols and has_existing_annotations:
442                     inner_query.group_by = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
443                 inner_query.default_cols = False
444 
445             relabels = {t: 'subquery' for t in inner_query.tables}
446             relabels[None] = 'subquery'
447             # Remove any aggregates marked for reduction from the subquery
448             # and move them to the outer AggregateQuery.
449             col_cnt = 0
450             for alias, expression in list(inner_query.annotation_select.items()):
451                 if expression.is_summary:
452                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
453                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
454                     del inner_query.annotations[alias]
455                 # Make sure the annotation_select wont use cached results.
456                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
457             if inner_query.select == [] and not inner_query.default_cols and not inner_query.annotation_select_mask:
458                 # In case of Model.objects[0:3].count(), there would be no
459                 # field selected in the inner query, yet we must use a subquery.
460                 # So, make sure at least one field is selected.
461                 inner_query.select = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
462             try:
463                 outer_query.add_subquery(inner_query, using)
464             except EmptyResultSet:
465                 return {
466                     alias: None
467                     for alias in outer_query.annotation_select
468                 }
469         else:
470             outer_query = self
471             self.select = []
472             self.default_cols = False
473             self._extra = {}
474 
475         outer_query.clear_ordering(True)
476         outer_query.clear_limits()
477         outer_query.select_for_update = False
478         outer_query.select_related = False
479         compiler = outer_query.get_compiler(using)
480         result = compiler.execute_sql(SINGLE)
481         if result is None:
482             result = [None for q in outer_query.annotation_select.items()]
483 
484         converters = compiler.get_converters(outer_query.annotation_select.values())
485         result = compiler.apply_converters(result, converters)
486 
487         return {
488             alias: val
489             for (alias, annotation), val
490             in zip(outer_query.annotation_select.items(), result)
491         }
492 
493     def get_count(self, using):
494         """
495         Performs a COUNT() query using the current filter constraints.
496         """
497         obj = self.clone()
498         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
499         number = obj.get_aggregation(using, ['__count'])['__count']
500         if number is None:
501             number = 0
502         return number
503 
504     def has_filters(self):
505         return self.where
506 
507     def has_results(self, using):
508         q = self.clone()
509         if not q.distinct:
510             if q.group_by is True:
511                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
512                 q.set_group_by()
513             q.clear_select_clause()
514         q.clear_ordering(True)
515         q.set_limits(high=1)
516         compiler = q.get_compiler(using=using)
517         return compiler.has_results()
518 
519     def combine(self, rhs, connector):
520         """
521         Merge the 'rhs' query into the current one (with any 'rhs' effects
522         being applied *after* (that is, "to the right of") anything in the
523         current query. 'rhs' is not modified during a call to this function.
524 
525         The 'connector' parameter describes how to connect filters from the
526         'rhs' query.
527         """
528         assert self.model == rhs.model, \
529             "Cannot combine queries on two different base models."
530         assert self.can_filter(), \
531             "Cannot combine queries once a slice has been taken."
532         assert self.distinct == rhs.distinct, \
533             "Cannot combine a unique query with a non-unique query."
534         assert self.distinct_fields == rhs.distinct_fields, \
535             "Cannot combine queries with different distinct fields."
536 
537         # Work out how to relabel the rhs aliases, if necessary.
538         change_map = {}
539         conjunction = (connector == AND)
540 
541         # Determine which existing joins can be reused. When combining the
542         # query with AND we must recreate all joins for m2m filters. When
543         # combining with OR we can reuse joins. The reason is that in AND
544         # case a single row can't fulfill a condition like:
545         #     revrel__col=1 & revrel__col=2
546         # But, there might be two different related rows matching this
547         # condition. In OR case a single True is enough, so single row is
548         # enough, too.
549         #
550         # Note that we will be creating duplicate joins for non-m2m joins in
551         # the AND case. The results will be correct but this creates too many
552         # joins. This is something that could be fixed later on.
553         reuse = set() if conjunction else set(self.tables)
554         # Base table must be present in the query - this is the same
555         # table on both sides.
556         self.get_initial_alias()
557         joinpromoter = JoinPromoter(connector, 2, False)
558         joinpromoter.add_votes(
559             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
560         rhs_votes = set()
561         # Now, add the joins from rhs query into the new query (skipping base
562         # table).
563         for alias in rhs.tables[1:]:
564             join = rhs.alias_map[alias]
565             # If the left side of the join was already relabeled, use the
566             # updated alias.
567             join = join.relabeled_clone(change_map)
568             new_alias = self.join(join, reuse=reuse)
569             if join.join_type == INNER:
570                 rhs_votes.add(new_alias)
571             # We can't reuse the same join again in the query. If we have two
572             # distinct joins for the same connection in rhs query, then the
573             # combined query must have two joins, too.
574             reuse.discard(new_alias)
575             if alias != new_alias:
576                 change_map[alias] = new_alias
577             if not rhs.alias_refcount[alias]:
578                 # The alias was unused in the rhs query. Unref it so that it
579                 # will be unused in the new query, too. We have to add and
580                 # unref the alias so that join promotion has information of
581                 # the join type for the unused alias.
582                 self.unref_alias(new_alias)
583         joinpromoter.add_votes(rhs_votes)
584         joinpromoter.update_join_types(self)
585 
586         # Now relabel a copy of the rhs where-clause and add it to the current
587         # one.
588         w = rhs.where.clone()
589         w.relabel_aliases(change_map)
590         self.where.add(w, connector)
591 
592         # Selection columns and extra extensions are those provided by 'rhs'.
593         self.select = []
594         for col in rhs.select:
595             self.add_select(col.relabeled_clone(change_map))
596 
597         if connector == OR:
598             # It would be nice to be able to handle this, but the queries don't
599             # really make sense (or return consistent value sets). Not worth
600             # the extra complexity when you can write a real query instead.
601             if self._extra and rhs._extra:
602                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
603         self.extra.update(rhs.extra)
604         extra_select_mask = set()
605         if self.extra_select_mask is not None:
606             extra_select_mask.update(self.extra_select_mask)
607         if rhs.extra_select_mask is not None:
608             extra_select_mask.update(rhs.extra_select_mask)
609         if extra_select_mask:
610             self.set_extra_mask(extra_select_mask)
611         self.extra_tables += rhs.extra_tables
612 
613         # Ordering uses the 'rhs' ordering, unless it has none, in which case
614         # the current ordering is used.
615         self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by
616         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
617 
618     def deferred_to_data(self, target, callback):
619         """
620         Converts the self.deferred_loading data structure to an alternate data
621         structure, describing the field that *will* be loaded. This is used to
622         compute the columns to select from the database and also by the
623         QuerySet class to work out which fields are being initialized on each
624         model. Models that have all their fields included aren't mentioned in
625         the result, only those that have field restrictions in place.
626 
627         The "target" parameter is the instance that is populated (in place).
628         The "callback" is a function that is called whenever a (model, field)
629         pair need to be added to "target". It accepts three parameters:
630         "target", and the model and list of fields being added for that model.
631         """
632         field_names, defer = self.deferred_loading
633         if not field_names:
634             return
635         orig_opts = self.get_meta()
636         seen = {}
637         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
638         for field_name in field_names:
639             parts = field_name.split(LOOKUP_SEP)
640             cur_model = self.model._meta.concrete_model
641             opts = orig_opts
642             for name in parts[:-1]:
643                 old_model = cur_model
644                 source = opts.get_field(name)
645                 if is_reverse_o2o(source):
646                     cur_model = source.related_model
647                 else:
648                     cur_model = source.remote_field.model
649                 opts = cur_model._meta
650                 # Even if we're "just passing through" this model, we must add
651                 # both the current model's pk and the related reference field
652                 # (if it's not a reverse relation) to the things we select.
653                 if not is_reverse_o2o(source):
654                     must_include[old_model].add(source)
655                 add_to_dict(must_include, cur_model, opts.pk)
656             field = opts.get_field(parts[-1])
657             is_reverse_object = field.auto_created and not field.concrete
658             model = field.related_model if is_reverse_object else field.model
659             model = model._meta.concrete_model
660             if model == opts.model:
661                 model = cur_model
662             if not is_reverse_o2o(field):
663                 add_to_dict(seen, model, field)
664 
665         if defer:
666             # We need to load all fields for each model, except those that
667             # appear in "seen" (for all models that appear in "seen"). The only
668             # slight complexity here is handling fields that exist on parent
669             # models.
670             workset = {}
671             for model, values in six.iteritems(seen):
672                 for field in model._meta.fields:
673                     if field in values:
674                         continue
675                     m = field.model._meta.concrete_model
676                     add_to_dict(workset, m, field)
677             for model, values in six.iteritems(must_include):
678                 # If we haven't included a model in workset, we don't add the
679                 # corresponding must_include fields for that model, since an
680                 # empty set means "include all fields". That's why there's no
681                 # "else" branch here.
682                 if model in workset:
683                     workset[model].update(values)
684             for model, values in six.iteritems(workset):
685                 callback(target, model, values)
686         else:
687             for model, values in six.iteritems(must_include):
688                 if model in seen:
689                     seen[model].update(values)
690                 else:
691                     # As we've passed through this model, but not explicitly
692                     # included any fields, we have to make sure it's mentioned
693                     # so that only the "must include" fields are pulled in.
694                     seen[model] = values
695             # Now ensure that every model in the inheritance chain is mentioned
696             # in the parent list. Again, it must be mentioned to ensure that
697             # only "must include" fields are pulled in.
698             for model in orig_opts.get_parent_list():
699                 if model not in seen:
700                     seen[model] = set()
701             for model, values in six.iteritems(seen):
702                 callback(target, model, values)
703 
704     def table_alias(self, table_name, create=False):
705         """
706         Returns a table alias for the given table_name and whether this is a
707         new alias or not.
708 
709         If 'create' is true, a new alias is always created. Otherwise, the
710         most recently created alias for the table (if one exists) is reused.
711         """
712         alias_list = self.table_map.get(table_name)
713         if not create and alias_list:
714             alias = alias_list[0]
715             self.alias_refcount[alias] += 1
716             return alias, False
717 
718         # Create a new alias for this table.
719         if alias_list:
720             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
721             alias_list.append(alias)
722         else:
723             # The first occurrence of a table uses the table name directly.
724             alias = table_name
725             self.table_map[alias] = [alias]
726         self.alias_refcount[alias] = 1
727         self.tables.append(alias)
728         return alias, True
729 
730     def ref_alias(self, alias):
731         """ Increases the reference count for this alias. """
732         self.alias_refcount[alias] += 1
733 
734     def unref_alias(self, alias, amount=1):
735         """ Decreases the reference count for this alias. """
736         self.alias_refcount[alias] -= amount
737 
738     def promote_joins(self, aliases):
739         """
740         Promotes recursively the join type of given aliases and its children to
741         an outer join. If 'unconditional' is False, the join is only promoted if
742         it is nullable or the parent join is an outer join.
743 
744         The children promotion is done to avoid join chains that contain a LOUTER
745         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
746         then we must also promote b->c automatically, or otherwise the promotion
747         of a->b doesn't actually change anything in the query results.
748         """
749         aliases = list(aliases)
750         while aliases:
751             alias = aliases.pop(0)
752             if self.alias_map[alias].join_type is None:
753                 # This is the base table (first FROM entry) - this table
754                 # isn't really joined at all in the query, so we should not
755                 # alter its join type.
756                 continue
757             # Only the first alias (skipped above) should have None join_type
758             assert self.alias_map[alias].join_type is not None
759             parent_alias = self.alias_map[alias].parent_alias
760             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
761             already_louter = self.alias_map[alias].join_type == LOUTER
762             if ((self.alias_map[alias].nullable or parent_louter) and
763                     not already_louter):
764                 self.alias_map[alias] = self.alias_map[alias].promote()
765                 # Join type of 'alias' changed, so re-examine all aliases that
766                 # refer to this one.
767                 aliases.extend(
768                     join for join in self.alias_map.keys()
769                     if self.alias_map[join].parent_alias == alias and join not in aliases
770                 )
771 
772     def demote_joins(self, aliases):
773         """
774         Change join type from LOUTER to INNER for all joins in aliases.
775 
776         Similarly to promote_joins(), this method must ensure no join chains
777         containing first an outer, then an inner join are generated. If we
778         are demoting b->c join in chain a LOUTER b LOUTER c then we must
779         demote a->b automatically, or otherwise the demotion of b->c doesn't
780         actually change anything in the query results. .
781         """
782         aliases = list(aliases)
783         while aliases:
784             alias = aliases.pop(0)
785             if self.alias_map[alias].join_type == LOUTER:
786                 self.alias_map[alias] = self.alias_map[alias].demote()
787                 parent_alias = self.alias_map[alias].parent_alias
788                 if self.alias_map[parent_alias].join_type == INNER:
789                     aliases.append(parent_alias)
790 
791     def reset_refcounts(self, to_counts):
792         """
793         This method will reset reference counts for aliases so that they match
794         the value passed in :param to_counts:.
795         """
796         for alias, cur_refcount in self.alias_refcount.copy().items():
797             unref_amount = cur_refcount - to_counts.get(alias, 0)
798             self.unref_alias(alias, unref_amount)
799 
800     def change_aliases(self, change_map):
801         """
802         Changes the aliases in change_map (which maps old-alias -> new-alias),
803         relabelling any references to them in select columns and the where
804         clause.
805         """
806         assert set(change_map.keys()).intersection(set(change_map.values())) == set()
807 
808         # 1. Update references in "select" (normal columns plus aliases),
809         # "group by" and "where".
810         self.where.relabel_aliases(change_map)
811         if isinstance(self.group_by, list):
812             self.group_by = [col.relabeled_clone(change_map) for col in self.group_by]
813         self.select = [col.relabeled_clone(change_map) for col in self.select]
814         if self._annotations:
815             self._annotations = OrderedDict(
816                 (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items())
817 
818         # 2. Rename the alias in the internal table/alias datastructures.
819         for old_alias, new_alias in six.iteritems(change_map):
820             if old_alias not in self.alias_map:
821                 continue
822             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
823             self.alias_map[new_alias] = alias_data
824             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
825             del self.alias_refcount[old_alias]
826             del self.alias_map[old_alias]
827 
828             table_aliases = self.table_map[alias_data.table_name]
829             for pos, alias in enumerate(table_aliases):
830                 if alias == old_alias:
831                     table_aliases[pos] = new_alias
832                     break
833         self.external_aliases = {change_map.get(alias, alias)
834                                  for alias in self.external_aliases}
835 
836     def bump_prefix(self, outer_query):
837         """
838         Changes the alias prefix to the next letter in the alphabet in a way
839         that the outer query's aliases and this query's aliases will not
840         conflict. Even tables that previously had no alias will get an alias
841         after this call.
842         """
843         def prefix_gen():
844             """
845             Generates a sequence of characters in alphabetical order:
846                 -> 'A', 'B', 'C', ...
847 
848             When the alphabet is finished, the sequence will continue with the
849             Cartesian product:
850                 -> 'AA', 'AB', 'AC', ...
851             """
852             alphabet = ascii_uppercase
853             prefix = chr(ord(self.alias_prefix) + 1)
854             yield prefix
855             for n in count(1):
856                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
857                 for s in product(seq, repeat=n):
858                     yield ''.join(s)
859                 prefix = None
860 
861         if self.alias_prefix != outer_query.alias_prefix:
862             # No clashes between self and outer query should be possible.
863             return
864 
865         local_recursion_limit = 127  # explicitly avoid infinite loop
866         for pos, prefix in enumerate(prefix_gen()):
867             if prefix not in self.subq_aliases:
868                 self.alias_prefix = prefix
869                 break
870             if pos > local_recursion_limit:
871                 raise RuntimeError(
872                     'Maximum recursion depth exceeded: too many subqueries.'
873                 )
874         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
875         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
876         change_map = OrderedDict()
877         for pos, alias in enumerate(self.tables):
878             new_alias = '%s%d' % (self.alias_prefix, pos)
879             change_map[alias] = new_alias
880             self.tables[pos] = new_alias
881         self.change_aliases(change_map)
882 
883     def get_initial_alias(self):
884         """
885         Returns the first alias for this query, after increasing its reference
886         count.
887         """
888         if self.tables:
889             alias = self.tables[0]
890             self.ref_alias(alias)
891         else:
892             alias = self.join(BaseTable(self.get_meta().db_table, None))
893         return alias
894 
895     def count_active_tables(self):
896         """
897         Returns the number of tables in this query with a non-zero reference
898         count. Note that after execution, the reference counts are zeroed, so
899         tables added in compiler will not be seen by this method.
900         """
901         return len([1 for count in self.alias_refcount.values() if count])
902 
903     def join(self, join, reuse=None):
904         """
905         Return an alias for the 'join', either reusing an existing alias for
906         that join or creating a new one. 'join' is either a
907         sql.datastructures.BaseTable or Join.
908 
909         The 'reuse' parameter can be either None which means all joins are
910         reusable, or it can be a set containing the aliases that can be reused.
911 
912         A join is always created as LOUTER if the lhs alias is LOUTER to make
913         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
914         joins are created as LOUTER if the join is nullable.
915         """
916         reuse = [a for a, j in self.alias_map.items()
917                  if (reuse is None or a in reuse) and j == join]
918         if reuse:
919             self.ref_alias(reuse[0])
920             return reuse[0]
921 
922         # No reuse is possible, so we need a new alias.
923         alias, _ = self.table_alias(join.table_name, create=True)
924         if join.join_type:
925             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
926                 join_type = LOUTER
927             else:
928                 join_type = INNER
929             join.join_type = join_type
930         join.table_alias = alias
931         self.alias_map[alias] = join
932         return alias
933 
934     def join_parent_model(self, opts, model, alias, seen):
935         """
936         Makes sure the given 'model' is joined in the query. If 'model' isn't
937         a parent of 'opts' or if it is None this method is a no-op.
938 
939         The 'alias' is the root alias for starting the join, 'seen' is a dict
940         of model -> alias of existing joins. It must also contain a mapping
941         of None -> some alias. This will be returned in the no-op case.
942         """
943         if model in seen:
944             return seen[model]
945         chain = opts.get_base_chain(model)
946         if not chain:
947             return alias
948         curr_opts = opts
949         for int_model in chain:
950             if int_model in seen:
951                 curr_opts = int_model._meta
952                 alias = seen[int_model]
953                 continue
954             # Proxy model have elements in base chain
955             # with no parents, assign the new options
956             # object and skip to the next base in that
957             # case
958             if not curr_opts.parents[int_model]:
959                 curr_opts = int_model._meta
960                 continue
961             link_field = curr_opts.get_ancestor_link(int_model)
962             _, _, _, joins, _ = self.setup_joins(
963                 [link_field.name], curr_opts, alias)
964             curr_opts = int_model._meta
965             alias = seen[int_model] = joins[-1]
966         return alias or seen[None]
967 
968     def add_annotation(self, annotation, alias, is_summary=False):
969         """
970         Adds a single annotation expression to the Query
971         """
972         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
973                                                    summarize=is_summary)
974         self.append_annotation_mask([alias])
975         self.annotations[alias] = annotation
976 
977     def _prepare_as_filter_value(self):
978         return self.clone()
979 
980     def prepare_lookup_value(self, value, lookups, can_reuse, allow_joins=True):
981         # Default lookup if none given is exact.
982         used_joins = []
983         if len(lookups) == 0:
984             lookups = ['exact']
985         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
986         # uses of None as a query value.
987         if value is None:
988             if lookups[-1] not in ('exact', 'iexact'):
989                 raise ValueError("Cannot use None as a query value")
990             return True, ['isnull'], used_joins
991         elif hasattr(value, 'resolve_expression'):
992             pre_joins = self.alias_refcount.copy()
993             value = value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
994             used_joins = [k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)]
995         elif isinstance(value, (list, tuple)):
996             # The items of the iterable may be expressions and therefore need
997             # to be resolved independently.
998             processed_values = []
999             used_joins = set()
1000             for sub_value in value:
1001                 if hasattr(sub_value, 'resolve_expression'):
1002                     pre_joins = self.alias_refcount.copy()
1003                     processed_values.append(
1004                         sub_value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
1005                     )
1006                     # The used_joins for a tuple of expressions is the union of
1007                     # the used_joins for the individual expressions.
1008                     used_joins |= set(k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0))
1009         # Subqueries need to use a different set of aliases than the
1010         # outer query. Call bump_prefix to change aliases of the inner
1011         # query (the value).
1012         if hasattr(value, '_prepare_as_filter_value'):
1013             value = value._prepare_as_filter_value()
1014             value.bump_prefix(self)
1015         # For Oracle '' is equivalent to null. The check needs to be done
1016         # at this stage because join promotion can't be done at compiler
1017         # stage. Using DEFAULT_DB_ALIAS isn't nice, but it is the best we
1018         # can do here. Similar thing is done in is_nullable(), too.
1019         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1020                 lookups[-1] == 'exact' and value == ''):
1021             value = True
1022             lookups[-1] = 'isnull'
1023         return value, lookups, used_joins
1024 
1025     def solve_lookup_type(self, lookup):
1026         """
1027         Solve the lookup type from the lookup (eg: 'foobar__id__icontains')
1028         """
1029         lookup_splitted = lookup.split(LOOKUP_SEP)
1030         if self._annotations:
1031             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1032             if expression:
1033                 return expression_lookups, (), expression
1034         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1035         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1036         if len(lookup_parts) == 0:
1037             lookup_parts = ['exact']
1038         elif len(lookup_parts) > 1:
1039             if not field_parts:
1040                 raise FieldError(
1041                     'Invalid lookup "%s" for model %s".' %
1042                     (lookup, self.get_meta().model.__name__))
1043         return lookup_parts, field_parts, False
1044 
1045     def check_query_object_type(self, value, opts, field):
1046         """
1047         Checks whether the object passed while querying is of the correct type.
1048         If not, it raises a ValueError specifying the wrong object.
1049         """
1050         if hasattr(value, '_meta'):
1051             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1052                 raise ValueError(
1053                     'Cannot query "%s": Must be "%s" instance.' %
1054                     (value, opts.object_name))
1055 
1056     def check_related_objects(self, field, value, opts):
1057         """
1058         Checks the type of object passed to query relations.
1059         """
1060         if field.is_relation:
1061             # Check that the field and the queryset use the same model in a
1062             # query like .filter(author=Author.objects.all()). For example, the
1063             # opts would be Author's (from the author field) and value.model
1064             # would be Author.objects.all() queryset's .model (Author also).
1065             # The field is the related field on the lhs side.
1066             # If _forced_pk isn't set, this isn't a queryset query or values()
1067             # or values_list() was specified by the developer in which case
1068             # that choice is trusted.
1069             if (getattr(value, '_forced_pk', False) and
1070                     not check_rel_lookup_compatibility(value.model, opts, field)):
1071                 raise ValueError(
1072                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1073                     (value.model._meta.object_name, opts.object_name)
1074                 )
1075             elif hasattr(value, '_meta'):
1076                 self.check_query_object_type(value, opts, field)
1077             elif hasattr(value, '__iter__'):
1078                 for v in value:
1079                     self.check_query_object_type(v, opts, field)
1080 
1081     def build_lookup(self, lookups, lhs, rhs):
1082         """
1083         Tries to extract transforms and lookup from given lhs.
1084 
1085         The lhs value is something that works like SQLExpression.
1086         The rhs value is what the lookup is going to compare against.
1087         The lookups is a list of names to extract using get_lookup()
1088         and get_transform().
1089         """
1090         lookups = lookups[:]
1091         while lookups:
1092             name = lookups[0]
1093             # If there is just one part left, try first get_lookup() so
1094             # that if the lhs supports both transform and lookup for the
1095             # name, then lookup will be picked.
1096             if len(lookups) == 1:
1097                 final_lookup = lhs.get_lookup(name)
1098                 if not final_lookup:
1099                     # We didn't find a lookup. We are going to interpret
1100                     # the name as transform, and do an Exact lookup against
1101                     # it.
1102                     lhs = self.try_transform(lhs, name, lookups)
1103                     final_lookup = lhs.get_lookup('exact')
1104                 return final_lookup(lhs, rhs)
1105             lhs = self.try_transform(lhs, name, lookups)
1106             lookups = lookups[1:]
1107 
1108     def try_transform(self, lhs, name, rest_of_lookups):
1109         """
1110         Helper method for build_lookup. Tries to fetch and initialize
1111         a transform for name parameter from lhs.
1112         """
1113         transform_class = lhs.get_transform(name)
1114         if transform_class:
1115             return transform_class(lhs)
1116         else:
1117             raise FieldError(
1118                 "Unsupported lookup '%s' for %s or join on the field not "
1119                 "permitted." %
1120                 (name, lhs.output_field.__class__.__name__))
1121 
1122     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1123                      can_reuse=None, connector=AND, allow_joins=True, split_subq=True):
1124         """
1125         Builds a WhereNode for a single filter clause, but doesn't add it
1126         to this Query. Query.add_q() will then add this filter to the where
1127         Node.
1128 
1129         The 'branch_negated' tells us if the current branch contains any
1130         negations. This will be used to determine if subqueries are needed.
1131 
1132         The 'current_negated' is used to determine if the current filter is
1133         negated or not and this will be used to determine if IS NULL filtering
1134         is needed.
1135 
1136         The difference between current_netageted and branch_negated is that
1137         branch_negated is set on first negation, but current_negated is
1138         flipped for each negation.
1139 
1140         Note that add_filter will not do any negating itself, that is done
1141         upper in the code by add_q().
1142 
1143         The 'can_reuse' is a set of reusable joins for multijoins.
1144 
1145         The method will create a filter clause that can be added to the current
1146         query. However, if the filter isn't added to the query then the caller
1147         is responsible for unreffing the joins used.
1148         """
1149         if isinstance(filter_expr, dict):
1150             raise FieldError("Cannot parse keyword query as dict")
1151         arg, value = filter_expr
1152         if not arg:
1153             raise FieldError("Cannot parse keyword query %r" % arg)
1154         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1155         if not allow_joins and len(parts) > 1:
1156             raise FieldError("Joined field references are not permitted in this query")
1157 
1158         # Work out the lookup type and remove it from the end of 'parts',
1159         # if necessary.
1160         value, lookups, used_joins = self.prepare_lookup_value(value, lookups, can_reuse, allow_joins)
1161 
1162         clause = self.where_class()
1163         if reffed_expression:
1164             condition = self.build_lookup(lookups, reffed_expression, value)
1165             clause.add(condition, AND)
1166             return clause, []
1167 
1168         opts = self.get_meta()
1169         alias = self.get_initial_alias()
1170         allow_many = not branch_negated or not split_subq
1171 
1172         try:
1173             field, sources, opts, join_list, path = self.setup_joins(
1174                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many)
1175 
1176             # Prevent iterator from being consumed by check_related_objects()
1177             if isinstance(value, Iterator):
1178                 value = list(value)
1179             self.check_related_objects(field, value, opts)
1180 
1181             # split_exclude() needs to know which joins were generated for the
1182             # lookup parts
1183             self._lookup_joins = join_list
1184         except MultiJoin as e:
1185             return self.split_exclude(filter_expr, LOOKUP_SEP.join(parts[:e.level]),
1186                                       can_reuse, e.names_with_path)
1187 
1188         if can_reuse is not None:
1189             can_reuse.update(join_list)
1190         used_joins = set(used_joins).union(set(join_list))
1191         targets, alias, join_list = self.trim_joins(sources, join_list, path)
1192 
1193         if field.is_relation:
1194             # No support for transforms for relational fields
1195             num_lookups = len(lookups)
1196             if num_lookups > 1:
1197                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1198             assert num_lookups > 0  # Likely a bug in Django if this fails.
1199             lookup_class = field.get_lookup(lookups[0])
1200             if lookup_class is None:
1201                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1202             if len(targets) == 1:
1203                 lhs = targets[0].get_col(alias, field)
1204             else:
1205                 lhs = MultiColSource(alias, targets, sources, field)
1206             condition = lookup_class(lhs, value)
1207             lookup_type = lookup_class.lookup_name
1208         else:
1209             col = targets[0].get_col(alias, field)
1210             condition = self.build_lookup(lookups, col, value)
1211             lookup_type = condition.lookup_name
1212 
1213         clause.add(condition, AND)
1214 
1215         require_outer = lookup_type == 'isnull' and value is True and not current_negated
1216         if current_negated and (lookup_type != 'isnull' or value is False):
1217             require_outer = True
1218             if (lookup_type != 'isnull' and (
1219                     self.is_nullable(targets[0]) or
1220                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1221                 # The condition added here will be SQL like this:
1222                 # NOT (col IS NOT NULL), where the first NOT is added in
1223                 # upper layers of code. The reason for addition is that if col
1224                 # is null, then col != someval will result in SQL "unknown"
1225                 # which isn't the same as in Python. The Python None handling
1226                 # is wanted, and it can be gotten by
1227                 # (col IS NULL OR col != someval)
1228                 #   <=>
1229                 # NOT (col IS NOT NULL AND col = someval).
1230                 lookup_class = targets[0].get_lookup('isnull')
1231                 clause.add(lookup_class(targets[0].get_col(alias, sources[0]), False), AND)
1232         return clause, used_joins if not require_outer else ()
1233 
1234     def add_filter(self, filter_clause):
1235         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1236 
1237     def add_q(self, q_object):
1238         """
1239         A preprocessor for the internal _add_q(). Responsible for doing final
1240         join promotion.
1241         """
1242         # For join promotion this case is doing an AND for the added q_object
1243         # and existing conditions. So, any existing inner join forces the join
1244         # type to remain inner. Existing outer joins can however be demoted.
1245         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1246         # rel_a doesn't produce any rows, then the whole condition must fail.
1247         # So, demotion is OK.
1248         existing_inner = set(
1249             (a for a in self.alias_map if self.alias_map[a].join_type == INNER))
1250         clause, _ = self._add_q(q_object, self.used_aliases)
1251         if clause:
1252             self.where.add(clause, AND)
1253         self.demote_joins(existing_inner)
1254 
1255     def _add_q(self, q_object, used_aliases, branch_negated=False,
1256                current_negated=False, allow_joins=True, split_subq=True):
1257         """
1258         Adds a Q-object to the current filter.
1259         """
1260         connector = q_object.connector
1261         current_negated = current_negated ^ q_object.negated
1262         branch_negated = branch_negated or q_object.negated
1263         target_clause = self.where_class(connector=connector,
1264                                          negated=q_object.negated)
1265         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1266         for child in q_object.children:
1267             if isinstance(child, Node):
1268                 child_clause, needed_inner = self._add_q(
1269                     child, used_aliases, branch_negated,
1270                     current_negated, allow_joins, split_subq)
1271                 joinpromoter.add_votes(needed_inner)
1272             else:
1273                 child_clause, needed_inner = self.build_filter(
1274                     child, can_reuse=used_aliases, branch_negated=branch_negated,
1275                     current_negated=current_negated, connector=connector,
1276                     allow_joins=allow_joins, split_subq=split_subq,
1277                 )
1278                 joinpromoter.add_votes(needed_inner)
1279             if child_clause:
1280                 target_clause.add(child_clause, connector)
1281         needed_inner = joinpromoter.update_join_types(self)
1282         return target_clause, needed_inner
1283 
1284     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1285         """
1286         Walks the list of names and turns them into PathInfo tuples. Note that
1287         a single name in 'names' can generate multiple PathInfos (m2m for
1288         example).
1289 
1290         'names' is the path of names to travel, 'opts' is the model Options we
1291         start the name resolving from, 'allow_many' is as for setup_joins().
1292         If fail_on_missing is set to True, then a name that can't be resolved
1293         will generate a FieldError.
1294 
1295         Returns a list of PathInfo tuples. In addition returns the final field
1296         (the last used join field), and target (which is a field guaranteed to
1297         contain the same value as the final field). Finally, the method returns
1298         those names that weren't found (which are likely transforms and the
1299         final lookup).
1300         """
1301         path, names_with_path = [], []
1302         for pos, name in enumerate(names):
1303             cur_names_with_path = (name, [])
1304             if name == 'pk':
1305                 name = opts.pk.name
1306 
1307             field = None
1308             try:
1309                 field = opts.get_field(name)
1310             except FieldDoesNotExist:
1311                 if name in self.annotation_select:
1312                     field = self.annotation_select[name].output_field
1313                 elif pos == 0:
1314                     for rel in opts.related_objects:
1315                         if (name == rel.related_model._meta.model_name and
1316                                 rel.related_name == rel.related_model._meta.default_related_name):
1317                             related_name = rel.related_name
1318                             field = opts.get_field(related_name)
1319                             warnings.warn(
1320                                 "Query lookup '%s' is deprecated in favor of "
1321                                 "Meta.default_related_name '%s'."
1322                                 % (name, related_name),
1323                                 RemovedInDjango20Warning, 2
1324                             )
1325                             break
1326 
1327             if field is not None:
1328                 # Fields that contain one-to-many relations with a generic
1329                 # model (like a GenericForeignKey) cannot generate reverse
1330                 # relations and therefore cannot be used for reverse querying.
1331                 if field.is_relation and not field.related_model:
1332                     raise FieldError(
1333                         "Field %r does not generate an automatic reverse "
1334                         "relation and therefore cannot be used for reverse "
1335                         "querying. If it is a GenericForeignKey, consider "
1336                         "adding a GenericRelation." % name
1337                     )
1338                 try:
1339                     model = field.model._meta.concrete_model
1340                 except AttributeError:
1341                     # QuerySet.annotate() may introduce fields that aren't
1342                     # attached to a model.
1343                     model = None
1344             else:
1345                 # We didn't find the current field, so move position back
1346                 # one step.
1347                 pos -= 1
1348                 if pos == -1 or fail_on_missing:
1349                     field_names = list(get_field_names_from_opts(opts))
1350                     available = sorted(field_names + list(self.annotation_select))
1351                     raise FieldError("Cannot resolve keyword '%s' into field. "
1352                                      "Choices are: %s" % (name, ", ".join(available)))
1353                 break
1354             # Check if we need any joins for concrete inheritance cases (the
1355             # field lives in parent, but we are currently in one of its
1356             # children)
1357             if model is not opts.model:
1358                 path_to_parent = opts.get_path_to_parent(model)
1359                 if path_to_parent:
1360                     path.extend(path_to_parent)
1361                     cur_names_with_path[1].extend(path_to_parent)
1362                     opts = path_to_parent[-1].to_opts
1363             if hasattr(field, 'get_path_info'):
1364                 pathinfos = field.get_path_info()
1365                 if not allow_many:
1366                     for inner_pos, p in enumerate(pathinfos):
1367                         if p.m2m:
1368                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1369                             names_with_path.append(cur_names_with_path)
1370                             raise MultiJoin(pos + 1, names_with_path)
1371                 last = pathinfos[-1]
1372                 path.extend(pathinfos)
1373                 final_field = last.join_field
1374                 opts = last.to_opts
1375                 targets = last.target_fields
1376                 cur_names_with_path[1].extend(pathinfos)
1377                 names_with_path.append(cur_names_with_path)
1378             else:
1379                 # Local non-relational field.
1380                 final_field = field
1381                 targets = (field,)
1382                 if fail_on_missing and pos + 1 != len(names):
1383                     raise FieldError(
1384                         "Cannot resolve keyword %r into field. Join on '%s'"
1385                         " not permitted." % (names[pos + 1], name))
1386                 break
1387         return path, final_field, targets, names[pos + 1:]
1388 
1389     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True):
1390         """
1391         Compute the necessary table joins for the passage through the fields
1392         given in 'names'. 'opts' is the Options class for the current model
1393         (which gives the table we are starting from), 'alias' is the alias for
1394         the table to start the joining from.
1395 
1396         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1397         can be None in which case all joins are reusable or a set of aliases
1398         that can be reused. Note that non-reverse foreign keys are always
1399         reusable when using setup_joins().
1400 
1401         If 'allow_many' is False, then any reverse foreign key seen will
1402         generate a MultiJoin exception.
1403 
1404         Returns the final field involved in the joins, the target field (used
1405         for any 'where' constraint), the final 'opts' value, the joins and the
1406         field path travelled to generate the joins.
1407 
1408         The target field is the field containing the concrete value. Final
1409         field can be something different, for example foreign key pointing to
1410         that value. Final field is needed for example in some value
1411         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1412         key field for example).
1413         """
1414         joins = [alias]
1415         # First, generate the path for the names
1416         path, final_field, targets, rest = self.names_to_path(
1417             names, opts, allow_many, fail_on_missing=True)
1418 
1419         # Then, add the path to the query's joins. Note that we can't trim
1420         # joins at this stage - we will need the information about join type
1421         # of the trimmed joins.
1422         for join in path:
1423             opts = join.to_opts
1424             if join.direct:
1425                 nullable = self.is_nullable(join.join_field)
1426             else:
1427                 nullable = True
1428             connection = Join(opts.db_table, alias, None, INNER, join.join_field, nullable)
1429             reuse = can_reuse if join.m2m else None
1430             alias = self.join(connection, reuse=reuse)
1431             joins.append(alias)
1432         return final_field, targets, opts, joins, path
1433 
1434     def trim_joins(self, targets, joins, path):
1435         """
1436         The 'target' parameter is the final field being joined to, 'joins'
1437         is the full list of join aliases. The 'path' contain the PathInfos
1438         used to create the joins.
1439 
1440         Returns the final target field and table alias and the new active
1441         joins.
1442 
1443         We will always trim any direct join if we have the target column
1444         available already in the previous table. Reverse joins can't be
1445         trimmed as we don't know if there is anything on the other side of
1446         the join.
1447         """
1448         joins = joins[:]
1449         for pos, info in enumerate(reversed(path)):
1450             if len(joins) == 1 or not info.direct:
1451                 break
1452             join_targets = set(t.column for t in info.join_field.foreign_related_fields)
1453             cur_targets = set(t.column for t in targets)
1454             if not cur_targets.issubset(join_targets):
1455                 break
1456             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1457             targets = tuple(targets_dict[t.column] for t in targets)
1458             self.unref_alias(joins.pop())
1459         return targets, joins[-1], joins
1460 
1461     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1462         if not allow_joins and LOOKUP_SEP in name:
1463             raise FieldError("Joined field references are not permitted in this query")
1464         if name in self.annotations:
1465             if summarize:
1466                 # Summarize currently means we are doing an aggregate() query
1467                 # which is executed as a wrapped subquery if any of the
1468                 # aggregate() elements reference an existing annotation. In
1469                 # that case we need to return a Ref to the subquery's annotation.
1470                 return Ref(name, self.annotation_select[name])
1471             else:
1472                 return self.annotation_select[name]
1473         else:
1474             field_list = name.split(LOOKUP_SEP)
1475             field, sources, opts, join_list, path = self.setup_joins(
1476                 field_list, self.get_meta(),
1477                 self.get_initial_alias(), reuse)
1478             targets, _, join_list = self.trim_joins(sources, join_list, path)
1479             if len(targets) > 1:
1480                 raise FieldError("Referencing multicolumn fields with F() objects "
1481                                  "isn't supported")
1482             if reuse is not None:
1483                 reuse.update(join_list)
1484             col = targets[0].get_col(join_list[-1], sources[0])
1485             return col
1486 
1487     def split_exclude(self, filter_expr, prefix, can_reuse, names_with_path):
1488         """
1489         When doing an exclude against any kind of N-to-many relation, we need
1490         to use a subquery. This method constructs the nested query, given the
1491         original exclude filter (filter_expr) and the portion up to the first
1492         N-to-many relation field.
1493 
1494         As an example we could have original filter ~Q(child__name='foo').
1495         We would get here with filter_expr = child__name, prefix = child and
1496         can_reuse is a set of joins usable for filters in the original query.
1497 
1498         We will turn this into equivalent of:
1499             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1500                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1501 
1502         It might be worth it to consider using WHERE NOT EXISTS as that has
1503         saner null handling, and is easier for the backend's optimizer to
1504         handle.
1505         """
1506         # Generate the inner query.
1507         query = Query(self.model)
1508         query.add_filter(filter_expr)
1509         query.clear_ordering(True)
1510         # Try to have as simple as possible subquery -> trim leading joins from
1511         # the subquery.
1512         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1513 
1514         # Add extra check to make sure the selected field will not be null
1515         # since we are adding an IN <subquery> clause. This prevents the
1516         # database from tripping over IN (...,NULL,...) selects and returning
1517         # nothing
1518         col = query.select[0]
1519         select_field = col.target
1520         alias = col.alias
1521         if self.is_nullable(select_field):
1522             lookup_class = select_field.get_lookup('isnull')
1523             lookup = lookup_class(select_field.get_col(alias), False)
1524             query.where.add(lookup, AND)
1525         if alias in can_reuse:
1526             pk = select_field.model._meta.pk
1527             # Need to add a restriction so that outer query's filters are in effect for
1528             # the subquery, too.
1529             query.bump_prefix(self)
1530             lookup_class = select_field.get_lookup('exact')
1531             # Note that the query.select[0].alias is different from alias
1532             # due to bump_prefix above.
1533             lookup = lookup_class(pk.get_col(query.select[0].alias),
1534                                   pk.get_col(alias))
1535             query.where.add(lookup, AND)
1536             query.external_aliases.add(alias)
1537 
1538         condition, needed_inner = self.build_filter(
1539             ('%s__in' % trimmed_prefix, query),
1540             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1541         if contains_louter:
1542             or_null_condition, _ = self.build_filter(
1543                 ('%s__isnull' % trimmed_prefix, True),
1544                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1545             condition.add(or_null_condition, OR)
1546             # Note that the end result will be:
1547             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1548             # This might look crazy but due to how IN works, this seems to be
1549             # correct. If the IS NOT NULL check is removed then outercol NOT
1550             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1551             # outercol IS NULL we will not match the row.
1552         return condition, needed_inner
1553 
1554     def set_empty(self):
1555         self.where.add(NothingNode(), AND)
1556 
1557     def is_empty(self):
1558         return any(isinstance(c, NothingNode) for c in self.where.children)
1559 
1560     def set_limits(self, low=None, high=None):
1561         """
1562         Adjusts the limits on the rows retrieved. We use low/high to set these,
1563         as it makes it more Pythonic to read and write. When the SQL query is
1564         created, they are converted to the appropriate offset and limit values.
1565 
1566         Any limits passed in here are applied relative to the existing
1567         constraints. So low is added to the current low value and both will be
1568         clamped to any existing high value.
1569         """
1570         if high is not None:
1571             if self.high_mark is not None:
1572                 self.high_mark = min(self.high_mark, self.low_mark + high)
1573             else:
1574                 self.high_mark = self.low_mark + high
1575         if low is not None:
1576             if self.high_mark is not None:
1577                 self.low_mark = min(self.high_mark, self.low_mark + low)
1578             else:
1579                 self.low_mark = self.low_mark + low
1580 
1581         if self.low_mark == self.high_mark:
1582             self.set_empty()
1583 
1584     def clear_limits(self):
1585         """
1586         Clears any existing limits.
1587         """
1588         self.low_mark, self.high_mark = 0, None
1589 
1590     def can_filter(self):
1591         """
1592         Returns True if adding filters to this instance is still possible.
1593 
1594         Typically, this means no limits or offsets have been put on the results.
1595         """
1596         return not self.low_mark and self.high_mark is None
1597 
1598     def clear_select_clause(self):
1599         """
1600         Removes all fields from SELECT clause.
1601         """
1602         self.select = []
1603         self.default_cols = False
1604         self.select_related = False
1605         self.set_extra_mask(())
1606         self.set_annotation_mask(())
1607 
1608     def clear_select_fields(self):
1609         """
1610         Clears the list of fields to select (but not extra_select columns).
1611         Some queryset types completely replace any existing list of select
1612         columns.
1613         """
1614         self.select = []
1615         self.values_select = []
1616 
1617     def add_select(self, col):
1618         self.default_cols = False
1619         self.select.append(col)
1620 
1621     def set_select(self, cols):
1622         self.default_cols = False
1623         self.select = cols
1624 
1625     def add_distinct_fields(self, *field_names):
1626         """
1627         Adds and resolves the given fields to the query's "distinct on" clause.
1628         """
1629         self.distinct_fields = field_names
1630         self.distinct = True
1631 
1632     def add_fields(self, field_names, allow_m2m=True):
1633         """
1634         Adds the given (model) fields to the select set. The field names are
1635         added in the order specified.
1636         """
1637         alias = self.get_initial_alias()
1638         opts = self.get_meta()
1639 
1640         try:
1641             for name in field_names:
1642                 # Join promotion note - we must not remove any rows here, so
1643                 # if there is no existing joins, use outer join.
1644                 _, targets, _, joins, path = self.setup_joins(
1645                     name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1646                 targets, final_alias, joins = self.trim_joins(targets, joins, path)
1647                 for target in targets:
1648                     self.add_select(target.get_col(final_alias))
1649         except MultiJoin:
1650             raise FieldError("Invalid field name: '%s'" % name)
1651         except FieldError:
1652             if LOOKUP_SEP in name:
1653                 # For lookups spanning over relationships, show the error
1654                 # from the model on which the lookup failed.
1655                 raise
1656             else:
1657                 names = sorted(list(get_field_names_from_opts(opts)) + list(self.extra) + list(self.annotation_select))
1658                 raise FieldError("Cannot resolve keyword %r into field. "
1659                                  "Choices are: %s" % (name, ", ".join(names)))
1660 
1661     def add_ordering(self, *ordering):
1662         """
1663         Adds items from the 'ordering' sequence to the query's "order by"
1664         clause. These items are either field names (not column names) --
1665         possibly with a direction prefix ('-' or '?') -- or OrderBy
1666         expressions.
1667 
1668         If 'ordering' is empty, all ordering is cleared from the query.
1669         """
1670         errors = []
1671         for item in ordering:
1672             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
1673                 errors.append(item)
1674             if getattr(item, 'contains_aggregate', False):
1675                 raise FieldError(
1676                     'Using an aggregate in order_by() without also including '
1677                     'it in annotate() is not allowed: %s' % item
1678                 )
1679         if errors:
1680             raise FieldError('Invalid order_by arguments: %s' % errors)
1681         if ordering:
1682             self.order_by.extend(ordering)
1683         else:
1684             self.default_ordering = False
1685 
1686     def clear_ordering(self, force_empty):
1687         """
1688         Removes any ordering settings. If 'force_empty' is True, there will be
1689         no ordering in the resulting query (not even the model's default).
1690         """
1691         self.order_by = []
1692         self.extra_order_by = ()
1693         if force_empty:
1694             self.default_ordering = False
1695 
1696     def set_group_by(self):
1697         """
1698         Expands the GROUP BY clause required by the query.
1699 
1700         This will usually be the set of all non-aggregate fields in the
1701         return data. If the database backend supports grouping by the
1702         primary key, and the query would be equivalent, the optimization
1703         will be made automatically.
1704         """
1705         self.group_by = []
1706 
1707         for col in self.select:
1708             self.group_by.append(col)
1709 
1710         if self.annotation_select:
1711             for alias, annotation in six.iteritems(self.annotation_select):
1712                 for col in annotation.get_group_by_cols():
1713                     self.group_by.append(col)
1714 
1715     def add_select_related(self, fields):
1716         """
1717         Sets up the select_related data structure so that we only select
1718         certain related models (as opposed to all models, when
1719         self.select_related=True).
1720         """
1721         if isinstance(self.select_related, bool):
1722             field_dict = {}
1723         else:
1724             field_dict = self.select_related
1725         for field in fields:
1726             d = field_dict
1727             for part in field.split(LOOKUP_SEP):
1728                 d = d.setdefault(part, {})
1729         self.select_related = field_dict
1730 
1731     def add_extra(self, select, select_params, where, params, tables, order_by):
1732         """
1733         Adds data to the various extra_* attributes for user-created additions
1734         to the query.
1735         """
1736         if select:
1737             # We need to pair any placeholder markers in the 'select'
1738             # dictionary with their parameters in 'select_params' so that
1739             # subsequent updates to the select dictionary also adjust the
1740             # parameters appropriately.
1741             select_pairs = OrderedDict()
1742             if select_params:
1743                 param_iter = iter(select_params)
1744             else:
1745                 param_iter = iter([])
1746             for name, entry in select.items():
1747                 entry = force_text(entry)
1748                 entry_params = []
1749                 pos = entry.find("%s")
1750                 while pos != -1:
1751                     if pos == 0 or entry[pos - 1] != '%':
1752                         entry_params.append(next(param_iter))
1753                     pos = entry.find("%s", pos + 2)
1754                 select_pairs[name] = (entry, entry_params)
1755             # This is order preserving, since self.extra_select is an OrderedDict.
1756             self.extra.update(select_pairs)
1757         if where or params:
1758             self.where.add(ExtraWhere(where, params), AND)
1759         if tables:
1760             self.extra_tables += tuple(tables)
1761         if order_by:
1762             self.extra_order_by = order_by
1763 
1764     def clear_deferred_loading(self):
1765         """
1766         Remove any fields from the deferred loading set.
1767         """
1768         self.deferred_loading = (set(), True)
1769 
1770     def add_deferred_loading(self, field_names):
1771         """
1772         Add the given list of model field names to the set of fields to
1773         exclude from loading from the database when automatic column selection
1774         is done. The new field names are added to any existing field names that
1775         are deferred (or removed from any existing field names that are marked
1776         as the only ones for immediate loading).
1777         """
1778         # Fields on related models are stored in the literal double-underscore
1779         # format, so that we can use a set datastructure. We do the foo__bar
1780         # splitting and handling when computing the SQL column names (as part of
1781         # get_columns()).
1782         existing, defer = self.deferred_loading
1783         if defer:
1784             # Add to existing deferred names.
1785             self.deferred_loading = existing.union(field_names), True
1786         else:
1787             # Remove names from the set of any existing "immediate load" names.
1788             self.deferred_loading = existing.difference(field_names), False
1789 
1790     def add_immediate_loading(self, field_names):
1791         """
1792         Add the given list of model field names to the set of fields to
1793         retrieve when the SQL is executed ("immediate loading" fields). The
1794         field names replace any existing immediate loading field names. If
1795         there are field names already specified for deferred loading, those
1796         names are removed from the new field_names before storing the new names
1797         for immediate loading. (That is, immediate loading overrides any
1798         existing immediate values, but respects existing deferrals.)
1799         """
1800         existing, defer = self.deferred_loading
1801         field_names = set(field_names)
1802         if 'pk' in field_names:
1803             field_names.remove('pk')
1804             field_names.add(self.get_meta().pk.name)
1805 
1806         if defer:
1807             # Remove any existing deferred names from the current set before
1808             # setting the new names.
1809             self.deferred_loading = field_names.difference(existing), False
1810         else:
1811             # Replace any existing "immediate load" field names.
1812             self.deferred_loading = field_names, False
1813 
1814     def get_loaded_field_names(self):
1815         """
1816         If any fields are marked to be deferred, returns a dictionary mapping
1817         models to a set of names in those fields that will be loaded. If a
1818         model is not in the returned dictionary, none of its fields are
1819         deferred.
1820 
1821         If no fields are marked for deferral, returns an empty dictionary.
1822         """
1823         # We cache this because we call this function multiple times
1824         # (compiler.fill_related_selections, query.iterator)
1825         try:
1826             return self._loaded_field_names_cache
1827         except AttributeError:
1828             collection = {}
1829             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
1830             self._loaded_field_names_cache = collection
1831             return collection
1832 
1833     def get_loaded_field_names_cb(self, target, model, fields):
1834         """
1835         Callback used by get_deferred_field_names().
1836         """
1837         target[model] = {f.attname for f in fields}
1838 
1839     def set_annotation_mask(self, names):
1840         "Set the mask of annotations that will actually be returned by the SELECT"
1841         if names is None:
1842             self.annotation_select_mask = None
1843         else:
1844             self.annotation_select_mask = set(names)
1845         self._annotation_select_cache = None
1846 
1847     def append_annotation_mask(self, names):
1848         if self.annotation_select_mask is not None:
1849             self.set_annotation_mask(set(names).union(self.annotation_select_mask))
1850 
1851     def set_extra_mask(self, names):
1852         """
1853         Set the mask of extra select items that will be returned by SELECT,
1854         we don't actually remove them from the Query since they might be used
1855         later
1856         """
1857         if names is None:
1858             self.extra_select_mask = None
1859         else:
1860             self.extra_select_mask = set(names)
1861         self._extra_select_cache = None
1862 
1863     def set_values(self, fields):
1864         self.select_related = False
1865         self.clear_deferred_loading()
1866         self.clear_select_fields()
1867 
1868         if self.group_by is True:
1869             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
1870             self.set_group_by()
1871             self.clear_select_fields()
1872 
1873         if fields:
1874             field_names = []
1875             extra_names = []
1876             annotation_names = []
1877             if not self._extra and not self._annotations:
1878                 # Shortcut - if there are no extra or annotations, then
1879                 # the values() clause must be just field names.
1880                 field_names = list(fields)
1881             else:
1882                 self.default_cols = False
1883                 for f in fields:
1884                     if f in self.extra_select:
1885                         extra_names.append(f)
1886                     elif f in self.annotation_select:
1887                         annotation_names.append(f)
1888                     else:
1889                         field_names.append(f)
1890             self.set_extra_mask(extra_names)
1891             self.set_annotation_mask(annotation_names)
1892         else:
1893             field_names = [f.attname for f in self.model._meta.concrete_fields]
1894 
1895         self.values_select = field_names
1896         self.add_fields(field_names, True)
1897 
1898     @property
1899     def annotation_select(self):
1900         """The OrderedDict of aggregate columns that are not masked, and should
1901         be used in the SELECT clause.
1902 
1903         This result is cached for optimization purposes.
1904         """
1905         if self._annotation_select_cache is not None:
1906             return self._annotation_select_cache
1907         elif not self._annotations:
1908             return {}
1909         elif self.annotation_select_mask is not None:
1910             self._annotation_select_cache = OrderedDict(
1911                 (k, v) for k, v in self.annotations.items()
1912                 if k in self.annotation_select_mask
1913             )
1914             return self._annotation_select_cache
1915         else:
1916             return self.annotations
1917 
1918     @property
1919     def extra_select(self):
1920         if self._extra_select_cache is not None:
1921             return self._extra_select_cache
1922         if not self._extra:
1923             return {}
1924         elif self.extra_select_mask is not None:
1925             self._extra_select_cache = OrderedDict(
1926                 (k, v) for k, v in self.extra.items()
1927                 if k in self.extra_select_mask
1928             )
1929             return self._extra_select_cache
1930         else:
1931             return self.extra
1932 
1933     def trim_start(self, names_with_path):
1934         """
1935         Trims joins from the start of the join path. The candidates for trim
1936         are the PathInfos in names_with_path structure that are m2m joins.
1937 
1938         Also sets the select column so the start matches the join.
1939 
1940         This method is meant to be used for generating the subquery joins &
1941         cols in split_exclude().
1942 
1943         Returns a lookup usable for doing outerq.filter(lookup=self). Returns
1944         also if the joins in the prefix contain a LEFT OUTER join.
1945         _"""
1946         all_paths = []
1947         for _, paths in names_with_path:
1948             all_paths.extend(paths)
1949         contains_louter = False
1950         # Trim and operate only on tables that were generated for
1951         # the lookup part of the query. That is, avoid trimming
1952         # joins generated for F() expressions.
1953         lookup_tables = [t for t in self.tables if t in self._lookup_joins or t == self.tables[0]]
1954         for trimmed_paths, path in enumerate(all_paths):
1955             if path.m2m:
1956                 break
1957             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
1958                 contains_louter = True
1959             alias = lookup_tables[trimmed_paths]
1960             self.unref_alias(alias)
1961         # The path.join_field is a Rel, lets get the other side's field
1962         join_field = path.join_field.field
1963         # Build the filter prefix.
1964         paths_in_prefix = trimmed_paths
1965         trimmed_prefix = []
1966         for name, path in names_with_path:
1967             if paths_in_prefix - len(path) < 0:
1968                 break
1969             trimmed_prefix.append(name)
1970             paths_in_prefix -= len(path)
1971         trimmed_prefix.append(
1972             join_field.foreign_related_fields[0].name)
1973         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
1974         # Lets still see if we can trim the first join from the inner query
1975         # (that is, self). We can't do this for LEFT JOINs because we would
1976         # miss those rows that have nothing on the outer side.
1977         if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type != LOUTER:
1978             select_fields = [r[0] for r in join_field.related_fields]
1979             select_alias = lookup_tables[trimmed_paths + 1]
1980             self.unref_alias(lookup_tables[trimmed_paths])
1981             extra_restriction = join_field.get_extra_restriction(
1982                 self.where_class, None, lookup_tables[trimmed_paths + 1])
1983             if extra_restriction:
1984                 self.where.add(extra_restriction, AND)
1985         else:
1986             # TODO: It might be possible to trim more joins from the start of the
1987             # inner query if it happens to have a longer join chain containing the
1988             # values in select_fields. Lets punt this one for now.
1989             select_fields = [r[1] for r in join_field.related_fields]
1990             select_alias = lookup_tables[trimmed_paths]
1991         # The found starting point is likely a Join instead of a BaseTable reference.
1992         # But the first entry in the query's FROM clause must not be a JOIN.
1993         for table in self.tables:
1994             if self.alias_refcount[table] > 0:
1995                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
1996                 break
1997         self.set_select([f.get_col(select_alias) for f in select_fields])
1998         return trimmed_prefix, contains_louter
1999 
2000     def is_nullable(self, field):
2001         """
2002         A helper to check if the given field should be treated as nullable.
2003 
2004         Some backends treat '' as null and Django treats such fields as
2005         nullable for those backends. In such situations field.null can be
2006         False even if we should treat the field as nullable.
2007         """
2008         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2009         # (nor should it have) knowledge of which connection is going to be
2010         # used. The proper fix would be to defer all decisions where
2011         # is_nullable() is needed to the compiler stage, but that is not easy
2012         # to do currently.
2013         if connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and field.empty_strings_allowed:
2014             return True
2015         else:
2016             return field.null
2017 
2018     def as_subquery_filter(self, db):
2019         self._db = db
2020         self.subquery = True
2021         # It's safe to drop ordering if the queryset isn't using slicing,
2022         # distinct(*fields) or select_for_update().
2023         if (self.low_mark == 0 and self.high_mark is None and
2024                 not self.distinct_fields and
2025                 not self.select_for_update):
2026             self.clear_ordering(True)
2027         return self

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 20:13:47.714 | INFO     | Search completed. Bug locations: [<file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>combine</method>
<code>
519     def combine(self, rhs, connector):
520         """
521         Merge the 'rhs' query into the current one (with any 'rhs' effects
522         being applied *after* (that is, "to the right of") anything in the
523         current query. 'rhs' is not modified during a call to this function.
524 
525         The 'connector' parameter describes how to connect filters from the
526         'rhs' query.
527         """
528         assert self.model == rhs.model, \
529             "Cannot combine queries on two different base models."
530         assert self.can_filter(), \
531             "Cannot combine queries once a slice has been taken."
532         assert self.distinct == rhs.distinct, \
533             "Cannot combine a unique query with a non-unique query."
534         assert self.distinct_fields == rhs.distinct_fields, \
535             "Cannot combine queries with different distinct fields."
536 
537         # Work out how to relabel the rhs aliases, if necessary.
538         change_map = {}
539         conjunction = (connector == AND)
540 
541         # Determine which existing joins can be reused. When combining the
542         # query with AND we must recreate all joins for m2m filters. When
543         # combining with OR we can reuse joins. The reason is that in AND
544         # case a single row can't fulfill a condition like:
545         #     revrel__col=1 & revrel__col=2
546         # But, there might be two different related rows matching this
547         # condition. In OR case a single True is enough, so single row is
548         # enough, too.
549         #
550         # Note that we will be creating duplicate joins for non-m2m joins in
551         # the AND case. The results will be correct but this creates too many
552         # joins. This is something that could be fixed later on.
553         reuse = set() if conjunction else set(self.tables)
554         # Base table must be present in the query - this is the same
555         # table on both sides.
556         self.get_initial_alias()
557         joinpromoter = JoinPromoter(connector, 2, False)
558         joinpromoter.add_votes(
559             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
560         rhs_votes = set()
561         # Now, add the joins from rhs query into the new query (skipping base
562         # table).
563         for alias in rhs.tables[1:]:
564             join = rhs.alias_map[alias]
565             # If the left side of the join was already relabeled, use the
566             # updated alias.
567             join = join.relabeled_clone(change_map)
568             new_alias = self.join(join, reuse=reuse)
569             if join.join_type == INNER:
570                 rhs_votes.add(new_alias)
571             # We can't reuse the same join again in the query. If we have two
572             # distinct joins for the same connection in rhs query, then the
573             # combined query must have two joins, too.
574             reuse.discard(new_alias)
575             if alias != new_alias:
576                 change_map[alias] = new_alias
577             if not rhs.alias_refcount[alias]:
578                 # The alias was unused in the rhs query. Unref it so that it
579                 # will be unused in the new query, too. We have to add and
580                 # unref the alias so that join promotion has information of
581                 # the join type for the unused alias.
582                 self.unref_alias(new_alias)
583         joinpromoter.add_votes(rhs_votes)
584         joinpromoter.update_join_types(self)
585 
586         # Now relabel a copy of the rhs where-clause and add it to the current
587         # one.
588         w = rhs.where.clone()
589         w.relabel_aliases(change_map)
590         self.where.add(w, connector)
591 
592         # Selection columns and extra extensions are those provided by 'rhs'.
593         self.select = []
594         for col in rhs.select:
595             self.add_select(col.relabeled_clone(change_map))
596 
597         if connector == OR:
598             # It would be nice to be able to handle this, but the queries don't
599             # really make sense (or return consistent value sets). Not worth
600             # the extra complexity when you can write a real query instead.
601             if self._extra and rhs._extra:
602                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
603         self.extra.update(rhs.extra)
604         extra_select_mask = set()
605         if self.extra_select_mask is not None:
606             extra_select_mask.update(self.extra_select_mask)
607         if rhs.extra_select_mask is not None:
608             extra_select_mask.update(rhs.extra_select_mask)
609         if extra_select_mask:
610             self.set_extra_mask(extra_select_mask)
611         self.extra_tables += rhs.extra_tables
612 
613         # Ordering uses the 'rhs' ordering, unless it has none, in which case
614         # the current ordering is used.
615         self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by
616         self.extra_order_by = rhs.extra_order_by or self.extra_order_by

</code><intended_behavior>The `combine` method should ensure stability and predictability in how queries are merged, regardless of the order in which dict or set operations occur. This might involve introducing order-independent checks or validations to prevent the creation of circular references in `change_map` or handling Python's non-deterministic collections in a manner that does not affect the query's outcome. Additionally, improving error handling around assertions or conditions that lead to the observed non-deterministic crashes could be part of the solution. Ensuring consistent behavior across executions with the same input should be a key aim.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
117 class Query(object):
118     """
119     A single SQL query.
120     """
121 
122     alias_prefix = 'T'
123     subq_aliases = frozenset([alias_prefix])
124     query_terms = QUERY_TERMS
125 
126     compiler = 'SQLCompiler'
127 
128     def __init__(self, model, where=WhereNode):
129         self.model = model
130         self.alias_refcount = {}
131         # alias_map is the most important data structure regarding joins.
132         # It's used for recording which joins exist in the query and what
133         # types they are. The key is the alias of the joined table (possibly
134         # the table name) and the value is a Join-like object (see
135         # sql.datastructures.Join for more information).
136         self.alias_map = {}
137         # Sometimes the query contains references to aliases in outer queries (as
138         # a result of split_exclude). Correct alias quoting needs to know these
139         # aliases too.
140         self.external_aliases = set()
141         self.table_map = {}     # Maps table names to list of aliases.
142         self.default_cols = True
143         self.default_ordering = True
144         self.standard_ordering = True
145         self.used_aliases = set()
146         self.filter_is_sticky = False
147         self.subquery = False
148 
149         # SQL-related attributes
150         # Select and related select clauses are expressions to use in the
151         # SELECT clause of the query.
152         # The select is used for cases where we want to set up the select
153         # clause to contain other than default fields (values(), subqueries...)
154         # Note that annotations go to annotations dictionary.
155         self.select = []
156         self.tables = []    # Aliases in the order they are created.
157         self.where = where()
158         self.where_class = where
159         # The group_by attribute can have one of the following forms:
160         #  - None: no group by at all in the query
161         #  - A list of expressions: group by (at least) those expressions.
162         #    String refs are also allowed for now.
163         #  - True: group by all select fields of the model
164         # See compiler.get_group_by() for details.
165         self.group_by = None
166         self.order_by = []
167         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
168         self.distinct = False
169         self.distinct_fields = []
170         self.select_for_update = False
171         self.select_for_update_nowait = False
172         self.select_for_update_skip_locked = False
173 
174         self.select_related = False
175         # Arbitrary limit for select_related to prevents infinite recursion.
176         self.max_depth = 5
177 
178         # Holds the selects defined by a call to values() or values_list()
179         # excluding annotation_select and extra_select.
180         self.values_select = []
181 
182         # SQL annotation-related attributes
183         # The _annotations will be an OrderedDict when used. Due to the cost
184         # of creating OrderedDict this attribute is created lazily (in
185         # self.annotations property).
186         self._annotations = None  # Maps alias -> Annotation Expression
187         self.annotation_select_mask = None
188         self._annotation_select_cache = None
189 
190         # Set combination attributes
191         self.combinator = None
192         self.combinator_all = False
193         self.combined_queries = ()
194 
195         # These are for extensions. The contents are more or less appended
196         # verbatim to the appropriate clause.
197         # The _extra attribute is an OrderedDict, lazily created similarly to
198         # .annotations
199         self._extra = None  # Maps col_alias -> (col_sql, params).
200         self.extra_select_mask = None
201         self._extra_select_cache = None
202 
203         self.extra_tables = ()
204         self.extra_order_by = ()
205 
206         # A tuple that is a set of model field names and either True, if these
207         # are the fields to defer, or False if these are the only fields to
208         # load.
209         self.deferred_loading = (set(), True)
210 
211         self.context = {}
212 
213     @property
214     def extra(self):
215         if self._extra is None:
216             self._extra = OrderedDict()
217         return self._extra
218 
219     @property
220     def annotations(self):
221         if self._annotations is None:
222             self._annotations = OrderedDict()
223         return self._annotations
224 
225     def __str__(self):
226         """
227         Returns the query as a string of SQL with the parameter values
228         substituted in (use sql_with_params() to see the unsubstituted string).
229 
230         Parameter values won't necessarily be quoted correctly, since that is
231         done by the database interface at execution time.
232         """
233         sql, params = self.sql_with_params()
234         return sql % params
235 
236     def sql_with_params(self):
237         """
238         Returns the query as an SQL string and the parameters that will be
239         substituted into the query.
240         """
241         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
242 
243     def __deepcopy__(self, memo):
244         result = self.clone(memo=memo)
245         memo[id(self)] = result
246         return result
247 
248     def _prepare(self, field):
249         return self
250 
251     def get_compiler(self, using=None, connection=None):
252         if using is None and connection is None:
253             raise ValueError("Need either using or connection")
254         if using:
255             connection = connections[using]
256         return connection.ops.compiler(self.compiler)(self, connection, using)
257 
258     def get_meta(self):
259         """
260         Returns the Options instance (the model._meta) from which to start
261         processing. Normally, this is self.model._meta, but it can be changed
262         by subclasses.
263         """
264         return self.model._meta
265 
266     def clone(self, klass=None, memo=None, **kwargs):
267         """
268         Creates a copy of the current instance. The 'kwargs' parameter can be
269         used by clients to update attributes after copying has taken place.
270         """
271         obj = Empty()
272         obj.__class__ = klass or self.__class__
273         obj.model = self.model
274         obj.alias_refcount = self.alias_refcount.copy()
275         obj.alias_map = self.alias_map.copy()
276         obj.external_aliases = self.external_aliases.copy()
277         obj.table_map = self.table_map.copy()
278         obj.default_cols = self.default_cols
279         obj.default_ordering = self.default_ordering
280         obj.standard_ordering = self.standard_ordering
281         obj.select = self.select[:]
282         obj.tables = self.tables[:]
283         obj.where = self.where.clone()
284         obj.where_class = self.where_class
285         if self.group_by is None:
286             obj.group_by = None
287         elif self.group_by is True:
288             obj.group_by = True
289         else:
290             obj.group_by = self.group_by[:]
291         obj.order_by = self.order_by[:]
292         obj.low_mark, obj.high_mark = self.low_mark, self.high_mark
293         obj.distinct = self.distinct
294         obj.distinct_fields = self.distinct_fields[:]
295         obj.select_for_update = self.select_for_update
296         obj.select_for_update_nowait = self.select_for_update_nowait
297         obj.select_for_update_skip_locked = self.select_for_update_skip_locked
298         obj.select_related = self.select_related
299         obj.values_select = self.values_select[:]
300         obj._annotations = self._annotations.copy() if self._annotations is not None else None
301         if self.annotation_select_mask is None:
302             obj.annotation_select_mask = None
303         else:
304             obj.annotation_select_mask = self.annotation_select_mask.copy()
305         # _annotation_select_cache cannot be copied, as doing so breaks the
306         # (necessary) state in which both annotations and
307         # _annotation_select_cache point to the same underlying objects.
308         # It will get re-populated in the cloned queryset the next time it's
309         # used.
310         obj._annotation_select_cache = None
311         obj.max_depth = self.max_depth
312         obj.combinator = self.combinator
313         obj.combinator_all = self.combinator_all
314         obj.combined_queries = self.combined_queries
315         obj._extra = self._extra.copy() if self._extra is not None else None
316         if self.extra_select_mask is None:
317             obj.extra_select_mask = None
318         else:
319             obj.extra_select_mask = self.extra_select_mask.copy()
320         if self._extra_select_cache is None:
321             obj._extra_select_cache = None
322         else:
323             obj._extra_select_cache = self._extra_select_cache.copy()
324         obj.extra_tables = self.extra_tables
325         obj.extra_order_by = self.extra_order_by
326         obj.deferred_loading = copy.copy(self.deferred_loading[0]), self.deferred_loading[1]
327         if self.filter_is_sticky and self.used_aliases:
328             obj.used_aliases = self.used_aliases.copy()
329         else:
330             obj.used_aliases = set()
331         obj.filter_is_sticky = False
332         obj.subquery = self.subquery
333         if 'alias_prefix' in self.__dict__:
334             obj.alias_prefix = self.alias_prefix
335         if 'subq_aliases' in self.__dict__:
336             obj.subq_aliases = self.subq_aliases.copy()
337 
338         obj.__dict__.update(kwargs)
339         if hasattr(obj, '_setup_query'):
340             obj._setup_query()
341         obj.context = self.context.copy()
342         obj._forced_pk = getattr(self, '_forced_pk', False)
343         return obj
344 
345     def add_context(self, key, value):
346         self.context[key] = value
347 
348     def get_context(self, key, default=None):
349         return self.context.get(key, default)
350 
351     def relabeled_clone(self, change_map):
352         clone = self.clone()
353         clone.change_aliases(change_map)
354         return clone
355 
356     def rewrite_cols(self, annotation, col_cnt):
357         # We must make sure the inner query has the referred columns in it.
358         # If we are aggregating over an annotation, then Django uses Ref()
359         # instances to note this. However, if we are annotating over a column
360         # of a related model, then it might be that column isn't part of the
361         # SELECT clause of the inner query, and we must manually make sure
362         # the column is selected. An example case is:
363         #    .aggregate(Sum('author__awards'))
364         # Resolving this expression results in a join to author, but there
365         # is no guarantee the awards column of author is in the select clause
366         # of the query. Thus we must manually add the column to the inner
367         # query.
368         orig_exprs = annotation.get_source_expressions()
369         new_exprs = []
370         for expr in orig_exprs:
371             # FIXME: These conditions are fairly arbitrary. Identify a better
372             # method of having expressions decide which code path they should
373             # take.
374             if isinstance(expr, Ref):
375                 # Its already a Ref to subquery (see resolve_ref() for
376                 # details)
377                 new_exprs.append(expr)
378             elif isinstance(expr, (WhereNode, Lookup)):
379                 # Decompose the subexpressions further. The code here is
380                 # copied from the else clause, but this condition must appear
381                 # before the contains_aggregate/is_summary condition below.
382                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
383                 new_exprs.append(new_expr)
384             elif isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
385                 # Reference to column. Make sure the referenced column
386                 # is selected.
387                 col_cnt += 1
388                 col_alias = '__col%d' % col_cnt
389                 self.annotations[col_alias] = expr
390                 self.append_annotation_mask([col_alias])
391                 new_exprs.append(Ref(col_alias, expr))
392             else:
393                 # Some other expression not referencing database values
394                 # directly. Its subexpression might contain Cols.
395                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
396                 new_exprs.append(new_expr)
397         annotation.set_source_expressions(new_exprs)
398         return annotation, col_cnt
399 
400     def get_aggregation(self, using, added_aggregate_names):
401         """
402         Returns the dictionary with the values of the existing aggregations.
403         """
404         if not self.annotation_select:
405             return {}
406         has_limit = self.low_mark != 0 or self.high_mark is not None
407         has_existing_annotations = any(
408             annotation for alias, annotation
409             in self.annotations.items()
410             if alias not in added_aggregate_names
411         )
412         # Decide if we need to use a subquery.
413         #
414         # Existing annotations would cause incorrect results as get_aggregation()
415         # must produce just one result and thus must not use GROUP BY. But we
416         # aren't smart enough to remove the existing annotations from the
417         # query, so those would force us to use GROUP BY.
418         #
419         # If the query has limit or distinct, or uses set operations, then
420         # those operations must be done in a subquery so that the query
421         # aggregates on the limit and/or distinct results instead of applying
422         # the distinct and limit after the aggregation.
423         if (isinstance(self.group_by, list) or has_limit or has_existing_annotations or
424                 self.distinct or self.combinator):
425             from django.db.models.sql.subqueries import AggregateQuery
426             outer_query = AggregateQuery(self.model)
427             inner_query = self.clone()
428             inner_query.select_for_update = False
429             inner_query.select_related = False
430             if not has_limit and not self.distinct_fields:
431                 # Queries with distinct_fields need ordering and when a limit
432                 # is applied we must take the slice from the ordered query.
433                 # Otherwise no need for ordering.
434                 inner_query.clear_ordering(True)
435             if not inner_query.distinct:
436                 # If the inner query uses default select and it has some
437                 # aggregate annotations, then we must make sure the inner
438                 # query is grouped by the main model's primary key. However,
439                 # clearing the select clause can alter results if distinct is
440                 # used.
441                 if inner_query.default_cols and has_existing_annotations:
442                     inner_query.group_by = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
443                 inner_query.default_cols = False
444 
445             relabels = {t: 'subquery' for t in inner_query.tables}
446             relabels[None] = 'subquery'
447             # Remove any aggregates marked for reduction from the subquery
448             # and move them to the outer AggregateQuery.
449             col_cnt = 0
450             for alias, expression in list(inner_query.annotation_select.items()):
451                 if expression.is_summary:
452                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
453                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
454                     del inner_query.annotations[alias]
455                 # Make sure the annotation_select wont use cached results.
456                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
457             if inner_query.select == [] and not inner_query.default_cols and not inner_query.annotation_select_mask:
458                 # In case of Model.objects[0:3].count(), there would be no
459                 # field selected in the inner query, yet we must use a subquery.
460                 # So, make sure at least one field is selected.
461                 inner_query.select = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
462             try:
463                 outer_query.add_subquery(inner_query, using)
464             except EmptyResultSet:
465                 return {
466                     alias: None
467                     for alias in outer_query.annotation_select
468                 }
469         else:
470             outer_query = self
471             self.select = []
472             self.default_cols = False
473             self._extra = {}
474 
475         outer_query.clear_ordering(True)
476         outer_query.clear_limits()
477         outer_query.select_for_update = False
478         outer_query.select_related = False
479         compiler = outer_query.get_compiler(using)
480         result = compiler.execute_sql(SINGLE)
481         if result is None:
482             result = [None for q in outer_query.annotation_select.items()]
483 
484         converters = compiler.get_converters(outer_query.annotation_select.values())
485         result = compiler.apply_converters(result, converters)
486 
487         return {
488             alias: val
489             for (alias, annotation), val
490             in zip(outer_query.annotation_select.items(), result)
491         }
492 
493     def get_count(self, using):
494         """
495         Performs a COUNT() query using the current filter constraints.
496         """
497         obj = self.clone()
498         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
499         number = obj.get_aggregation(using, ['__count'])['__count']
500         if number is None:
501             number = 0
502         return number
503 
504     def has_filters(self):
505         return self.where
506 
507     def has_results(self, using):
508         q = self.clone()
509         if not q.distinct:
510             if q.group_by is True:
511                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
512                 q.set_group_by()
513             q.clear_select_clause()
514         q.clear_ordering(True)
515         q.set_limits(high=1)
516         compiler = q.get_compiler(using=using)
517         return compiler.has_results()
518 
519     def combine(self, rhs, connector):
520         """
521         Merge the 'rhs' query into the current one (with any 'rhs' effects
522         being applied *after* (that is, "to the right of") anything in the
523         current query. 'rhs' is not modified during a call to this function.
524 
525         The 'connector' parameter describes how to connect filters from the
526         'rhs' query.
527         """
528         assert self.model == rhs.model, \
529             "Cannot combine queries on two different base models."
530         assert self.can_filter(), \
531             "Cannot combine queries once a slice has been taken."
532         assert self.distinct == rhs.distinct, \
533             "Cannot combine a unique query with a non-unique query."
534         assert self.distinct_fields == rhs.distinct_fields, \
535             "Cannot combine queries with different distinct fields."
536 
537         # Work out how to relabel the rhs aliases, if necessary.
538         change_map = {}
539         conjunction = (connector == AND)
540 
541         # Determine which existing joins can be reused. When combining the
542         # query with AND we must recreate all joins for m2m filters. When
543         # combining with OR we can reuse joins. The reason is that in AND
544         # case a single row can't fulfill a condition like:
545         #     revrel__col=1 & revrel__col=2
546         # But, there might be two different related rows matching this
547         # condition. In OR case a single True is enough, so single row is
548         # enough, too.
549         #
550         # Note that we will be creating duplicate joins for non-m2m joins in
551         # the AND case. The results will be correct but this creates too many
552         # joins. This is something that could be fixed later on.
553         reuse = set() if conjunction else set(self.tables)
554         # Base table must be present in the query - this is the same
555         # table on both sides.
556         self.get_initial_alias()
557         joinpromoter = JoinPromoter(connector, 2, False)
558         joinpromoter.add_votes(
559             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
560         rhs_votes = set()
561         # Now, add the joins from rhs query into the new query (skipping base
562         # table).
563         for alias in rhs.tables[1:]:
564             join = rhs.alias_map[alias]
565             # If the left side of the join was already relabeled, use the
566             # updated alias.
567             join = join.relabeled_clone(change_map)
568             new_alias = self.join(join, reuse=reuse)
569             if join.join_type == INNER:
570                 rhs_votes.add(new_alias)
571             # We can't reuse the same join again in the query. If we have two
572             # distinct joins for the same connection in rhs query, then the
573             # combined query must have two joins, too.
574             reuse.discard(new_alias)
575             if alias != new_alias:
576                 change_map[alias] = new_alias
577             if not rhs.alias_refcount[alias]:
578                 # The alias was unused in the rhs query. Unref it so that it
579                 # will be unused in the new query, too. We have to add and
580                 # unref the alias so that join promotion has information of
581                 # the join type for the unused alias.
582                 self.unref_alias(new_alias)
583         joinpromoter.add_votes(rhs_votes)
584         joinpromoter.update_join_types(self)
585 
586         # Now relabel a copy of the rhs where-clause and add it to the current
587         # one.
588         w = rhs.where.clone()
589         w.relabel_aliases(change_map)
590         self.where.add(w, connector)
591 
592         # Selection columns and extra extensions are those provided by 'rhs'.
593         self.select = []
594         for col in rhs.select:
595             self.add_select(col.relabeled_clone(change_map))
596 
597         if connector == OR:
598             # It would be nice to be able to handle this, but the queries don't
599             # really make sense (or return consistent value sets). Not worth
600             # the extra complexity when you can write a real query instead.
601             if self._extra and rhs._extra:
602                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
603         self.extra.update(rhs.extra)
604         extra_select_mask = set()
605         if self.extra_select_mask is not None:
606             extra_select_mask.update(self.extra_select_mask)
607         if rhs.extra_select_mask is not None:
608             extra_select_mask.update(rhs.extra_select_mask)
609         if extra_select_mask:
610             self.set_extra_mask(extra_select_mask)
611         self.extra_tables += rhs.extra_tables
612 
613         # Ordering uses the 'rhs' ordering, unless it has none, in which case
614         # the current ordering is used.
615         self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by
616         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
617 
618     def deferred_to_data(self, target, callback):
619         """
620         Converts the self.deferred_loading data structure to an alternate data
621         structure, describing the field that *will* be loaded. This is used to
622         compute the columns to select from the database and also by the
623         QuerySet class to work out which fields are being initialized on each
624         model. Models that have all their fields included aren't mentioned in
625         the result, only those that have field restrictions in place.
626 
627         The "target" parameter is the instance that is populated (in place).
628         The "callback" is a function that is called whenever a (model, field)
629         pair need to be added to "target". It accepts three parameters:
630         "target", and the model and list of fields being added for that model.
631         """
632         field_names, defer = self.deferred_loading
633         if not field_names:
634             return
635         orig_opts = self.get_meta()
636         seen = {}
637         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
638         for field_name in field_names:
639             parts = field_name.split(LOOKUP_SEP)
640             cur_model = self.model._meta.concrete_model
641             opts = orig_opts
642             for name in parts[:-1]:
643                 old_model = cur_model
644                 source = opts.get_field(name)
645                 if is_reverse_o2o(source):
646                     cur_model = source.related_model
647                 else:
648                     cur_model = source.remote_field.model
649                 opts = cur_model._meta
650                 # Even if we're "just passing through" this model, we must add
651                 # both the current model's pk and the related reference field
652                 # (if it's not a reverse relation) to the things we select.
653                 if not is_reverse_o2o(source):
654                     must_include[old_model].add(source)
655                 add_to_dict(must_include, cur_model, opts.pk)
656             field = opts.get_field(parts[-1])
657             is_reverse_object = field.auto_created and not field.concrete
658             model = field.related_model if is_reverse_object else field.model
659             model = model._meta.concrete_model
660             if model == opts.model:
661                 model = cur_model
662             if not is_reverse_o2o(field):
663                 add_to_dict(seen, model, field)
664 
665         if defer:
666             # We need to load all fields for each model, except those that
667             # appear in "seen" (for all models that appear in "seen"). The only
668             # slight complexity here is handling fields that exist on parent
669             # models.
670             workset = {}
671             for model, values in six.iteritems(seen):
672                 for field in model._meta.fields:
673                     if field in values:
674                         continue
675                     m = field.model._meta.concrete_model
676                     add_to_dict(workset, m, field)
677             for model, values in six.iteritems(must_include):
678                 # If we haven't included a model in workset, we don't add the
679                 # corresponding must_include fields for that model, since an
680                 # empty set means "include all fields". That's why there's no
681                 # "else" branch here.
682                 if model in workset:
683                     workset[model].update(values)
684             for model, values in six.iteritems(workset):
685                 callback(target, model, values)
686         else:
687             for model, values in six.iteritems(must_include):
688                 if model in seen:
689                     seen[model].update(values)
690                 else:
691                     # As we've passed through this model, but not explicitly
692                     # included any fields, we have to make sure it's mentioned
693                     # so that only the "must include" fields are pulled in.
694                     seen[model] = values
695             # Now ensure that every model in the inheritance chain is mentioned
696             # in the parent list. Again, it must be mentioned to ensure that
697             # only "must include" fields are pulled in.
698             for model in orig_opts.get_parent_list():
699                 if model not in seen:
700                     seen[model] = set()
701             for model, values in six.iteritems(seen):
702                 callback(target, model, values)
703 
704     def table_alias(self, table_name, create=False):
705         """
706         Returns a table alias for the given table_name and whether this is a
707         new alias or not.
708 
709         If 'create' is true, a new alias is always created. Otherwise, the
710         most recently created alias for the table (if one exists) is reused.
711         """
712         alias_list = self.table_map.get(table_name)
713         if not create and alias_list:
714             alias = alias_list[0]
715             self.alias_refcount[alias] += 1
716             return alias, False
717 
718         # Create a new alias for this table.
719         if alias_list:
720             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
721             alias_list.append(alias)
722         else:
723             # The first occurrence of a table uses the table name directly.
724             alias = table_name
725             self.table_map[alias] = [alias]
726         self.alias_refcount[alias] = 1
727         self.tables.append(alias)
728         return alias, True
729 
730     def ref_alias(self, alias):
731         """ Increases the reference count for this alias. """
732         self.alias_refcount[alias] += 1
733 
734     def unref_alias(self, alias, amount=1):
735         """ Decreases the reference count for this alias. """
736         self.alias_refcount[alias] -= amount
737 
738     def promote_joins(self, aliases):
739         """
740         Promotes recursively the join type of given aliases and its children to
741         an outer join. If 'unconditional' is False, the join is only promoted if
742         it is nullable or the parent join is an outer join.
743 
744         The children promotion is done to avoid join chains that contain a LOUTER
745         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
746         then we must also promote b->c automatically, or otherwise the promotion
747         of a->b doesn't actually change anything in the query results.
748         """
749         aliases = list(aliases)
750         while aliases:
751             alias = aliases.pop(0)
752             if self.alias_map[alias].join_type is None:
753                 # This is the base table (first FROM entry) - this table
754                 # isn't really joined at all in the query, so we should not
755                 # alter its join type.
756                 continue
757             # Only the first alias (skipped above) should have None join_type
758             assert self.alias_map[alias].join_type is not None
759             parent_alias = self.alias_map[alias].parent_alias
760             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
761             already_louter = self.alias_map[alias].join_type == LOUTER
762             if ((self.alias_map[alias].nullable or parent_louter) and
763                     not already_louter):
764                 self.alias_map[alias] = self.alias_map[alias].promote()
765                 # Join type of 'alias' changed, so re-examine all aliases that
766                 # refer to this one.
767                 aliases.extend(
768                     join for join in self.alias_map.keys()
769                     if self.alias_map[join].parent_alias == alias and join not in aliases
770                 )
771 
772     def demote_joins(self, aliases):
773         """
774         Change join type from LOUTER to INNER for all joins in aliases.
775 
776         Similarly to promote_joins(), this method must ensure no join chains
777         containing first an outer, then an inner join are generated. If we
778         are demoting b->c join in chain a LOUTER b LOUTER c then we must
779         demote a->b automatically, or otherwise the demotion of b->c doesn't
780         actually change anything in the query results. .
781         """
782         aliases = list(aliases)
783         while aliases:
784             alias = aliases.pop(0)
785             if self.alias_map[alias].join_type == LOUTER:
786                 self.alias_map[alias] = self.alias_map[alias].demote()
787                 parent_alias = self.alias_map[alias].parent_alias
788                 if self.alias_map[parent_alias].join_type == INNER:
789                     aliases.append(parent_alias)
790 
791     def reset_refcounts(self, to_counts):
792         """
793         This method will reset reference counts for aliases so that they match
794         the value passed in :param to_counts:.
795         """
796         for alias, cur_refcount in self.alias_refcount.copy().items():
797             unref_amount = cur_refcount - to_counts.get(alias, 0)
798             self.unref_alias(alias, unref_amount)
799 
800     def change_aliases(self, change_map):
801         """
802         Changes the aliases in change_map (which maps old-alias -> new-alias),
803         relabelling any references to them in select columns and the where
804         clause.
805         """
806         assert set(change_map.keys()).intersection(set(change_map.values())) == set()
807 
808         # 1. Update references in "select" (normal columns plus aliases),
809         # "group by" and "where".
810         self.where.relabel_aliases(change_map)
811         if isinstance(self.group_by, list):
812             self.group_by = [col.relabeled_clone(change_map) for col in self.group_by]
813         self.select = [col.relabeled_clone(change_map) for col in self.select]
814         if self._annotations:
815             self._annotations = OrderedDict(
816                 (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items())
817 
818         # 2. Rename the alias in the internal table/alias datastructures.
819         for old_alias, new_alias in six.iteritems(change_map):
820             if old_alias not in self.alias_map:
821                 continue
822             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
823             self.alias_map[new_alias] = alias_data
824             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
825             del self.alias_refcount[old_alias]
826             del self.alias_map[old_alias]
827 
828             table_aliases = self.table_map[alias_data.table_name]
829             for pos, alias in enumerate(table_aliases):
830                 if alias == old_alias:
831                     table_aliases[pos] = new_alias
832                     break
833         self.external_aliases = {change_map.get(alias, alias)
834                                  for alias in self.external_aliases}
835 
836     def bump_prefix(self, outer_query):
837         """
838         Changes the alias prefix to the next letter in the alphabet in a way
839         that the outer query's aliases and this query's aliases will not
840         conflict. Even tables that previously had no alias will get an alias
841         after this call.
842         """
843         def prefix_gen():
844             """
845             Generates a sequence of characters in alphabetical order:
846                 -> 'A', 'B', 'C', ...
847 
848             When the alphabet is finished, the sequence will continue with the
849             Cartesian product:
850                 -> 'AA', 'AB', 'AC', ...
851             """
852             alphabet = ascii_uppercase
853             prefix = chr(ord(self.alias_prefix) + 1)
854             yield prefix
855             for n in count(1):
856                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
857                 for s in product(seq, repeat=n):
858                     yield ''.join(s)
859                 prefix = None
860 
861         if self.alias_prefix != outer_query.alias_prefix:
862             # No clashes between self and outer query should be possible.
863             return
864 
865         local_recursion_limit = 127  # explicitly avoid infinite loop
866         for pos, prefix in enumerate(prefix_gen()):
867             if prefix not in self.subq_aliases:
868                 self.alias_prefix = prefix
869                 break
870             if pos > local_recursion_limit:
871                 raise RuntimeError(
872                     'Maximum recursion depth exceeded: too many subqueries.'
873                 )
874         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
875         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
876         change_map = OrderedDict()
877         for pos, alias in enumerate(self.tables):
878             new_alias = '%s%d' % (self.alias_prefix, pos)
879             change_map[alias] = new_alias
880             self.tables[pos] = new_alias
881         self.change_aliases(change_map)
882 
883     def get_initial_alias(self):
884         """
885         Returns the first alias for this query, after increasing its reference
886         count.
887         """
888         if self.tables:
889             alias = self.tables[0]
890             self.ref_alias(alias)
891         else:
892             alias = self.join(BaseTable(self.get_meta().db_table, None))
893         return alias
894 
895     def count_active_tables(self):
896         """
897         Returns the number of tables in this query with a non-zero reference
898         count. Note that after execution, the reference counts are zeroed, so
899         tables added in compiler will not be seen by this method.
900         """
901         return len([1 for count in self.alias_refcount.values() if count])
902 
903     def join(self, join, reuse=None):
904         """
905         Return an alias for the 'join', either reusing an existing alias for
906         that join or creating a new one. 'join' is either a
907         sql.datastructures.BaseTable or Join.
908 
909         The 'reuse' parameter can be either None which means all joins are
910         reusable, or it can be a set containing the aliases that can be reused.
911 
912         A join is always created as LOUTER if the lhs alias is LOUTER to make
913         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
914         joins are created as LOUTER if the join is nullable.
915         """
916         reuse = [a for a, j in self.alias_map.items()
917                  if (reuse is None or a in reuse) and j == join]
918         if reuse:
919             self.ref_alias(reuse[0])
920             return reuse[0]
921 
922         # No reuse is possible, so we need a new alias.
923         alias, _ = self.table_alias(join.table_name, create=True)
924         if join.join_type:
925             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
926                 join_type = LOUTER
927             else:
928                 join_type = INNER
929             join.join_type = join_type
930         join.table_alias = alias
931         self.alias_map[alias] = join
932         return alias
933 
934     def join_parent_model(self, opts, model, alias, seen):
935         """
936         Makes sure the given 'model' is joined in the query. If 'model' isn't
937         a parent of 'opts' or if it is None this method is a no-op.
938 
939         The 'alias' is the root alias for starting the join, 'seen' is a dict
940         of model -> alias of existing joins. It must also contain a mapping
941         of None -> some alias. This will be returned in the no-op case.
942         """
943         if model in seen:
944             return seen[model]
945         chain = opts.get_base_chain(model)
946         if not chain:
947             return alias
948         curr_opts = opts
949         for int_model in chain:
950             if int_model in seen:
951                 curr_opts = int_model._meta
952                 alias = seen[int_model]
953                 continue
954             # Proxy model have elements in base chain
955             # with no parents, assign the new options
956             # object and skip to the next base in that
957             # case
958             if not curr_opts.parents[int_model]:
959                 curr_opts = int_model._meta
960                 continue
961             link_field = curr_opts.get_ancestor_link(int_model)
962             _, _, _, joins, _ = self.setup_joins(
963                 [link_field.name], curr_opts, alias)
964             curr_opts = int_model._meta
965             alias = seen[int_model] = joins[-1]
966         return alias or seen[None]
967 
968     def add_annotation(self, annotation, alias, is_summary=False):
969         """
970         Adds a single annotation expression to the Query
971         """
972         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
973                                                    summarize=is_summary)
974         self.append_annotation_mask([alias])
975         self.annotations[alias] = annotation
976 
977     def _prepare_as_filter_value(self):
978         return self.clone()
979 
980     def prepare_lookup_value(self, value, lookups, can_reuse, allow_joins=True):
981         # Default lookup if none given is exact.
982         used_joins = []
983         if len(lookups) == 0:
984             lookups = ['exact']
985         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
986         # uses of None as a query value.
987         if value is None:
988             if lookups[-1] not in ('exact', 'iexact'):
989                 raise ValueError("Cannot use None as a query value")
990             return True, ['isnull'], used_joins
991         elif hasattr(value, 'resolve_expression'):
992             pre_joins = self.alias_refcount.copy()
993             value = value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
994             used_joins = [k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)]
995         elif isinstance(value, (list, tuple)):
996             # The items of the iterable may be expressions and therefore need
997             # to be resolved independently.
998             processed_values = []
999             used_joins = set()
1000             for sub_value in value:
1001                 if hasattr(sub_value, 'resolve_expression'):
1002                     pre_joins = self.alias_refcount.copy()
1003                     processed_values.append(
1004                         sub_value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
1005                     )
1006                     # The used_joins for a tuple of expressions is the union of
1007                     # the used_joins for the individual expressions.
1008                     used_joins |= set(k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0))
1009         # Subqueries need to use a different set of aliases than the
1010         # outer query. Call bump_prefix to change aliases of the inner
1011         # query (the value).
1012         if hasattr(value, '_prepare_as_filter_value'):
1013             value = value._prepare_as_filter_value()
1014             value.bump_prefix(self)
1015         # For Oracle '' is equivalent to null. The check needs to be done
1016         # at this stage because join promotion can't be done at compiler
1017         # stage. Using DEFAULT_DB_ALIAS isn't nice, but it is the best we
1018         # can do here. Similar thing is done in is_nullable(), too.
1019         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1020                 lookups[-1] == 'exact' and value == ''):
1021             value = True
1022             lookups[-1] = 'isnull'
1023         return value, lookups, used_joins
1024 
1025     def solve_lookup_type(self, lookup):
1026         """
1027         Solve the lookup type from the lookup (eg: 'foobar__id__icontains')
1028         """
1029         lookup_splitted = lookup.split(LOOKUP_SEP)
1030         if self._annotations:
1031             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1032             if expression:
1033                 return expression_lookups, (), expression
1034         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1035         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1036         if len(lookup_parts) == 0:
1037             lookup_parts = ['exact']
1038         elif len(lookup_parts) > 1:
1039             if not field_parts:
1040                 raise FieldError(
1041                     'Invalid lookup "%s" for model %s".' %
1042                     (lookup, self.get_meta().model.__name__))
1043         return lookup_parts, field_parts, False
1044 
1045     def check_query_object_type(self, value, opts, field):
1046         """
1047         Checks whether the object passed while querying is of the correct type.
1048         If not, it raises a ValueError specifying the wrong object.
1049         """
1050         if hasattr(value, '_meta'):
1051             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1052                 raise ValueError(
1053                     'Cannot query "%s": Must be "%s" instance.' %
1054                     (value, opts.object_name))
1055 
1056     def check_related_objects(self, field, value, opts):
1057         """
1058         Checks the type of object passed to query relations.
1059         """
1060         if field.is_relation:
1061             # Check that the field and the queryset use the same model in a
1062             # query like .filter(author=Author.objects.all()). For example, the
1063             # opts would be Author's (from the author field) and value.model
1064             # would be Author.objects.all() queryset's .model (Author also).
1065             # The field is the related field on the lhs side.
1066             # If _forced_pk isn't set, this isn't a queryset query or values()
1067             # or values_list() was specified by the developer in which case
1068             # that choice is trusted.
1069             if (getattr(value, '_forced_pk', False) and
1070                     not check_rel_lookup_compatibility(value.model, opts, field)):
1071                 raise ValueError(
1072                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1073                     (value.model._meta.object_name, opts.object_name)
1074                 )
1075             elif hasattr(value, '_meta'):
1076                 self.check_query_object_type(value, opts, field)
1077             elif hasattr(value, '__iter__'):
1078                 for v in value:
1079                     self.check_query_object_type(v, opts, field)
1080 
1081     def build_lookup(self, lookups, lhs, rhs):
1082         """
1083         Tries to extract transforms and lookup from given lhs.
1084 
1085         The lhs value is something that works like SQLExpression.
1086         The rhs value is what the lookup is going to compare against.
1087         The lookups is a list of names to extract using get_lookup()
1088         and get_transform().
1089         """
1090         lookups = lookups[:]
1091         while lookups:
1092             name = lookups[0]
1093             # If there is just one part left, try first get_lookup() so
1094             # that if the lhs supports both transform and lookup for the
1095             # name, then lookup will be picked.
1096             if len(lookups) == 1:
1097                 final_lookup = lhs.get_lookup(name)
1098                 if not final_lookup:
1099                     # We didn't find a lookup. We are going to interpret
1100                     # the name as transform, and do an Exact lookup against
1101                     # it.
1102                     lhs = self.try_transform(lhs, name, lookups)
1103                     final_lookup = lhs.get_lookup('exact')
1104                 return final_lookup(lhs, rhs)
1105             lhs = self.try_transform(lhs, name, lookups)
1106             lookups = lookups[1:]
1107 
1108     def try_transform(self, lhs, name, rest_of_lookups):
1109         """
1110         Helper method for build_lookup. Tries to fetch and initialize
1111         a transform for name parameter from lhs.
1112         """
1113         transform_class = lhs.get_transform(name)
1114         if transform_class:
1115             return transform_class(lhs)
1116         else:
1117             raise FieldError(
1118                 "Unsupported lookup '%s' for %s or join on the field not "
1119                 "permitted." %
1120                 (name, lhs.output_field.__class__.__name__))
1121 
1122     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1123                      can_reuse=None, connector=AND, allow_joins=True, split_subq=True):
1124         """
1125         Builds a WhereNode for a single filter clause, but doesn't add it
1126         to this Query. Query.add_q() will then add this filter to the where
1127         Node.
1128 
1129         The 'branch_negated' tells us if the current branch contains any
1130         negations. This will be used to determine if subqueries are needed.
1131 
1132         The 'current_negated' is used to determine if the current filter is
1133         negated or not and this will be used to determine if IS NULL filtering
1134         is needed.
1135 
1136         The difference between current_netageted and branch_negated is that
1137         branch_negated is set on first negation, but current_negated is
1138         flipped for each negation.
1139 
1140         Note that add_filter will not do any negating itself, that is done
1141         upper in the code by add_q().
1142 
1143         The 'can_reuse' is a set of reusable joins for multijoins.
1144 
1145         The method will create a filter clause that can be added to the current
1146         query. However, if the filter isn't added to the query then the caller
1147         is responsible for unreffing the joins used.
1148         """
1149         if isinstance(filter_expr, dict):
1150             raise FieldError("Cannot parse keyword query as dict")
1151         arg, value = filter_expr
1152         if not arg:
1153             raise FieldError("Cannot parse keyword query %r" % arg)
1154         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1155         if not allow_joins and len(parts) > 1:
1156             raise FieldError("Joined field references are not permitted in this query")
1157 
1158         # Work out the lookup type and remove it from the end of 'parts',
1159         # if necessary.
1160         value, lookups, used_joins = self.prepare_lookup_value(value, lookups, can_reuse, allow_joins)
1161 
1162         clause = self.where_class()
1163         if reffed_expression:
1164             condition = self.build_lookup(lookups, reffed_expression, value)
1165             clause.add(condition, AND)
1166             return clause, []
1167 
1168         opts = self.get_meta()
1169         alias = self.get_initial_alias()
1170         allow_many = not branch_negated or not split_subq
1171 
1172         try:
1173             field, sources, opts, join_list, path = self.setup_joins(
1174                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many)
1175 
1176             # Prevent iterator from being consumed by check_related_objects()
1177             if isinstance(value, Iterator):
1178                 value = list(value)
1179             self.check_related_objects(field, value, opts)
1180 
1181             # split_exclude() needs to know which joins were generated for the
1182             # lookup parts
1183             self._lookup_joins = join_list
1184         except MultiJoin as e:
1185             return self.split_exclude(filter_expr, LOOKUP_SEP.join(parts[:e.level]),
1186                                       can_reuse, e.names_with_path)
1187 
1188         if can_reuse is not None:
1189             can_reuse.update(join_list)
1190         used_joins = set(used_joins).union(set(join_list))
1191         targets, alias, join_list = self.trim_joins(sources, join_list, path)
1192 
1193         if field.is_relation:
1194             # No support for transforms for relational fields
1195             num_lookups = len(lookups)
1196             if num_lookups > 1:
1197                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1198             assert num_lookups > 0  # Likely a bug in Django if this fails.
1199             lookup_class = field.get_lookup(lookups[0])
1200             if lookup_class is None:
1201                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1202             if len(targets) == 1:
1203                 lhs = targets[0].get_col(alias, field)
1204             else:
1205                 lhs = MultiColSource(alias, targets, sources, field)
1206             condition = lookup_class(lhs, value)
1207             lookup_type = lookup_class.lookup_name
1208         else:
1209             col = targets[0].get_col(alias, field)
1210             condition = self.build_lookup(lookups, col, value)
1211             lookup_type = condition.lookup_name
1212 
1213         clause.add(condition, AND)
1214 
1215         require_outer = lookup_type == 'isnull' and value is True and not current_negated
1216         if current_negated and (lookup_type != 'isnull' or value is False):
1217             require_outer = True
1218             if (lookup_type != 'isnull' and (
1219                     self.is_nullable(targets[0]) or
1220                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1221                 # The condition added here will be SQL like this:
1222                 # NOT (col IS NOT NULL), where the first NOT is added in
1223                 # upper layers of code. The reason for addition is that if col
1224                 # is null, then col != someval will result in SQL "unknown"
1225                 # which isn't the same as in Python. The Python None handling
1226                 # is wanted, and it can be gotten by
1227                 # (col IS NULL OR col != someval)
1228                 #   <=>
1229                 # NOT (col IS NOT NULL AND col = someval).
1230                 lookup_class = targets[0].get_lookup('isnull')
1231                 clause.add(lookup_class(targets[0].get_col(alias, sources[0]), False), AND)
1232         return clause, used_joins if not require_outer else ()
1233 
1234     def add_filter(self, filter_clause):
1235         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1236 
1237     def add_q(self, q_object):
1238         """
1239         A preprocessor for the internal _add_q(). Responsible for doing final
1240         join promotion.
1241         """
1242         # For join promotion this case is doing an AND for the added q_object
1243         # and existing conditions. So, any existing inner join forces the join
1244         # type to remain inner. Existing outer joins can however be demoted.
1245         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1246         # rel_a doesn't produce any rows, then the whole condition must fail.
1247         # So, demotion is OK.
1248         existing_inner = set(
1249             (a for a in self.alias_map if self.alias_map[a].join_type == INNER))
1250         clause, _ = self._add_q(q_object, self.used_aliases)
1251         if clause:
1252             self.where.add(clause, AND)
1253         self.demote_joins(existing_inner)
1254 
1255     def _add_q(self, q_object, used_aliases, branch_negated=False,
1256                current_negated=False, allow_joins=True, split_subq=True):
1257         """
1258         Adds a Q-object to the current filter.
1259         """
1260         connector = q_object.connector
1261         current_negated = current_negated ^ q_object.negated
1262         branch_negated = branch_negated or q_object.negated
1263         target_clause = self.where_class(connector=connector,
1264                                          negated=q_object.negated)
1265         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1266         for child in q_object.children:
1267             if isinstance(child, Node):
1268                 child_clause, needed_inner = self._add_q(
1269                     child, used_aliases, branch_negated,
1270                     current_negated, allow_joins, split_subq)
1271                 joinpromoter.add_votes(needed_inner)
1272             else:
1273                 child_clause, needed_inner = self.build_filter(
1274                     child, can_reuse=used_aliases, branch_negated=branch_negated,
1275                     current_negated=current_negated, connector=connector,
1276                     allow_joins=allow_joins, split_subq=split_subq,
1277                 )
1278                 joinpromoter.add_votes(needed_inner)
1279             if child_clause:
1280                 target_clause.add(child_clause, connector)
1281         needed_inner = joinpromoter.update_join_types(self)
1282         return target_clause, needed_inner
1283 
1284     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1285         """
1286         Walks the list of names and turns them into PathInfo tuples. Note that
1287         a single name in 'names' can generate multiple PathInfos (m2m for
1288         example).
1289 
1290         'names' is the path of names to travel, 'opts' is the model Options we
1291         start the name resolving from, 'allow_many' is as for setup_joins().
1292         If fail_on_missing is set to True, then a name that can't be resolved
1293         will generate a FieldError.
1294 
1295         Returns a list of PathInfo tuples. In addition returns the final field
1296         (the last used join field), and target (which is a field guaranteed to
1297         contain the same value as the final field). Finally, the method returns
1298         those names that weren't found (which are likely transforms and the
1299         final lookup).
1300         """
1301         path, names_with_path = [], []
1302         for pos, name in enumerate(names):
1303             cur_names_with_path = (name, [])
1304             if name == 'pk':
1305                 name = opts.pk.name
1306 
1307             field = None
1308             try:
1309                 field = opts.get_field(name)
1310             except FieldDoesNotExist:
1311                 if name in self.annotation_select:
1312                     field = self.annotation_select[name].output_field
1313                 elif pos == 0:
1314                     for rel in opts.related_objects:
1315                         if (name == rel.related_model._meta.model_name and
1316                                 rel.related_name == rel.related_model._meta.default_related_name):
1317                             related_name = rel.related_name
1318                             field = opts.get_field(related_name)
1319                             warnings.warn(
1320                                 "Query lookup '%s' is deprecated in favor of "
1321                                 "Meta.default_related_name '%s'."
1322                                 % (name, related_name),
1323                                 RemovedInDjango20Warning, 2
1324                             )
1325                             break
1326 
1327             if field is not None:
1328                 # Fields that contain one-to-many relations with a generic
1329                 # model (like a GenericForeignKey) cannot generate reverse
1330                 # relations and therefore cannot be used for reverse querying.
1331                 if field.is_relation and not field.related_model:
1332                     raise FieldError(
1333                         "Field %r does not generate an automatic reverse "
1334                         "relation and therefore cannot be used for reverse "
1335                         "querying. If it is a GenericForeignKey, consider "
1336                         "adding a GenericRelation." % name
1337                     )
1338                 try:
1339                     model = field.model._meta.concrete_model
1340                 except AttributeError:
1341                     # QuerySet.annotate() may introduce fields that aren't
1342                     # attached to a model.
1343                     model = None
1344             else:
1345                 # We didn't find the current field, so move position back
1346                 # one step.
1347                 pos -= 1
1348                 if pos == -1 or fail_on_missing:
1349                     field_names = list(get_field_names_from_opts(opts))
1350                     available = sorted(field_names + list(self.annotation_select))
1351                     raise FieldError("Cannot resolve keyword '%s' into field. "
1352                                      "Choices are: %s" % (name, ", ".join(available)))
1353                 break
1354             # Check if we need any joins for concrete inheritance cases (the
1355             # field lives in parent, but we are currently in one of its
1356             # children)
1357             if model is not opts.model:
1358                 path_to_parent = opts.get_path_to_parent(model)
1359                 if path_to_parent:
1360                     path.extend(path_to_parent)
1361                     cur_names_with_path[1].extend(path_to_parent)
1362                     opts = path_to_parent[-1].to_opts
1363             if hasattr(field, 'get_path_info'):
1364                 pathinfos = field.get_path_info()
1365                 if not allow_many:
1366                     for inner_pos, p in enumerate(pathinfos):
1367                         if p.m2m:
1368                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1369                             names_with_path.append(cur_names_with_path)
1370                             raise MultiJoin(pos + 1, names_with_path)
1371                 last = pathinfos[-1]
1372                 path.extend(pathinfos)
1373                 final_field = last.join_field
1374                 opts = last.to_opts
1375                 targets = last.target_fields
1376                 cur_names_with_path[1].extend(pathinfos)
1377                 names_with_path.append(cur_names_with_path)
1378             else:
1379                 # Local non-relational field.
1380                 final_field = field
1381                 targets = (field,)
1382                 if fail_on_missing and pos + 1 != len(names):
1383                     raise FieldError(
1384                         "Cannot resolve keyword %r into field. Join on '%s'"
1385                         " not permitted." % (names[pos + 1], name))
1386                 break
1387         return path, final_field, targets, names[pos + 1:]
1388 
1389     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True):
1390         """
1391         Compute the necessary table joins for the passage through the fields
1392         given in 'names'. 'opts' is the Options class for the current model
1393         (which gives the table we are starting from), 'alias' is the alias for
1394         the table to start the joining from.
1395 
1396         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1397         can be None in which case all joins are reusable or a set of aliases
1398         that can be reused. Note that non-reverse foreign keys are always
1399         reusable when using setup_joins().
1400 
1401         If 'allow_many' is False, then any reverse foreign key seen will
1402         generate a MultiJoin exception.
1403 
1404         Returns the final field involved in the joins, the target field (used
1405         for any 'where' constraint), the final 'opts' value, the joins and the
1406         field path travelled to generate the joins.
1407 
1408         The target field is the field containing the concrete value. Final
1409         field can be something different, for example foreign key pointing to
1410         that value. Final field is needed for example in some value
1411         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1412         key field for example).
1413         """
1414         joins = [alias]
1415         # First, generate the path for the names
1416         path, final_field, targets, rest = self.names_to_path(
1417             names, opts, allow_many, fail_on_missing=True)
1418 
1419         # Then, add the path to the query's joins. Note that we can't trim
1420         # joins at this stage - we will need the information about join type
1421         # of the trimmed joins.
1422         for join in path:
1423             opts = join.to_opts
1424             if join.direct:
1425                 nullable = self.is_nullable(join.join_field)
1426             else:
1427                 nullable = True
1428             connection = Join(opts.db_table, alias, None, INNER, join.join_field, nullable)
1429             reuse = can_reuse if join.m2m else None
1430             alias = self.join(connection, reuse=reuse)
1431             joins.append(alias)
1432         return final_field, targets, opts, joins, path
1433 
1434     def trim_joins(self, targets, joins, path):
1435         """
1436         The 'target' parameter is the final field being joined to, 'joins'
1437         is the full list of join aliases. The 'path' contain the PathInfos
1438         used to create the joins.
1439 
1440         Returns the final target field and table alias and the new active
1441         joins.
1442 
1443         We will always trim any direct join if we have the target column
1444         available already in the previous table. Reverse joins can't be
1445         trimmed as we don't know if there is anything on the other side of
1446         the join.
1447         """
1448         joins = joins[:]
1449         for pos, info in enumerate(reversed(path)):
1450             if len(joins) == 1 or not info.direct:
1451                 break
1452             join_targets = set(t.column for t in info.join_field.foreign_related_fields)
1453             cur_targets = set(t.column for t in targets)
1454             if not cur_targets.issubset(join_targets):
1455                 break
1456             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1457             targets = tuple(targets_dict[t.column] for t in targets)
1458             self.unref_alias(joins.pop())
1459         return targets, joins[-1], joins
1460 
1461     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1462         if not allow_joins and LOOKUP_SEP in name:
1463             raise FieldError("Joined field references are not permitted in this query")
1464         if name in self.annotations:
1465             if summarize:
1466                 # Summarize currently means we are doing an aggregate() query
1467                 # which is executed as a wrapped subquery if any of the
1468                 # aggregate() elements reference an existing annotation. In
1469                 # that case we need to return a Ref to the subquery's annotation.
1470                 return Ref(name, self.annotation_select[name])
1471             else:
1472                 return self.annotation_select[name]
1473         else:
1474             field_list = name.split(LOOKUP_SEP)
1475             field, sources, opts, join_list, path = self.setup_joins(
1476                 field_list, self.get_meta(),
1477                 self.get_initial_alias(), reuse)
1478             targets, _, join_list = self.trim_joins(sources, join_list, path)
1479             if len(targets) > 1:
1480                 raise FieldError("Referencing multicolumn fields with F() objects "
1481                                  "isn't supported")
1482             if reuse is not None:
1483                 reuse.update(join_list)
1484             col = targets[0].get_col(join_list[-1], sources[0])
1485             return col
1486 
1487     def split_exclude(self, filter_expr, prefix, can_reuse, names_with_path):
1488         """
1489         When doing an exclude against any kind of N-to-many relation, we need
1490         to use a subquery. This method constructs the nested query, given the
1491         original exclude filter (filter_expr) and the portion up to the first
1492         N-to-many relation field.
1493 
1494         As an example we could have original filter ~Q(child__name='foo').
1495         We would get here with filter_expr = child__name, prefix = child and
1496         can_reuse is a set of joins usable for filters in the original query.
1497 
1498         We will turn this into equivalent of:
1499             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1500                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1501 
1502         It might be worth it to consider using WHERE NOT EXISTS as that has
1503         saner null handling, and is easier for the backend's optimizer to
1504         handle.
1505         """
1506         # Generate the inner query.
1507         query = Query(self.model)
1508         query.add_filter(filter_expr)
1509         query.clear_ordering(True)
1510         # Try to have as simple as possible subquery -> trim leading joins from
1511         # the subquery.
1512         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1513 
1514         # Add extra check to make sure the selected field will not be null
1515         # since we are adding an IN <subquery> clause. This prevents the
1516         # database from tripping over IN (...,NULL,...) selects and returning
1517         # nothing
1518         col = query.select[0]
1519         select_field = col.target
1520         alias = col.alias
1521         if self.is_nullable(select_field):
1522             lookup_class = select_field.get_lookup('isnull')
1523             lookup = lookup_class(select_field.get_col(alias), False)
1524             query.where.add(lookup, AND)
1525         if alias in can_reuse:
1526             pk = select_field.model._meta.pk
1527             # Need to add a restriction so that outer query's filters are in effect for
1528             # the subquery, too.
1529             query.bump_prefix(self)
1530             lookup_class = select_field.get_lookup('exact')
1531             # Note that the query.select[0].alias is different from alias
1532             # due to bump_prefix above.
1533             lookup = lookup_class(pk.get_col(query.select[0].alias),
1534                                   pk.get_col(alias))
1535             query.where.add(lookup, AND)
1536             query.external_aliases.add(alias)
1537 
1538         condition, needed_inner = self.build_filter(
1539             ('%s__in' % trimmed_prefix, query),
1540             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1541         if contains_louter:
1542             or_null_condition, _ = self.build_filter(
1543                 ('%s__isnull' % trimmed_prefix, True),
1544                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1545             condition.add(or_null_condition, OR)
1546             # Note that the end result will be:
1547             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1548             # This might look crazy but due to how IN works, this seems to be
1549             # correct. If the IS NOT NULL check is removed then outercol NOT
1550             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1551             # outercol IS NULL we will not match the row.
1552         return condition, needed_inner
1553 
1554     def set_empty(self):
1555         self.where.add(NothingNode(), AND)
1556 
1557     def is_empty(self):
1558         return any(isinstance(c, NothingNode) for c in self.where.children)
1559 
1560     def set_limits(self, low=None, high=None):
1561         """
1562         Adjusts the limits on the rows retrieved. We use low/high to set these,
1563         as it makes it more Pythonic to read and write. When the SQL query is
1564         created, they are converted to the appropriate offset and limit values.
1565 
1566         Any limits passed in here are applied relative to the existing
1567         constraints. So low is added to the current low value and both will be
1568         clamped to any existing high value.
1569         """
1570         if high is not None:
1571             if self.high_mark is not None:
1572                 self.high_mark = min(self.high_mark, self.low_mark + high)
1573             else:
1574                 self.high_mark = self.low_mark + high
1575         if low is not None:
1576             if self.high_mark is not None:
1577                 self.low_mark = min(self.high_mark, self.low_mark + low)
1578             else:
1579                 self.low_mark = self.low_mark + low
1580 
1581         if self.low_mark == self.high_mark:
1582             self.set_empty()
1583 
1584     def clear_limits(self):
1585         """
1586         Clears any existing limits.
1587         """
1588         self.low_mark, self.high_mark = 0, None
1589 
1590     def can_filter(self):
1591         """
1592         Returns True if adding filters to this instance is still possible.
1593 
1594         Typically, this means no limits or offsets have been put on the results.
1595         """
1596         return not self.low_mark and self.high_mark is None
1597 
1598     def clear_select_clause(self):
1599         """
1600         Removes all fields from SELECT clause.
1601         """
1602         self.select = []
1603         self.default_cols = False
1604         self.select_related = False
1605         self.set_extra_mask(())
1606         self.set_annotation_mask(())
1607 
1608     def clear_select_fields(self):
1609         """
1610         Clears the list of fields to select (but not extra_select columns).
1611         Some queryset types completely replace any existing list of select
1612         columns.
1613         """
1614         self.select = []
1615         self.values_select = []
1616 
1617     def add_select(self, col):
1618         self.default_cols = False
1619         self.select.append(col)
1620 
1621     def set_select(self, cols):
1622         self.default_cols = False
1623         self.select = cols
1624 
1625     def add_distinct_fields(self, *field_names):
1626         """
1627         Adds and resolves the given fields to the query's "distinct on" clause.
1628         """
1629         self.distinct_fields = field_names
1630         self.distinct = True
1631 
1632     def add_fields(self, field_names, allow_m2m=True):
1633         """
1634         Adds the given (model) fields to the select set. The field names are
1635         added in the order specified.
1636         """
1637         alias = self.get_initial_alias()
1638         opts = self.get_meta()
1639 
1640         try:
1641             for name in field_names:
1642                 # Join promotion note - we must not remove any rows here, so
1643                 # if there is no existing joins, use outer join.
1644                 _, targets, _, joins, path = self.setup_joins(
1645                     name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1646                 targets, final_alias, joins = self.trim_joins(targets, joins, path)
1647                 for target in targets:
1648                     self.add_select(target.get_col(final_alias))
1649         except MultiJoin:
1650             raise FieldError("Invalid field name: '%s'" % name)
1651         except FieldError:
1652             if LOOKUP_SEP in name:
1653                 # For lookups spanning over relationships, show the error
1654                 # from the model on which the lookup failed.
1655                 raise
1656             else:
1657                 names = sorted(list(get_field_names_from_opts(opts)) + list(self.extra) + list(self.annotation_select))
1658                 raise FieldError("Cannot resolve keyword %r into field. "
1659                                  "Choices are: %s" % (name, ", ".join(names)))
1660 
1661     def add_ordering(self, *ordering):
1662         """
1663         Adds items from the 'ordering' sequence to the query's "order by"
1664         clause. These items are either field names (not column names) --
1665         possibly with a direction prefix ('-' or '?') -- or OrderBy
1666         expressions.
1667 
1668         If 'ordering' is empty, all ordering is cleared from the query.
1669         """
1670         errors = []
1671         for item in ordering:
1672             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
1673                 errors.append(item)
1674             if getattr(item, 'contains_aggregate', False):
1675                 raise FieldError(
1676                     'Using an aggregate in order_by() without also including '
1677                     'it in annotate() is not allowed: %s' % item
1678                 )
1679         if errors:
1680             raise FieldError('Invalid order_by arguments: %s' % errors)
1681         if ordering:
1682             self.order_by.extend(ordering)
1683         else:
1684             self.default_ordering = False
1685 
1686     def clear_ordering(self, force_empty):
1687         """
1688         Removes any ordering settings. If 'force_empty' is True, there will be
1689         no ordering in the resulting query (not even the model's default).
1690         """
1691         self.order_by = []
1692         self.extra_order_by = ()
1693         if force_empty:
1694             self.default_ordering = False
1695 
1696     def set_group_by(self):
1697         """
1698         Expands the GROUP BY clause required by the query.
1699 
1700         This will usually be the set of all non-aggregate fields in the
1701         return data. If the database backend supports grouping by the
1702         primary key, and the query would be equivalent, the optimization
1703         will be made automatically.
1704         """
1705         self.group_by = []
1706 
1707         for col in self.select:
1708             self.group_by.append(col)
1709 
1710         if self.annotation_select:
1711             for alias, annotation in six.iteritems(self.annotation_select):
1712                 for col in annotation.get_group_by_cols():
1713                     self.group_by.append(col)
1714 
1715     def add_select_related(self, fields):
1716         """
1717         Sets up the select_related data structure so that we only select
1718         certain related models (as opposed to all models, when
1719         self.select_related=True).
1720         """
1721         if isinstance(self.select_related, bool):
1722             field_dict = {}
1723         else:
1724             field_dict = self.select_related
1725         for field in fields:
1726             d = field_dict
1727             for part in field.split(LOOKUP_SEP):
1728                 d = d.setdefault(part, {})
1729         self.select_related = field_dict
1730 
1731     def add_extra(self, select, select_params, where, params, tables, order_by):
1732         """
1733         Adds data to the various extra_* attributes for user-created additions
1734         to the query.
1735         """
1736         if select:
1737             # We need to pair any placeholder markers in the 'select'
1738             # dictionary with their parameters in 'select_params' so that
1739             # subsequent updates to the select dictionary also adjust the
1740             # parameters appropriately.
1741             select_pairs = OrderedDict()
1742             if select_params:
1743                 param_iter = iter(select_params)
1744             else:
1745                 param_iter = iter([])
1746             for name, entry in select.items():
1747                 entry = force_text(entry)
1748                 entry_params = []
1749                 pos = entry.find("%s")
1750                 while pos != -1:
1751                     if pos == 0 or entry[pos - 1] != '%':
1752                         entry_params.append(next(param_iter))
1753                     pos = entry.find("%s", pos + 2)
1754                 select_pairs[name] = (entry, entry_params)
1755             # This is order preserving, since self.extra_select is an OrderedDict.
1756             self.extra.update(select_pairs)
1757         if where or params:
1758             self.where.add(ExtraWhere(where, params), AND)
1759         if tables:
1760             self.extra_tables += tuple(tables)
1761         if order_by:
1762             self.extra_order_by = order_by
1763 
1764     def clear_deferred_loading(self):
1765         """
1766         Remove any fields from the deferred loading set.
1767         """
1768         self.deferred_loading = (set(), True)
1769 
1770     def add_deferred_loading(self, field_names):
1771         """
1772         Add the given list of model field names to the set of fields to
1773         exclude from loading from the database when automatic column selection
1774         is done. The new field names are added to any existing field names that
1775         are deferred (or removed from any existing field names that are marked
1776         as the only ones for immediate loading).
1777         """
1778         # Fields on related models are stored in the literal double-underscore
1779         # format, so that we can use a set datastructure. We do the foo__bar
1780         # splitting and handling when computing the SQL column names (as part of
1781         # get_columns()).
1782         existing, defer = self.deferred_loading
1783         if defer:
1784             # Add to existing deferred names.
1785             self.deferred_loading = existing.union(field_names), True
1786         else:
1787             # Remove names from the set of any existing "immediate load" names.
1788             self.deferred_loading = existing.difference(field_names), False
1789 
1790     def add_immediate_loading(self, field_names):
1791         """
1792         Add the given list of model field names to the set of fields to
1793         retrieve when the SQL is executed ("immediate loading" fields). The
1794         field names replace any existing immediate loading field names. If
1795         there are field names already specified for deferred loading, those
1796         names are removed from the new field_names before storing the new names
1797         for immediate loading. (That is, immediate loading overrides any
1798         existing immediate values, but respects existing deferrals.)
1799         """
1800         existing, defer = self.deferred_loading
1801         field_names = set(field_names)
1802         if 'pk' in field_names:
1803             field_names.remove('pk')
1804             field_names.add(self.get_meta().pk.name)
1805 
1806         if defer:
1807             # Remove any existing deferred names from the current set before
1808             # setting the new names.
1809             self.deferred_loading = field_names.difference(existing), False
1810         else:
1811             # Replace any existing "immediate load" field names.
1812             self.deferred_loading = field_names, False
1813 
1814     def get_loaded_field_names(self):
1815         """
1816         If any fields are marked to be deferred, returns a dictionary mapping
1817         models to a set of names in those fields that will be loaded. If a
1818         model is not in the returned dictionary, none of its fields are
1819         deferred.
1820 
1821         If no fields are marked for deferral, returns an empty dictionary.
1822         """
1823         # We cache this because we call this function multiple times
1824         # (compiler.fill_related_selections, query.iterator)
1825         try:
1826             return self._loaded_field_names_cache
1827         except AttributeError:
1828             collection = {}
1829             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
1830             self._loaded_field_names_cache = collection
1831             return collection
1832 
1833     def get_loaded_field_names_cb(self, target, model, fields):
1834         """
1835         Callback used by get_deferred_field_names().
1836         """
1837         target[model] = {f.attname for f in fields}
1838 
1839     def set_annotation_mask(self, names):
1840         "Set the mask of annotations that will actually be returned by the SELECT"
1841         if names is None:
1842             self.annotation_select_mask = None
1843         else:
1844             self.annotation_select_mask = set(names)
1845         self._annotation_select_cache = None
1846 
1847     def append_annotation_mask(self, names):
1848         if self.annotation_select_mask is not None:
1849             self.set_annotation_mask(set(names).union(self.annotation_select_mask))
1850 
1851     def set_extra_mask(self, names):
1852         """
1853         Set the mask of extra select items that will be returned by SELECT,
1854         we don't actually remove them from the Query since they might be used
1855         later
1856         """
1857         if names is None:
1858             self.extra_select_mask = None
1859         else:
1860             self.extra_select_mask = set(names)
1861         self._extra_select_cache = None
1862 
1863     def set_values(self, fields):
1864         self.select_related = False
1865         self.clear_deferred_loading()
1866         self.clear_select_fields()
1867 
1868         if self.group_by is True:
1869             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
1870             self.set_group_by()
1871             self.clear_select_fields()
1872 
1873         if fields:
1874             field_names = []
1875             extra_names = []
1876             annotation_names = []
1877             if not self._extra and not self._annotations:
1878                 # Shortcut - if there are no extra or annotations, then
1879                 # the values() clause must be just field names.
1880                 field_names = list(fields)
1881             else:
1882                 self.default_cols = False
1883                 for f in fields:
1884                     if f in self.extra_select:
1885                         extra_names.append(f)
1886                     elif f in self.annotation_select:
1887                         annotation_names.append(f)
1888                     else:
1889                         field_names.append(f)
1890             self.set_extra_mask(extra_names)
1891             self.set_annotation_mask(annotation_names)
1892         else:
1893             field_names = [f.attname for f in self.model._meta.concrete_fields]
1894 
1895         self.values_select = field_names
1896         self.add_fields(field_names, True)
1897 
1898     @property
1899     def annotation_select(self):
1900         """The OrderedDict of aggregate columns that are not masked, and should
1901         be used in the SELECT clause.
1902 
1903         This result is cached for optimization purposes.
1904         """
1905         if self._annotation_select_cache is not None:
1906             return self._annotation_select_cache
1907         elif not self._annotations:
1908             return {}
1909         elif self.annotation_select_mask is not None:
1910             self._annotation_select_cache = OrderedDict(
1911                 (k, v) for k, v in self.annotations.items()
1912                 if k in self.annotation_select_mask
1913             )
1914             return self._annotation_select_cache
1915         else:
1916             return self.annotations
1917 
1918     @property
1919     def extra_select(self):
1920         if self._extra_select_cache is not None:
1921             return self._extra_select_cache
1922         if not self._extra:
1923             return {}
1924         elif self.extra_select_mask is not None:
1925             self._extra_select_cache = OrderedDict(
1926                 (k, v) for k, v in self.extra.items()
1927                 if k in self.extra_select_mask
1928             )
1929             return self._extra_select_cache
1930         else:
1931             return self.extra
1932 
1933     def trim_start(self, names_with_path):
1934         """
1935         Trims joins from the start of the join path. The candidates for trim
1936         are the PathInfos in names_with_path structure that are m2m joins.
1937 
1938         Also sets the select column so the start matches the join.
1939 
1940         This method is meant to be used for generating the subquery joins &
1941         cols in split_exclude().
1942 
1943         Returns a lookup usable for doing outerq.filter(lookup=self). Returns
1944         also if the joins in the prefix contain a LEFT OUTER join.
1945         _"""
1946         all_paths = []
1947         for _, paths in names_with_path:
1948             all_paths.extend(paths)
1949         contains_louter = False
1950         # Trim and operate only on tables that were generated for
1951         # the lookup part of the query. That is, avoid trimming
1952         # joins generated for F() expressions.
1953         lookup_tables = [t for t in self.tables if t in self._lookup_joins or t == self.tables[0]]
1954         for trimmed_paths, path in enumerate(all_paths):
1955             if path.m2m:
1956                 break
1957             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
1958                 contains_louter = True
1959             alias = lookup_tables[trimmed_paths]
1960             self.unref_alias(alias)
1961         # The path.join_field is a Rel, lets get the other side's field
1962         join_field = path.join_field.field
1963         # Build the filter prefix.
1964         paths_in_prefix = trimmed_paths
1965         trimmed_prefix = []
1966         for name, path in names_with_path:
1967             if paths_in_prefix - len(path) < 0:
1968                 break
1969             trimmed_prefix.append(name)
1970             paths_in_prefix -= len(path)
1971         trimmed_prefix.append(
1972             join_field.foreign_related_fields[0].name)
1973         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
1974         # Lets still see if we can trim the first join from the inner query
1975         # (that is, self). We can't do this for LEFT JOINs because we would
1976         # miss those rows that have nothing on the outer side.
1977         if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type != LOUTER:
1978             select_fields = [r[0] for r in join_field.related_fields]
1979             select_alias = lookup_tables[trimmed_paths + 1]
1980             self.unref_alias(lookup_tables[trimmed_paths])
1981             extra_restriction = join_field.get_extra_restriction(
1982                 self.where_class, None, lookup_tables[trimmed_paths + 1])
1983             if extra_restriction:
1984                 self.where.add(extra_restriction, AND)
1985         else:
1986             # TODO: It might be possible to trim more joins from the start of the
1987             # inner query if it happens to have a longer join chain containing the
1988             # values in select_fields. Lets punt this one for now.
1989             select_fields = [r[1] for r in join_field.related_fields]
1990             select_alias = lookup_tables[trimmed_paths]
1991         # The found starting point is likely a Join instead of a BaseTable reference.
1992         # But the first entry in the query's FROM clause must not be a JOIN.
1993         for table in self.tables:
1994             if self.alias_refcount[table] > 0:
1995                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
1996                 break
1997         self.set_select([f.get_col(select_alias) for f in select_fields])
1998         return trimmed_prefix, contains_louter
1999 
2000     def is_nullable(self, field):
2001         """
2002         A helper to check if the given field should be treated as nullable.
2003 
2004         Some backends treat '' as null and Django treats such fields as
2005         nullable for those backends. In such situations field.null can be
2006         False even if we should treat the field as nullable.
2007         """
2008         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2009         # (nor should it have) knowledge of which connection is going to be
2010         # used. The proper fix would be to defer all decisions where
2011         # is_nullable() is needed to the compiler stage, but that is not easy
2012         # to do currently.
2013         if connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and field.empty_strings_allowed:
2014             return True
2015         else:
2016             return field.null
2017 
2018     def as_subquery_filter(self, db):
2019         self._db = db
2020         self.subquery = True
2021         # It's safe to drop ordering if the queryset isn't using slicing,
2022         # distinct(*fields) or select_for_update().
2023         if (self.low_mark == 0 and self.high_mark is None and
2024                 not self.distinct_fields and
2025                 not self.select_for_update):
2026             self.clear_ordering(True)
2027         return self

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>change_aliases</method>
<code>
800     def change_aliases(self, change_map):
801         """
802         Changes the aliases in change_map (which maps old-alias -> new-alias),
803         relabelling any references to them in select columns and the where
804         clause.
805         """
806         assert set(change_map.keys()).intersection(set(change_map.values())) == set()
807 
808         # 1. Update references in "select" (normal columns plus aliases),
809         # "group by" and "where".
810         self.where.relabel_aliases(change_map)
811         if isinstance(self.group_by, list):
812             self.group_by = [col.relabeled_clone(change_map) for col in self.group_by]
813         self.select = [col.relabeled_clone(change_map) for col in self.select]
814         if self._annotations:
815             self._annotations = OrderedDict(
816                 (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items())
817 
818         # 2. Rename the alias in the internal table/alias datastructures.
819         for old_alias, new_alias in six.iteritems(change_map):
820             if old_alias not in self.alias_map:
821                 continue
822             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
823             self.alias_map[new_alias] = alias_data
824             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
825             del self.alias_refcount[old_alias]
826             del self.alias_map[old_alias]
827 
828             table_aliases = self.table_map[alias_data.table_name]
829             for pos, alias in enumerate(table_aliases):
830                 if alias == old_alias:
831                     table_aliases[pos] = new_alias
832                     break
833         self.external_aliases = {change_map.get(alias, alias)
834                                  for alias in self.external_aliases}

</code><intended_behavior>In the `change_aliases` method, the critical assertion that checks for circular references in `change_map` should be preserved, but the analysis and manipulation of `change_map` may need enhancement. Specifically, the method should implement a more robust mechanism for managing alias changes that inherently avoids the possibility of creating circular references, regardless of the complexity of the query structures or the interactions of combined queries. This might require reevaluating the logic that leads to alias modifications, ensuring that alias changes do not introduce logical inconsistencies or dependencies that break this assertion.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
117 class Query(object):
118     """
119     A single SQL query.
120     """
121 
122     alias_prefix = 'T'
123     subq_aliases = frozenset([alias_prefix])
124     query_terms = QUERY_TERMS
125 
126     compiler = 'SQLCompiler'
127 
128     def __init__(self, model, where=WhereNode):
129         self.model = model
130         self.alias_refcount = {}
131         # alias_map is the most important data structure regarding joins.
132         # It's used for recording which joins exist in the query and what
133         # types they are. The key is the alias of the joined table (possibly
134         # the table name) and the value is a Join-like object (see
135         # sql.datastructures.Join for more information).
136         self.alias_map = {}
137         # Sometimes the query contains references to aliases in outer queries (as
138         # a result of split_exclude). Correct alias quoting needs to know these
139         # aliases too.
140         self.external_aliases = set()
141         self.table_map = {}     # Maps table names to list of aliases.
142         self.default_cols = True
143         self.default_ordering = True
144         self.standard_ordering = True
145         self.used_aliases = set()
146         self.filter_is_sticky = False
147         self.subquery = False
148 
149         # SQL-related attributes
150         # Select and related select clauses are expressions to use in the
151         # SELECT clause of the query.
152         # The select is used for cases where we want to set up the select
153         # clause to contain other than default fields (values(), subqueries...)
154         # Note that annotations go to annotations dictionary.
155         self.select = []
156         self.tables = []    # Aliases in the order they are created.
157         self.where = where()
158         self.where_class = where
159         # The group_by attribute can have one of the following forms:
160         #  - None: no group by at all in the query
161         #  - A list of expressions: group by (at least) those expressions.
162         #    String refs are also allowed for now.
163         #  - True: group by all select fields of the model
164         # See compiler.get_group_by() for details.
165         self.group_by = None
166         self.order_by = []
167         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
168         self.distinct = False
169         self.distinct_fields = []
170         self.select_for_update = False
171         self.select_for_update_nowait = False
172         self.select_for_update_skip_locked = False
173 
174         self.select_related = False
175         # Arbitrary limit for select_related to prevents infinite recursion.
176         self.max_depth = 5
177 
178         # Holds the selects defined by a call to values() or values_list()
179         # excluding annotation_select and extra_select.
180         self.values_select = []
181 
182         # SQL annotation-related attributes
183         # The _annotations will be an OrderedDict when used. Due to the cost
184         # of creating OrderedDict this attribute is created lazily (in
185         # self.annotations property).
186         self._annotations = None  # Maps alias -> Annotation Expression
187         self.annotation_select_mask = None
188         self._annotation_select_cache = None
189 
190         # Set combination attributes
191         self.combinator = None
192         self.combinator_all = False
193         self.combined_queries = ()
194 
195         # These are for extensions. The contents are more or less appended
196         # verbatim to the appropriate clause.
197         # The _extra attribute is an OrderedDict, lazily created similarly to
198         # .annotations
199         self._extra = None  # Maps col_alias -> (col_sql, params).
200         self.extra_select_mask = None
201         self._extra_select_cache = None
202 
203         self.extra_tables = ()
204         self.extra_order_by = ()
205 
206         # A tuple that is a set of model field names and either True, if these
207         # are the fields to defer, or False if these are the only fields to
208         # load.
209         self.deferred_loading = (set(), True)
210 
211         self.context = {}
212 
213     @property
214     def extra(self):
215         if self._extra is None:
216             self._extra = OrderedDict()
217         return self._extra
218 
219     @property
220     def annotations(self):
221         if self._annotations is None:
222             self._annotations = OrderedDict()
223         return self._annotations
224 
225     def __str__(self):
226         """
227         Returns the query as a string of SQL with the parameter values
228         substituted in (use sql_with_params() to see the unsubstituted string).
229 
230         Parameter values won't necessarily be quoted correctly, since that is
231         done by the database interface at execution time.
232         """
233         sql, params = self.sql_with_params()
234         return sql % params
235 
236     def sql_with_params(self):
237         """
238         Returns the query as an SQL string and the parameters that will be
239         substituted into the query.
240         """
241         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
242 
243     def __deepcopy__(self, memo):
244         result = self.clone(memo=memo)
245         memo[id(self)] = result
246         return result
247 
248     def _prepare(self, field):
249         return self
250 
251     def get_compiler(self, using=None, connection=None):
252         if using is None and connection is None:
253             raise ValueError("Need either using or connection")
254         if using:
255             connection = connections[using]
256         return connection.ops.compiler(self.compiler)(self, connection, using)
257 
258     def get_meta(self):
259         """
260         Returns the Options instance (the model._meta) from which to start
261         processing. Normally, this is self.model._meta, but it can be changed
262         by subclasses.
263         """
264         return self.model._meta
265 
266     def clone(self, klass=None, memo=None, **kwargs):
267         """
268         Creates a copy of the current instance. The 'kwargs' parameter can be
269         used by clients to update attributes after copying has taken place.
270         """
271         obj = Empty()
272         obj.__class__ = klass or self.__class__
273         obj.model = self.model
274         obj.alias_refcount = self.alias_refcount.copy()
275         obj.alias_map = self.alias_map.copy()
276         obj.external_aliases = self.external_aliases.copy()
277         obj.table_map = self.table_map.copy()
278         obj.default_cols = self.default_cols
279         obj.default_ordering = self.default_ordering
280         obj.standard_ordering = self.standard_ordering
281         obj.select = self.select[:]
282         obj.tables = self.tables[:]
283         obj.where = self.where.clone()
284         obj.where_class = self.where_class
285         if self.group_by is None:
286             obj.group_by = None
287         elif self.group_by is True:
288             obj.group_by = True
289         else:
290             obj.group_by = self.group_by[:]
291         obj.order_by = self.order_by[:]
292         obj.low_mark, obj.high_mark = self.low_mark, self.high_mark
293         obj.distinct = self.distinct
294         obj.distinct_fields = self.distinct_fields[:]
295         obj.select_for_update = self.select_for_update
296         obj.select_for_update_nowait = self.select_for_update_nowait
297         obj.select_for_update_skip_locked = self.select_for_update_skip_locked
298         obj.select_related = self.select_related
299         obj.values_select = self.values_select[:]
300         obj._annotations = self._annotations.copy() if self._annotations is not None else None
301         if self.annotation_select_mask is None:
302             obj.annotation_select_mask = None
303         else:
304             obj.annotation_select_mask = self.annotation_select_mask.copy()
305         # _annotation_select_cache cannot be copied, as doing so breaks the
306         # (necessary) state in which both annotations and
307         # _annotation_select_cache point to the same underlying objects.
308         # It will get re-populated in the cloned queryset the next time it's
309         # used.
310         obj._annotation_select_cache = None
311         obj.max_depth = self.max_depth
312         obj.combinator = self.combinator
313         obj.combinator_all = self.combinator_all
314         obj.combined_queries = self.combined_queries
315         obj._extra = self._extra.copy() if self._extra is not None else None
316         if self.extra_select_mask is None:
317             obj.extra_select_mask = None
318         else:
319             obj.extra_select_mask = self.extra_select_mask.copy()
320         if self._extra_select_cache is None:
321             obj._extra_select_cache = None
322         else:
323             obj._extra_select_cache = self._extra_select_cache.copy()
324         obj.extra_tables = self.extra_tables
325         obj.extra_order_by = self.extra_order_by
326         obj.deferred_loading = copy.copy(self.deferred_loading[0]), self.deferred_loading[1]
327         if self.filter_is_sticky and self.used_aliases:
328             obj.used_aliases = self.used_aliases.copy()
329         else:
330             obj.used_aliases = set()
331         obj.filter_is_sticky = False
332         obj.subquery = self.subquery
333         if 'alias_prefix' in self.__dict__:
334             obj.alias_prefix = self.alias_prefix
335         if 'subq_aliases' in self.__dict__:
336             obj.subq_aliases = self.subq_aliases.copy()
337 
338         obj.__dict__.update(kwargs)
339         if hasattr(obj, '_setup_query'):
340             obj._setup_query()
341         obj.context = self.context.copy()
342         obj._forced_pk = getattr(self, '_forced_pk', False)
343         return obj
344 
345     def add_context(self, key, value):
346         self.context[key] = value
347 
348     def get_context(self, key, default=None):
349         return self.context.get(key, default)
350 
351     def relabeled_clone(self, change_map):
352         clone = self.clone()
353         clone.change_aliases(change_map)
354         return clone
355 
356     def rewrite_cols(self, annotation, col_cnt):
357         # We must make sure the inner query has the referred columns in it.
358         # If we are aggregating over an annotation, then Django uses Ref()
359         # instances to note this. However, if we are annotating over a column
360         # of a related model, then it might be that column isn't part of the
361         # SELECT clause of the inner query, and we must manually make sure
362         # the column is selected. An example case is:
363         #    .aggregate(Sum('author__awards'))
364         # Resolving this expression results in a join to author, but there
365         # is no guarantee the awards column of author is in the select clause
366         # of the query. Thus we must manually add the column to the inner
367         # query.
368         orig_exprs = annotation.get_source_expressions()
369         new_exprs = []
370         for expr in orig_exprs:
371             # FIXME: These conditions are fairly arbitrary. Identify a better
372             # method of having expressions decide which code path they should
373             # take.
374             if isinstance(expr, Ref):
375                 # Its already a Ref to subquery (see resolve_ref() for
376                 # details)
377                 new_exprs.append(expr)
378             elif isinstance(expr, (WhereNode, Lookup)):
379                 # Decompose the subexpressions further. The code here is
380                 # copied from the else clause, but this condition must appear
381                 # before the contains_aggregate/is_summary condition below.
382                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
383                 new_exprs.append(new_expr)
384             elif isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
385                 # Reference to column. Make sure the referenced column
386                 # is selected.
387                 col_cnt += 1
388                 col_alias = '__col%d' % col_cnt
389                 self.annotations[col_alias] = expr
390                 self.append_annotation_mask([col_alias])
391                 new_exprs.append(Ref(col_alias, expr))
392             else:
393                 # Some other expression not referencing database values
394                 # directly. Its subexpression might contain Cols.
395                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
396                 new_exprs.append(new_expr)
397         annotation.set_source_expressions(new_exprs)
398         return annotation, col_cnt
399 
400     def get_aggregation(self, using, added_aggregate_names):
401         """
402         Returns the dictionary with the values of the existing aggregations.
403         """
404         if not self.annotation_select:
405             return {}
406         has_limit = self.low_mark != 0 or self.high_mark is not None
407         has_existing_annotations = any(
408             annotation for alias, annotation
409             in self.annotations.items()
410             if alias not in added_aggregate_names
411         )
412         # Decide if we need to use a subquery.
413         #
414         # Existing annotations would cause incorrect results as get_aggregation()
415         # must produce just one result and thus must not use GROUP BY. But we
416         # aren't smart enough to remove the existing annotations from the
417         # query, so those would force us to use GROUP BY.
418         #
419         # If the query has limit or distinct, or uses set operations, then
420         # those operations must be done in a subquery so that the query
421         # aggregates on the limit and/or distinct results instead of applying
422         # the distinct and limit after the aggregation.
423         if (isinstance(self.group_by, list) or has_limit or has_existing_annotations or
424                 self.distinct or self.combinator):
425             from django.db.models.sql.subqueries import AggregateQuery
426             outer_query = AggregateQuery(self.model)
427             inner_query = self.clone()
428             inner_query.select_for_update = False
429             inner_query.select_related = False
430             if not has_limit and not self.distinct_fields:
431                 # Queries with distinct_fields need ordering and when a limit
432                 # is applied we must take the slice from the ordered query.
433                 # Otherwise no need for ordering.
434                 inner_query.clear_ordering(True)
435             if not inner_query.distinct:
436                 # If the inner query uses default select and it has some
437                 # aggregate annotations, then we must make sure the inner
438                 # query is grouped by the main model's primary key. However,
439                 # clearing the select clause can alter results if distinct is
440                 # used.
441                 if inner_query.default_cols and has_existing_annotations:
442                     inner_query.group_by = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
443                 inner_query.default_cols = False
444 
445             relabels = {t: 'subquery' for t in inner_query.tables}
446             relabels[None] = 'subquery'
447             # Remove any aggregates marked for reduction from the subquery
448             # and move them to the outer AggregateQuery.
449             col_cnt = 0
450             for alias, expression in list(inner_query.annotation_select.items()):
451                 if expression.is_summary:
452                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
453                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
454                     del inner_query.annotations[alias]
455                 # Make sure the annotation_select wont use cached results.
456                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
457             if inner_query.select == [] and not inner_query.default_cols and not inner_query.annotation_select_mask:
458                 # In case of Model.objects[0:3].count(), there would be no
459                 # field selected in the inner query, yet we must use a subquery.
460                 # So, make sure at least one field is selected.
461                 inner_query.select = [self.model._meta.pk.get_col(inner_query.get_initial_alias())]
462             try:
463                 outer_query.add_subquery(inner_query, using)
464             except EmptyResultSet:
465                 return {
466                     alias: None
467                     for alias in outer_query.annotation_select
468                 }
469         else:
470             outer_query = self
471             self.select = []
472             self.default_cols = False
473             self._extra = {}
474 
475         outer_query.clear_ordering(True)
476         outer_query.clear_limits()
477         outer_query.select_for_update = False
478         outer_query.select_related = False
479         compiler = outer_query.get_compiler(using)
480         result = compiler.execute_sql(SINGLE)
481         if result is None:
482             result = [None for q in outer_query.annotation_select.items()]
483 
484         converters = compiler.get_converters(outer_query.annotation_select.values())
485         result = compiler.apply_converters(result, converters)
486 
487         return {
488             alias: val
489             for (alias, annotation), val
490             in zip(outer_query.annotation_select.items(), result)
491         }
492 
493     def get_count(self, using):
494         """
495         Performs a COUNT() query using the current filter constraints.
496         """
497         obj = self.clone()
498         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
499         number = obj.get_aggregation(using, ['__count'])['__count']
500         if number is None:
501             number = 0
502         return number
503 
504     def has_filters(self):
505         return self.where
506 
507     def has_results(self, using):
508         q = self.clone()
509         if not q.distinct:
510             if q.group_by is True:
511                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
512                 q.set_group_by()
513             q.clear_select_clause()
514         q.clear_ordering(True)
515         q.set_limits(high=1)
516         compiler = q.get_compiler(using=using)
517         return compiler.has_results()
518 
519     def combine(self, rhs, connector):
520         """
521         Merge the 'rhs' query into the current one (with any 'rhs' effects
522         being applied *after* (that is, "to the right of") anything in the
523         current query. 'rhs' is not modified during a call to this function.
524 
525         The 'connector' parameter describes how to connect filters from the
526         'rhs' query.
527         """
528         assert self.model == rhs.model, \
529             "Cannot combine queries on two different base models."
530         assert self.can_filter(), \
531             "Cannot combine queries once a slice has been taken."
532         assert self.distinct == rhs.distinct, \
533             "Cannot combine a unique query with a non-unique query."
534         assert self.distinct_fields == rhs.distinct_fields, \
535             "Cannot combine queries with different distinct fields."
536 
537         # Work out how to relabel the rhs aliases, if necessary.
538         change_map = {}
539         conjunction = (connector == AND)
540 
541         # Determine which existing joins can be reused. When combining the
542         # query with AND we must recreate all joins for m2m filters. When
543         # combining with OR we can reuse joins. The reason is that in AND
544         # case a single row can't fulfill a condition like:
545         #     revrel__col=1 & revrel__col=2
546         # But, there might be two different related rows matching this
547         # condition. In OR case a single True is enough, so single row is
548         # enough, too.
549         #
550         # Note that we will be creating duplicate joins for non-m2m joins in
551         # the AND case. The results will be correct but this creates too many
552         # joins. This is something that could be fixed later on.
553         reuse = set() if conjunction else set(self.tables)
554         # Base table must be present in the query - this is the same
555         # table on both sides.
556         self.get_initial_alias()
557         joinpromoter = JoinPromoter(connector, 2, False)
558         joinpromoter.add_votes(
559             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
560         rhs_votes = set()
561         # Now, add the joins from rhs query into the new query (skipping base
562         # table).
563         for alias in rhs.tables[1:]:
564             join = rhs.alias_map[alias]
565             # If the left side of the join was already relabeled, use the
566             # updated alias.
567             join = join.relabeled_clone(change_map)
568             new_alias = self.join(join, reuse=reuse)
569             if join.join_type == INNER:
570                 rhs_votes.add(new_alias)
571             # We can't reuse the same join again in the query. If we have two
572             # distinct joins for the same connection in rhs query, then the
573             # combined query must have two joins, too.
574             reuse.discard(new_alias)
575             if alias != new_alias:
576                 change_map[alias] = new_alias
577             if not rhs.alias_refcount[alias]:
578                 # The alias was unused in the rhs query. Unref it so that it
579                 # will be unused in the new query, too. We have to add and
580                 # unref the alias so that join promotion has information of
581                 # the join type for the unused alias.
582                 self.unref_alias(new_alias)
583         joinpromoter.add_votes(rhs_votes)
584         joinpromoter.update_join_types(self)
585 
586         # Now relabel a copy of the rhs where-clause and add it to the current
587         # one.
588         w = rhs.where.clone()
589         w.relabel_aliases(change_map)
590         self.where.add(w, connector)
591 
592         # Selection columns and extra extensions are those provided by 'rhs'.
593         self.select = []
594         for col in rhs.select:
595             self.add_select(col.relabeled_clone(change_map))
596 
597         if connector == OR:
598             # It would be nice to be able to handle this, but the queries don't
599             # really make sense (or return consistent value sets). Not worth
600             # the extra complexity when you can write a real query instead.
601             if self._extra and rhs._extra:
602                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
603         self.extra.update(rhs.extra)
604         extra_select_mask = set()
605         if self.extra_select_mask is not None:
606             extra_select_mask.update(self.extra_select_mask)
607         if rhs.extra_select_mask is not None:
608             extra_select_mask.update(rhs.extra_select_mask)
609         if extra_select_mask:
610             self.set_extra_mask(extra_select_mask)
611         self.extra_tables += rhs.extra_tables
612 
613         # Ordering uses the 'rhs' ordering, unless it has none, in which case
614         # the current ordering is used.
615         self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by
616         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
617 
618     def deferred_to_data(self, target, callback):
619         """
620         Converts the self.deferred_loading data structure to an alternate data
621         structure, describing the field that *will* be loaded. This is used to
622         compute the columns to select from the database and also by the
623         QuerySet class to work out which fields are being initialized on each
624         model. Models that have all their fields included aren't mentioned in
625         the result, only those that have field restrictions in place.
626 
627         The "target" parameter is the instance that is populated (in place).
628         The "callback" is a function that is called whenever a (model, field)
629         pair need to be added to "target". It accepts three parameters:
630         "target", and the model and list of fields being added for that model.
631         """
632         field_names, defer = self.deferred_loading
633         if not field_names:
634             return
635         orig_opts = self.get_meta()
636         seen = {}
637         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
638         for field_name in field_names:
639             parts = field_name.split(LOOKUP_SEP)
640             cur_model = self.model._meta.concrete_model
641             opts = orig_opts
642             for name in parts[:-1]:
643                 old_model = cur_model
644                 source = opts.get_field(name)
645                 if is_reverse_o2o(source):
646                     cur_model = source.related_model
647                 else:
648                     cur_model = source.remote_field.model
649                 opts = cur_model._meta
650                 # Even if we're "just passing through" this model, we must add
651                 # both the current model's pk and the related reference field
652                 # (if it's not a reverse relation) to the things we select.
653                 if not is_reverse_o2o(source):
654                     must_include[old_model].add(source)
655                 add_to_dict(must_include, cur_model, opts.pk)
656             field = opts.get_field(parts[-1])
657             is_reverse_object = field.auto_created and not field.concrete
658             model = field.related_model if is_reverse_object else field.model
659             model = model._meta.concrete_model
660             if model == opts.model:
661                 model = cur_model
662             if not is_reverse_o2o(field):
663                 add_to_dict(seen, model, field)
664 
665         if defer:
666             # We need to load all fields for each model, except those that
667             # appear in "seen" (for all models that appear in "seen"). The only
668             # slight complexity here is handling fields that exist on parent
669             # models.
670             workset = {}
671             for model, values in six.iteritems(seen):
672                 for field in model._meta.fields:
673                     if field in values:
674                         continue
675                     m = field.model._meta.concrete_model
676                     add_to_dict(workset, m, field)
677             for model, values in six.iteritems(must_include):
678                 # If we haven't included a model in workset, we don't add the
679                 # corresponding must_include fields for that model, since an
680                 # empty set means "include all fields". That's why there's no
681                 # "else" branch here.
682                 if model in workset:
683                     workset[model].update(values)
684             for model, values in six.iteritems(workset):
685                 callback(target, model, values)
686         else:
687             for model, values in six.iteritems(must_include):
688                 if model in seen:
689                     seen[model].update(values)
690                 else:
691                     # As we've passed through this model, but not explicitly
692                     # included any fields, we have to make sure it's mentioned
693                     # so that only the "must include" fields are pulled in.
694                     seen[model] = values
695             # Now ensure that every model in the inheritance chain is mentioned
696             # in the parent list. Again, it must be mentioned to ensure that
697             # only "must include" fields are pulled in.
698             for model in orig_opts.get_parent_list():
699                 if model not in seen:
700                     seen[model] = set()
701             for model, values in six.iteritems(seen):
702                 callback(target, model, values)
703 
704     def table_alias(self, table_name, create=False):
705         """
706         Returns a table alias for the given table_name and whether this is a
707         new alias or not.
708 
709         If 'create' is true, a new alias is always created. Otherwise, the
710         most recently created alias for the table (if one exists) is reused.
711         """
712         alias_list = self.table_map.get(table_name)
713         if not create and alias_list:
714             alias = alias_list[0]
715             self.alias_refcount[alias] += 1
716             return alias, False
717 
718         # Create a new alias for this table.
719         if alias_list:
720             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
721             alias_list.append(alias)
722         else:
723             # The first occurrence of a table uses the table name directly.
724             alias = table_name
725             self.table_map[alias] = [alias]
726         self.alias_refcount[alias] = 1
727         self.tables.append(alias)
728         return alias, True
729 
730     def ref_alias(self, alias):
731         """ Increases the reference count for this alias. """
732         self.alias_refcount[alias] += 1
733 
734     def unref_alias(self, alias, amount=1):
735         """ Decreases the reference count for this alias. """
736         self.alias_refcount[alias] -= amount
737 
738     def promote_joins(self, aliases):
739         """
740         Promotes recursively the join type of given aliases and its children to
741         an outer join. If 'unconditional' is False, the join is only promoted if
742         it is nullable or the parent join is an outer join.
743 
744         The children promotion is done to avoid join chains that contain a LOUTER
745         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
746         then we must also promote b->c automatically, or otherwise the promotion
747         of a->b doesn't actually change anything in the query results.
748         """
749         aliases = list(aliases)
750         while aliases:
751             alias = aliases.pop(0)
752             if self.alias_map[alias].join_type is None:
753                 # This is the base table (first FROM entry) - this table
754                 # isn't really joined at all in the query, so we should not
755                 # alter its join type.
756                 continue
757             # Only the first alias (skipped above) should have None join_type
758             assert self.alias_map[alias].join_type is not None
759             parent_alias = self.alias_map[alias].parent_alias
760             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
761             already_louter = self.alias_map[alias].join_type == LOUTER
762             if ((self.alias_map[alias].nullable or parent_louter) and
763                     not already_louter):
764                 self.alias_map[alias] = self.alias_map[alias].promote()
765                 # Join type of 'alias' changed, so re-examine all aliases that
766                 # refer to this one.
767                 aliases.extend(
768                     join for join in self.alias_map.keys()
769                     if self.alias_map[join].parent_alias == alias and join not in aliases
770                 )
771 
772     def demote_joins(self, aliases):
773         """
774         Change join type from LOUTER to INNER for all joins in aliases.
775 
776         Similarly to promote_joins(), this method must ensure no join chains
777         containing first an outer, then an inner join are generated. If we
778         are demoting b->c join in chain a LOUTER b LOUTER c then we must
779         demote a->b automatically, or otherwise the demotion of b->c doesn't
780         actually change anything in the query results. .
781         """
782         aliases = list(aliases)
783         while aliases:
784             alias = aliases.pop(0)
785             if self.alias_map[alias].join_type == LOUTER:
786                 self.alias_map[alias] = self.alias_map[alias].demote()
787                 parent_alias = self.alias_map[alias].parent_alias
788                 if self.alias_map[parent_alias].join_type == INNER:
789                     aliases.append(parent_alias)
790 
791     def reset_refcounts(self, to_counts):
792         """
793         This method will reset reference counts for aliases so that they match
794         the value passed in :param to_counts:.
795         """
796         for alias, cur_refcount in self.alias_refcount.copy().items():
797             unref_amount = cur_refcount - to_counts.get(alias, 0)
798             self.unref_alias(alias, unref_amount)
799 
800     def change_aliases(self, change_map):
801         """
802         Changes the aliases in change_map (which maps old-alias -> new-alias),
803         relabelling any references to them in select columns and the where
804         clause.
805         """
806         assert set(change_map.keys()).intersection(set(change_map.values())) == set()
807 
808         # 1. Update references in "select" (normal columns plus aliases),
809         # "group by" and "where".
810         self.where.relabel_aliases(change_map)
811         if isinstance(self.group_by, list):
812             self.group_by = [col.relabeled_clone(change_map) for col in self.group_by]
813         self.select = [col.relabeled_clone(change_map) for col in self.select]
814         if self._annotations:
815             self._annotations = OrderedDict(
816                 (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items())
817 
818         # 2. Rename the alias in the internal table/alias datastructures.
819         for old_alias, new_alias in six.iteritems(change_map):
820             if old_alias not in self.alias_map:
821                 continue
822             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
823             self.alias_map[new_alias] = alias_data
824             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
825             del self.alias_refcount[old_alias]
826             del self.alias_map[old_alias]
827 
828             table_aliases = self.table_map[alias_data.table_name]
829             for pos, alias in enumerate(table_aliases):
830                 if alias == old_alias:
831                     table_aliases[pos] = new_alias
832                     break
833         self.external_aliases = {change_map.get(alias, alias)
834                                  for alias in self.external_aliases}
835 
836     def bump_prefix(self, outer_query):
837         """
838         Changes the alias prefix to the next letter in the alphabet in a way
839         that the outer query's aliases and this query's aliases will not
840         conflict. Even tables that previously had no alias will get an alias
841         after this call.
842         """
843         def prefix_gen():
844             """
845             Generates a sequence of characters in alphabetical order:
846                 -> 'A', 'B', 'C', ...
847 
848             When the alphabet is finished, the sequence will continue with the
849             Cartesian product:
850                 -> 'AA', 'AB', 'AC', ...
851             """
852             alphabet = ascii_uppercase
853             prefix = chr(ord(self.alias_prefix) + 1)
854             yield prefix
855             for n in count(1):
856                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
857                 for s in product(seq, repeat=n):
858                     yield ''.join(s)
859                 prefix = None
860 
861         if self.alias_prefix != outer_query.alias_prefix:
862             # No clashes between self and outer query should be possible.
863             return
864 
865         local_recursion_limit = 127  # explicitly avoid infinite loop
866         for pos, prefix in enumerate(prefix_gen()):
867             if prefix not in self.subq_aliases:
868                 self.alias_prefix = prefix
869                 break
870             if pos > local_recursion_limit:
871                 raise RuntimeError(
872                     'Maximum recursion depth exceeded: too many subqueries.'
873                 )
874         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
875         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
876         change_map = OrderedDict()
877         for pos, alias in enumerate(self.tables):
878             new_alias = '%s%d' % (self.alias_prefix, pos)
879             change_map[alias] = new_alias
880             self.tables[pos] = new_alias
881         self.change_aliases(change_map)
882 
883     def get_initial_alias(self):
884         """
885         Returns the first alias for this query, after increasing its reference
886         count.
887         """
888         if self.tables:
889             alias = self.tables[0]
890             self.ref_alias(alias)
891         else:
892             alias = self.join(BaseTable(self.get_meta().db_table, None))
893         return alias
894 
895     def count_active_tables(self):
896         """
897         Returns the number of tables in this query with a non-zero reference
898         count. Note that after execution, the reference counts are zeroed, so
899         tables added in compiler will not be seen by this method.
900         """
901         return len([1 for count in self.alias_refcount.values() if count])
902 
903     def join(self, join, reuse=None):
904         """
905         Return an alias for the 'join', either reusing an existing alias for
906         that join or creating a new one. 'join' is either a
907         sql.datastructures.BaseTable or Join.
908 
909         The 'reuse' parameter can be either None which means all joins are
910         reusable, or it can be a set containing the aliases that can be reused.
911 
912         A join is always created as LOUTER if the lhs alias is LOUTER to make
913         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
914         joins are created as LOUTER if the join is nullable.
915         """
916         reuse = [a for a, j in self.alias_map.items()
917                  if (reuse is None or a in reuse) and j == join]
918         if reuse:
919             self.ref_alias(reuse[0])
920             return reuse[0]
921 
922         # No reuse is possible, so we need a new alias.
923         alias, _ = self.table_alias(join.table_name, create=True)
924         if join.join_type:
925             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
926                 join_type = LOUTER
927             else:
928                 join_type = INNER
929             join.join_type = join_type
930         join.table_alias = alias
931         self.alias_map[alias] = join
932         return alias
933 
934     def join_parent_model(self, opts, model, alias, seen):
935         """
936         Makes sure the given 'model' is joined in the query. If 'model' isn't
937         a parent of 'opts' or if it is None this method is a no-op.
938 
939         The 'alias' is the root alias for starting the join, 'seen' is a dict
940         of model -> alias of existing joins. It must also contain a mapping
941         of None -> some alias. This will be returned in the no-op case.
942         """
943         if model in seen:
944             return seen[model]
945         chain = opts.get_base_chain(model)
946         if not chain:
947             return alias
948         curr_opts = opts
949         for int_model in chain:
950             if int_model in seen:
951                 curr_opts = int_model._meta
952                 alias = seen[int_model]
953                 continue
954             # Proxy model have elements in base chain
955             # with no parents, assign the new options
956             # object and skip to the next base in that
957             # case
958             if not curr_opts.parents[int_model]:
959                 curr_opts = int_model._meta
960                 continue
961             link_field = curr_opts.get_ancestor_link(int_model)
962             _, _, _, joins, _ = self.setup_joins(
963                 [link_field.name], curr_opts, alias)
964             curr_opts = int_model._meta
965             alias = seen[int_model] = joins[-1]
966         return alias or seen[None]
967 
968     def add_annotation(self, annotation, alias, is_summary=False):
969         """
970         Adds a single annotation expression to the Query
971         """
972         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
973                                                    summarize=is_summary)
974         self.append_annotation_mask([alias])
975         self.annotations[alias] = annotation
976 
977     def _prepare_as_filter_value(self):
978         return self.clone()
979 
980     def prepare_lookup_value(self, value, lookups, can_reuse, allow_joins=True):
981         # Default lookup if none given is exact.
982         used_joins = []
983         if len(lookups) == 0:
984             lookups = ['exact']
985         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
986         # uses of None as a query value.
987         if value is None:
988             if lookups[-1] not in ('exact', 'iexact'):
989                 raise ValueError("Cannot use None as a query value")
990             return True, ['isnull'], used_joins
991         elif hasattr(value, 'resolve_expression'):
992             pre_joins = self.alias_refcount.copy()
993             value = value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
994             used_joins = [k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)]
995         elif isinstance(value, (list, tuple)):
996             # The items of the iterable may be expressions and therefore need
997             # to be resolved independently.
998             processed_values = []
999             used_joins = set()
1000             for sub_value in value:
1001                 if hasattr(sub_value, 'resolve_expression'):
1002                     pre_joins = self.alias_refcount.copy()
1003                     processed_values.append(
1004                         sub_value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
1005                     )
1006                     # The used_joins for a tuple of expressions is the union of
1007                     # the used_joins for the individual expressions.
1008                     used_joins |= set(k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0))
1009         # Subqueries need to use a different set of aliases than the
1010         # outer query. Call bump_prefix to change aliases of the inner
1011         # query (the value).
1012         if hasattr(value, '_prepare_as_filter_value'):
1013             value = value._prepare_as_filter_value()
1014             value.bump_prefix(self)
1015         # For Oracle '' is equivalent to null. The check needs to be done
1016         # at this stage because join promotion can't be done at compiler
1017         # stage. Using DEFAULT_DB_ALIAS isn't nice, but it is the best we
1018         # can do here. Similar thing is done in is_nullable(), too.
1019         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1020                 lookups[-1] == 'exact' and value == ''):
1021             value = True
1022             lookups[-1] = 'isnull'
1023         return value, lookups, used_joins
1024 
1025     def solve_lookup_type(self, lookup):
1026         """
1027         Solve the lookup type from the lookup (eg: 'foobar__id__icontains')
1028         """
1029         lookup_splitted = lookup.split(LOOKUP_SEP)
1030         if self._annotations:
1031             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1032             if expression:
1033                 return expression_lookups, (), expression
1034         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1035         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1036         if len(lookup_parts) == 0:
1037             lookup_parts = ['exact']
1038         elif len(lookup_parts) > 1:
1039             if not field_parts:
1040                 raise FieldError(
1041                     'Invalid lookup "%s" for model %s".' %
1042                     (lookup, self.get_meta().model.__name__))
1043         return lookup_parts, field_parts, False
1044 
1045     def check_query_object_type(self, value, opts, field):
1046         """
1047         Checks whether the object passed while querying is of the correct type.
1048         If not, it raises a ValueError specifying the wrong object.
1049         """
1050         if hasattr(value, '_meta'):
1051             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1052                 raise ValueError(
1053                     'Cannot query "%s": Must be "%s" instance.' %
1054                     (value, opts.object_name))
1055 
1056     def check_related_objects(self, field, value, opts):
1057         """
1058         Checks the type of object passed to query relations.
1059         """
1060         if field.is_relation:
1061             # Check that the field and the queryset use the same model in a
1062             # query like .filter(author=Author.objects.all()). For example, the
1063             # opts would be Author's (from the author field) and value.model
1064             # would be Author.objects.all() queryset's .model (Author also).
1065             # The field is the related field on the lhs side.
1066             # If _forced_pk isn't set, this isn't a queryset query or values()
1067             # or values_list() was specified by the developer in which case
1068             # that choice is trusted.
1069             if (getattr(value, '_forced_pk', False) and
1070                     not check_rel_lookup_compatibility(value.model, opts, field)):
1071                 raise ValueError(
1072                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1073                     (value.model._meta.object_name, opts.object_name)
1074                 )
1075             elif hasattr(value, '_meta'):
1076                 self.check_query_object_type(value, opts, field)
1077             elif hasattr(value, '__iter__'):
1078                 for v in value:
1079                     self.check_query_object_type(v, opts, field)
1080 
1081     def build_lookup(self, lookups, lhs, rhs):
1082         """
1083         Tries to extract transforms and lookup from given lhs.
1084 
1085         The lhs value is something that works like SQLExpression.
1086         The rhs value is what the lookup is going to compare against.
1087         The lookups is a list of names to extract using get_lookup()
1088         and get_transform().
1089         """
1090         lookups = lookups[:]
1091         while lookups:
1092             name = lookups[0]
1093             # If there is just one part left, try first get_lookup() so
1094             # that if the lhs supports both transform and lookup for the
1095             # name, then lookup will be picked.
1096             if len(lookups) == 1:
1097                 final_lookup = lhs.get_lookup(name)
1098                 if not final_lookup:
1099                     # We didn't find a lookup. We are going to interpret
1100                     # the name as transform, and do an Exact lookup against
1101                     # it.
1102                     lhs = self.try_transform(lhs, name, lookups)
1103                     final_lookup = lhs.get_lookup('exact')
1104                 return final_lookup(lhs, rhs)
1105             lhs = self.try_transform(lhs, name, lookups)
1106             lookups = lookups[1:]
1107 
1108     def try_transform(self, lhs, name, rest_of_lookups):
1109         """
1110         Helper method for build_lookup. Tries to fetch and initialize
1111         a transform for name parameter from lhs.
1112         """
1113         transform_class = lhs.get_transform(name)
1114         if transform_class:
1115             return transform_class(lhs)
1116         else:
1117             raise FieldError(
1118                 "Unsupported lookup '%s' for %s or join on the field not "
1119                 "permitted." %
1120                 (name, lhs.output_field.__class__.__name__))
1121 
1122     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1123                      can_reuse=None, connector=AND, allow_joins=True, split_subq=True):
1124         """
1125         Builds a WhereNode for a single filter clause, but doesn't add it
1126         to this Query. Query.add_q() will then add this filter to the where
1127         Node.
1128 
1129         The 'branch_negated' tells us if the current branch contains any
1130         negations. This will be used to determine if subqueries are needed.
1131 
1132         The 'current_negated' is used to determine if the current filter is
1133         negated or not and this will be used to determine if IS NULL filtering
1134         is needed.
1135 
1136         The difference between current_netageted and branch_negated is that
1137         branch_negated is set on first negation, but current_negated is
1138         flipped for each negation.
1139 
1140         Note that add_filter will not do any negating itself, that is done
1141         upper in the code by add_q().
1142 
1143         The 'can_reuse' is a set of reusable joins for multijoins.
1144 
1145         The method will create a filter clause that can be added to the current
1146         query. However, if the filter isn't added to the query then the caller
1147         is responsible for unreffing the joins used.
1148         """
1149         if isinstance(filter_expr, dict):
1150             raise FieldError("Cannot parse keyword query as dict")
1151         arg, value = filter_expr
1152         if not arg:
1153             raise FieldError("Cannot parse keyword query %r" % arg)
1154         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1155         if not allow_joins and len(parts) > 1:
1156             raise FieldError("Joined field references are not permitted in this query")
1157 
1158         # Work out the lookup type and remove it from the end of 'parts',
1159         # if necessary.
1160         value, lookups, used_joins = self.prepare_lookup_value(value, lookups, can_reuse, allow_joins)
1161 
1162         clause = self.where_class()
1163         if reffed_expression:
1164             condition = self.build_lookup(lookups, reffed_expression, value)
1165             clause.add(condition, AND)
1166             return clause, []
1167 
1168         opts = self.get_meta()
1169         alias = self.get_initial_alias()
1170         allow_many = not branch_negated or not split_subq
1171 
1172         try:
1173             field, sources, opts, join_list, path = self.setup_joins(
1174                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many)
1175 
1176             # Prevent iterator from being consumed by check_related_objects()
1177             if isinstance(value, Iterator):
1178                 value = list(value)
1179             self.check_related_objects(field, value, opts)
1180 
1181             # split_exclude() needs to know which joins were generated for the
1182             # lookup parts
1183             self._lookup_joins = join_list
1184         except MultiJoin as e:
1185             return self.split_exclude(filter_expr, LOOKUP_SEP.join(parts[:e.level]),
1186                                       can_reuse, e.names_with_path)
1187 
1188         if can_reuse is not None:
1189             can_reuse.update(join_list)
1190         used_joins = set(used_joins).union(set(join_list))
1191         targets, alias, join_list = self.trim_joins(sources, join_list, path)
1192 
1193         if field.is_relation:
1194             # No support for transforms for relational fields
1195             num_lookups = len(lookups)
1196             if num_lookups > 1:
1197                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1198             assert num_lookups > 0  # Likely a bug in Django if this fails.
1199             lookup_class = field.get_lookup(lookups[0])
1200             if lookup_class is None:
1201                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1202             if len(targets) == 1:
1203                 lhs = targets[0].get_col(alias, field)
1204             else:
1205                 lhs = MultiColSource(alias, targets, sources, field)
1206             condition = lookup_class(lhs, value)
1207             lookup_type = lookup_class.lookup_name
1208         else:
1209             col = targets[0].get_col(alias, field)
1210             condition = self.build_lookup(lookups, col, value)
1211             lookup_type = condition.lookup_name
1212 
1213         clause.add(condition, AND)
1214 
1215         require_outer = lookup_type == 'isnull' and value is True and not current_negated
1216         if current_negated and (lookup_type != 'isnull' or value is False):
1217             require_outer = True
1218             if (lookup_type != 'isnull' and (
1219                     self.is_nullable(targets[0]) or
1220                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1221                 # The condition added here will be SQL like this:
1222                 # NOT (col IS NOT NULL), where the first NOT is added in
1223                 # upper layers of code. The reason for addition is that if col
1224                 # is null, then col != someval will result in SQL "unknown"
1225                 # which isn't the same as in Python. The Python None handling
1226                 # is wanted, and it can be gotten by
1227                 # (col IS NULL OR col != someval)
1228                 #   <=>
1229                 # NOT (col IS NOT NULL AND col = someval).
1230                 lookup_class = targets[0].get_lookup('isnull')
1231                 clause.add(lookup_class(targets[0].get_col(alias, sources[0]), False), AND)
1232         return clause, used_joins if not require_outer else ()
1233 
1234     def add_filter(self, filter_clause):
1235         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1236 
1237     def add_q(self, q_object):
1238         """
1239         A preprocessor for the internal _add_q(). Responsible for doing final
1240         join promotion.
1241         """
1242         # For join promotion this case is doing an AND for the added q_object
1243         # and existing conditions. So, any existing inner join forces the join
1244         # type to remain inner. Existing outer joins can however be demoted.
1245         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1246         # rel_a doesn't produce any rows, then the whole condition must fail.
1247         # So, demotion is OK.
1248         existing_inner = set(
1249             (a for a in self.alias_map if self.alias_map[a].join_type == INNER))
1250         clause, _ = self._add_q(q_object, self.used_aliases)
1251         if clause:
1252             self.where.add(clause, AND)
1253         self.demote_joins(existing_inner)
1254 
1255     def _add_q(self, q_object, used_aliases, branch_negated=False,
1256                current_negated=False, allow_joins=True, split_subq=True):
1257         """
1258         Adds a Q-object to the current filter.
1259         """
1260         connector = q_object.connector
1261         current_negated = current_negated ^ q_object.negated
1262         branch_negated = branch_negated or q_object.negated
1263         target_clause = self.where_class(connector=connector,
1264                                          negated=q_object.negated)
1265         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1266         for child in q_object.children:
1267             if isinstance(child, Node):
1268                 child_clause, needed_inner = self._add_q(
1269                     child, used_aliases, branch_negated,
1270                     current_negated, allow_joins, split_subq)
1271                 joinpromoter.add_votes(needed_inner)
1272             else:
1273                 child_clause, needed_inner = self.build_filter(
1274                     child, can_reuse=used_aliases, branch_negated=branch_negated,
1275                     current_negated=current_negated, connector=connector,
1276                     allow_joins=allow_joins, split_subq=split_subq,
1277                 )
1278                 joinpromoter.add_votes(needed_inner)
1279             if child_clause:
1280                 target_clause.add(child_clause, connector)
1281         needed_inner = joinpromoter.update_join_types(self)
1282         return target_clause, needed_inner
1283 
1284     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1285         """
1286         Walks the list of names and turns them into PathInfo tuples. Note that
1287         a single name in 'names' can generate multiple PathInfos (m2m for
1288         example).
1289 
1290         'names' is the path of names to travel, 'opts' is the model Options we
1291         start the name resolving from, 'allow_many' is as for setup_joins().
1292         If fail_on_missing is set to True, then a name that can't be resolved
1293         will generate a FieldError.
1294 
1295         Returns a list of PathInfo tuples. In addition returns the final field
1296         (the last used join field), and target (which is a field guaranteed to
1297         contain the same value as the final field). Finally, the method returns
1298         those names that weren't found (which are likely transforms and the
1299         final lookup).
1300         """
1301         path, names_with_path = [], []
1302         for pos, name in enumerate(names):
1303             cur_names_with_path = (name, [])
1304             if name == 'pk':
1305                 name = opts.pk.name
1306 
1307             field = None
1308             try:
1309                 field = opts.get_field(name)
1310             except FieldDoesNotExist:
1311                 if name in self.annotation_select:
1312                     field = self.annotation_select[name].output_field
1313                 elif pos == 0:
1314                     for rel in opts.related_objects:
1315                         if (name == rel.related_model._meta.model_name and
1316                                 rel.related_name == rel.related_model._meta.default_related_name):
1317                             related_name = rel.related_name
1318                             field = opts.get_field(related_name)
1319                             warnings.warn(
1320                                 "Query lookup '%s' is deprecated in favor of "
1321                                 "Meta.default_related_name '%s'."
1322                                 % (name, related_name),
1323                                 RemovedInDjango20Warning, 2
1324                             )
1325                             break
1326 
1327             if field is not None:
1328                 # Fields that contain one-to-many relations with a generic
1329                 # model (like a GenericForeignKey) cannot generate reverse
1330                 # relations and therefore cannot be used for reverse querying.
1331                 if field.is_relation and not field.related_model:
1332                     raise FieldError(
1333                         "Field %r does not generate an automatic reverse "
1334                         "relation and therefore cannot be used for reverse "
1335                         "querying. If it is a GenericForeignKey, consider "
1336                         "adding a GenericRelation." % name
1337                     )
1338                 try:
1339                     model = field.model._meta.concrete_model
1340                 except AttributeError:
1341                     # QuerySet.annotate() may introduce fields that aren't
1342                     # attached to a model.
1343                     model = None
1344             else:
1345                 # We didn't find the current field, so move position back
1346                 # one step.
1347                 pos -= 1
1348                 if pos == -1 or fail_on_missing:
1349                     field_names = list(get_field_names_from_opts(opts))
1350                     available = sorted(field_names + list(self.annotation_select))
1351                     raise FieldError("Cannot resolve keyword '%s' into field. "
1352                                      "Choices are: %s" % (name, ", ".join(available)))
1353                 break
1354             # Check if we need any joins for concrete inheritance cases (the
1355             # field lives in parent, but we are currently in one of its
1356             # children)
1357             if model is not opts.model:
1358                 path_to_parent = opts.get_path_to_parent(model)
1359                 if path_to_parent:
1360                     path.extend(path_to_parent)
1361                     cur_names_with_path[1].extend(path_to_parent)
1362                     opts = path_to_parent[-1].to_opts
1363             if hasattr(field, 'get_path_info'):
1364                 pathinfos = field.get_path_info()
1365                 if not allow_many:
1366                     for inner_pos, p in enumerate(pathinfos):
1367                         if p.m2m:
1368                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1369                             names_with_path.append(cur_names_with_path)
1370                             raise MultiJoin(pos + 1, names_with_path)
1371                 last = pathinfos[-1]
1372                 path.extend(pathinfos)
1373                 final_field = last.join_field
1374                 opts = last.to_opts
1375                 targets = last.target_fields
1376                 cur_names_with_path[1].extend(pathinfos)
1377                 names_with_path.append(cur_names_with_path)
1378             else:
1379                 # Local non-relational field.
1380                 final_field = field
1381                 targets = (field,)
1382                 if fail_on_missing and pos + 1 != len(names):
1383                     raise FieldError(
1384                         "Cannot resolve keyword %r into field. Join on '%s'"
1385                         " not permitted." % (names[pos + 1], name))
1386                 break
1387         return path, final_field, targets, names[pos + 1:]
1388 
1389     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True):
1390         """
1391         Compute the necessary table joins for the passage through the fields
1392         given in 'names'. 'opts' is the Options class for the current model
1393         (which gives the table we are starting from), 'alias' is the alias for
1394         the table to start the joining from.
1395 
1396         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1397         can be None in which case all joins are reusable or a set of aliases
1398         that can be reused. Note that non-reverse foreign keys are always
1399         reusable when using setup_joins().
1400 
1401         If 'allow_many' is False, then any reverse foreign key seen will
1402         generate a MultiJoin exception.
1403 
1404         Returns the final field involved in the joins, the target field (used
1405         for any 'where' constraint), the final 'opts' value, the joins and the
1406         field path travelled to generate the joins.
1407 
1408         The target field is the field containing the concrete value. Final
1409         field can be something different, for example foreign key pointing to
1410         that value. Final field is needed for example in some value
1411         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1412         key field for example).
1413         """
1414         joins = [alias]
1415         # First, generate the path for the names
1416         path, final_field, targets, rest = self.names_to_path(
1417             names, opts, allow_many, fail_on_missing=True)
1418 
1419         # Then, add the path to the query's joins. Note that we can't trim
1420         # joins at this stage - we will need the information about join type
1421         # of the trimmed joins.
1422         for join in path:
1423             opts = join.to_opts
1424             if join.direct:
1425                 nullable = self.is_nullable(join.join_field)
1426             else:
1427                 nullable = True
1428             connection = Join(opts.db_table, alias, None, INNER, join.join_field, nullable)
1429             reuse = can_reuse if join.m2m else None
1430             alias = self.join(connection, reuse=reuse)
1431             joins.append(alias)
1432         return final_field, targets, opts, joins, path
1433 
1434     def trim_joins(self, targets, joins, path):
1435         """
1436         The 'target' parameter is the final field being joined to, 'joins'
1437         is the full list of join aliases. The 'path' contain the PathInfos
1438         used to create the joins.
1439 
1440         Returns the final target field and table alias and the new active
1441         joins.
1442 
1443         We will always trim any direct join if we have the target column
1444         available already in the previous table. Reverse joins can't be
1445         trimmed as we don't know if there is anything on the other side of
1446         the join.
1447         """
1448         joins = joins[:]
1449         for pos, info in enumerate(reversed(path)):
1450             if len(joins) == 1 or not info.direct:
1451                 break
1452             join_targets = set(t.column for t in info.join_field.foreign_related_fields)
1453             cur_targets = set(t.column for t in targets)
1454             if not cur_targets.issubset(join_targets):
1455                 break
1456             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1457             targets = tuple(targets_dict[t.column] for t in targets)
1458             self.unref_alias(joins.pop())
1459         return targets, joins[-1], joins
1460 
1461     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1462         if not allow_joins and LOOKUP_SEP in name:
1463             raise FieldError("Joined field references are not permitted in this query")
1464         if name in self.annotations:
1465             if summarize:
1466                 # Summarize currently means we are doing an aggregate() query
1467                 # which is executed as a wrapped subquery if any of the
1468                 # aggregate() elements reference an existing annotation. In
1469                 # that case we need to return a Ref to the subquery's annotation.
1470                 return Ref(name, self.annotation_select[name])
1471             else:
1472                 return self.annotation_select[name]
1473         else:
1474             field_list = name.split(LOOKUP_SEP)
1475             field, sources, opts, join_list, path = self.setup_joins(
1476                 field_list, self.get_meta(),
1477                 self.get_initial_alias(), reuse)
1478             targets, _, join_list = self.trim_joins(sources, join_list, path)
1479             if len(targets) > 1:
1480                 raise FieldError("Referencing multicolumn fields with F() objects "
1481                                  "isn't supported")
1482             if reuse is not None:
1483                 reuse.update(join_list)
1484             col = targets[0].get_col(join_list[-1], sources[0])
1485             return col
1486 
1487     def split_exclude(self, filter_expr, prefix, can_reuse, names_with_path):
1488         """
1489         When doing an exclude against any kind of N-to-many relation, we need
1490         to use a subquery. This method constructs the nested query, given the
1491         original exclude filter (filter_expr) and the portion up to the first
1492         N-to-many relation field.
1493 
1494         As an example we could have original filter ~Q(child__name='foo').
1495         We would get here with filter_expr = child__name, prefix = child and
1496         can_reuse is a set of joins usable for filters in the original query.
1497 
1498         We will turn this into equivalent of:
1499             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1500                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1501 
1502         It might be worth it to consider using WHERE NOT EXISTS as that has
1503         saner null handling, and is easier for the backend's optimizer to
1504         handle.
1505         """
1506         # Generate the inner query.
1507         query = Query(self.model)
1508         query.add_filter(filter_expr)
1509         query.clear_ordering(True)
1510         # Try to have as simple as possible subquery -> trim leading joins from
1511         # the subquery.
1512         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1513 
1514         # Add extra check to make sure the selected field will not be null
1515         # since we are adding an IN <subquery> clause. This prevents the
1516         # database from tripping over IN (...,NULL,...) selects and returning
1517         # nothing
1518         col = query.select[0]
1519         select_field = col.target
1520         alias = col.alias
1521         if self.is_nullable(select_field):
1522             lookup_class = select_field.get_lookup('isnull')
1523             lookup = lookup_class(select_field.get_col(alias), False)
1524             query.where.add(lookup, AND)
1525         if alias in can_reuse:
1526             pk = select_field.model._meta.pk
1527             # Need to add a restriction so that outer query's filters are in effect for
1528             # the subquery, too.
1529             query.bump_prefix(self)
1530             lookup_class = select_field.get_lookup('exact')
1531             # Note that the query.select[0].alias is different from alias
1532             # due to bump_prefix above.
1533             lookup = lookup_class(pk.get_col(query.select[0].alias),
1534                                   pk.get_col(alias))
1535             query.where.add(lookup, AND)
1536             query.external_aliases.add(alias)
1537 
1538         condition, needed_inner = self.build_filter(
1539             ('%s__in' % trimmed_prefix, query),
1540             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1541         if contains_louter:
1542             or_null_condition, _ = self.build_filter(
1543                 ('%s__isnull' % trimmed_prefix, True),
1544                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1545             condition.add(or_null_condition, OR)
1546             # Note that the end result will be:
1547             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1548             # This might look crazy but due to how IN works, this seems to be
1549             # correct. If the IS NOT NULL check is removed then outercol NOT
1550             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1551             # outercol IS NULL we will not match the row.
1552         return condition, needed_inner
1553 
1554     def set_empty(self):
1555         self.where.add(NothingNode(), AND)
1556 
1557     def is_empty(self):
1558         return any(isinstance(c, NothingNode) for c in self.where.children)
1559 
1560     def set_limits(self, low=None, high=None):
1561         """
1562         Adjusts the limits on the rows retrieved. We use low/high to set these,
1563         as it makes it more Pythonic to read and write. When the SQL query is
1564         created, they are converted to the appropriate offset and limit values.
1565 
1566         Any limits passed in here are applied relative to the existing
1567         constraints. So low is added to the current low value and both will be
1568         clamped to any existing high value.
1569         """
1570         if high is not None:
1571             if self.high_mark is not None:
1572                 self.high_mark = min(self.high_mark, self.low_mark + high)
1573             else:
1574                 self.high_mark = self.low_mark + high
1575         if low is not None:
1576             if self.high_mark is not None:
1577                 self.low_mark = min(self.high_mark, self.low_mark + low)
1578             else:
1579                 self.low_mark = self.low_mark + low
1580 
1581         if self.low_mark == self.high_mark:
1582             self.set_empty()
1583 
1584     def clear_limits(self):
1585         """
1586         Clears any existing limits.
1587         """
1588         self.low_mark, self.high_mark = 0, None
1589 
1590     def can_filter(self):
1591         """
1592         Returns True if adding filters to this instance is still possible.
1593 
1594         Typically, this means no limits or offsets have been put on the results.
1595         """
1596         return not self.low_mark and self.high_mark is None
1597 
1598     def clear_select_clause(self):
1599         """
1600         Removes all fields from SELECT clause.
1601         """
1602         self.select = []
1603         self.default_cols = False
1604         self.select_related = False
1605         self.set_extra_mask(())
1606         self.set_annotation_mask(())
1607 
1608     def clear_select_fields(self):
1609         """
1610         Clears the list of fields to select (but not extra_select columns).
1611         Some queryset types completely replace any existing list of select
1612         columns.
1613         """
1614         self.select = []
1615         self.values_select = []
1616 
1617     def add_select(self, col):
1618         self.default_cols = False
1619         self.select.append(col)
1620 
1621     def set_select(self, cols):
1622         self.default_cols = False
1623         self.select = cols
1624 
1625     def add_distinct_fields(self, *field_names):
1626         """
1627         Adds and resolves the given fields to the query's "distinct on" clause.
1628         """
1629         self.distinct_fields = field_names
1630         self.distinct = True
1631 
1632     def add_fields(self, field_names, allow_m2m=True):
1633         """
1634         Adds the given (model) fields to the select set. The field names are
1635         added in the order specified.
1636         """
1637         alias = self.get_initial_alias()
1638         opts = self.get_meta()
1639 
1640         try:
1641             for name in field_names:
1642                 # Join promotion note - we must not remove any rows here, so
1643                 # if there is no existing joins, use outer join.
1644                 _, targets, _, joins, path = self.setup_joins(
1645                     name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1646                 targets, final_alias, joins = self.trim_joins(targets, joins, path)
1647                 for target in targets:
1648                     self.add_select(target.get_col(final_alias))
1649         except MultiJoin:
1650             raise FieldError("Invalid field name: '%s'" % name)
1651         except FieldError:
1652             if LOOKUP_SEP in name:
1653                 # For lookups spanning over relationships, show the error
1654                 # from the model on which the lookup failed.
1655                 raise
1656             else:
1657                 names = sorted(list(get_field_names_from_opts(opts)) + list(self.extra) + list(self.annotation_select))
1658                 raise FieldError("Cannot resolve keyword %r into field. "
1659                                  "Choices are: %s" % (name, ", ".join(names)))
1660 
1661     def add_ordering(self, *ordering):
1662         """
1663         Adds items from the 'ordering' sequence to the query's "order by"
1664         clause. These items are either field names (not column names) --
1665         possibly with a direction prefix ('-' or '?') -- or OrderBy
1666         expressions.
1667 
1668         If 'ordering' is empty, all ordering is cleared from the query.
1669         """
1670         errors = []
1671         for item in ordering:
1672             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
1673                 errors.append(item)
1674             if getattr(item, 'contains_aggregate', False):
1675                 raise FieldError(
1676                     'Using an aggregate in order_by() without also including '
1677                     'it in annotate() is not allowed: %s' % item
1678                 )
1679         if errors:
1680             raise FieldError('Invalid order_by arguments: %s' % errors)
1681         if ordering:
1682             self.order_by.extend(ordering)
1683         else:
1684             self.default_ordering = False
1685 
1686     def clear_ordering(self, force_empty):
1687         """
1688         Removes any ordering settings. If 'force_empty' is True, there will be
1689         no ordering in the resulting query (not even the model's default).
1690         """
1691         self.order_by = []
1692         self.extra_order_by = ()
1693         if force_empty:
1694             self.default_ordering = False
1695 
1696     def set_group_by(self):
1697         """
1698         Expands the GROUP BY clause required by the query.
1699 
1700         This will usually be the set of all non-aggregate fields in the
1701         return data. If the database backend supports grouping by the
1702         primary key, and the query would be equivalent, the optimization
1703         will be made automatically.
1704         """
1705         self.group_by = []
1706 
1707         for col in self.select:
1708             self.group_by.append(col)
1709 
1710         if self.annotation_select:
1711             for alias, annotation in six.iteritems(self.annotation_select):
1712                 for col in annotation.get_group_by_cols():
1713                     self.group_by.append(col)
1714 
1715     def add_select_related(self, fields):
1716         """
1717         Sets up the select_related data structure so that we only select
1718         certain related models (as opposed to all models, when
1719         self.select_related=True).
1720         """
1721         if isinstance(self.select_related, bool):
1722             field_dict = {}
1723         else:
1724             field_dict = self.select_related
1725         for field in fields:
1726             d = field_dict
1727             for part in field.split(LOOKUP_SEP):
1728                 d = d.setdefault(part, {})
1729         self.select_related = field_dict
1730 
1731     def add_extra(self, select, select_params, where, params, tables, order_by):
1732         """
1733         Adds data to the various extra_* attributes for user-created additions
1734         to the query.
1735         """
1736         if select:
1737             # We need to pair any placeholder markers in the 'select'
1738             # dictionary with their parameters in 'select_params' so that
1739             # subsequent updates to the select dictionary also adjust the
1740             # parameters appropriately.
1741             select_pairs = OrderedDict()
1742             if select_params:
1743                 param_iter = iter(select_params)
1744             else:
1745                 param_iter = iter([])
1746             for name, entry in select.items():
1747                 entry = force_text(entry)
1748                 entry_params = []
1749                 pos = entry.find("%s")
1750                 while pos != -1:
1751                     if pos == 0 or entry[pos - 1] != '%':
1752                         entry_params.append(next(param_iter))
1753                     pos = entry.find("%s", pos + 2)
1754                 select_pairs[name] = (entry, entry_params)
1755             # This is order preserving, since self.extra_select is an OrderedDict.
1756             self.extra.update(select_pairs)
1757         if where or params:
1758             self.where.add(ExtraWhere(where, params), AND)
1759         if tables:
1760             self.extra_tables += tuple(tables)
1761         if order_by:
1762             self.extra_order_by = order_by
1763 
1764     def clear_deferred_loading(self):
1765         """
1766         Remove any fields from the deferred loading set.
1767         """
1768         self.deferred_loading = (set(), True)
1769 
1770     def add_deferred_loading(self, field_names):
1771         """
1772         Add the given list of model field names to the set of fields to
1773         exclude from loading from the database when automatic column selection
1774         is done. The new field names are added to any existing field names that
1775         are deferred (or removed from any existing field names that are marked
1776         as the only ones for immediate loading).
1777         """
1778         # Fields on related models are stored in the literal double-underscore
1779         # format, so that we can use a set datastructure. We do the foo__bar
1780         # splitting and handling when computing the SQL column names (as part of
1781         # get_columns()).
1782         existing, defer = self.deferred_loading
1783         if defer:
1784             # Add to existing deferred names.
1785             self.deferred_loading = existing.union(field_names), True
1786         else:
1787             # Remove names from the set of any existing "immediate load" names.
1788             self.deferred_loading = existing.difference(field_names), False
1789 
1790     def add_immediate_loading(self, field_names):
1791         """
1792         Add the given list of model field names to the set of fields to
1793         retrieve when the SQL is executed ("immediate loading" fields). The
1794         field names replace any existing immediate loading field names. If
1795         there are field names already specified for deferred loading, those
1796         names are removed from the new field_names before storing the new names
1797         for immediate loading. (That is, immediate loading overrides any
1798         existing immediate values, but respects existing deferrals.)
1799         """
1800         existing, defer = self.deferred_loading
1801         field_names = set(field_names)
1802         if 'pk' in field_names:
1803             field_names.remove('pk')
1804             field_names.add(self.get_meta().pk.name)
1805 
1806         if defer:
1807             # Remove any existing deferred names from the current set before
1808             # setting the new names.
1809             self.deferred_loading = field_names.difference(existing), False
1810         else:
1811             # Replace any existing "immediate load" field names.
1812             self.deferred_loading = field_names, False
1813 
1814     def get_loaded_field_names(self):
1815         """
1816         If any fields are marked to be deferred, returns a dictionary mapping
1817         models to a set of names in those fields that will be loaded. If a
1818         model is not in the returned dictionary, none of its fields are
1819         deferred.
1820 
1821         If no fields are marked for deferral, returns an empty dictionary.
1822         """
1823         # We cache this because we call this function multiple times
1824         # (compiler.fill_related_selections, query.iterator)
1825         try:
1826             return self._loaded_field_names_cache
1827         except AttributeError:
1828             collection = {}
1829             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
1830             self._loaded_field_names_cache = collection
1831             return collection
1832 
1833     def get_loaded_field_names_cb(self, target, model, fields):
1834         """
1835         Callback used by get_deferred_field_names().
1836         """
1837         target[model] = {f.attname for f in fields}
1838 
1839     def set_annotation_mask(self, names):
1840         "Set the mask of annotations that will actually be returned by the SELECT"
1841         if names is None:
1842             self.annotation_select_mask = None
1843         else:
1844             self.annotation_select_mask = set(names)
1845         self._annotation_select_cache = None
1846 
1847     def append_annotation_mask(self, names):
1848         if self.annotation_select_mask is not None:
1849             self.set_annotation_mask(set(names).union(self.annotation_select_mask))
1850 
1851     def set_extra_mask(self, names):
1852         """
1853         Set the mask of extra select items that will be returned by SELECT,
1854         we don't actually remove them from the Query since they might be used
1855         later
1856         """
1857         if names is None:
1858             self.extra_select_mask = None
1859         else:
1860             self.extra_select_mask = set(names)
1861         self._extra_select_cache = None
1862 
1863     def set_values(self, fields):
1864         self.select_related = False
1865         self.clear_deferred_loading()
1866         self.clear_select_fields()
1867 
1868         if self.group_by is True:
1869             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
1870             self.set_group_by()
1871             self.clear_select_fields()
1872 
1873         if fields:
1874             field_names = []
1875             extra_names = []
1876             annotation_names = []
1877             if not self._extra and not self._annotations:
1878                 # Shortcut - if there are no extra or annotations, then
1879                 # the values() clause must be just field names.
1880                 field_names = list(fields)
1881             else:
1882                 self.default_cols = False
1883                 for f in fields:
1884                     if f in self.extra_select:
1885                         extra_names.append(f)
1886                     elif f in self.annotation_select:
1887                         annotation_names.append(f)
1888                     else:
1889                         field_names.append(f)
1890             self.set_extra_mask(extra_names)
1891             self.set_annotation_mask(annotation_names)
1892         else:
1893             field_names = [f.attname for f in self.model._meta.concrete_fields]
1894 
1895         self.values_select = field_names
1896         self.add_fields(field_names, True)
1897 
1898     @property
1899     def annotation_select(self):
1900         """The OrderedDict of aggregate columns that are not masked, and should
1901         be used in the SELECT clause.
1902 
1903         This result is cached for optimization purposes.
1904         """
1905         if self._annotation_select_cache is not None:
1906             return self._annotation_select_cache
1907         elif not self._annotations:
1908             return {}
1909         elif self.annotation_select_mask is not None:
1910             self._annotation_select_cache = OrderedDict(
1911                 (k, v) for k, v in self.annotations.items()
1912                 if k in self.annotation_select_mask
1913             )
1914             return self._annotation_select_cache
1915         else:
1916             return self.annotations
1917 
1918     @property
1919     def extra_select(self):
1920         if self._extra_select_cache is not None:
1921             return self._extra_select_cache
1922         if not self._extra:
1923             return {}
1924         elif self.extra_select_mask is not None:
1925             self._extra_select_cache = OrderedDict(
1926                 (k, v) for k, v in self.extra.items()
1927                 if k in self.extra_select_mask
1928             )
1929             return self._extra_select_cache
1930         else:
1931             return self.extra
1932 
1933     def trim_start(self, names_with_path):
1934         """
1935         Trims joins from the start of the join path. The candidates for trim
1936         are the PathInfos in names_with_path structure that are m2m joins.
1937 
1938         Also sets the select column so the start matches the join.
1939 
1940         This method is meant to be used for generating the subquery joins &
1941         cols in split_exclude().
1942 
1943         Returns a lookup usable for doing outerq.filter(lookup=self). Returns
1944         also if the joins in the prefix contain a LEFT OUTER join.
1945         _"""
1946         all_paths = []
1947         for _, paths in names_with_path:
1948             all_paths.extend(paths)
1949         contains_louter = False
1950         # Trim and operate only on tables that were generated for
1951         # the lookup part of the query. That is, avoid trimming
1952         # joins generated for F() expressions.
1953         lookup_tables = [t for t in self.tables if t in self._lookup_joins or t == self.tables[0]]
1954         for trimmed_paths, path in enumerate(all_paths):
1955             if path.m2m:
1956                 break
1957             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
1958                 contains_louter = True
1959             alias = lookup_tables[trimmed_paths]
1960             self.unref_alias(alias)
1961         # The path.join_field is a Rel, lets get the other side's field
1962         join_field = path.join_field.field
1963         # Build the filter prefix.
1964         paths_in_prefix = trimmed_paths
1965         trimmed_prefix = []
1966         for name, path in names_with_path:
1967             if paths_in_prefix - len(path) < 0:
1968                 break
1969             trimmed_prefix.append(name)
1970             paths_in_prefix -= len(path)
1971         trimmed_prefix.append(
1972             join_field.foreign_related_fields[0].name)
1973         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
1974         # Lets still see if we can trim the first join from the inner query
1975         # (that is, self). We can't do this for LEFT JOINs because we would
1976         # miss those rows that have nothing on the outer side.
1977         if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type != LOUTER:
1978             select_fields = [r[0] for r in join_field.related_fields]
1979             select_alias = lookup_tables[trimmed_paths + 1]
1980             self.unref_alias(lookup_tables[trimmed_paths])
1981             extra_restriction = join_field.get_extra_restriction(
1982                 self.where_class, None, lookup_tables[trimmed_paths + 1])
1983             if extra_restriction:
1984                 self.where.add(extra_restriction, AND)
1985         else:
1986             # TODO: It might be possible to trim more joins from the start of the
1987             # inner query if it happens to have a longer join chain containing the
1988             # values in select_fields. Lets punt this one for now.
1989             select_fields = [r[1] for r in join_field.related_fields]
1990             select_alias = lookup_tables[trimmed_paths]
1991         # The found starting point is likely a Join instead of a BaseTable reference.
1992         # But the first entry in the query's FROM clause must not be a JOIN.
1993         for table in self.tables:
1994             if self.alias_refcount[table] > 0:
1995                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
1996                 break
1997         self.set_select([f.get_col(select_alias) for f in select_fields])
1998         return trimmed_prefix, contains_louter
1999 
2000     def is_nullable(self, field):
2001         """
2002         A helper to check if the given field should be treated as nullable.
2003 
2004         Some backends treat '' as null and Django treats such fields as
2005         nullable for those backends. In such situations field.null can be
2006         False even if we should treat the field as nullable.
2007         """
2008         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2009         # (nor should it have) knowledge of which connection is going to be
2010         # used. The proper fix would be to defer all decisions where
2011         # is_nullable() is needed to the compiler stage, but that is not easy
2012         # to do currently.
2013         if connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and field.empty_strings_allowed:
2014             return True
2015         else:
2016             return field.null
2017 
2018     def as_subquery_filter(self, db):
2019         self._db = db
2020         self.subquery = True
2021         # It's safe to drop ordering if the queryset isn't using slicing,
2022         # distinct(*fields) or select_for_update().
2023         if (self.low_mark == 0 and self.high_mark is None and
2024                 not self.distinct_fields and
2025                 not self.select_for_update):
2026             self.clear_ordering(True)
2027         return self

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 20:13:47.715 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 20:13:47.715 | INFO     | Search-only: skipping patch selection
2025-10-15 20:13:47.836 | INFO     | Task django__django-9003 completed successfully.
2025-10-15 20:13:47.837 | INFO     | No patch generated. You can try running ACR again.
