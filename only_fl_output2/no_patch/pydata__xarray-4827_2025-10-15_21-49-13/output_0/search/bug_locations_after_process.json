[
    {
        "rel_file_path": "xarray/core/merge.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/merge.py",
        "start": 1,
        "end": 940,
        "class_name": null,
        "method_name": null,
        "code": "1 from typing import (\n2     TYPE_CHECKING,\n3     AbstractSet,\n4     Any,\n5     Dict,\n6     Hashable,\n7     Iterable,\n8     List,\n9     Mapping,\n10     NamedTuple,\n11     Optional,\n12     Sequence,\n13     Set,\n14     Tuple,\n15     Union,\n16 )\n17 \n18 import pandas as pd\n19 \n20 from . import dtypes, pdcompat\n21 from .alignment import deep_align\n22 from .duck_array_ops import lazy_array_equiv\n23 from .utils import Frozen, compat_dict_union, dict_equiv\n24 from .variable import Variable, as_variable, assert_unique_multiindex_level_names\n25 \n26 if TYPE_CHECKING:\n27     from .coordinates import Coordinates\n28     from .dataarray import DataArray\n29     from .dataset import Dataset\n30 \n31     DimsLike = Union[Hashable, Sequence[Hashable]]\n32     ArrayLike = Any\n33     VariableLike = Union[\n34         ArrayLike,\n35         Tuple[DimsLike, ArrayLike],\n36         Tuple[DimsLike, ArrayLike, Mapping],\n37         Tuple[DimsLike, ArrayLike, Mapping, Mapping],\n38     ]\n39     XarrayValue = Union[DataArray, Variable, VariableLike]\n40     DatasetLike = Union[Dataset, Mapping[Hashable, XarrayValue]]\n41     CoercibleValue = Union[XarrayValue, pd.Series, pd.DataFrame]\n42     CoercibleMapping = Union[Dataset, Mapping[Hashable, CoercibleValue]]\n43 \n44 \n45 PANDAS_TYPES = (pd.Series, pd.DataFrame, pdcompat.Panel)\n46 \n47 _VALID_COMPAT = Frozen(\n48     {\n49         \"identical\": 0,\n50         \"equals\": 1,\n51         \"broadcast_equals\": 2,\n52         \"minimal\": 3,\n53         \"no_conflicts\": 4,\n54         \"override\": 5,\n55     }\n56 )\n57 \n58 \n59 def broadcast_dimension_size(variables: List[Variable]) -> Dict[Hashable, int]:\n60     \"\"\"Extract dimension sizes from a dictionary of variables.\n61 \n62     Raises ValueError if any dimensions have different sizes.\n63     \"\"\"\n64     dims: Dict[Hashable, int] = {}\n65     for var in variables:\n66         for dim, size in zip(var.dims, var.shape):\n67             if dim in dims and size != dims[dim]:\n68                 raise ValueError(\"index %r not aligned\" % dim)\n69             dims[dim] = size\n70     return dims\n71 \n72 \n73 class MergeError(ValueError):\n74     \"\"\"Error class for merge failures due to incompatible arguments.\"\"\"\n75 \n76     # inherits from ValueError for backward compatibility\n77     # TODO: move this to an xarray.exceptions module?\n78 \n79 \n80 def unique_variable(\n81     name: Hashable,\n82     variables: List[Variable],\n83     compat: str = \"broadcast_equals\",\n84     equals: bool = None,\n85 ) -> Variable:\n86     \"\"\"Return the unique variable from a list of variables or raise MergeError.\n87 \n88     Parameters\n89     ----------\n90     name : hashable\n91         Name for this variable.\n92     variables : list of Variable\n93         List of Variable objects, all of which go by the same name in different\n94         inputs.\n95     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n96         Type of equality check to use.\n97     equals : None or bool, optional\n98         corresponding to result of compat test\n99 \n100     Returns\n101     -------\n102     Variable to use in the result.\n103 \n104     Raises\n105     ------\n106     MergeError: if any of the variables are not equal.\n107     \"\"\"\n108     out = variables[0]\n109 \n110     if len(variables) == 1 or compat == \"override\":\n111         return out\n112 \n113     combine_method = None\n114 \n115     if compat == \"minimal\":\n116         compat = \"broadcast_equals\"\n117 \n118     if compat == \"broadcast_equals\":\n119         dim_lengths = broadcast_dimension_size(variables)\n120         out = out.set_dims(dim_lengths)\n121 \n122     if compat == \"no_conflicts\":\n123         combine_method = \"fillna\"\n124 \n125     if equals is None:\n126         # first check without comparing values i.e. no computes\n127         for var in variables[1:]:\n128             equals = getattr(out, compat)(var, equiv=lazy_array_equiv)\n129             if equals is not True:\n130                 break\n131 \n132         if equals is None:\n133             # now compare values with minimum number of computes\n134             out = out.compute()\n135             for var in variables[1:]:\n136                 equals = getattr(out, compat)(var)\n137                 if not equals:\n138                     break\n139 \n140     if not equals:\n141         raise MergeError(\n142             f\"conflicting values for variable {name!r} on objects to be combined. \"\n143             \"You can skip this check by specifying compat='override'.\"\n144         )\n145 \n146     if combine_method:\n147         for var in variables[1:]:\n148             out = getattr(out, combine_method)(var)\n149 \n150     return out\n151 \n152 \n153 def _assert_compat_valid(compat):\n154     if compat not in _VALID_COMPAT:\n155         raise ValueError(\n156             \"compat={!r} invalid: must be {}\".format(compat, set(_VALID_COMPAT))\n157         )\n158 \n159 \n160 MergeElement = Tuple[Variable, Optional[pd.Index]]\n161 \n162 \n163 def merge_collected(\n164     grouped: Dict[Hashable, List[MergeElement]],\n165     prioritized: Mapping[Hashable, MergeElement] = None,\n166     compat: str = \"minimal\",\n167 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n168     \"\"\"Merge dicts of variables, while resolving conflicts appropriately.\n169 \n170     Parameters\n171     ----------\n172     grouped : mapping\n173     prioritized : mapping\n174     compat : str\n175         Type of equality check to use when checking for conflicts.\n176 \n177     Returns\n178     -------\n179     Dict with keys taken by the union of keys on list_of_mappings,\n180     and Variable values corresponding to those that should be found on the\n181     merged result.\n182     \"\"\"\n183     if prioritized is None:\n184         prioritized = {}\n185 \n186     _assert_compat_valid(compat)\n187 \n188     merged_vars: Dict[Hashable, Variable] = {}\n189     merged_indexes: Dict[Hashable, pd.Index] = {}\n190 \n191     for name, elements_list in grouped.items():\n192         if name in prioritized:\n193             variable, index = prioritized[name]\n194             merged_vars[name] = variable\n195             if index is not None:\n196                 merged_indexes[name] = index\n197         else:\n198             indexed_elements = [\n199                 (variable, index)\n200                 for variable, index in elements_list\n201                 if index is not None\n202             ]\n203 \n204             if indexed_elements:\n205                 # TODO(shoyer): consider adjusting this logic. Are we really\n206                 # OK throwing away variable without an index in favor of\n207                 # indexed variables, without even checking if values match?\n208                 variable, index = indexed_elements[0]\n209                 for _, other_index in indexed_elements[1:]:\n210                     if not index.equals(other_index):\n211                         raise MergeError(\n212                             \"conflicting values for index %r on objects to be \"\n213                             \"combined:\\nfirst value: %r\\nsecond value: %r\"\n214                             % (name, index, other_index)\n215                         )\n216                 if compat == \"identical\":\n217                     for other_variable, _ in indexed_elements[1:]:\n218                         if not dict_equiv(variable.attrs, other_variable.attrs):\n219                             raise MergeError(\n220                                 \"conflicting attribute values on combined \"\n221                                 \"variable %r:\\nfirst value: %r\\nsecond value: %r\"\n222                                 % (name, variable.attrs, other_variable.attrs)\n223                             )\n224                 merged_vars[name] = variable\n225                 merged_indexes[name] = index\n226             else:\n227                 variables = [variable for variable, _ in elements_list]\n228                 try:\n229                     merged_vars[name] = unique_variable(name, variables, compat)\n230                 except MergeError:\n231                     if compat != \"minimal\":\n232                         # we need more than \"minimal\" compatibility (for which\n233                         # we drop conflicting coordinates)\n234                         raise\n235 \n236     return merged_vars, merged_indexes\n237 \n238 \n239 def collect_variables_and_indexes(\n240     list_of_mappings: \"List[DatasetLike]\",\n241 ) -> Dict[Hashable, List[MergeElement]]:\n242     \"\"\"Collect variables and indexes from list of mappings of xarray objects.\n243 \n244     Mappings must either be Dataset objects, or have values of one of the\n245     following types:\n246     - an xarray.Variable\n247     - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in\n248       an xarray.Variable\n249     - or an xarray.DataArray\n250     \"\"\"\n251     from .dataarray import DataArray\n252     from .dataset import Dataset\n253 \n254     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n255 \n256     def append(name, variable, index):\n257         values = grouped.setdefault(name, [])\n258         values.append((variable, index))\n259 \n260     def append_all(variables, indexes):\n261         for name, variable in variables.items():\n262             append(name, variable, indexes.get(name))\n263 \n264     for mapping in list_of_mappings:\n265         if isinstance(mapping, Dataset):\n266             append_all(mapping.variables, mapping.indexes)\n267             continue\n268 \n269         for name, variable in mapping.items():\n270             if isinstance(variable, DataArray):\n271                 coords = variable._coords.copy()  # use private API for speed\n272                 indexes = dict(variable.indexes)\n273                 # explicitly overwritten variables should take precedence\n274                 coords.pop(name, None)\n275                 indexes.pop(name, None)\n276                 append_all(coords, indexes)\n277 \n278             variable = as_variable(variable, name=name)\n279             if variable.dims == (name,):\n280                 variable = variable.to_index_variable()\n281                 index = variable.to_index()\n282             else:\n283                 index = None\n284             append(name, variable, index)\n285 \n286     return grouped\n287 \n288 \n289 def collect_from_coordinates(\n290     list_of_coords: \"List[Coordinates]\",\n291 ) -> Dict[Hashable, List[MergeElement]]:\n292     \"\"\"Collect variables and indexes to be merged from Coordinate objects.\"\"\"\n293     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}\n294 \n295     for coords in list_of_coords:\n296         variables = coords.variables\n297         indexes = coords.indexes\n298         for name, variable in variables.items():\n299             value = grouped.setdefault(name, [])\n300             value.append((variable, indexes.get(name)))\n301     return grouped\n302 \n303 \n304 def merge_coordinates_without_align(\n305     objects: \"List[Coordinates]\",\n306     prioritized: Mapping[Hashable, MergeElement] = None,\n307     exclude_dims: AbstractSet = frozenset(),\n308 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n309     \"\"\"Merge variables/indexes from coordinates without automatic alignments.\n310 \n311     This function is used for merging coordinate from pre-existing xarray\n312     objects.\n313     \"\"\"\n314     collected = collect_from_coordinates(objects)\n315 \n316     if exclude_dims:\n317         filtered: Dict[Hashable, List[MergeElement]] = {}\n318         for name, elements in collected.items():\n319             new_elements = [\n320                 (variable, index)\n321                 for variable, index in elements\n322                 if exclude_dims.isdisjoint(variable.dims)\n323             ]\n324             if new_elements:\n325                 filtered[name] = new_elements\n326     else:\n327         filtered = collected\n328 \n329     return merge_collected(filtered, prioritized)\n330 \n331 \n332 def determine_coords(\n333     list_of_mappings: Iterable[\"DatasetLike\"],\n334 ) -> Tuple[Set[Hashable], Set[Hashable]]:\n335     \"\"\"Given a list of dicts with xarray object values, identify coordinates.\n336 \n337     Parameters\n338     ----------\n339     list_of_mappings : list of dict or list of Dataset\n340         Of the same form as the arguments to expand_variable_dicts.\n341 \n342     Returns\n343     -------\n344     coord_names : set of variable names\n345     noncoord_names : set of variable names\n346         All variable found in the input should appear in either the set of\n347         coordinate or non-coordinate names.\n348     \"\"\"\n349     from .dataarray import DataArray\n350     from .dataset import Dataset\n351 \n352     coord_names: Set[Hashable] = set()\n353     noncoord_names: Set[Hashable] = set()\n354 \n355     for mapping in list_of_mappings:\n356         if isinstance(mapping, Dataset):\n357             coord_names.update(mapping.coords)\n358             noncoord_names.update(mapping.data_vars)\n359         else:\n360             for name, var in mapping.items():\n361                 if isinstance(var, DataArray):\n362                     coords = set(var._coords)  # use private API for speed\n363                     # explicitly overwritten variables should take precedence\n364                     coords.discard(name)\n365                     coord_names.update(coords)\n366 \n367     return coord_names, noncoord_names\n368 \n369 \n370 def coerce_pandas_values(objects: Iterable[\"CoercibleMapping\"]) -> List[\"DatasetLike\"]:\n371     \"\"\"Convert pandas values found in a list of labeled objects.\n372 \n373     Parameters\n374     ----------\n375     objects : list of Dataset or mapping\n376         The mappings may contain any sort of objects coercible to\n377         xarray.Variables as keys, including pandas objects.\n378 \n379     Returns\n380     -------\n381     List of Dataset or dictionary objects. Any inputs or values in the inputs\n382     that were pandas objects have been converted into native xarray objects.\n383     \"\"\"\n384     from .dataarray import DataArray\n385     from .dataset import Dataset\n386 \n387     out = []\n388     for obj in objects:\n389         if isinstance(obj, Dataset):\n390             variables: \"DatasetLike\" = obj\n391         else:\n392             variables = {}\n393             if isinstance(obj, PANDAS_TYPES):\n394                 obj = dict(obj.iteritems())\n395             for k, v in obj.items():\n396                 if isinstance(v, PANDAS_TYPES):\n397                     v = DataArray(v)\n398                 variables[k] = v\n399         out.append(variables)\n400     return out\n401 \n402 \n403 def _get_priority_vars_and_indexes(\n404     objects: List[\"DatasetLike\"], priority_arg: Optional[int], compat: str = \"equals\"\n405 ) -> Dict[Hashable, MergeElement]:\n406     \"\"\"Extract the priority variable from a list of mappings.\n407 \n408     We need this method because in some cases the priority argument itself\n409     might have conflicting values (e.g., if it is a dict with two DataArray\n410     values with conflicting coordinate values).\n411 \n412     Parameters\n413     ----------\n414     objects : list of dict-like of Variable\n415         Dictionaries in which to find the priority variables.\n416     priority_arg : int or None\n417         Integer object whose variable should take priority.\n418     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\"}, optional\n419         Compatibility checks to use when merging variables.\n420 \n421     Returns\n422     -------\n423     A dictionary of variables and associated indexes (if any) to prioritize.\n424     \"\"\"\n425     if priority_arg is None:\n426         return {}\n427 \n428     collected = collect_variables_and_indexes([objects[priority_arg]])\n429     variables, indexes = merge_collected(collected, compat=compat)\n430     grouped: Dict[Hashable, MergeElement] = {}\n431     for name, variable in variables.items():\n432         grouped[name] = (variable, indexes.get(name))\n433     return grouped\n434 \n435 \n436 def merge_coords(\n437     objects: Iterable[\"CoercibleMapping\"],\n438     compat: str = \"minimal\",\n439     join: str = \"outer\",\n440     priority_arg: Optional[int] = None,\n441     indexes: Optional[Mapping[Hashable, pd.Index]] = None,\n442     fill_value: object = dtypes.NA,\n443 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:\n444     \"\"\"Merge coordinate variables.\n445 \n446     See merge_core below for argument descriptions. This works similarly to\n447     merge_core, except everything we don't worry about whether variables are\n448     coordinates or not.\n449     \"\"\"\n450     _assert_compat_valid(compat)\n451     coerced = coerce_pandas_values(objects)\n452     aligned = deep_align(\n453         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value\n454     )\n455     collected = collect_variables_and_indexes(aligned)\n456     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n457     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n458     assert_unique_multiindex_level_names(variables)\n459     return variables, out_indexes\n460 \n461 \n462 def merge_data_and_coords(data, coords, compat=\"broadcast_equals\", join=\"outer\"):\n463     \"\"\"Used in Dataset.__init__.\"\"\"\n464     objects = [data, coords]\n465     explicit_coords = coords.keys()\n466     indexes = dict(_extract_indexes_from_coords(coords))\n467     return merge_core(\n468         objects, compat, join, explicit_coords=explicit_coords, indexes=indexes\n469     )\n470 \n471 \n472 def _extract_indexes_from_coords(coords):\n473     \"\"\"Yields the name & index of valid indexes from a mapping of coords\"\"\"\n474     for name, variable in coords.items():\n475         variable = as_variable(variable, name=name)\n476         if variable.dims == (name,):\n477             yield name, variable.to_index()\n478 \n479 \n480 def assert_valid_explicit_coords(variables, dims, explicit_coords):\n481     \"\"\"Validate explicit coordinate names/dims.\n482 \n483     Raise a MergeError if an explicit coord shares a name with a dimension\n484     but is comprised of arbitrary dimensions.\n485     \"\"\"\n486     for coord_name in explicit_coords:\n487         if coord_name in dims and variables[coord_name].dims != (coord_name,):\n488             raise MergeError(\n489                 \"coordinate %s shares a name with a dataset dimension, but is \"\n490                 \"not a 1D variable along that dimension. This is disallowed \"\n491                 \"by the xarray data model.\" % coord_name\n492             )\n493 \n494 \n495 def merge_attrs(variable_attrs, combine_attrs):\n496     \"\"\"Combine attributes from different variables according to combine_attrs\"\"\"\n497     if not variable_attrs:\n498         # no attributes to merge\n499         return None\n500 \n501     if combine_attrs == \"drop\":\n502         return {}\n503     elif combine_attrs == \"override\":\n504         return dict(variable_attrs[0])\n505     elif combine_attrs == \"no_conflicts\":\n506         result = dict(variable_attrs[0])\n507         for attrs in variable_attrs[1:]:\n508             try:\n509                 result = compat_dict_union(result, attrs)\n510             except ValueError:\n511                 raise MergeError(\n512                     \"combine_attrs='no_conflicts', but some values are not \"\n513                     \"the same. Merging %s with %s\" % (str(result), str(attrs))\n514                 )\n515         return result\n516     elif combine_attrs == \"identical\":\n517         result = dict(variable_attrs[0])\n518         for attrs in variable_attrs[1:]:\n519             if not dict_equiv(result, attrs):\n520                 raise MergeError(\n521                     \"combine_attrs='identical', but attrs differ. First is %s \"\n522                     \", other is %s.\" % (str(result), str(attrs))\n523                 )\n524         return result\n525     else:\n526         raise ValueError(\"Unrecognised value for combine_attrs=%s\" % combine_attrs)\n527 \n528 \n529 class _MergeResult(NamedTuple):\n530     variables: Dict[Hashable, Variable]\n531     coord_names: Set[Hashable]\n532     dims: Dict[Hashable, int]\n533     indexes: Dict[Hashable, pd.Index]\n534     attrs: Dict[Hashable, Any]\n535 \n536 \n537 def merge_core(\n538     objects: Iterable[\"CoercibleMapping\"],\n539     compat: str = \"broadcast_equals\",\n540     join: str = \"outer\",\n541     combine_attrs: Optional[str] = \"override\",\n542     priority_arg: Optional[int] = None,\n543     explicit_coords: Optional[Sequence] = None,\n544     indexes: Optional[Mapping[Hashable, pd.Index]] = None,\n545     fill_value: object = dtypes.NA,\n546 ) -> _MergeResult:\n547     \"\"\"Core logic for merging labeled objects.\n548 \n549     This is not public API.\n550 \n551     Parameters\n552     ----------\n553     objects : list of mapping\n554         All values must be convertable to labeled arrays.\n555     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n556         Compatibility checks to use when merging variables.\n557     join : {\"outer\", \"inner\", \"left\", \"right\"}, optional\n558         How to combine objects with different indexes.\n559     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, optional\n560         How to combine attributes of objects\n561     priority_arg : int, optional\n562         Optional argument in `objects` that takes precedence over the others.\n563     explicit_coords : set, optional\n564         An explicit list of variables from `objects` that are coordinates.\n565     indexes : dict, optional\n566         Dictionary with values given by pandas.Index objects.\n567     fill_value : scalar, optional\n568         Value to use for newly missing values\n569 \n570     Returns\n571     -------\n572     variables : dict\n573         Dictionary of Variable objects.\n574     coord_names : set\n575         Set of coordinate names.\n576     dims : dict\n577         Dictionary mapping from dimension names to sizes.\n578     attrs : dict\n579         Dictionary of attributes\n580 \n581     Raises\n582     ------\n583     MergeError if the merge cannot be done successfully.\n584     \"\"\"\n585     from .dataarray import DataArray\n586     from .dataset import Dataset, calculate_dimensions\n587 \n588     _assert_compat_valid(compat)\n589 \n590     coerced = coerce_pandas_values(objects)\n591     aligned = deep_align(\n592         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value\n593     )\n594     collected = collect_variables_and_indexes(aligned)\n595 \n596     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)\n597     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)\n598     assert_unique_multiindex_level_names(variables)\n599 \n600     dims = calculate_dimensions(variables)\n601 \n602     coord_names, noncoord_names = determine_coords(coerced)\n603     if explicit_coords is not None:\n604         assert_valid_explicit_coords(variables, dims, explicit_coords)\n605         coord_names.update(explicit_coords)\n606     for dim, size in dims.items():\n607         if dim in variables:\n608             coord_names.add(dim)\n609     ambiguous_coords = coord_names.intersection(noncoord_names)\n610     if ambiguous_coords:\n611         raise MergeError(\n612             \"unable to determine if these variables should be \"\n613             \"coordinates or not in the merged result: %s\" % ambiguous_coords\n614         )\n615 \n616     attrs = merge_attrs(\n617         [\n618             var.attrs\n619             for var in coerced\n620             if isinstance(var, Dataset) or isinstance(var, DataArray)\n621         ],\n622         combine_attrs,\n623     )\n624 \n625     return _MergeResult(variables, coord_names, dims, out_indexes, attrs)\n626 \n627 \n628 def merge(\n629     objects: Iterable[Union[\"DataArray\", \"CoercibleMapping\"]],\n630     compat: str = \"no_conflicts\",\n631     join: str = \"outer\",\n632     fill_value: object = dtypes.NA,\n633     combine_attrs: str = \"drop\",\n634 ) -> \"Dataset\":\n635     \"\"\"Merge any number of xarray objects into a single Dataset as variables.\n636 \n637     Parameters\n638     ----------\n639     objects : iterable of Dataset or iterable of DataArray or iterable of dict-like\n640         Merge together all variables from these objects. If any of them are\n641         DataArray objects, they must have a name.\n642     compat : {\"identical\", \"equals\", \"broadcast_equals\", \"no_conflicts\", \"override\"}, optional\n643         String indicating how to compare variables of the same name for\n644         potential conflicts:\n645 \n646         - \"broadcast_equals\": all values must be equal when variables are\n647           broadcast against each other to ensure common dimensions.\n648         - \"equals\": all values and dimensions must be the same.\n649         - \"identical\": all values, dimensions and attributes must be the\n650           same.\n651         - \"no_conflicts\": only values which are not null in both datasets\n652           must be equal. The returned dataset then contains the combination\n653           of all non-null values.\n654         - \"override\": skip comparing and pick variable from first dataset\n655     join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n656         String indicating how to combine differing indexes in objects.\n657 \n658         - \"outer\": use the union of object indexes\n659         - \"inner\": use the intersection of object indexes\n660         - \"left\": use indexes from the first object with each dimension\n661         - \"right\": use indexes from the last object with each dimension\n662         - \"exact\": instead of aligning, raise `ValueError` when indexes to be\n663           aligned are not equal\n664         - \"override\": if indexes are of same size, rewrite indexes to be\n665           those of the first object with that dimension. Indexes for the same\n666           dimension must have the same size in all objects.\n667     fill_value : scalar or dict-like, optional\n668         Value to use for newly missing values. If a dict-like, maps\n669         variable names to fill values. Use a data array's name to\n670         refer to its values.\n671     combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"override\"}, \\\n672                     default: \"drop\"\n673         String indicating how to combine attrs of the objects being merged:\n674 \n675         - \"drop\": empty attrs on returned Dataset.\n676         - \"identical\": all attrs must be the same on every object.\n677         - \"no_conflicts\": attrs from all objects are combined, any that have\n678           the same name must also have the same value.\n679         - \"override\": skip comparing and copy attrs from the first dataset to\n680           the result.\n681 \n682     Returns\n683     -------\n684     Dataset\n685         Dataset with combined variables from each object.\n686 \n687     Examples\n688     --------\n689     >>> import xarray as xr\n690     >>> x = xr.DataArray(\n691     ...     [[1.0, 2.0], [3.0, 5.0]],\n692     ...     dims=(\"lat\", \"lon\"),\n693     ...     coords={\"lat\": [35.0, 40.0], \"lon\": [100.0, 120.0]},\n694     ...     name=\"var1\",\n695     ... )\n696     >>> y = xr.DataArray(\n697     ...     [[5.0, 6.0], [7.0, 8.0]],\n698     ...     dims=(\"lat\", \"lon\"),\n699     ...     coords={\"lat\": [35.0, 42.0], \"lon\": [100.0, 150.0]},\n700     ...     name=\"var2\",\n701     ... )\n702     >>> z = xr.DataArray(\n703     ...     [[0.0, 3.0], [4.0, 9.0]],\n704     ...     dims=(\"time\", \"lon\"),\n705     ...     coords={\"time\": [30.0, 60.0], \"lon\": [100.0, 150.0]},\n706     ...     name=\"var3\",\n707     ... )\n708 \n709     >>> x\n710     <xarray.DataArray 'var1' (lat: 2, lon: 2)>\n711     array([[1., 2.],\n712            [3., 5.]])\n713     Coordinates:\n714       * lat      (lat) float64 35.0 40.0\n715       * lon      (lon) float64 100.0 120.0\n716 \n717     >>> y\n718     <xarray.DataArray 'var2' (lat: 2, lon: 2)>\n719     array([[5., 6.],\n720            [7., 8.]])\n721     Coordinates:\n722       * lat      (lat) float64 35.0 42.0\n723       * lon      (lon) float64 100.0 150.0\n724 \n725     >>> z\n726     <xarray.DataArray 'var3' (time: 2, lon: 2)>\n727     array([[0., 3.],\n728            [4., 9.]])\n729     Coordinates:\n730       * time     (time) float64 30.0 60.0\n731       * lon      (lon) float64 100.0 150.0\n732 \n733     >>> xr.merge([x, y, z])\n734     <xarray.Dataset>\n735     Dimensions:  (lat: 3, lon: 3, time: 2)\n736     Coordinates:\n737       * lat      (lat) float64 35.0 40.0 42.0\n738       * lon      (lon) float64 100.0 120.0 150.0\n739       * time     (time) float64 30.0 60.0\n740     Data variables:\n741         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n742         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n743         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n744 \n745     >>> xr.merge([x, y, z], compat=\"identical\")\n746     <xarray.Dataset>\n747     Dimensions:  (lat: 3, lon: 3, time: 2)\n748     Coordinates:\n749       * lat      (lat) float64 35.0 40.0 42.0\n750       * lon      (lon) float64 100.0 120.0 150.0\n751       * time     (time) float64 30.0 60.0\n752     Data variables:\n753         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n754         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n755         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n756 \n757     >>> xr.merge([x, y, z], compat=\"equals\")\n758     <xarray.Dataset>\n759     Dimensions:  (lat: 3, lon: 3, time: 2)\n760     Coordinates:\n761       * lat      (lat) float64 35.0 40.0 42.0\n762       * lon      (lon) float64 100.0 120.0 150.0\n763       * time     (time) float64 30.0 60.0\n764     Data variables:\n765         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n766         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n767         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n768 \n769     >>> xr.merge([x, y, z], compat=\"equals\", fill_value=-999.0)\n770     <xarray.Dataset>\n771     Dimensions:  (lat: 3, lon: 3, time: 2)\n772     Coordinates:\n773       * lat      (lat) float64 35.0 40.0 42.0\n774       * lon      (lon) float64 100.0 120.0 150.0\n775       * time     (time) float64 30.0 60.0\n776     Data variables:\n777         var1     (lat, lon) float64 1.0 2.0 -999.0 3.0 ... -999.0 -999.0 -999.0\n778         var2     (lat, lon) float64 5.0 -999.0 6.0 -999.0 ... -999.0 7.0 -999.0 8.0\n779         var3     (time, lon) float64 0.0 -999.0 3.0 4.0 -999.0 9.0\n780 \n781     >>> xr.merge([x, y, z], join=\"override\")\n782     <xarray.Dataset>\n783     Dimensions:  (lat: 2, lon: 2, time: 2)\n784     Coordinates:\n785       * lat      (lat) float64 35.0 40.0\n786       * lon      (lon) float64 100.0 120.0\n787       * time     (time) float64 30.0 60.0\n788     Data variables:\n789         var1     (lat, lon) float64 1.0 2.0 3.0 5.0\n790         var2     (lat, lon) float64 5.0 6.0 7.0 8.0\n791         var3     (time, lon) float64 0.0 3.0 4.0 9.0\n792 \n793     >>> xr.merge([x, y, z], join=\"inner\")\n794     <xarray.Dataset>\n795     Dimensions:  (lat: 1, lon: 1, time: 2)\n796     Coordinates:\n797       * lat      (lat) float64 35.0\n798       * lon      (lon) float64 100.0\n799       * time     (time) float64 30.0 60.0\n800     Data variables:\n801         var1     (lat, lon) float64 1.0\n802         var2     (lat, lon) float64 5.0\n803         var3     (time, lon) float64 0.0 4.0\n804 \n805     >>> xr.merge([x, y, z], compat=\"identical\", join=\"inner\")\n806     <xarray.Dataset>\n807     Dimensions:  (lat: 1, lon: 1, time: 2)\n808     Coordinates:\n809       * lat      (lat) float64 35.0\n810       * lon      (lon) float64 100.0\n811       * time     (time) float64 30.0 60.0\n812     Data variables:\n813         var1     (lat, lon) float64 1.0\n814         var2     (lat, lon) float64 5.0\n815         var3     (time, lon) float64 0.0 4.0\n816 \n817     >>> xr.merge([x, y, z], compat=\"broadcast_equals\", join=\"outer\")\n818     <xarray.Dataset>\n819     Dimensions:  (lat: 3, lon: 3, time: 2)\n820     Coordinates:\n821       * lat      (lat) float64 35.0 40.0 42.0\n822       * lon      (lon) float64 100.0 120.0 150.0\n823       * time     (time) float64 30.0 60.0\n824     Data variables:\n825         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan\n826         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0\n827         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0\n828 \n829     >>> xr.merge([x, y, z], join=\"exact\")\n830     Traceback (most recent call last):\n831     ...\n832     ValueError: indexes along dimension 'lat' are not equal\n833 \n834     Raises\n835     ------\n836     xarray.MergeError\n837         If any variables with the same name have conflicting values.\n838 \n839     See also\n840     --------\n841     concat\n842     \"\"\"\n843     from .dataarray import DataArray\n844     from .dataset import Dataset\n845 \n846     dict_like_objects = []\n847     for obj in objects:\n848         if not isinstance(obj, (DataArray, Dataset, dict)):\n849             raise TypeError(\n850                 \"objects must be an iterable containing only \"\n851                 \"Dataset(s), DataArray(s), and dictionaries.\"\n852             )\n853 \n854         obj = obj.to_dataset(promote_attrs=True) if isinstance(obj, DataArray) else obj\n855         dict_like_objects.append(obj)\n856 \n857     merge_result = merge_core(\n858         dict_like_objects,\n859         compat,\n860         join,\n861         combine_attrs=combine_attrs,\n862         fill_value=fill_value,\n863     )\n864     merged = Dataset._construct_direct(**merge_result._asdict())\n865     return merged\n866 \n867 \n868 def dataset_merge_method(\n869     dataset: \"Dataset\",\n870     other: \"CoercibleMapping\",\n871     overwrite_vars: Union[Hashable, Iterable[Hashable]],\n872     compat: str,\n873     join: str,\n874     fill_value: Any,\n875 ) -> _MergeResult:\n876     \"\"\"Guts of the Dataset.merge method.\"\"\"\n877     # we are locked into supporting overwrite_vars for the Dataset.merge\n878     # method due for backwards compatibility\n879     # TODO: consider deprecating it?\n880 \n881     if isinstance(overwrite_vars, Iterable) and not isinstance(overwrite_vars, str):\n882         overwrite_vars = set(overwrite_vars)\n883     else:\n884         overwrite_vars = {overwrite_vars}\n885 \n886     if not overwrite_vars:\n887         objs = [dataset, other]\n888         priority_arg = None\n889     elif overwrite_vars == set(other):\n890         objs = [dataset, other]\n891         priority_arg = 1\n892     else:\n893         other_overwrite: Dict[Hashable, CoercibleValue] = {}\n894         other_no_overwrite: Dict[Hashable, CoercibleValue] = {}\n895         for k, v in other.items():\n896             if k in overwrite_vars:\n897                 other_overwrite[k] = v\n898             else:\n899                 other_no_overwrite[k] = v\n900         objs = [dataset, other_no_overwrite, other_overwrite]\n901         priority_arg = 2\n902 \n903     return merge_core(\n904         objs, compat, join, priority_arg=priority_arg, fill_value=fill_value\n905     )\n906 \n907 \n908 def dataset_update_method(\n909     dataset: \"Dataset\", other: \"CoercibleMapping\"\n910 ) -> _MergeResult:\n911     \"\"\"Guts of the Dataset.update method.\n912 \n913     This drops a duplicated coordinates from `other` if `other` is not an\n914     `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n915     GH2180).\n916     \"\"\"\n917     from .dataarray import DataArray\n918     from .dataset import Dataset\n919 \n920     if not isinstance(other, Dataset):\n921         other = dict(other)\n922         for key, value in other.items():\n923             if isinstance(value, DataArray):\n924                 # drop conflicting coordinates\n925                 coord_names = [\n926                     c\n927                     for c in value.coords\n928                     if c not in value.dims and c in dataset.coords\n929                 ]\n930                 if coord_names:\n931                     other[key] = value.drop_vars(coord_names)\n932 \n933     # use ds.coords and not ds.indexes, else str coords are cast to object\n934     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n935     return merge_core(\n936         [dataset, other],\n937         priority_arg=1,\n938         indexes=indexes,\n939         combine_attrs=\"override\",\n940     )\n",
        "intended_behavior": "This file should be modified to include the new combine_attrs=\"drop_conflicts\" option. The merge function within this file currently processes combine_attrs to decide how to combine attributes from different Dataset objects being merged. The intended behavior is to extend this logic to support a new option, \"drop_conflicts\", which will combine attributes from all objects like the existing \"no_conflicts\" option but will drop any attribute where conflicting values are found instead of raising an error. This behavior will offer a more flexible approach to handling attributes during merges, fitting scenarios where users prefer to avoid errors over attribute conflicts. This adjustment should preserve all existing functionality and only extend additional behavior for managing attrs."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataset.py",
        "start": 1,
        "end": 6985,
        "class_name": null,
        "method_name": null,
        "code": "1 import copy\n2 import datetime\n3 import functools\n4 import sys\n5 import warnings\n6 from collections import defaultdict\n7 from distutils.version import LooseVersion\n8 from html import escape\n9 from numbers import Number\n10 from operator import methodcaller\n11 from pathlib import Path\n12 from typing import (\n13     TYPE_CHECKING,\n14     Any,\n15     Callable,\n16     DefaultDict,\n17     Dict,\n18     Hashable,\n19     Iterable,\n20     Iterator,\n21     List,\n22     Mapping,\n23     MutableMapping,\n24     Optional,\n25     Sequence,\n26     Set,\n27     Tuple,\n28     TypeVar,\n29     Union,\n30     cast,\n31     overload,\n32 )\n33 \n34 import numpy as np\n35 import pandas as pd\n36 \n37 import xarray as xr\n38 \n39 from ..coding.cftimeindex import _parse_array_of_cftime_strings\n40 from ..plot.dataset_plot import _Dataset_PlotMethods\n41 from . import (\n42     alignment,\n43     dtypes,\n44     duck_array_ops,\n45     formatting,\n46     formatting_html,\n47     groupby,\n48     ops,\n49     resample,\n50     rolling,\n51     utils,\n52     weighted,\n53 )\n54 from .alignment import _broadcast_helper, _get_broadcast_dims_map_common_coords, align\n55 from .common import (\n56     DataWithCoords,\n57     ImplementsDatasetReduce,\n58     _contains_datetime_like_objects,\n59 )\n60 from .coordinates import (\n61     DatasetCoordinates,\n62     assert_coordinate_consistent,\n63     remap_label_indexers,\n64 )\n65 from .duck_array_ops import datetime_to_numeric\n66 from .indexes import (\n67     Indexes,\n68     default_indexes,\n69     isel_variable_and_index,\n70     propagate_indexes,\n71     remove_unused_levels_categories,\n72     roll_index,\n73 )\n74 from .indexing import is_fancy_indexer\n75 from .merge import (\n76     dataset_merge_method,\n77     dataset_update_method,\n78     merge_coordinates_without_align,\n79     merge_data_and_coords,\n80 )\n81 from .missing import get_clean_interp_index\n82 from .options import OPTIONS, _get_keep_attrs\n83 from .pycompat import is_duck_dask_array, sparse_array_type\n84 from .utils import (\n85     Default,\n86     Frozen,\n87     HybridMappingProxy,\n88     SortedKeysDict,\n89     _default,\n90     decode_numpy_dict_values,\n91     drop_dims_from_indexers,\n92     either_dict_or_kwargs,\n93     hashable,\n94     infix_dims,\n95     is_dict_like,\n96     is_scalar,\n97     maybe_wrap_array,\n98 )\n99 from .variable import (\n100     IndexVariable,\n101     Variable,\n102     as_variable,\n103     assert_unique_multiindex_level_names,\n104     broadcast_variables,\n105 )\n106 \n107 if TYPE_CHECKING:\n108     from ..backends import AbstractDataStore, ZarrStore\n109     from .dataarray import DataArray\n110     from .merge import CoercibleMapping\n111 \n112     T_DSorDA = TypeVar(\"T_DSorDA\", DataArray, \"Dataset\")\n113 \n114     try:\n115         from dask.delayed import Delayed\n116     except ImportError:\n117         Delayed = None\n118 \n119 \n120 # list of attributes of pd.DatetimeIndex that are ndarrays of time info\n121 _DATETIMEINDEX_COMPONENTS = [\n122     \"year\",\n123     \"month\",\n124     \"day\",\n125     \"hour\",\n126     \"minute\",\n127     \"second\",\n128     \"microsecond\",\n129     \"nanosecond\",\n130     \"date\",\n131     \"time\",\n132     \"dayofyear\",\n133     \"weekofyear\",\n134     \"dayofweek\",\n135     \"quarter\",\n136 ]\n137 \n138 \n139 def _get_virtual_variable(\n140     variables, key: Hashable, level_vars: Mapping = None, dim_sizes: Mapping = None\n141 ) -> Tuple[Hashable, Hashable, Variable]:\n142     \"\"\"Get a virtual variable (e.g., 'time.year' or a MultiIndex level)\n143     from a dict of xarray.Variable objects (if possible)\n144     \"\"\"\n145     if level_vars is None:\n146         level_vars = {}\n147     if dim_sizes is None:\n148         dim_sizes = {}\n149 \n150     if key in dim_sizes:\n151         data = pd.Index(range(dim_sizes[key]), name=key)\n152         variable = IndexVariable((key,), data)\n153         return key, key, variable\n154 \n155     if not isinstance(key, str):\n156         raise KeyError(key)\n157 \n158     split_key = key.split(\".\", 1)\n159     var_name: Optional[str]\n160     if len(split_key) == 2:\n161         ref_name, var_name = split_key\n162     elif len(split_key) == 1:\n163         ref_name, var_name = key, None\n164     else:\n165         raise KeyError(key)\n166 \n167     if ref_name in level_vars:\n168         dim_var = variables[level_vars[ref_name]]\n169         ref_var = dim_var.to_index_variable().get_level_variable(ref_name)\n170     else:\n171         ref_var = variables[ref_name]\n172 \n173     if var_name is None:\n174         virtual_var = ref_var\n175         var_name = key\n176     else:\n177         if _contains_datetime_like_objects(ref_var):\n178             ref_var = xr.DataArray(ref_var)\n179             data = getattr(ref_var.dt, var_name).data\n180         else:\n181             data = getattr(ref_var, var_name).data\n182         virtual_var = Variable(ref_var.dims, data)\n183 \n184     return ref_name, var_name, virtual_var\n185 \n186 \n187 def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> Dict[Hashable, int]:\n188     \"\"\"Calculate the dimensions corresponding to a set of variables.\n189 \n190     Returns dictionary mapping from dimension names to sizes. Raises ValueError\n191     if any of the dimension sizes conflict.\n192     \"\"\"\n193     dims: Dict[Hashable, int] = {}\n194     last_used = {}\n195     scalar_vars = {k for k, v in variables.items() if not v.dims}\n196     for k, var in variables.items():\n197         for dim, size in zip(var.dims, var.shape):\n198             if dim in scalar_vars:\n199                 raise ValueError(\n200                     \"dimension %r already exists as a scalar variable\" % dim\n201                 )\n202             if dim not in dims:\n203                 dims[dim] = size\n204                 last_used[dim] = k\n205             elif dims[dim] != size:\n206                 raise ValueError(\n207                     \"conflicting sizes for dimension %r: \"\n208                     \"length %s on %r and length %s on %r\"\n209                     % (dim, size, k, dims[dim], last_used[dim])\n210                 )\n211     return dims\n212 \n213 \n214 def merge_indexes(\n215     indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]],\n216     variables: Mapping[Hashable, Variable],\n217     coord_names: Set[Hashable],\n218     append: bool = False,\n219 ) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:\n220     \"\"\"Merge variables into multi-indexes.\n221 \n222     Not public API. Used in Dataset and DataArray set_index\n223     methods.\n224     \"\"\"\n225     vars_to_replace: Dict[Hashable, Variable] = {}\n226     vars_to_remove: List[Hashable] = []\n227     dims_to_replace: Dict[Hashable, Hashable] = {}\n228     error_msg = \"{} is not the name of an existing variable.\"\n229 \n230     for dim, var_names in indexes.items():\n231         if isinstance(var_names, str) or not isinstance(var_names, Sequence):\n232             var_names = [var_names]\n233 \n234         names: List[Hashable] = []\n235         codes: List[List[int]] = []\n236         levels: List[List[int]] = []\n237         current_index_variable = variables.get(dim)\n238 \n239         for n in var_names:\n240             try:\n241                 var = variables[n]\n242             except KeyError:\n243                 raise ValueError(error_msg.format(n))\n244             if (\n245                 current_index_variable is not None\n246                 and var.dims != current_index_variable.dims\n247             ):\n248                 raise ValueError(\n249                     \"dimension mismatch between %r %s and %r %s\"\n250                     % (dim, current_index_variable.dims, n, var.dims)\n251                 )\n252 \n253         if current_index_variable is not None and append:\n254             current_index = current_index_variable.to_index()\n255             if isinstance(current_index, pd.MultiIndex):\n256                 names.extend(current_index.names)\n257                 codes.extend(current_index.codes)\n258                 levels.extend(current_index.levels)\n259             else:\n260                 names.append(\"%s_level_0\" % dim)\n261                 cat = pd.Categorical(current_index.values, ordered=True)\n262                 codes.append(cat.codes)\n263                 levels.append(cat.categories)\n264 \n265         if not len(names) and len(var_names) == 1:\n266             idx = pd.Index(variables[var_names[0]].values)\n267 \n268         else:  # MultiIndex\n269             for n in var_names:\n270                 try:\n271                     var = variables[n]\n272                 except KeyError:\n273                     raise ValueError(error_msg.format(n))\n274                 names.append(n)\n275                 cat = pd.Categorical(var.values, ordered=True)\n276                 codes.append(cat.codes)\n277                 levels.append(cat.categories)\n278 \n279             idx = pd.MultiIndex(levels, codes, names=names)\n280             for n in names:\n281                 dims_to_replace[n] = dim\n282 \n283         vars_to_replace[dim] = IndexVariable(dim, idx)\n284         vars_to_remove.extend(var_names)\n285 \n286     new_variables = {k: v for k, v in variables.items() if k not in vars_to_remove}\n287     new_variables.update(vars_to_replace)\n288 \n289     # update dimensions if necessary, GH: 3512\n290     for k, v in new_variables.items():\n291         if any(d in dims_to_replace for d in v.dims):\n292             new_dims = [dims_to_replace.get(d, d) for d in v.dims]\n293             new_variables[k] = v._replace(dims=new_dims)\n294     new_coord_names = coord_names | set(vars_to_replace)\n295     new_coord_names -= set(vars_to_remove)\n296     return new_variables, new_coord_names\n297 \n298 \n299 def split_indexes(\n300     dims_or_levels: Union[Hashable, Sequence[Hashable]],\n301     variables: Mapping[Hashable, Variable],\n302     coord_names: Set[Hashable],\n303     level_coords: Mapping[Hashable, Hashable],\n304     drop: bool = False,\n305 ) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:\n306     \"\"\"Extract (multi-)indexes (levels) as variables.\n307 \n308     Not public API. Used in Dataset and DataArray reset_index\n309     methods.\n310     \"\"\"\n311     if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):\n312         dims_or_levels = [dims_or_levels]\n313 \n314     dim_levels: DefaultDict[Any, List[Hashable]] = defaultdict(list)\n315     dims = []\n316     for k in dims_or_levels:\n317         if k in level_coords:\n318             dim_levels[level_coords[k]].append(k)\n319         else:\n320             dims.append(k)\n321 \n322     vars_to_replace = {}\n323     vars_to_create: Dict[Hashable, Variable] = {}\n324     vars_to_remove = []\n325 \n326     for d in dims:\n327         index = variables[d].to_index()\n328         if isinstance(index, pd.MultiIndex):\n329             dim_levels[d] = index.names\n330         else:\n331             vars_to_remove.append(d)\n332             if not drop:\n333                 vars_to_create[str(d) + \"_\"] = Variable(d, index, variables[d].attrs)\n334 \n335     for d, levs in dim_levels.items():\n336         index = variables[d].to_index()\n337         if len(levs) == index.nlevels:\n338             vars_to_remove.append(d)\n339         else:\n340             vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))\n341 \n342         if not drop:\n343             for lev in levs:\n344                 idx = index.get_level_values(lev)\n345                 vars_to_create[idx.name] = Variable(d, idx, variables[d].attrs)\n346 \n347     new_variables = dict(variables)\n348     for v in set(vars_to_remove):\n349         del new_variables[v]\n350     new_variables.update(vars_to_replace)\n351     new_variables.update(vars_to_create)\n352     new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n353 \n354     return new_variables, new_coord_names\n355 \n356 \n357 def _assert_empty(args: tuple, msg: str = \"%s\") -> None:\n358     if args:\n359         raise ValueError(msg % args)\n360 \n361 \n362 def _check_chunks_compatibility(var, chunks, preferred_chunks):\n363     for dim in var.dims:\n364         if dim not in chunks or (dim not in preferred_chunks):\n365             continue\n366 \n367         preferred_chunks_dim = preferred_chunks.get(dim)\n368         chunks_dim = chunks.get(dim)\n369 \n370         if isinstance(chunks_dim, int):\n371             chunks_dim = (chunks_dim,)\n372         else:\n373             chunks_dim = chunks_dim[:-1]\n374 \n375         if any(s % preferred_chunks_dim for s in chunks_dim):\n376             warnings.warn(\n377                 f\"Specified Dask chunks {chunks[dim]} would separate \"\n378                 f\"on disks chunk shape {preferred_chunks[dim]} for dimension {dim}. \"\n379                 \"This could degrade performance. \"\n380                 \"Consider rechunking after loading instead.\",\n381                 stacklevel=2,\n382             )\n383 \n384 \n385 def _get_chunk(var, chunks):\n386     # chunks need to be explicity computed to take correctly into accout\n387     # backend preferred chunking\n388     import dask.array as da\n389 \n390     if isinstance(var, IndexVariable):\n391         return {}\n392 \n393     if isinstance(chunks, int) or (chunks == \"auto\"):\n394         chunks = dict.fromkeys(var.dims, chunks)\n395 \n396     preferred_chunks = var.encoding.get(\"preferred_chunks\", {})\n397     preferred_chunks_list = [\n398         preferred_chunks.get(dim, shape) for dim, shape in zip(var.dims, var.shape)\n399     ]\n400 \n401     chunks_list = [\n402         chunks.get(dim, None) or preferred_chunks.get(dim, None) for dim in var.dims\n403     ]\n404 \n405     output_chunks_list = da.core.normalize_chunks(\n406         chunks_list,\n407         shape=var.shape,\n408         dtype=var.dtype,\n409         previous_chunks=preferred_chunks_list,\n410     )\n411 \n412     output_chunks = dict(zip(var.dims, output_chunks_list))\n413     _check_chunks_compatibility(var, output_chunks, preferred_chunks)\n414 \n415     return output_chunks\n416 \n417 \n418 def _maybe_chunk(\n419     name,\n420     var,\n421     chunks,\n422     token=None,\n423     lock=None,\n424     name_prefix=\"xarray-\",\n425     overwrite_encoded_chunks=False,\n426 ):\n427     from dask.base import tokenize\n428 \n429     if chunks is not None:\n430         chunks = {dim: chunks[dim] for dim in var.dims if dim in chunks}\n431     if var.ndim:\n432         # when rechunking by different amounts, make sure dask names change\n433         # by provinding chunks as an input to tokenize.\n434         # subtle bugs result otherwise. see GH3350\n435         token2 = tokenize(name, token if token else var._data, chunks)\n436         name2 = f\"{name_prefix}{name}-{token2}\"\n437         var = var.chunk(chunks, name=name2, lock=lock)\n438 \n439         if overwrite_encoded_chunks and var.chunks is not None:\n440             var.encoding[\"chunks\"] = tuple(x[0] for x in var.chunks)\n441         return var\n442     else:\n443         return var\n444 \n445 \n446 def as_dataset(obj: Any) -> \"Dataset\":\n447     \"\"\"Cast the given object to a Dataset.\n448 \n449     Handles Datasets, DataArrays and dictionaries of variables. A new Dataset\n450     object is only created if the provided object is not already one.\n451     \"\"\"\n452     if hasattr(obj, \"to_dataset\"):\n453         obj = obj.to_dataset()\n454     if not isinstance(obj, Dataset):\n455         obj = Dataset(obj)\n456     return obj\n457 \n458 \n459 class DataVariables(Mapping[Hashable, \"DataArray\"]):\n460     __slots__ = (\"_dataset\",)\n461 \n462     def __init__(self, dataset: \"Dataset\"):\n463         self._dataset = dataset\n464 \n465     def __iter__(self) -> Iterator[Hashable]:\n466         return (\n467             key\n468             for key in self._dataset._variables\n469             if key not in self._dataset._coord_names\n470         )\n471 \n472     def __len__(self) -> int:\n473         return len(self._dataset._variables) - len(self._dataset._coord_names)\n474 \n475     def __contains__(self, key: Hashable) -> bool:\n476         return key in self._dataset._variables and key not in self._dataset._coord_names\n477 \n478     def __getitem__(self, key: Hashable) -> \"DataArray\":\n479         if key not in self._dataset._coord_names:\n480             return cast(\"DataArray\", self._dataset[key])\n481         raise KeyError(key)\n482 \n483     def __repr__(self) -> str:\n484         return formatting.data_vars_repr(self)\n485 \n486     @property\n487     def variables(self) -> Mapping[Hashable, Variable]:\n488         all_variables = self._dataset.variables\n489         return Frozen({k: all_variables[k] for k in self})\n490 \n491     def _ipython_key_completions_(self):\n492         \"\"\"Provide method for the key-autocompletions in IPython. \"\"\"\n493         return [\n494             key\n495             for key in self._dataset._ipython_key_completions_()\n496             if key not in self._dataset._coord_names\n497         ]\n498 \n499 \n500 class _LocIndexer:\n501     __slots__ = (\"dataset\",)\n502 \n503     def __init__(self, dataset: \"Dataset\"):\n504         self.dataset = dataset\n505 \n506     def __getitem__(self, key: Mapping[Hashable, Any]) -> \"Dataset\":\n507         if not utils.is_dict_like(key):\n508             raise TypeError(\"can only lookup dictionaries from Dataset.loc\")\n509         return self.dataset.sel(key)\n510 \n511 \n512 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n513     \"\"\"A multi-dimensional, in memory, array database.\n514 \n515     A dataset resembles an in-memory representation of a NetCDF file,\n516     and consists of variables, coordinates and attributes which\n517     together form a self describing dataset.\n518 \n519     Dataset implements the mapping interface with keys given by variable\n520     names and values given by DataArray objects for each variable name.\n521 \n522     One dimensional variables with name equal to their dimension are\n523     index coordinates used for label based indexing.\n524 \n525     To load data from a file or file-like object, use the `open_dataset`\n526     function.\n527 \n528     Parameters\n529     ----------\n530     data_vars : dict-like, optional\n531         A mapping from variable names to :py:class:`~xarray.DataArray`\n532         objects, :py:class:`~xarray.Variable` objects or to tuples of\n533         the form ``(dims, data[, attrs])`` which can be used as\n534         arguments to create a new ``Variable``. Each dimension must\n535         have the same length in all variables in which it appears.\n536 \n537         The following notations are accepted:\n538 \n539         - mapping {var name: DataArray}\n540         - mapping {var name: Variable}\n541         - mapping {var name: (dimension name, array-like)}\n542         - mapping {var name: (tuple of dimension names, array-like)}\n543         - mapping {dimension name: array-like}\n544           (it will be automatically moved to coords, see below)\n545 \n546         Each dimension must have the same length in all variables in\n547         which it appears.\n548     coords : dict-like, optional\n549         Another mapping in similar form as the `data_vars` argument,\n550         except the each item is saved on the dataset as a \"coordinate\".\n551         These variables have an associated meaning: they describe\n552         constant/fixed/independent quantities, unlike the\n553         varying/measured/dependent quantities that belong in\n554         `variables`. Coordinates values may be given by 1-dimensional\n555         arrays or scalars, in which case `dims` do not need to be\n556         supplied: 1D arrays will be assumed to give index values along\n557         the dimension with the same name.\n558 \n559         The following notations are accepted:\n560 \n561         - mapping {coord name: DataArray}\n562         - mapping {coord name: Variable}\n563         - mapping {coord name: (dimension name, array-like)}\n564         - mapping {coord name: (tuple of dimension names, array-like)}\n565         - mapping {dimension name: array-like}\n566           (the dimension name is implicitly set to be the same as the\n567           coord name)\n568 \n569         The last notation implies that the coord name is the same as\n570         the dimension name.\n571 \n572     attrs : dict-like, optional\n573         Global attributes to save on this dataset.\n574 \n575     Examples\n576     --------\n577     Create data:\n578 \n579     >>> np.random.seed(0)\n580     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n581     >>> precipitation = 10 * np.random.rand(2, 2, 3)\n582     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n583     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n584     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n585     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n586 \n587     Initialize a dataset with multiple dimensions:\n588 \n589     >>> ds = xr.Dataset(\n590     ...     data_vars=dict(\n591     ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n592     ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n593     ...     ),\n594     ...     coords=dict(\n595     ...         lon=([\"x\", \"y\"], lon),\n596     ...         lat=([\"x\", \"y\"], lat),\n597     ...         time=time,\n598     ...         reference_time=reference_time,\n599     ...     ),\n600     ...     attrs=dict(description=\"Weather related data.\"),\n601     ... )\n602     >>> ds\n603     <xarray.Dataset>\n604     Dimensions:         (time: 3, x: 2, y: 2)\n605     Coordinates:\n606         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n607         lat             (x, y) float64 42.25 42.21 42.63 42.59\n608       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n609         reference_time  datetime64[ns] 2014-09-05\n610     Dimensions without coordinates: x, y\n611     Data variables:\n612         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n613         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n614     Attributes:\n615         description:  Weather related data.\n616 \n617     Find out where the coldest temperature was and what values the\n618     other variables had:\n619 \n620     >>> ds.isel(ds.temperature.argmin(...))\n621     <xarray.Dataset>\n622     Dimensions:         ()\n623     Coordinates:\n624         lon             float64 -99.32\n625         lat             float64 42.21\n626         time            datetime64[ns] 2014-09-08\n627         reference_time  datetime64[ns] 2014-09-05\n628     Data variables:\n629         temperature     float64 7.182\n630         precipitation   float64 8.326\n631     Attributes:\n632         description:  Weather related data.\n633     \"\"\"\n634 \n635     _attrs: Optional[Dict[Hashable, Any]]\n636     _cache: Dict[str, Any]\n637     _coord_names: Set[Hashable]\n638     _dims: Dict[Hashable, int]\n639     _encoding: Optional[Dict[Hashable, Any]]\n640     _close: Optional[Callable[[], None]]\n641     _indexes: Optional[Dict[Hashable, pd.Index]]\n642     _variables: Dict[Hashable, Variable]\n643 \n644     __slots__ = (\n645         \"_attrs\",\n646         \"_cache\",\n647         \"_coord_names\",\n648         \"_dims\",\n649         \"_encoding\",\n650         \"_close\",\n651         \"_indexes\",\n652         \"_variables\",\n653         \"__weakref__\",\n654     )\n655 \n656     _groupby_cls = groupby.DatasetGroupBy\n657     _rolling_cls = rolling.DatasetRolling\n658     _coarsen_cls = rolling.DatasetCoarsen\n659     _resample_cls = resample.DatasetResample\n660     _weighted_cls = weighted.DatasetWeighted\n661 \n662     def __init__(\n663         self,\n664         # could make a VariableArgs to use more generally, and refine these\n665         # categories\n666         data_vars: Mapping[Hashable, Any] = None,\n667         coords: Mapping[Hashable, Any] = None,\n668         attrs: Mapping[Hashable, Any] = None,\n669     ):\n670         # TODO(shoyer): expose indexes as a public argument in __init__\n671 \n672         if data_vars is None:\n673             data_vars = {}\n674         if coords is None:\n675             coords = {}\n676 \n677         both_data_and_coords = set(data_vars) & set(coords)\n678         if both_data_and_coords:\n679             raise ValueError(\n680                 \"variables %r are found in both data_vars and coords\"\n681                 % both_data_and_coords\n682             )\n683 \n684         if isinstance(coords, Dataset):\n685             coords = coords.variables\n686 \n687         variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n688             data_vars, coords, compat=\"broadcast_equals\"\n689         )\n690 \n691         self._attrs = dict(attrs) if attrs is not None else None\n692         self._close = None\n693         self._encoding = None\n694         self._variables = variables\n695         self._coord_names = coord_names\n696         self._dims = dims\n697         self._indexes = indexes\n698 \n699     @classmethod\n700     def load_store(cls, store, decoder=None) -> \"Dataset\":\n701         \"\"\"Create a new dataset from the contents of a backends.*DataStore\n702         object\n703         \"\"\"\n704         variables, attributes = store.load()\n705         if decoder:\n706             variables, attributes = decoder(variables, attributes)\n707         obj = cls(variables, attrs=attributes)\n708         obj.set_close(store.close)\n709         return obj\n710 \n711     @property\n712     def variables(self) -> Mapping[Hashable, Variable]:\n713         \"\"\"Low level interface to Dataset contents as dict of Variable objects.\n714 \n715         This ordered dictionary is frozen to prevent mutation that could\n716         violate Dataset invariants. It contains all variable objects\n717         constituting the Dataset, including both data variables and\n718         coordinates.\n719         \"\"\"\n720         return Frozen(self._variables)\n721 \n722     @property\n723     def attrs(self) -> Dict[Hashable, Any]:\n724         \"\"\"Dictionary of global attributes on this dataset\"\"\"\n725         if self._attrs is None:\n726             self._attrs = {}\n727         return self._attrs\n728 \n729     @attrs.setter\n730     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n731         self._attrs = dict(value)\n732 \n733     @property\n734     def encoding(self) -> Dict:\n735         \"\"\"Dictionary of global encoding attributes on this dataset\"\"\"\n736         if self._encoding is None:\n737             self._encoding = {}\n738         return self._encoding\n739 \n740     @encoding.setter\n741     def encoding(self, value: Mapping) -> None:\n742         self._encoding = dict(value)\n743 \n744     @property\n745     def dims(self) -> Mapping[Hashable, int]:\n746         \"\"\"Mapping from dimension names to lengths.\n747 \n748         Cannot be modified directly, but is updated when adding new variables.\n749 \n750         Note that type of this object differs from `DataArray.dims`.\n751         See `Dataset.sizes` and `DataArray.sizes` for consistently named\n752         properties.\n753         \"\"\"\n754         return Frozen(SortedKeysDict(self._dims))\n755 \n756     @property\n757     def sizes(self) -> Mapping[Hashable, int]:\n758         \"\"\"Mapping from dimension names to lengths.\n759 \n760         Cannot be modified directly, but is updated when adding new variables.\n761 \n762         This is an alias for `Dataset.dims` provided for the benefit of\n763         consistency with `DataArray.sizes`.\n764 \n765         See Also\n766         --------\n767         DataArray.sizes\n768         \"\"\"\n769         return self.dims\n770 \n771     def load(self, **kwargs) -> \"Dataset\":\n772         \"\"\"Manually trigger loading and/or computation of this dataset's data\n773         from disk or a remote source into memory and return this dataset.\n774         Unlike compute, the original dataset is modified and returned.\n775 \n776         Normally, it should not be necessary to call this method in user code,\n777         because all xarray functions should either work on deferred data or\n778         load data automatically. However, this method can be necessary when\n779         working with many file objects on disk.\n780 \n781         Parameters\n782         ----------\n783         **kwargs : dict\n784             Additional keyword arguments passed on to ``dask.compute``.\n785 \n786         See Also\n787         --------\n788         dask.compute\n789         \"\"\"\n790         # access .data to coerce everything to numpy or dask arrays\n791         lazy_data = {\n792             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n793         }\n794         if lazy_data:\n795             import dask.array as da\n796 \n797             # evaluate all the dask arrays simultaneously\n798             evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n799 \n800             for k, data in zip(lazy_data, evaluated_data):\n801                 self.variables[k].data = data\n802 \n803         # load everything else sequentially\n804         for k, v in self.variables.items():\n805             if k not in lazy_data:\n806                 v.load()\n807 \n808         return self\n809 \n810     def __dask_tokenize__(self):\n811         from dask.base import normalize_token\n812 \n813         return normalize_token(\n814             (type(self), self._variables, self._coord_names, self._attrs)\n815         )\n816 \n817     def __dask_graph__(self):\n818         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}\n819         graphs = {k: v for k, v in graphs.items() if v is not None}\n820         if not graphs:\n821             return None\n822         else:\n823             try:\n824                 from dask.highlevelgraph import HighLevelGraph\n825 \n826                 return HighLevelGraph.merge(*graphs.values())\n827             except ImportError:\n828                 from dask import sharedict\n829 \n830                 return sharedict.merge(*graphs.values())\n831 \n832     def __dask_keys__(self):\n833         import dask\n834 \n835         return [\n836             v.__dask_keys__()\n837             for v in self.variables.values()\n838             if dask.is_dask_collection(v)\n839         ]\n840 \n841     def __dask_layers__(self):\n842         import dask\n843 \n844         return sum(\n845             [\n846                 v.__dask_layers__()\n847                 for v in self.variables.values()\n848                 if dask.is_dask_collection(v)\n849             ],\n850             (),\n851         )\n852 \n853     @property\n854     def __dask_optimize__(self):\n855         import dask.array as da\n856 \n857         return da.Array.__dask_optimize__\n858 \n859     @property\n860     def __dask_scheduler__(self):\n861         import dask.array as da\n862 \n863         return da.Array.__dask_scheduler__\n864 \n865     def __dask_postcompute__(self):\n866         import dask\n867 \n868         info = [\n869             (True, k, v.__dask_postcompute__())\n870             if dask.is_dask_collection(v)\n871             else (False, k, v)\n872             for k, v in self._variables.items()\n873         ]\n874         args = (\n875             info,\n876             self._coord_names,\n877             self._dims,\n878             self._attrs,\n879             self._indexes,\n880             self._encoding,\n881             self._close,\n882         )\n883         return self._dask_postcompute, args\n884 \n885     def __dask_postpersist__(self):\n886         import dask\n887 \n888         info = [\n889             (True, k, v.__dask_postpersist__())\n890             if dask.is_dask_collection(v)\n891             else (False, k, v)\n892             for k, v in self._variables.items()\n893         ]\n894         args = (\n895             info,\n896             self._coord_names,\n897             self._dims,\n898             self._attrs,\n899             self._indexes,\n900             self._encoding,\n901             self._close,\n902         )\n903         return self._dask_postpersist, args\n904 \n905     @staticmethod\n906     def _dask_postcompute(results, info, *args):\n907         variables = {}\n908         results2 = list(results[::-1])\n909         for is_dask, k, v in info:\n910             if is_dask:\n911                 func, args2 = v\n912                 r = results2.pop()\n913                 result = func(r, *args2)\n914             else:\n915                 result = v\n916             variables[k] = result\n917 \n918         final = Dataset._construct_direct(variables, *args)\n919         return final\n920 \n921     @staticmethod\n922     def _dask_postpersist(dsk, info, *args):\n923         variables = {}\n924         # postpersist is called in both dask.optimize and dask.persist\n925         # When persisting, we want to filter out unrelated keys for\n926         # each Variable's task graph.\n927         is_persist = len(dsk) == len(info)\n928         for is_dask, k, v in info:\n929             if is_dask:\n930                 func, args2 = v\n931                 if is_persist:\n932                     name = args2[1][0]\n933                     dsk2 = {k: v for k, v in dsk.items() if k[0] == name}\n934                 else:\n935                     dsk2 = dsk\n936                 result = func(dsk2, *args2)\n937             else:\n938                 result = v\n939             variables[k] = result\n940 \n941         return Dataset._construct_direct(variables, *args)\n942 \n943     def compute(self, **kwargs) -> \"Dataset\":\n944         \"\"\"Manually trigger loading and/or computation of this dataset's data\n945         from disk or a remote source into memory and return a new dataset.\n946         Unlike load, the original dataset is left unaltered.\n947 \n948         Normally, it should not be necessary to call this method in user code,\n949         because all xarray functions should either work on deferred data or\n950         load data automatically. However, this method can be necessary when\n951         working with many file objects on disk.\n952 \n953         Parameters\n954         ----------\n955         **kwargs : dict\n956             Additional keyword arguments passed on to ``dask.compute``.\n957 \n958         See Also\n959         --------\n960         dask.compute\n961         \"\"\"\n962         new = self.copy(deep=False)\n963         return new.load(**kwargs)\n964 \n965     def _persist_inplace(self, **kwargs) -> \"Dataset\":\n966         \"\"\"Persist all Dask arrays in memory\"\"\"\n967         # access .data to coerce everything to numpy or dask arrays\n968         lazy_data = {\n969             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n970         }\n971         if lazy_data:\n972             import dask\n973 \n974             # evaluate all the dask arrays simultaneously\n975             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n976 \n977             for k, data in zip(lazy_data, evaluated_data):\n978                 self.variables[k].data = data\n979 \n980         return self\n981 \n982     def persist(self, **kwargs) -> \"Dataset\":\n983         \"\"\"Trigger computation, keeping data as dask arrays\n984 \n985         This operation can be used to trigger computation on underlying dask\n986         arrays, similar to ``.compute()`` or ``.load()``.  However this\n987         operation keeps the data as dask arrays. This is particularly useful\n988         when using the dask.distributed scheduler and you want to load a large\n989         amount of data into distributed memory.\n990 \n991         Parameters\n992         ----------\n993         **kwargs : dict\n994             Additional keyword arguments passed on to ``dask.persist``.\n995 \n996         See Also\n997         --------\n998         dask.persist\n999         \"\"\"\n1000         new = self.copy(deep=False)\n1001         return new._persist_inplace(**kwargs)\n1002 \n1003     @classmethod\n1004     def _construct_direct(\n1005         cls,\n1006         variables,\n1007         coord_names,\n1008         dims=None,\n1009         attrs=None,\n1010         indexes=None,\n1011         encoding=None,\n1012         close=None,\n1013     ):\n1014         \"\"\"Shortcut around __init__ for internal use when we want to skip\n1015         costly validation\n1016         \"\"\"\n1017         if dims is None:\n1018             dims = calculate_dimensions(variables)\n1019         obj = object.__new__(cls)\n1020         obj._variables = variables\n1021         obj._coord_names = coord_names\n1022         obj._dims = dims\n1023         obj._indexes = indexes\n1024         obj._attrs = attrs\n1025         obj._close = close\n1026         obj._encoding = encoding\n1027         return obj\n1028 \n1029     def _replace(\n1030         self,\n1031         variables: Dict[Hashable, Variable] = None,\n1032         coord_names: Set[Hashable] = None,\n1033         dims: Dict[Any, int] = None,\n1034         attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n1035         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,\n1036         encoding: Union[dict, None, Default] = _default,\n1037         inplace: bool = False,\n1038     ) -> \"Dataset\":\n1039         \"\"\"Fastpath constructor for internal use.\n1040 \n1041         Returns an object with optionally with replaced attributes.\n1042 \n1043         Explicitly passed arguments are *not* copied when placed on the new\n1044         dataset. It is up to the caller to ensure that they have the right type\n1045         and are not used elsewhere.\n1046         \"\"\"\n1047         if inplace:\n1048             if variables is not None:\n1049                 self._variables = variables\n1050             if coord_names is not None:\n1051                 self._coord_names = coord_names\n1052             if dims is not None:\n1053                 self._dims = dims\n1054             if attrs is not _default:\n1055                 self._attrs = attrs\n1056             if indexes is not _default:\n1057                 self._indexes = indexes\n1058             if encoding is not _default:\n1059                 self._encoding = encoding\n1060             obj = self\n1061         else:\n1062             if variables is None:\n1063                 variables = self._variables.copy()\n1064             if coord_names is None:\n1065                 coord_names = self._coord_names.copy()\n1066             if dims is None:\n1067                 dims = self._dims.copy()\n1068             if attrs is _default:\n1069                 attrs = copy.copy(self._attrs)\n1070             if indexes is _default:\n1071                 indexes = copy.copy(self._indexes)\n1072             if encoding is _default:\n1073                 encoding = copy.copy(self._encoding)\n1074             obj = self._construct_direct(\n1075                 variables, coord_names, dims, attrs, indexes, encoding\n1076             )\n1077         return obj\n1078 \n1079     def _replace_with_new_dims(\n1080         self,\n1081         variables: Dict[Hashable, Variable],\n1082         coord_names: set = None,\n1083         attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n1084         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,\n1085         inplace: bool = False,\n1086     ) -> \"Dataset\":\n1087         \"\"\"Replace variables with recalculated dimensions.\"\"\"\n1088         dims = calculate_dimensions(variables)\n1089         return self._replace(\n1090             variables, coord_names, dims, attrs, indexes, inplace=inplace\n1091         )\n1092 \n1093     def _replace_vars_and_dims(\n1094         self,\n1095         variables: Dict[Hashable, Variable],\n1096         coord_names: set = None,\n1097         dims: Dict[Hashable, int] = None,\n1098         attrs: Union[Dict[Hashable, Any], None, Default] = _default,\n1099         inplace: bool = False,\n1100     ) -> \"Dataset\":\n1101         \"\"\"Deprecated version of _replace_with_new_dims().\n1102 \n1103         Unlike _replace_with_new_dims(), this method always recalculates\n1104         indexes from variables.\n1105         \"\"\"\n1106         if dims is None:\n1107             dims = calculate_dimensions(variables)\n1108         return self._replace(\n1109             variables, coord_names, dims, attrs, indexes=None, inplace=inplace\n1110         )\n1111 \n1112     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> \"Dataset\":\n1113         if not indexes:\n1114             return self\n1115 \n1116         variables = self._variables.copy()\n1117         new_indexes = dict(self.indexes)\n1118         for name, idx in indexes.items():\n1119             variables[name] = IndexVariable(name, idx)\n1120             new_indexes[name] = idx\n1121         obj = self._replace(variables, indexes=new_indexes)\n1122 \n1123         # switch from dimension to level names, if necessary\n1124         dim_names: Dict[Hashable, str] = {}\n1125         for dim, idx in indexes.items():\n1126             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n1127                 dim_names[dim] = idx.name\n1128         if dim_names:\n1129             obj = obj.rename(dim_names)\n1130         return obj\n1131 \n1132     def copy(self, deep: bool = False, data: Mapping = None) -> \"Dataset\":\n1133         \"\"\"Returns a copy of this dataset.\n1134 \n1135         If `deep=True`, a deep copy is made of each of the component variables.\n1136         Otherwise, a shallow copy of each of the component variable is made, so\n1137         that the underlying memory region of the new dataset is the same as in\n1138         the original dataset.\n1139 \n1140         Use `data` to create a new object with the same structure as\n1141         original but entirely new data.\n1142 \n1143         Parameters\n1144         ----------\n1145         deep : bool, optional\n1146             Whether each component variable is loaded into memory and copied onto\n1147             the new object. Default is False.\n1148         data : dict-like, optional\n1149             Data to use in the new object. Each item in `data` must have same\n1150             shape as corresponding data variable in original. When `data` is\n1151             used, `deep` is ignored for the data variables and only used for\n1152             coords.\n1153 \n1154         Returns\n1155         -------\n1156         object : Dataset\n1157             New object with dimensions, attributes, coordinates, name, encoding,\n1158             and optionally data copied from original.\n1159 \n1160         Examples\n1161         --------\n1162         Shallow copy versus deep copy\n1163 \n1164         >>> da = xr.DataArray(np.random.randn(2, 3))\n1165         >>> ds = xr.Dataset(\n1166         ...     {\"foo\": da, \"bar\": (\"x\", [-1, 2])},\n1167         ...     coords={\"x\": [\"one\", \"two\"]},\n1168         ... )\n1169         >>> ds.copy()\n1170         <xarray.Dataset>\n1171         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1172         Coordinates:\n1173           * x        (x) <U3 'one' 'two'\n1174         Dimensions without coordinates: dim_0, dim_1\n1175         Data variables:\n1176             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n1177             bar      (x) int64 -1 2\n1178 \n1179         >>> ds_0 = ds.copy(deep=False)\n1180         >>> ds_0[\"foo\"][0, 0] = 7\n1181         >>> ds_0\n1182         <xarray.Dataset>\n1183         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1184         Coordinates:\n1185           * x        (x) <U3 'one' 'two'\n1186         Dimensions without coordinates: dim_0, dim_1\n1187         Data variables:\n1188             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1189             bar      (x) int64 -1 2\n1190 \n1191         >>> ds\n1192         <xarray.Dataset>\n1193         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1194         Coordinates:\n1195           * x        (x) <U3 'one' 'two'\n1196         Dimensions without coordinates: dim_0, dim_1\n1197         Data variables:\n1198             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1199             bar      (x) int64 -1 2\n1200 \n1201         Changing the data using the ``data`` argument maintains the\n1202         structure of the original object, but with the new data. Original\n1203         object is unaffected.\n1204 \n1205         >>> ds.copy(data={\"foo\": np.arange(6).reshape(2, 3), \"bar\": [\"a\", \"b\"]})\n1206         <xarray.Dataset>\n1207         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1208         Coordinates:\n1209           * x        (x) <U3 'one' 'two'\n1210         Dimensions without coordinates: dim_0, dim_1\n1211         Data variables:\n1212             foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n1213             bar      (x) <U1 'a' 'b'\n1214 \n1215         >>> ds\n1216         <xarray.Dataset>\n1217         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1218         Coordinates:\n1219           * x        (x) <U3 'one' 'two'\n1220         Dimensions without coordinates: dim_0, dim_1\n1221         Data variables:\n1222             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1223             bar      (x) int64 -1 2\n1224 \n1225         See Also\n1226         --------\n1227         pandas.DataFrame.copy\n1228         \"\"\"\n1229         if data is None:\n1230             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}\n1231         elif not utils.is_dict_like(data):\n1232             raise ValueError(\"Data must be dict-like\")\n1233         else:\n1234             var_keys = set(self.data_vars.keys())\n1235             data_keys = set(data.keys())\n1236             keys_not_in_vars = data_keys - var_keys\n1237             if keys_not_in_vars:\n1238                 raise ValueError(\n1239                     \"Data must only contain variables in original \"\n1240                     \"dataset. Extra variables: {}\".format(keys_not_in_vars)\n1241                 )\n1242             keys_missing_from_data = var_keys - data_keys\n1243             if keys_missing_from_data:\n1244                 raise ValueError(\n1245                     \"Data must contain all variables in original \"\n1246                     \"dataset. Data is missing {}\".format(keys_missing_from_data)\n1247                 )\n1248             variables = {\n1249                 k: v.copy(deep=deep, data=data.get(k))\n1250                 for k, v in self._variables.items()\n1251             }\n1252 \n1253         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)\n1254 \n1255         return self._replace(variables, attrs=attrs)\n1256 \n1257     @property\n1258     def _level_coords(self) -> Dict[str, Hashable]:\n1259         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n1260         coordinate name.\n1261         \"\"\"\n1262         level_coords: Dict[str, Hashable] = {}\n1263         for name, index in self.indexes.items():\n1264             if isinstance(index, pd.MultiIndex):\n1265                 level_names = index.names\n1266                 (dim,) = self.variables[name].dims\n1267                 level_coords.update({lname: dim for lname in level_names})\n1268         return level_coords\n1269 \n1270     def _copy_listed(self, names: Iterable[Hashable]) -> \"Dataset\":\n1271         \"\"\"Create a new Dataset with the listed variables from this dataset and\n1272         the all relevant coordinates. Skips all validation.\n1273         \"\"\"\n1274         variables: Dict[Hashable, Variable] = {}\n1275         coord_names = set()\n1276         indexes: Dict[Hashable, pd.Index] = {}\n1277 \n1278         for name in names:\n1279             try:\n1280                 variables[name] = self._variables[name]\n1281             except KeyError:\n1282                 ref_name, var_name, var = _get_virtual_variable(\n1283                     self._variables, name, self._level_coords, self.dims\n1284                 )\n1285                 variables[var_name] = var\n1286                 if ref_name in self._coord_names or ref_name in self.dims:\n1287                     coord_names.add(var_name)\n1288                 if (var_name,) == var.dims:\n1289                     indexes[var_name] = var.to_index()\n1290 \n1291         needed_dims: Set[Hashable] = set()\n1292         for v in variables.values():\n1293             needed_dims.update(v.dims)\n1294 \n1295         dims = {k: self.dims[k] for k in needed_dims}\n1296 \n1297         # preserves ordering of coordinates\n1298         for k in self._variables:\n1299             if k not in self._coord_names:\n1300                 continue\n1301 \n1302             if set(self.variables[k].dims) <= needed_dims:\n1303                 variables[k] = self._variables[k]\n1304                 coord_names.add(k)\n1305                 if k in self.indexes:\n1306                     indexes[k] = self.indexes[k]\n1307 \n1308         return self._replace(variables, coord_names, dims, indexes=indexes)\n1309 \n1310     def _construct_dataarray(self, name: Hashable) -> \"DataArray\":\n1311         \"\"\"Construct a DataArray by indexing this dataset\"\"\"\n1312         from .dataarray import DataArray\n1313 \n1314         try:\n1315             variable = self._variables[name]\n1316         except KeyError:\n1317             _, name, variable = _get_virtual_variable(\n1318                 self._variables, name, self._level_coords, self.dims\n1319             )\n1320 \n1321         needed_dims = set(variable.dims)\n1322 \n1323         coords: Dict[Hashable, Variable] = {}\n1324         # preserve ordering\n1325         for k in self._variables:\n1326             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:\n1327                 coords[k] = self.variables[k]\n1328 \n1329         if self._indexes is None:\n1330             indexes = None\n1331         else:\n1332             indexes = {k: v for k, v in self._indexes.items() if k in coords}\n1333 \n1334         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1335 \n1336     def __copy__(self) -> \"Dataset\":\n1337         return self.copy(deep=False)\n1338 \n1339     def __deepcopy__(self, memo=None) -> \"Dataset\":\n1340         # memo does nothing but is required for compatibility with\n1341         # copy.deepcopy\n1342         return self.copy(deep=True)\n1343 \n1344     @property\n1345     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1346         \"\"\"Places to look-up items for attribute-style access\"\"\"\n1347         yield from self._item_sources\n1348         yield self.attrs\n1349 \n1350     @property\n1351     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1352         \"\"\"Places to look-up items for key-completion\"\"\"\n1353         yield self.data_vars\n1354         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)\n1355 \n1356         # virtual coordinates\n1357         yield HybridMappingProxy(keys=self.dims, mapping=self)\n1358 \n1359         # uses empty dict -- everything here can already be found in self.coords.\n1360         yield HybridMappingProxy(keys=self._level_coords, mapping={})\n1361 \n1362     def __contains__(self, key: object) -> bool:\n1363         \"\"\"The 'in' operator will return true or false depending on whether\n1364         'key' is an array in the dataset or not.\n1365         \"\"\"\n1366         return key in self._variables\n1367 \n1368     def __len__(self) -> int:\n1369         return len(self.data_vars)\n1370 \n1371     def __bool__(self) -> bool:\n1372         return bool(self.data_vars)\n1373 \n1374     def __iter__(self) -> Iterator[Hashable]:\n1375         return iter(self.data_vars)\n1376 \n1377     def __array__(self, dtype=None):\n1378         raise TypeError(\n1379             \"cannot directly convert an xarray.Dataset into a \"\n1380             \"numpy array. Instead, create an xarray.DataArray \"\n1381             \"first, either with indexing on the Dataset or by \"\n1382             \"invoking the `to_array()` method.\"\n1383         )\n1384 \n1385     @property\n1386     def nbytes(self) -> int:\n1387         return sum(v.nbytes for v in self.variables.values())\n1388 \n1389     @property\n1390     def loc(self) -> _LocIndexer:\n1391         \"\"\"Attribute for location based indexing. Only supports __getitem__,\n1392         and only when the key is a dict of the form {dim: labels}.\n1393         \"\"\"\n1394         return _LocIndexer(self)\n1395 \n1396     # FIXME https://github.com/python/mypy/issues/7328\n1397     @overload\n1398     def __getitem__(self, key: Mapping) -> \"Dataset\":  # type: ignore\n1399         ...\n1400 \n1401     @overload\n1402     def __getitem__(self, key: Hashable) -> \"DataArray\":  # type: ignore\n1403         ...\n1404 \n1405     @overload\n1406     def __getitem__(self, key: Any) -> \"Dataset\":\n1407         ...\n1408 \n1409     def __getitem__(self, key):\n1410         \"\"\"Access variables or coordinates this dataset as a\n1411         :py:class:`~xarray.DataArray`.\n1412 \n1413         Indexing with a list of names will return a new ``Dataset`` object.\n1414         \"\"\"\n1415         if utils.is_dict_like(key):\n1416             return self.isel(**cast(Mapping, key))\n1417 \n1418         if hashable(key):\n1419             return self._construct_dataarray(key)\n1420         else:\n1421             return self._copy_listed(np.asarray(key))\n1422 \n1423     def __setitem__(self, key: Hashable, value) -> None:\n1424         \"\"\"Add an array to this dataset.\n1425 \n1426         If value is a `DataArray`, call its `select_vars()` method, rename it\n1427         to `key` and merge the contents of the resulting dataset into this\n1428         dataset.\n1429 \n1430         If value is an `Variable` object (or tuple of form\n1431         ``(dims, data[, attrs])``), add it to this dataset as a new\n1432         variable.\n1433         \"\"\"\n1434         if utils.is_dict_like(key):\n1435             raise NotImplementedError(\n1436                 \"cannot yet use a dictionary as a key to set Dataset values\"\n1437             )\n1438 \n1439         self.update({key: value})\n1440 \n1441     def __delitem__(self, key: Hashable) -> None:\n1442         \"\"\"Remove a variable from this dataset.\"\"\"\n1443         del self._variables[key]\n1444         self._coord_names.discard(key)\n1445         if key in self.indexes:\n1446             assert self._indexes is not None\n1447             del self._indexes[key]\n1448         self._dims = calculate_dimensions(self._variables)\n1449 \n1450     # mutable objects should not be hashable\n1451     # https://github.com/python/mypy/issues/4266\n1452     __hash__ = None  # type: ignore\n1453 \n1454     def _all_compat(self, other: \"Dataset\", compat_str: str) -> bool:\n1455         \"\"\"Helper function for equals and identical\"\"\"\n1456 \n1457         # some stores (e.g., scipy) do not seem to preserve order, so don't\n1458         # require matching order for equality\n1459         def compat(x: Variable, y: Variable) -> bool:\n1460             return getattr(x, compat_str)(y)\n1461 \n1462         return self._coord_names == other._coord_names and utils.dict_equiv(\n1463             self._variables, other._variables, compat=compat\n1464         )\n1465 \n1466     def broadcast_equals(self, other: \"Dataset\") -> bool:\n1467         \"\"\"Two Datasets are broadcast equal if they are equal after\n1468         broadcasting all variables against each other.\n1469 \n1470         For example, variables that are scalar in one dataset but non-scalar in\n1471         the other dataset can still be broadcast equal if the the non-scalar\n1472         variable is a constant.\n1473 \n1474         See Also\n1475         --------\n1476         Dataset.equals\n1477         Dataset.identical\n1478         \"\"\"\n1479         try:\n1480             return self._all_compat(other, \"broadcast_equals\")\n1481         except (TypeError, AttributeError):\n1482             return False\n1483 \n1484     def equals(self, other: \"Dataset\") -> bool:\n1485         \"\"\"Two Datasets are equal if they have matching variables and\n1486         coordinates, all of which are equal.\n1487 \n1488         Datasets can still be equal (like pandas objects) if they have NaN\n1489         values in the same locations.\n1490 \n1491         This method is necessary because `v1 == v2` for ``Dataset``\n1492         does element-wise comparisons (like numpy.ndarrays).\n1493 \n1494         See Also\n1495         --------\n1496         Dataset.broadcast_equals\n1497         Dataset.identical\n1498         \"\"\"\n1499         try:\n1500             return self._all_compat(other, \"equals\")\n1501         except (TypeError, AttributeError):\n1502             return False\n1503 \n1504     def identical(self, other: \"Dataset\") -> bool:\n1505         \"\"\"Like equals, but also checks all dataset attributes and the\n1506         attributes on all variables and coordinates.\n1507 \n1508         See Also\n1509         --------\n1510         Dataset.broadcast_equals\n1511         Dataset.equals\n1512         \"\"\"\n1513         try:\n1514             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(\n1515                 other, \"identical\"\n1516             )\n1517         except (TypeError, AttributeError):\n1518             return False\n1519 \n1520     @property\n1521     def indexes(self) -> Indexes:\n1522         \"\"\"Mapping of pandas.Index objects used for label based indexing\"\"\"\n1523         if self._indexes is None:\n1524             self._indexes = default_indexes(self._variables, self._dims)\n1525         return Indexes(self._indexes)\n1526 \n1527     @property\n1528     def coords(self) -> DatasetCoordinates:\n1529         \"\"\"Dictionary of xarray.DataArray objects corresponding to coordinate\n1530         variables\n1531         \"\"\"\n1532         return DatasetCoordinates(self)\n1533 \n1534     @property\n1535     def data_vars(self) -> DataVariables:\n1536         \"\"\"Dictionary of DataArray objects corresponding to data variables\"\"\"\n1537         return DataVariables(self)\n1538 \n1539     def set_coords(self, names: \"Union[Hashable, Iterable[Hashable]]\") -> \"Dataset\":\n1540         \"\"\"Given names of one or more variables, set them as coordinates\n1541 \n1542         Parameters\n1543         ----------\n1544         names : hashable or iterable of hashable\n1545             Name(s) of variables in this dataset to convert into coordinates.\n1546 \n1547         Returns\n1548         -------\n1549         Dataset\n1550 \n1551         See Also\n1552         --------\n1553         Dataset.swap_dims\n1554         \"\"\"\n1555         # TODO: allow inserting new coordinates with this method, like\n1556         # DataFrame.set_index?\n1557         # nb. check in self._variables, not self.data_vars to insure that the\n1558         # operation is idempotent\n1559         if isinstance(names, str) or not isinstance(names, Iterable):\n1560             names = [names]\n1561         else:\n1562             names = list(names)\n1563         self._assert_all_in_dataset(names)\n1564         obj = self.copy()\n1565         obj._coord_names.update(names)\n1566         return obj\n1567 \n1568     def reset_coords(\n1569         self,\n1570         names: \"Union[Hashable, Iterable[Hashable], None]\" = None,\n1571         drop: bool = False,\n1572     ) -> \"Dataset\":\n1573         \"\"\"Given names of coordinates, reset them to become variables\n1574 \n1575         Parameters\n1576         ----------\n1577         names : hashable or iterable of hashable, optional\n1578             Name(s) of non-index coordinates in this dataset to reset into\n1579             variables. By default, all non-index coordinates are reset.\n1580         drop : bool, optional\n1581             If True, remove coordinates instead of converting them into\n1582             variables.\n1583 \n1584         Returns\n1585         -------\n1586         Dataset\n1587         \"\"\"\n1588         if names is None:\n1589             names = self._coord_names - set(self.dims)\n1590         else:\n1591             if isinstance(names, str) or not isinstance(names, Iterable):\n1592                 names = [names]\n1593             else:\n1594                 names = list(names)\n1595             self._assert_all_in_dataset(names)\n1596             bad_coords = set(names) & set(self.dims)\n1597             if bad_coords:\n1598                 raise ValueError(\n1599                     \"cannot remove index coordinates with reset_coords: %s\" % bad_coords\n1600                 )\n1601         obj = self.copy()\n1602         obj._coord_names.difference_update(names)\n1603         if drop:\n1604             for name in names:\n1605                 del obj._variables[name]\n1606         return obj\n1607 \n1608     def dump_to_store(self, store: \"AbstractDataStore\", **kwargs) -> None:\n1609         \"\"\"Store dataset contents to a backends.*DataStore object.\"\"\"\n1610         from ..backends.api import dump_to_store\n1611 \n1612         # TODO: rename and/or cleanup this method to make it more consistent\n1613         # with to_netcdf()\n1614         dump_to_store(self, store, **kwargs)\n1615 \n1616     def to_netcdf(\n1617         self,\n1618         path=None,\n1619         mode: str = \"w\",\n1620         format: str = None,\n1621         group: str = None,\n1622         engine: str = None,\n1623         encoding: Mapping = None,\n1624         unlimited_dims: Iterable[Hashable] = None,\n1625         compute: bool = True,\n1626         invalid_netcdf: bool = False,\n1627     ) -> Union[bytes, \"Delayed\", None]:\n1628         \"\"\"Write dataset contents to a netCDF file.\n1629 \n1630         Parameters\n1631         ----------\n1632         path : str, Path or file-like, optional\n1633             Path to which to save this dataset. File-like objects are only\n1634             supported by the scipy engine. If no path is provided, this\n1635             function returns the resulting netCDF file as bytes; in this case,\n1636             we need to use scipy, which does not support netCDF version 4 (the\n1637             default format becomes NETCDF3_64BIT).\n1638         mode : {\"w\", \"a\"}, default: \"w\"\n1639             Write ('w') or append ('a') mode. If mode='w', any existing file at\n1640             this location will be overwritten. If mode='a', existing variables\n1641             will be overwritten.\n1642         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n1643                   \"NETCDF3_CLASSIC\"}, optional\n1644             File format for the resulting netCDF file:\n1645 \n1646             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n1647               features.\n1648             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n1649               netCDF 3 compatible API features.\n1650             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n1651               which fully supports 2+ GB files, but is only compatible with\n1652               clients linked against netCDF version 3.6.0 or later.\n1653             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n1654               handle 2+ GB files very well.\n1655 \n1656             All formats are supported by the netCDF4-python library.\n1657             scipy.io.netcdf only supports the last two formats.\n1658 \n1659             The default format is NETCDF4 if you are saving a file to disk and\n1660             have the netCDF4-python library available. Otherwise, xarray falls\n1661             back to using scipy to write netCDF files and defaults to the\n1662             NETCDF3_64BIT format (scipy does not support netCDF4).\n1663         group : str, optional\n1664             Path to the netCDF4 group in the given file to open (only works for\n1665             format='NETCDF4'). The group(s) will be created if necessary.\n1666         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n1667             Engine to use when writing netCDF files. If not provided, the\n1668             default engine is chosen based on available dependencies, with a\n1669             preference for 'netcdf4' if writing to a file on disk.\n1670         encoding : dict, optional\n1671             Nested dictionary with variable names as keys and dictionaries of\n1672             variable specific encodings as values, e.g.,\n1673             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n1674             \"zlib\": True}, ...}``\n1675 \n1676             The `h5netcdf` engine supports both the NetCDF4-style compression\n1677             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n1678             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n1679             This allows using any compression plugin installed in the HDF5\n1680             library, e.g. LZF.\n1681 \n1682         unlimited_dims : iterable of hashable, optional\n1683             Dimension(s) that should be serialized as unlimited dimensions.\n1684             By default, no dimensions are treated as unlimited dimensions.\n1685             Note that unlimited_dims may also be set via\n1686             ``dataset.encoding[\"unlimited_dims\"]``.\n1687         compute: bool, default: True\n1688             If true compute immediately, otherwise return a\n1689             ``dask.delayed.Delayed`` object that can be computed later.\n1690         invalid_netcdf: bool, default: False\n1691             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n1692             hdf5 files which are invalid netcdf as described in\n1693             https://github.com/shoyer/h5netcdf.\n1694         \"\"\"\n1695         if encoding is None:\n1696             encoding = {}\n1697         from ..backends.api import to_netcdf\n1698 \n1699         return to_netcdf(\n1700             self,\n1701             path,\n1702             mode,\n1703             format=format,\n1704             group=group,\n1705             engine=engine,\n1706             encoding=encoding,\n1707             unlimited_dims=unlimited_dims,\n1708             compute=compute,\n1709             invalid_netcdf=invalid_netcdf,\n1710         )\n1711 \n1712     def to_zarr(\n1713         self,\n1714         store: Union[MutableMapping, str, Path] = None,\n1715         chunk_store: Union[MutableMapping, str, Path] = None,\n1716         mode: str = None,\n1717         synchronizer=None,\n1718         group: str = None,\n1719         encoding: Mapping = None,\n1720         compute: bool = True,\n1721         consolidated: bool = False,\n1722         append_dim: Hashable = None,\n1723         region: Mapping[str, slice] = None,\n1724     ) -> \"ZarrStore\":\n1725         \"\"\"Write dataset contents to a zarr group.\n1726 \n1727         .. note:: Experimental\n1728                   The Zarr backend is new and experimental. Please report any\n1729                   unexpected behavior via github issues.\n1730 \n1731         Parameters\n1732         ----------\n1733         store : MutableMapping, str or Path, optional\n1734             Store or path to directory in file system.\n1735         chunk_store : MutableMapping, str or Path, optional\n1736             Store or path to directory in file system only for Zarr array chunks.\n1737             Requires zarr-python v2.4.0 or later.\n1738         mode : {\"w\", \"w-\", \"a\", None}, optional\n1739             Persistence mode: \"w\" means create (overwrite if exists);\n1740             \"w-\" means create (fail if exists);\n1741             \"a\" means override existing variables (create if does not exist).\n1742             If ``append_dim`` is set, ``mode`` can be omitted as it is\n1743             internally set to ``\"a\"``. Otherwise, ``mode`` will default to\n1744             `w-` if not set.\n1745         synchronizer : object, optional\n1746             Zarr array synchronizer.\n1747         group : str, optional\n1748             Group path. (a.k.a. `path` in zarr terminology.)\n1749         encoding : dict, optional\n1750             Nested dictionary with variable names as keys and dictionaries of\n1751             variable specific encodings as values, e.g.,\n1752             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n1753         compute : bool, optional\n1754             If True write array data immediately, otherwise return a\n1755             ``dask.delayed.Delayed`` object that can be computed to write\n1756             array data later. Metadata is always updated eagerly.\n1757         consolidated : bool, optional\n1758             If True, apply zarr's `consolidate_metadata` function to the store\n1759             after writing metadata.\n1760         append_dim : hashable, optional\n1761             If set, the dimension along which the data will be appended. All\n1762             other dimensions on overriden variables must remain the same size.\n1763         region : dict, optional\n1764             Optional mapping from dimension names to integer slices along\n1765             dataset dimensions to indicate the region of existing zarr array(s)\n1766             in which to write this dataset's data. For example,\n1767             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n1768             that values should be written to the region ``0:1000`` along ``x``\n1769             and ``10000:11000`` along ``y``.\n1770 \n1771             Two restrictions apply to the use of ``region``:\n1772 \n1773             - If ``region`` is set, _all_ variables in a dataset must have at\n1774               least one dimension in common with the region. Other variables\n1775               should be written in a separate call to ``to_zarr()``.\n1776             - Dimensions cannot be included in both ``region`` and\n1777               ``append_dim`` at the same time. To create empty arrays to fill\n1778               in with ``region``, use a separate call to ``to_zarr()`` with\n1779               ``compute=False``. See \"Appending to existing Zarr stores\" in\n1780               the reference documentation for full details.\n1781 \n1782         References\n1783         ----------\n1784         https://zarr.readthedocs.io/\n1785 \n1786         Notes\n1787         -----\n1788         Zarr chunking behavior:\n1789             If chunks are found in the encoding argument or attribute\n1790             corresponding to any DataArray, those chunks are used.\n1791             If a DataArray is a dask array, it is written with those chunks.\n1792             If not other chunks are found, Zarr uses its own heuristics to\n1793             choose automatic chunk sizes.\n1794         \"\"\"\n1795         from ..backends.api import to_zarr\n1796 \n1797         if encoding is None:\n1798             encoding = {}\n1799 \n1800         return to_zarr(\n1801             self,\n1802             store=store,\n1803             chunk_store=chunk_store,\n1804             mode=mode,\n1805             synchronizer=synchronizer,\n1806             group=group,\n1807             encoding=encoding,\n1808             compute=compute,\n1809             consolidated=consolidated,\n1810             append_dim=append_dim,\n1811             region=region,\n1812         )\n1813 \n1814     def __repr__(self) -> str:\n1815         return formatting.dataset_repr(self)\n1816 \n1817     def _repr_html_(self):\n1818         if OPTIONS[\"display_style\"] == \"text\":\n1819             return f\"<pre>{escape(repr(self))}</pre>\"\n1820         return formatting_html.dataset_repr(self)\n1821 \n1822     def info(self, buf=None) -> None:\n1823         \"\"\"\n1824         Concise summary of a Dataset variables and attributes.\n1825 \n1826         Parameters\n1827         ----------\n1828         buf : file-like, default: sys.stdout\n1829             writable buffer\n1830 \n1831         See Also\n1832         --------\n1833         pandas.DataFrame.assign\n1834         ncdump : netCDF's ncdump\n1835         \"\"\"\n1836         if buf is None:  # pragma: no cover\n1837             buf = sys.stdout\n1838 \n1839         lines = []\n1840         lines.append(\"xarray.Dataset {\")\n1841         lines.append(\"dimensions:\")\n1842         for name, size in self.dims.items():\n1843             lines.append(f\"\\t{name} = {size} ;\")\n1844         lines.append(\"\\nvariables:\")\n1845         for name, da in self.variables.items():\n1846             dims = \", \".join(da.dims)\n1847             lines.append(f\"\\t{da.dtype} {name}({dims}) ;\")\n1848             for k, v in da.attrs.items():\n1849                 lines.append(f\"\\t\\t{name}:{k} = {v} ;\")\n1850         lines.append(\"\\n// global attributes:\")\n1851         for k, v in self.attrs.items():\n1852             lines.append(f\"\\t:{k} = {v} ;\")\n1853         lines.append(\"}\")\n1854 \n1855         buf.write(\"\\n\".join(lines))\n1856 \n1857     @property\n1858     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:\n1859         \"\"\"Block dimensions for this dataset's data or None if it's not a dask\n1860         array.\n1861         \"\"\"\n1862         chunks: Dict[Hashable, Tuple[int, ...]] = {}\n1863         for v in self.variables.values():\n1864             if v.chunks is not None:\n1865                 for dim, c in zip(v.dims, v.chunks):\n1866                     if dim in chunks and c != chunks[dim]:\n1867                         raise ValueError(\n1868                             f\"Object has inconsistent chunks along dimension {dim}. \"\n1869                             \"This can be fixed by calling unify_chunks().\"\n1870                         )\n1871                     chunks[dim] = c\n1872         return Frozen(SortedKeysDict(chunks))\n1873 \n1874     def chunk(\n1875         self,\n1876         chunks: Union[\n1877             Number,\n1878             str,\n1879             Mapping[Hashable, Union[None, Number, str, Tuple[Number, ...]]],\n1880         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n1881         name_prefix: str = \"xarray-\",\n1882         token: str = None,\n1883         lock: bool = False,\n1884     ) -> \"Dataset\":\n1885         \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n1886         chunks.\n1887 \n1888         Non-dask arrays in this dataset will be converted to dask arrays. Dask\n1889         arrays will be rechunked to the given chunk sizes.\n1890 \n1891         If neither chunks is not provided for one or more dimensions, chunk\n1892         sizes along that dimension will not be updated; non-dask arrays will be\n1893         converted into dask arrays with a single block.\n1894 \n1895         Parameters\n1896         ----------\n1897         chunks : int, 'auto' or mapping, optional\n1898             Chunk sizes along each dimension, e.g., ``5`` or\n1899             ``{\"x\": 5, \"y\": 5}``.\n1900         name_prefix : str, optional\n1901             Prefix for the name of any new dask arrays.\n1902         token : str, optional\n1903             Token uniquely identifying this dataset.\n1904         lock : optional\n1905             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1906             already as dask array.\n1907 \n1908         Returns\n1909         -------\n1910         chunked : xarray.Dataset\n1911         \"\"\"\n1912         if chunks is None:\n1913             warnings.warn(\n1914                 \"None value for 'chunks' is deprecated. \"\n1915                 \"It will raise an error in the future. Use instead '{}'\",\n1916                 category=FutureWarning,\n1917             )\n1918             chunks = {}\n1919 \n1920         if isinstance(chunks, (Number, str)):\n1921             chunks = dict.fromkeys(self.dims, chunks)\n1922 \n1923         bad_dims = chunks.keys() - self.dims.keys()\n1924         if bad_dims:\n1925             raise ValueError(\n1926                 \"some chunks keys are not dimensions on this \" \"object: %s\" % bad_dims\n1927             )\n1928 \n1929         variables = {\n1930             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)\n1931             for k, v in self.variables.items()\n1932         }\n1933         return self._replace(variables)\n1934 \n1935     def _validate_indexers(\n1936         self, indexers: Mapping[Hashable, Any], missing_dims: str = \"raise\"\n1937     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:\n1938         \"\"\"Here we make sure\n1939         + indexer has a valid keys\n1940         + indexer is in a valid data type\n1941         + string indexers are cast to the appropriate date type if the\n1942           associated index is a DatetimeIndex or CFTimeIndex\n1943         \"\"\"\n1944         from .dataarray import DataArray\n1945 \n1946         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n1947 \n1948         # all indexers should be int, slice, np.ndarrays, or Variable\n1949         for k, v in indexers.items():\n1950             if isinstance(v, (int, slice, Variable)):\n1951                 yield k, v\n1952             elif isinstance(v, DataArray):\n1953                 yield k, v.variable\n1954             elif isinstance(v, tuple):\n1955                 yield k, as_variable(v)\n1956             elif isinstance(v, Dataset):\n1957                 raise TypeError(\"cannot use a Dataset as an indexer\")\n1958             elif isinstance(v, Sequence) and len(v) == 0:\n1959                 yield k, np.empty((0,), dtype=\"int64\")\n1960             else:\n1961                 v = np.asarray(v)\n1962 \n1963                 if v.dtype.kind in \"US\":\n1964                     index = self.indexes[k]\n1965                     if isinstance(index, pd.DatetimeIndex):\n1966                         v = v.astype(\"datetime64[ns]\")\n1967                     elif isinstance(index, xr.CFTimeIndex):\n1968                         v = _parse_array_of_cftime_strings(v, index.date_type)\n1969 \n1970                 if v.ndim > 1:\n1971                     raise IndexError(\n1972                         \"Unlabeled multi-dimensional array cannot be \"\n1973                         \"used for indexing: {}\".format(k)\n1974                     )\n1975                 yield k, v\n1976 \n1977     def _validate_interp_indexers(\n1978         self, indexers: Mapping[Hashable, Any]\n1979     ) -> Iterator[Tuple[Hashable, Variable]]:\n1980         \"\"\"Variant of _validate_indexers to be used for interpolation\"\"\"\n1981         for k, v in self._validate_indexers(indexers):\n1982             if isinstance(v, Variable):\n1983                 if v.ndim == 1:\n1984                     yield k, v.to_index_variable()\n1985                 else:\n1986                     yield k, v\n1987             elif isinstance(v, int):\n1988                 yield k, Variable((), v)\n1989             elif isinstance(v, np.ndarray):\n1990                 if v.ndim == 0:\n1991                     yield k, Variable((), v)\n1992                 elif v.ndim == 1:\n1993                     yield k, IndexVariable((k,), v)\n1994                 else:\n1995                     raise AssertionError()  # Already tested by _validate_indexers\n1996             else:\n1997                 raise TypeError(type(v))\n1998 \n1999     def _get_indexers_coords_and_indexes(self, indexers):\n2000         \"\"\"Extract coordinates and indexes from indexers.\n2001 \n2002         Only coordinate with a name different from any of self.variables will\n2003         be attached.\n2004         \"\"\"\n2005         from .dataarray import DataArray\n2006 \n2007         coords_list = []\n2008         for k, v in indexers.items():\n2009             if isinstance(v, DataArray):\n2010                 if v.dtype.kind == \"b\":\n2011                     if v.ndim != 1:  # we only support 1-d boolean array\n2012                         raise ValueError(\n2013                             \"{:d}d-boolean array is used for indexing along \"\n2014                             \"dimension {!r}, but only 1d boolean arrays are \"\n2015                             \"supported.\".format(v.ndim, k)\n2016                         )\n2017                     # Make sure in case of boolean DataArray, its\n2018                     # coordinate also should be indexed.\n2019                     v_coords = v[v.values.nonzero()[0]].coords\n2020                 else:\n2021                     v_coords = v.coords\n2022                 coords_list.append(v_coords)\n2023 \n2024         # we don't need to call align() explicitly or check indexes for\n2025         # alignment, because merge_variables already checks for exact alignment\n2026         # between dimension coordinates\n2027         coords, indexes = merge_coordinates_without_align(coords_list)\n2028         assert_coordinate_consistent(self, coords)\n2029 \n2030         # silently drop the conflicted variables.\n2031         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n2032         attached_indexes = {\n2033             k: v for k, v in indexes.items() if k not in self._variables\n2034         }\n2035         return attached_coords, attached_indexes\n2036 \n2037     def isel(\n2038         self,\n2039         indexers: Mapping[Hashable, Any] = None,\n2040         drop: bool = False,\n2041         missing_dims: str = \"raise\",\n2042         **indexers_kwargs: Any,\n2043     ) -> \"Dataset\":\n2044         \"\"\"Returns a new dataset with each array indexed along the specified\n2045         dimension(s).\n2046 \n2047         This method selects values from each array using its `__getitem__`\n2048         method, except this method does not require knowing the order of\n2049         each array's dimensions.\n2050 \n2051         Parameters\n2052         ----------\n2053         indexers : dict, optional\n2054             A dict with keys matching dimensions and values given\n2055             by integers, slice objects or arrays.\n2056             indexer can be a integer, slice, array-like or DataArray.\n2057             If DataArrays are passed as indexers, xarray-style indexing will be\n2058             carried out. See :ref:`indexing` for the details.\n2059             One of indexers or indexers_kwargs must be provided.\n2060         drop : bool, optional\n2061             If ``drop=True``, drop coordinates variables indexed by integers\n2062             instead of making them scalar.\n2063         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2064             What to do if dimensions that should be selected from are not present in the\n2065             Dataset:\n2066             - \"raise\": raise an exception\n2067             - \"warning\": raise a warning, and ignore the missing dimensions\n2068             - \"ignore\": ignore the missing dimensions\n2069         **indexers_kwargs : {dim: indexer, ...}, optional\n2070             The keyword arguments form of ``indexers``.\n2071             One of indexers or indexers_kwargs must be provided.\n2072 \n2073         Returns\n2074         -------\n2075         obj : Dataset\n2076             A new Dataset with the same contents as this dataset, except each\n2077             array and dimension is indexed by the appropriate indexers.\n2078             If indexer DataArrays have coordinates that do not conflict with\n2079             this object, then these coordinates will be attached.\n2080             In general, each array's data will be a view of the array's data\n2081             in this dataset, unless vectorized indexing was triggered by using\n2082             an array indexer, in which case the data will be a copy.\n2083 \n2084         See Also\n2085         --------\n2086         Dataset.sel\n2087         DataArray.isel\n2088         \"\"\"\n2089         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n2090         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n2091             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n2092 \n2093         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n2094         # lists, or zero or one-dimensional np.ndarray's\n2095         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2096 \n2097         variables = {}\n2098         dims: Dict[Hashable, Tuple[int, ...]] = {}\n2099         coord_names = self._coord_names.copy()\n2100         indexes = self._indexes.copy() if self._indexes is not None else None\n2101 \n2102         for var_name, var_value in self._variables.items():\n2103             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}\n2104             if var_indexers:\n2105                 var_value = var_value.isel(var_indexers)\n2106                 if drop and var_value.ndim == 0 and var_name in coord_names:\n2107                     coord_names.remove(var_name)\n2108                     if indexes:\n2109                         indexes.pop(var_name, None)\n2110                     continue\n2111                 if indexes and var_name in indexes:\n2112                     if var_value.ndim == 1:\n2113                         indexes[var_name] = var_value.to_index()\n2114                     else:\n2115                         del indexes[var_name]\n2116             variables[var_name] = var_value\n2117             dims.update(zip(var_value.dims, var_value.shape))\n2118 \n2119         return self._construct_direct(\n2120             variables=variables,\n2121             coord_names=coord_names,\n2122             dims=dims,\n2123             attrs=self._attrs,\n2124             indexes=indexes,\n2125             encoding=self._encoding,\n2126             close=self._close,\n2127         )\n2128 \n2129     def _isel_fancy(\n2130         self,\n2131         indexers: Mapping[Hashable, Any],\n2132         *,\n2133         drop: bool,\n2134         missing_dims: str = \"raise\",\n2135     ) -> \"Dataset\":\n2136         # Note: we need to preserve the original indexers variable in order to merge the\n2137         # coords below\n2138         indexers_list = list(self._validate_indexers(indexers, missing_dims))\n2139 \n2140         variables: Dict[Hashable, Variable] = {}\n2141         indexes: Dict[Hashable, pd.Index] = {}\n2142 \n2143         for name, var in self.variables.items():\n2144             var_indexers = {k: v for k, v in indexers_list if k in var.dims}\n2145             if drop and name in var_indexers:\n2146                 continue  # drop this variable\n2147 \n2148             if name in self.indexes:\n2149                 new_var, new_index = isel_variable_and_index(\n2150                     name, var, self.indexes[name], var_indexers\n2151                 )\n2152                 if new_index is not None:\n2153                     indexes[name] = new_index\n2154             elif var_indexers:\n2155                 new_var = var.isel(indexers=var_indexers)\n2156             else:\n2157                 new_var = var.copy(deep=False)\n2158 \n2159             variables[name] = new_var\n2160 \n2161         coord_names = self._coord_names & variables.keys()\n2162         selected = self._replace_with_new_dims(variables, coord_names, indexes)\n2163 \n2164         # Extract coordinates from indexers\n2165         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n2166         variables.update(coord_vars)\n2167         indexes.update(new_indexes)\n2168         coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n2169         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2170 \n2171     def sel(\n2172         self,\n2173         indexers: Mapping[Hashable, Any] = None,\n2174         method: str = None,\n2175         tolerance: Number = None,\n2176         drop: bool = False,\n2177         **indexers_kwargs: Any,\n2178     ) -> \"Dataset\":\n2179         \"\"\"Returns a new dataset with each array indexed by tick labels\n2180         along the specified dimension(s).\n2181 \n2182         In contrast to `Dataset.isel`, indexers for this method should use\n2183         labels instead of integers.\n2184 \n2185         Under the hood, this method is powered by using pandas's powerful Index\n2186         objects. This makes label based indexing essentially just as fast as\n2187         using integer indexing.\n2188 \n2189         It also means this method uses pandas's (well documented) logic for\n2190         indexing. This means you can use string shortcuts for datetime indexes\n2191         (e.g., '2000-01' to select all values in January 2000). It also means\n2192         that slices are treated as inclusive of both the start and stop values,\n2193         unlike normal Python indexing.\n2194 \n2195         Parameters\n2196         ----------\n2197         indexers : dict, optional\n2198             A dict with keys matching dimensions and values given\n2199             by scalars, slices or arrays of tick labels. For dimensions with\n2200             multi-index, the indexer may also be a dict-like object with keys\n2201             matching index level names.\n2202             If DataArrays are passed as indexers, xarray-style indexing will be\n2203             carried out. See :ref:`indexing` for the details.\n2204             One of indexers or indexers_kwargs must be provided.\n2205         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n2206             Method to use for inexact matches:\n2207 \n2208             * None (default): only exact matches\n2209             * pad / ffill: propagate last valid index value forward\n2210             * backfill / bfill: propagate next valid index value backward\n2211             * nearest: use nearest valid index value\n2212         tolerance : optional\n2213             Maximum distance between original and new labels for inexact\n2214             matches. The values of the index at the matching locations must\n2215             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2216         drop : bool, optional\n2217             If ``drop=True``, drop coordinates variables in `indexers` instead\n2218             of making them scalar.\n2219         **indexers_kwargs : {dim: indexer, ...}, optional\n2220             The keyword arguments form of ``indexers``.\n2221             One of indexers or indexers_kwargs must be provided.\n2222 \n2223         Returns\n2224         -------\n2225         obj : Dataset\n2226             A new Dataset with the same contents as this dataset, except each\n2227             variable and dimension is indexed by the appropriate indexers.\n2228             If indexer DataArrays have coordinates that do not conflict with\n2229             this object, then these coordinates will be attached.\n2230             In general, each array's data will be a view of the array's data\n2231             in this dataset, unless vectorized indexing was triggered by using\n2232             an array indexer, in which case the data will be a copy.\n2233 \n2234         See Also\n2235         --------\n2236         Dataset.isel\n2237         DataArray.sel\n2238         \"\"\"\n2239         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"sel\")\n2240         pos_indexers, new_indexes = remap_label_indexers(\n2241             self, indexers=indexers, method=method, tolerance=tolerance\n2242         )\n2243         result = self.isel(indexers=pos_indexers, drop=drop)\n2244         return result._overwrite_indexes(new_indexes)\n2245 \n2246     def head(\n2247         self,\n2248         indexers: Union[Mapping[Hashable, int], int] = None,\n2249         **indexers_kwargs: Any,\n2250     ) -> \"Dataset\":\n2251         \"\"\"Returns a new dataset with the first `n` values of each array\n2252         for the specified dimension(s).\n2253 \n2254         Parameters\n2255         ----------\n2256         indexers : dict or int, default: 5\n2257             A dict with keys matching dimensions and integer values `n`\n2258             or a single integer `n` applied over all dimensions.\n2259             One of indexers or indexers_kwargs must be provided.\n2260         **indexers_kwargs : {dim: n, ...}, optional\n2261             The keyword arguments form of ``indexers``.\n2262             One of indexers or indexers_kwargs must be provided.\n2263 \n2264         See Also\n2265         --------\n2266         Dataset.tail\n2267         Dataset.thin\n2268         DataArray.head\n2269         \"\"\"\n2270         if not indexers_kwargs:\n2271             if indexers is None:\n2272                 indexers = 5\n2273             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2274                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2275         if isinstance(indexers, int):\n2276             indexers = {dim: indexers for dim in self.dims}\n2277         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"head\")\n2278         for k, v in indexers.items():\n2279             if not isinstance(v, int):\n2280                 raise TypeError(\n2281                     \"expected integer type indexer for \"\n2282                     \"dimension %r, found %r\" % (k, type(v))\n2283                 )\n2284             elif v < 0:\n2285                 raise ValueError(\n2286                     \"expected positive integer as indexer \"\n2287                     \"for dimension %r, found %s\" % (k, v)\n2288                 )\n2289         indexers_slices = {k: slice(val) for k, val in indexers.items()}\n2290         return self.isel(indexers_slices)\n2291 \n2292     def tail(\n2293         self,\n2294         indexers: Union[Mapping[Hashable, int], int] = None,\n2295         **indexers_kwargs: Any,\n2296     ) -> \"Dataset\":\n2297         \"\"\"Returns a new dataset with the last `n` values of each array\n2298         for the specified dimension(s).\n2299 \n2300         Parameters\n2301         ----------\n2302         indexers : dict or int, default: 5\n2303             A dict with keys matching dimensions and integer values `n`\n2304             or a single integer `n` applied over all dimensions.\n2305             One of indexers or indexers_kwargs must be provided.\n2306         **indexers_kwargs : {dim: n, ...}, optional\n2307             The keyword arguments form of ``indexers``.\n2308             One of indexers or indexers_kwargs must be provided.\n2309 \n2310         See Also\n2311         --------\n2312         Dataset.head\n2313         Dataset.thin\n2314         DataArray.tail\n2315         \"\"\"\n2316         if not indexers_kwargs:\n2317             if indexers is None:\n2318                 indexers = 5\n2319             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2320                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2321         if isinstance(indexers, int):\n2322             indexers = {dim: indexers for dim in self.dims}\n2323         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"tail\")\n2324         for k, v in indexers.items():\n2325             if not isinstance(v, int):\n2326                 raise TypeError(\n2327                     \"expected integer type indexer for \"\n2328                     \"dimension %r, found %r\" % (k, type(v))\n2329                 )\n2330             elif v < 0:\n2331                 raise ValueError(\n2332                     \"expected positive integer as indexer \"\n2333                     \"for dimension %r, found %s\" % (k, v)\n2334                 )\n2335         indexers_slices = {\n2336             k: slice(-val, None) if val != 0 else slice(val)\n2337             for k, val in indexers.items()\n2338         }\n2339         return self.isel(indexers_slices)\n2340 \n2341     def thin(\n2342         self,\n2343         indexers: Union[Mapping[Hashable, int], int] = None,\n2344         **indexers_kwargs: Any,\n2345     ) -> \"Dataset\":\n2346         \"\"\"Returns a new dataset with each array indexed along every `n`-th\n2347         value for the specified dimension(s)\n2348 \n2349         Parameters\n2350         ----------\n2351         indexers : dict or int\n2352             A dict with keys matching dimensions and integer values `n`\n2353             or a single integer `n` applied over all dimensions.\n2354             One of indexers or indexers_kwargs must be provided.\n2355         **indexers_kwargs : {dim: n, ...}, optional\n2356             The keyword arguments form of ``indexers``.\n2357             One of indexers or indexers_kwargs must be provided.\n2358 \n2359         See Also\n2360         --------\n2361         Dataset.head\n2362         Dataset.tail\n2363         DataArray.thin\n2364         \"\"\"\n2365         if (\n2366             not indexers_kwargs\n2367             and not isinstance(indexers, int)\n2368             and not is_dict_like(indexers)\n2369         ):\n2370             raise TypeError(\"indexers must be either dict-like or a single integer\")\n2371         if isinstance(indexers, int):\n2372             indexers = {dim: indexers for dim in self.dims}\n2373         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"thin\")\n2374         for k, v in indexers.items():\n2375             if not isinstance(v, int):\n2376                 raise TypeError(\n2377                     \"expected integer type indexer for \"\n2378                     \"dimension %r, found %r\" % (k, type(v))\n2379                 )\n2380             elif v < 0:\n2381                 raise ValueError(\n2382                     \"expected positive integer as indexer \"\n2383                     \"for dimension %r, found %s\" % (k, v)\n2384                 )\n2385             elif v == 0:\n2386                 raise ValueError(\"step cannot be zero\")\n2387         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}\n2388         return self.isel(indexers_slices)\n2389 \n2390     def broadcast_like(\n2391         self, other: Union[\"Dataset\", \"DataArray\"], exclude: Iterable[Hashable] = None\n2392     ) -> \"Dataset\":\n2393         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n2394         This is equivalent to xr.broadcast(other, self)[1]\n2395 \n2396         Parameters\n2397         ----------\n2398         other : Dataset or DataArray\n2399             Object against which to broadcast this array.\n2400         exclude : iterable of hashable, optional\n2401             Dimensions that must not be broadcasted\n2402 \n2403         \"\"\"\n2404         if exclude is None:\n2405             exclude = set()\n2406         else:\n2407             exclude = set(exclude)\n2408         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n2409 \n2410         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n2411 \n2412         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n2413 \n2414     def reindex_like(\n2415         self,\n2416         other: Union[\"Dataset\", \"DataArray\"],\n2417         method: str = None,\n2418         tolerance: Number = None,\n2419         copy: bool = True,\n2420         fill_value: Any = dtypes.NA,\n2421     ) -> \"Dataset\":\n2422         \"\"\"Conform this object onto the indexes of another object, filling in\n2423         missing values with ``fill_value``. The default fill value is NaN.\n2424 \n2425         Parameters\n2426         ----------\n2427         other : Dataset or DataArray\n2428             Object with an 'indexes' attribute giving a mapping from dimension\n2429             names to pandas.Index objects, which provides coordinates upon\n2430             which to index the variables in this dataset. The indexes on this\n2431             other object need not be the same as the indexes on this\n2432             dataset. Any mis-matched index values will be filled in with\n2433             NaN, and any mis-matched dimension names will simply be ignored.\n2434         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n2435             Method to use for filling index values from other not found in this\n2436             dataset:\n2437 \n2438             * None (default): don't fill gaps\n2439             * pad / ffill: propagate last valid index value forward\n2440             * backfill / bfill: propagate next valid index value backward\n2441             * nearest: use nearest valid index value\n2442         tolerance : optional\n2443             Maximum distance between original and new labels for inexact\n2444             matches. The values of the index at the matching locations must\n2445             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2446         copy : bool, optional\n2447             If ``copy=True``, data in the return value is always copied. If\n2448             ``copy=False`` and reindexing is unnecessary, or can be performed\n2449             with only slice operations, then the output may share memory with\n2450             the input. In either case, a new xarray object is always returned.\n2451         fill_value : scalar or dict-like, optional\n2452             Value to use for newly missing values. If a dict-like maps\n2453             variable names to fill values.\n2454 \n2455         Returns\n2456         -------\n2457         reindexed : Dataset\n2458             Another dataset, with this dataset's data but coordinates from the\n2459             other object.\n2460 \n2461         See Also\n2462         --------\n2463         Dataset.reindex\n2464         align\n2465         \"\"\"\n2466         indexers = alignment.reindex_like_indexers(self, other)\n2467         return self.reindex(\n2468             indexers=indexers,\n2469             method=method,\n2470             copy=copy,\n2471             fill_value=fill_value,\n2472             tolerance=tolerance,\n2473         )\n2474 \n2475     def reindex(\n2476         self,\n2477         indexers: Mapping[Hashable, Any] = None,\n2478         method: str = None,\n2479         tolerance: Number = None,\n2480         copy: bool = True,\n2481         fill_value: Any = dtypes.NA,\n2482         **indexers_kwargs: Any,\n2483     ) -> \"Dataset\":\n2484         \"\"\"Conform this object onto a new set of indexes, filling in\n2485         missing values with ``fill_value``. The default fill value is NaN.\n2486 \n2487         Parameters\n2488         ----------\n2489         indexers : dict, optional\n2490             Dictionary with keys given by dimension names and values given by\n2491             arrays of coordinates tick labels. Any mis-matched coordinate\n2492             values will be filled in with NaN, and any mis-matched dimension\n2493             names will simply be ignored.\n2494             One of indexers or indexers_kwargs must be provided.\n2495         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n2496             Method to use for filling index values in ``indexers`` not found in\n2497             this dataset:\n2498 \n2499             * None (default): don't fill gaps\n2500             * pad / ffill: propagate last valid index value forward\n2501             * backfill / bfill: propagate next valid index value backward\n2502             * nearest: use nearest valid index value\n2503         tolerance : optional\n2504             Maximum distance between original and new labels for inexact\n2505             matches. The values of the index at the matching locations must\n2506             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2507         copy : bool, optional\n2508             If ``copy=True``, data in the return value is always copied. If\n2509             ``copy=False`` and reindexing is unnecessary, or can be performed\n2510             with only slice operations, then the output may share memory with\n2511             the input. In either case, a new xarray object is always returned.\n2512         fill_value : scalar or dict-like, optional\n2513             Value to use for newly missing values. If a dict-like,\n2514             maps variable names (including coordinates) to fill values.\n2515         sparse : bool, default: False\n2516             use sparse-array.\n2517         **indexers_kwargs : {dim: indexer, ...}, optional\n2518             Keyword arguments in the same form as ``indexers``.\n2519             One of indexers or indexers_kwargs must be provided.\n2520 \n2521         Returns\n2522         -------\n2523         reindexed : Dataset\n2524             Another dataset, with this dataset's data but replaced coordinates.\n2525 \n2526         See Also\n2527         --------\n2528         Dataset.reindex_like\n2529         align\n2530         pandas.Index.get_indexer\n2531 \n2532         Examples\n2533         --------\n2534         Create a dataset with some fictional data.\n2535 \n2536         >>> import xarray as xr\n2537         >>> import pandas as pd\n2538         >>> x = xr.Dataset(\n2539         ...     {\n2540         ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n2541         ...         \"pressure\": (\"station\", 500 * np.random.rand(4)),\n2542         ...     },\n2543         ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]},\n2544         ... )\n2545         >>> x\n2546         <xarray.Dataset>\n2547         Dimensions:      (station: 4)\n2548         Coordinates:\n2549           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n2550         Data variables:\n2551             temperature  (station) float64 10.98 14.3 12.06 10.9\n2552             pressure     (station) float64 211.8 322.9 218.8 445.9\n2553         >>> x.indexes\n2554         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n2555 \n2556         Create a new index and reindex the dataset. By default values in the new index that\n2557         do not have corresponding records in the dataset are assigned `NaN`.\n2558 \n2559         >>> new_index = [\"boston\", \"austin\", \"seattle\", \"lincoln\"]\n2560         >>> x.reindex({\"station\": new_index})\n2561         <xarray.Dataset>\n2562         Dimensions:      (station: 4)\n2563         Coordinates:\n2564           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n2565         Data variables:\n2566             temperature  (station) float64 10.98 nan 12.06 nan\n2567             pressure     (station) float64 211.8 nan 218.8 nan\n2568 \n2569         We can fill in the missing values by passing a value to the keyword `fill_value`.\n2570 \n2571         >>> x.reindex({\"station\": new_index}, fill_value=0)\n2572         <xarray.Dataset>\n2573         Dimensions:      (station: 4)\n2574         Coordinates:\n2575           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n2576         Data variables:\n2577             temperature  (station) float64 10.98 0.0 12.06 0.0\n2578             pressure     (station) float64 211.8 0.0 218.8 0.0\n2579 \n2580         We can also use different fill values for each variable.\n2581 \n2582         >>> x.reindex(\n2583         ...     {\"station\": new_index}, fill_value={\"temperature\": 0, \"pressure\": 100}\n2584         ... )\n2585         <xarray.Dataset>\n2586         Dimensions:      (station: 4)\n2587         Coordinates:\n2588           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n2589         Data variables:\n2590             temperature  (station) float64 10.98 0.0 12.06 0.0\n2591             pressure     (station) float64 211.8 100.0 218.8 100.0\n2592 \n2593         Because the index is not monotonically increasing or decreasing, we cannot use arguments\n2594         to the keyword method to fill the `NaN` values.\n2595 \n2596         >>> x.reindex({\"station\": new_index}, method=\"nearest\")\n2597         Traceback (most recent call last):\n2598         ...\n2599             raise ValueError('index must be monotonic increasing or decreasing')\n2600         ValueError: index must be monotonic increasing or decreasing\n2601 \n2602         To further illustrate the filling functionality in reindex, we will create a\n2603         dataset with a monotonically increasing index (for example, a sequence of dates).\n2604 \n2605         >>> x2 = xr.Dataset(\n2606         ...     {\n2607         ...         \"temperature\": (\n2608         ...             \"time\",\n2609         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n2610         ...         ),\n2611         ...         \"pressure\": (\"time\", 500 * np.random.rand(6)),\n2612         ...     },\n2613         ...     coords={\"time\": pd.date_range(\"01/01/2019\", periods=6, freq=\"D\")},\n2614         ... )\n2615         >>> x2\n2616         <xarray.Dataset>\n2617         Dimensions:      (time: 6)\n2618         Coordinates:\n2619           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n2620         Data variables:\n2621             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n2622             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8\n2623 \n2624         Suppose we decide to expand the dataset to cover a wider date range.\n2625 \n2626         >>> time_index2 = pd.date_range(\"12/29/2018\", periods=10, freq=\"D\")\n2627         >>> x2.reindex({\"time\": time_index2})\n2628         <xarray.Dataset>\n2629         Dimensions:      (time: 10)\n2630         Coordinates:\n2631           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n2632         Data variables:\n2633             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n2634             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan\n2635 \n2636         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n2637         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n2638 \n2639         For example, to back-propagate the last valid value to fill the `NaN` values,\n2640         pass `bfill` as an argument to the `method` keyword.\n2641 \n2642         >>> x3 = x2.reindex({\"time\": time_index2}, method=\"bfill\")\n2643         >>> x3\n2644         <xarray.Dataset>\n2645         Dimensions:      (time: 10)\n2646         Coordinates:\n2647           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n2648         Data variables:\n2649             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n2650             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan\n2651 \n2652         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n2653         will not be filled by any of the value propagation schemes.\n2654 \n2655         >>> x2.where(x2.temperature.isnull(), drop=True)\n2656         <xarray.Dataset>\n2657         Dimensions:      (time: 1)\n2658         Coordinates:\n2659           * time         (time) datetime64[ns] 2019-01-03\n2660         Data variables:\n2661             temperature  (time) float64 nan\n2662             pressure     (time) float64 395.9\n2663         >>> x3.where(x3.temperature.isnull(), drop=True)\n2664         <xarray.Dataset>\n2665         Dimensions:      (time: 2)\n2666         Coordinates:\n2667           * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n2668         Data variables:\n2669             temperature  (time) float64 nan nan\n2670             pressure     (time) float64 395.9 nan\n2671 \n2672         This is because filling while reindexing does not look at dataset values, but only compares\n2673         the original and desired indexes. If you do want to fill in the `NaN` values present in the\n2674         original dataset, use the :py:meth:`~Dataset.fillna()` method.\n2675 \n2676         \"\"\"\n2677         return self._reindex(\n2678             indexers,\n2679             method,\n2680             tolerance,\n2681             copy,\n2682             fill_value,\n2683             sparse=False,\n2684             **indexers_kwargs,\n2685         )\n2686 \n2687     def _reindex(\n2688         self,\n2689         indexers: Mapping[Hashable, Any] = None,\n2690         method: str = None,\n2691         tolerance: Number = None,\n2692         copy: bool = True,\n2693         fill_value: Any = dtypes.NA,\n2694         sparse: bool = False,\n2695         **indexers_kwargs: Any,\n2696     ) -> \"Dataset\":\n2697         \"\"\"\n2698         same to _reindex but support sparse option\n2699         \"\"\"\n2700         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n2701 \n2702         bad_dims = [d for d in indexers if d not in self.dims]\n2703         if bad_dims:\n2704             raise ValueError(\"invalid reindex dimensions: %s\" % bad_dims)\n2705 \n2706         variables, indexes = alignment.reindex_variables(\n2707             self.variables,\n2708             self.sizes,\n2709             self.indexes,\n2710             indexers,\n2711             method,\n2712             tolerance,\n2713             copy=copy,\n2714             fill_value=fill_value,\n2715             sparse=sparse,\n2716         )\n2717         coord_names = set(self._coord_names)\n2718         coord_names.update(indexers)\n2719         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2720 \n2721     def interp(\n2722         self,\n2723         coords: Mapping[Hashable, Any] = None,\n2724         method: str = \"linear\",\n2725         assume_sorted: bool = False,\n2726         kwargs: Mapping[str, Any] = None,\n2727         **coords_kwargs: Any,\n2728     ) -> \"Dataset\":\n2729         \"\"\"Multidimensional interpolation of Dataset.\n2730 \n2731         Parameters\n2732         ----------\n2733         coords : dict, optional\n2734             Mapping from dimension names to the new coordinates.\n2735             New coordinate can be a scalar, array-like or DataArray.\n2736             If DataArrays are passed as new coordinates, their dimensions are\n2737             used for the broadcasting. Missing values are skipped.\n2738         method : str, optional\n2739             {\"linear\", \"nearest\"} for multidimensional array,\n2740             {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\"}\n2741             for 1-dimensional array. \"linear\" is used by default.\n2742         assume_sorted : bool, optional\n2743             If False, values of coordinates that are interpolated over can be\n2744             in any order and they are sorted first. If True, interpolated\n2745             coordinates are assumed to be an array of monotonically increasing\n2746             values.\n2747         kwargs : dict, optional\n2748             Additional keyword arguments passed to scipy's interpolator. Valid\n2749             options and their behavior depend on if 1-dimensional or\n2750             multi-dimensional interpolation is used.\n2751         **coords_kwargs : {dim: coordinate, ...}, optional\n2752             The keyword arguments form of ``coords``.\n2753             One of coords or coords_kwargs must be provided.\n2754 \n2755         Returns\n2756         -------\n2757         interpolated : Dataset\n2758             New dataset on the new coordinates.\n2759 \n2760         Notes\n2761         -----\n2762         scipy is required.\n2763 \n2764         See Also\n2765         --------\n2766         scipy.interpolate.interp1d\n2767         scipy.interpolate.interpn\n2768 \n2769         Examples\n2770         --------\n2771         >>> ds = xr.Dataset(\n2772         ...     data_vars={\n2773         ...         \"a\": (\"x\", [5, 7, 4]),\n2774         ...         \"b\": (\n2775         ...             (\"x\", \"y\"),\n2776         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n2777         ...         ),\n2778         ...     },\n2779         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n2780         ... )\n2781         >>> ds\n2782         <xarray.Dataset>\n2783         Dimensions:  (x: 3, y: 4)\n2784         Coordinates:\n2785           * x        (x) int64 0 1 2\n2786           * y        (y) int64 10 12 14 16\n2787         Data variables:\n2788             a        (x) int64 5 7 4\n2789             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0\n2790 \n2791         1D interpolation with the default method (linear):\n2792 \n2793         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])\n2794         <xarray.Dataset>\n2795         Dimensions:  (x: 4, y: 4)\n2796         Coordinates:\n2797           * y        (y) int64 10 12 14 16\n2798           * x        (x) float64 0.0 0.75 1.25 1.75\n2799         Data variables:\n2800             a        (x) float64 5.0 6.5 6.25 4.75\n2801             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan\n2802 \n2803         1D interpolation with a different method:\n2804 \n2805         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n2806         <xarray.Dataset>\n2807         Dimensions:  (x: 4, y: 4)\n2808         Coordinates:\n2809           * y        (y) int64 10 12 14 16\n2810           * x        (x) float64 0.0 0.75 1.25 1.75\n2811         Data variables:\n2812             a        (x) float64 5.0 7.0 7.0 4.0\n2813             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0\n2814 \n2815         1D extrapolation:\n2816 \n2817         >>> ds.interp(\n2818         ...     x=[1, 1.5, 2.5, 3.5],\n2819         ...     method=\"linear\",\n2820         ...     kwargs={\"fill_value\": \"extrapolate\"},\n2821         ... )\n2822         <xarray.Dataset>\n2823         Dimensions:  (x: 4, y: 4)\n2824         Coordinates:\n2825           * y        (y) int64 10 12 14 16\n2826           * x        (x) float64 1.0 1.5 2.5 3.5\n2827         Data variables:\n2828             a        (x) float64 7.0 5.5 2.5 -0.5\n2829             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan\n2830 \n2831         2D interpolation:\n2832 \n2833         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n2834         <xarray.Dataset>\n2835         Dimensions:  (x: 4, y: 3)\n2836         Coordinates:\n2837           * x        (x) float64 0.0 0.75 1.25 1.75\n2838           * y        (y) int64 11 13 15\n2839         Data variables:\n2840             a        (x) float64 5.0 6.5 6.25 4.75\n2841             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan\n2842         \"\"\"\n2843         from . import missing\n2844 \n2845         if kwargs is None:\n2846             kwargs = {}\n2847 \n2848         coords = either_dict_or_kwargs(coords, coords_kwargs, \"interp\")\n2849         indexers = dict(self._validate_interp_indexers(coords))\n2850 \n2851         if coords:\n2852             # This avoids broadcasting over coordinates that are both in\n2853             # the original array AND in the indexing array. It essentially\n2854             # forces interpolation along the shared coordinates.\n2855             sdims = (\n2856                 set(self.dims)\n2857                 .intersection(*[set(nx.dims) for nx in indexers.values()])\n2858                 .difference(coords.keys())\n2859             )\n2860             indexers.update({d: self.variables[d] for d in sdims})\n2861 \n2862         obj = self if assume_sorted else self.sortby([k for k in coords])\n2863 \n2864         def maybe_variable(obj, k):\n2865             # workaround to get variable for dimension without coordinate.\n2866             try:\n2867                 return obj._variables[k]\n2868             except KeyError:\n2869                 return as_variable((k, range(obj.dims[k])))\n2870 \n2871         def _validate_interp_indexer(x, new_x):\n2872             # In the case of datetimes, the restrictions placed on indexers\n2873             # used with interp are stronger than those which are placed on\n2874             # isel, so we need an additional check after _validate_indexers.\n2875             if _contains_datetime_like_objects(\n2876                 x\n2877             ) and not _contains_datetime_like_objects(new_x):\n2878                 raise TypeError(\n2879                     \"When interpolating over a datetime-like \"\n2880                     \"coordinate, the coordinates to \"\n2881                     \"interpolate to must be either datetime \"\n2882                     \"strings or datetimes. \"\n2883                     \"Instead got\\n{}\".format(new_x)\n2884                 )\n2885             return x, new_x\n2886 \n2887         variables: Dict[Hashable, Variable] = {}\n2888         for name, var in obj._variables.items():\n2889             if name in indexers:\n2890                 continue\n2891 \n2892             if var.dtype.kind in \"uifc\":\n2893                 var_indexers = {\n2894                     k: _validate_interp_indexer(maybe_variable(obj, k), v)\n2895                     for k, v in indexers.items()\n2896                     if k in var.dims\n2897                 }\n2898                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)\n2899             elif all(d not in indexers for d in var.dims):\n2900                 # keep unrelated object array\n2901                 variables[name] = var\n2902 \n2903         coord_names = obj._coord_names & variables.keys()\n2904         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}\n2905         selected = self._replace_with_new_dims(\n2906             variables.copy(), coord_names, indexes=indexes\n2907         )\n2908 \n2909         # attach indexer as coordinate\n2910         variables.update(indexers)\n2911         for k, v in indexers.items():\n2912             assert isinstance(v, Variable)\n2913             if v.dims == (k,):\n2914                 indexes[k] = v.to_index()\n2915 \n2916         # Extract coordinates from indexers\n2917         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)\n2918         variables.update(coord_vars)\n2919         indexes.update(new_indexes)\n2920 \n2921         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()\n2922         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2923 \n2924     def interp_like(\n2925         self,\n2926         other: Union[\"Dataset\", \"DataArray\"],\n2927         method: str = \"linear\",\n2928         assume_sorted: bool = False,\n2929         kwargs: Mapping[str, Any] = None,\n2930     ) -> \"Dataset\":\n2931         \"\"\"Interpolate this object onto the coordinates of another object,\n2932         filling the out of range values with NaN.\n2933 \n2934         Parameters\n2935         ----------\n2936         other : Dataset or DataArray\n2937             Object with an 'indexes' attribute giving a mapping from dimension\n2938             names to an 1d array-like, which provides coordinates upon\n2939             which to index the variables in this dataset. Missing values are skipped.\n2940         method : str, optional\n2941             {\"linear\", \"nearest\"} for multidimensional array,\n2942             {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\"}\n2943             for 1-dimensional array. 'linear' is used by default.\n2944         assume_sorted : bool, optional\n2945             If False, values of coordinates that are interpolated over can be\n2946             in any order and they are sorted first. If True, interpolated\n2947             coordinates are assumed to be an array of monotonically increasing\n2948             values.\n2949         kwargs : dict, optional\n2950             Additional keyword passed to scipy's interpolator.\n2951 \n2952         Returns\n2953         -------\n2954         interpolated : Dataset\n2955             Another dataset by interpolating this dataset's data along the\n2956             coordinates of the other object.\n2957 \n2958         Notes\n2959         -----\n2960         scipy is required.\n2961         If the dataset has object-type coordinates, reindex is used for these\n2962         coordinates instead of the interpolation.\n2963 \n2964         See Also\n2965         --------\n2966         Dataset.interp\n2967         Dataset.reindex_like\n2968         \"\"\"\n2969         if kwargs is None:\n2970             kwargs = {}\n2971         coords = alignment.reindex_like_indexers(self, other)\n2972 \n2973         numeric_coords: Dict[Hashable, pd.Index] = {}\n2974         object_coords: Dict[Hashable, pd.Index] = {}\n2975         for k, v in coords.items():\n2976             if v.dtype.kind in \"uifcMm\":\n2977                 numeric_coords[k] = v\n2978             else:\n2979                 object_coords[k] = v\n2980 \n2981         ds = self\n2982         if object_coords:\n2983             # We do not support interpolation along object coordinate.\n2984             # reindex instead.\n2985             ds = self.reindex(object_coords)\n2986         return ds.interp(numeric_coords, method, assume_sorted, kwargs)\n2987 \n2988     # Helper methods for rename()\n2989     def _rename_vars(self, name_dict, dims_dict):\n2990         variables = {}\n2991         coord_names = set()\n2992         for k, v in self.variables.items():\n2993             var = v.copy(deep=False)\n2994             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n2995             name = name_dict.get(k, k)\n2996             if name in variables:\n2997                 raise ValueError(f\"the new name {name!r} conflicts\")\n2998             variables[name] = var\n2999             if k in self._coord_names:\n3000                 coord_names.add(name)\n3001         return variables, coord_names\n3002 \n3003     def _rename_dims(self, name_dict):\n3004         return {name_dict.get(k, k): v for k, v in self.dims.items()}\n3005 \n3006     def _rename_indexes(self, name_dict, dims_set):\n3007         if self._indexes is None:\n3008             return None\n3009         indexes = {}\n3010         for k, v in self.indexes.items():\n3011             new_name = name_dict.get(k, k)\n3012             if new_name not in dims_set:\n3013                 continue\n3014             if isinstance(v, pd.MultiIndex):\n3015                 new_names = [name_dict.get(k, k) for k in v.names]\n3016                 index = v.rename(names=new_names)\n3017             else:\n3018                 index = v.rename(new_name)\n3019             indexes[new_name] = index\n3020         return indexes\n3021 \n3022     def _rename_all(self, name_dict, dims_dict):\n3023         variables, coord_names = self._rename_vars(name_dict, dims_dict)\n3024         dims = self._rename_dims(dims_dict)\n3025         indexes = self._rename_indexes(name_dict, dims.keys())\n3026         return variables, coord_names, dims, indexes\n3027 \n3028     def rename(\n3029         self,\n3030         name_dict: Mapping[Hashable, Hashable] = None,\n3031         **names: Hashable,\n3032     ) -> \"Dataset\":\n3033         \"\"\"Returns a new object with renamed variables and dimensions.\n3034 \n3035         Parameters\n3036         ----------\n3037         name_dict : dict-like, optional\n3038             Dictionary whose keys are current variable or dimension names and\n3039             whose values are the desired names.\n3040         **names : optional\n3041             Keyword form of ``name_dict``.\n3042             One of name_dict or names must be provided.\n3043 \n3044         Returns\n3045         -------\n3046         renamed : Dataset\n3047             Dataset with renamed variables and dimensions.\n3048 \n3049         See Also\n3050         --------\n3051         Dataset.swap_dims\n3052         Dataset.rename_vars\n3053         Dataset.rename_dims\n3054         DataArray.rename\n3055         \"\"\"\n3056         name_dict = either_dict_or_kwargs(name_dict, names, \"rename\")\n3057         for k in name_dict.keys():\n3058             if k not in self and k not in self.dims:\n3059                 raise ValueError(\n3060                     \"cannot rename %r because it is not a \"\n3061                     \"variable or dimension in this dataset\" % k\n3062                 )\n3063 \n3064         variables, coord_names, dims, indexes = self._rename_all(\n3065             name_dict=name_dict, dims_dict=name_dict\n3066         )\n3067         assert_unique_multiindex_level_names(variables)\n3068         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3069 \n3070     def rename_dims(\n3071         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable\n3072     ) -> \"Dataset\":\n3073         \"\"\"Returns a new object with renamed dimensions only.\n3074 \n3075         Parameters\n3076         ----------\n3077         dims_dict : dict-like, optional\n3078             Dictionary whose keys are current dimension names and\n3079             whose values are the desired names. The desired names must\n3080             not be the name of an existing dimension or Variable in the Dataset.\n3081         **dims : optional\n3082             Keyword form of ``dims_dict``.\n3083             One of dims_dict or dims must be provided.\n3084 \n3085         Returns\n3086         -------\n3087         renamed : Dataset\n3088             Dataset with renamed dimensions.\n3089 \n3090         See Also\n3091         --------\n3092         Dataset.swap_dims\n3093         Dataset.rename\n3094         Dataset.rename_vars\n3095         DataArray.rename\n3096         \"\"\"\n3097         dims_dict = either_dict_or_kwargs(dims_dict, dims, \"rename_dims\")\n3098         for k, v in dims_dict.items():\n3099             if k not in self.dims:\n3100                 raise ValueError(\n3101                     \"cannot rename %r because it is not a \"\n3102                     \"dimension in this dataset\" % k\n3103                 )\n3104             if v in self.dims or v in self:\n3105                 raise ValueError(\n3106                     f\"Cannot rename {k} to {v} because {v} already exists. \"\n3107                     \"Try using swap_dims instead.\"\n3108                 )\n3109 \n3110         variables, coord_names, sizes, indexes = self._rename_all(\n3111             name_dict={}, dims_dict=dims_dict\n3112         )\n3113         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)\n3114 \n3115     def rename_vars(\n3116         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable\n3117     ) -> \"Dataset\":\n3118         \"\"\"Returns a new object with renamed variables including coordinates\n3119 \n3120         Parameters\n3121         ----------\n3122         name_dict : dict-like, optional\n3123             Dictionary whose keys are current variable or coordinate names and\n3124             whose values are the desired names.\n3125         **names : optional\n3126             Keyword form of ``name_dict``.\n3127             One of name_dict or names must be provided.\n3128 \n3129         Returns\n3130         -------\n3131         renamed : Dataset\n3132             Dataset with renamed variables including coordinates\n3133 \n3134         See Also\n3135         --------\n3136         Dataset.swap_dims\n3137         Dataset.rename\n3138         Dataset.rename_dims\n3139         DataArray.rename\n3140         \"\"\"\n3141         name_dict = either_dict_or_kwargs(name_dict, names, \"rename_vars\")\n3142         for k in name_dict:\n3143             if k not in self:\n3144                 raise ValueError(\n3145                     \"cannot rename %r because it is not a \"\n3146                     \"variable or coordinate in this dataset\" % k\n3147                 )\n3148         variables, coord_names, dims, indexes = self._rename_all(\n3149             name_dict=name_dict, dims_dict={}\n3150         )\n3151         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3152 \n3153     def swap_dims(\n3154         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs\n3155     ) -> \"Dataset\":\n3156         \"\"\"Returns a new object with swapped dimensions.\n3157 \n3158         Parameters\n3159         ----------\n3160         dims_dict : dict-like\n3161             Dictionary whose keys are current dimension names and whose values\n3162             are new names.\n3163         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n3164             The keyword arguments form of ``dims_dict``.\n3165             One of dims_dict or dims_kwargs must be provided.\n3166 \n3167         Returns\n3168         -------\n3169         swapped : Dataset\n3170             Dataset with swapped dimensions.\n3171 \n3172         Examples\n3173         --------\n3174         >>> ds = xr.Dataset(\n3175         ...     data_vars={\"a\": (\"x\", [5, 7]), \"b\": (\"x\", [0.1, 2.4])},\n3176         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n3177         ... )\n3178         >>> ds\n3179         <xarray.Dataset>\n3180         Dimensions:  (x: 2)\n3181         Coordinates:\n3182           * x        (x) <U1 'a' 'b'\n3183             y        (x) int64 0 1\n3184         Data variables:\n3185             a        (x) int64 5 7\n3186             b        (x) float64 0.1 2.4\n3187 \n3188         >>> ds.swap_dims({\"x\": \"y\"})\n3189         <xarray.Dataset>\n3190         Dimensions:  (y: 2)\n3191         Coordinates:\n3192             x        (y) <U1 'a' 'b'\n3193           * y        (y) int64 0 1\n3194         Data variables:\n3195             a        (y) int64 5 7\n3196             b        (y) float64 0.1 2.4\n3197 \n3198         >>> ds.swap_dims({\"x\": \"z\"})\n3199         <xarray.Dataset>\n3200         Dimensions:  (z: 2)\n3201         Coordinates:\n3202             x        (z) <U1 'a' 'b'\n3203             y        (z) int64 0 1\n3204         Dimensions without coordinates: z\n3205         Data variables:\n3206             a        (z) int64 5 7\n3207             b        (z) float64 0.1 2.4\n3208 \n3209         See Also\n3210         --------\n3211         Dataset.rename\n3212         DataArray.swap_dims\n3213         \"\"\"\n3214         # TODO: deprecate this method in favor of a (less confusing)\n3215         # rename_dims() method that only renames dimensions.\n3216 \n3217         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n3218         for k, v in dims_dict.items():\n3219             if k not in self.dims:\n3220                 raise ValueError(\n3221                     \"cannot swap from dimension %r because it is \"\n3222                     \"not an existing dimension\" % k\n3223                 )\n3224             if v in self.variables and self.variables[v].dims != (k,):\n3225                 raise ValueError(\n3226                     \"replacement dimension %r is not a 1D \"\n3227                     \"variable along the old dimension %r\" % (v, k)\n3228                 )\n3229 \n3230         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}\n3231 \n3232         coord_names = self._coord_names.copy()\n3233         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})\n3234 \n3235         variables: Dict[Hashable, Variable] = {}\n3236         indexes: Dict[Hashable, pd.Index] = {}\n3237         for k, v in self.variables.items():\n3238             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3239             if k in result_dims:\n3240                 var = v.to_index_variable()\n3241                 if k in self.indexes:\n3242                     indexes[k] = self.indexes[k]\n3243                 else:\n3244                     new_index = var.to_index()\n3245                     if new_index.nlevels == 1:\n3246                         # make sure index name matches dimension name\n3247                         new_index = new_index.rename(k)\n3248                     indexes[k] = new_index\n3249             else:\n3250                 var = v.to_base_variable()\n3251             var.dims = dims\n3252             variables[k] = var\n3253 \n3254         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3255 \n3256     def expand_dims(\n3257         self,\n3258         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n3259         axis: Union[None, int, Sequence[int]] = None,\n3260         **dim_kwargs: Any,\n3261     ) -> \"Dataset\":\n3262         \"\"\"Return a new object with an additional axis (or axes) inserted at\n3263         the corresponding position in the array shape.  The new object is a\n3264         view into the underlying array, not a copy.\n3265 \n3266         If dim is already a scalar coordinate, it will be promoted to a 1D\n3267         coordinate consisting of a single value.\n3268 \n3269         Parameters\n3270         ----------\n3271         dim : hashable, sequence of hashable, mapping, or None\n3272             Dimensions to include on the new variable. If provided as hashable\n3273             or sequence of hashable, then dimensions are inserted with length\n3274             1. If provided as a mapping, then the keys are the new dimensions\n3275             and the values are either integers (giving the length of the new\n3276             dimensions) or array-like (giving the coordinates of the new\n3277             dimensions).\n3278         axis : int, sequence of int, or None\n3279             Axis position(s) where new axis is to be inserted (position(s) on\n3280             the result array). If a list (or tuple) of integers is passed,\n3281             multiple axes are inserted. In this case, dim arguments should be\n3282             same length list. If axis=None is passed, all the axes will be\n3283             inserted to the start of the result array.\n3284         **dim_kwargs : int or sequence or ndarray\n3285             The keywords are arbitrary dimensions being inserted and the values\n3286             are either the lengths of the new dims (if int is given), or their\n3287             coordinates. Note, this is an alternative to passing a dict to the\n3288             dim kwarg and will only be used if dim is None.\n3289 \n3290         Returns\n3291         -------\n3292         expanded : same type as caller\n3293             This object, but with an additional dimension(s).\n3294         \"\"\"\n3295         if dim is None:\n3296             pass\n3297         elif isinstance(dim, Mapping):\n3298             # We're later going to modify dim in place; don't tamper with\n3299             # the input\n3300             dim = dict(dim)\n3301         elif isinstance(dim, int):\n3302             raise TypeError(\n3303                 \"dim should be hashable or sequence of hashables or mapping\"\n3304             )\n3305         elif isinstance(dim, str) or not isinstance(dim, Sequence):\n3306             dim = {dim: 1}\n3307         elif isinstance(dim, Sequence):\n3308             if len(dim) != len(set(dim)):\n3309                 raise ValueError(\"dims should not contain duplicate values.\")\n3310             dim = {d: 1 for d in dim}\n3311 \n3312         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n3313         assert isinstance(dim, MutableMapping)\n3314 \n3315         if axis is None:\n3316             axis = list(range(len(dim)))\n3317         elif not isinstance(axis, Sequence):\n3318             axis = [axis]\n3319 \n3320         if len(dim) != len(axis):\n3321             raise ValueError(\"lengths of dim and axis should be identical.\")\n3322         for d in dim:\n3323             if d in self.dims:\n3324                 raise ValueError(f\"Dimension {d} already exists.\")\n3325             if d in self._variables and not utils.is_scalar(self._variables[d]):\n3326                 raise ValueError(\n3327                     \"{dim} already exists as coordinate or\"\n3328                     \" variable name.\".format(dim=d)\n3329                 )\n3330 \n3331         variables: Dict[Hashable, Variable] = {}\n3332         coord_names = self._coord_names.copy()\n3333         # If dim is a dict, then ensure that the values are either integers\n3334         # or iterables.\n3335         for k, v in dim.items():\n3336             if hasattr(v, \"__iter__\"):\n3337                 # If the value for the new dimension is an iterable, then\n3338                 # save the coordinates to the variables dict, and set the\n3339                 # value within the dim dict to the length of the iterable\n3340                 # for later use.\n3341                 variables[k] = xr.IndexVariable((k,), v)\n3342                 coord_names.add(k)\n3343                 dim[k] = variables[k].size\n3344             elif isinstance(v, int):\n3345                 pass  # Do nothing if the dimensions value is just an int\n3346             else:\n3347                 raise TypeError(\n3348                     \"The value of new dimension {k} must be \"\n3349                     \"an iterable or an int\".format(k=k)\n3350                 )\n3351 \n3352         for k, v in self._variables.items():\n3353             if k not in dim:\n3354                 if k in coord_names:  # Do not change coordinates\n3355                     variables[k] = v\n3356                 else:\n3357                     result_ndim = len(v.dims) + len(axis)\n3358                     for a in axis:\n3359                         if a < -result_ndim or result_ndim - 1 < a:\n3360                             raise IndexError(\n3361                                 f\"Axis {a} of variable {k} is out of bounds of the \"\n3362                                 f\"expanded dimension size {result_ndim}\"\n3363                             )\n3364 \n3365                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n3366                     if len(axis_pos) != len(set(axis_pos)):\n3367                         raise ValueError(\"axis should not contain duplicate values\")\n3368                     # We need to sort them to make sure `axis` equals to the\n3369                     # axis positions of the result array.\n3370                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n3371 \n3372                     all_dims = list(zip(v.dims, v.shape))\n3373                     for d, c in zip_axis_dim:\n3374                         all_dims.insert(d, c)\n3375                     variables[k] = v.set_dims(dict(all_dims))\n3376             else:\n3377                 # If dims includes a label of a non-dimension coordinate,\n3378                 # it will be promoted to a 1D coordinate with a single value.\n3379                 variables[k] = v.set_dims(k).to_index_variable()\n3380 \n3381         new_dims = self._dims.copy()\n3382         new_dims.update(dim)\n3383 \n3384         return self._replace_vars_and_dims(\n3385             variables, dims=new_dims, coord_names=coord_names\n3386         )\n3387 \n3388     def set_index(\n3389         self,\n3390         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n3391         append: bool = False,\n3392         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n3393     ) -> \"Dataset\":\n3394         \"\"\"Set Dataset (multi-)indexes using one or more existing coordinates\n3395         or variables.\n3396 \n3397         Parameters\n3398         ----------\n3399         indexes : {dim: index, ...}\n3400             Mapping from names matching dimensions and values given\n3401             by (lists of) the names of existing coordinates or variables to set\n3402             as new (multi-)index.\n3403         append : bool, optional\n3404             If True, append the supplied index(es) to the existing index(es).\n3405             Otherwise replace the existing index(es) (default).\n3406         **indexes_kwargs : optional\n3407             The keyword arguments form of ``indexes``.\n3408             One of indexes or indexes_kwargs must be provided.\n3409 \n3410         Returns\n3411         -------\n3412         obj : Dataset\n3413             Another dataset, with this dataset's data but replaced coordinates.\n3414 \n3415         Examples\n3416         --------\n3417         >>> arr = xr.DataArray(\n3418         ...     data=np.ones((2, 3)),\n3419         ...     dims=[\"x\", \"y\"],\n3420         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n3421         ... )\n3422         >>> ds = xr.Dataset({\"v\": arr})\n3423         >>> ds\n3424         <xarray.Dataset>\n3425         Dimensions:  (x: 2, y: 3)\n3426         Coordinates:\n3427           * x        (x) int64 0 1\n3428           * y        (y) int64 0 1 2\n3429             a        (x) int64 3 4\n3430         Data variables:\n3431             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n3432         >>> ds.set_index(x=\"a\")\n3433         <xarray.Dataset>\n3434         Dimensions:  (x: 2, y: 3)\n3435         Coordinates:\n3436           * x        (x) int64 3 4\n3437           * y        (y) int64 0 1 2\n3438         Data variables:\n3439             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n3440 \n3441         See Also\n3442         --------\n3443         Dataset.reset_index\n3444         Dataset.swap_dims\n3445         \"\"\"\n3446         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, \"set_index\")\n3447         variables, coord_names = merge_indexes(\n3448             indexes, self._variables, self._coord_names, append=append\n3449         )\n3450         return self._replace_vars_and_dims(variables, coord_names=coord_names)\n3451 \n3452     def reset_index(\n3453         self,\n3454         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n3455         drop: bool = False,\n3456     ) -> \"Dataset\":\n3457         \"\"\"Reset the specified index(es) or multi-index level(s).\n3458 \n3459         Parameters\n3460         ----------\n3461         dims_or_levels : str or list\n3462             Name(s) of the dimension(s) and/or multi-index level(s) that will\n3463             be reset.\n3464         drop : bool, optional\n3465             If True, remove the specified indexes and/or multi-index levels\n3466             instead of extracting them as new coordinates (default: False).\n3467 \n3468         Returns\n3469         -------\n3470         obj : Dataset\n3471             Another dataset, with this dataset's data but replaced coordinates.\n3472 \n3473         See Also\n3474         --------\n3475         Dataset.set_index\n3476         \"\"\"\n3477         variables, coord_names = split_indexes(\n3478             dims_or_levels,\n3479             self._variables,\n3480             self._coord_names,\n3481             cast(Mapping[Hashable, Hashable], self._level_coords),\n3482             drop=drop,\n3483         )\n3484         return self._replace_vars_and_dims(variables, coord_names=coord_names)\n3485 \n3486     def reorder_levels(\n3487         self,\n3488         dim_order: Mapping[Hashable, Sequence[int]] = None,\n3489         **dim_order_kwargs: Sequence[int],\n3490     ) -> \"Dataset\":\n3491         \"\"\"Rearrange index levels using input order.\n3492 \n3493         Parameters\n3494         ----------\n3495         dim_order : optional\n3496             Mapping from names matching dimensions and values given\n3497             by lists representing new level orders. Every given dimension\n3498             must have a multi-index.\n3499         **dim_order_kwargs : optional\n3500             The keyword arguments form of ``dim_order``.\n3501             One of dim_order or dim_order_kwargs must be provided.\n3502 \n3503         Returns\n3504         -------\n3505         obj : Dataset\n3506             Another dataset, with this dataset's data but replaced\n3507             coordinates.\n3508         \"\"\"\n3509         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n3510         variables = self._variables.copy()\n3511         indexes = dict(self.indexes)\n3512         for dim, order in dim_order.items():\n3513             coord = self._variables[dim]\n3514             index = self.indexes[dim]\n3515             if not isinstance(index, pd.MultiIndex):\n3516                 raise ValueError(f\"coordinate {dim} has no MultiIndex\")\n3517             new_index = index.reorder_levels(order)\n3518             variables[dim] = IndexVariable(coord.dims, new_index)\n3519             indexes[dim] = new_index\n3520 \n3521         return self._replace(variables, indexes=indexes)\n3522 \n3523     def _stack_once(self, dims, new_dim):\n3524         if ... in dims:\n3525             dims = list(infix_dims(dims, self.dims))\n3526         variables = {}\n3527         for name, var in self.variables.items():\n3528             if name not in dims:\n3529                 if any(d in var.dims for d in dims):\n3530                     add_dims = [d for d in dims if d not in var.dims]\n3531                     vdims = list(var.dims) + add_dims\n3532                     shape = [self.dims[d] for d in vdims]\n3533                     exp_var = var.set_dims(vdims, shape)\n3534                     stacked_var = exp_var.stack(**{new_dim: dims})\n3535                     variables[name] = stacked_var\n3536                 else:\n3537                     variables[name] = var.copy(deep=False)\n3538 \n3539         # consider dropping levels that are unused?\n3540         levels = [self.get_index(dim) for dim in dims]\n3541         idx = utils.multiindex_from_product_levels(levels, names=dims)\n3542         variables[new_dim] = IndexVariable(new_dim, idx)\n3543 \n3544         coord_names = set(self._coord_names) - set(dims) | {new_dim}\n3545 \n3546         indexes = {k: v for k, v in self.indexes.items() if k not in dims}\n3547         indexes[new_dim] = idx\n3548 \n3549         return self._replace_with_new_dims(\n3550             variables, coord_names=coord_names, indexes=indexes\n3551         )\n3552 \n3553     def stack(\n3554         self,\n3555         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n3556         **dimensions_kwargs: Sequence[Hashable],\n3557     ) -> \"Dataset\":\n3558         \"\"\"\n3559         Stack any number of existing dimensions into a single new dimension.\n3560 \n3561         New dimensions will be added at the end, and the corresponding\n3562         coordinate variables will be combined into a MultiIndex.\n3563 \n3564         Parameters\n3565         ----------\n3566         dimensions : mapping of hashable to sequence of hashable\n3567             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new\n3568             dimensions, and the existing dimensions that they replace. An\n3569             ellipsis (`...`) will be replaced by all unlisted dimensions.\n3570             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n3571             all dimensions.\n3572         **dimensions_kwargs\n3573             The keyword arguments form of ``dimensions``.\n3574             One of dimensions or dimensions_kwargs must be provided.\n3575 \n3576         Returns\n3577         -------\n3578         stacked : Dataset\n3579             Dataset with stacked data.\n3580 \n3581         See Also\n3582         --------\n3583         Dataset.unstack\n3584         \"\"\"\n3585         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n3586         result = self\n3587         for new_dim, dims in dimensions.items():\n3588             result = result._stack_once(dims, new_dim)\n3589         return result\n3590 \n3591     def to_stacked_array(\n3592         self,\n3593         new_dim: Hashable,\n3594         sample_dims: Sequence[Hashable],\n3595         variable_dim: str = \"variable\",\n3596         name: Hashable = None,\n3597     ) -> \"DataArray\":\n3598         \"\"\"Combine variables of differing dimensionality into a DataArray\n3599         without broadcasting.\n3600 \n3601         This method is similar to Dataset.to_array but does not broadcast the\n3602         variables.\n3603 \n3604         Parameters\n3605         ----------\n3606         new_dim : hashable\n3607             Name of the new stacked coordinate\n3608         sample_dims : sequence of hashable\n3609             Dimensions that **will not** be stacked. Each array in the dataset\n3610             must share these dimensions. For machine learning applications,\n3611             these define the dimensions over which samples are drawn.\n3612         variable_dim : str, optional\n3613             Name of the level in the stacked coordinate which corresponds to\n3614             the variables.\n3615         name : str, optional\n3616             Name of the new data array.\n3617 \n3618         Returns\n3619         -------\n3620         stacked : DataArray\n3621             DataArray with the specified dimensions and data variables\n3622             stacked together. The stacked coordinate is named ``new_dim``\n3623             and represented by a MultiIndex object with a level containing the\n3624             data variable names. The name of this level is controlled using\n3625             the ``variable_dim`` argument.\n3626 \n3627         See Also\n3628         --------\n3629         Dataset.to_array\n3630         Dataset.stack\n3631         DataArray.to_unstacked_dataset\n3632 \n3633         Examples\n3634         --------\n3635         >>> data = xr.Dataset(\n3636         ...     data_vars={\n3637         ...         \"a\": ((\"x\", \"y\"), [[0, 1, 2], [3, 4, 5]]),\n3638         ...         \"b\": (\"x\", [6, 7]),\n3639         ...     },\n3640         ...     coords={\"y\": [\"u\", \"v\", \"w\"]},\n3641         ... )\n3642 \n3643         >>> data\n3644         <xarray.Dataset>\n3645         Dimensions:  (x: 2, y: 3)\n3646         Coordinates:\n3647           * y        (y) <U1 'u' 'v' 'w'\n3648         Dimensions without coordinates: x\n3649         Data variables:\n3650             a        (x, y) int64 0 1 2 3 4 5\n3651             b        (x) int64 6 7\n3652 \n3653         >>> data.to_stacked_array(\"z\", sample_dims=[\"x\"])\n3654         <xarray.DataArray 'a' (x: 2, z: 4)>\n3655         array([[0, 1, 2, 6],\n3656                [3, 4, 5, 7]])\n3657         Coordinates:\n3658           * z         (z) MultiIndex\n3659           - variable  (z) object 'a' 'a' 'a' 'b'\n3660           - y         (z) object 'u' 'v' 'w' nan\n3661         Dimensions without coordinates: x\n3662 \n3663         \"\"\"\n3664         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n3665 \n3666         for variable in self:\n3667             dims = self[variable].dims\n3668             dims_include_sample_dims = set(sample_dims) <= set(dims)\n3669             if not dims_include_sample_dims:\n3670                 raise ValueError(\n3671                     \"All variables in the dataset must contain the \"\n3672                     \"dimensions {}.\".format(dims)\n3673                 )\n3674 \n3675         def ensure_stackable(val):\n3676             assign_coords = {variable_dim: val.name}\n3677             for dim in stacking_dims:\n3678                 if dim not in val.dims:\n3679                     assign_coords[dim] = None\n3680 \n3681             expand_dims = set(stacking_dims).difference(set(val.dims))\n3682             expand_dims.add(variable_dim)\n3683             # must be list for .expand_dims\n3684             expand_dims = list(expand_dims)\n3685 \n3686             return (\n3687                 val.assign_coords(**assign_coords)\n3688                 .expand_dims(expand_dims)\n3689                 .stack({new_dim: (variable_dim,) + stacking_dims})\n3690             )\n3691 \n3692         # concatenate the arrays\n3693         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n3694         data_array = xr.concat(stackable_vars, dim=new_dim)\n3695 \n3696         # coerce the levels of the MultiIndex to have the same type as the\n3697         # input dimensions. This code is messy, so it might be better to just\n3698         # input a dummy value for the singleton dimension.\n3699         idx = data_array.indexes[new_dim]\n3700         levels = [idx.levels[0]] + [\n3701             level.astype(self[level.name].dtype) for level in idx.levels[1:]\n3702         ]\n3703         new_idx = idx.set_levels(levels)\n3704         data_array[new_dim] = IndexVariable(new_dim, new_idx)\n3705 \n3706         if name is not None:\n3707             data_array.name = name\n3708 \n3709         return data_array\n3710 \n3711     def _unstack_once(self, dim: Hashable, fill_value) -> \"Dataset\":\n3712         index = self.get_index(dim)\n3713         index = remove_unused_levels_categories(index)\n3714 \n3715         variables: Dict[Hashable, Variable] = {}\n3716         indexes = {k: v for k, v in self.indexes.items() if k != dim}\n3717 \n3718         for name, var in self.variables.items():\n3719             if name != dim:\n3720                 if dim in var.dims:\n3721                     if isinstance(fill_value, Mapping):\n3722                         fill_value_ = fill_value[name]\n3723                     else:\n3724                         fill_value_ = fill_value\n3725 \n3726                     variables[name] = var._unstack_once(\n3727                         index=index, dim=dim, fill_value=fill_value_\n3728                     )\n3729                 else:\n3730                     variables[name] = var\n3731 \n3732         for name, lev in zip(index.names, index.levels):\n3733             variables[name] = IndexVariable(name, lev)\n3734             indexes[name] = lev\n3735 \n3736         coord_names = set(self._coord_names) - {dim} | set(index.names)\n3737 \n3738         return self._replace_with_new_dims(\n3739             variables, coord_names=coord_names, indexes=indexes\n3740         )\n3741 \n3742     def _unstack_full_reindex(\n3743         self, dim: Hashable, fill_value, sparse: bool\n3744     ) -> \"Dataset\":\n3745         index = self.get_index(dim)\n3746         index = remove_unused_levels_categories(index)\n3747         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)\n3748 \n3749         # take a shortcut in case the MultiIndex was not modified.\n3750         if index.equals(full_idx):\n3751             obj = self\n3752         else:\n3753             obj = self._reindex(\n3754                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse\n3755             )\n3756 \n3757         new_dim_names = index.names\n3758         new_dim_sizes = [lev.size for lev in index.levels]\n3759 \n3760         variables: Dict[Hashable, Variable] = {}\n3761         indexes = {k: v for k, v in self.indexes.items() if k != dim}\n3762 \n3763         for name, var in obj.variables.items():\n3764             if name != dim:\n3765                 if dim in var.dims:\n3766                     new_dims = dict(zip(new_dim_names, new_dim_sizes))\n3767                     variables[name] = var.unstack({dim: new_dims})\n3768                 else:\n3769                     variables[name] = var\n3770 \n3771         for name, lev in zip(new_dim_names, index.levels):\n3772             variables[name] = IndexVariable(name, lev)\n3773             indexes[name] = lev\n3774 \n3775         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)\n3776 \n3777         return self._replace_with_new_dims(\n3778             variables, coord_names=coord_names, indexes=indexes\n3779         )\n3780 \n3781     def unstack(\n3782         self,\n3783         dim: Union[Hashable, Iterable[Hashable]] = None,\n3784         fill_value: Any = dtypes.NA,\n3785         sparse: bool = False,\n3786     ) -> \"Dataset\":\n3787         \"\"\"\n3788         Unstack existing dimensions corresponding to MultiIndexes into\n3789         multiple new dimensions.\n3790 \n3791         New dimensions will be added at the end.\n3792 \n3793         Parameters\n3794         ----------\n3795         dim : hashable or iterable of hashable, optional\n3796             Dimension(s) over which to unstack. By default unstacks all\n3797             MultiIndexes.\n3798         fill_value : scalar or dict-like, default: nan\n3799             value to be filled. If a dict-like, maps variable names to\n3800             fill values. If not provided or if the dict-like does not\n3801             contain all variables, the dtype's NA value will be used.\n3802         sparse : bool, default: False\n3803             use sparse-array if True\n3804 \n3805         Returns\n3806         -------\n3807         unstacked : Dataset\n3808             Dataset with unstacked data.\n3809 \n3810         See Also\n3811         --------\n3812         Dataset.stack\n3813         \"\"\"\n3814         if dim is None:\n3815             dims = [\n3816                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)\n3817             ]\n3818         else:\n3819             if isinstance(dim, str) or not isinstance(dim, Iterable):\n3820                 dims = [dim]\n3821             else:\n3822                 dims = list(dim)\n3823 \n3824             missing_dims = [d for d in dims if d not in self.dims]\n3825             if missing_dims:\n3826                 raise ValueError(\n3827                     \"Dataset does not contain the dimensions: %s\" % missing_dims\n3828                 )\n3829 \n3830             non_multi_dims = [\n3831                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)\n3832             ]\n3833             if non_multi_dims:\n3834                 raise ValueError(\n3835                     \"cannot unstack dimensions that do not \"\n3836                     \"have a MultiIndex: %s\" % non_multi_dims\n3837                 )\n3838 \n3839         result = self.copy(deep=False)\n3840         for dim in dims:\n3841 \n3842             if (\n3843                 # Dask arrays don't support assignment by index, which the fast unstack\n3844                 # function requires.\n3845                 # https://github.com/pydata/xarray/pull/4746#issuecomment-753282125\n3846                 any(is_duck_dask_array(v.data) for v in self.variables.values())\n3847                 # Sparse doesn't currently support (though we could special-case\n3848                 # it)\n3849                 # https://github.com/pydata/sparse/issues/422\n3850                 or any(\n3851                     isinstance(v.data, sparse_array_type)\n3852                     for v in self.variables.values()\n3853                 )\n3854                 or sparse\n3855                 # numpy full_like only added `shape` in 1.17\n3856                 or LooseVersion(np.__version__) < LooseVersion(\"1.17\")\n3857                 # Until https://github.com/pydata/xarray/pull/4751 is resolved,\n3858                 # we check explicitly whether it's a numpy array. Once that is\n3859                 # resolved, explicitly exclude pint arrays.\n3860                 # # pint doesn't implement `np.full_like` in a way that's\n3861                 # # currently compatible.\n3862                 # # https://github.com/pydata/xarray/pull/4746#issuecomment-753425173\n3863                 # # or any(\n3864                 # #     isinstance(v.data, pint_array_type) for v in self.variables.values()\n3865                 # # )\n3866                 or any(\n3867                     not isinstance(v.data, np.ndarray) for v in self.variables.values()\n3868                 )\n3869             ):\n3870                 result = result._unstack_full_reindex(dim, fill_value, sparse)\n3871             else:\n3872                 result = result._unstack_once(dim, fill_value)\n3873         return result\n3874 \n3875     def update(self, other: \"CoercibleMapping\") -> \"Dataset\":\n3876         \"\"\"Update this dataset's variables with those from another dataset.\n3877 \n3878         Parameters\n3879         ----------\n3880         other : Dataset or mapping\n3881             Variables with which to update this dataset. One of:\n3882 \n3883             - Dataset\n3884             - mapping {var name: DataArray}\n3885             - mapping {var name: Variable}\n3886             - mapping {var name: (dimension name, array-like)}\n3887             - mapping {var name: (tuple of dimension names, array-like)}\n3888 \n3889         Returns\n3890         -------\n3891         updated : Dataset\n3892             Updated dataset.\n3893 \n3894         Raises\n3895         ------\n3896         ValueError\n3897             If any dimensions would have inconsistent sizes in the updated\n3898             dataset.\n3899         \"\"\"\n3900         merge_result = dataset_update_method(self, other)\n3901         return self._replace(inplace=True, **merge_result._asdict())\n3902 \n3903     def merge(\n3904         self,\n3905         other: Union[\"CoercibleMapping\", \"DataArray\"],\n3906         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),\n3907         compat: str = \"no_conflicts\",\n3908         join: str = \"outer\",\n3909         fill_value: Any = dtypes.NA,\n3910     ) -> \"Dataset\":\n3911         \"\"\"Merge the arrays of two datasets into a single dataset.\n3912 \n3913         This method generally does not allow for overriding data, with the\n3914         exception of attributes, which are ignored on the second dataset.\n3915         Variables with the same name are checked for conflicts via the equals\n3916         or identical methods.\n3917 \n3918         Parameters\n3919         ----------\n3920         other : Dataset or mapping\n3921             Dataset or variables to merge with this dataset.\n3922         overwrite_vars : hashable or iterable of hashable, optional\n3923             If provided, update variables of these name(s) without checking for\n3924             conflicts in this dataset.\n3925         compat : {\"broadcast_equals\", \"equals\", \"identical\", \\\n3926                   \"no_conflicts\"}, optional\n3927             String indicating how to compare variables of the same name for\n3928             potential conflicts:\n3929 \n3930             - 'broadcast_equals': all values must be equal when variables are\n3931               broadcast against each other to ensure common dimensions.\n3932             - 'equals': all values and dimensions must be the same.\n3933             - 'identical': all values, dimensions and attributes must be the\n3934               same.\n3935             - 'no_conflicts': only values which are not null in both datasets\n3936               must be equal. The returned dataset then contains the combination\n3937               of all non-null values.\n3938 \n3939         join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n3940             Method for joining ``self`` and ``other`` along shared dimensions:\n3941 \n3942             - 'outer': use the union of the indexes\n3943             - 'inner': use the intersection of the indexes\n3944             - 'left': use indexes from ``self``\n3945             - 'right': use indexes from ``other``\n3946             - 'exact': error instead of aligning non-equal indexes\n3947         fill_value : scalar or dict-like, optional\n3948             Value to use for newly missing values. If a dict-like, maps\n3949             variable names (including coordinates) to fill values.\n3950 \n3951         Returns\n3952         -------\n3953         merged : Dataset\n3954             Merged dataset.\n3955 \n3956         Raises\n3957         ------\n3958         MergeError\n3959             If any variables conflict (see ``compat``).\n3960         \"\"\"\n3961         other = other.to_dataset() if isinstance(other, xr.DataArray) else other\n3962         merge_result = dataset_merge_method(\n3963             self,\n3964             other,\n3965             overwrite_vars=overwrite_vars,\n3966             compat=compat,\n3967             join=join,\n3968             fill_value=fill_value,\n3969         )\n3970         return self._replace(**merge_result._asdict())\n3971 \n3972     def _assert_all_in_dataset(\n3973         self, names: Iterable[Hashable], virtual_okay: bool = False\n3974     ) -> None:\n3975         bad_names = set(names) - set(self._variables)\n3976         if virtual_okay:\n3977             bad_names -= self.virtual_variables\n3978         if bad_names:\n3979             raise ValueError(\n3980                 \"One or more of the specified variables \"\n3981                 \"cannot be found in this dataset\"\n3982             )\n3983 \n3984     def drop_vars(\n3985         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n3986     ) -> \"Dataset\":\n3987         \"\"\"Drop variables from this dataset.\n3988 \n3989         Parameters\n3990         ----------\n3991         names : hashable or iterable of hashable\n3992             Name(s) of variables to drop.\n3993         errors : {\"raise\", \"ignore\"}, optional\n3994             If 'raise' (default), raises a ValueError error if any of the variable\n3995             passed are not in the dataset. If 'ignore', any given names that are in the\n3996             dataset are dropped and no error is raised.\n3997 \n3998         Returns\n3999         -------\n4000         dropped : Dataset\n4001 \n4002         \"\"\"\n4003         # the Iterable check is required for mypy\n4004         if is_scalar(names) or not isinstance(names, Iterable):\n4005             names = {names}\n4006         else:\n4007             names = set(names)\n4008         if errors == \"raise\":\n4009             self._assert_all_in_dataset(names)\n4010 \n4011         variables = {k: v for k, v in self._variables.items() if k not in names}\n4012         coord_names = {k for k in self._coord_names if k in variables}\n4013         indexes = {k: v for k, v in self.indexes.items() if k not in names}\n4014         return self._replace_with_new_dims(\n4015             variables, coord_names=coord_names, indexes=indexes\n4016         )\n4017 \n4018     def drop(self, labels=None, dim=None, *, errors=\"raise\", **labels_kwargs):\n4019         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n4020 \n4021         Using either `drop_vars` or `drop_sel` is encouraged\n4022 \n4023         See Also\n4024         --------\n4025         Dataset.drop_vars\n4026         Dataset.drop_sel\n4027         \"\"\"\n4028         if errors not in [\"raise\", \"ignore\"]:\n4029             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n4030 \n4031         if is_dict_like(labels) and not isinstance(labels, dict):\n4032             warnings.warn(\n4033                 \"dropping coordinates using `drop` is be deprecated; use drop_vars.\",\n4034                 FutureWarning,\n4035                 stacklevel=2,\n4036             )\n4037             return self.drop_vars(labels, errors=errors)\n4038 \n4039         if labels_kwargs or isinstance(labels, dict):\n4040             if dim is not None:\n4041                 raise ValueError(\"cannot specify dim and dict-like arguments.\")\n4042             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n4043 \n4044         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n4045             warnings.warn(\n4046                 \"dropping variables using `drop` will be deprecated; using drop_vars is encouraged.\",\n4047                 PendingDeprecationWarning,\n4048                 stacklevel=2,\n4049             )\n4050             return self.drop_vars(labels, errors=errors)\n4051         if dim is not None:\n4052             warnings.warn(\n4053                 \"dropping labels using list-like labels is deprecated; using \"\n4054                 \"dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).\",\n4055                 DeprecationWarning,\n4056                 stacklevel=2,\n4057             )\n4058             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n4059 \n4060         warnings.warn(\n4061             \"dropping labels using `drop` will be deprecated; using drop_sel is encouraged.\",\n4062             PendingDeprecationWarning,\n4063             stacklevel=2,\n4064         )\n4065         return self.drop_sel(labels, errors=errors)\n4066 \n4067     def drop_sel(self, labels=None, *, errors=\"raise\", **labels_kwargs):\n4068         \"\"\"Drop index labels from this dataset.\n4069 \n4070         Parameters\n4071         ----------\n4072         labels : mapping of hashable to Any\n4073             Index labels to drop\n4074         errors : {\"raise\", \"ignore\"}, optional\n4075             If 'raise' (default), raises a ValueError error if\n4076             any of the index labels passed are not\n4077             in the dataset. If 'ignore', any given labels that are in the\n4078             dataset are dropped and no error is raised.\n4079         **labels_kwargs : {dim: label, ...}, optional\n4080             The keyword arguments form of ``dim`` and ``labels``\n4081 \n4082         Returns\n4083         -------\n4084         dropped : Dataset\n4085 \n4086         Examples\n4087         --------\n4088         >>> data = np.arange(6).reshape(2, 3)\n4089         >>> labels = [\"a\", \"b\", \"c\"]\n4090         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n4091         >>> ds\n4092         <xarray.Dataset>\n4093         Dimensions:  (x: 2, y: 3)\n4094         Coordinates:\n4095           * y        (y) <U1 'a' 'b' 'c'\n4096         Dimensions without coordinates: x\n4097         Data variables:\n4098             A        (x, y) int64 0 1 2 3 4 5\n4099         >>> ds.drop_sel(y=[\"a\", \"c\"])\n4100         <xarray.Dataset>\n4101         Dimensions:  (x: 2, y: 1)\n4102         Coordinates:\n4103           * y        (y) <U1 'b'\n4104         Dimensions without coordinates: x\n4105         Data variables:\n4106             A        (x, y) int64 1 4\n4107         >>> ds.drop_sel(y=\"b\")\n4108         <xarray.Dataset>\n4109         Dimensions:  (x: 2, y: 2)\n4110         Coordinates:\n4111           * y        (y) <U1 'a' 'c'\n4112         Dimensions without coordinates: x\n4113         Data variables:\n4114             A        (x, y) int64 0 2 3 5\n4115         \"\"\"\n4116         if errors not in [\"raise\", \"ignore\"]:\n4117             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n4118 \n4119         labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop_sel\")\n4120 \n4121         ds = self\n4122         for dim, labels_for_dim in labels.items():\n4123             # Don't cast to set, as it would harm performance when labels\n4124             # is a large numpy array\n4125             if utils.is_scalar(labels_for_dim):\n4126                 labels_for_dim = [labels_for_dim]\n4127             labels_for_dim = np.asarray(labels_for_dim)\n4128             try:\n4129                 index = self.get_index(dim)\n4130             except KeyError:\n4131                 raise ValueError(\"dimension %r does not have coordinate labels\" % dim)\n4132             new_index = index.drop(labels_for_dim, errors=errors)\n4133             ds = ds.loc[{dim: new_index}]\n4134         return ds\n4135 \n4136     def drop_isel(self, indexers=None, **indexers_kwargs):\n4137         \"\"\"Drop index positions from this Dataset.\n4138 \n4139         Parameters\n4140         ----------\n4141         indexers : mapping of hashable to Any\n4142             Index locations to drop\n4143         **indexers_kwargs : {dim: position, ...}, optional\n4144             The keyword arguments form of ``dim`` and ``positions``\n4145 \n4146         Returns\n4147         -------\n4148         dropped : Dataset\n4149 \n4150         Raises\n4151         ------\n4152         IndexError\n4153 \n4154         Examples\n4155         --------\n4156         >>> data = np.arange(6).reshape(2, 3)\n4157         >>> labels = [\"a\", \"b\", \"c\"]\n4158         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n4159         >>> ds\n4160         <xarray.Dataset>\n4161         Dimensions:  (x: 2, y: 3)\n4162         Coordinates:\n4163           * y        (y) <U1 'a' 'b' 'c'\n4164         Dimensions without coordinates: x\n4165         Data variables:\n4166             A        (x, y) int64 0 1 2 3 4 5\n4167         >>> ds.drop_isel(y=[0, 2])\n4168         <xarray.Dataset>\n4169         Dimensions:  (x: 2, y: 1)\n4170         Coordinates:\n4171           * y        (y) <U1 'b'\n4172         Dimensions without coordinates: x\n4173         Data variables:\n4174             A        (x, y) int64 1 4\n4175         >>> ds.drop_isel(y=1)\n4176         <xarray.Dataset>\n4177         Dimensions:  (x: 2, y: 2)\n4178         Coordinates:\n4179           * y        (y) <U1 'a' 'c'\n4180         Dimensions without coordinates: x\n4181         Data variables:\n4182             A        (x, y) int64 0 2 3 5\n4183         \"\"\"\n4184 \n4185         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"drop_isel\")\n4186 \n4187         ds = self\n4188         dimension_index = {}\n4189         for dim, pos_for_dim in indexers.items():\n4190             # Don't cast to set, as it would harm performance when labels\n4191             # is a large numpy array\n4192             if utils.is_scalar(pos_for_dim):\n4193                 pos_for_dim = [pos_for_dim]\n4194             pos_for_dim = np.asarray(pos_for_dim)\n4195             index = self.get_index(dim)\n4196             new_index = index.delete(pos_for_dim)\n4197             dimension_index[dim] = new_index\n4198         ds = ds.loc[dimension_index]\n4199         return ds\n4200 \n4201     def drop_dims(\n4202         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n4203     ) -> \"Dataset\":\n4204         \"\"\"Drop dimensions and associated variables from this dataset.\n4205 \n4206         Parameters\n4207         ----------\n4208         drop_dims : hashable or iterable of hashable\n4209             Dimension or dimensions to drop.\n4210         errors : {\"raise\", \"ignore\"}, optional\n4211             If 'raise' (default), raises a ValueError error if any of the\n4212             dimensions passed are not in the dataset. If 'ignore', any given\n4213             labels that are in the dataset are dropped and no error is raised.\n4214 \n4215         Returns\n4216         -------\n4217         obj : Dataset\n4218             The dataset without the given dimensions (or any variables\n4219             containing those dimensions)\n4220         errors : {\"raise\", \"ignore\"}, optional\n4221             If 'raise' (default), raises a ValueError error if\n4222             any of the dimensions passed are not\n4223             in the dataset. If 'ignore', any given dimensions that are in the\n4224             dataset are dropped and no error is raised.\n4225         \"\"\"\n4226         if errors not in [\"raise\", \"ignore\"]:\n4227             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n4228 \n4229         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):\n4230             drop_dims = {drop_dims}\n4231         else:\n4232             drop_dims = set(drop_dims)\n4233 \n4234         if errors == \"raise\":\n4235             missing_dims = drop_dims - set(self.dims)\n4236             if missing_dims:\n4237                 raise ValueError(\n4238                     \"Dataset does not contain the dimensions: %s\" % missing_dims\n4239                 )\n4240 \n4241         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}\n4242         return self.drop_vars(drop_vars)\n4243 \n4244     def transpose(self, *dims: Hashable) -> \"Dataset\":\n4245         \"\"\"Return a new Dataset object with all array dimensions transposed.\n4246 \n4247         Although the order of dimensions on each array will change, the dataset\n4248         dimensions themselves will remain in fixed (sorted) order.\n4249 \n4250         Parameters\n4251         ----------\n4252         *dims : hashable, optional\n4253             By default, reverse the dimensions on each array. Otherwise,\n4254             reorder the dimensions to this order.\n4255 \n4256         Returns\n4257         -------\n4258         transposed : Dataset\n4259             Each array in the dataset (including) coordinates will be\n4260             transposed to the given order.\n4261 \n4262         Notes\n4263         -----\n4264         This operation returns a view of each array's data. It is\n4265         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n4266         -- the data will be fully loaded into memory.\n4267 \n4268         See Also\n4269         --------\n4270         numpy.transpose\n4271         DataArray.transpose\n4272         \"\"\"\n4273         if dims:\n4274             if set(dims) ^ set(self.dims) and ... not in dims:\n4275                 raise ValueError(\n4276                     \"arguments to transpose (%s) must be \"\n4277                     \"permuted dataset dimensions (%s)\" % (dims, tuple(self.dims))\n4278                 )\n4279         ds = self.copy()\n4280         for name, var in self._variables.items():\n4281             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))\n4282             ds._variables[name] = var.transpose(*var_dims)\n4283         return ds\n4284 \n4285     def dropna(\n4286         self,\n4287         dim: Hashable,\n4288         how: str = \"any\",\n4289         thresh: int = None,\n4290         subset: Iterable[Hashable] = None,\n4291     ):\n4292         \"\"\"Returns a new dataset with dropped labels for missing values along\n4293         the provided dimension.\n4294 \n4295         Parameters\n4296         ----------\n4297         dim : hashable\n4298             Dimension along which to drop missing values. Dropping along\n4299             multiple dimensions simultaneously is not yet supported.\n4300         how : {\"any\", \"all\"}, default: \"any\"\n4301             * any : if any NA values are present, drop that label\n4302             * all : if all values are NA, drop that label\n4303         thresh : int, default: None\n4304             If supplied, require this many non-NA values.\n4305         subset : iterable of hashable, optional\n4306             Which variables to check for missing values. By default, all\n4307             variables in the dataset are checked.\n4308 \n4309         Returns\n4310         -------\n4311         Dataset\n4312         \"\"\"\n4313         # TODO: consider supporting multiple dimensions? Or not, given that\n4314         # there are some ugly edge cases, e.g., pandas's dropna differs\n4315         # depending on the order of the supplied axes.\n4316 \n4317         if dim not in self.dims:\n4318             raise ValueError(\"%s must be a single dataset dimension\" % dim)\n4319 \n4320         if subset is None:\n4321             subset = iter(self.data_vars)\n4322 \n4323         count = np.zeros(self.dims[dim], dtype=np.int64)\n4324         size = 0\n4325 \n4326         for k in subset:\n4327             array = self._variables[k]\n4328             if dim in array.dims:\n4329                 dims = [d for d in array.dims if d != dim]\n4330                 count += np.asarray(array.count(dims))  # type: ignore\n4331                 size += np.prod([self.dims[d] for d in dims])\n4332 \n4333         if thresh is not None:\n4334             mask = count >= thresh\n4335         elif how == \"any\":\n4336             mask = count == size\n4337         elif how == \"all\":\n4338             mask = count > 0\n4339         elif how is not None:\n4340             raise ValueError(\"invalid how option: %s\" % how)\n4341         else:\n4342             raise TypeError(\"must specify how or thresh\")\n4343 \n4344         return self.isel({dim: mask})\n4345 \n4346     def fillna(self, value: Any) -> \"Dataset\":\n4347         \"\"\"Fill missing values in this object.\n4348 \n4349         This operation follows the normal broadcasting and alignment rules that\n4350         xarray uses for binary arithmetic, except the result is aligned to this\n4351         object (``join='left'``) instead of aligned to the intersection of\n4352         index coordinates (``join='inner'``).\n4353 \n4354         Parameters\n4355         ----------\n4356         value : scalar, ndarray, DataArray, dict or Dataset\n4357             Used to fill all matching missing values in this dataset's data\n4358             variables. Scalars, ndarrays or DataArrays arguments are used to\n4359             fill all data with aligned coordinates (for DataArrays).\n4360             Dictionaries or datasets match data variables and then align\n4361             coordinates if necessary.\n4362 \n4363         Returns\n4364         -------\n4365         Dataset\n4366 \n4367         Examples\n4368         --------\n4369         >>> import numpy as np\n4370         >>> import xarray as xr\n4371         >>> ds = xr.Dataset(\n4372         ...     {\n4373         ...         \"A\": (\"x\", [np.nan, 2, np.nan, 0]),\n4374         ...         \"B\": (\"x\", [3, 4, np.nan, 1]),\n4375         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5]),\n4376         ...         \"D\": (\"x\", [np.nan, 3, np.nan, 4]),\n4377         ...     },\n4378         ...     coords={\"x\": [0, 1, 2, 3]},\n4379         ... )\n4380         >>> ds\n4381         <xarray.Dataset>\n4382         Dimensions:  (x: 4)\n4383         Coordinates:\n4384           * x        (x) int64 0 1 2 3\n4385         Data variables:\n4386             A        (x) float64 nan 2.0 nan 0.0\n4387             B        (x) float64 3.0 4.0 nan 1.0\n4388             C        (x) float64 nan nan nan 5.0\n4389             D        (x) float64 nan 3.0 nan 4.0\n4390 \n4391         Replace all `NaN` values with 0s.\n4392 \n4393         >>> ds.fillna(0)\n4394         <xarray.Dataset>\n4395         Dimensions:  (x: 4)\n4396         Coordinates:\n4397           * x        (x) int64 0 1 2 3\n4398         Data variables:\n4399             A        (x) float64 0.0 2.0 0.0 0.0\n4400             B        (x) float64 3.0 4.0 0.0 1.0\n4401             C        (x) float64 0.0 0.0 0.0 5.0\n4402             D        (x) float64 0.0 3.0 0.0 4.0\n4403 \n4404         Replace all `NaN` elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.\n4405 \n4406         >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n4407         >>> ds.fillna(value=values)\n4408         <xarray.Dataset>\n4409         Dimensions:  (x: 4)\n4410         Coordinates:\n4411           * x        (x) int64 0 1 2 3\n4412         Data variables:\n4413             A        (x) float64 0.0 2.0 0.0 0.0\n4414             B        (x) float64 3.0 4.0 1.0 1.0\n4415             C        (x) float64 2.0 2.0 2.0 5.0\n4416             D        (x) float64 3.0 3.0 3.0 4.0\n4417         \"\"\"\n4418         if utils.is_dict_like(value):\n4419             value_keys = getattr(value, \"data_vars\", value).keys()\n4420             if not set(value_keys) <= set(self.data_vars.keys()):\n4421                 raise ValueError(\n4422                     \"all variables in the argument to `fillna` \"\n4423                     \"must be contained in the original dataset\"\n4424                 )\n4425         out = ops.fillna(self, value)\n4426         return out\n4427 \n4428     def interpolate_na(\n4429         self,\n4430         dim: Hashable = None,\n4431         method: str = \"linear\",\n4432         limit: int = None,\n4433         use_coordinate: Union[bool, Hashable] = True,\n4434         max_gap: Union[\n4435             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n4436         ] = None,\n4437         **kwargs: Any,\n4438     ) -> \"Dataset\":\n4439         \"\"\"Fill in NaNs by interpolating according to different methods.\n4440 \n4441         Parameters\n4442         ----------\n4443         dim : str\n4444             Specifies the dimension along which to interpolate.\n4445         method : str, optional\n4446             String indicating which method to use for interpolation:\n4447 \n4448             - 'linear': linear interpolation (Default). Additional keyword\n4449               arguments are passed to :py:func:`numpy.interp`\n4450             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n4451               are passed to :py:func:`scipy.interpolate.interp1d`. If\n4452               ``method='polynomial'``, the ``order`` keyword argument must also be\n4453               provided.\n4454             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n4455               respective :py:class:`scipy.interpolate` classes.\n4456         use_coordinate : bool, str, default: True\n4457             Specifies which index to use as the x values in the interpolation\n4458             formulated as `y = f(x)`. If False, values are treated as if\n4459             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n4460             used. If ``use_coordinate`` is a string, it specifies the name of a\n4461             coordinate variariable to use as the index.\n4462         limit : int, default: None\n4463             Maximum number of consecutive NaNs to fill. Must be greater than 0\n4464             or None for no limit. This filling is done regardless of the size of\n4465             the gap in the data. To only interpolate over gaps less than a given length,\n4466             see ``max_gap``.\n4467         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n4468             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n4469             Use None for no limit. When interpolating along a datetime64 dimension\n4470             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n4471 \n4472             - a string that is valid input for pandas.to_timedelta\n4473             - a :py:class:`numpy.timedelta64` object\n4474             - a :py:class:`pandas.Timedelta` object\n4475             - a :py:class:`datetime.timedelta` object\n4476 \n4477             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n4478             dimensions has not been implemented yet. Gap length is defined as the difference\n4479             between coordinate values at the first data point after a gap and the last value\n4480             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n4481             between coordinate values at the first (last) valid data point and the first (last) NaN.\n4482             For example, consider::\n4483 \n4484                 <xarray.DataArray (x: 9)>\n4485                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n4486                 Coordinates:\n4487                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n4488 \n4489             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n4490         kwargs : dict, optional\n4491             parameters passed verbatim to the underlying interpolation function\n4492 \n4493         Returns\n4494         -------\n4495         interpolated: Dataset\n4496             Filled in Dataset.\n4497 \n4498         See Also\n4499         --------\n4500         numpy.interp\n4501         scipy.interpolate\n4502 \n4503         Examples\n4504         --------\n4505         >>> ds = xr.Dataset(\n4506         ...     {\n4507         ...         \"A\": (\"x\", [np.nan, 2, 3, np.nan, 0]),\n4508         ...         \"B\": (\"x\", [3, 4, np.nan, 1, 7]),\n4509         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5, 0]),\n4510         ...         \"D\": (\"x\", [np.nan, 3, np.nan, -1, 4]),\n4511         ...     },\n4512         ...     coords={\"x\": [0, 1, 2, 3, 4]},\n4513         ... )\n4514         >>> ds\n4515         <xarray.Dataset>\n4516         Dimensions:  (x: 5)\n4517         Coordinates:\n4518           * x        (x) int64 0 1 2 3 4\n4519         Data variables:\n4520             A        (x) float64 nan 2.0 3.0 nan 0.0\n4521             B        (x) float64 3.0 4.0 nan 1.0 7.0\n4522             C        (x) float64 nan nan nan 5.0 0.0\n4523             D        (x) float64 nan 3.0 nan -1.0 4.0\n4524 \n4525         >>> ds.interpolate_na(dim=\"x\", method=\"linear\")\n4526         <xarray.Dataset>\n4527         Dimensions:  (x: 5)\n4528         Coordinates:\n4529           * x        (x) int64 0 1 2 3 4\n4530         Data variables:\n4531             A        (x) float64 nan 2.0 3.0 1.5 0.0\n4532             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n4533             C        (x) float64 nan nan nan 5.0 0.0\n4534             D        (x) float64 nan 3.0 1.0 -1.0 4.0\n4535 \n4536         >>> ds.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n4537         <xarray.Dataset>\n4538         Dimensions:  (x: 5)\n4539         Coordinates:\n4540           * x        (x) int64 0 1 2 3 4\n4541         Data variables:\n4542             A        (x) float64 1.0 2.0 3.0 1.5 0.0\n4543             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n4544             C        (x) float64 20.0 15.0 10.0 5.0 0.0\n4545             D        (x) float64 5.0 3.0 1.0 -1.0 4.0\n4546         \"\"\"\n4547         from .missing import _apply_over_vars_with_dim, interp_na\n4548 \n4549         new = _apply_over_vars_with_dim(\n4550             interp_na,\n4551             self,\n4552             dim=dim,\n4553             method=method,\n4554             limit=limit,\n4555             use_coordinate=use_coordinate,\n4556             max_gap=max_gap,\n4557             **kwargs,\n4558         )\n4559         return new\n4560 \n4561     def ffill(self, dim: Hashable, limit: int = None) -> \"Dataset\":\n4562         \"\"\"Fill NaN values by propogating values forward\n4563 \n4564         *Requires bottleneck.*\n4565 \n4566         Parameters\n4567         ----------\n4568         dim : Hashable\n4569             Specifies the dimension along which to propagate values when\n4570             filling.\n4571         limit : int, default: None\n4572             The maximum number of consecutive NaN values to forward fill. In\n4573             other words, if there is a gap with more than this number of\n4574             consecutive NaNs, it will only be partially filled. Must be greater\n4575             than 0 or None for no limit.\n4576 \n4577         Returns\n4578         -------\n4579         Dataset\n4580         \"\"\"\n4581         from .missing import _apply_over_vars_with_dim, ffill\n4582 \n4583         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n4584         return new\n4585 \n4586     def bfill(self, dim: Hashable, limit: int = None) -> \"Dataset\":\n4587         \"\"\"Fill NaN values by propogating values backward\n4588 \n4589         *Requires bottleneck.*\n4590 \n4591         Parameters\n4592         ----------\n4593         dim : str\n4594             Specifies the dimension along which to propagate values when\n4595             filling.\n4596         limit : int, default: None\n4597             The maximum number of consecutive NaN values to backward fill. In\n4598             other words, if there is a gap with more than this number of\n4599             consecutive NaNs, it will only be partially filled. Must be greater\n4600             than 0 or None for no limit.\n4601 \n4602         Returns\n4603         -------\n4604         Dataset\n4605         \"\"\"\n4606         from .missing import _apply_over_vars_with_dim, bfill\n4607 \n4608         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n4609         return new\n4610 \n4611     def combine_first(self, other: \"Dataset\") -> \"Dataset\":\n4612         \"\"\"Combine two Datasets, default to data_vars of self.\n4613 \n4614         The new coordinates follow the normal broadcasting and alignment rules\n4615         of ``join='outer'``.  Vacant cells in the expanded coordinates are\n4616         filled with np.nan.\n4617 \n4618         Parameters\n4619         ----------\n4620         other : Dataset\n4621             Used to fill all matching missing values in this array.\n4622 \n4623         Returns\n4624         -------\n4625         Dataset\n4626         \"\"\"\n4627         out = ops.fillna(self, other, join=\"outer\", dataset_join=\"outer\")\n4628         return out\n4629 \n4630     def reduce(\n4631         self,\n4632         func: Callable,\n4633         dim: Union[Hashable, Iterable[Hashable]] = None,\n4634         keep_attrs: bool = None,\n4635         keepdims: bool = False,\n4636         numeric_only: bool = False,\n4637         **kwargs: Any,\n4638     ) -> \"Dataset\":\n4639         \"\"\"Reduce this dataset by applying `func` along some dimension(s).\n4640 \n4641         Parameters\n4642         ----------\n4643         func : callable\n4644             Function which can be called in the form\n4645             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n4646             np.ndarray over an integer valued axis.\n4647         dim : str or sequence of str, optional\n4648             Dimension(s) over which to apply `func`.  By default `func` is\n4649             applied over all dimensions.\n4650         keep_attrs : bool, optional\n4651             If True, the dataset's attributes (`attrs`) will be copied from\n4652             the original object to the new one.  If False (default), the new\n4653             object will be returned without attributes.\n4654         keepdims : bool, default: False\n4655             If True, the dimensions which are reduced are left in the result\n4656             as dimensions of size one. Coordinates that use these dimensions\n4657             are removed.\n4658         numeric_only : bool, optional\n4659             If True, only apply ``func`` to variables with a numeric dtype.\n4660         **kwargs : Any\n4661             Additional keyword arguments passed on to ``func``.\n4662 \n4663         Returns\n4664         -------\n4665         reduced : Dataset\n4666             Dataset with this object's DataArrays replaced with new DataArrays\n4667             of summarized data and the indicated dimension(s) removed.\n4668         \"\"\"\n4669         if dim is None or dim is ...:\n4670             dims = set(self.dims)\n4671         elif isinstance(dim, str) or not isinstance(dim, Iterable):\n4672             dims = {dim}\n4673         else:\n4674             dims = set(dim)\n4675 \n4676         missing_dimensions = [d for d in dims if d not in self.dims]\n4677         if missing_dimensions:\n4678             raise ValueError(\n4679                 \"Dataset does not contain the dimensions: %s\" % missing_dimensions\n4680             )\n4681 \n4682         if keep_attrs is None:\n4683             keep_attrs = _get_keep_attrs(default=False)\n4684 \n4685         variables: Dict[Hashable, Variable] = {}\n4686         for name, var in self._variables.items():\n4687             reduce_dims = [d for d in var.dims if d in dims]\n4688             if name in self.coords:\n4689                 if not reduce_dims:\n4690                     variables[name] = var\n4691             else:\n4692                 if (\n4693                     not numeric_only\n4694                     or np.issubdtype(var.dtype, np.number)\n4695                     or (var.dtype == np.bool_)\n4696                 ):\n4697                     if len(reduce_dims) == 1:\n4698                         # unpack dimensions for the benefit of functions\n4699                         # like np.argmin which can't handle tuple arguments\n4700                         (reduce_dims,) = reduce_dims\n4701                     elif len(reduce_dims) == var.ndim:\n4702                         # prefer to aggregate over axis=None rather than\n4703                         # axis=(0, 1) if they will be equivalent, because\n4704                         # the former is often more efficient\n4705                         reduce_dims = None  # type: ignore\n4706                     variables[name] = var.reduce(\n4707                         func,\n4708                         dim=reduce_dims,\n4709                         keep_attrs=keep_attrs,\n4710                         keepdims=keepdims,\n4711                         **kwargs,\n4712                     )\n4713 \n4714         coord_names = {k for k in self.coords if k in variables}\n4715         indexes = {k: v for k, v in self.indexes.items() if k in variables}\n4716         attrs = self.attrs if keep_attrs else None\n4717         return self._replace_with_new_dims(\n4718             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n4719         )\n4720 \n4721     def map(\n4722         self,\n4723         func: Callable,\n4724         keep_attrs: bool = None,\n4725         args: Iterable[Any] = (),\n4726         **kwargs: Any,\n4727     ) -> \"Dataset\":\n4728         \"\"\"Apply a function to each variable in this dataset\n4729 \n4730         Parameters\n4731         ----------\n4732         func : callable\n4733             Function which can be called in the form `func(x, *args, **kwargs)`\n4734             to transform each DataArray `x` in this dataset into another\n4735             DataArray.\n4736         keep_attrs : bool, optional\n4737             If True, the dataset's attributes (`attrs`) will be copied from\n4738             the original object to the new one. If False, the new object will\n4739             be returned without attributes.\n4740         args : tuple, optional\n4741             Positional arguments passed on to `func`.\n4742         **kwargs : Any\n4743             Keyword arguments passed on to `func`.\n4744 \n4745         Returns\n4746         -------\n4747         applied : Dataset\n4748             Resulting dataset from applying ``func`` to each data variable.\n4749 \n4750         Examples\n4751         --------\n4752         >>> da = xr.DataArray(np.random.randn(2, 3))\n4753         >>> ds = xr.Dataset({\"foo\": da, \"bar\": (\"x\", [-1, 2])})\n4754         >>> ds\n4755         <xarray.Dataset>\n4756         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n4757         Dimensions without coordinates: dim_0, dim_1, x\n4758         Data variables:\n4759             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n4760             bar      (x) int64 -1 2\n4761         >>> ds.map(np.fabs)\n4762         <xarray.Dataset>\n4763         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n4764         Dimensions without coordinates: dim_0, dim_1, x\n4765         Data variables:\n4766             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773\n4767             bar      (x) float64 1.0 2.0\n4768         \"\"\"\n4769         if keep_attrs is None:\n4770             keep_attrs = _get_keep_attrs(default=False)\n4771         variables = {\n4772             k: maybe_wrap_array(v, func(v, *args, **kwargs))\n4773             for k, v in self.data_vars.items()\n4774         }\n4775         if keep_attrs:\n4776             for k, v in variables.items():\n4777                 v._copy_attrs_from(self.data_vars[k])\n4778         attrs = self.attrs if keep_attrs else None\n4779         return type(self)(variables, attrs=attrs)\n4780 \n4781     def apply(\n4782         self,\n4783         func: Callable,\n4784         keep_attrs: bool = None,\n4785         args: Iterable[Any] = (),\n4786         **kwargs: Any,\n4787     ) -> \"Dataset\":\n4788         \"\"\"\n4789         Backward compatible implementation of ``map``\n4790 \n4791         See Also\n4792         --------\n4793         Dataset.map\n4794         \"\"\"\n4795         warnings.warn(\n4796             \"Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged\",\n4797             PendingDeprecationWarning,\n4798             stacklevel=2,\n4799         )\n4800         return self.map(func, keep_attrs, args, **kwargs)\n4801 \n4802     def assign(\n4803         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable\n4804     ) -> \"Dataset\":\n4805         \"\"\"Assign new data variables to a Dataset, returning a new object\n4806         with all the original variables in addition to the new ones.\n4807 \n4808         Parameters\n4809         ----------\n4810         variables : mapping of hashable to Any\n4811             Mapping from variables names to the new values. If the new values\n4812             are callable, they are computed on the Dataset and assigned to new\n4813             data variables. If the values are not callable, (e.g. a DataArray,\n4814             scalar, or array), they are simply assigned.\n4815         **variables_kwargs\n4816             The keyword arguments form of ``variables``.\n4817             One of variables or variables_kwargs must be provided.\n4818 \n4819         Returns\n4820         -------\n4821         ds : Dataset\n4822             A new Dataset with the new variables in addition to all the\n4823             existing variables.\n4824 \n4825         Notes\n4826         -----\n4827         Since ``kwargs`` is a dictionary, the order of your arguments may not\n4828         be preserved, and so the order of the new variables is not well\n4829         defined. Assigning multiple variables within the same ``assign`` is\n4830         possible, but you cannot reference other variables created within the\n4831         same ``assign`` call.\n4832 \n4833         See Also\n4834         --------\n4835         pandas.DataFrame.assign\n4836 \n4837         Examples\n4838         --------\n4839         >>> x = xr.Dataset(\n4840         ...     {\n4841         ...         \"temperature_c\": (\n4842         ...             (\"lat\", \"lon\"),\n4843         ...             20 * np.random.rand(4).reshape(2, 2),\n4844         ...         ),\n4845         ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n4846         ...     },\n4847         ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n4848         ... )\n4849         >>> x\n4850         <xarray.Dataset>\n4851         Dimensions:        (lat: 2, lon: 2)\n4852         Coordinates:\n4853           * lat            (lat) int64 10 20\n4854           * lon            (lon) int64 150 160\n4855         Data variables:\n4856             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n4857             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n4858 \n4859         Where the value is a callable, evaluated on dataset:\n4860 \n4861         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n4862         <xarray.Dataset>\n4863         Dimensions:        (lat: 2, lon: 2)\n4864         Coordinates:\n4865           * lat            (lat) int64 10 20\n4866           * lon            (lon) int64 150 160\n4867         Data variables:\n4868             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n4869             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n4870             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n4871 \n4872         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n4873 \n4874         >>> x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)\n4875         <xarray.Dataset>\n4876         Dimensions:        (lat: 2, lon: 2)\n4877         Coordinates:\n4878           * lat            (lat) int64 10 20\n4879           * lon            (lon) int64 150 160\n4880         Data variables:\n4881             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n4882             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n4883             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n4884 \n4885         \"\"\"\n4886         variables = either_dict_or_kwargs(variables, variables_kwargs, \"assign\")\n4887         data = self.copy()\n4888         # do all calculations first...\n4889         results = data._calc_assign_results(variables)\n4890         # ... and then assign\n4891         data.update(results)\n4892         return data\n4893 \n4894     def to_array(self, dim=\"variable\", name=None):\n4895         \"\"\"Convert this dataset into an xarray.DataArray\n4896 \n4897         The data variables of this dataset will be broadcast against each other\n4898         and stacked along the first axis of the new array. All coordinates of\n4899         this dataset will remain coordinates.\n4900 \n4901         Parameters\n4902         ----------\n4903         dim : str, optional\n4904             Name of the new dimension.\n4905         name : str, optional\n4906             Name of the new data array.\n4907 \n4908         Returns\n4909         -------\n4910         array : xarray.DataArray\n4911         \"\"\"\n4912         from .dataarray import DataArray\n4913 \n4914         data_vars = [self.variables[k] for k in self.data_vars]\n4915         broadcast_vars = broadcast_variables(*data_vars)\n4916         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n4917 \n4918         coords = dict(self.coords)\n4919         coords[dim] = list(self.data_vars)\n4920         indexes = propagate_indexes(self._indexes)\n4921 \n4922         dims = (dim,) + broadcast_vars[0].dims\n4923 \n4924         return DataArray(\n4925             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes\n4926         )\n4927 \n4928     def _normalize_dim_order(\n4929         self, dim_order: List[Hashable] = None\n4930     ) -> Dict[Hashable, int]:\n4931         \"\"\"\n4932         Check the validity of the provided dimensions if any and return the mapping\n4933         between dimension name and their size.\n4934 \n4935         Parameters\n4936         ----------\n4937         dim_order\n4938             Dimension order to validate (default to the alphabetical order if None).\n4939 \n4940         Returns\n4941         -------\n4942         result\n4943             Validated dimensions mapping.\n4944 \n4945         \"\"\"\n4946         if dim_order is None:\n4947             dim_order = list(self.dims)\n4948         elif set(dim_order) != set(self.dims):\n4949             raise ValueError(\n4950                 \"dim_order {} does not match the set of dimensions of this \"\n4951                 \"Dataset: {}\".format(dim_order, list(self.dims))\n4952             )\n4953 \n4954         ordered_dims = {k: self.dims[k] for k in dim_order}\n4955 \n4956         return ordered_dims\n4957 \n4958     def _to_dataframe(self, ordered_dims: Mapping[Hashable, int]):\n4959         columns = [k for k in self.variables if k not in self.dims]\n4960         data = [\n4961             self._variables[k].set_dims(ordered_dims).values.reshape(-1)\n4962             for k in columns\n4963         ]\n4964         index = self.coords.to_index([*ordered_dims])\n4965         return pd.DataFrame(dict(zip(columns, data)), index=index)\n4966 \n4967     def to_dataframe(self, dim_order: List[Hashable] = None) -> pd.DataFrame:\n4968         \"\"\"Convert this dataset into a pandas.DataFrame.\n4969 \n4970         Non-index variables in this dataset form the columns of the\n4971         DataFrame. The DataFrame is indexed by the Cartesian product of\n4972         this dataset's indices.\n4973 \n4974         Parameters\n4975         ----------\n4976         dim_order\n4977             Hierarchical dimension order for the resulting dataframe. All\n4978             arrays are transposed to this order and then written out as flat\n4979             vectors in contiguous order, so the last dimension in this list\n4980             will be contiguous in the resulting DataFrame. This has a major\n4981             influence on which operations are efficient on the resulting\n4982             dataframe.\n4983 \n4984             If provided, must include all dimensions of this dataset. By\n4985             default, dimensions are sorted alphabetically.\n4986 \n4987         Returns\n4988         -------\n4989         result\n4990             Dataset as a pandas DataFrame.\n4991 \n4992         \"\"\"\n4993 \n4994         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n4995 \n4996         return self._to_dataframe(ordered_dims=ordered_dims)\n4997 \n4998     def _set_sparse_data_from_dataframe(\n4999         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple\n5000     ) -> None:\n5001         from sparse import COO\n5002 \n5003         if isinstance(idx, pd.MultiIndex):\n5004             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)\n5005             is_sorted = idx.is_lexsorted()\n5006             shape = tuple(lev.size for lev in idx.levels)\n5007         else:\n5008             coords = np.arange(idx.size).reshape(1, -1)\n5009             is_sorted = True\n5010             shape = (idx.size,)\n5011 \n5012         for name, values in arrays:\n5013             # In virtually all real use cases, the sparse array will now have\n5014             # missing values and needs a fill_value. For consistency, don't\n5015             # special case the rare exceptions (e.g., dtype=int without a\n5016             # MultiIndex).\n5017             dtype, fill_value = dtypes.maybe_promote(values.dtype)\n5018             values = np.asarray(values, dtype=dtype)\n5019 \n5020             data = COO(\n5021                 coords,\n5022                 values,\n5023                 shape,\n5024                 has_duplicates=False,\n5025                 sorted=is_sorted,\n5026                 fill_value=fill_value,\n5027             )\n5028             self[name] = (dims, data)\n5029 \n5030     def _set_numpy_data_from_dataframe(\n5031         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple\n5032     ) -> None:\n5033         if not isinstance(idx, pd.MultiIndex):\n5034             for name, values in arrays:\n5035                 self[name] = (dims, values)\n5036             return\n5037 \n5038         # NB: similar, more general logic, now exists in\n5039         # variable.unstack_once; we could consider combining them at some\n5040         # point.\n5041 \n5042         shape = tuple(lev.size for lev in idx.levels)\n5043         indexer = tuple(idx.codes)\n5044 \n5045         # We already verified that the MultiIndex has all unique values, so\n5046         # there are missing values if and only if the size of output arrays is\n5047         # larger that the index.\n5048         missing_values = np.prod(shape) > idx.shape[0]\n5049 \n5050         for name, values in arrays:\n5051             # NumPy indexing is much faster than using DataFrame.reindex() to\n5052             # fill in missing values:\n5053             # https://stackoverflow.com/a/35049899/809705\n5054             if missing_values:\n5055                 dtype, fill_value = dtypes.maybe_promote(values.dtype)\n5056                 data = np.full(shape, fill_value, dtype)\n5057             else:\n5058                 # If there are no missing values, keep the existing dtype\n5059                 # instead of promoting to support NA, e.g., keep integer\n5060                 # columns as integers.\n5061                 # TODO: consider removing this special case, which doesn't\n5062                 # exist for sparse=True.\n5063                 data = np.zeros(shape, values.dtype)\n5064             data[indexer] = values\n5065             self[name] = (dims, data)\n5066 \n5067     @classmethod\n5068     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> \"Dataset\":\n5069         \"\"\"Convert a pandas.DataFrame into an xarray.Dataset\n5070 \n5071         Each column will be converted into an independent variable in the\n5072         Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n5073         into a tensor product of one-dimensional indices (filling in missing\n5074         values with NaN). This method will produce a Dataset very similar to\n5075         that on which the 'to_dataframe' method was called, except with\n5076         possibly redundant dimensions (since all dataset variables will have\n5077         the same dimensionality)\n5078 \n5079         Parameters\n5080         ----------\n5081         dataframe : DataFrame\n5082             DataFrame from which to copy data and indices.\n5083         sparse : bool, default: False\n5084             If true, create a sparse arrays instead of dense numpy arrays. This\n5085             can potentially save a large amount of memory if the DataFrame has\n5086             a MultiIndex. Requires the sparse package (sparse.pydata.org).\n5087 \n5088         Returns\n5089         -------\n5090         New Dataset.\n5091 \n5092         See Also\n5093         --------\n5094         xarray.DataArray.from_series\n5095         pandas.DataFrame.to_xarray\n5096         \"\"\"\n5097         # TODO: Add an option to remove dimensions along which the variables\n5098         # are constant, to enable consistent serialization to/from a dataframe,\n5099         # even if some variables have different dimensionality.\n5100 \n5101         if not dataframe.columns.is_unique:\n5102             raise ValueError(\"cannot convert DataFrame with non-unique columns\")\n5103 \n5104         idx = remove_unused_levels_categories(dataframe.index)\n5105 \n5106         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:\n5107             raise ValueError(\n5108                 \"cannot convert a DataFrame with a non-unique MultiIndex into xarray\"\n5109             )\n5110 \n5111         # Cast to a NumPy array first, in case the Series is a pandas Extension\n5112         # array (which doesn't have a valid NumPy dtype)\n5113         # TODO: allow users to control how this casting happens, e.g., by\n5114         # forwarding arguments to pandas.Series.to_numpy?\n5115         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]\n5116 \n5117         obj = cls()\n5118 \n5119         if isinstance(idx, pd.MultiIndex):\n5120             dims = tuple(\n5121                 name if name is not None else \"level_%i\" % n\n5122                 for n, name in enumerate(idx.names)\n5123             )\n5124             for dim, lev in zip(dims, idx.levels):\n5125                 obj[dim] = (dim, lev)\n5126         else:\n5127             index_name = idx.name if idx.name is not None else \"index\"\n5128             dims = (index_name,)\n5129             obj[index_name] = (dims, idx)\n5130 \n5131         if sparse:\n5132             obj._set_sparse_data_from_dataframe(idx, arrays, dims)\n5133         else:\n5134             obj._set_numpy_data_from_dataframe(idx, arrays, dims)\n5135         return obj\n5136 \n5137     def to_dask_dataframe(self, dim_order=None, set_index=False):\n5138         \"\"\"\n5139         Convert this dataset into a dask.dataframe.DataFrame.\n5140 \n5141         The dimensions, coordinates and data variables in this dataset form\n5142         the columns of the DataFrame.\n5143 \n5144         Parameters\n5145         ----------\n5146         dim_order : list, optional\n5147             Hierarchical dimension order for the resulting dataframe. All\n5148             arrays are transposed to this order and then written out as flat\n5149             vectors in contiguous order, so the last dimension in this list\n5150             will be contiguous in the resulting DataFrame. This has a major\n5151             influence on which operations are efficient on the resulting dask\n5152             dataframe.\n5153 \n5154             If provided, must include all dimensions of this dataset. By\n5155             default, dimensions are sorted alphabetically.\n5156         set_index : bool, optional\n5157             If set_index=True, the dask DataFrame is indexed by this dataset's\n5158             coordinate. Since dask DataFrames do not support multi-indexes,\n5159             set_index only works if the dataset only contains one dimension.\n5160 \n5161         Returns\n5162         -------\n5163         dask.dataframe.DataFrame\n5164         \"\"\"\n5165 \n5166         import dask.array as da\n5167         import dask.dataframe as dd\n5168 \n5169         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n5170 \n5171         columns = list(ordered_dims)\n5172         columns.extend(k for k in self.coords if k not in self.dims)\n5173         columns.extend(self.data_vars)\n5174 \n5175         series_list = []\n5176         for name in columns:\n5177             try:\n5178                 var = self.variables[name]\n5179             except KeyError:\n5180                 # dimension without a matching coordinate\n5181                 size = self.dims[name]\n5182                 data = da.arange(size, chunks=size, dtype=np.int64)\n5183                 var = Variable((name,), data)\n5184 \n5185             # IndexVariable objects have a dummy .chunk() method\n5186             if isinstance(var, IndexVariable):\n5187                 var = var.to_base_variable()\n5188 \n5189             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n5190             series = dd.from_array(dask_array.reshape(-1), columns=[name])\n5191             series_list.append(series)\n5192 \n5193         df = dd.concat(series_list, axis=1)\n5194 \n5195         if set_index:\n5196             dim_order = [*ordered_dims]\n5197 \n5198             if len(dim_order) == 1:\n5199                 (dim,) = dim_order\n5200                 df = df.set_index(dim)\n5201             else:\n5202                 # triggers an error about multi-indexes, even if only one\n5203                 # dimension is passed\n5204                 df = df.set_index(dim_order)\n5205 \n5206         return df\n5207 \n5208     def to_dict(self, data=True):\n5209         \"\"\"\n5210         Convert this dataset to a dictionary following xarray naming\n5211         conventions.\n5212 \n5213         Converts all variables and attributes to native Python objects\n5214         Useful for converting to json. To avoid datetime incompatibility\n5215         use decode_times=False kwarg in xarrray.open_dataset.\n5216 \n5217         Parameters\n5218         ----------\n5219         data : bool, optional\n5220             Whether to include the actual data in the dictionary. When set to\n5221             False, returns just the schema.\n5222 \n5223         See Also\n5224         --------\n5225         Dataset.from_dict\n5226         \"\"\"\n5227         d = {\n5228             \"coords\": {},\n5229             \"attrs\": decode_numpy_dict_values(self.attrs),\n5230             \"dims\": dict(self.dims),\n5231             \"data_vars\": {},\n5232         }\n5233         for k in self.coords:\n5234             d[\"coords\"].update({k: self[k].variable.to_dict(data=data)})\n5235         for k in self.data_vars:\n5236             d[\"data_vars\"].update({k: self[k].variable.to_dict(data=data)})\n5237         return d\n5238 \n5239     @classmethod\n5240     def from_dict(cls, d):\n5241         \"\"\"\n5242         Convert a dictionary into an xarray.Dataset.\n5243 \n5244         Input dict can take several forms:\n5245 \n5246         .. code:: python\n5247 \n5248             d = {\n5249                 \"t\": {\"dims\": (\"t\"), \"data\": t},\n5250                 \"a\": {\"dims\": (\"t\"), \"data\": x},\n5251                 \"b\": {\"dims\": (\"t\"), \"data\": y},\n5252             }\n5253 \n5254             d = {\n5255                 \"coords\": {\"t\": {\"dims\": \"t\", \"data\": t, \"attrs\": {\"units\": \"s\"}}},\n5256                 \"attrs\": {\"title\": \"air temperature\"},\n5257                 \"dims\": \"t\",\n5258                 \"data_vars\": {\n5259                     \"a\": {\"dims\": \"t\", \"data\": x},\n5260                     \"b\": {\"dims\": \"t\", \"data\": y},\n5261                 },\n5262             }\n5263 \n5264         where \"t\" is the name of the dimesion, \"a\" and \"b\" are names of data\n5265         variables and t, x, and y are lists, numpy.arrays or pandas objects.\n5266 \n5267         Parameters\n5268         ----------\n5269         d : dict-like\n5270             Mapping with a minimum structure of\n5271                 ``{\"var_0\": {\"dims\": [..], \"data\": [..]}, \\\n5272                             ...}``\n5273 \n5274         Returns\n5275         -------\n5276         obj : xarray.Dataset\n5277 \n5278         See also\n5279         --------\n5280         Dataset.to_dict\n5281         DataArray.from_dict\n5282         \"\"\"\n5283 \n5284         if not {\"coords\", \"data_vars\"}.issubset(set(d)):\n5285             variables = d.items()\n5286         else:\n5287             import itertools\n5288 \n5289             variables = itertools.chain(\n5290                 d.get(\"coords\", {}).items(), d.get(\"data_vars\", {}).items()\n5291             )\n5292         try:\n5293             variable_dict = {\n5294                 k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\")) for k, v in variables\n5295             }\n5296         except KeyError as e:\n5297             raise ValueError(\n5298                 \"cannot convert dict without the key \"\n5299                 \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n5300             )\n5301         obj = cls(variable_dict)\n5302 \n5303         # what if coords aren't dims?\n5304         coords = set(d.get(\"coords\", {})) - set(d.get(\"dims\", {}))\n5305         obj = obj.set_coords(coords)\n5306 \n5307         obj.attrs.update(d.get(\"attrs\", {}))\n5308 \n5309         return obj\n5310 \n5311     @staticmethod\n5312     def _unary_op(f):\n5313         @functools.wraps(f)\n5314         def func(self, *args, **kwargs):\n5315             variables = {}\n5316             keep_attrs = kwargs.pop(\"keep_attrs\", None)\n5317             if keep_attrs is None:\n5318                 keep_attrs = _get_keep_attrs(default=True)\n5319             for k, v in self._variables.items():\n5320                 if k in self._coord_names:\n5321                     variables[k] = v\n5322                 else:\n5323                     variables[k] = f(v, *args, **kwargs)\n5324                     if keep_attrs:\n5325                         variables[k].attrs = v._attrs\n5326             attrs = self._attrs if keep_attrs else None\n5327             return self._replace_with_new_dims(variables, attrs=attrs)\n5328 \n5329         return func\n5330 \n5331     @staticmethod\n5332     def _binary_op(f, reflexive=False, join=None):\n5333         @functools.wraps(f)\n5334         def func(self, other):\n5335             from .dataarray import DataArray\n5336 \n5337             if isinstance(other, groupby.GroupBy):\n5338                 return NotImplemented\n5339             align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n5340             if isinstance(other, (DataArray, Dataset)):\n5341                 self, other = align(self, other, join=align_type, copy=False)\n5342             g = f if not reflexive else lambda x, y: f(y, x)\n5343             ds = self._calculate_binary_op(g, other, join=align_type)\n5344             return ds\n5345 \n5346         return func\n5347 \n5348     @staticmethod\n5349     def _inplace_binary_op(f):\n5350         @functools.wraps(f)\n5351         def func(self, other):\n5352             from .dataarray import DataArray\n5353 \n5354             if isinstance(other, groupby.GroupBy):\n5355                 raise TypeError(\n5356                     \"in-place operations between a Dataset and \"\n5357                     \"a grouped object are not permitted\"\n5358                 )\n5359             # we don't actually modify arrays in-place with in-place Dataset\n5360             # arithmetic -- this lets us automatically align things\n5361             if isinstance(other, (DataArray, Dataset)):\n5362                 other = other.reindex_like(self, copy=False)\n5363             g = ops.inplace_to_noninplace_op(f)\n5364             ds = self._calculate_binary_op(g, other, inplace=True)\n5365             self._replace_with_new_dims(\n5366                 ds._variables,\n5367                 ds._coord_names,\n5368                 attrs=ds._attrs,\n5369                 indexes=ds._indexes,\n5370                 inplace=True,\n5371             )\n5372             return self\n5373 \n5374         return func\n5375 \n5376     def _calculate_binary_op(self, f, other, join=\"inner\", inplace=False):\n5377         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n5378             if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n5379                 raise ValueError(\n5380                     \"datasets must have the same data variables \"\n5381                     \"for in-place arithmetic operations: %s, %s\"\n5382                     % (list(lhs_data_vars), list(rhs_data_vars))\n5383                 )\n5384 \n5385             dest_vars = {}\n5386 \n5387             for k in lhs_data_vars:\n5388                 if k in rhs_data_vars:\n5389                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n5390                 elif join in [\"left\", \"outer\"]:\n5391                     dest_vars[k] = f(lhs_vars[k], np.nan)\n5392             for k in rhs_data_vars:\n5393                 if k not in dest_vars and join in [\"right\", \"outer\"]:\n5394                     dest_vars[k] = f(rhs_vars[k], np.nan)\n5395             return dest_vars\n5396 \n5397         if utils.is_dict_like(other) and not isinstance(other, Dataset):\n5398             # can't use our shortcut of doing the binary operation with\n5399             # Variable objects, so apply over our data vars instead.\n5400             new_data_vars = apply_over_both(\n5401                 self.data_vars, other, self.data_vars, other\n5402             )\n5403             return Dataset(new_data_vars)\n5404 \n5405         other_coords = getattr(other, \"coords\", None)\n5406         ds = self.coords.merge(other_coords)\n5407 \n5408         if isinstance(other, Dataset):\n5409             new_vars = apply_over_both(\n5410                 self.data_vars, other.data_vars, self.variables, other.variables\n5411             )\n5412         else:\n5413             other_variable = getattr(other, \"variable\", other)\n5414             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n5415         ds._variables.update(new_vars)\n5416         ds._dims = calculate_dimensions(ds._variables)\n5417         return ds\n5418 \n5419     def _copy_attrs_from(self, other):\n5420         self.attrs = other.attrs\n5421         for v in other.variables:\n5422             if v in self.variables:\n5423                 self.variables[v].attrs = other.variables[v].attrs\n5424 \n5425     def diff(self, dim, n=1, label=\"upper\"):\n5426         \"\"\"Calculate the n-th order discrete difference along given axis.\n5427 \n5428         Parameters\n5429         ----------\n5430         dim : str\n5431             Dimension over which to calculate the finite difference.\n5432         n : int, optional\n5433             The number of times values are differenced.\n5434         label : str, optional\n5435             The new coordinate in dimension ``dim`` will have the\n5436             values of either the minuend's or subtrahend's coordinate\n5437             for values 'upper' and 'lower', respectively.  Other\n5438             values are not supported.\n5439 \n5440         Returns\n5441         -------\n5442         difference : same type as caller\n5443             The n-th order finite difference of this object.\n5444         .. note::\n5445             `n` matches numpy's behavior and is different from pandas' first\n5446             argument named `periods`.\n5447 \n5448         Examples\n5449         --------\n5450         >>> ds = xr.Dataset({\"foo\": (\"x\", [5, 5, 6, 6])})\n5451         >>> ds.diff(\"x\")\n5452         <xarray.Dataset>\n5453         Dimensions:  (x: 3)\n5454         Dimensions without coordinates: x\n5455         Data variables:\n5456             foo      (x) int64 0 1 0\n5457         >>> ds.diff(\"x\", 2)\n5458         <xarray.Dataset>\n5459         Dimensions:  (x: 2)\n5460         Dimensions without coordinates: x\n5461         Data variables:\n5462             foo      (x) int64 1 -1\n5463 \n5464         See Also\n5465         --------\n5466         Dataset.differentiate\n5467         \"\"\"\n5468         if n == 0:\n5469             return self\n5470         if n < 0:\n5471             raise ValueError(f\"order `n` must be non-negative but got {n}\")\n5472 \n5473         # prepare slices\n5474         kwargs_start = {dim: slice(None, -1)}\n5475         kwargs_end = {dim: slice(1, None)}\n5476 \n5477         # prepare new coordinate\n5478         if label == \"upper\":\n5479             kwargs_new = kwargs_end\n5480         elif label == \"lower\":\n5481             kwargs_new = kwargs_start\n5482         else:\n5483             raise ValueError(\"The 'label' argument has to be either 'upper' or 'lower'\")\n5484 \n5485         variables = {}\n5486 \n5487         for name, var in self.variables.items():\n5488             if dim in var.dims:\n5489                 if name in self.data_vars:\n5490                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)\n5491                 else:\n5492                     variables[name] = var.isel(**kwargs_new)\n5493             else:\n5494                 variables[name] = var\n5495 \n5496         indexes = dict(self.indexes)\n5497         if dim in indexes:\n5498             indexes[dim] = indexes[dim][kwargs_new[dim]]\n5499 \n5500         difference = self._replace_with_new_dims(variables, indexes=indexes)\n5501 \n5502         if n > 1:\n5503             return difference.diff(dim, n - 1)\n5504         else:\n5505             return difference\n5506 \n5507     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n5508         \"\"\"Shift this dataset by an offset along one or more dimensions.\n5509 \n5510         Only data variables are moved; coordinates stay in place. This is\n5511         consistent with the behavior of ``shift`` in pandas.\n5512 \n5513         Parameters\n5514         ----------\n5515         shifts : mapping of hashable to int\n5516             Integer offset to shift along each of the given dimensions.\n5517             Positive offsets shift to the right; negative offsets shift to the\n5518             left.\n5519         fill_value : scalar or dict-like, optional\n5520             Value to use for newly missing values. If a dict-like, maps\n5521             variable names (including coordinates) to fill values.\n5522         **shifts_kwargs\n5523             The keyword arguments form of ``shifts``.\n5524             One of shifts or shifts_kwargs must be provided.\n5525 \n5526         Returns\n5527         -------\n5528         shifted : Dataset\n5529             Dataset with the same coordinates and attributes but shifted data\n5530             variables.\n5531 \n5532         See Also\n5533         --------\n5534         roll\n5535 \n5536         Examples\n5537         --------\n5538         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n5539         >>> ds.shift(x=2)\n5540         <xarray.Dataset>\n5541         Dimensions:  (x: 5)\n5542         Dimensions without coordinates: x\n5543         Data variables:\n5544             foo      (x) object nan nan 'a' 'b' 'c'\n5545         \"\"\"\n5546         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n5547         invalid = [k for k in shifts if k not in self.dims]\n5548         if invalid:\n5549             raise ValueError(\"dimensions %r do not exist\" % invalid)\n5550 \n5551         variables = {}\n5552         for name, var in self.variables.items():\n5553             if name in self.data_vars:\n5554                 fill_value_ = (\n5555                     fill_value.get(name, dtypes.NA)\n5556                     if isinstance(fill_value, dict)\n5557                     else fill_value\n5558                 )\n5559 \n5560                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}\n5561                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)\n5562             else:\n5563                 variables[name] = var\n5564 \n5565         return self._replace(variables)\n5566 \n5567     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):\n5568         \"\"\"Roll this dataset by an offset along one or more dimensions.\n5569 \n5570         Unlike shift, roll may rotate all variables, including coordinates\n5571         if specified. The direction of rotation is consistent with\n5572         :py:func:`numpy.roll`.\n5573 \n5574         Parameters\n5575         ----------\n5576         shifts : dict, optional\n5577             A dict with keys matching dimensions and values given\n5578             by integers to rotate each of the given dimensions. Positive\n5579             offsets roll to the right; negative offsets roll to the left.\n5580         roll_coords : bool\n5581             Indicates whether to  roll the coordinates by the offset\n5582             The current default of roll_coords (None, equivalent to True) is\n5583             deprecated and will change to False in a future version.\n5584             Explicitly pass roll_coords to silence the warning.\n5585         **shifts_kwargs : {dim: offset, ...}, optional\n5586             The keyword arguments form of ``shifts``.\n5587             One of shifts or shifts_kwargs must be provided.\n5588         Returns\n5589         -------\n5590         rolled : Dataset\n5591             Dataset with the same coordinates and attributes but rolled\n5592             variables.\n5593 \n5594         See Also\n5595         --------\n5596         shift\n5597 \n5598         Examples\n5599         --------\n5600         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n5601         >>> ds.roll(x=2)\n5602         <xarray.Dataset>\n5603         Dimensions:  (x: 5)\n5604         Dimensions without coordinates: x\n5605         Data variables:\n5606             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n5607         \"\"\"\n5608         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n5609         invalid = [k for k in shifts if k not in self.dims]\n5610         if invalid:\n5611             raise ValueError(\"dimensions %r do not exist\" % invalid)\n5612 \n5613         if roll_coords is None:\n5614             warnings.warn(\n5615                 \"roll_coords will be set to False in the future.\"\n5616                 \" Explicitly set roll_coords to silence warning.\",\n5617                 FutureWarning,\n5618                 stacklevel=2,\n5619             )\n5620             roll_coords = True\n5621 \n5622         unrolled_vars = () if roll_coords else self.coords\n5623 \n5624         variables = {}\n5625         for k, v in self.variables.items():\n5626             if k not in unrolled_vars:\n5627                 variables[k] = v.roll(\n5628                     **{k: s for k, s in shifts.items() if k in v.dims}\n5629                 )\n5630             else:\n5631                 variables[k] = v\n5632 \n5633         if roll_coords:\n5634             indexes = {}\n5635             for k, v in self.indexes.items():\n5636                 (dim,) = self.variables[k].dims\n5637                 if dim in shifts:\n5638                     indexes[k] = roll_index(v, shifts[dim])\n5639                 else:\n5640                     indexes[k] = v\n5641         else:\n5642             indexes = dict(self.indexes)\n5643 \n5644         return self._replace(variables, indexes=indexes)\n5645 \n5646     def sortby(self, variables, ascending=True):\n5647         \"\"\"\n5648         Sort object by labels or values (along an axis).\n5649 \n5650         Sorts the dataset, either along specified dimensions,\n5651         or according to values of 1-D dataarrays that share dimension\n5652         with calling object.\n5653 \n5654         If the input variables are dataarrays, then the dataarrays are aligned\n5655         (via left-join) to the calling object prior to sorting by cell values.\n5656         NaNs are sorted to the end, following Numpy convention.\n5657 \n5658         If multiple sorts along the same dimension is\n5659         given, numpy's lexsort is performed along that dimension:\n5660         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n5661         and the FIRST key in the sequence is used as the primary sort key,\n5662         followed by the 2nd key, etc.\n5663 \n5664         Parameters\n5665         ----------\n5666         variables : str, DataArray, or list of str or DataArray\n5667             1D DataArray objects or name(s) of 1D variable(s) in\n5668             coords/data_vars whose values are used to sort the dataset.\n5669         ascending : bool, optional\n5670             Whether to sort by ascending or descending order.\n5671 \n5672         Returns\n5673         -------\n5674         sorted : Dataset\n5675             A new dataset where all the specified dims are sorted by dim\n5676             labels.\n5677         \"\"\"\n5678         from .dataarray import DataArray\n5679 \n5680         if not isinstance(variables, list):\n5681             variables = [variables]\n5682         else:\n5683             variables = variables\n5684         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]\n5685         aligned_vars = align(self, *variables, join=\"left\")\n5686         aligned_self = aligned_vars[0]\n5687         aligned_other_vars = aligned_vars[1:]\n5688         vars_by_dim = defaultdict(list)\n5689         for data_array in aligned_other_vars:\n5690             if data_array.ndim != 1:\n5691                 raise ValueError(\"Input DataArray is not 1-D.\")\n5692             (key,) = data_array.dims\n5693             vars_by_dim[key].append(data_array)\n5694 \n5695         indices = {}\n5696         for key, arrays in vars_by_dim.items():\n5697             order = np.lexsort(tuple(reversed(arrays)))\n5698             indices[key] = order if ascending else order[::-1]\n5699         return aligned_self.isel(**indices)\n5700 \n5701     def quantile(\n5702         self,\n5703         q,\n5704         dim=None,\n5705         interpolation=\"linear\",\n5706         numeric_only=False,\n5707         keep_attrs=None,\n5708         skipna=True,\n5709     ):\n5710         \"\"\"Compute the qth quantile of the data along the specified dimension.\n5711 \n5712         Returns the qth quantiles(s) of the array elements for each variable\n5713         in the Dataset.\n5714 \n5715         Parameters\n5716         ----------\n5717         q : float or array-like of float\n5718             Quantile to compute, which must be between 0 and 1 inclusive.\n5719         dim : str or sequence of str, optional\n5720             Dimension(s) over which to apply quantile.\n5721         interpolation : {\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\"}, default: \"linear\"\n5722             This optional parameter specifies the interpolation method to\n5723             use when the desired quantile lies between two data points\n5724             ``i < j``:\n5725 \n5726                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n5727                   the fractional part of the index surrounded by ``i`` and\n5728                   ``j``.\n5729                 * lower: ``i``.\n5730                 * higher: ``j``.\n5731                 * nearest: ``i`` or ``j``, whichever is nearest.\n5732                 * midpoint: ``(i + j) / 2``.\n5733         keep_attrs : bool, optional\n5734             If True, the dataset's attributes (`attrs`) will be copied from\n5735             the original object to the new one.  If False (default), the new\n5736             object will be returned without attributes.\n5737         numeric_only : bool, optional\n5738             If True, only apply ``func`` to variables with a numeric dtype.\n5739         skipna : bool, optional\n5740             Whether to skip missing values when aggregating.\n5741 \n5742         Returns\n5743         -------\n5744         quantiles : Dataset\n5745             If `q` is a single quantile, then the result is a scalar for each\n5746             variable in data_vars. If multiple percentiles are given, first\n5747             axis of the result corresponds to the quantile and a quantile\n5748             dimension is added to the return Dataset. The other dimensions are\n5749             the dimensions that remain after the reduction of the array.\n5750 \n5751         See Also\n5752         --------\n5753         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n5754 \n5755         Examples\n5756         --------\n5757         >>> ds = xr.Dataset(\n5758         ...     {\"a\": ((\"x\", \"y\"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n5759         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n5760         ... )\n5761         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n5762         <xarray.Dataset>\n5763         Dimensions:   ()\n5764         Coordinates:\n5765             quantile  float64 0.0\n5766         Data variables:\n5767             a         float64 0.7\n5768         >>> ds.quantile(0, dim=\"x\")\n5769         <xarray.Dataset>\n5770         Dimensions:   (y: 4)\n5771         Coordinates:\n5772           * y         (y) float64 1.0 1.5 2.0 2.5\n5773             quantile  float64 0.0\n5774         Data variables:\n5775             a         (y) float64 0.7 4.2 2.6 1.5\n5776         >>> ds.quantile([0, 0.5, 1])\n5777         <xarray.Dataset>\n5778         Dimensions:   (quantile: 3)\n5779         Coordinates:\n5780           * quantile  (quantile) float64 0.0 0.5 1.0\n5781         Data variables:\n5782             a         (quantile) float64 0.7 3.4 9.4\n5783         >>> ds.quantile([0, 0.5, 1], dim=\"x\")\n5784         <xarray.Dataset>\n5785         Dimensions:   (quantile: 3, y: 4)\n5786         Coordinates:\n5787           * y         (y) float64 1.0 1.5 2.0 2.5\n5788           * quantile  (quantile) float64 0.0 0.5 1.0\n5789         Data variables:\n5790             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n5791         \"\"\"\n5792 \n5793         if isinstance(dim, str):\n5794             dims = {dim}\n5795         elif dim in [None, ...]:\n5796             dims = set(self.dims)\n5797         else:\n5798             dims = set(dim)\n5799 \n5800         _assert_empty(\n5801             [d for d in dims if d not in self.dims],\n5802             \"Dataset does not contain the dimensions: %s\",\n5803         )\n5804 \n5805         q = np.asarray(q, dtype=np.float64)\n5806 \n5807         variables = {}\n5808         for name, var in self.variables.items():\n5809             reduce_dims = [d for d in var.dims if d in dims]\n5810             if reduce_dims or not var.dims:\n5811                 if name not in self.coords:\n5812                     if (\n5813                         not numeric_only\n5814                         or np.issubdtype(var.dtype, np.number)\n5815                         or var.dtype == np.bool_\n5816                     ):\n5817                         if len(reduce_dims) == var.ndim:\n5818                             # prefer to aggregate over axis=None rather than\n5819                             # axis=(0, 1) if they will be equivalent, because\n5820                             # the former is often more efficient\n5821                             reduce_dims = None\n5822                         variables[name] = var.quantile(\n5823                             q,\n5824                             dim=reduce_dims,\n5825                             interpolation=interpolation,\n5826                             keep_attrs=keep_attrs,\n5827                             skipna=skipna,\n5828                         )\n5829 \n5830             else:\n5831                 variables[name] = var\n5832 \n5833         # construct the new dataset\n5834         coord_names = {k for k in self.coords if k in variables}\n5835         indexes = {k: v for k, v in self.indexes.items() if k in variables}\n5836         if keep_attrs is None:\n5837             keep_attrs = _get_keep_attrs(default=False)\n5838         attrs = self.attrs if keep_attrs else None\n5839         new = self._replace_with_new_dims(\n5840             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n5841         )\n5842         return new.assign_coords(quantile=q)\n5843 \n5844     def rank(self, dim, pct=False, keep_attrs=None):\n5845         \"\"\"Ranks the data.\n5846 \n5847         Equal values are assigned a rank that is the average of the ranks that\n5848         would have been otherwise assigned to all of the values within\n5849         that set.\n5850         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n5851 \n5852         NaNs in the input array are returned as NaNs.\n5853 \n5854         The `bottleneck` library is required.\n5855 \n5856         Parameters\n5857         ----------\n5858         dim : str\n5859             Dimension over which to compute rank.\n5860         pct : bool, optional\n5861             If True, compute percentage ranks, otherwise compute integer ranks.\n5862         keep_attrs : bool, optional\n5863             If True, the dataset's attributes (`attrs`) will be copied from\n5864             the original object to the new one.  If False (default), the new\n5865             object will be returned without attributes.\n5866 \n5867         Returns\n5868         -------\n5869         ranked : Dataset\n5870             Variables that do not depend on `dim` are dropped.\n5871         \"\"\"\n5872         if dim not in self.dims:\n5873             raise ValueError(\"Dataset does not contain the dimension: %s\" % dim)\n5874 \n5875         variables = {}\n5876         for name, var in self.variables.items():\n5877             if name in self.data_vars:\n5878                 if dim in var.dims:\n5879                     variables[name] = var.rank(dim, pct=pct)\n5880             else:\n5881                 variables[name] = var\n5882 \n5883         coord_names = set(self.coords)\n5884         if keep_attrs is None:\n5885             keep_attrs = _get_keep_attrs(default=False)\n5886         attrs = self.attrs if keep_attrs else None\n5887         return self._replace(variables, coord_names, attrs=attrs)\n5888 \n5889     def differentiate(self, coord, edge_order=1, datetime_unit=None):\n5890         \"\"\" Differentiate with the second order accurate central\n5891         differences.\n5892 \n5893         .. note::\n5894             This feature is limited to simple cartesian geometry, i.e. coord\n5895             must be one dimensional.\n5896 \n5897         Parameters\n5898         ----------\n5899         coord : str\n5900             The coordinate to be used to compute the gradient.\n5901         edge_order : {1, 2}, default: 1\n5902             N-th order accurate differences at the boundaries.\n5903         datetime_unit : None or {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n5904             \"us\", \"ns\", \"ps\", \"fs\", \"as\"}, default: None\n5905             Unit to compute gradient. Only valid for datetime coordinate.\n5906 \n5907         Returns\n5908         -------\n5909         differentiated: Dataset\n5910 \n5911         See also\n5912         --------\n5913         numpy.gradient: corresponding numpy function\n5914         \"\"\"\n5915         from .variable import Variable\n5916 \n5917         if coord not in self.variables and coord not in self.dims:\n5918             raise ValueError(f\"Coordinate {coord} does not exist.\")\n5919 \n5920         coord_var = self[coord].variable\n5921         if coord_var.ndim != 1:\n5922             raise ValueError(\n5923                 \"Coordinate {} must be 1 dimensional but is {}\"\n5924                 \" dimensional\".format(coord, coord_var.ndim)\n5925             )\n5926 \n5927         dim = coord_var.dims[0]\n5928         if _contains_datetime_like_objects(coord_var):\n5929             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n5930                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n5931             elif datetime_unit is None:\n5932                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n5933             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n5934 \n5935         variables = {}\n5936         for k, v in self.variables.items():\n5937             if k in self.data_vars and dim in v.dims and k not in self.coords:\n5938                 if _contains_datetime_like_objects(v):\n5939                     v = v._to_numeric(datetime_unit=datetime_unit)\n5940                 grad = duck_array_ops.gradient(\n5941                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)\n5942                 )\n5943                 variables[k] = Variable(v.dims, grad)\n5944             else:\n5945                 variables[k] = v\n5946         return self._replace(variables)\n5947 \n5948     def integrate(\n5949         self, coord: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n5950     ) -> \"Dataset\":\n5951         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n5952 \n5953         .. note::\n5954             This feature is limited to simple cartesian geometry, i.e. coord\n5955             must be one dimensional.\n5956 \n5957         Parameters\n5958         ----------\n5959         coord : hashable, or sequence of hashable\n5960             Coordinate(s) used for the integration.\n5961         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n5962                         'ps', 'fs', 'as'}, optional\n5963             Specify the unit if datetime coordinate is used.\n5964 \n5965         Returns\n5966         -------\n5967         integrated : Dataset\n5968 \n5969         See also\n5970         --------\n5971         DataArray.integrate\n5972         numpy.trapz : corresponding numpy function\n5973 \n5974         Examples\n5975         --------\n5976         >>> ds = xr.Dataset(\n5977         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n5978         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n5979         ... )\n5980         >>> ds\n5981         <xarray.Dataset>\n5982         Dimensions:  (x: 4)\n5983         Coordinates:\n5984           * x        (x) int64 0 1 2 3\n5985             y        (x) int64 1 7 3 5\n5986         Data variables:\n5987             a        (x) int64 5 5 6 6\n5988             b        (x) int64 1 2 1 0\n5989         >>> ds.integrate(\"x\")\n5990         <xarray.Dataset>\n5991         Dimensions:  ()\n5992         Data variables:\n5993             a        float64 16.5\n5994             b        float64 3.5\n5995         >>> ds.integrate(\"y\")\n5996         <xarray.Dataset>\n5997         Dimensions:  ()\n5998         Data variables:\n5999             a        float64 20.0\n6000             b        float64 4.0\n6001         \"\"\"\n6002         if not isinstance(coord, (list, tuple)):\n6003             coord = (coord,)\n6004         result = self\n6005         for c in coord:\n6006             result = result._integrate_one(c, datetime_unit=datetime_unit)\n6007         return result\n6008 \n6009     def _integrate_one(self, coord, datetime_unit=None):\n6010         from .variable import Variable\n6011 \n6012         if coord not in self.variables and coord not in self.dims:\n6013             raise ValueError(f\"Coordinate {coord} does not exist.\")\n6014 \n6015         coord_var = self[coord].variable\n6016         if coord_var.ndim != 1:\n6017             raise ValueError(\n6018                 \"Coordinate {} must be 1 dimensional but is {}\"\n6019                 \" dimensional\".format(coord, coord_var.ndim)\n6020             )\n6021 \n6022         dim = coord_var.dims[0]\n6023         if _contains_datetime_like_objects(coord_var):\n6024             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n6025                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n6026             elif datetime_unit is None:\n6027                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n6028             coord_var = coord_var._replace(\n6029                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)\n6030             )\n6031 \n6032         variables = {}\n6033         coord_names = set()\n6034         for k, v in self.variables.items():\n6035             if k in self.coords:\n6036                 if dim not in v.dims:\n6037                     variables[k] = v\n6038                     coord_names.add(k)\n6039             else:\n6040                 if k in self.data_vars and dim in v.dims:\n6041                     if _contains_datetime_like_objects(v):\n6042                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)\n6043                     integ = duck_array_ops.trapz(\n6044                         v.data, coord_var.data, axis=v.get_axis_num(dim)\n6045                     )\n6046                     v_dims = list(v.dims)\n6047                     v_dims.remove(dim)\n6048                     variables[k] = Variable(v_dims, integ)\n6049                 else:\n6050                     variables[k] = v\n6051         indexes = {k: v for k, v in self.indexes.items() if k in variables}\n6052         return self._replace_with_new_dims(\n6053             variables, coord_names=coord_names, indexes=indexes\n6054         )\n6055 \n6056     @property\n6057     def real(self):\n6058         return self.map(lambda x: x.real, keep_attrs=True)\n6059 \n6060     @property\n6061     def imag(self):\n6062         return self.map(lambda x: x.imag, keep_attrs=True)\n6063 \n6064     plot = utils.UncachedAccessor(_Dataset_PlotMethods)\n6065 \n6066     def filter_by_attrs(self, **kwargs):\n6067         \"\"\"Returns a ``Dataset`` with variables that match specific conditions.\n6068 \n6069         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n6070         containing only the variables for which all the filter tests pass.\n6071         These tests are either ``key=value`` for which the attribute ``key``\n6072         has the exact value ``value`` or the callable passed into\n6073         ``key=callable`` returns True. The callable will be passed a single\n6074         value, either the value of the attribute ``key`` or ``None`` if the\n6075         DataArray does not have an attribute with the name ``key``.\n6076 \n6077         Parameters\n6078         ----------\n6079         **kwargs\n6080             key : str\n6081                 Attribute name.\n6082             value : callable or obj\n6083                 If value is a callable, it should return a boolean in the form\n6084                 of bool = func(attr) where attr is da.attrs[key].\n6085                 Otherwise, value will be compared to the each\n6086                 DataArray's attrs[key].\n6087 \n6088         Returns\n6089         -------\n6090         new : Dataset\n6091             New dataset with variables filtered by attribute.\n6092 \n6093         Examples\n6094         --------\n6095         >>> # Create an example dataset:\n6096         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n6097         >>> precip = 10 * np.random.rand(2, 2, 3)\n6098         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n6099         >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n6100         >>> dims = [\"x\", \"y\", \"time\"]\n6101         >>> temp_attr = dict(standard_name=\"air_potential_temperature\")\n6102         >>> precip_attr = dict(standard_name=\"convective_precipitation_flux\")\n6103         >>> ds = xr.Dataset(\n6104         ...     {\n6105         ...         \"temperature\": (dims, temp, temp_attr),\n6106         ...         \"precipitation\": (dims, precip, precip_attr),\n6107         ...     },\n6108         ...     coords={\n6109         ...         \"lon\": ([\"x\", \"y\"], lon),\n6110         ...         \"lat\": ([\"x\", \"y\"], lat),\n6111         ...         \"time\": pd.date_range(\"2014-09-06\", periods=3),\n6112         ...         \"reference_time\": pd.Timestamp(\"2014-09-05\"),\n6113         ...     },\n6114         ... )\n6115         >>> # Get variables matching a specific standard_name.\n6116         >>> ds.filter_by_attrs(standard_name=\"convective_precipitation_flux\")\n6117         <xarray.Dataset>\n6118         Dimensions:         (time: 3, x: 2, y: 2)\n6119         Coordinates:\n6120             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n6121             lat             (x, y) float64 42.25 42.21 42.63 42.59\n6122           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n6123             reference_time  datetime64[ns] 2014-09-05\n6124         Dimensions without coordinates: x, y\n6125         Data variables:\n6126             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n6127         >>> # Get all variables that have a standard_name attribute.\n6128         >>> standard_name = lambda v: v is not None\n6129         >>> ds.filter_by_attrs(standard_name=standard_name)\n6130         <xarray.Dataset>\n6131         Dimensions:         (time: 3, x: 2, y: 2)\n6132         Coordinates:\n6133             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n6134             lat             (x, y) float64 42.25 42.21 42.63 42.59\n6135           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n6136             reference_time  datetime64[ns] 2014-09-05\n6137         Dimensions without coordinates: x, y\n6138         Data variables:\n6139             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n6140             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n6141 \n6142         \"\"\"\n6143         selection = []\n6144         for var_name, variable in self.variables.items():\n6145             has_value_flag = False\n6146             for attr_name, pattern in kwargs.items():\n6147                 attr_value = variable.attrs.get(attr_name)\n6148                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:\n6149                     has_value_flag = True\n6150                 else:\n6151                     has_value_flag = False\n6152                     break\n6153             if has_value_flag is True:\n6154                 selection.append(var_name)\n6155         return self[selection]\n6156 \n6157     def unify_chunks(self) -> \"Dataset\":\n6158         \"\"\"Unify chunk size along all chunked dimensions of this Dataset.\n6159 \n6160         Returns\n6161         -------\n6162         Dataset with consistent chunk sizes for all dask-array variables\n6163 \n6164         See Also\n6165         --------\n6166         dask.array.core.unify_chunks\n6167         \"\"\"\n6168 \n6169         try:\n6170             self.chunks\n6171         except ValueError:  # \"inconsistent chunks\"\n6172             pass\n6173         else:\n6174             # No variables with dask backend, or all chunks are already aligned\n6175             return self.copy()\n6176 \n6177         # import dask is placed after the quick exit test above to allow\n6178         # running this method if dask isn't installed and there are no chunks\n6179         import dask.array\n6180 \n6181         ds = self.copy()\n6182 \n6183         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}\n6184 \n6185         dask_array_names = []\n6186         dask_unify_args = []\n6187         for name, variable in ds.variables.items():\n6188             if isinstance(variable.data, dask.array.Array):\n6189                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]\n6190                 dask_array_names.append(name)\n6191                 dask_unify_args.append(variable.data)\n6192                 dask_unify_args.append(dims_tuple)\n6193 \n6194         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)\n6195 \n6196         for name, new_array in zip(dask_array_names, rechunked_arrays):\n6197             ds.variables[name]._data = new_array\n6198 \n6199         return ds\n6200 \n6201     def map_blocks(\n6202         self,\n6203         func: \"Callable[..., T_DSorDA]\",\n6204         args: Sequence[Any] = (),\n6205         kwargs: Mapping[str, Any] = None,\n6206         template: Union[\"DataArray\", \"Dataset\"] = None,\n6207     ) -> \"T_DSorDA\":\n6208         \"\"\"\n6209         Apply a function to each block of this Dataset.\n6210 \n6211         .. warning::\n6212             This method is experimental and its signature may change.\n6213 \n6214         Parameters\n6215         ----------\n6216         func : callable\n6217             User-provided function that accepts a Dataset as its first\n6218             parameter. The function will receive a subset or 'block' of this Dataset (see below),\n6219             corresponding to one chunk along each chunked dimension. ``func`` will be\n6220             executed as ``func(subset_dataset, *subset_args, **kwargs)``.\n6221 \n6222             This function must return either a single DataArray or a single Dataset.\n6223 \n6224             This function cannot add a new chunked dimension.\n6225         args : sequence\n6226             Passed to func after unpacking and subsetting any xarray objects by blocks.\n6227             xarray objects in args must be aligned with obj, otherwise an error is raised.\n6228         kwargs : mapping\n6229             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n6230             subset to blocks. Passing dask collections in kwargs is not allowed.\n6231         template : DataArray or Dataset, optional\n6232             xarray object representing the final result after compute is called. If not provided,\n6233             the function will be first run on mocked-up data, that looks like this object but\n6234             has sizes 0, to determine properties of the returned object such as dtype,\n6235             variable names, attributes, new dimensions and new indexes (if any).\n6236             ``template`` must be provided if the function changes the size of existing dimensions.\n6237             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n6238             ``attrs`` set by ``func`` will be ignored.\n6239 \n6240         Returns\n6241         -------\n6242         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n6243         function.\n6244 \n6245         Notes\n6246         -----\n6247         This function is designed for when ``func`` needs to manipulate a whole xarray object\n6248         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n6249         recommended to use ``apply_ufunc``.\n6250 \n6251         If none of the variables in this object is backed by dask arrays, calling this function is\n6252         equivalent to calling ``func(obj, *args, **kwargs)``.\n6253 \n6254         See Also\n6255         --------\n6256         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n6257         xarray.DataArray.map_blocks\n6258 \n6259         Examples\n6260         --------\n6261         Calculate an anomaly from climatology using ``.groupby()``. Using\n6262         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n6263         its indices, and its methods like ``.groupby()``.\n6264 \n6265         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n6266         ...     gb = da.groupby(groupby_type)\n6267         ...     clim = gb.mean(dim=\"time\")\n6268         ...     return gb - clim\n6269         ...\n6270         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n6271         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n6272         >>> np.random.seed(123)\n6273         >>> array = xr.DataArray(\n6274         ...     np.random.rand(len(time)),\n6275         ...     dims=[\"time\"],\n6276         ...     coords={\"time\": time, \"month\": month},\n6277         ... ).chunk()\n6278         >>> ds = xr.Dataset({\"a\": array})\n6279         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()\n6280         <xarray.Dataset>\n6281         Dimensions:  (time: 24)\n6282         Coordinates:\n6283           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n6284             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n6285         Data variables:\n6286             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901\n6287 \n6288         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n6289         to the function being applied in ``xr.map_blocks()``:\n6290 \n6291         >>> ds.map_blocks(\n6292         ...     calculate_anomaly,\n6293         ...     kwargs={\"groupby_type\": \"time.year\"},\n6294         ...     template=ds,\n6295         ... )\n6296         <xarray.Dataset>\n6297         Dimensions:  (time: 24)\n6298         Coordinates:\n6299           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n6300             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n6301         Data variables:\n6302             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>\n6303         \"\"\"\n6304         from .parallel import map_blocks\n6305 \n6306         return map_blocks(func, self, args, kwargs, template)\n6307 \n6308     def polyfit(\n6309         self,\n6310         dim: Hashable,\n6311         deg: int,\n6312         skipna: bool = None,\n6313         rcond: float = None,\n6314         w: Union[Hashable, Any] = None,\n6315         full: bool = False,\n6316         cov: Union[bool, str] = False,\n6317     ):\n6318         \"\"\"\n6319         Least squares polynomial fit.\n6320 \n6321         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n6322         invalid values when `skipna = True`.\n6323 \n6324         Parameters\n6325         ----------\n6326         dim : hashable\n6327             Coordinate along which to fit the polynomials.\n6328         deg : int\n6329             Degree of the fitting polynomial.\n6330         skipna : bool, optional\n6331             If True, removes all invalid values before fitting each 1D slices of the array.\n6332             Default is True if data is stored in a dask.array or if there is any\n6333             invalid values, False otherwise.\n6334         rcond : float, optional\n6335             Relative condition number to the fit.\n6336         w : hashable or Any, optional\n6337             Weights to apply to the y-coordinate of the sample points.\n6338             Can be an array-like object or the name of a coordinate in the dataset.\n6339         full : bool, optional\n6340             Whether to return the residuals, matrix rank and singular values in addition\n6341             to the coefficients.\n6342         cov : bool or str, optional\n6343             Whether to return to the covariance matrix in addition to the coefficients.\n6344             The matrix is not scaled if `cov='unscaled'`.\n6345 \n6346         Returns\n6347         -------\n6348         polyfit_results : Dataset\n6349             A single dataset which contains (for each \"var\" in the input dataset):\n6350 \n6351             [var]_polyfit_coefficients\n6352                 The coefficients of the best fit for each variable in this dataset.\n6353             [var]_polyfit_residuals\n6354                 The residuals of the least-square computation for each variable (only included if `full=True`)\n6355                 When the matrix rank is deficient, np.nan is returned.\n6356             [dim]_matrix_rank\n6357                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n6358                 The rank is computed ignoring the NaN values that might be skipped.\n6359             [dim]_singular_values\n6360                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n6361             [var]_polyfit_covariance\n6362                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n6363 \n6364         Warns\n6365         -----\n6366         RankWarning\n6367             The rank of the coefficient matrix in the least-squares fit is deficient.\n6368             The warning is not raised with in-memory (not dask) data and `full=True`.\n6369 \n6370         See Also\n6371         --------\n6372         numpy.polyfit\n6373         \"\"\"\n6374         variables = {}\n6375         skipna_da = skipna\n6376 \n6377         x = get_clean_interp_index(self, dim, strict=False)\n6378         xname = \"{}_\".format(self[dim].name)\n6379         order = int(deg) + 1\n6380         lhs = np.vander(x, order)\n6381 \n6382         if rcond is None:\n6383             rcond = x.shape[0] * np.core.finfo(x.dtype).eps\n6384 \n6385         # Weights:\n6386         if w is not None:\n6387             if isinstance(w, Hashable):\n6388                 w = self.coords[w]\n6389             w = np.asarray(w)\n6390             if w.ndim != 1:\n6391                 raise TypeError(\"Expected a 1-d array for weights.\")\n6392             if w.shape[0] != lhs.shape[0]:\n6393                 raise TypeError(\"Expected w and {} to have the same length\".format(dim))\n6394             lhs *= w[:, np.newaxis]\n6395 \n6396         # Scaling\n6397         scale = np.sqrt((lhs * lhs).sum(axis=0))\n6398         lhs /= scale\n6399 \n6400         degree_dim = utils.get_temp_dimname(self.dims, \"degree\")\n6401 \n6402         rank = np.linalg.matrix_rank(lhs)\n6403 \n6404         if full:\n6405             rank = xr.DataArray(rank, name=xname + \"matrix_rank\")\n6406             variables[rank.name] = rank\n6407             sing = np.linalg.svd(lhs, compute_uv=False)\n6408             sing = xr.DataArray(\n6409                 sing,\n6410                 dims=(degree_dim,),\n6411                 coords={degree_dim: np.arange(rank - 1, -1, -1)},\n6412                 name=xname + \"singular_values\",\n6413             )\n6414             variables[sing.name] = sing\n6415 \n6416         for name, da in self.data_vars.items():\n6417             if dim not in da.dims:\n6418                 continue\n6419 \n6420             if is_duck_dask_array(da.data) and (\n6421                 rank != order or full or skipna is None\n6422             ):\n6423                 # Current algorithm with dask and skipna=False neither supports\n6424                 # deficient ranks nor does it output the \"full\" info (issue dask/dask#6516)\n6425                 skipna_da = True\n6426             elif skipna is None:\n6427                 skipna_da = np.any(da.isnull())\n6428 \n6429             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]\n6430             stacked_coords: Dict[Hashable, DataArray] = {}\n6431             if dims_to_stack:\n6432                 stacked_dim = utils.get_temp_dimname(dims_to_stack, \"stacked\")\n6433                 rhs = da.transpose(dim, *dims_to_stack).stack(\n6434                     {stacked_dim: dims_to_stack}\n6435                 )\n6436                 stacked_coords = {stacked_dim: rhs[stacked_dim]}\n6437                 scale_da = scale[:, np.newaxis]\n6438             else:\n6439                 rhs = da\n6440                 scale_da = scale\n6441 \n6442             if w is not None:\n6443                 rhs *= w[:, np.newaxis]\n6444 \n6445             with warnings.catch_warnings():\n6446                 if full:  # Copy np.polyfit behavior\n6447                     warnings.simplefilter(\"ignore\", np.RankWarning)\n6448                 else:  # Raise only once per variable\n6449                     warnings.simplefilter(\"once\", np.RankWarning)\n6450 \n6451                 coeffs, residuals = duck_array_ops.least_squares(\n6452                     lhs, rhs.data, rcond=rcond, skipna=skipna_da\n6453                 )\n6454 \n6455             if isinstance(name, str):\n6456                 name = \"{}_\".format(name)\n6457             else:\n6458                 # Thus a ReprObject => polyfit was called on a DataArray\n6459                 name = \"\"\n6460 \n6461             coeffs = xr.DataArray(\n6462                 coeffs / scale_da,\n6463                 dims=[degree_dim] + list(stacked_coords.keys()),\n6464                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},\n6465                 name=name + \"polyfit_coefficients\",\n6466             )\n6467             if dims_to_stack:\n6468                 coeffs = coeffs.unstack(stacked_dim)\n6469             variables[coeffs.name] = coeffs\n6470 \n6471             if full or (cov is True):\n6472                 residuals = xr.DataArray(\n6473                     residuals if dims_to_stack else residuals.squeeze(),\n6474                     dims=list(stacked_coords.keys()),\n6475                     coords=stacked_coords,\n6476                     name=name + \"polyfit_residuals\",\n6477                 )\n6478                 if dims_to_stack:\n6479                     residuals = residuals.unstack(stacked_dim)\n6480                 variables[residuals.name] = residuals\n6481 \n6482             if cov:\n6483                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))\n6484                 Vbase /= np.outer(scale, scale)\n6485                 if cov == \"unscaled\":\n6486                     fac = 1\n6487                 else:\n6488                     if x.shape[0] <= order:\n6489                         raise ValueError(\n6490                             \"The number of data points must exceed order to scale the covariance matrix.\"\n6491                         )\n6492                     fac = residuals / (x.shape[0] - order)\n6493                 covariance = xr.DataArray(Vbase, dims=(\"cov_i\", \"cov_j\")) * fac\n6494                 variables[name + \"polyfit_covariance\"] = covariance\n6495 \n6496         return Dataset(data_vars=variables, attrs=self.attrs.copy())\n6497 \n6498     def pad(\n6499         self,\n6500         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n6501         mode: str = \"constant\",\n6502         stat_length: Union[\n6503             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n6504         ] = None,\n6505         constant_values: Union[\n6506             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n6507         ] = None,\n6508         end_values: Union[\n6509             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n6510         ] = None,\n6511         reflect_type: str = None,\n6512         **pad_width_kwargs: Any,\n6513     ) -> \"Dataset\":\n6514         \"\"\"Pad this dataset along one or more dimensions.\n6515 \n6516         .. warning::\n6517             This function is experimental and its behaviour is likely to change\n6518             especially regarding padding of dimension coordinates (or IndexVariables).\n6519 \n6520         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n6521         coordinates will be padded with the same mode, otherwise coordinates\n6522         are padded using the \"constant\" mode with fill_value dtypes.NA.\n6523 \n6524         Parameters\n6525         ----------\n6526         pad_width : mapping of hashable to tuple of int\n6527             Mapping with the form of {dim: (pad_before, pad_after)}\n6528             describing the number of values padded along each dimension.\n6529             {dim: pad} is a shortcut for pad_before = pad_after = pad\n6530         mode : str, default: \"constant\"\n6531             One of the following string values (taken from numpy docs).\n6532 \n6533             'constant' (default)\n6534                 Pads with a constant value.\n6535             'edge'\n6536                 Pads with the edge values of array.\n6537             'linear_ramp'\n6538                 Pads with the linear ramp between end_value and the\n6539                 array edge value.\n6540             'maximum'\n6541                 Pads with the maximum value of all or part of the\n6542                 vector along each axis.\n6543             'mean'\n6544                 Pads with the mean value of all or part of the\n6545                 vector along each axis.\n6546             'median'\n6547                 Pads with the median value of all or part of the\n6548                 vector along each axis.\n6549             'minimum'\n6550                 Pads with the minimum value of all or part of the\n6551                 vector along each axis.\n6552             'reflect'\n6553                 Pads with the reflection of the vector mirrored on\n6554                 the first and last values of the vector along each\n6555                 axis.\n6556             'symmetric'\n6557                 Pads with the reflection of the vector mirrored\n6558                 along the edge of the array.\n6559             'wrap'\n6560                 Pads with the wrap of the vector along the axis.\n6561                 The first values are used to pad the end and the\n6562                 end values are used to pad the beginning.\n6563         stat_length : int, tuple or mapping of hashable to tuple, default: None\n6564             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n6565             values at edge of each axis used to calculate the statistic value.\n6566             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n6567             statistic lengths along each dimension.\n6568             ((before, after),) yields same before and after statistic lengths\n6569             for each dimension.\n6570             (stat_length,) or int is a shortcut for before = after = statistic\n6571             length for all axes.\n6572             Default is ``None``, to use the entire axis.\n6573         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0\n6574             Used in 'constant'.  The values to set the padded values for each\n6575             axis.\n6576             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n6577             pad constants along each dimension.\n6578             ``((before, after),)`` yields same before and after constants for each\n6579             dimension.\n6580             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n6581             all dimensions.\n6582             Default is 0.\n6583         end_values : scalar, tuple or mapping of hashable to tuple, default: 0\n6584             Used in 'linear_ramp'.  The values used for the ending value of the\n6585             linear_ramp and that will form the edge of the padded array.\n6586             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n6587             end values along each dimension.\n6588             ``((before, after),)`` yields same before and after end values for each\n6589             axis.\n6590             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n6591             all axes.\n6592             Default is 0.\n6593         reflect_type : {\"even\", \"odd\"}, optional\n6594             Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n6595             default with an unaltered reflection around the edge value.  For\n6596             the \"odd\" style, the extended part of the array is created by\n6597             subtracting the reflected values from two times the edge value.\n6598         **pad_width_kwargs\n6599             The keyword arguments form of ``pad_width``.\n6600             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n6601 \n6602         Returns\n6603         -------\n6604         padded : Dataset\n6605             Dataset with the padded coordinates and data.\n6606 \n6607         See Also\n6608         --------\n6609         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad\n6610 \n6611         Notes\n6612         -----\n6613         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n6614         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n6615         specify ``constant_values=np.nan``\n6616 \n6617         Examples\n6618         --------\n6619         >>> ds = xr.Dataset({\"foo\": (\"x\", range(5))})\n6620         >>> ds.pad(x=(1, 2))\n6621         <xarray.Dataset>\n6622         Dimensions:  (x: 8)\n6623         Dimensions without coordinates: x\n6624         Data variables:\n6625             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan\n6626         \"\"\"\n6627         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n6628 \n6629         if mode in (\"edge\", \"reflect\", \"symmetric\", \"wrap\"):\n6630             coord_pad_mode = mode\n6631             coord_pad_options = {\n6632                 \"stat_length\": stat_length,\n6633                 \"constant_values\": constant_values,\n6634                 \"end_values\": end_values,\n6635                 \"reflect_type\": reflect_type,\n6636             }\n6637         else:\n6638             coord_pad_mode = \"constant\"\n6639             coord_pad_options = {}\n6640 \n6641         variables = {}\n6642         for name, var in self.variables.items():\n6643             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}\n6644             if not var_pad_width:\n6645                 variables[name] = var\n6646             elif name in self.data_vars:\n6647                 variables[name] = var.pad(\n6648                     pad_width=var_pad_width,\n6649                     mode=mode,\n6650                     stat_length=stat_length,\n6651                     constant_values=constant_values,\n6652                     end_values=end_values,\n6653                     reflect_type=reflect_type,\n6654                 )\n6655             else:\n6656                 variables[name] = var.pad(\n6657                     pad_width=var_pad_width,\n6658                     mode=coord_pad_mode,\n6659                     **coord_pad_options,  # type: ignore\n6660                 )\n6661 \n6662         return self._replace_vars_and_dims(variables)\n6663 \n6664     def idxmin(\n6665         self,\n6666         dim: Hashable = None,\n6667         skipna: bool = None,\n6668         fill_value: Any = dtypes.NA,\n6669         keep_attrs: bool = None,\n6670     ) -> \"Dataset\":\n6671         \"\"\"Return the coordinate label of the minimum value along a dimension.\n6672 \n6673         Returns a new `Dataset` named after the dimension with the values of\n6674         the coordinate labels along that dimension corresponding to minimum\n6675         values along that dimension.\n6676 \n6677         In comparison to :py:meth:`~Dataset.argmin`, this returns the\n6678         coordinate label while :py:meth:`~Dataset.argmin` returns the index.\n6679 \n6680         Parameters\n6681         ----------\n6682         dim : str, optional\n6683             Dimension over which to apply `idxmin`.  This is optional for 1D\n6684             variables, but required for variables with 2 or more dimensions.\n6685         skipna : bool or None, default: None\n6686             If True, skip missing values (as marked by NaN). By default, only\n6687             skips missing values for ``float``, ``complex``, and ``object``\n6688             dtypes; other dtypes either do not have a sentinel missing value\n6689             (``int``) or ``skipna=True`` has not been implemented\n6690             (``datetime64`` or ``timedelta64``).\n6691         fill_value : Any, default: NaN\n6692             Value to be filled in case all of the values along a dimension are\n6693             null.  By default this is NaN.  The fill value and result are\n6694             automatically converted to a compatible dtype if possible.\n6695             Ignored if ``skipna`` is False.\n6696         keep_attrs : bool, default: False\n6697             If True, the attributes (``attrs``) will be copied from the\n6698             original object to the new one.  If False (default), the new object\n6699             will be returned without attributes.\n6700 \n6701         Returns\n6702         -------\n6703         reduced : Dataset\n6704             New `Dataset` object with `idxmin` applied to its data and the\n6705             indicated dimension removed.\n6706 \n6707         See Also\n6708         --------\n6709         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin\n6710 \n6711         Examples\n6712         --------\n6713         >>> array1 = xr.DataArray(\n6714         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n6715         ... )\n6716         >>> array2 = xr.DataArray(\n6717         ...     [\n6718         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n6719         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n6720         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n6721         ...     ],\n6722         ...     dims=[\"y\", \"x\"],\n6723         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n6724         ... )\n6725         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n6726         >>> ds.min(dim=\"x\")\n6727         <xarray.Dataset>\n6728         Dimensions:  (y: 3)\n6729         Coordinates:\n6730           * y        (y) int64 -1 0 1\n6731         Data variables:\n6732             int      int64 -2\n6733             float    (y) float64 -2.0 -4.0 1.0\n6734         >>> ds.argmin(dim=\"x\")\n6735         <xarray.Dataset>\n6736         Dimensions:  (y: 3)\n6737         Coordinates:\n6738           * y        (y) int64 -1 0 1\n6739         Data variables:\n6740             int      int64 4\n6741             float    (y) int64 4 0 2\n6742         >>> ds.idxmin(dim=\"x\")\n6743         <xarray.Dataset>\n6744         Dimensions:  (y: 3)\n6745         Coordinates:\n6746           * y        (y) int64 -1 0 1\n6747         Data variables:\n6748             int      <U1 'e'\n6749             float    (y) object 'e' 'a' 'c'\n6750         \"\"\"\n6751         return self.map(\n6752             methodcaller(\n6753                 \"idxmin\",\n6754                 dim=dim,\n6755                 skipna=skipna,\n6756                 fill_value=fill_value,\n6757                 keep_attrs=keep_attrs,\n6758             )\n6759         )\n6760 \n6761     def idxmax(\n6762         self,\n6763         dim: Hashable = None,\n6764         skipna: bool = None,\n6765         fill_value: Any = dtypes.NA,\n6766         keep_attrs: bool = None,\n6767     ) -> \"Dataset\":\n6768         \"\"\"Return the coordinate label of the maximum value along a dimension.\n6769 \n6770         Returns a new `Dataset` named after the dimension with the values of\n6771         the coordinate labels along that dimension corresponding to maximum\n6772         values along that dimension.\n6773 \n6774         In comparison to :py:meth:`~Dataset.argmax`, this returns the\n6775         coordinate label while :py:meth:`~Dataset.argmax` returns the index.\n6776 \n6777         Parameters\n6778         ----------\n6779         dim : str, optional\n6780             Dimension over which to apply `idxmax`.  This is optional for 1D\n6781             variables, but required for variables with 2 or more dimensions.\n6782         skipna : bool or None, default: None\n6783             If True, skip missing values (as marked by NaN). By default, only\n6784             skips missing values for ``float``, ``complex``, and ``object``\n6785             dtypes; other dtypes either do not have a sentinel missing value\n6786             (``int``) or ``skipna=True`` has not been implemented\n6787             (``datetime64`` or ``timedelta64``).\n6788         fill_value : Any, default: NaN\n6789             Value to be filled in case all of the values along a dimension are\n6790             null.  By default this is NaN.  The fill value and result are\n6791             automatically converted to a compatible dtype if possible.\n6792             Ignored if ``skipna`` is False.\n6793         keep_attrs : bool, default: False\n6794             If True, the attributes (``attrs``) will be copied from the\n6795             original object to the new one.  If False (default), the new object\n6796             will be returned without attributes.\n6797 \n6798         Returns\n6799         -------\n6800         reduced : Dataset\n6801             New `Dataset` object with `idxmax` applied to its data and the\n6802             indicated dimension removed.\n6803 \n6804         See Also\n6805         --------\n6806         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax\n6807 \n6808         Examples\n6809         --------\n6810         >>> array1 = xr.DataArray(\n6811         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n6812         ... )\n6813         >>> array2 = xr.DataArray(\n6814         ...     [\n6815         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n6816         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n6817         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n6818         ...     ],\n6819         ...     dims=[\"y\", \"x\"],\n6820         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n6821         ... )\n6822         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n6823         >>> ds.max(dim=\"x\")\n6824         <xarray.Dataset>\n6825         Dimensions:  (y: 3)\n6826         Coordinates:\n6827           * y        (y) int64 -1 0 1\n6828         Data variables:\n6829             int      int64 2\n6830             float    (y) float64 2.0 2.0 1.0\n6831         >>> ds.argmax(dim=\"x\")\n6832         <xarray.Dataset>\n6833         Dimensions:  (y: 3)\n6834         Coordinates:\n6835           * y        (y) int64 -1 0 1\n6836         Data variables:\n6837             int      int64 1\n6838             float    (y) int64 0 2 2\n6839         >>> ds.idxmax(dim=\"x\")\n6840         <xarray.Dataset>\n6841         Dimensions:  (y: 3)\n6842         Coordinates:\n6843           * y        (y) int64 -1 0 1\n6844         Data variables:\n6845             int      <U1 'b'\n6846             float    (y) object 'a' 'c' 'c'\n6847         \"\"\"\n6848         return self.map(\n6849             methodcaller(\n6850                 \"idxmax\",\n6851                 dim=dim,\n6852                 skipna=skipna,\n6853                 fill_value=fill_value,\n6854                 keep_attrs=keep_attrs,\n6855             )\n6856         )\n6857 \n6858     def argmin(self, dim=None, axis=None, **kwargs):\n6859         \"\"\"Indices of the minima of the member variables.\n6860 \n6861         If there are multiple minima, the indices of the first one found will be\n6862         returned.\n6863 \n6864         Parameters\n6865         ----------\n6866         dim : str, optional\n6867             The dimension over which to find the minimum. By default, finds minimum over\n6868             all dimensions - for now returning an int for backward compatibility, but\n6869             this is deprecated, in future will be an error, since DataArray.argmin will\n6870             return a dict with indices for all dimensions, which does not make sense for\n6871             a Dataset.\n6872         axis : int, optional\n6873             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n6874             can be supplied.\n6875         keep_attrs : bool, optional\n6876             If True, the attributes (`attrs`) will be copied from the original\n6877             object to the new one.  If False (default), the new object will be\n6878             returned without attributes.\n6879         skipna : bool, optional\n6880             If True, skip missing values (as marked by NaN). By default, only\n6881             skips missing values for float dtypes; other dtypes either do not\n6882             have a sentinel missing value (int) or skipna=True has not been\n6883             implemented (object, datetime64 or timedelta64).\n6884 \n6885         Returns\n6886         -------\n6887         result : Dataset\n6888 \n6889         See Also\n6890         --------\n6891         DataArray.argmin\n6892 \n6893         \"\"\"\n6894         if dim is None and axis is None:\n6895             warnings.warn(\n6896                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() with \"\n6897                 \"neither dim nor axis argument changes to return a dict of indices of \"\n6898                 \"each dimension, for consistency it will be an error to call \"\n6899                 \"Dataset.argmin() with no argument, since we don't return a dict of \"\n6900                 \"Datasets.\",\n6901                 DeprecationWarning,\n6902                 stacklevel=2,\n6903             )\n6904         if (\n6905             dim is None\n6906             or axis is not None\n6907             or (not isinstance(dim, Sequence) and dim is not ...)\n6908             or isinstance(dim, str)\n6909         ):\n6910             # Return int index if single dimension is passed, and is not part of a\n6911             # sequence\n6912             argmin_func = getattr(duck_array_ops, \"argmin\")\n6913             return self.reduce(argmin_func, dim=dim, axis=axis, **kwargs)\n6914         else:\n6915             raise ValueError(\n6916                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n6917                 \"dicts cannot be contained in a Dataset, so cannot call \"\n6918                 \"Dataset.argmin() with a sequence or ... for dim\"\n6919             )\n6920 \n6921     def argmax(self, dim=None, axis=None, **kwargs):\n6922         \"\"\"Indices of the maxima of the member variables.\n6923 \n6924         If there are multiple maxima, the indices of the first one found will be\n6925         returned.\n6926 \n6927         Parameters\n6928         ----------\n6929         dim : str, optional\n6930             The dimension over which to find the maximum. By default, finds maximum over\n6931             all dimensions - for now returning an int for backward compatibility, but\n6932             this is deprecated, in future will be an error, since DataArray.argmax will\n6933             return a dict with indices for all dimensions, which does not make sense for\n6934             a Dataset.\n6935         axis : int, optional\n6936             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments\n6937             can be supplied.\n6938         keep_attrs : bool, optional\n6939             If True, the attributes (`attrs`) will be copied from the original\n6940             object to the new one.  If False (default), the new object will be\n6941             returned without attributes.\n6942         skipna : bool, optional\n6943             If True, skip missing values (as marked by NaN). By default, only\n6944             skips missing values for float dtypes; other dtypes either do not\n6945             have a sentinel missing value (int) or skipna=True has not been\n6946             implemented (object, datetime64 or timedelta64).\n6947 \n6948         Returns\n6949         -------\n6950         result : Dataset\n6951 \n6952         See Also\n6953         --------\n6954         DataArray.argmax\n6955 \n6956         \"\"\"\n6957         if dim is None and axis is None:\n6958             warnings.warn(\n6959                 \"Once the behaviour of DataArray.argmax() and Variable.argmax() with \"\n6960                 \"neither dim nor axis argument changes to return a dict of indices of \"\n6961                 \"each dimension, for consistency it will be an error to call \"\n6962                 \"Dataset.argmax() with no argument, since we don't return a dict of \"\n6963                 \"Datasets.\",\n6964                 DeprecationWarning,\n6965                 stacklevel=2,\n6966             )\n6967         if (\n6968             dim is None\n6969             or axis is not None\n6970             or (not isinstance(dim, Sequence) and dim is not ...)\n6971             or isinstance(dim, str)\n6972         ):\n6973             # Return int index if single dimension is passed, and is not part of a\n6974             # sequence\n6975             argmax_func = getattr(duck_array_ops, \"argmax\")\n6976             return self.reduce(argmax_func, dim=dim, axis=axis, **kwargs)\n6977         else:\n6978             raise ValueError(\n6979                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n6980                 \"dicts cannot be contained in a Dataset, so cannot call \"\n6981                 \"Dataset.argmin() with a sequence or ... for dim\"\n6982             )\n6983 \n6984 \n6985 ops.inject_all_ops_and_reduce_methods(Dataset, array_only=False)\n",
        "intended_behavior": "Assuming the merge() method in this file calls into the core merge logic and handles dataset-level merging specifics, it might need to be updated to correctly pass through the new combine_attrs option and to handle the expected behavior of dropping conflicting attributes when combine_attrs=\"drop_conflicts\" is specified. This ensures that dataset-level merge operations respect the new option, seamlessly integrating it into higher-level APIs. The existing functionality for handling other combine_attrs options must remain unaffected."
    }
]