2025-10-15 21:49:13.575 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 21:49:15.515 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=343, output_tokens=12, cost=0.003790
2025-10-15 21:49:15.516 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 21:49:15.520 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:49:21.754 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=642, output_tokens=260, cost=0.014220
2025-10-15 21:49:21.763 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:49:24.740 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=614, output_tokens=33, cost=0.007130
2025-10-15 21:49:24.741 | DEBUG    | Extracted a valid json.
2025-10-15 21:49:24.741 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"merge\")",
        "search_code(\"combine_attrs\")"
    ],
    "bug_locations": []
}
2025-10-15 21:49:24.790 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 21:49:24.791 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 21:49:42.093 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1182, output_tokens=797, cost=0.035730
2025-10-15 21:49:42.108 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:49:50.833 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2195, output_tokens=361, cost=0.032780
2025-10-15 21:49:50.841 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:49:58.714 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=715, output_tokens=307, cost=0.016360
2025-10-15 21:49:58.715 | DEBUG    | Extracted a valid json.
2025-10-15 21:49:58.715 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/merge.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file should be modified to include the new combine_attrs=\"drop_conflicts\" option. The merge function within this file currently processes combine_attrs to decide how to combine attributes from different Dataset objects being merged. The intended behavior is to extend this logic to support a new option, \"drop_conflicts\", which will combine attributes from all objects like the existing \"no_conflicts\" option but will drop any attribute where conflicting values are found instead of raising an error. This behavior will offer a more flexible approach to handling attributes during merges, fitting scenarios where users prefer to avoid errors over attribute conflicts. This adjustment should preserve all existing functionality and only extend additional behavior for managing attrs."
        },
        {
            "file": "xarray/core/dataset.py",
            "class": "",
            "method": "",
            "intended_behavior": "Assuming the merge() method in this file calls into the core merge logic and handles dataset-level merging specifics, it might need to be updated to correctly pass through the new combine_attrs option and to handle the expected behavior of dropping conflicting attributes when combine_attrs=\"drop_conflicts\" is specified. This ensures that dataset-level merge operations respect the new option, seamlessly integrating it into higher-level APIs. The existing functionality for handling other combine_attrs options must remain unaffected."
        }
    ]
}
2025-10-15 21:49:58.738 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/merge.py</file>
<class>None</class>
<method>None</method>
<code>
1 from typing import (
2     TYPE_CHECKING,
3     AbstractSet,
4     Any,
5     Dict,
6     Hashable,
7     Iterable,
8     List,
9     Mapping,
10     NamedTuple,
11     Optional,
12     Sequence,
13     Set,
14     Tuple,
15     Union,
16 )
17 
18 import pandas as pd
19 
20 from . import dtypes, pdcompat
21 from .alignment import deep_align
22 from .duck_array_ops import lazy_array_equiv
23 from .utils import Frozen, compat_dict_union, dict_equiv
24 from .variable import Variable, as_variable, assert_unique_multiindex_level_names
25 
26 if TYPE_CHECKING:
27     from .coordinates import Coordinates
28     from .dataarray import DataArray
29     from .dataset import Dataset
30 
31     DimsLike = Union[Hashable, Sequence[Hashable]]
32     ArrayLike = Any
33     VariableLike = Union[
34         ArrayLike,
35         Tuple[DimsLike, ArrayLike],
36         Tuple[DimsLike, ArrayLike, Mapping],
37         Tuple[DimsLike, ArrayLike, Mapping, Mapping],
38     ]
39     XarrayValue = Union[DataArray, Variable, VariableLike]
40     DatasetLike = Union[Dataset, Mapping[Hashable, XarrayValue]]
41     CoercibleValue = Union[XarrayValue, pd.Series, pd.DataFrame]
42     CoercibleMapping = Union[Dataset, Mapping[Hashable, CoercibleValue]]
43 
44 
45 PANDAS_TYPES = (pd.Series, pd.DataFrame, pdcompat.Panel)
46 
47 _VALID_COMPAT = Frozen(
48     {
49         "identical": 0,
50         "equals": 1,
51         "broadcast_equals": 2,
52         "minimal": 3,
53         "no_conflicts": 4,
54         "override": 5,
55     }
56 )
57 
58 
59 def broadcast_dimension_size(variables: List[Variable]) -> Dict[Hashable, int]:
60     """Extract dimension sizes from a dictionary of variables.
61 
62     Raises ValueError if any dimensions have different sizes.
63     """
64     dims: Dict[Hashable, int] = {}
65     for var in variables:
66         for dim, size in zip(var.dims, var.shape):
67             if dim in dims and size != dims[dim]:
68                 raise ValueError("index %r not aligned" % dim)
69             dims[dim] = size
70     return dims
71 
72 
73 class MergeError(ValueError):
74     """Error class for merge failures due to incompatible arguments."""
75 
76     # inherits from ValueError for backward compatibility
77     # TODO: move this to an xarray.exceptions module?
78 
79 
80 def unique_variable(
81     name: Hashable,
82     variables: List[Variable],
83     compat: str = "broadcast_equals",
84     equals: bool = None,
85 ) -> Variable:
86     """Return the unique variable from a list of variables or raise MergeError.
87 
88     Parameters
89     ----------
90     name : hashable
91         Name for this variable.
92     variables : list of Variable
93         List of Variable objects, all of which go by the same name in different
94         inputs.
95     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
96         Type of equality check to use.
97     equals : None or bool, optional
98         corresponding to result of compat test
99 
100     Returns
101     -------
102     Variable to use in the result.
103 
104     Raises
105     ------
106     MergeError: if any of the variables are not equal.
107     """
108     out = variables[0]
109 
110     if len(variables) == 1 or compat == "override":
111         return out
112 
113     combine_method = None
114 
115     if compat == "minimal":
116         compat = "broadcast_equals"
117 
118     if compat == "broadcast_equals":
119         dim_lengths = broadcast_dimension_size(variables)
120         out = out.set_dims(dim_lengths)
121 
122     if compat == "no_conflicts":
123         combine_method = "fillna"
124 
125     if equals is None:
126         # first check without comparing values i.e. no computes
127         for var in variables[1:]:
128             equals = getattr(out, compat)(var, equiv=lazy_array_equiv)
129             if equals is not True:
130                 break
131 
132         if equals is None:
133             # now compare values with minimum number of computes
134             out = out.compute()
135             for var in variables[1:]:
136                 equals = getattr(out, compat)(var)
137                 if not equals:
138                     break
139 
140     if not equals:
141         raise MergeError(
142             f"conflicting values for variable {name!r} on objects to be combined. "
143             "You can skip this check by specifying compat='override'."
144         )
145 
146     if combine_method:
147         for var in variables[1:]:
148             out = getattr(out, combine_method)(var)
149 
150     return out
151 
152 
153 def _assert_compat_valid(compat):
154     if compat not in _VALID_COMPAT:
155         raise ValueError(
156             "compat={!r} invalid: must be {}".format(compat, set(_VALID_COMPAT))
157         )
158 
159 
160 MergeElement = Tuple[Variable, Optional[pd.Index]]
161 
162 
163 def merge_collected(
164     grouped: Dict[Hashable, List[MergeElement]],
165     prioritized: Mapping[Hashable, MergeElement] = None,
166     compat: str = "minimal",
167 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
168     """Merge dicts of variables, while resolving conflicts appropriately.
169 
170     Parameters
171     ----------
172     grouped : mapping
173     prioritized : mapping
174     compat : str
175         Type of equality check to use when checking for conflicts.
176 
177     Returns
178     -------
179     Dict with keys taken by the union of keys on list_of_mappings,
180     and Variable values corresponding to those that should be found on the
181     merged result.
182     """
183     if prioritized is None:
184         prioritized = {}
185 
186     _assert_compat_valid(compat)
187 
188     merged_vars: Dict[Hashable, Variable] = {}
189     merged_indexes: Dict[Hashable, pd.Index] = {}
190 
191     for name, elements_list in grouped.items():
192         if name in prioritized:
193             variable, index = prioritized[name]
194             merged_vars[name] = variable
195             if index is not None:
196                 merged_indexes[name] = index
197         else:
198             indexed_elements = [
199                 (variable, index)
200                 for variable, index in elements_list
201                 if index is not None
202             ]
203 
204             if indexed_elements:
205                 # TODO(shoyer): consider adjusting this logic. Are we really
206                 # OK throwing away variable without an index in favor of
207                 # indexed variables, without even checking if values match?
208                 variable, index = indexed_elements[0]
209                 for _, other_index in indexed_elements[1:]:
210                     if not index.equals(other_index):
211                         raise MergeError(
212                             "conflicting values for index %r on objects to be "
213                             "combined:\nfirst value: %r\nsecond value: %r"
214                             % (name, index, other_index)
215                         )
216                 if compat == "identical":
217                     for other_variable, _ in indexed_elements[1:]:
218                         if not dict_equiv(variable.attrs, other_variable.attrs):
219                             raise MergeError(
220                                 "conflicting attribute values on combined "
221                                 "variable %r:\nfirst value: %r\nsecond value: %r"
222                                 % (name, variable.attrs, other_variable.attrs)
223                             )
224                 merged_vars[name] = variable
225                 merged_indexes[name] = index
226             else:
227                 variables = [variable for variable, _ in elements_list]
228                 try:
229                     merged_vars[name] = unique_variable(name, variables, compat)
230                 except MergeError:
231                     if compat != "minimal":
232                         # we need more than "minimal" compatibility (for which
233                         # we drop conflicting coordinates)
234                         raise
235 
236     return merged_vars, merged_indexes
237 
238 
239 def collect_variables_and_indexes(
240     list_of_mappings: "List[DatasetLike]",
241 ) -> Dict[Hashable, List[MergeElement]]:
242     """Collect variables and indexes from list of mappings of xarray objects.
243 
244     Mappings must either be Dataset objects, or have values of one of the
245     following types:
246     - an xarray.Variable
247     - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in
248       an xarray.Variable
249     - or an xarray.DataArray
250     """
251     from .dataarray import DataArray
252     from .dataset import Dataset
253 
254     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}
255 
256     def append(name, variable, index):
257         values = grouped.setdefault(name, [])
258         values.append((variable, index))
259 
260     def append_all(variables, indexes):
261         for name, variable in variables.items():
262             append(name, variable, indexes.get(name))
263 
264     for mapping in list_of_mappings:
265         if isinstance(mapping, Dataset):
266             append_all(mapping.variables, mapping.indexes)
267             continue
268 
269         for name, variable in mapping.items():
270             if isinstance(variable, DataArray):
271                 coords = variable._coords.copy()  # use private API for speed
272                 indexes = dict(variable.indexes)
273                 # explicitly overwritten variables should take precedence
274                 coords.pop(name, None)
275                 indexes.pop(name, None)
276                 append_all(coords, indexes)
277 
278             variable = as_variable(variable, name=name)
279             if variable.dims == (name,):
280                 variable = variable.to_index_variable()
281                 index = variable.to_index()
282             else:
283                 index = None
284             append(name, variable, index)
285 
286     return grouped
287 
288 
289 def collect_from_coordinates(
290     list_of_coords: "List[Coordinates]",
291 ) -> Dict[Hashable, List[MergeElement]]:
292     """Collect variables and indexes to be merged from Coordinate objects."""
293     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}
294 
295     for coords in list_of_coords:
296         variables = coords.variables
297         indexes = coords.indexes
298         for name, variable in variables.items():
299             value = grouped.setdefault(name, [])
300             value.append((variable, indexes.get(name)))
301     return grouped
302 
303 
304 def merge_coordinates_without_align(
305     objects: "List[Coordinates]",
306     prioritized: Mapping[Hashable, MergeElement] = None,
307     exclude_dims: AbstractSet = frozenset(),
308 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
309     """Merge variables/indexes from coordinates without automatic alignments.
310 
311     This function is used for merging coordinate from pre-existing xarray
312     objects.
313     """
314     collected = collect_from_coordinates(objects)
315 
316     if exclude_dims:
317         filtered: Dict[Hashable, List[MergeElement]] = {}
318         for name, elements in collected.items():
319             new_elements = [
320                 (variable, index)
321                 for variable, index in elements
322                 if exclude_dims.isdisjoint(variable.dims)
323             ]
324             if new_elements:
325                 filtered[name] = new_elements
326     else:
327         filtered = collected
328 
329     return merge_collected(filtered, prioritized)
330 
331 
332 def determine_coords(
333     list_of_mappings: Iterable["DatasetLike"],
334 ) -> Tuple[Set[Hashable], Set[Hashable]]:
335     """Given a list of dicts with xarray object values, identify coordinates.
336 
337     Parameters
338     ----------
339     list_of_mappings : list of dict or list of Dataset
340         Of the same form as the arguments to expand_variable_dicts.
341 
342     Returns
343     -------
344     coord_names : set of variable names
345     noncoord_names : set of variable names
346         All variable found in the input should appear in either the set of
347         coordinate or non-coordinate names.
348     """
349     from .dataarray import DataArray
350     from .dataset import Dataset
351 
352     coord_names: Set[Hashable] = set()
353     noncoord_names: Set[Hashable] = set()
354 
355     for mapping in list_of_mappings:
356         if isinstance(mapping, Dataset):
357             coord_names.update(mapping.coords)
358             noncoord_names.update(mapping.data_vars)
359         else:
360             for name, var in mapping.items():
361                 if isinstance(var, DataArray):
362                     coords = set(var._coords)  # use private API for speed
363                     # explicitly overwritten variables should take precedence
364                     coords.discard(name)
365                     coord_names.update(coords)
366 
367     return coord_names, noncoord_names
368 
369 
370 def coerce_pandas_values(objects: Iterable["CoercibleMapping"]) -> List["DatasetLike"]:
371     """Convert pandas values found in a list of labeled objects.
372 
373     Parameters
374     ----------
375     objects : list of Dataset or mapping
376         The mappings may contain any sort of objects coercible to
377         xarray.Variables as keys, including pandas objects.
378 
379     Returns
380     -------
381     List of Dataset or dictionary objects. Any inputs or values in the inputs
382     that were pandas objects have been converted into native xarray objects.
383     """
384     from .dataarray import DataArray
385     from .dataset import Dataset
386 
387     out = []
388     for obj in objects:
389         if isinstance(obj, Dataset):
390             variables: "DatasetLike" = obj
391         else:
392             variables = {}
393             if isinstance(obj, PANDAS_TYPES):
394                 obj = dict(obj.iteritems())
395             for k, v in obj.items():
396                 if isinstance(v, PANDAS_TYPES):
397                     v = DataArray(v)
398                 variables[k] = v
399         out.append(variables)
400     return out
401 
402 
403 def _get_priority_vars_and_indexes(
404     objects: List["DatasetLike"], priority_arg: Optional[int], compat: str = "equals"
405 ) -> Dict[Hashable, MergeElement]:
406     """Extract the priority variable from a list of mappings.
407 
408     We need this method because in some cases the priority argument itself
409     might have conflicting values (e.g., if it is a dict with two DataArray
410     values with conflicting coordinate values).
411 
412     Parameters
413     ----------
414     objects : list of dict-like of Variable
415         Dictionaries in which to find the priority variables.
416     priority_arg : int or None
417         Integer object whose variable should take priority.
418     compat : {"identical", "equals", "broadcast_equals", "no_conflicts"}, optional
419         Compatibility checks to use when merging variables.
420 
421     Returns
422     -------
423     A dictionary of variables and associated indexes (if any) to prioritize.
424     """
425     if priority_arg is None:
426         return {}
427 
428     collected = collect_variables_and_indexes([objects[priority_arg]])
429     variables, indexes = merge_collected(collected, compat=compat)
430     grouped: Dict[Hashable, MergeElement] = {}
431     for name, variable in variables.items():
432         grouped[name] = (variable, indexes.get(name))
433     return grouped
434 
435 
436 def merge_coords(
437     objects: Iterable["CoercibleMapping"],
438     compat: str = "minimal",
439     join: str = "outer",
440     priority_arg: Optional[int] = None,
441     indexes: Optional[Mapping[Hashable, pd.Index]] = None,
442     fill_value: object = dtypes.NA,
443 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
444     """Merge coordinate variables.
445 
446     See merge_core below for argument descriptions. This works similarly to
447     merge_core, except everything we don't worry about whether variables are
448     coordinates or not.
449     """
450     _assert_compat_valid(compat)
451     coerced = coerce_pandas_values(objects)
452     aligned = deep_align(
453         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value
454     )
455     collected = collect_variables_and_indexes(aligned)
456     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)
457     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)
458     assert_unique_multiindex_level_names(variables)
459     return variables, out_indexes
460 
461 
462 def merge_data_and_coords(data, coords, compat="broadcast_equals", join="outer"):
463     """Used in Dataset.__init__."""
464     objects = [data, coords]
465     explicit_coords = coords.keys()
466     indexes = dict(_extract_indexes_from_coords(coords))
467     return merge_core(
468         objects, compat, join, explicit_coords=explicit_coords, indexes=indexes
469     )
470 
471 
472 def _extract_indexes_from_coords(coords):
473     """Yields the name & index of valid indexes from a mapping of coords"""
474     for name, variable in coords.items():
475         variable = as_variable(variable, name=name)
476         if variable.dims == (name,):
477             yield name, variable.to_index()
478 
479 
480 def assert_valid_explicit_coords(variables, dims, explicit_coords):
481     """Validate explicit coordinate names/dims.
482 
483     Raise a MergeError if an explicit coord shares a name with a dimension
484     but is comprised of arbitrary dimensions.
485     """
486     for coord_name in explicit_coords:
487         if coord_name in dims and variables[coord_name].dims != (coord_name,):
488             raise MergeError(
489                 "coordinate %s shares a name with a dataset dimension, but is "
490                 "not a 1D variable along that dimension. This is disallowed "
491                 "by the xarray data model." % coord_name
492             )
493 
494 
495 def merge_attrs(variable_attrs, combine_attrs):
496     """Combine attributes from different variables according to combine_attrs"""
497     if not variable_attrs:
498         # no attributes to merge
499         return None
500 
501     if combine_attrs == "drop":
502         return {}
503     elif combine_attrs == "override":
504         return dict(variable_attrs[0])
505     elif combine_attrs == "no_conflicts":
506         result = dict(variable_attrs[0])
507         for attrs in variable_attrs[1:]:
508             try:
509                 result = compat_dict_union(result, attrs)
510             except ValueError:
511                 raise MergeError(
512                     "combine_attrs='no_conflicts', but some values are not "
513                     "the same. Merging %s with %s" % (str(result), str(attrs))
514                 )
515         return result
516     elif combine_attrs == "identical":
517         result = dict(variable_attrs[0])
518         for attrs in variable_attrs[1:]:
519             if not dict_equiv(result, attrs):
520                 raise MergeError(
521                     "combine_attrs='identical', but attrs differ. First is %s "
522                     ", other is %s." % (str(result), str(attrs))
523                 )
524         return result
525     else:
526         raise ValueError("Unrecognised value for combine_attrs=%s" % combine_attrs)
527 
528 
529 class _MergeResult(NamedTuple):
530     variables: Dict[Hashable, Variable]
531     coord_names: Set[Hashable]
532     dims: Dict[Hashable, int]
533     indexes: Dict[Hashable, pd.Index]
534     attrs: Dict[Hashable, Any]
535 
536 
537 def merge_core(
538     objects: Iterable["CoercibleMapping"],
539     compat: str = "broadcast_equals",
540     join: str = "outer",
541     combine_attrs: Optional[str] = "override",
542     priority_arg: Optional[int] = None,
543     explicit_coords: Optional[Sequence] = None,
544     indexes: Optional[Mapping[Hashable, pd.Index]] = None,
545     fill_value: object = dtypes.NA,
546 ) -> _MergeResult:
547     """Core logic for merging labeled objects.
548 
549     This is not public API.
550 
551     Parameters
552     ----------
553     objects : list of mapping
554         All values must be convertable to labeled arrays.
555     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
556         Compatibility checks to use when merging variables.
557     join : {"outer", "inner", "left", "right"}, optional
558         How to combine objects with different indexes.
559     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, optional
560         How to combine attributes of objects
561     priority_arg : int, optional
562         Optional argument in `objects` that takes precedence over the others.
563     explicit_coords : set, optional
564         An explicit list of variables from `objects` that are coordinates.
565     indexes : dict, optional
566         Dictionary with values given by pandas.Index objects.
567     fill_value : scalar, optional
568         Value to use for newly missing values
569 
570     Returns
571     -------
572     variables : dict
573         Dictionary of Variable objects.
574     coord_names : set
575         Set of coordinate names.
576     dims : dict
577         Dictionary mapping from dimension names to sizes.
578     attrs : dict
579         Dictionary of attributes
580 
581     Raises
582     ------
583     MergeError if the merge cannot be done successfully.
584     """
585     from .dataarray import DataArray
586     from .dataset import Dataset, calculate_dimensions
587 
588     _assert_compat_valid(compat)
589 
590     coerced = coerce_pandas_values(objects)
591     aligned = deep_align(
592         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value
593     )
594     collected = collect_variables_and_indexes(aligned)
595 
596     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)
597     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)
598     assert_unique_multiindex_level_names(variables)
599 
600     dims = calculate_dimensions(variables)
601 
602     coord_names, noncoord_names = determine_coords(coerced)
603     if explicit_coords is not None:
604         assert_valid_explicit_coords(variables, dims, explicit_coords)
605         coord_names.update(explicit_coords)
606     for dim, size in dims.items():
607         if dim in variables:
608             coord_names.add(dim)
609     ambiguous_coords = coord_names.intersection(noncoord_names)
610     if ambiguous_coords:
611         raise MergeError(
612             "unable to determine if these variables should be "
613             "coordinates or not in the merged result: %s" % ambiguous_coords
614         )
615 
616     attrs = merge_attrs(
617         [
618             var.attrs
619             for var in coerced
620             if isinstance(var, Dataset) or isinstance(var, DataArray)
621         ],
622         combine_attrs,
623     )
624 
625     return _MergeResult(variables, coord_names, dims, out_indexes, attrs)
626 
627 
628 def merge(
629     objects: Iterable[Union["DataArray", "CoercibleMapping"]],
630     compat: str = "no_conflicts",
631     join: str = "outer",
632     fill_value: object = dtypes.NA,
633     combine_attrs: str = "drop",
634 ) -> "Dataset":
635     """Merge any number of xarray objects into a single Dataset as variables.
636 
637     Parameters
638     ----------
639     objects : iterable of Dataset or iterable of DataArray or iterable of dict-like
640         Merge together all variables from these objects. If any of them are
641         DataArray objects, they must have a name.
642     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
643         String indicating how to compare variables of the same name for
644         potential conflicts:
645 
646         - "broadcast_equals": all values must be equal when variables are
647           broadcast against each other to ensure common dimensions.
648         - "equals": all values and dimensions must be the same.
649         - "identical": all values, dimensions and attributes must be the
650           same.
651         - "no_conflicts": only values which are not null in both datasets
652           must be equal. The returned dataset then contains the combination
653           of all non-null values.
654         - "override": skip comparing and pick variable from first dataset
655     join : {"outer", "inner", "left", "right", "exact"}, optional
656         String indicating how to combine differing indexes in objects.
657 
658         - "outer": use the union of object indexes
659         - "inner": use the intersection of object indexes
660         - "left": use indexes from the first object with each dimension
661         - "right": use indexes from the last object with each dimension
662         - "exact": instead of aligning, raise `ValueError` when indexes to be
663           aligned are not equal
664         - "override": if indexes are of same size, rewrite indexes to be
665           those of the first object with that dimension. Indexes for the same
666           dimension must have the same size in all objects.
667     fill_value : scalar or dict-like, optional
668         Value to use for newly missing values. If a dict-like, maps
669         variable names to fill values. Use a data array's name to
670         refer to its values.
671     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
672                     default: "drop"
673         String indicating how to combine attrs of the objects being merged:
674 
675         - "drop": empty attrs on returned Dataset.
676         - "identical": all attrs must be the same on every object.
677         - "no_conflicts": attrs from all objects are combined, any that have
678           the same name must also have the same value.
679         - "override": skip comparing and copy attrs from the first dataset to
680           the result.
681 
682     Returns
683     -------
684     Dataset
685         Dataset with combined variables from each object.
686 
687     Examples
688     --------
689     >>> import xarray as xr
690     >>> x = xr.DataArray(
691     ...     [[1.0, 2.0], [3.0, 5.0]],
692     ...     dims=("lat", "lon"),
693     ...     coords={"lat": [35.0, 40.0], "lon": [100.0, 120.0]},
694     ...     name="var1",
695     ... )
696     >>> y = xr.DataArray(
697     ...     [[5.0, 6.0], [7.0, 8.0]],
698     ...     dims=("lat", "lon"),
699     ...     coords={"lat": [35.0, 42.0], "lon": [100.0, 150.0]},
700     ...     name="var2",
701     ... )
702     >>> z = xr.DataArray(
703     ...     [[0.0, 3.0], [4.0, 9.0]],
704     ...     dims=("time", "lon"),
705     ...     coords={"time": [30.0, 60.0], "lon": [100.0, 150.0]},
706     ...     name="var3",
707     ... )
708 
709     >>> x
710     <xarray.DataArray 'var1' (lat: 2, lon: 2)>
711     array([[1., 2.],
712            [3., 5.]])
713     Coordinates:
714       * lat      (lat) float64 35.0 40.0
715       * lon      (lon) float64 100.0 120.0
716 
717     >>> y
718     <xarray.DataArray 'var2' (lat: 2, lon: 2)>
719     array([[5., 6.],
720            [7., 8.]])
721     Coordinates:
722       * lat      (lat) float64 35.0 42.0
723       * lon      (lon) float64 100.0 150.0
724 
725     >>> z
726     <xarray.DataArray 'var3' (time: 2, lon: 2)>
727     array([[0., 3.],
728            [4., 9.]])
729     Coordinates:
730       * time     (time) float64 30.0 60.0
731       * lon      (lon) float64 100.0 150.0
732 
733     >>> xr.merge([x, y, z])
734     <xarray.Dataset>
735     Dimensions:  (lat: 3, lon: 3, time: 2)
736     Coordinates:
737       * lat      (lat) float64 35.0 40.0 42.0
738       * lon      (lon) float64 100.0 120.0 150.0
739       * time     (time) float64 30.0 60.0
740     Data variables:
741         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
742         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
743         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
744 
745     >>> xr.merge([x, y, z], compat="identical")
746     <xarray.Dataset>
747     Dimensions:  (lat: 3, lon: 3, time: 2)
748     Coordinates:
749       * lat      (lat) float64 35.0 40.0 42.0
750       * lon      (lon) float64 100.0 120.0 150.0
751       * time     (time) float64 30.0 60.0
752     Data variables:
753         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
754         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
755         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
756 
757     >>> xr.merge([x, y, z], compat="equals")
758     <xarray.Dataset>
759     Dimensions:  (lat: 3, lon: 3, time: 2)
760     Coordinates:
761       * lat      (lat) float64 35.0 40.0 42.0
762       * lon      (lon) float64 100.0 120.0 150.0
763       * time     (time) float64 30.0 60.0
764     Data variables:
765         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
766         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
767         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
768 
769     >>> xr.merge([x, y, z], compat="equals", fill_value=-999.0)
770     <xarray.Dataset>
771     Dimensions:  (lat: 3, lon: 3, time: 2)
772     Coordinates:
773       * lat      (lat) float64 35.0 40.0 42.0
774       * lon      (lon) float64 100.0 120.0 150.0
775       * time     (time) float64 30.0 60.0
776     Data variables:
777         var1     (lat, lon) float64 1.0 2.0 -999.0 3.0 ... -999.0 -999.0 -999.0
778         var2     (lat, lon) float64 5.0 -999.0 6.0 -999.0 ... -999.0 7.0 -999.0 8.0
779         var3     (time, lon) float64 0.0 -999.0 3.0 4.0 -999.0 9.0
780 
781     >>> xr.merge([x, y, z], join="override")
782     <xarray.Dataset>
783     Dimensions:  (lat: 2, lon: 2, time: 2)
784     Coordinates:
785       * lat      (lat) float64 35.0 40.0
786       * lon      (lon) float64 100.0 120.0
787       * time     (time) float64 30.0 60.0
788     Data variables:
789         var1     (lat, lon) float64 1.0 2.0 3.0 5.0
790         var2     (lat, lon) float64 5.0 6.0 7.0 8.0
791         var3     (time, lon) float64 0.0 3.0 4.0 9.0
792 
793     >>> xr.merge([x, y, z], join="inner")
794     <xarray.Dataset>
795     Dimensions:  (lat: 1, lon: 1, time: 2)
796     Coordinates:
797       * lat      (lat) float64 35.0
798       * lon      (lon) float64 100.0
799       * time     (time) float64 30.0 60.0
800     Data variables:
801         var1     (lat, lon) float64 1.0
802         var2     (lat, lon) float64 5.0
803         var3     (time, lon) float64 0.0 4.0
804 
805     >>> xr.merge([x, y, z], compat="identical", join="inner")
806     <xarray.Dataset>
807     Dimensions:  (lat: 1, lon: 1, time: 2)
808     Coordinates:
809       * lat      (lat) float64 35.0
810       * lon      (lon) float64 100.0
811       * time     (time) float64 30.0 60.0
812     Data variables:
813         var1     (lat, lon) float64 1.0
814         var2     (lat, lon) float64 5.0
815         var3     (time, lon) float64 0.0 4.0
816 
817     >>> xr.merge([x, y, z], compat="broadcast_equals", join="outer")
818     <xarray.Dataset>
819     Dimensions:  (lat: 3, lon: 3, time: 2)
820     Coordinates:
821       * lat      (lat) float64 35.0 40.0 42.0
822       * lon      (lon) float64 100.0 120.0 150.0
823       * time     (time) float64 30.0 60.0
824     Data variables:
825         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
826         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
827         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
828 
829     >>> xr.merge([x, y, z], join="exact")
830     Traceback (most recent call last):
831     ...
832     ValueError: indexes along dimension 'lat' are not equal
833 
834     Raises
835     ------
836     xarray.MergeError
837         If any variables with the same name have conflicting values.
838 
839     See also
840     --------
841     concat
842     """
843     from .dataarray import DataArray
844     from .dataset import Dataset
845 
846     dict_like_objects = []
847     for obj in objects:
848         if not isinstance(obj, (DataArray, Dataset, dict)):
849             raise TypeError(
850                 "objects must be an iterable containing only "
851                 "Dataset(s), DataArray(s), and dictionaries."
852             )
853 
854         obj = obj.to_dataset(promote_attrs=True) if isinstance(obj, DataArray) else obj
855         dict_like_objects.append(obj)
856 
857     merge_result = merge_core(
858         dict_like_objects,
859         compat,
860         join,
861         combine_attrs=combine_attrs,
862         fill_value=fill_value,
863     )
864     merged = Dataset._construct_direct(**merge_result._asdict())
865     return merged
866 
867 
868 def dataset_merge_method(
869     dataset: "Dataset",
870     other: "CoercibleMapping",
871     overwrite_vars: Union[Hashable, Iterable[Hashable]],
872     compat: str,
873     join: str,
874     fill_value: Any,
875 ) -> _MergeResult:
876     """Guts of the Dataset.merge method."""
877     # we are locked into supporting overwrite_vars for the Dataset.merge
878     # method due for backwards compatibility
879     # TODO: consider deprecating it?
880 
881     if isinstance(overwrite_vars, Iterable) and not isinstance(overwrite_vars, str):
882         overwrite_vars = set(overwrite_vars)
883     else:
884         overwrite_vars = {overwrite_vars}
885 
886     if not overwrite_vars:
887         objs = [dataset, other]
888         priority_arg = None
889     elif overwrite_vars == set(other):
890         objs = [dataset, other]
891         priority_arg = 1
892     else:
893         other_overwrite: Dict[Hashable, CoercibleValue] = {}
894         other_no_overwrite: Dict[Hashable, CoercibleValue] = {}
895         for k, v in other.items():
896             if k in overwrite_vars:
897                 other_overwrite[k] = v
898             else:
899                 other_no_overwrite[k] = v
900         objs = [dataset, other_no_overwrite, other_overwrite]
901         priority_arg = 2
902 
903     return merge_core(
904         objs, compat, join, priority_arg=priority_arg, fill_value=fill_value
905     )
906 
907 
908 def dataset_update_method(
909     dataset: "Dataset", other: "CoercibleMapping"
910 ) -> _MergeResult:
911     """Guts of the Dataset.update method.
912 
913     This drops a duplicated coordinates from `other` if `other` is not an
914     `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,
915     GH2180).
916     """
917     from .dataarray import DataArray
918     from .dataset import Dataset
919 
920     if not isinstance(other, Dataset):
921         other = dict(other)
922         for key, value in other.items():
923             if isinstance(value, DataArray):
924                 # drop conflicting coordinates
925                 coord_names = [
926                     c
927                     for c in value.coords
928                     if c not in value.dims and c in dataset.coords
929                 ]
930                 if coord_names:
931                     other[key] = value.drop_vars(coord_names)
932 
933     # use ds.coords and not ds.indexes, else str coords are cast to object
934     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}
935     return merge_core(
936         [dataset, other],
937         priority_arg=1,
938         indexes=indexes,
939         combine_attrs="override",
940     )

</code><intended_behavior>This file should be modified to include the new combine_attrs="drop_conflicts" option. The merge function within this file currently processes combine_attrs to decide how to combine attributes from different Dataset objects being merged. The intended behavior is to extend this logic to support a new option, "drop_conflicts", which will combine attributes from all objects like the existing "no_conflicts" option but will drop any attribute where conflicting values are found instead of raising an error. This behavior will offer a more flexible approach to handling attributes during merges, fitting scenarios where users prefer to avoid errors over attribute conflicts. This adjustment should preserve all existing functionality and only extend additional behavior for managing attrs.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>None</class>
<method>None</method>
<code>
1 import copy
2 import datetime
3 import functools
4 import sys
5 import warnings
6 from collections import defaultdict
7 from distutils.version import LooseVersion
8 from html import escape
9 from numbers import Number
10 from operator import methodcaller
11 from pathlib import Path
12 from typing import (
13     TYPE_CHECKING,
14     Any,
15     Callable,
16     DefaultDict,
17     Dict,
18     Hashable,
19     Iterable,
20     Iterator,
21     List,
22     Mapping,
23     MutableMapping,
24     Optional,
25     Sequence,
26     Set,
27     Tuple,
28     TypeVar,
29     Union,
30     cast,
31     overload,
32 )
33 
34 import numpy as np
35 import pandas as pd
36 
37 import xarray as xr
38 
39 from ..coding.cftimeindex import _parse_array_of_cftime_strings
40 from ..plot.dataset_plot import _Dataset_PlotMethods
41 from . import (
42     alignment,
43     dtypes,
44     duck_array_ops,
45     formatting,
46     formatting_html,
47     groupby,
48     ops,
49     resample,
50     rolling,
51     utils,
52     weighted,
53 )
54 from .alignment import _broadcast_helper, _get_broadcast_dims_map_common_coords, align
55 from .common import (
56     DataWithCoords,
57     ImplementsDatasetReduce,
58     _contains_datetime_like_objects,
59 )
60 from .coordinates import (
61     DatasetCoordinates,
62     assert_coordinate_consistent,
63     remap_label_indexers,
64 )
65 from .duck_array_ops import datetime_to_numeric
66 from .indexes import (
67     Indexes,
68     default_indexes,
69     isel_variable_and_index,
70     propagate_indexes,
71     remove_unused_levels_categories,
72     roll_index,
73 )
74 from .indexing import is_fancy_indexer
75 from .merge import (
76     dataset_merge_method,
77     dataset_update_method,
78     merge_coordinates_without_align,
79     merge_data_and_coords,
80 )
81 from .missing import get_clean_interp_index
82 from .options import OPTIONS, _get_keep_attrs
83 from .pycompat import is_duck_dask_array, sparse_array_type
84 from .utils import (
85     Default,
86     Frozen,
87     HybridMappingProxy,
88     SortedKeysDict,
89     _default,
90     decode_numpy_dict_values,
91     drop_dims_from_indexers,
92     either_dict_or_kwargs,
93     hashable,
94     infix_dims,
95     is_dict_like,
96     is_scalar,
97     maybe_wrap_array,
98 )
99 from .variable import (
100     IndexVariable,
101     Variable,
102     as_variable,
103     assert_unique_multiindex_level_names,
104     broadcast_variables,
105 )
106 
107 if TYPE_CHECKING:
108     from ..backends import AbstractDataStore, ZarrStore
109     from .dataarray import DataArray
110     from .merge import CoercibleMapping
111 
112     T_DSorDA = TypeVar("T_DSorDA", DataArray, "Dataset")
113 
114     try:
115         from dask.delayed import Delayed
116     except ImportError:
117         Delayed = None
118 
119 
120 # list of attributes of pd.DatetimeIndex that are ndarrays of time info
121 _DATETIMEINDEX_COMPONENTS = [
122     "year",
123     "month",
124     "day",
125     "hour",
126     "minute",
127     "second",
128     "microsecond",
129     "nanosecond",
130     "date",
131     "time",
132     "dayofyear",
133     "weekofyear",
134     "dayofweek",
135     "quarter",
136 ]
137 
138 
139 def _get_virtual_variable(
140     variables, key: Hashable, level_vars: Mapping = None, dim_sizes: Mapping = None
141 ) -> Tuple[Hashable, Hashable, Variable]:
142     """Get a virtual variable (e.g., 'time.year' or a MultiIndex level)
143     from a dict of xarray.Variable objects (if possible)
144     """
145     if level_vars is None:
146         level_vars = {}
147     if dim_sizes is None:
148         dim_sizes = {}
149 
150     if key in dim_sizes:
151         data = pd.Index(range(dim_sizes[key]), name=key)
152         variable = IndexVariable((key,), data)
153         return key, key, variable
154 
155     if not isinstance(key, str):
156         raise KeyError(key)
157 
158     split_key = key.split(".", 1)
159     var_name: Optional[str]
160     if len(split_key) == 2:
161         ref_name, var_name = split_key
162     elif len(split_key) == 1:
163         ref_name, var_name = key, None
164     else:
165         raise KeyError(key)
166 
167     if ref_name in level_vars:
168         dim_var = variables[level_vars[ref_name]]
169         ref_var = dim_var.to_index_variable().get_level_variable(ref_name)
170     else:
171         ref_var = variables[ref_name]
172 
173     if var_name is None:
174         virtual_var = ref_var
175         var_name = key
176     else:
177         if _contains_datetime_like_objects(ref_var):
178             ref_var = xr.DataArray(ref_var)
179             data = getattr(ref_var.dt, var_name).data
180         else:
181             data = getattr(ref_var, var_name).data
182         virtual_var = Variable(ref_var.dims, data)
183 
184     return ref_name, var_name, virtual_var
185 
186 
187 def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> Dict[Hashable, int]:
188     """Calculate the dimensions corresponding to a set of variables.
189 
190     Returns dictionary mapping from dimension names to sizes. Raises ValueError
191     if any of the dimension sizes conflict.
192     """
193     dims: Dict[Hashable, int] = {}
194     last_used = {}
195     scalar_vars = {k for k, v in variables.items() if not v.dims}
196     for k, var in variables.items():
197         for dim, size in zip(var.dims, var.shape):
198             if dim in scalar_vars:
199                 raise ValueError(
200                     "dimension %r already exists as a scalar variable" % dim
201                 )
202             if dim not in dims:
203                 dims[dim] = size
204                 last_used[dim] = k
205             elif dims[dim] != size:
206                 raise ValueError(
207                     "conflicting sizes for dimension %r: "
208                     "length %s on %r and length %s on %r"
209                     % (dim, size, k, dims[dim], last_used[dim])
210                 )
211     return dims
212 
213 
214 def merge_indexes(
215     indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]],
216     variables: Mapping[Hashable, Variable],
217     coord_names: Set[Hashable],
218     append: bool = False,
219 ) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:
220     """Merge variables into multi-indexes.
221 
222     Not public API. Used in Dataset and DataArray set_index
223     methods.
224     """
225     vars_to_replace: Dict[Hashable, Variable] = {}
226     vars_to_remove: List[Hashable] = []
227     dims_to_replace: Dict[Hashable, Hashable] = {}
228     error_msg = "{} is not the name of an existing variable."
229 
230     for dim, var_names in indexes.items():
231         if isinstance(var_names, str) or not isinstance(var_names, Sequence):
232             var_names = [var_names]
233 
234         names: List[Hashable] = []
235         codes: List[List[int]] = []
236         levels: List[List[int]] = []
237         current_index_variable = variables.get(dim)
238 
239         for n in var_names:
240             try:
241                 var = variables[n]
242             except KeyError:
243                 raise ValueError(error_msg.format(n))
244             if (
245                 current_index_variable is not None
246                 and var.dims != current_index_variable.dims
247             ):
248                 raise ValueError(
249                     "dimension mismatch between %r %s and %r %s"
250                     % (dim, current_index_variable.dims, n, var.dims)
251                 )
252 
253         if current_index_variable is not None and append:
254             current_index = current_index_variable.to_index()
255             if isinstance(current_index, pd.MultiIndex):
256                 names.extend(current_index.names)
257                 codes.extend(current_index.codes)
258                 levels.extend(current_index.levels)
259             else:
260                 names.append("%s_level_0" % dim)
261                 cat = pd.Categorical(current_index.values, ordered=True)
262                 codes.append(cat.codes)
263                 levels.append(cat.categories)
264 
265         if not len(names) and len(var_names) == 1:
266             idx = pd.Index(variables[var_names[0]].values)
267 
268         else:  # MultiIndex
269             for n in var_names:
270                 try:
271                     var = variables[n]
272                 except KeyError:
273                     raise ValueError(error_msg.format(n))
274                 names.append(n)
275                 cat = pd.Categorical(var.values, ordered=True)
276                 codes.append(cat.codes)
277                 levels.append(cat.categories)
278 
279             idx = pd.MultiIndex(levels, codes, names=names)
280             for n in names:
281                 dims_to_replace[n] = dim
282 
283         vars_to_replace[dim] = IndexVariable(dim, idx)
284         vars_to_remove.extend(var_names)
285 
286     new_variables = {k: v for k, v in variables.items() if k not in vars_to_remove}
287     new_variables.update(vars_to_replace)
288 
289     # update dimensions if necessary, GH: 3512
290     for k, v in new_variables.items():
291         if any(d in dims_to_replace for d in v.dims):
292             new_dims = [dims_to_replace.get(d, d) for d in v.dims]
293             new_variables[k] = v._replace(dims=new_dims)
294     new_coord_names = coord_names | set(vars_to_replace)
295     new_coord_names -= set(vars_to_remove)
296     return new_variables, new_coord_names
297 
298 
299 def split_indexes(
300     dims_or_levels: Union[Hashable, Sequence[Hashable]],
301     variables: Mapping[Hashable, Variable],
302     coord_names: Set[Hashable],
303     level_coords: Mapping[Hashable, Hashable],
304     drop: bool = False,
305 ) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:
306     """Extract (multi-)indexes (levels) as variables.
307 
308     Not public API. Used in Dataset and DataArray reset_index
309     methods.
310     """
311     if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
312         dims_or_levels = [dims_or_levels]
313 
314     dim_levels: DefaultDict[Any, List[Hashable]] = defaultdict(list)
315     dims = []
316     for k in dims_or_levels:
317         if k in level_coords:
318             dim_levels[level_coords[k]].append(k)
319         else:
320             dims.append(k)
321 
322     vars_to_replace = {}
323     vars_to_create: Dict[Hashable, Variable] = {}
324     vars_to_remove = []
325 
326     for d in dims:
327         index = variables[d].to_index()
328         if isinstance(index, pd.MultiIndex):
329             dim_levels[d] = index.names
330         else:
331             vars_to_remove.append(d)
332             if not drop:
333                 vars_to_create[str(d) + "_"] = Variable(d, index, variables[d].attrs)
334 
335     for d, levs in dim_levels.items():
336         index = variables[d].to_index()
337         if len(levs) == index.nlevels:
338             vars_to_remove.append(d)
339         else:
340             vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))
341 
342         if not drop:
343             for lev in levs:
344                 idx = index.get_level_values(lev)
345                 vars_to_create[idx.name] = Variable(d, idx, variables[d].attrs)
346 
347     new_variables = dict(variables)
348     for v in set(vars_to_remove):
349         del new_variables[v]
350     new_variables.update(vars_to_replace)
351     new_variables.update(vars_to_create)
352     new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)
353 
354     return new_variables, new_coord_names
355 
356 
357 def _assert_empty(args: tuple, msg: str = "%s") -> None:
358     if args:
359         raise ValueError(msg % args)
360 
361 
362 def _check_chunks_compatibility(var, chunks, preferred_chunks):
363     for dim in var.dims:
364         if dim not in chunks or (dim not in preferred_chunks):
365             continue
366 
367         preferred_chunks_dim = preferred_chunks.get(dim)
368         chunks_dim = chunks.get(dim)
369 
370         if isinstance(chunks_dim, int):
371             chunks_dim = (chunks_dim,)
372         else:
373             chunks_dim = chunks_dim[:-1]
374 
375         if any(s % preferred_chunks_dim for s in chunks_dim):
376             warnings.warn(
377                 f"Specified Dask chunks {chunks[dim]} would separate "
378                 f"on disks chunk shape {preferred_chunks[dim]} for dimension {dim}. "
379                 "This could degrade performance. "
380                 "Consider rechunking after loading instead.",
381                 stacklevel=2,
382             )
383 
384 
385 def _get_chunk(var, chunks):
386     # chunks need to be explicity computed to take correctly into accout
387     # backend preferred chunking
388     import dask.array as da
389 
390     if isinstance(var, IndexVariable):
391         return {}
392 
393     if isinstance(chunks, int) or (chunks == "auto"):
394         chunks = dict.fromkeys(var.dims, chunks)
395 
396     preferred_chunks = var.encoding.get("preferred_chunks", {})
397     preferred_chunks_list = [
398         preferred_chunks.get(dim, shape) for dim, shape in zip(var.dims, var.shape)
399     ]
400 
401     chunks_list = [
402         chunks.get(dim, None) or preferred_chunks.get(dim, None) for dim in var.dims
403     ]
404 
405     output_chunks_list = da.core.normalize_chunks(
406         chunks_list,
407         shape=var.shape,
408         dtype=var.dtype,
409         previous_chunks=preferred_chunks_list,
410     )
411 
412     output_chunks = dict(zip(var.dims, output_chunks_list))
413     _check_chunks_compatibility(var, output_chunks, preferred_chunks)
414 
415     return output_chunks
416 
417 
418 def _maybe_chunk(
419     name,
420     var,
421     chunks,
422     token=None,
423     lock=None,
424     name_prefix="xarray-",
425     overwrite_encoded_chunks=False,
426 ):
427     from dask.base import tokenize
428 
429     if chunks is not None:
430         chunks = {dim: chunks[dim] for dim in var.dims if dim in chunks}
431     if var.ndim:
432         # when rechunking by different amounts, make sure dask names change
433         # by provinding chunks as an input to tokenize.
434         # subtle bugs result otherwise. see GH3350
435         token2 = tokenize(name, token if token else var._data, chunks)
436         name2 = f"{name_prefix}{name}-{token2}"
437         var = var.chunk(chunks, name=name2, lock=lock)
438 
439         if overwrite_encoded_chunks and var.chunks is not None:
440             var.encoding["chunks"] = tuple(x[0] for x in var.chunks)
441         return var
442     else:
443         return var
444 
445 
446 def as_dataset(obj: Any) -> "Dataset":
447     """Cast the given object to a Dataset.
448 
449     Handles Datasets, DataArrays and dictionaries of variables. A new Dataset
450     object is only created if the provided object is not already one.
451     """
452     if hasattr(obj, "to_dataset"):
453         obj = obj.to_dataset()
454     if not isinstance(obj, Dataset):
455         obj = Dataset(obj)
456     return obj
457 
458 
459 class DataVariables(Mapping[Hashable, "DataArray"]):
460     __slots__ = ("_dataset",)
461 
462     def __init__(self, dataset: "Dataset"):
463         self._dataset = dataset
464 
465     def __iter__(self) -> Iterator[Hashable]:
466         return (
467             key
468             for key in self._dataset._variables
469             if key not in self._dataset._coord_names
470         )
471 
472     def __len__(self) -> int:
473         return len(self._dataset._variables) - len(self._dataset._coord_names)
474 
475     def __contains__(self, key: Hashable) -> bool:
476         return key in self._dataset._variables and key not in self._dataset._coord_names
477 
478     def __getitem__(self, key: Hashable) -> "DataArray":
479         if key not in self._dataset._coord_names:
480             return cast("DataArray", self._dataset[key])
481         raise KeyError(key)
482 
483     def __repr__(self) -> str:
484         return formatting.data_vars_repr(self)
485 
486     @property
487     def variables(self) -> Mapping[Hashable, Variable]:
488         all_variables = self._dataset.variables
489         return Frozen({k: all_variables[k] for k in self})
490 
491     def _ipython_key_completions_(self):
492         """Provide method for the key-autocompletions in IPython. """
493         return [
494             key
495             for key in self._dataset._ipython_key_completions_()
496             if key not in self._dataset._coord_names
497         ]
498 
499 
500 class _LocIndexer:
501     __slots__ = ("dataset",)
502 
503     def __init__(self, dataset: "Dataset"):
504         self.dataset = dataset
505 
506     def __getitem__(self, key: Mapping[Hashable, Any]) -> "Dataset":
507         if not utils.is_dict_like(key):
508             raise TypeError("can only lookup dictionaries from Dataset.loc")
509         return self.dataset.sel(key)
510 
511 
512 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
513     """A multi-dimensional, in memory, array database.
514 
515     A dataset resembles an in-memory representation of a NetCDF file,
516     and consists of variables, coordinates and attributes which
517     together form a self describing dataset.
518 
519     Dataset implements the mapping interface with keys given by variable
520     names and values given by DataArray objects for each variable name.
521 
522     One dimensional variables with name equal to their dimension are
523     index coordinates used for label based indexing.
524 
525     To load data from a file or file-like object, use the `open_dataset`
526     function.
527 
528     Parameters
529     ----------
530     data_vars : dict-like, optional
531         A mapping from variable names to :py:class:`~xarray.DataArray`
532         objects, :py:class:`~xarray.Variable` objects or to tuples of
533         the form ``(dims, data[, attrs])`` which can be used as
534         arguments to create a new ``Variable``. Each dimension must
535         have the same length in all variables in which it appears.
536 
537         The following notations are accepted:
538 
539         - mapping {var name: DataArray}
540         - mapping {var name: Variable}
541         - mapping {var name: (dimension name, array-like)}
542         - mapping {var name: (tuple of dimension names, array-like)}
543         - mapping {dimension name: array-like}
544           (it will be automatically moved to coords, see below)
545 
546         Each dimension must have the same length in all variables in
547         which it appears.
548     coords : dict-like, optional
549         Another mapping in similar form as the `data_vars` argument,
550         except the each item is saved on the dataset as a "coordinate".
551         These variables have an associated meaning: they describe
552         constant/fixed/independent quantities, unlike the
553         varying/measured/dependent quantities that belong in
554         `variables`. Coordinates values may be given by 1-dimensional
555         arrays or scalars, in which case `dims` do not need to be
556         supplied: 1D arrays will be assumed to give index values along
557         the dimension with the same name.
558 
559         The following notations are accepted:
560 
561         - mapping {coord name: DataArray}
562         - mapping {coord name: Variable}
563         - mapping {coord name: (dimension name, array-like)}
564         - mapping {coord name: (tuple of dimension names, array-like)}
565         - mapping {dimension name: array-like}
566           (the dimension name is implicitly set to be the same as the
567           coord name)
568 
569         The last notation implies that the coord name is the same as
570         the dimension name.
571 
572     attrs : dict-like, optional
573         Global attributes to save on this dataset.
574 
575     Examples
576     --------
577     Create data:
578 
579     >>> np.random.seed(0)
580     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
581     >>> precipitation = 10 * np.random.rand(2, 2, 3)
582     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
583     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
584     >>> time = pd.date_range("2014-09-06", periods=3)
585     >>> reference_time = pd.Timestamp("2014-09-05")
586 
587     Initialize a dataset with multiple dimensions:
588 
589     >>> ds = xr.Dataset(
590     ...     data_vars=dict(
591     ...         temperature=(["x", "y", "time"], temperature),
592     ...         precipitation=(["x", "y", "time"], precipitation),
593     ...     ),
594     ...     coords=dict(
595     ...         lon=(["x", "y"], lon),
596     ...         lat=(["x", "y"], lat),
597     ...         time=time,
598     ...         reference_time=reference_time,
599     ...     ),
600     ...     attrs=dict(description="Weather related data."),
601     ... )
602     >>> ds
603     <xarray.Dataset>
604     Dimensions:         (time: 3, x: 2, y: 2)
605     Coordinates:
606         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
607         lat             (x, y) float64 42.25 42.21 42.63 42.59
608       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
609         reference_time  datetime64[ns] 2014-09-05
610     Dimensions without coordinates: x, y
611     Data variables:
612         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
613         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
614     Attributes:
615         description:  Weather related data.
616 
617     Find out where the coldest temperature was and what values the
618     other variables had:
619 
620     >>> ds.isel(ds.temperature.argmin(...))
621     <xarray.Dataset>
622     Dimensions:         ()
623     Coordinates:
624         lon             float64 -99.32
625         lat             float64 42.21
626         time            datetime64[ns] 2014-09-08
627         reference_time  datetime64[ns] 2014-09-05
628     Data variables:
629         temperature     float64 7.182
630         precipitation   float64 8.326
631     Attributes:
632         description:  Weather related data.
633     """
634 
635     _attrs: Optional[Dict[Hashable, Any]]
636     _cache: Dict[str, Any]
637     _coord_names: Set[Hashable]
638     _dims: Dict[Hashable, int]
639     _encoding: Optional[Dict[Hashable, Any]]
640     _close: Optional[Callable[[], None]]
641     _indexes: Optional[Dict[Hashable, pd.Index]]
642     _variables: Dict[Hashable, Variable]
643 
644     __slots__ = (
645         "_attrs",
646         "_cache",
647         "_coord_names",
648         "_dims",
649         "_encoding",
650         "_close",
651         "_indexes",
652         "_variables",
653         "__weakref__",
654     )
655 
656     _groupby_cls = groupby.DatasetGroupBy
657     _rolling_cls = rolling.DatasetRolling
658     _coarsen_cls = rolling.DatasetCoarsen
659     _resample_cls = resample.DatasetResample
660     _weighted_cls = weighted.DatasetWeighted
661 
662     def __init__(
663         self,
664         # could make a VariableArgs to use more generally, and refine these
665         # categories
666         data_vars: Mapping[Hashable, Any] = None,
667         coords: Mapping[Hashable, Any] = None,
668         attrs: Mapping[Hashable, Any] = None,
669     ):
670         # TODO(shoyer): expose indexes as a public argument in __init__
671 
672         if data_vars is None:
673             data_vars = {}
674         if coords is None:
675             coords = {}
676 
677         both_data_and_coords = set(data_vars) & set(coords)
678         if both_data_and_coords:
679             raise ValueError(
680                 "variables %r are found in both data_vars and coords"
681                 % both_data_and_coords
682             )
683 
684         if isinstance(coords, Dataset):
685             coords = coords.variables
686 
687         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
688             data_vars, coords, compat="broadcast_equals"
689         )
690 
691         self._attrs = dict(attrs) if attrs is not None else None
692         self._close = None
693         self._encoding = None
694         self._variables = variables
695         self._coord_names = coord_names
696         self._dims = dims
697         self._indexes = indexes
698 
699     @classmethod
700     def load_store(cls, store, decoder=None) -> "Dataset":
701         """Create a new dataset from the contents of a backends.*DataStore
702         object
703         """
704         variables, attributes = store.load()
705         if decoder:
706             variables, attributes = decoder(variables, attributes)
707         obj = cls(variables, attrs=attributes)
708         obj.set_close(store.close)
709         return obj
710 
711     @property
712     def variables(self) -> Mapping[Hashable, Variable]:
713         """Low level interface to Dataset contents as dict of Variable objects.
714 
715         This ordered dictionary is frozen to prevent mutation that could
716         violate Dataset invariants. It contains all variable objects
717         constituting the Dataset, including both data variables and
718         coordinates.
719         """
720         return Frozen(self._variables)
721 
722     @property
723     def attrs(self) -> Dict[Hashable, Any]:
724         """Dictionary of global attributes on this dataset"""
725         if self._attrs is None:
726             self._attrs = {}
727         return self._attrs
728 
729     @attrs.setter
730     def attrs(self, value: Mapping[Hashable, Any]) -> None:
731         self._attrs = dict(value)
732 
733     @property
734     def encoding(self) -> Dict:
735         """Dictionary of global encoding attributes on this dataset"""
736         if self._encoding is None:
737             self._encoding = {}
738         return self._encoding
739 
740     @encoding.setter
741     def encoding(self, value: Mapping) -> None:
742         self._encoding = dict(value)
743 
744     @property
745     def dims(self) -> Mapping[Hashable, int]:
746         """Mapping from dimension names to lengths.
747 
748         Cannot be modified directly, but is updated when adding new variables.
749 
750         Note that type of this object differs from `DataArray.dims`.
751         See `Dataset.sizes` and `DataArray.sizes` for consistently named
752         properties.
753         """
754         return Frozen(SortedKeysDict(self._dims))
755 
756     @property
757     def sizes(self) -> Mapping[Hashable, int]:
758         """Mapping from dimension names to lengths.
759 
760         Cannot be modified directly, but is updated when adding new variables.
761 
762         This is an alias for `Dataset.dims` provided for the benefit of
763         consistency with `DataArray.sizes`.
764 
765         See Also
766         --------
767         DataArray.sizes
768         """
769         return self.dims
770 
771     def load(self, **kwargs) -> "Dataset":
772         """Manually trigger loading and/or computation of this dataset's data
773         from disk or a remote source into memory and return this dataset.
774         Unlike compute, the original dataset is modified and returned.
775 
776         Normally, it should not be necessary to call this method in user code,
777         because all xarray functions should either work on deferred data or
778         load data automatically. However, this method can be necessary when
779         working with many file objects on disk.
780 
781         Parameters
782         ----------
783         **kwargs : dict
784             Additional keyword arguments passed on to ``dask.compute``.
785 
786         See Also
787         --------
788         dask.compute
789         """
790         # access .data to coerce everything to numpy or dask arrays
791         lazy_data = {
792             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
793         }
794         if lazy_data:
795             import dask.array as da
796 
797             # evaluate all the dask arrays simultaneously
798             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
799 
800             for k, data in zip(lazy_data, evaluated_data):
801                 self.variables[k].data = data
802 
803         # load everything else sequentially
804         for k, v in self.variables.items():
805             if k not in lazy_data:
806                 v.load()
807 
808         return self
809 
810     def __dask_tokenize__(self):
811         from dask.base import normalize_token
812 
813         return normalize_token(
814             (type(self), self._variables, self._coord_names, self._attrs)
815         )
816 
817     def __dask_graph__(self):
818         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
819         graphs = {k: v for k, v in graphs.items() if v is not None}
820         if not graphs:
821             return None
822         else:
823             try:
824                 from dask.highlevelgraph import HighLevelGraph
825 
826                 return HighLevelGraph.merge(*graphs.values())
827             except ImportError:
828                 from dask import sharedict
829 
830                 return sharedict.merge(*graphs.values())
831 
832     def __dask_keys__(self):
833         import dask
834 
835         return [
836             v.__dask_keys__()
837             for v in self.variables.values()
838             if dask.is_dask_collection(v)
839         ]
840 
841     def __dask_layers__(self):
842         import dask
843 
844         return sum(
845             [
846                 v.__dask_layers__()
847                 for v in self.variables.values()
848                 if dask.is_dask_collection(v)
849             ],
850             (),
851         )
852 
853     @property
854     def __dask_optimize__(self):
855         import dask.array as da
856 
857         return da.Array.__dask_optimize__
858 
859     @property
860     def __dask_scheduler__(self):
861         import dask.array as da
862 
863         return da.Array.__dask_scheduler__
864 
865     def __dask_postcompute__(self):
866         import dask
867 
868         info = [
869             (True, k, v.__dask_postcompute__())
870             if dask.is_dask_collection(v)
871             else (False, k, v)
872             for k, v in self._variables.items()
873         ]
874         args = (
875             info,
876             self._coord_names,
877             self._dims,
878             self._attrs,
879             self._indexes,
880             self._encoding,
881             self._close,
882         )
883         return self._dask_postcompute, args
884 
885     def __dask_postpersist__(self):
886         import dask
887 
888         info = [
889             (True, k, v.__dask_postpersist__())
890             if dask.is_dask_collection(v)
891             else (False, k, v)
892             for k, v in self._variables.items()
893         ]
894         args = (
895             info,
896             self._coord_names,
897             self._dims,
898             self._attrs,
899             self._indexes,
900             self._encoding,
901             self._close,
902         )
903         return self._dask_postpersist, args
904 
905     @staticmethod
906     def _dask_postcompute(results, info, *args):
907         variables = {}
908         results2 = list(results[::-1])
909         for is_dask, k, v in info:
910             if is_dask:
911                 func, args2 = v
912                 r = results2.pop()
913                 result = func(r, *args2)
914             else:
915                 result = v
916             variables[k] = result
917 
918         final = Dataset._construct_direct(variables, *args)
919         return final
920 
921     @staticmethod
922     def _dask_postpersist(dsk, info, *args):
923         variables = {}
924         # postpersist is called in both dask.optimize and dask.persist
925         # When persisting, we want to filter out unrelated keys for
926         # each Variable's task graph.
927         is_persist = len(dsk) == len(info)
928         for is_dask, k, v in info:
929             if is_dask:
930                 func, args2 = v
931                 if is_persist:
932                     name = args2[1][0]
933                     dsk2 = {k: v for k, v in dsk.items() if k[0] == name}
934                 else:
935                     dsk2 = dsk
936                 result = func(dsk2, *args2)
937             else:
938                 result = v
939             variables[k] = result
940 
941         return Dataset._construct_direct(variables, *args)
942 
943     def compute(self, **kwargs) -> "Dataset":
944         """Manually trigger loading and/or computation of this dataset's data
945         from disk or a remote source into memory and return a new dataset.
946         Unlike load, the original dataset is left unaltered.
947 
948         Normally, it should not be necessary to call this method in user code,
949         because all xarray functions should either work on deferred data or
950         load data automatically. However, this method can be necessary when
951         working with many file objects on disk.
952 
953         Parameters
954         ----------
955         **kwargs : dict
956             Additional keyword arguments passed on to ``dask.compute``.
957 
958         See Also
959         --------
960         dask.compute
961         """
962         new = self.copy(deep=False)
963         return new.load(**kwargs)
964 
965     def _persist_inplace(self, **kwargs) -> "Dataset":
966         """Persist all Dask arrays in memory"""
967         # access .data to coerce everything to numpy or dask arrays
968         lazy_data = {
969             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
970         }
971         if lazy_data:
972             import dask
973 
974             # evaluate all the dask arrays simultaneously
975             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
976 
977             for k, data in zip(lazy_data, evaluated_data):
978                 self.variables[k].data = data
979 
980         return self
981 
982     def persist(self, **kwargs) -> "Dataset":
983         """Trigger computation, keeping data as dask arrays
984 
985         This operation can be used to trigger computation on underlying dask
986         arrays, similar to ``.compute()`` or ``.load()``.  However this
987         operation keeps the data as dask arrays. This is particularly useful
988         when using the dask.distributed scheduler and you want to load a large
989         amount of data into distributed memory.
990 
991         Parameters
992         ----------
993         **kwargs : dict
994             Additional keyword arguments passed on to ``dask.persist``.
995 
996         See Also
997         --------
998         dask.persist
999         """
1000         new = self.copy(deep=False)
1001         return new._persist_inplace(**kwargs)
1002 
1003     @classmethod
1004     def _construct_direct(
1005         cls,
1006         variables,
1007         coord_names,
1008         dims=None,
1009         attrs=None,
1010         indexes=None,
1011         encoding=None,
1012         close=None,
1013     ):
1014         """Shortcut around __init__ for internal use when we want to skip
1015         costly validation
1016         """
1017         if dims is None:
1018             dims = calculate_dimensions(variables)
1019         obj = object.__new__(cls)
1020         obj._variables = variables
1021         obj._coord_names = coord_names
1022         obj._dims = dims
1023         obj._indexes = indexes
1024         obj._attrs = attrs
1025         obj._close = close
1026         obj._encoding = encoding
1027         return obj
1028 
1029     def _replace(
1030         self,
1031         variables: Dict[Hashable, Variable] = None,
1032         coord_names: Set[Hashable] = None,
1033         dims: Dict[Any, int] = None,
1034         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1035         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
1036         encoding: Union[dict, None, Default] = _default,
1037         inplace: bool = False,
1038     ) -> "Dataset":
1039         """Fastpath constructor for internal use.
1040 
1041         Returns an object with optionally with replaced attributes.
1042 
1043         Explicitly passed arguments are *not* copied when placed on the new
1044         dataset. It is up to the caller to ensure that they have the right type
1045         and are not used elsewhere.
1046         """
1047         if inplace:
1048             if variables is not None:
1049                 self._variables = variables
1050             if coord_names is not None:
1051                 self._coord_names = coord_names
1052             if dims is not None:
1053                 self._dims = dims
1054             if attrs is not _default:
1055                 self._attrs = attrs
1056             if indexes is not _default:
1057                 self._indexes = indexes
1058             if encoding is not _default:
1059                 self._encoding = encoding
1060             obj = self
1061         else:
1062             if variables is None:
1063                 variables = self._variables.copy()
1064             if coord_names is None:
1065                 coord_names = self._coord_names.copy()
1066             if dims is None:
1067                 dims = self._dims.copy()
1068             if attrs is _default:
1069                 attrs = copy.copy(self._attrs)
1070             if indexes is _default:
1071                 indexes = copy.copy(self._indexes)
1072             if encoding is _default:
1073                 encoding = copy.copy(self._encoding)
1074             obj = self._construct_direct(
1075                 variables, coord_names, dims, attrs, indexes, encoding
1076             )
1077         return obj
1078 
1079     def _replace_with_new_dims(
1080         self,
1081         variables: Dict[Hashable, Variable],
1082         coord_names: set = None,
1083         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1084         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
1085         inplace: bool = False,
1086     ) -> "Dataset":
1087         """Replace variables with recalculated dimensions."""
1088         dims = calculate_dimensions(variables)
1089         return self._replace(
1090             variables, coord_names, dims, attrs, indexes, inplace=inplace
1091         )
1092 
1093     def _replace_vars_and_dims(
1094         self,
1095         variables: Dict[Hashable, Variable],
1096         coord_names: set = None,
1097         dims: Dict[Hashable, int] = None,
1098         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1099         inplace: bool = False,
1100     ) -> "Dataset":
1101         """Deprecated version of _replace_with_new_dims().
1102 
1103         Unlike _replace_with_new_dims(), this method always recalculates
1104         indexes from variables.
1105         """
1106         if dims is None:
1107             dims = calculate_dimensions(variables)
1108         return self._replace(
1109             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1110         )
1111 
1112     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
1113         if not indexes:
1114             return self
1115 
1116         variables = self._variables.copy()
1117         new_indexes = dict(self.indexes)
1118         for name, idx in indexes.items():
1119             variables[name] = IndexVariable(name, idx)
1120             new_indexes[name] = idx
1121         obj = self._replace(variables, indexes=new_indexes)
1122 
1123         # switch from dimension to level names, if necessary
1124         dim_names: Dict[Hashable, str] = {}
1125         for dim, idx in indexes.items():
1126             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
1127                 dim_names[dim] = idx.name
1128         if dim_names:
1129             obj = obj.rename(dim_names)
1130         return obj
1131 
1132     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
1133         """Returns a copy of this dataset.
1134 
1135         If `deep=True`, a deep copy is made of each of the component variables.
1136         Otherwise, a shallow copy of each of the component variable is made, so
1137         that the underlying memory region of the new dataset is the same as in
1138         the original dataset.
1139 
1140         Use `data` to create a new object with the same structure as
1141         original but entirely new data.
1142 
1143         Parameters
1144         ----------
1145         deep : bool, optional
1146             Whether each component variable is loaded into memory and copied onto
1147             the new object. Default is False.
1148         data : dict-like, optional
1149             Data to use in the new object. Each item in `data` must have same
1150             shape as corresponding data variable in original. When `data` is
1151             used, `deep` is ignored for the data variables and only used for
1152             coords.
1153 
1154         Returns
1155         -------
1156         object : Dataset
1157             New object with dimensions, attributes, coordinates, name, encoding,
1158             and optionally data copied from original.
1159 
1160         Examples
1161         --------
1162         Shallow copy versus deep copy
1163 
1164         >>> da = xr.DataArray(np.random.randn(2, 3))
1165         >>> ds = xr.Dataset(
1166         ...     {"foo": da, "bar": ("x", [-1, 2])},
1167         ...     coords={"x": ["one", "two"]},
1168         ... )
1169         >>> ds.copy()
1170         <xarray.Dataset>
1171         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1172         Coordinates:
1173           * x        (x) <U3 'one' 'two'
1174         Dimensions without coordinates: dim_0, dim_1
1175         Data variables:
1176             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1177             bar      (x) int64 -1 2
1178 
1179         >>> ds_0 = ds.copy(deep=False)
1180         >>> ds_0["foo"][0, 0] = 7
1181         >>> ds_0
1182         <xarray.Dataset>
1183         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1184         Coordinates:
1185           * x        (x) <U3 'one' 'two'
1186         Dimensions without coordinates: dim_0, dim_1
1187         Data variables:
1188             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1189             bar      (x) int64 -1 2
1190 
1191         >>> ds
1192         <xarray.Dataset>
1193         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1194         Coordinates:
1195           * x        (x) <U3 'one' 'two'
1196         Dimensions without coordinates: dim_0, dim_1
1197         Data variables:
1198             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1199             bar      (x) int64 -1 2
1200 
1201         Changing the data using the ``data`` argument maintains the
1202         structure of the original object, but with the new data. Original
1203         object is unaffected.
1204 
1205         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1206         <xarray.Dataset>
1207         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1208         Coordinates:
1209           * x        (x) <U3 'one' 'two'
1210         Dimensions without coordinates: dim_0, dim_1
1211         Data variables:
1212             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1213             bar      (x) <U1 'a' 'b'
1214 
1215         >>> ds
1216         <xarray.Dataset>
1217         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1218         Coordinates:
1219           * x        (x) <U3 'one' 'two'
1220         Dimensions without coordinates: dim_0, dim_1
1221         Data variables:
1222             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1223             bar      (x) int64 -1 2
1224 
1225         See Also
1226         --------
1227         pandas.DataFrame.copy
1228         """
1229         if data is None:
1230             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1231         elif not utils.is_dict_like(data):
1232             raise ValueError("Data must be dict-like")
1233         else:
1234             var_keys = set(self.data_vars.keys())
1235             data_keys = set(data.keys())
1236             keys_not_in_vars = data_keys - var_keys
1237             if keys_not_in_vars:
1238                 raise ValueError(
1239                     "Data must only contain variables in original "
1240                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1241                 )
1242             keys_missing_from_data = var_keys - data_keys
1243             if keys_missing_from_data:
1244                 raise ValueError(
1245                     "Data must contain all variables in original "
1246                     "dataset. Data is missing {}".format(keys_missing_from_data)
1247                 )
1248             variables = {
1249                 k: v.copy(deep=deep, data=data.get(k))
1250                 for k, v in self._variables.items()
1251             }
1252 
1253         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1254 
1255         return self._replace(variables, attrs=attrs)
1256 
1257     @property
1258     def _level_coords(self) -> Dict[str, Hashable]:
1259         """Return a mapping of all MultiIndex levels and their corresponding
1260         coordinate name.
1261         """
1262         level_coords: Dict[str, Hashable] = {}
1263         for name, index in self.indexes.items():
1264             if isinstance(index, pd.MultiIndex):
1265                 level_names = index.names
1266                 (dim,) = self.variables[name].dims
1267                 level_coords.update({lname: dim for lname in level_names})
1268         return level_coords
1269 
1270     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: Dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: Dict[Hashable, pd.Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self._level_coords, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     indexes[var_name] = var.to_index()
1290 
1291         needed_dims: Set[Hashable] = set()
1292         for v in variables.values():
1293             needed_dims.update(v.dims)
1294 
1295         dims = {k: self.dims[k] for k in needed_dims}
1296 
1297         # preserves ordering of coordinates
1298         for k in self._variables:
1299             if k not in self._coord_names:
1300                 continue
1301 
1302             if set(self.variables[k].dims) <= needed_dims:
1303                 variables[k] = self._variables[k]
1304                 coord_names.add(k)
1305                 if k in self.indexes:
1306                     indexes[k] = self.indexes[k]
1307 
1308         return self._replace(variables, coord_names, dims, indexes=indexes)
1309 
1310     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1311         """Construct a DataArray by indexing this dataset"""
1312         from .dataarray import DataArray
1313 
1314         try:
1315             variable = self._variables[name]
1316         except KeyError:
1317             _, name, variable = _get_virtual_variable(
1318                 self._variables, name, self._level_coords, self.dims
1319             )
1320 
1321         needed_dims = set(variable.dims)
1322 
1323         coords: Dict[Hashable, Variable] = {}
1324         # preserve ordering
1325         for k in self._variables:
1326             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1327                 coords[k] = self.variables[k]
1328 
1329         if self._indexes is None:
1330             indexes = None
1331         else:
1332             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1333 
1334         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1335 
1336     def __copy__(self) -> "Dataset":
1337         return self.copy(deep=False)
1338 
1339     def __deepcopy__(self, memo=None) -> "Dataset":
1340         # memo does nothing but is required for compatibility with
1341         # copy.deepcopy
1342         return self.copy(deep=True)
1343 
1344     @property
1345     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1346         """Places to look-up items for attribute-style access"""
1347         yield from self._item_sources
1348         yield self.attrs
1349 
1350     @property
1351     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1352         """Places to look-up items for key-completion"""
1353         yield self.data_vars
1354         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1355 
1356         # virtual coordinates
1357         yield HybridMappingProxy(keys=self.dims, mapping=self)
1358 
1359         # uses empty dict -- everything here can already be found in self.coords.
1360         yield HybridMappingProxy(keys=self._level_coords, mapping={})
1361 
1362     def __contains__(self, key: object) -> bool:
1363         """The 'in' operator will return true or false depending on whether
1364         'key' is an array in the dataset or not.
1365         """
1366         return key in self._variables
1367 
1368     def __len__(self) -> int:
1369         return len(self.data_vars)
1370 
1371     def __bool__(self) -> bool:
1372         return bool(self.data_vars)
1373 
1374     def __iter__(self) -> Iterator[Hashable]:
1375         return iter(self.data_vars)
1376 
1377     def __array__(self, dtype=None):
1378         raise TypeError(
1379             "cannot directly convert an xarray.Dataset into a "
1380             "numpy array. Instead, create an xarray.DataArray "
1381             "first, either with indexing on the Dataset or by "
1382             "invoking the `to_array()` method."
1383         )
1384 
1385     @property
1386     def nbytes(self) -> int:
1387         return sum(v.nbytes for v in self.variables.values())
1388 
1389     @property
1390     def loc(self) -> _LocIndexer:
1391         """Attribute for location based indexing. Only supports __getitem__,
1392         and only when the key is a dict of the form {dim: labels}.
1393         """
1394         return _LocIndexer(self)
1395 
1396     # FIXME https://github.com/python/mypy/issues/7328
1397     @overload
1398     def __getitem__(self, key: Mapping) -> "Dataset":  # type: ignore
1399         ...
1400 
1401     @overload
1402     def __getitem__(self, key: Hashable) -> "DataArray":  # type: ignore
1403         ...
1404 
1405     @overload
1406     def __getitem__(self, key: Any) -> "Dataset":
1407         ...
1408 
1409     def __getitem__(self, key):
1410         """Access variables or coordinates this dataset as a
1411         :py:class:`~xarray.DataArray`.
1412 
1413         Indexing with a list of names will return a new ``Dataset`` object.
1414         """
1415         if utils.is_dict_like(key):
1416             return self.isel(**cast(Mapping, key))
1417 
1418         if hashable(key):
1419             return self._construct_dataarray(key)
1420         else:
1421             return self._copy_listed(np.asarray(key))
1422 
1423     def __setitem__(self, key: Hashable, value) -> None:
1424         """Add an array to this dataset.
1425 
1426         If value is a `DataArray`, call its `select_vars()` method, rename it
1427         to `key` and merge the contents of the resulting dataset into this
1428         dataset.
1429 
1430         If value is an `Variable` object (or tuple of form
1431         ``(dims, data[, attrs])``), add it to this dataset as a new
1432         variable.
1433         """
1434         if utils.is_dict_like(key):
1435             raise NotImplementedError(
1436                 "cannot yet use a dictionary as a key to set Dataset values"
1437             )
1438 
1439         self.update({key: value})
1440 
1441     def __delitem__(self, key: Hashable) -> None:
1442         """Remove a variable from this dataset."""
1443         del self._variables[key]
1444         self._coord_names.discard(key)
1445         if key in self.indexes:
1446             assert self._indexes is not None
1447             del self._indexes[key]
1448         self._dims = calculate_dimensions(self._variables)
1449 
1450     # mutable objects should not be hashable
1451     # https://github.com/python/mypy/issues/4266
1452     __hash__ = None  # type: ignore
1453 
1454     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1455         """Helper function for equals and identical"""
1456 
1457         # some stores (e.g., scipy) do not seem to preserve order, so don't
1458         # require matching order for equality
1459         def compat(x: Variable, y: Variable) -> bool:
1460             return getattr(x, compat_str)(y)
1461 
1462         return self._coord_names == other._coord_names and utils.dict_equiv(
1463             self._variables, other._variables, compat=compat
1464         )
1465 
1466     def broadcast_equals(self, other: "Dataset") -> bool:
1467         """Two Datasets are broadcast equal if they are equal after
1468         broadcasting all variables against each other.
1469 
1470         For example, variables that are scalar in one dataset but non-scalar in
1471         the other dataset can still be broadcast equal if the the non-scalar
1472         variable is a constant.
1473 
1474         See Also
1475         --------
1476         Dataset.equals
1477         Dataset.identical
1478         """
1479         try:
1480             return self._all_compat(other, "broadcast_equals")
1481         except (TypeError, AttributeError):
1482             return False
1483 
1484     def equals(self, other: "Dataset") -> bool:
1485         """Two Datasets are equal if they have matching variables and
1486         coordinates, all of which are equal.
1487 
1488         Datasets can still be equal (like pandas objects) if they have NaN
1489         values in the same locations.
1490 
1491         This method is necessary because `v1 == v2` for ``Dataset``
1492         does element-wise comparisons (like numpy.ndarrays).
1493 
1494         See Also
1495         --------
1496         Dataset.broadcast_equals
1497         Dataset.identical
1498         """
1499         try:
1500             return self._all_compat(other, "equals")
1501         except (TypeError, AttributeError):
1502             return False
1503 
1504     def identical(self, other: "Dataset") -> bool:
1505         """Like equals, but also checks all dataset attributes and the
1506         attributes on all variables and coordinates.
1507 
1508         See Also
1509         --------
1510         Dataset.broadcast_equals
1511         Dataset.equals
1512         """
1513         try:
1514             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1515                 other, "identical"
1516             )
1517         except (TypeError, AttributeError):
1518             return False
1519 
1520     @property
1521     def indexes(self) -> Indexes:
1522         """Mapping of pandas.Index objects used for label based indexing"""
1523         if self._indexes is None:
1524             self._indexes = default_indexes(self._variables, self._dims)
1525         return Indexes(self._indexes)
1526 
1527     @property
1528     def coords(self) -> DatasetCoordinates:
1529         """Dictionary of xarray.DataArray objects corresponding to coordinate
1530         variables
1531         """
1532         return DatasetCoordinates(self)
1533 
1534     @property
1535     def data_vars(self) -> DataVariables:
1536         """Dictionary of DataArray objects corresponding to data variables"""
1537         return DataVariables(self)
1538 
1539     def set_coords(self, names: "Union[Hashable, Iterable[Hashable]]") -> "Dataset":
1540         """Given names of one or more variables, set them as coordinates
1541 
1542         Parameters
1543         ----------
1544         names : hashable or iterable of hashable
1545             Name(s) of variables in this dataset to convert into coordinates.
1546 
1547         Returns
1548         -------
1549         Dataset
1550 
1551         See Also
1552         --------
1553         Dataset.swap_dims
1554         """
1555         # TODO: allow inserting new coordinates with this method, like
1556         # DataFrame.set_index?
1557         # nb. check in self._variables, not self.data_vars to insure that the
1558         # operation is idempotent
1559         if isinstance(names, str) or not isinstance(names, Iterable):
1560             names = [names]
1561         else:
1562             names = list(names)
1563         self._assert_all_in_dataset(names)
1564         obj = self.copy()
1565         obj._coord_names.update(names)
1566         return obj
1567 
1568     def reset_coords(
1569         self,
1570         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1571         drop: bool = False,
1572     ) -> "Dataset":
1573         """Given names of coordinates, reset them to become variables
1574 
1575         Parameters
1576         ----------
1577         names : hashable or iterable of hashable, optional
1578             Name(s) of non-index coordinates in this dataset to reset into
1579             variables. By default, all non-index coordinates are reset.
1580         drop : bool, optional
1581             If True, remove coordinates instead of converting them into
1582             variables.
1583 
1584         Returns
1585         -------
1586         Dataset
1587         """
1588         if names is None:
1589             names = self._coord_names - set(self.dims)
1590         else:
1591             if isinstance(names, str) or not isinstance(names, Iterable):
1592                 names = [names]
1593             else:
1594                 names = list(names)
1595             self._assert_all_in_dataset(names)
1596             bad_coords = set(names) & set(self.dims)
1597             if bad_coords:
1598                 raise ValueError(
1599                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1600                 )
1601         obj = self.copy()
1602         obj._coord_names.difference_update(names)
1603         if drop:
1604             for name in names:
1605                 del obj._variables[name]
1606         return obj
1607 
1608     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1609         """Store dataset contents to a backends.*DataStore object."""
1610         from ..backends.api import dump_to_store
1611 
1612         # TODO: rename and/or cleanup this method to make it more consistent
1613         # with to_netcdf()
1614         dump_to_store(self, store, **kwargs)
1615 
1616     def to_netcdf(
1617         self,
1618         path=None,
1619         mode: str = "w",
1620         format: str = None,
1621         group: str = None,
1622         engine: str = None,
1623         encoding: Mapping = None,
1624         unlimited_dims: Iterable[Hashable] = None,
1625         compute: bool = True,
1626         invalid_netcdf: bool = False,
1627     ) -> Union[bytes, "Delayed", None]:
1628         """Write dataset contents to a netCDF file.
1629 
1630         Parameters
1631         ----------
1632         path : str, Path or file-like, optional
1633             Path to which to save this dataset. File-like objects are only
1634             supported by the scipy engine. If no path is provided, this
1635             function returns the resulting netCDF file as bytes; in this case,
1636             we need to use scipy, which does not support netCDF version 4 (the
1637             default format becomes NETCDF3_64BIT).
1638         mode : {"w", "a"}, default: "w"
1639             Write ('w') or append ('a') mode. If mode='w', any existing file at
1640             this location will be overwritten. If mode='a', existing variables
1641             will be overwritten.
1642         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1643                   "NETCDF3_CLASSIC"}, optional
1644             File format for the resulting netCDF file:
1645 
1646             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1647               features.
1648             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1649               netCDF 3 compatible API features.
1650             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1651               which fully supports 2+ GB files, but is only compatible with
1652               clients linked against netCDF version 3.6.0 or later.
1653             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1654               handle 2+ GB files very well.
1655 
1656             All formats are supported by the netCDF4-python library.
1657             scipy.io.netcdf only supports the last two formats.
1658 
1659             The default format is NETCDF4 if you are saving a file to disk and
1660             have the netCDF4-python library available. Otherwise, xarray falls
1661             back to using scipy to write netCDF files and defaults to the
1662             NETCDF3_64BIT format (scipy does not support netCDF4).
1663         group : str, optional
1664             Path to the netCDF4 group in the given file to open (only works for
1665             format='NETCDF4'). The group(s) will be created if necessary.
1666         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1667             Engine to use when writing netCDF files. If not provided, the
1668             default engine is chosen based on available dependencies, with a
1669             preference for 'netcdf4' if writing to a file on disk.
1670         encoding : dict, optional
1671             Nested dictionary with variable names as keys and dictionaries of
1672             variable specific encodings as values, e.g.,
1673             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1674             "zlib": True}, ...}``
1675 
1676             The `h5netcdf` engine supports both the NetCDF4-style compression
1677             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1678             ones ``{"compression": "gzip", "compression_opts": 9}``.
1679             This allows using any compression plugin installed in the HDF5
1680             library, e.g. LZF.
1681 
1682         unlimited_dims : iterable of hashable, optional
1683             Dimension(s) that should be serialized as unlimited dimensions.
1684             By default, no dimensions are treated as unlimited dimensions.
1685             Note that unlimited_dims may also be set via
1686             ``dataset.encoding["unlimited_dims"]``.
1687         compute: bool, default: True
1688             If true compute immediately, otherwise return a
1689             ``dask.delayed.Delayed`` object that can be computed later.
1690         invalid_netcdf: bool, default: False
1691             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1692             hdf5 files which are invalid netcdf as described in
1693             https://github.com/shoyer/h5netcdf.
1694         """
1695         if encoding is None:
1696             encoding = {}
1697         from ..backends.api import to_netcdf
1698 
1699         return to_netcdf(
1700             self,
1701             path,
1702             mode,
1703             format=format,
1704             group=group,
1705             engine=engine,
1706             encoding=encoding,
1707             unlimited_dims=unlimited_dims,
1708             compute=compute,
1709             invalid_netcdf=invalid_netcdf,
1710         )
1711 
1712     def to_zarr(
1713         self,
1714         store: Union[MutableMapping, str, Path] = None,
1715         chunk_store: Union[MutableMapping, str, Path] = None,
1716         mode: str = None,
1717         synchronizer=None,
1718         group: str = None,
1719         encoding: Mapping = None,
1720         compute: bool = True,
1721         consolidated: bool = False,
1722         append_dim: Hashable = None,
1723         region: Mapping[str, slice] = None,
1724     ) -> "ZarrStore":
1725         """Write dataset contents to a zarr group.
1726 
1727         .. note:: Experimental
1728                   The Zarr backend is new and experimental. Please report any
1729                   unexpected behavior via github issues.
1730 
1731         Parameters
1732         ----------
1733         store : MutableMapping, str or Path, optional
1734             Store or path to directory in file system.
1735         chunk_store : MutableMapping, str or Path, optional
1736             Store or path to directory in file system only for Zarr array chunks.
1737             Requires zarr-python v2.4.0 or later.
1738         mode : {"w", "w-", "a", None}, optional
1739             Persistence mode: "w" means create (overwrite if exists);
1740             "w-" means create (fail if exists);
1741             "a" means override existing variables (create if does not exist).
1742             If ``append_dim`` is set, ``mode`` can be omitted as it is
1743             internally set to ``"a"``. Otherwise, ``mode`` will default to
1744             `w-` if not set.
1745         synchronizer : object, optional
1746             Zarr array synchronizer.
1747         group : str, optional
1748             Group path. (a.k.a. `path` in zarr terminology.)
1749         encoding : dict, optional
1750             Nested dictionary with variable names as keys and dictionaries of
1751             variable specific encodings as values, e.g.,
1752             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1753         compute : bool, optional
1754             If True write array data immediately, otherwise return a
1755             ``dask.delayed.Delayed`` object that can be computed to write
1756             array data later. Metadata is always updated eagerly.
1757         consolidated : bool, optional
1758             If True, apply zarr's `consolidate_metadata` function to the store
1759             after writing metadata.
1760         append_dim : hashable, optional
1761             If set, the dimension along which the data will be appended. All
1762             other dimensions on overriden variables must remain the same size.
1763         region : dict, optional
1764             Optional mapping from dimension names to integer slices along
1765             dataset dimensions to indicate the region of existing zarr array(s)
1766             in which to write this dataset's data. For example,
1767             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
1768             that values should be written to the region ``0:1000`` along ``x``
1769             and ``10000:11000`` along ``y``.
1770 
1771             Two restrictions apply to the use of ``region``:
1772 
1773             - If ``region`` is set, _all_ variables in a dataset must have at
1774               least one dimension in common with the region. Other variables
1775               should be written in a separate call to ``to_zarr()``.
1776             - Dimensions cannot be included in both ``region`` and
1777               ``append_dim`` at the same time. To create empty arrays to fill
1778               in with ``region``, use a separate call to ``to_zarr()`` with
1779               ``compute=False``. See "Appending to existing Zarr stores" in
1780               the reference documentation for full details.
1781 
1782         References
1783         ----------
1784         https://zarr.readthedocs.io/
1785 
1786         Notes
1787         -----
1788         Zarr chunking behavior:
1789             If chunks are found in the encoding argument or attribute
1790             corresponding to any DataArray, those chunks are used.
1791             If a DataArray is a dask array, it is written with those chunks.
1792             If not other chunks are found, Zarr uses its own heuristics to
1793             choose automatic chunk sizes.
1794         """
1795         from ..backends.api import to_zarr
1796 
1797         if encoding is None:
1798             encoding = {}
1799 
1800         return to_zarr(
1801             self,
1802             store=store,
1803             chunk_store=chunk_store,
1804             mode=mode,
1805             synchronizer=synchronizer,
1806             group=group,
1807             encoding=encoding,
1808             compute=compute,
1809             consolidated=consolidated,
1810             append_dim=append_dim,
1811             region=region,
1812         )
1813 
1814     def __repr__(self) -> str:
1815         return formatting.dataset_repr(self)
1816 
1817     def _repr_html_(self):
1818         if OPTIONS["display_style"] == "text":
1819             return f"<pre>{escape(repr(self))}</pre>"
1820         return formatting_html.dataset_repr(self)
1821 
1822     def info(self, buf=None) -> None:
1823         """
1824         Concise summary of a Dataset variables and attributes.
1825 
1826         Parameters
1827         ----------
1828         buf : file-like, default: sys.stdout
1829             writable buffer
1830 
1831         See Also
1832         --------
1833         pandas.DataFrame.assign
1834         ncdump : netCDF's ncdump
1835         """
1836         if buf is None:  # pragma: no cover
1837             buf = sys.stdout
1838 
1839         lines = []
1840         lines.append("xarray.Dataset {")
1841         lines.append("dimensions:")
1842         for name, size in self.dims.items():
1843             lines.append(f"\t{name} = {size} ;")
1844         lines.append("\nvariables:")
1845         for name, da in self.variables.items():
1846             dims = ", ".join(da.dims)
1847             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1848             for k, v in da.attrs.items():
1849                 lines.append(f"\t\t{name}:{k} = {v} ;")
1850         lines.append("\n// global attributes:")
1851         for k, v in self.attrs.items():
1852             lines.append(f"\t:{k} = {v} ;")
1853         lines.append("}")
1854 
1855         buf.write("\n".join(lines))
1856 
1857     @property
1858     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1859         """Block dimensions for this dataset's data or None if it's not a dask
1860         array.
1861         """
1862         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1863         for v in self.variables.values():
1864             if v.chunks is not None:
1865                 for dim, c in zip(v.dims, v.chunks):
1866                     if dim in chunks and c != chunks[dim]:
1867                         raise ValueError(
1868                             f"Object has inconsistent chunks along dimension {dim}. "
1869                             "This can be fixed by calling unify_chunks()."
1870                         )
1871                     chunks[dim] = c
1872         return Frozen(SortedKeysDict(chunks))
1873 
1874     def chunk(
1875         self,
1876         chunks: Union[
1877             Number,
1878             str,
1879             Mapping[Hashable, Union[None, Number, str, Tuple[Number, ...]]],
1880         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1881         name_prefix: str = "xarray-",
1882         token: str = None,
1883         lock: bool = False,
1884     ) -> "Dataset":
1885         """Coerce all arrays in this dataset into dask arrays with the given
1886         chunks.
1887 
1888         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1889         arrays will be rechunked to the given chunk sizes.
1890 
1891         If neither chunks is not provided for one or more dimensions, chunk
1892         sizes along that dimension will not be updated; non-dask arrays will be
1893         converted into dask arrays with a single block.
1894 
1895         Parameters
1896         ----------
1897         chunks : int, 'auto' or mapping, optional
1898             Chunk sizes along each dimension, e.g., ``5`` or
1899             ``{"x": 5, "y": 5}``.
1900         name_prefix : str, optional
1901             Prefix for the name of any new dask arrays.
1902         token : str, optional
1903             Token uniquely identifying this dataset.
1904         lock : optional
1905             Passed on to :py:func:`dask.array.from_array`, if the array is not
1906             already as dask array.
1907 
1908         Returns
1909         -------
1910         chunked : xarray.Dataset
1911         """
1912         if chunks is None:
1913             warnings.warn(
1914                 "None value for 'chunks' is deprecated. "
1915                 "It will raise an error in the future. Use instead '{}'",
1916                 category=FutureWarning,
1917             )
1918             chunks = {}
1919 
1920         if isinstance(chunks, (Number, str)):
1921             chunks = dict.fromkeys(self.dims, chunks)
1922 
1923         bad_dims = chunks.keys() - self.dims.keys()
1924         if bad_dims:
1925             raise ValueError(
1926                 "some chunks keys are not dimensions on this " "object: %s" % bad_dims
1927             )
1928 
1929         variables = {
1930             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
1931             for k, v in self.variables.items()
1932         }
1933         return self._replace(variables)
1934 
1935     def _validate_indexers(
1936         self, indexers: Mapping[Hashable, Any], missing_dims: str = "raise"
1937     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1938         """Here we make sure
1939         + indexer has a valid keys
1940         + indexer is in a valid data type
1941         + string indexers are cast to the appropriate date type if the
1942           associated index is a DatetimeIndex or CFTimeIndex
1943         """
1944         from .dataarray import DataArray
1945 
1946         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1947 
1948         # all indexers should be int, slice, np.ndarrays, or Variable
1949         for k, v in indexers.items():
1950             if isinstance(v, (int, slice, Variable)):
1951                 yield k, v
1952             elif isinstance(v, DataArray):
1953                 yield k, v.variable
1954             elif isinstance(v, tuple):
1955                 yield k, as_variable(v)
1956             elif isinstance(v, Dataset):
1957                 raise TypeError("cannot use a Dataset as an indexer")
1958             elif isinstance(v, Sequence) and len(v) == 0:
1959                 yield k, np.empty((0,), dtype="int64")
1960             else:
1961                 v = np.asarray(v)
1962 
1963                 if v.dtype.kind in "US":
1964                     index = self.indexes[k]
1965                     if isinstance(index, pd.DatetimeIndex):
1966                         v = v.astype("datetime64[ns]")
1967                     elif isinstance(index, xr.CFTimeIndex):
1968                         v = _parse_array_of_cftime_strings(v, index.date_type)
1969 
1970                 if v.ndim > 1:
1971                     raise IndexError(
1972                         "Unlabeled multi-dimensional array cannot be "
1973                         "used for indexing: {}".format(k)
1974                     )
1975                 yield k, v
1976 
1977     def _validate_interp_indexers(
1978         self, indexers: Mapping[Hashable, Any]
1979     ) -> Iterator[Tuple[Hashable, Variable]]:
1980         """Variant of _validate_indexers to be used for interpolation"""
1981         for k, v in self._validate_indexers(indexers):
1982             if isinstance(v, Variable):
1983                 if v.ndim == 1:
1984                     yield k, v.to_index_variable()
1985                 else:
1986                     yield k, v
1987             elif isinstance(v, int):
1988                 yield k, Variable((), v)
1989             elif isinstance(v, np.ndarray):
1990                 if v.ndim == 0:
1991                     yield k, Variable((), v)
1992                 elif v.ndim == 1:
1993                     yield k, IndexVariable((k,), v)
1994                 else:
1995                     raise AssertionError()  # Already tested by _validate_indexers
1996             else:
1997                 raise TypeError(type(v))
1998 
1999     def _get_indexers_coords_and_indexes(self, indexers):
2000         """Extract coordinates and indexes from indexers.
2001 
2002         Only coordinate with a name different from any of self.variables will
2003         be attached.
2004         """
2005         from .dataarray import DataArray
2006 
2007         coords_list = []
2008         for k, v in indexers.items():
2009             if isinstance(v, DataArray):
2010                 if v.dtype.kind == "b":
2011                     if v.ndim != 1:  # we only support 1-d boolean array
2012                         raise ValueError(
2013                             "{:d}d-boolean array is used for indexing along "
2014                             "dimension {!r}, but only 1d boolean arrays are "
2015                             "supported.".format(v.ndim, k)
2016                         )
2017                     # Make sure in case of boolean DataArray, its
2018                     # coordinate also should be indexed.
2019                     v_coords = v[v.values.nonzero()[0]].coords
2020                 else:
2021                     v_coords = v.coords
2022                 coords_list.append(v_coords)
2023 
2024         # we don't need to call align() explicitly or check indexes for
2025         # alignment, because merge_variables already checks for exact alignment
2026         # between dimension coordinates
2027         coords, indexes = merge_coordinates_without_align(coords_list)
2028         assert_coordinate_consistent(self, coords)
2029 
2030         # silently drop the conflicted variables.
2031         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2032         attached_indexes = {
2033             k: v for k, v in indexes.items() if k not in self._variables
2034         }
2035         return attached_coords, attached_indexes
2036 
2037     def isel(
2038         self,
2039         indexers: Mapping[Hashable, Any] = None,
2040         drop: bool = False,
2041         missing_dims: str = "raise",
2042         **indexers_kwargs: Any,
2043     ) -> "Dataset":
2044         """Returns a new dataset with each array indexed along the specified
2045         dimension(s).
2046 
2047         This method selects values from each array using its `__getitem__`
2048         method, except this method does not require knowing the order of
2049         each array's dimensions.
2050 
2051         Parameters
2052         ----------
2053         indexers : dict, optional
2054             A dict with keys matching dimensions and values given
2055             by integers, slice objects or arrays.
2056             indexer can be a integer, slice, array-like or DataArray.
2057             If DataArrays are passed as indexers, xarray-style indexing will be
2058             carried out. See :ref:`indexing` for the details.
2059             One of indexers or indexers_kwargs must be provided.
2060         drop : bool, optional
2061             If ``drop=True``, drop coordinates variables indexed by integers
2062             instead of making them scalar.
2063         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2064             What to do if dimensions that should be selected from are not present in the
2065             Dataset:
2066             - "raise": raise an exception
2067             - "warning": raise a warning, and ignore the missing dimensions
2068             - "ignore": ignore the missing dimensions
2069         **indexers_kwargs : {dim: indexer, ...}, optional
2070             The keyword arguments form of ``indexers``.
2071             One of indexers or indexers_kwargs must be provided.
2072 
2073         Returns
2074         -------
2075         obj : Dataset
2076             A new Dataset with the same contents as this dataset, except each
2077             array and dimension is indexed by the appropriate indexers.
2078             If indexer DataArrays have coordinates that do not conflict with
2079             this object, then these coordinates will be attached.
2080             In general, each array's data will be a view of the array's data
2081             in this dataset, unless vectorized indexing was triggered by using
2082             an array indexer, in which case the data will be a copy.
2083 
2084         See Also
2085         --------
2086         Dataset.sel
2087         DataArray.isel
2088         """
2089         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2090         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2091             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2092 
2093         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2094         # lists, or zero or one-dimensional np.ndarray's
2095         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2096 
2097         variables = {}
2098         dims: Dict[Hashable, Tuple[int, ...]] = {}
2099         coord_names = self._coord_names.copy()
2100         indexes = self._indexes.copy() if self._indexes is not None else None
2101 
2102         for var_name, var_value in self._variables.items():
2103             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
2104             if var_indexers:
2105                 var_value = var_value.isel(var_indexers)
2106                 if drop and var_value.ndim == 0 and var_name in coord_names:
2107                     coord_names.remove(var_name)
2108                     if indexes:
2109                         indexes.pop(var_name, None)
2110                     continue
2111                 if indexes and var_name in indexes:
2112                     if var_value.ndim == 1:
2113                         indexes[var_name] = var_value.to_index()
2114                     else:
2115                         del indexes[var_name]
2116             variables[var_name] = var_value
2117             dims.update(zip(var_value.dims, var_value.shape))
2118 
2119         return self._construct_direct(
2120             variables=variables,
2121             coord_names=coord_names,
2122             dims=dims,
2123             attrs=self._attrs,
2124             indexes=indexes,
2125             encoding=self._encoding,
2126             close=self._close,
2127         )
2128 
2129     def _isel_fancy(
2130         self,
2131         indexers: Mapping[Hashable, Any],
2132         *,
2133         drop: bool,
2134         missing_dims: str = "raise",
2135     ) -> "Dataset":
2136         # Note: we need to preserve the original indexers variable in order to merge the
2137         # coords below
2138         indexers_list = list(self._validate_indexers(indexers, missing_dims))
2139 
2140         variables: Dict[Hashable, Variable] = {}
2141         indexes: Dict[Hashable, pd.Index] = {}
2142 
2143         for name, var in self.variables.items():
2144             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
2145             if drop and name in var_indexers:
2146                 continue  # drop this variable
2147 
2148             if name in self.indexes:
2149                 new_var, new_index = isel_variable_and_index(
2150                     name, var, self.indexes[name], var_indexers
2151                 )
2152                 if new_index is not None:
2153                     indexes[name] = new_index
2154             elif var_indexers:
2155                 new_var = var.isel(indexers=var_indexers)
2156             else:
2157                 new_var = var.copy(deep=False)
2158 
2159             variables[name] = new_var
2160 
2161         coord_names = self._coord_names & variables.keys()
2162         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2163 
2164         # Extract coordinates from indexers
2165         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2166         variables.update(coord_vars)
2167         indexes.update(new_indexes)
2168         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2169         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2170 
2171     def sel(
2172         self,
2173         indexers: Mapping[Hashable, Any] = None,
2174         method: str = None,
2175         tolerance: Number = None,
2176         drop: bool = False,
2177         **indexers_kwargs: Any,
2178     ) -> "Dataset":
2179         """Returns a new dataset with each array indexed by tick labels
2180         along the specified dimension(s).
2181 
2182         In contrast to `Dataset.isel`, indexers for this method should use
2183         labels instead of integers.
2184 
2185         Under the hood, this method is powered by using pandas's powerful Index
2186         objects. This makes label based indexing essentially just as fast as
2187         using integer indexing.
2188 
2189         It also means this method uses pandas's (well documented) logic for
2190         indexing. This means you can use string shortcuts for datetime indexes
2191         (e.g., '2000-01' to select all values in January 2000). It also means
2192         that slices are treated as inclusive of both the start and stop values,
2193         unlike normal Python indexing.
2194 
2195         Parameters
2196         ----------
2197         indexers : dict, optional
2198             A dict with keys matching dimensions and values given
2199             by scalars, slices or arrays of tick labels. For dimensions with
2200             multi-index, the indexer may also be a dict-like object with keys
2201             matching index level names.
2202             If DataArrays are passed as indexers, xarray-style indexing will be
2203             carried out. See :ref:`indexing` for the details.
2204             One of indexers or indexers_kwargs must be provided.
2205         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2206             Method to use for inexact matches:
2207 
2208             * None (default): only exact matches
2209             * pad / ffill: propagate last valid index value forward
2210             * backfill / bfill: propagate next valid index value backward
2211             * nearest: use nearest valid index value
2212         tolerance : optional
2213             Maximum distance between original and new labels for inexact
2214             matches. The values of the index at the matching locations must
2215             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2216         drop : bool, optional
2217             If ``drop=True``, drop coordinates variables in `indexers` instead
2218             of making them scalar.
2219         **indexers_kwargs : {dim: indexer, ...}, optional
2220             The keyword arguments form of ``indexers``.
2221             One of indexers or indexers_kwargs must be provided.
2222 
2223         Returns
2224         -------
2225         obj : Dataset
2226             A new Dataset with the same contents as this dataset, except each
2227             variable and dimension is indexed by the appropriate indexers.
2228             If indexer DataArrays have coordinates that do not conflict with
2229             this object, then these coordinates will be attached.
2230             In general, each array's data will be a view of the array's data
2231             in this dataset, unless vectorized indexing was triggered by using
2232             an array indexer, in which case the data will be a copy.
2233 
2234         See Also
2235         --------
2236         Dataset.isel
2237         DataArray.sel
2238         """
2239         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2240         pos_indexers, new_indexes = remap_label_indexers(
2241             self, indexers=indexers, method=method, tolerance=tolerance
2242         )
2243         result = self.isel(indexers=pos_indexers, drop=drop)
2244         return result._overwrite_indexes(new_indexes)
2245 
2246     def head(
2247         self,
2248         indexers: Union[Mapping[Hashable, int], int] = None,
2249         **indexers_kwargs: Any,
2250     ) -> "Dataset":
2251         """Returns a new dataset with the first `n` values of each array
2252         for the specified dimension(s).
2253 
2254         Parameters
2255         ----------
2256         indexers : dict or int, default: 5
2257             A dict with keys matching dimensions and integer values `n`
2258             or a single integer `n` applied over all dimensions.
2259             One of indexers or indexers_kwargs must be provided.
2260         **indexers_kwargs : {dim: n, ...}, optional
2261             The keyword arguments form of ``indexers``.
2262             One of indexers or indexers_kwargs must be provided.
2263 
2264         See Also
2265         --------
2266         Dataset.tail
2267         Dataset.thin
2268         DataArray.head
2269         """
2270         if not indexers_kwargs:
2271             if indexers is None:
2272                 indexers = 5
2273             if not isinstance(indexers, int) and not is_dict_like(indexers):
2274                 raise TypeError("indexers must be either dict-like or a single integer")
2275         if isinstance(indexers, int):
2276             indexers = {dim: indexers for dim in self.dims}
2277         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2278         for k, v in indexers.items():
2279             if not isinstance(v, int):
2280                 raise TypeError(
2281                     "expected integer type indexer for "
2282                     "dimension %r, found %r" % (k, type(v))
2283                 )
2284             elif v < 0:
2285                 raise ValueError(
2286                     "expected positive integer as indexer "
2287                     "for dimension %r, found %s" % (k, v)
2288                 )
2289         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2290         return self.isel(indexers_slices)
2291 
2292     def tail(
2293         self,
2294         indexers: Union[Mapping[Hashable, int], int] = None,
2295         **indexers_kwargs: Any,
2296     ) -> "Dataset":
2297         """Returns a new dataset with the last `n` values of each array
2298         for the specified dimension(s).
2299 
2300         Parameters
2301         ----------
2302         indexers : dict or int, default: 5
2303             A dict with keys matching dimensions and integer values `n`
2304             or a single integer `n` applied over all dimensions.
2305             One of indexers or indexers_kwargs must be provided.
2306         **indexers_kwargs : {dim: n, ...}, optional
2307             The keyword arguments form of ``indexers``.
2308             One of indexers or indexers_kwargs must be provided.
2309 
2310         See Also
2311         --------
2312         Dataset.head
2313         Dataset.thin
2314         DataArray.tail
2315         """
2316         if not indexers_kwargs:
2317             if indexers is None:
2318                 indexers = 5
2319             if not isinstance(indexers, int) and not is_dict_like(indexers):
2320                 raise TypeError("indexers must be either dict-like or a single integer")
2321         if isinstance(indexers, int):
2322             indexers = {dim: indexers for dim in self.dims}
2323         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2324         for k, v in indexers.items():
2325             if not isinstance(v, int):
2326                 raise TypeError(
2327                     "expected integer type indexer for "
2328                     "dimension %r, found %r" % (k, type(v))
2329                 )
2330             elif v < 0:
2331                 raise ValueError(
2332                     "expected positive integer as indexer "
2333                     "for dimension %r, found %s" % (k, v)
2334                 )
2335         indexers_slices = {
2336             k: slice(-val, None) if val != 0 else slice(val)
2337             for k, val in indexers.items()
2338         }
2339         return self.isel(indexers_slices)
2340 
2341     def thin(
2342         self,
2343         indexers: Union[Mapping[Hashable, int], int] = None,
2344         **indexers_kwargs: Any,
2345     ) -> "Dataset":
2346         """Returns a new dataset with each array indexed along every `n`-th
2347         value for the specified dimension(s)
2348 
2349         Parameters
2350         ----------
2351         indexers : dict or int
2352             A dict with keys matching dimensions and integer values `n`
2353             or a single integer `n` applied over all dimensions.
2354             One of indexers or indexers_kwargs must be provided.
2355         **indexers_kwargs : {dim: n, ...}, optional
2356             The keyword arguments form of ``indexers``.
2357             One of indexers or indexers_kwargs must be provided.
2358 
2359         See Also
2360         --------
2361         Dataset.head
2362         Dataset.tail
2363         DataArray.thin
2364         """
2365         if (
2366             not indexers_kwargs
2367             and not isinstance(indexers, int)
2368             and not is_dict_like(indexers)
2369         ):
2370             raise TypeError("indexers must be either dict-like or a single integer")
2371         if isinstance(indexers, int):
2372             indexers = {dim: indexers for dim in self.dims}
2373         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2374         for k, v in indexers.items():
2375             if not isinstance(v, int):
2376                 raise TypeError(
2377                     "expected integer type indexer for "
2378                     "dimension %r, found %r" % (k, type(v))
2379                 )
2380             elif v < 0:
2381                 raise ValueError(
2382                     "expected positive integer as indexer "
2383                     "for dimension %r, found %s" % (k, v)
2384                 )
2385             elif v == 0:
2386                 raise ValueError("step cannot be zero")
2387         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2388         return self.isel(indexers_slices)
2389 
2390     def broadcast_like(
2391         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2392     ) -> "Dataset":
2393         """Broadcast this DataArray against another Dataset or DataArray.
2394         This is equivalent to xr.broadcast(other, self)[1]
2395 
2396         Parameters
2397         ----------
2398         other : Dataset or DataArray
2399             Object against which to broadcast this array.
2400         exclude : iterable of hashable, optional
2401             Dimensions that must not be broadcasted
2402 
2403         """
2404         if exclude is None:
2405             exclude = set()
2406         else:
2407             exclude = set(exclude)
2408         args = align(other, self, join="outer", copy=False, exclude=exclude)
2409 
2410         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2411 
2412         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2413 
2414     def reindex_like(
2415         self,
2416         other: Union["Dataset", "DataArray"],
2417         method: str = None,
2418         tolerance: Number = None,
2419         copy: bool = True,
2420         fill_value: Any = dtypes.NA,
2421     ) -> "Dataset":
2422         """Conform this object onto the indexes of another object, filling in
2423         missing values with ``fill_value``. The default fill value is NaN.
2424 
2425         Parameters
2426         ----------
2427         other : Dataset or DataArray
2428             Object with an 'indexes' attribute giving a mapping from dimension
2429             names to pandas.Index objects, which provides coordinates upon
2430             which to index the variables in this dataset. The indexes on this
2431             other object need not be the same as the indexes on this
2432             dataset. Any mis-matched index values will be filled in with
2433             NaN, and any mis-matched dimension names will simply be ignored.
2434         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2435             Method to use for filling index values from other not found in this
2436             dataset:
2437 
2438             * None (default): don't fill gaps
2439             * pad / ffill: propagate last valid index value forward
2440             * backfill / bfill: propagate next valid index value backward
2441             * nearest: use nearest valid index value
2442         tolerance : optional
2443             Maximum distance between original and new labels for inexact
2444             matches. The values of the index at the matching locations must
2445             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2446         copy : bool, optional
2447             If ``copy=True``, data in the return value is always copied. If
2448             ``copy=False`` and reindexing is unnecessary, or can be performed
2449             with only slice operations, then the output may share memory with
2450             the input. In either case, a new xarray object is always returned.
2451         fill_value : scalar or dict-like, optional
2452             Value to use for newly missing values. If a dict-like maps
2453             variable names to fill values.
2454 
2455         Returns
2456         -------
2457         reindexed : Dataset
2458             Another dataset, with this dataset's data but coordinates from the
2459             other object.
2460 
2461         See Also
2462         --------
2463         Dataset.reindex
2464         align
2465         """
2466         indexers = alignment.reindex_like_indexers(self, other)
2467         return self.reindex(
2468             indexers=indexers,
2469             method=method,
2470             copy=copy,
2471             fill_value=fill_value,
2472             tolerance=tolerance,
2473         )
2474 
2475     def reindex(
2476         self,
2477         indexers: Mapping[Hashable, Any] = None,
2478         method: str = None,
2479         tolerance: Number = None,
2480         copy: bool = True,
2481         fill_value: Any = dtypes.NA,
2482         **indexers_kwargs: Any,
2483     ) -> "Dataset":
2484         """Conform this object onto a new set of indexes, filling in
2485         missing values with ``fill_value``. The default fill value is NaN.
2486 
2487         Parameters
2488         ----------
2489         indexers : dict, optional
2490             Dictionary with keys given by dimension names and values given by
2491             arrays of coordinates tick labels. Any mis-matched coordinate
2492             values will be filled in with NaN, and any mis-matched dimension
2493             names will simply be ignored.
2494             One of indexers or indexers_kwargs must be provided.
2495         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2496             Method to use for filling index values in ``indexers`` not found in
2497             this dataset:
2498 
2499             * None (default): don't fill gaps
2500             * pad / ffill: propagate last valid index value forward
2501             * backfill / bfill: propagate next valid index value backward
2502             * nearest: use nearest valid index value
2503         tolerance : optional
2504             Maximum distance between original and new labels for inexact
2505             matches. The values of the index at the matching locations must
2506             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2507         copy : bool, optional
2508             If ``copy=True``, data in the return value is always copied. If
2509             ``copy=False`` and reindexing is unnecessary, or can be performed
2510             with only slice operations, then the output may share memory with
2511             the input. In either case, a new xarray object is always returned.
2512         fill_value : scalar or dict-like, optional
2513             Value to use for newly missing values. If a dict-like,
2514             maps variable names (including coordinates) to fill values.
2515         sparse : bool, default: False
2516             use sparse-array.
2517         **indexers_kwargs : {dim: indexer, ...}, optional
2518             Keyword arguments in the same form as ``indexers``.
2519             One of indexers or indexers_kwargs must be provided.
2520 
2521         Returns
2522         -------
2523         reindexed : Dataset
2524             Another dataset, with this dataset's data but replaced coordinates.
2525 
2526         See Also
2527         --------
2528         Dataset.reindex_like
2529         align
2530         pandas.Index.get_indexer
2531 
2532         Examples
2533         --------
2534         Create a dataset with some fictional data.
2535 
2536         >>> import xarray as xr
2537         >>> import pandas as pd
2538         >>> x = xr.Dataset(
2539         ...     {
2540         ...         "temperature": ("station", 20 * np.random.rand(4)),
2541         ...         "pressure": ("station", 500 * np.random.rand(4)),
2542         ...     },
2543         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2544         ... )
2545         >>> x
2546         <xarray.Dataset>
2547         Dimensions:      (station: 4)
2548         Coordinates:
2549           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2550         Data variables:
2551             temperature  (station) float64 10.98 14.3 12.06 10.9
2552             pressure     (station) float64 211.8 322.9 218.8 445.9
2553         >>> x.indexes
2554         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2555 
2556         Create a new index and reindex the dataset. By default values in the new index that
2557         do not have corresponding records in the dataset are assigned `NaN`.
2558 
2559         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2560         >>> x.reindex({"station": new_index})
2561         <xarray.Dataset>
2562         Dimensions:      (station: 4)
2563         Coordinates:
2564           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2565         Data variables:
2566             temperature  (station) float64 10.98 nan 12.06 nan
2567             pressure     (station) float64 211.8 nan 218.8 nan
2568 
2569         We can fill in the missing values by passing a value to the keyword `fill_value`.
2570 
2571         >>> x.reindex({"station": new_index}, fill_value=0)
2572         <xarray.Dataset>
2573         Dimensions:      (station: 4)
2574         Coordinates:
2575           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2576         Data variables:
2577             temperature  (station) float64 10.98 0.0 12.06 0.0
2578             pressure     (station) float64 211.8 0.0 218.8 0.0
2579 
2580         We can also use different fill values for each variable.
2581 
2582         >>> x.reindex(
2583         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
2584         ... )
2585         <xarray.Dataset>
2586         Dimensions:      (station: 4)
2587         Coordinates:
2588           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2589         Data variables:
2590             temperature  (station) float64 10.98 0.0 12.06 0.0
2591             pressure     (station) float64 211.8 100.0 218.8 100.0
2592 
2593         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2594         to the keyword method to fill the `NaN` values.
2595 
2596         >>> x.reindex({"station": new_index}, method="nearest")
2597         Traceback (most recent call last):
2598         ...
2599             raise ValueError('index must be monotonic increasing or decreasing')
2600         ValueError: index must be monotonic increasing or decreasing
2601 
2602         To further illustrate the filling functionality in reindex, we will create a
2603         dataset with a monotonically increasing index (for example, a sequence of dates).
2604 
2605         >>> x2 = xr.Dataset(
2606         ...     {
2607         ...         "temperature": (
2608         ...             "time",
2609         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2610         ...         ),
2611         ...         "pressure": ("time", 500 * np.random.rand(6)),
2612         ...     },
2613         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2614         ... )
2615         >>> x2
2616         <xarray.Dataset>
2617         Dimensions:      (time: 6)
2618         Coordinates:
2619           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2620         Data variables:
2621             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2622             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
2623 
2624         Suppose we decide to expand the dataset to cover a wider date range.
2625 
2626         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2627         >>> x2.reindex({"time": time_index2})
2628         <xarray.Dataset>
2629         Dimensions:      (time: 10)
2630         Coordinates:
2631           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2632         Data variables:
2633             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2634             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
2635 
2636         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2637         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2638 
2639         For example, to back-propagate the last valid value to fill the `NaN` values,
2640         pass `bfill` as an argument to the `method` keyword.
2641 
2642         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2643         >>> x3
2644         <xarray.Dataset>
2645         Dimensions:      (time: 10)
2646         Coordinates:
2647           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2648         Data variables:
2649             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2650             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
2651 
2652         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2653         will not be filled by any of the value propagation schemes.
2654 
2655         >>> x2.where(x2.temperature.isnull(), drop=True)
2656         <xarray.Dataset>
2657         Dimensions:      (time: 1)
2658         Coordinates:
2659           * time         (time) datetime64[ns] 2019-01-03
2660         Data variables:
2661             temperature  (time) float64 nan
2662             pressure     (time) float64 395.9
2663         >>> x3.where(x3.temperature.isnull(), drop=True)
2664         <xarray.Dataset>
2665         Dimensions:      (time: 2)
2666         Coordinates:
2667           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2668         Data variables:
2669             temperature  (time) float64 nan nan
2670             pressure     (time) float64 395.9 nan
2671 
2672         This is because filling while reindexing does not look at dataset values, but only compares
2673         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2674         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2675 
2676         """
2677         return self._reindex(
2678             indexers,
2679             method,
2680             tolerance,
2681             copy,
2682             fill_value,
2683             sparse=False,
2684             **indexers_kwargs,
2685         )
2686 
2687     def _reindex(
2688         self,
2689         indexers: Mapping[Hashable, Any] = None,
2690         method: str = None,
2691         tolerance: Number = None,
2692         copy: bool = True,
2693         fill_value: Any = dtypes.NA,
2694         sparse: bool = False,
2695         **indexers_kwargs: Any,
2696     ) -> "Dataset":
2697         """
2698         same to _reindex but support sparse option
2699         """
2700         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2701 
2702         bad_dims = [d for d in indexers if d not in self.dims]
2703         if bad_dims:
2704             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2705 
2706         variables, indexes = alignment.reindex_variables(
2707             self.variables,
2708             self.sizes,
2709             self.indexes,
2710             indexers,
2711             method,
2712             tolerance,
2713             copy=copy,
2714             fill_value=fill_value,
2715             sparse=sparse,
2716         )
2717         coord_names = set(self._coord_names)
2718         coord_names.update(indexers)
2719         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2720 
2721     def interp(
2722         self,
2723         coords: Mapping[Hashable, Any] = None,
2724         method: str = "linear",
2725         assume_sorted: bool = False,
2726         kwargs: Mapping[str, Any] = None,
2727         **coords_kwargs: Any,
2728     ) -> "Dataset":
2729         """Multidimensional interpolation of Dataset.
2730 
2731         Parameters
2732         ----------
2733         coords : dict, optional
2734             Mapping from dimension names to the new coordinates.
2735             New coordinate can be a scalar, array-like or DataArray.
2736             If DataArrays are passed as new coordinates, their dimensions are
2737             used for the broadcasting. Missing values are skipped.
2738         method : str, optional
2739             {"linear", "nearest"} for multidimensional array,
2740             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2741             for 1-dimensional array. "linear" is used by default.
2742         assume_sorted : bool, optional
2743             If False, values of coordinates that are interpolated over can be
2744             in any order and they are sorted first. If True, interpolated
2745             coordinates are assumed to be an array of monotonically increasing
2746             values.
2747         kwargs : dict, optional
2748             Additional keyword arguments passed to scipy's interpolator. Valid
2749             options and their behavior depend on if 1-dimensional or
2750             multi-dimensional interpolation is used.
2751         **coords_kwargs : {dim: coordinate, ...}, optional
2752             The keyword arguments form of ``coords``.
2753             One of coords or coords_kwargs must be provided.
2754 
2755         Returns
2756         -------
2757         interpolated : Dataset
2758             New dataset on the new coordinates.
2759 
2760         Notes
2761         -----
2762         scipy is required.
2763 
2764         See Also
2765         --------
2766         scipy.interpolate.interp1d
2767         scipy.interpolate.interpn
2768 
2769         Examples
2770         --------
2771         >>> ds = xr.Dataset(
2772         ...     data_vars={
2773         ...         "a": ("x", [5, 7, 4]),
2774         ...         "b": (
2775         ...             ("x", "y"),
2776         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2777         ...         ),
2778         ...     },
2779         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2780         ... )
2781         >>> ds
2782         <xarray.Dataset>
2783         Dimensions:  (x: 3, y: 4)
2784         Coordinates:
2785           * x        (x) int64 0 1 2
2786           * y        (y) int64 10 12 14 16
2787         Data variables:
2788             a        (x) int64 5 7 4
2789             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
2790 
2791         1D interpolation with the default method (linear):
2792 
2793         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
2794         <xarray.Dataset>
2795         Dimensions:  (x: 4, y: 4)
2796         Coordinates:
2797           * y        (y) int64 10 12 14 16
2798           * x        (x) float64 0.0 0.75 1.25 1.75
2799         Data variables:
2800             a        (x) float64 5.0 6.5 6.25 4.75
2801             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
2802 
2803         1D interpolation with a different method:
2804 
2805         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2806         <xarray.Dataset>
2807         Dimensions:  (x: 4, y: 4)
2808         Coordinates:
2809           * y        (y) int64 10 12 14 16
2810           * x        (x) float64 0.0 0.75 1.25 1.75
2811         Data variables:
2812             a        (x) float64 5.0 7.0 7.0 4.0
2813             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
2814 
2815         1D extrapolation:
2816 
2817         >>> ds.interp(
2818         ...     x=[1, 1.5, 2.5, 3.5],
2819         ...     method="linear",
2820         ...     kwargs={"fill_value": "extrapolate"},
2821         ... )
2822         <xarray.Dataset>
2823         Dimensions:  (x: 4, y: 4)
2824         Coordinates:
2825           * y        (y) int64 10 12 14 16
2826           * x        (x) float64 1.0 1.5 2.5 3.5
2827         Data variables:
2828             a        (x) float64 7.0 5.5 2.5 -0.5
2829             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
2830 
2831         2D interpolation:
2832 
2833         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2834         <xarray.Dataset>
2835         Dimensions:  (x: 4, y: 3)
2836         Coordinates:
2837           * x        (x) float64 0.0 0.75 1.25 1.75
2838           * y        (y) int64 11 13 15
2839         Data variables:
2840             a        (x) float64 5.0 6.5 6.25 4.75
2841             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
2842         """
2843         from . import missing
2844 
2845         if kwargs is None:
2846             kwargs = {}
2847 
2848         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2849         indexers = dict(self._validate_interp_indexers(coords))
2850 
2851         if coords:
2852             # This avoids broadcasting over coordinates that are both in
2853             # the original array AND in the indexing array. It essentially
2854             # forces interpolation along the shared coordinates.
2855             sdims = (
2856                 set(self.dims)
2857                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2858                 .difference(coords.keys())
2859             )
2860             indexers.update({d: self.variables[d] for d in sdims})
2861 
2862         obj = self if assume_sorted else self.sortby([k for k in coords])
2863 
2864         def maybe_variable(obj, k):
2865             # workaround to get variable for dimension without coordinate.
2866             try:
2867                 return obj._variables[k]
2868             except KeyError:
2869                 return as_variable((k, range(obj.dims[k])))
2870 
2871         def _validate_interp_indexer(x, new_x):
2872             # In the case of datetimes, the restrictions placed on indexers
2873             # used with interp are stronger than those which are placed on
2874             # isel, so we need an additional check after _validate_indexers.
2875             if _contains_datetime_like_objects(
2876                 x
2877             ) and not _contains_datetime_like_objects(new_x):
2878                 raise TypeError(
2879                     "When interpolating over a datetime-like "
2880                     "coordinate, the coordinates to "
2881                     "interpolate to must be either datetime "
2882                     "strings or datetimes. "
2883                     "Instead got\n{}".format(new_x)
2884                 )
2885             return x, new_x
2886 
2887         variables: Dict[Hashable, Variable] = {}
2888         for name, var in obj._variables.items():
2889             if name in indexers:
2890                 continue
2891 
2892             if var.dtype.kind in "uifc":
2893                 var_indexers = {
2894                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2895                     for k, v in indexers.items()
2896                     if k in var.dims
2897                 }
2898                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2899             elif all(d not in indexers for d in var.dims):
2900                 # keep unrelated object array
2901                 variables[name] = var
2902 
2903         coord_names = obj._coord_names & variables.keys()
2904         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2905         selected = self._replace_with_new_dims(
2906             variables.copy(), coord_names, indexes=indexes
2907         )
2908 
2909         # attach indexer as coordinate
2910         variables.update(indexers)
2911         for k, v in indexers.items():
2912             assert isinstance(v, Variable)
2913             if v.dims == (k,):
2914                 indexes[k] = v.to_index()
2915 
2916         # Extract coordinates from indexers
2917         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2918         variables.update(coord_vars)
2919         indexes.update(new_indexes)
2920 
2921         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2922         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2923 
2924     def interp_like(
2925         self,
2926         other: Union["Dataset", "DataArray"],
2927         method: str = "linear",
2928         assume_sorted: bool = False,
2929         kwargs: Mapping[str, Any] = None,
2930     ) -> "Dataset":
2931         """Interpolate this object onto the coordinates of another object,
2932         filling the out of range values with NaN.
2933 
2934         Parameters
2935         ----------
2936         other : Dataset or DataArray
2937             Object with an 'indexes' attribute giving a mapping from dimension
2938             names to an 1d array-like, which provides coordinates upon
2939             which to index the variables in this dataset. Missing values are skipped.
2940         method : str, optional
2941             {"linear", "nearest"} for multidimensional array,
2942             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2943             for 1-dimensional array. 'linear' is used by default.
2944         assume_sorted : bool, optional
2945             If False, values of coordinates that are interpolated over can be
2946             in any order and they are sorted first. If True, interpolated
2947             coordinates are assumed to be an array of monotonically increasing
2948             values.
2949         kwargs : dict, optional
2950             Additional keyword passed to scipy's interpolator.
2951 
2952         Returns
2953         -------
2954         interpolated : Dataset
2955             Another dataset by interpolating this dataset's data along the
2956             coordinates of the other object.
2957 
2958         Notes
2959         -----
2960         scipy is required.
2961         If the dataset has object-type coordinates, reindex is used for these
2962         coordinates instead of the interpolation.
2963 
2964         See Also
2965         --------
2966         Dataset.interp
2967         Dataset.reindex_like
2968         """
2969         if kwargs is None:
2970             kwargs = {}
2971         coords = alignment.reindex_like_indexers(self, other)
2972 
2973         numeric_coords: Dict[Hashable, pd.Index] = {}
2974         object_coords: Dict[Hashable, pd.Index] = {}
2975         for k, v in coords.items():
2976             if v.dtype.kind in "uifcMm":
2977                 numeric_coords[k] = v
2978             else:
2979                 object_coords[k] = v
2980 
2981         ds = self
2982         if object_coords:
2983             # We do not support interpolation along object coordinate.
2984             # reindex instead.
2985             ds = self.reindex(object_coords)
2986         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2987 
2988     # Helper methods for rename()
2989     def _rename_vars(self, name_dict, dims_dict):
2990         variables = {}
2991         coord_names = set()
2992         for k, v in self.variables.items():
2993             var = v.copy(deep=False)
2994             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2995             name = name_dict.get(k, k)
2996             if name in variables:
2997                 raise ValueError(f"the new name {name!r} conflicts")
2998             variables[name] = var
2999             if k in self._coord_names:
3000                 coord_names.add(name)
3001         return variables, coord_names
3002 
3003     def _rename_dims(self, name_dict):
3004         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3005 
3006     def _rename_indexes(self, name_dict, dims_set):
3007         if self._indexes is None:
3008             return None
3009         indexes = {}
3010         for k, v in self.indexes.items():
3011             new_name = name_dict.get(k, k)
3012             if new_name not in dims_set:
3013                 continue
3014             if isinstance(v, pd.MultiIndex):
3015                 new_names = [name_dict.get(k, k) for k in v.names]
3016                 index = v.rename(names=new_names)
3017             else:
3018                 index = v.rename(new_name)
3019             indexes[new_name] = index
3020         return indexes
3021 
3022     def _rename_all(self, name_dict, dims_dict):
3023         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3024         dims = self._rename_dims(dims_dict)
3025         indexes = self._rename_indexes(name_dict, dims.keys())
3026         return variables, coord_names, dims, indexes
3027 
3028     def rename(
3029         self,
3030         name_dict: Mapping[Hashable, Hashable] = None,
3031         **names: Hashable,
3032     ) -> "Dataset":
3033         """Returns a new object with renamed variables and dimensions.
3034 
3035         Parameters
3036         ----------
3037         name_dict : dict-like, optional
3038             Dictionary whose keys are current variable or dimension names and
3039             whose values are the desired names.
3040         **names : optional
3041             Keyword form of ``name_dict``.
3042             One of name_dict or names must be provided.
3043 
3044         Returns
3045         -------
3046         renamed : Dataset
3047             Dataset with renamed variables and dimensions.
3048 
3049         See Also
3050         --------
3051         Dataset.swap_dims
3052         Dataset.rename_vars
3053         Dataset.rename_dims
3054         DataArray.rename
3055         """
3056         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3057         for k in name_dict.keys():
3058             if k not in self and k not in self.dims:
3059                 raise ValueError(
3060                     "cannot rename %r because it is not a "
3061                     "variable or dimension in this dataset" % k
3062                 )
3063 
3064         variables, coord_names, dims, indexes = self._rename_all(
3065             name_dict=name_dict, dims_dict=name_dict
3066         )
3067         assert_unique_multiindex_level_names(variables)
3068         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3069 
3070     def rename_dims(
3071         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
3072     ) -> "Dataset":
3073         """Returns a new object with renamed dimensions only.
3074 
3075         Parameters
3076         ----------
3077         dims_dict : dict-like, optional
3078             Dictionary whose keys are current dimension names and
3079             whose values are the desired names. The desired names must
3080             not be the name of an existing dimension or Variable in the Dataset.
3081         **dims : optional
3082             Keyword form of ``dims_dict``.
3083             One of dims_dict or dims must be provided.
3084 
3085         Returns
3086         -------
3087         renamed : Dataset
3088             Dataset with renamed dimensions.
3089 
3090         See Also
3091         --------
3092         Dataset.swap_dims
3093         Dataset.rename
3094         Dataset.rename_vars
3095         DataArray.rename
3096         """
3097         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3098         for k, v in dims_dict.items():
3099             if k not in self.dims:
3100                 raise ValueError(
3101                     "cannot rename %r because it is not a "
3102                     "dimension in this dataset" % k
3103                 )
3104             if v in self.dims or v in self:
3105                 raise ValueError(
3106                     f"Cannot rename {k} to {v} because {v} already exists. "
3107                     "Try using swap_dims instead."
3108                 )
3109 
3110         variables, coord_names, sizes, indexes = self._rename_all(
3111             name_dict={}, dims_dict=dims_dict
3112         )
3113         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3114 
3115     def rename_vars(
3116         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
3117     ) -> "Dataset":
3118         """Returns a new object with renamed variables including coordinates
3119 
3120         Parameters
3121         ----------
3122         name_dict : dict-like, optional
3123             Dictionary whose keys are current variable or coordinate names and
3124             whose values are the desired names.
3125         **names : optional
3126             Keyword form of ``name_dict``.
3127             One of name_dict or names must be provided.
3128 
3129         Returns
3130         -------
3131         renamed : Dataset
3132             Dataset with renamed variables including coordinates
3133 
3134         See Also
3135         --------
3136         Dataset.swap_dims
3137         Dataset.rename
3138         Dataset.rename_dims
3139         DataArray.rename
3140         """
3141         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3142         for k in name_dict:
3143             if k not in self:
3144                 raise ValueError(
3145                     "cannot rename %r because it is not a "
3146                     "variable or coordinate in this dataset" % k
3147                 )
3148         variables, coord_names, dims, indexes = self._rename_all(
3149             name_dict=name_dict, dims_dict={}
3150         )
3151         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3152 
3153     def swap_dims(
3154         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs
3155     ) -> "Dataset":
3156         """Returns a new object with swapped dimensions.
3157 
3158         Parameters
3159         ----------
3160         dims_dict : dict-like
3161             Dictionary whose keys are current dimension names and whose values
3162             are new names.
3163         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3164             The keyword arguments form of ``dims_dict``.
3165             One of dims_dict or dims_kwargs must be provided.
3166 
3167         Returns
3168         -------
3169         swapped : Dataset
3170             Dataset with swapped dimensions.
3171 
3172         Examples
3173         --------
3174         >>> ds = xr.Dataset(
3175         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3176         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3177         ... )
3178         >>> ds
3179         <xarray.Dataset>
3180         Dimensions:  (x: 2)
3181         Coordinates:
3182           * x        (x) <U1 'a' 'b'
3183             y        (x) int64 0 1
3184         Data variables:
3185             a        (x) int64 5 7
3186             b        (x) float64 0.1 2.4
3187 
3188         >>> ds.swap_dims({"x": "y"})
3189         <xarray.Dataset>
3190         Dimensions:  (y: 2)
3191         Coordinates:
3192             x        (y) <U1 'a' 'b'
3193           * y        (y) int64 0 1
3194         Data variables:
3195             a        (y) int64 5 7
3196             b        (y) float64 0.1 2.4
3197 
3198         >>> ds.swap_dims({"x": "z"})
3199         <xarray.Dataset>
3200         Dimensions:  (z: 2)
3201         Coordinates:
3202             x        (z) <U1 'a' 'b'
3203             y        (z) int64 0 1
3204         Dimensions without coordinates: z
3205         Data variables:
3206             a        (z) int64 5 7
3207             b        (z) float64 0.1 2.4
3208 
3209         See Also
3210         --------
3211         Dataset.rename
3212         DataArray.swap_dims
3213         """
3214         # TODO: deprecate this method in favor of a (less confusing)
3215         # rename_dims() method that only renames dimensions.
3216 
3217         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3218         for k, v in dims_dict.items():
3219             if k not in self.dims:
3220                 raise ValueError(
3221                     "cannot swap from dimension %r because it is "
3222                     "not an existing dimension" % k
3223                 )
3224             if v in self.variables and self.variables[v].dims != (k,):
3225                 raise ValueError(
3226                     "replacement dimension %r is not a 1D "
3227                     "variable along the old dimension %r" % (v, k)
3228                 )
3229 
3230         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3231 
3232         coord_names = self._coord_names.copy()
3233         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3234 
3235         variables: Dict[Hashable, Variable] = {}
3236         indexes: Dict[Hashable, pd.Index] = {}
3237         for k, v in self.variables.items():
3238             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3239             if k in result_dims:
3240                 var = v.to_index_variable()
3241                 if k in self.indexes:
3242                     indexes[k] = self.indexes[k]
3243                 else:
3244                     new_index = var.to_index()
3245                     if new_index.nlevels == 1:
3246                         # make sure index name matches dimension name
3247                         new_index = new_index.rename(k)
3248                     indexes[k] = new_index
3249             else:
3250                 var = v.to_base_variable()
3251             var.dims = dims
3252             variables[k] = var
3253 
3254         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3255 
3256     def expand_dims(
3257         self,
3258         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
3259         axis: Union[None, int, Sequence[int]] = None,
3260         **dim_kwargs: Any,
3261     ) -> "Dataset":
3262         """Return a new object with an additional axis (or axes) inserted at
3263         the corresponding position in the array shape.  The new object is a
3264         view into the underlying array, not a copy.
3265 
3266         If dim is already a scalar coordinate, it will be promoted to a 1D
3267         coordinate consisting of a single value.
3268 
3269         Parameters
3270         ----------
3271         dim : hashable, sequence of hashable, mapping, or None
3272             Dimensions to include on the new variable. If provided as hashable
3273             or sequence of hashable, then dimensions are inserted with length
3274             1. If provided as a mapping, then the keys are the new dimensions
3275             and the values are either integers (giving the length of the new
3276             dimensions) or array-like (giving the coordinates of the new
3277             dimensions).
3278         axis : int, sequence of int, or None
3279             Axis position(s) where new axis is to be inserted (position(s) on
3280             the result array). If a list (or tuple) of integers is passed,
3281             multiple axes are inserted. In this case, dim arguments should be
3282             same length list. If axis=None is passed, all the axes will be
3283             inserted to the start of the result array.
3284         **dim_kwargs : int or sequence or ndarray
3285             The keywords are arbitrary dimensions being inserted and the values
3286             are either the lengths of the new dims (if int is given), or their
3287             coordinates. Note, this is an alternative to passing a dict to the
3288             dim kwarg and will only be used if dim is None.
3289 
3290         Returns
3291         -------
3292         expanded : same type as caller
3293             This object, but with an additional dimension(s).
3294         """
3295         if dim is None:
3296             pass
3297         elif isinstance(dim, Mapping):
3298             # We're later going to modify dim in place; don't tamper with
3299             # the input
3300             dim = dict(dim)
3301         elif isinstance(dim, int):
3302             raise TypeError(
3303                 "dim should be hashable or sequence of hashables or mapping"
3304             )
3305         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3306             dim = {dim: 1}
3307         elif isinstance(dim, Sequence):
3308             if len(dim) != len(set(dim)):
3309                 raise ValueError("dims should not contain duplicate values.")
3310             dim = {d: 1 for d in dim}
3311 
3312         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3313         assert isinstance(dim, MutableMapping)
3314 
3315         if axis is None:
3316             axis = list(range(len(dim)))
3317         elif not isinstance(axis, Sequence):
3318             axis = [axis]
3319 
3320         if len(dim) != len(axis):
3321             raise ValueError("lengths of dim and axis should be identical.")
3322         for d in dim:
3323             if d in self.dims:
3324                 raise ValueError(f"Dimension {d} already exists.")
3325             if d in self._variables and not utils.is_scalar(self._variables[d]):
3326                 raise ValueError(
3327                     "{dim} already exists as coordinate or"
3328                     " variable name.".format(dim=d)
3329                 )
3330 
3331         variables: Dict[Hashable, Variable] = {}
3332         coord_names = self._coord_names.copy()
3333         # If dim is a dict, then ensure that the values are either integers
3334         # or iterables.
3335         for k, v in dim.items():
3336             if hasattr(v, "__iter__"):
3337                 # If the value for the new dimension is an iterable, then
3338                 # save the coordinates to the variables dict, and set the
3339                 # value within the dim dict to the length of the iterable
3340                 # for later use.
3341                 variables[k] = xr.IndexVariable((k,), v)
3342                 coord_names.add(k)
3343                 dim[k] = variables[k].size
3344             elif isinstance(v, int):
3345                 pass  # Do nothing if the dimensions value is just an int
3346             else:
3347                 raise TypeError(
3348                     "The value of new dimension {k} must be "
3349                     "an iterable or an int".format(k=k)
3350                 )
3351 
3352         for k, v in self._variables.items():
3353             if k not in dim:
3354                 if k in coord_names:  # Do not change coordinates
3355                     variables[k] = v
3356                 else:
3357                     result_ndim = len(v.dims) + len(axis)
3358                     for a in axis:
3359                         if a < -result_ndim or result_ndim - 1 < a:
3360                             raise IndexError(
3361                                 f"Axis {a} of variable {k} is out of bounds of the "
3362                                 f"expanded dimension size {result_ndim}"
3363                             )
3364 
3365                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3366                     if len(axis_pos) != len(set(axis_pos)):
3367                         raise ValueError("axis should not contain duplicate values")
3368                     # We need to sort them to make sure `axis` equals to the
3369                     # axis positions of the result array.
3370                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3371 
3372                     all_dims = list(zip(v.dims, v.shape))
3373                     for d, c in zip_axis_dim:
3374                         all_dims.insert(d, c)
3375                     variables[k] = v.set_dims(dict(all_dims))
3376             else:
3377                 # If dims includes a label of a non-dimension coordinate,
3378                 # it will be promoted to a 1D coordinate with a single value.
3379                 variables[k] = v.set_dims(k).to_index_variable()
3380 
3381         new_dims = self._dims.copy()
3382         new_dims.update(dim)
3383 
3384         return self._replace_vars_and_dims(
3385             variables, dims=new_dims, coord_names=coord_names
3386         )
3387 
3388     def set_index(
3389         self,
3390         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3391         append: bool = False,
3392         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3393     ) -> "Dataset":
3394         """Set Dataset (multi-)indexes using one or more existing coordinates
3395         or variables.
3396 
3397         Parameters
3398         ----------
3399         indexes : {dim: index, ...}
3400             Mapping from names matching dimensions and values given
3401             by (lists of) the names of existing coordinates or variables to set
3402             as new (multi-)index.
3403         append : bool, optional
3404             If True, append the supplied index(es) to the existing index(es).
3405             Otherwise replace the existing index(es) (default).
3406         **indexes_kwargs : optional
3407             The keyword arguments form of ``indexes``.
3408             One of indexes or indexes_kwargs must be provided.
3409 
3410         Returns
3411         -------
3412         obj : Dataset
3413             Another dataset, with this dataset's data but replaced coordinates.
3414 
3415         Examples
3416         --------
3417         >>> arr = xr.DataArray(
3418         ...     data=np.ones((2, 3)),
3419         ...     dims=["x", "y"],
3420         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3421         ... )
3422         >>> ds = xr.Dataset({"v": arr})
3423         >>> ds
3424         <xarray.Dataset>
3425         Dimensions:  (x: 2, y: 3)
3426         Coordinates:
3427           * x        (x) int64 0 1
3428           * y        (y) int64 0 1 2
3429             a        (x) int64 3 4
3430         Data variables:
3431             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3432         >>> ds.set_index(x="a")
3433         <xarray.Dataset>
3434         Dimensions:  (x: 2, y: 3)
3435         Coordinates:
3436           * x        (x) int64 3 4
3437           * y        (y) int64 0 1 2
3438         Data variables:
3439             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3440 
3441         See Also
3442         --------
3443         Dataset.reset_index
3444         Dataset.swap_dims
3445         """
3446         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3447         variables, coord_names = merge_indexes(
3448             indexes, self._variables, self._coord_names, append=append
3449         )
3450         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3451 
3452     def reset_index(
3453         self,
3454         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3455         drop: bool = False,
3456     ) -> "Dataset":
3457         """Reset the specified index(es) or multi-index level(s).
3458 
3459         Parameters
3460         ----------
3461         dims_or_levels : str or list
3462             Name(s) of the dimension(s) and/or multi-index level(s) that will
3463             be reset.
3464         drop : bool, optional
3465             If True, remove the specified indexes and/or multi-index levels
3466             instead of extracting them as new coordinates (default: False).
3467 
3468         Returns
3469         -------
3470         obj : Dataset
3471             Another dataset, with this dataset's data but replaced coordinates.
3472 
3473         See Also
3474         --------
3475         Dataset.set_index
3476         """
3477         variables, coord_names = split_indexes(
3478             dims_or_levels,
3479             self._variables,
3480             self._coord_names,
3481             cast(Mapping[Hashable, Hashable], self._level_coords),
3482             drop=drop,
3483         )
3484         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3485 
3486     def reorder_levels(
3487         self,
3488         dim_order: Mapping[Hashable, Sequence[int]] = None,
3489         **dim_order_kwargs: Sequence[int],
3490     ) -> "Dataset":
3491         """Rearrange index levels using input order.
3492 
3493         Parameters
3494         ----------
3495         dim_order : optional
3496             Mapping from names matching dimensions and values given
3497             by lists representing new level orders. Every given dimension
3498             must have a multi-index.
3499         **dim_order_kwargs : optional
3500             The keyword arguments form of ``dim_order``.
3501             One of dim_order or dim_order_kwargs must be provided.
3502 
3503         Returns
3504         -------
3505         obj : Dataset
3506             Another dataset, with this dataset's data but replaced
3507             coordinates.
3508         """
3509         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3510         variables = self._variables.copy()
3511         indexes = dict(self.indexes)
3512         for dim, order in dim_order.items():
3513             coord = self._variables[dim]
3514             index = self.indexes[dim]
3515             if not isinstance(index, pd.MultiIndex):
3516                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3517             new_index = index.reorder_levels(order)
3518             variables[dim] = IndexVariable(coord.dims, new_index)
3519             indexes[dim] = new_index
3520 
3521         return self._replace(variables, indexes=indexes)
3522 
3523     def _stack_once(self, dims, new_dim):
3524         if ... in dims:
3525             dims = list(infix_dims(dims, self.dims))
3526         variables = {}
3527         for name, var in self.variables.items():
3528             if name not in dims:
3529                 if any(d in var.dims for d in dims):
3530                     add_dims = [d for d in dims if d not in var.dims]
3531                     vdims = list(var.dims) + add_dims
3532                     shape = [self.dims[d] for d in vdims]
3533                     exp_var = var.set_dims(vdims, shape)
3534                     stacked_var = exp_var.stack(**{new_dim: dims})
3535                     variables[name] = stacked_var
3536                 else:
3537                     variables[name] = var.copy(deep=False)
3538 
3539         # consider dropping levels that are unused?
3540         levels = [self.get_index(dim) for dim in dims]
3541         idx = utils.multiindex_from_product_levels(levels, names=dims)
3542         variables[new_dim] = IndexVariable(new_dim, idx)
3543 
3544         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3545 
3546         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3547         indexes[new_dim] = idx
3548 
3549         return self._replace_with_new_dims(
3550             variables, coord_names=coord_names, indexes=indexes
3551         )
3552 
3553     def stack(
3554         self,
3555         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3556         **dimensions_kwargs: Sequence[Hashable],
3557     ) -> "Dataset":
3558         """
3559         Stack any number of existing dimensions into a single new dimension.
3560 
3561         New dimensions will be added at the end, and the corresponding
3562         coordinate variables will be combined into a MultiIndex.
3563 
3564         Parameters
3565         ----------
3566         dimensions : mapping of hashable to sequence of hashable
3567             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
3568             dimensions, and the existing dimensions that they replace. An
3569             ellipsis (`...`) will be replaced by all unlisted dimensions.
3570             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
3571             all dimensions.
3572         **dimensions_kwargs
3573             The keyword arguments form of ``dimensions``.
3574             One of dimensions or dimensions_kwargs must be provided.
3575 
3576         Returns
3577         -------
3578         stacked : Dataset
3579             Dataset with stacked data.
3580 
3581         See Also
3582         --------
3583         Dataset.unstack
3584         """
3585         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3586         result = self
3587         for new_dim, dims in dimensions.items():
3588             result = result._stack_once(dims, new_dim)
3589         return result
3590 
3591     def to_stacked_array(
3592         self,
3593         new_dim: Hashable,
3594         sample_dims: Sequence[Hashable],
3595         variable_dim: str = "variable",
3596         name: Hashable = None,
3597     ) -> "DataArray":
3598         """Combine variables of differing dimensionality into a DataArray
3599         without broadcasting.
3600 
3601         This method is similar to Dataset.to_array but does not broadcast the
3602         variables.
3603 
3604         Parameters
3605         ----------
3606         new_dim : hashable
3607             Name of the new stacked coordinate
3608         sample_dims : sequence of hashable
3609             Dimensions that **will not** be stacked. Each array in the dataset
3610             must share these dimensions. For machine learning applications,
3611             these define the dimensions over which samples are drawn.
3612         variable_dim : str, optional
3613             Name of the level in the stacked coordinate which corresponds to
3614             the variables.
3615         name : str, optional
3616             Name of the new data array.
3617 
3618         Returns
3619         -------
3620         stacked : DataArray
3621             DataArray with the specified dimensions and data variables
3622             stacked together. The stacked coordinate is named ``new_dim``
3623             and represented by a MultiIndex object with a level containing the
3624             data variable names. The name of this level is controlled using
3625             the ``variable_dim`` argument.
3626 
3627         See Also
3628         --------
3629         Dataset.to_array
3630         Dataset.stack
3631         DataArray.to_unstacked_dataset
3632 
3633         Examples
3634         --------
3635         >>> data = xr.Dataset(
3636         ...     data_vars={
3637         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3638         ...         "b": ("x", [6, 7]),
3639         ...     },
3640         ...     coords={"y": ["u", "v", "w"]},
3641         ... )
3642 
3643         >>> data
3644         <xarray.Dataset>
3645         Dimensions:  (x: 2, y: 3)
3646         Coordinates:
3647           * y        (y) <U1 'u' 'v' 'w'
3648         Dimensions without coordinates: x
3649         Data variables:
3650             a        (x, y) int64 0 1 2 3 4 5
3651             b        (x) int64 6 7
3652 
3653         >>> data.to_stacked_array("z", sample_dims=["x"])
3654         <xarray.DataArray 'a' (x: 2, z: 4)>
3655         array([[0, 1, 2, 6],
3656                [3, 4, 5, 7]])
3657         Coordinates:
3658           * z         (z) MultiIndex
3659           - variable  (z) object 'a' 'a' 'a' 'b'
3660           - y         (z) object 'u' 'v' 'w' nan
3661         Dimensions without coordinates: x
3662 
3663         """
3664         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3665 
3666         for variable in self:
3667             dims = self[variable].dims
3668             dims_include_sample_dims = set(sample_dims) <= set(dims)
3669             if not dims_include_sample_dims:
3670                 raise ValueError(
3671                     "All variables in the dataset must contain the "
3672                     "dimensions {}.".format(dims)
3673                 )
3674 
3675         def ensure_stackable(val):
3676             assign_coords = {variable_dim: val.name}
3677             for dim in stacking_dims:
3678                 if dim not in val.dims:
3679                     assign_coords[dim] = None
3680 
3681             expand_dims = set(stacking_dims).difference(set(val.dims))
3682             expand_dims.add(variable_dim)
3683             # must be list for .expand_dims
3684             expand_dims = list(expand_dims)
3685 
3686             return (
3687                 val.assign_coords(**assign_coords)
3688                 .expand_dims(expand_dims)
3689                 .stack({new_dim: (variable_dim,) + stacking_dims})
3690             )
3691 
3692         # concatenate the arrays
3693         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3694         data_array = xr.concat(stackable_vars, dim=new_dim)
3695 
3696         # coerce the levels of the MultiIndex to have the same type as the
3697         # input dimensions. This code is messy, so it might be better to just
3698         # input a dummy value for the singleton dimension.
3699         idx = data_array.indexes[new_dim]
3700         levels = [idx.levels[0]] + [
3701             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3702         ]
3703         new_idx = idx.set_levels(levels)
3704         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3705 
3706         if name is not None:
3707             data_array.name = name
3708 
3709         return data_array
3710 
3711     def _unstack_once(self, dim: Hashable, fill_value) -> "Dataset":
3712         index = self.get_index(dim)
3713         index = remove_unused_levels_categories(index)
3714 
3715         variables: Dict[Hashable, Variable] = {}
3716         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3717 
3718         for name, var in self.variables.items():
3719             if name != dim:
3720                 if dim in var.dims:
3721                     if isinstance(fill_value, Mapping):
3722                         fill_value_ = fill_value[name]
3723                     else:
3724                         fill_value_ = fill_value
3725 
3726                     variables[name] = var._unstack_once(
3727                         index=index, dim=dim, fill_value=fill_value_
3728                     )
3729                 else:
3730                     variables[name] = var
3731 
3732         for name, lev in zip(index.names, index.levels):
3733             variables[name] = IndexVariable(name, lev)
3734             indexes[name] = lev
3735 
3736         coord_names = set(self._coord_names) - {dim} | set(index.names)
3737 
3738         return self._replace_with_new_dims(
3739             variables, coord_names=coord_names, indexes=indexes
3740         )
3741 
3742     def _unstack_full_reindex(
3743         self, dim: Hashable, fill_value, sparse: bool
3744     ) -> "Dataset":
3745         index = self.get_index(dim)
3746         index = remove_unused_levels_categories(index)
3747         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3748 
3749         # take a shortcut in case the MultiIndex was not modified.
3750         if index.equals(full_idx):
3751             obj = self
3752         else:
3753             obj = self._reindex(
3754                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3755             )
3756 
3757         new_dim_names = index.names
3758         new_dim_sizes = [lev.size for lev in index.levels]
3759 
3760         variables: Dict[Hashable, Variable] = {}
3761         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3762 
3763         for name, var in obj.variables.items():
3764             if name != dim:
3765                 if dim in var.dims:
3766                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3767                     variables[name] = var.unstack({dim: new_dims})
3768                 else:
3769                     variables[name] = var
3770 
3771         for name, lev in zip(new_dim_names, index.levels):
3772             variables[name] = IndexVariable(name, lev)
3773             indexes[name] = lev
3774 
3775         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3776 
3777         return self._replace_with_new_dims(
3778             variables, coord_names=coord_names, indexes=indexes
3779         )
3780 
3781     def unstack(
3782         self,
3783         dim: Union[Hashable, Iterable[Hashable]] = None,
3784         fill_value: Any = dtypes.NA,
3785         sparse: bool = False,
3786     ) -> "Dataset":
3787         """
3788         Unstack existing dimensions corresponding to MultiIndexes into
3789         multiple new dimensions.
3790 
3791         New dimensions will be added at the end.
3792 
3793         Parameters
3794         ----------
3795         dim : hashable or iterable of hashable, optional
3796             Dimension(s) over which to unstack. By default unstacks all
3797             MultiIndexes.
3798         fill_value : scalar or dict-like, default: nan
3799             value to be filled. If a dict-like, maps variable names to
3800             fill values. If not provided or if the dict-like does not
3801             contain all variables, the dtype's NA value will be used.
3802         sparse : bool, default: False
3803             use sparse-array if True
3804 
3805         Returns
3806         -------
3807         unstacked : Dataset
3808             Dataset with unstacked data.
3809 
3810         See Also
3811         --------
3812         Dataset.stack
3813         """
3814         if dim is None:
3815             dims = [
3816                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3817             ]
3818         else:
3819             if isinstance(dim, str) or not isinstance(dim, Iterable):
3820                 dims = [dim]
3821             else:
3822                 dims = list(dim)
3823 
3824             missing_dims = [d for d in dims if d not in self.dims]
3825             if missing_dims:
3826                 raise ValueError(
3827                     "Dataset does not contain the dimensions: %s" % missing_dims
3828                 )
3829 
3830             non_multi_dims = [
3831                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3832             ]
3833             if non_multi_dims:
3834                 raise ValueError(
3835                     "cannot unstack dimensions that do not "
3836                     "have a MultiIndex: %s" % non_multi_dims
3837                 )
3838 
3839         result = self.copy(deep=False)
3840         for dim in dims:
3841 
3842             if (
3843                 # Dask arrays don't support assignment by index, which the fast unstack
3844                 # function requires.
3845                 # https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
3846                 any(is_duck_dask_array(v.data) for v in self.variables.values())
3847                 # Sparse doesn't currently support (though we could special-case
3848                 # it)
3849                 # https://github.com/pydata/sparse/issues/422
3850                 or any(
3851                     isinstance(v.data, sparse_array_type)
3852                     for v in self.variables.values()
3853                 )
3854                 or sparse
3855                 # numpy full_like only added `shape` in 1.17
3856                 or LooseVersion(np.__version__) < LooseVersion("1.17")
3857                 # Until https://github.com/pydata/xarray/pull/4751 is resolved,
3858                 # we check explicitly whether it's a numpy array. Once that is
3859                 # resolved, explicitly exclude pint arrays.
3860                 # # pint doesn't implement `np.full_like` in a way that's
3861                 # # currently compatible.
3862                 # # https://github.com/pydata/xarray/pull/4746#issuecomment-753425173
3863                 # # or any(
3864                 # #     isinstance(v.data, pint_array_type) for v in self.variables.values()
3865                 # # )
3866                 or any(
3867                     not isinstance(v.data, np.ndarray) for v in self.variables.values()
3868                 )
3869             ):
3870                 result = result._unstack_full_reindex(dim, fill_value, sparse)
3871             else:
3872                 result = result._unstack_once(dim, fill_value)
3873         return result
3874 
3875     def update(self, other: "CoercibleMapping") -> "Dataset":
3876         """Update this dataset's variables with those from another dataset.
3877 
3878         Parameters
3879         ----------
3880         other : Dataset or mapping
3881             Variables with which to update this dataset. One of:
3882 
3883             - Dataset
3884             - mapping {var name: DataArray}
3885             - mapping {var name: Variable}
3886             - mapping {var name: (dimension name, array-like)}
3887             - mapping {var name: (tuple of dimension names, array-like)}
3888 
3889         Returns
3890         -------
3891         updated : Dataset
3892             Updated dataset.
3893 
3894         Raises
3895         ------
3896         ValueError
3897             If any dimensions would have inconsistent sizes in the updated
3898             dataset.
3899         """
3900         merge_result = dataset_update_method(self, other)
3901         return self._replace(inplace=True, **merge_result._asdict())
3902 
3903     def merge(
3904         self,
3905         other: Union["CoercibleMapping", "DataArray"],
3906         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3907         compat: str = "no_conflicts",
3908         join: str = "outer",
3909         fill_value: Any = dtypes.NA,
3910     ) -> "Dataset":
3911         """Merge the arrays of two datasets into a single dataset.
3912 
3913         This method generally does not allow for overriding data, with the
3914         exception of attributes, which are ignored on the second dataset.
3915         Variables with the same name are checked for conflicts via the equals
3916         or identical methods.
3917 
3918         Parameters
3919         ----------
3920         other : Dataset or mapping
3921             Dataset or variables to merge with this dataset.
3922         overwrite_vars : hashable or iterable of hashable, optional
3923             If provided, update variables of these name(s) without checking for
3924             conflicts in this dataset.
3925         compat : {"broadcast_equals", "equals", "identical", \
3926                   "no_conflicts"}, optional
3927             String indicating how to compare variables of the same name for
3928             potential conflicts:
3929 
3930             - 'broadcast_equals': all values must be equal when variables are
3931               broadcast against each other to ensure common dimensions.
3932             - 'equals': all values and dimensions must be the same.
3933             - 'identical': all values, dimensions and attributes must be the
3934               same.
3935             - 'no_conflicts': only values which are not null in both datasets
3936               must be equal. The returned dataset then contains the combination
3937               of all non-null values.
3938 
3939         join : {"outer", "inner", "left", "right", "exact"}, optional
3940             Method for joining ``self`` and ``other`` along shared dimensions:
3941 
3942             - 'outer': use the union of the indexes
3943             - 'inner': use the intersection of the indexes
3944             - 'left': use indexes from ``self``
3945             - 'right': use indexes from ``other``
3946             - 'exact': error instead of aligning non-equal indexes
3947         fill_value : scalar or dict-like, optional
3948             Value to use for newly missing values. If a dict-like, maps
3949             variable names (including coordinates) to fill values.
3950 
3951         Returns
3952         -------
3953         merged : Dataset
3954             Merged dataset.
3955 
3956         Raises
3957         ------
3958         MergeError
3959             If any variables conflict (see ``compat``).
3960         """
3961         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3962         merge_result = dataset_merge_method(
3963             self,
3964             other,
3965             overwrite_vars=overwrite_vars,
3966             compat=compat,
3967             join=join,
3968             fill_value=fill_value,
3969         )
3970         return self._replace(**merge_result._asdict())
3971 
3972     def _assert_all_in_dataset(
3973         self, names: Iterable[Hashable], virtual_okay: bool = False
3974     ) -> None:
3975         bad_names = set(names) - set(self._variables)
3976         if virtual_okay:
3977             bad_names -= self.virtual_variables
3978         if bad_names:
3979             raise ValueError(
3980                 "One or more of the specified variables "
3981                 "cannot be found in this dataset"
3982             )
3983 
3984     def drop_vars(
3985         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3986     ) -> "Dataset":
3987         """Drop variables from this dataset.
3988 
3989         Parameters
3990         ----------
3991         names : hashable or iterable of hashable
3992             Name(s) of variables to drop.
3993         errors : {"raise", "ignore"}, optional
3994             If 'raise' (default), raises a ValueError error if any of the variable
3995             passed are not in the dataset. If 'ignore', any given names that are in the
3996             dataset are dropped and no error is raised.
3997 
3998         Returns
3999         -------
4000         dropped : Dataset
4001 
4002         """
4003         # the Iterable check is required for mypy
4004         if is_scalar(names) or not isinstance(names, Iterable):
4005             names = {names}
4006         else:
4007             names = set(names)
4008         if errors == "raise":
4009             self._assert_all_in_dataset(names)
4010 
4011         variables = {k: v for k, v in self._variables.items() if k not in names}
4012         coord_names = {k for k in self._coord_names if k in variables}
4013         indexes = {k: v for k, v in self.indexes.items() if k not in names}
4014         return self._replace_with_new_dims(
4015             variables, coord_names=coord_names, indexes=indexes
4016         )
4017 
4018     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
4019         """Backward compatible method based on `drop_vars` and `drop_sel`
4020 
4021         Using either `drop_vars` or `drop_sel` is encouraged
4022 
4023         See Also
4024         --------
4025         Dataset.drop_vars
4026         Dataset.drop_sel
4027         """
4028         if errors not in ["raise", "ignore"]:
4029             raise ValueError('errors must be either "raise" or "ignore"')
4030 
4031         if is_dict_like(labels) and not isinstance(labels, dict):
4032             warnings.warn(
4033                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
4034                 FutureWarning,
4035                 stacklevel=2,
4036             )
4037             return self.drop_vars(labels, errors=errors)
4038 
4039         if labels_kwargs or isinstance(labels, dict):
4040             if dim is not None:
4041                 raise ValueError("cannot specify dim and dict-like arguments.")
4042             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
4043 
4044         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
4045             warnings.warn(
4046                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
4047                 PendingDeprecationWarning,
4048                 stacklevel=2,
4049             )
4050             return self.drop_vars(labels, errors=errors)
4051         if dim is not None:
4052             warnings.warn(
4053                 "dropping labels using list-like labels is deprecated; using "
4054                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
4055                 DeprecationWarning,
4056                 stacklevel=2,
4057             )
4058             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
4059 
4060         warnings.warn(
4061             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
4062             PendingDeprecationWarning,
4063             stacklevel=2,
4064         )
4065         return self.drop_sel(labels, errors=errors)
4066 
4067     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
4068         """Drop index labels from this dataset.
4069 
4070         Parameters
4071         ----------
4072         labels : mapping of hashable to Any
4073             Index labels to drop
4074         errors : {"raise", "ignore"}, optional
4075             If 'raise' (default), raises a ValueError error if
4076             any of the index labels passed are not
4077             in the dataset. If 'ignore', any given labels that are in the
4078             dataset are dropped and no error is raised.
4079         **labels_kwargs : {dim: label, ...}, optional
4080             The keyword arguments form of ``dim`` and ``labels``
4081 
4082         Returns
4083         -------
4084         dropped : Dataset
4085 
4086         Examples
4087         --------
4088         >>> data = np.arange(6).reshape(2, 3)
4089         >>> labels = ["a", "b", "c"]
4090         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4091         >>> ds
4092         <xarray.Dataset>
4093         Dimensions:  (x: 2, y: 3)
4094         Coordinates:
4095           * y        (y) <U1 'a' 'b' 'c'
4096         Dimensions without coordinates: x
4097         Data variables:
4098             A        (x, y) int64 0 1 2 3 4 5
4099         >>> ds.drop_sel(y=["a", "c"])
4100         <xarray.Dataset>
4101         Dimensions:  (x: 2, y: 1)
4102         Coordinates:
4103           * y        (y) <U1 'b'
4104         Dimensions without coordinates: x
4105         Data variables:
4106             A        (x, y) int64 1 4
4107         >>> ds.drop_sel(y="b")
4108         <xarray.Dataset>
4109         Dimensions:  (x: 2, y: 2)
4110         Coordinates:
4111           * y        (y) <U1 'a' 'c'
4112         Dimensions without coordinates: x
4113         Data variables:
4114             A        (x, y) int64 0 2 3 5
4115         """
4116         if errors not in ["raise", "ignore"]:
4117             raise ValueError('errors must be either "raise" or "ignore"')
4118 
4119         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
4120 
4121         ds = self
4122         for dim, labels_for_dim in labels.items():
4123             # Don't cast to set, as it would harm performance when labels
4124             # is a large numpy array
4125             if utils.is_scalar(labels_for_dim):
4126                 labels_for_dim = [labels_for_dim]
4127             labels_for_dim = np.asarray(labels_for_dim)
4128             try:
4129                 index = self.get_index(dim)
4130             except KeyError:
4131                 raise ValueError("dimension %r does not have coordinate labels" % dim)
4132             new_index = index.drop(labels_for_dim, errors=errors)
4133             ds = ds.loc[{dim: new_index}]
4134         return ds
4135 
4136     def drop_isel(self, indexers=None, **indexers_kwargs):
4137         """Drop index positions from this Dataset.
4138 
4139         Parameters
4140         ----------
4141         indexers : mapping of hashable to Any
4142             Index locations to drop
4143         **indexers_kwargs : {dim: position, ...}, optional
4144             The keyword arguments form of ``dim`` and ``positions``
4145 
4146         Returns
4147         -------
4148         dropped : Dataset
4149 
4150         Raises
4151         ------
4152         IndexError
4153 
4154         Examples
4155         --------
4156         >>> data = np.arange(6).reshape(2, 3)
4157         >>> labels = ["a", "b", "c"]
4158         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4159         >>> ds
4160         <xarray.Dataset>
4161         Dimensions:  (x: 2, y: 3)
4162         Coordinates:
4163           * y        (y) <U1 'a' 'b' 'c'
4164         Dimensions without coordinates: x
4165         Data variables:
4166             A        (x, y) int64 0 1 2 3 4 5
4167         >>> ds.drop_isel(y=[0, 2])
4168         <xarray.Dataset>
4169         Dimensions:  (x: 2, y: 1)
4170         Coordinates:
4171           * y        (y) <U1 'b'
4172         Dimensions without coordinates: x
4173         Data variables:
4174             A        (x, y) int64 1 4
4175         >>> ds.drop_isel(y=1)
4176         <xarray.Dataset>
4177         Dimensions:  (x: 2, y: 2)
4178         Coordinates:
4179           * y        (y) <U1 'a' 'c'
4180         Dimensions without coordinates: x
4181         Data variables:
4182             A        (x, y) int64 0 2 3 5
4183         """
4184 
4185         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
4186 
4187         ds = self
4188         dimension_index = {}
4189         for dim, pos_for_dim in indexers.items():
4190             # Don't cast to set, as it would harm performance when labels
4191             # is a large numpy array
4192             if utils.is_scalar(pos_for_dim):
4193                 pos_for_dim = [pos_for_dim]
4194             pos_for_dim = np.asarray(pos_for_dim)
4195             index = self.get_index(dim)
4196             new_index = index.delete(pos_for_dim)
4197             dimension_index[dim] = new_index
4198         ds = ds.loc[dimension_index]
4199         return ds
4200 
4201     def drop_dims(
4202         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
4203     ) -> "Dataset":
4204         """Drop dimensions and associated variables from this dataset.
4205 
4206         Parameters
4207         ----------
4208         drop_dims : hashable or iterable of hashable
4209             Dimension or dimensions to drop.
4210         errors : {"raise", "ignore"}, optional
4211             If 'raise' (default), raises a ValueError error if any of the
4212             dimensions passed are not in the dataset. If 'ignore', any given
4213             labels that are in the dataset are dropped and no error is raised.
4214 
4215         Returns
4216         -------
4217         obj : Dataset
4218             The dataset without the given dimensions (or any variables
4219             containing those dimensions)
4220         errors : {"raise", "ignore"}, optional
4221             If 'raise' (default), raises a ValueError error if
4222             any of the dimensions passed are not
4223             in the dataset. If 'ignore', any given dimensions that are in the
4224             dataset are dropped and no error is raised.
4225         """
4226         if errors not in ["raise", "ignore"]:
4227             raise ValueError('errors must be either "raise" or "ignore"')
4228 
4229         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
4230             drop_dims = {drop_dims}
4231         else:
4232             drop_dims = set(drop_dims)
4233 
4234         if errors == "raise":
4235             missing_dims = drop_dims - set(self.dims)
4236             if missing_dims:
4237                 raise ValueError(
4238                     "Dataset does not contain the dimensions: %s" % missing_dims
4239                 )
4240 
4241         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
4242         return self.drop_vars(drop_vars)
4243 
4244     def transpose(self, *dims: Hashable) -> "Dataset":
4245         """Return a new Dataset object with all array dimensions transposed.
4246 
4247         Although the order of dimensions on each array will change, the dataset
4248         dimensions themselves will remain in fixed (sorted) order.
4249 
4250         Parameters
4251         ----------
4252         *dims : hashable, optional
4253             By default, reverse the dimensions on each array. Otherwise,
4254             reorder the dimensions to this order.
4255 
4256         Returns
4257         -------
4258         transposed : Dataset
4259             Each array in the dataset (including) coordinates will be
4260             transposed to the given order.
4261 
4262         Notes
4263         -----
4264         This operation returns a view of each array's data. It is
4265         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
4266         -- the data will be fully loaded into memory.
4267 
4268         See Also
4269         --------
4270         numpy.transpose
4271         DataArray.transpose
4272         """
4273         if dims:
4274             if set(dims) ^ set(self.dims) and ... not in dims:
4275                 raise ValueError(
4276                     "arguments to transpose (%s) must be "
4277                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
4278                 )
4279         ds = self.copy()
4280         for name, var in self._variables.items():
4281             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
4282             ds._variables[name] = var.transpose(*var_dims)
4283         return ds
4284 
4285     def dropna(
4286         self,
4287         dim: Hashable,
4288         how: str = "any",
4289         thresh: int = None,
4290         subset: Iterable[Hashable] = None,
4291     ):
4292         """Returns a new dataset with dropped labels for missing values along
4293         the provided dimension.
4294 
4295         Parameters
4296         ----------
4297         dim : hashable
4298             Dimension along which to drop missing values. Dropping along
4299             multiple dimensions simultaneously is not yet supported.
4300         how : {"any", "all"}, default: "any"
4301             * any : if any NA values are present, drop that label
4302             * all : if all values are NA, drop that label
4303         thresh : int, default: None
4304             If supplied, require this many non-NA values.
4305         subset : iterable of hashable, optional
4306             Which variables to check for missing values. By default, all
4307             variables in the dataset are checked.
4308 
4309         Returns
4310         -------
4311         Dataset
4312         """
4313         # TODO: consider supporting multiple dimensions? Or not, given that
4314         # there are some ugly edge cases, e.g., pandas's dropna differs
4315         # depending on the order of the supplied axes.
4316 
4317         if dim not in self.dims:
4318             raise ValueError("%s must be a single dataset dimension" % dim)
4319 
4320         if subset is None:
4321             subset = iter(self.data_vars)
4322 
4323         count = np.zeros(self.dims[dim], dtype=np.int64)
4324         size = 0
4325 
4326         for k in subset:
4327             array = self._variables[k]
4328             if dim in array.dims:
4329                 dims = [d for d in array.dims if d != dim]
4330                 count += np.asarray(array.count(dims))  # type: ignore
4331                 size += np.prod([self.dims[d] for d in dims])
4332 
4333         if thresh is not None:
4334             mask = count >= thresh
4335         elif how == "any":
4336             mask = count == size
4337         elif how == "all":
4338             mask = count > 0
4339         elif how is not None:
4340             raise ValueError("invalid how option: %s" % how)
4341         else:
4342             raise TypeError("must specify how or thresh")
4343 
4344         return self.isel({dim: mask})
4345 
4346     def fillna(self, value: Any) -> "Dataset":
4347         """Fill missing values in this object.
4348 
4349         This operation follows the normal broadcasting and alignment rules that
4350         xarray uses for binary arithmetic, except the result is aligned to this
4351         object (``join='left'``) instead of aligned to the intersection of
4352         index coordinates (``join='inner'``).
4353 
4354         Parameters
4355         ----------
4356         value : scalar, ndarray, DataArray, dict or Dataset
4357             Used to fill all matching missing values in this dataset's data
4358             variables. Scalars, ndarrays or DataArrays arguments are used to
4359             fill all data with aligned coordinates (for DataArrays).
4360             Dictionaries or datasets match data variables and then align
4361             coordinates if necessary.
4362 
4363         Returns
4364         -------
4365         Dataset
4366 
4367         Examples
4368         --------
4369         >>> import numpy as np
4370         >>> import xarray as xr
4371         >>> ds = xr.Dataset(
4372         ...     {
4373         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
4374         ...         "B": ("x", [3, 4, np.nan, 1]),
4375         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
4376         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
4377         ...     },
4378         ...     coords={"x": [0, 1, 2, 3]},
4379         ... )
4380         >>> ds
4381         <xarray.Dataset>
4382         Dimensions:  (x: 4)
4383         Coordinates:
4384           * x        (x) int64 0 1 2 3
4385         Data variables:
4386             A        (x) float64 nan 2.0 nan 0.0
4387             B        (x) float64 3.0 4.0 nan 1.0
4388             C        (x) float64 nan nan nan 5.0
4389             D        (x) float64 nan 3.0 nan 4.0
4390 
4391         Replace all `NaN` values with 0s.
4392 
4393         >>> ds.fillna(0)
4394         <xarray.Dataset>
4395         Dimensions:  (x: 4)
4396         Coordinates:
4397           * x        (x) int64 0 1 2 3
4398         Data variables:
4399             A        (x) float64 0.0 2.0 0.0 0.0
4400             B        (x) float64 3.0 4.0 0.0 1.0
4401             C        (x) float64 0.0 0.0 0.0 5.0
4402             D        (x) float64 0.0 3.0 0.0 4.0
4403 
4404         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
4405 
4406         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4407         >>> ds.fillna(value=values)
4408         <xarray.Dataset>
4409         Dimensions:  (x: 4)
4410         Coordinates:
4411           * x        (x) int64 0 1 2 3
4412         Data variables:
4413             A        (x) float64 0.0 2.0 0.0 0.0
4414             B        (x) float64 3.0 4.0 1.0 1.0
4415             C        (x) float64 2.0 2.0 2.0 5.0
4416             D        (x) float64 3.0 3.0 3.0 4.0
4417         """
4418         if utils.is_dict_like(value):
4419             value_keys = getattr(value, "data_vars", value).keys()
4420             if not set(value_keys) <= set(self.data_vars.keys()):
4421                 raise ValueError(
4422                     "all variables in the argument to `fillna` "
4423                     "must be contained in the original dataset"
4424                 )
4425         out = ops.fillna(self, value)
4426         return out
4427 
4428     def interpolate_na(
4429         self,
4430         dim: Hashable = None,
4431         method: str = "linear",
4432         limit: int = None,
4433         use_coordinate: Union[bool, Hashable] = True,
4434         max_gap: Union[
4435             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4436         ] = None,
4437         **kwargs: Any,
4438     ) -> "Dataset":
4439         """Fill in NaNs by interpolating according to different methods.
4440 
4441         Parameters
4442         ----------
4443         dim : str
4444             Specifies the dimension along which to interpolate.
4445         method : str, optional
4446             String indicating which method to use for interpolation:
4447 
4448             - 'linear': linear interpolation (Default). Additional keyword
4449               arguments are passed to :py:func:`numpy.interp`
4450             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4451               are passed to :py:func:`scipy.interpolate.interp1d`. If
4452               ``method='polynomial'``, the ``order`` keyword argument must also be
4453               provided.
4454             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4455               respective :py:class:`scipy.interpolate` classes.
4456         use_coordinate : bool, str, default: True
4457             Specifies which index to use as the x values in the interpolation
4458             formulated as `y = f(x)`. If False, values are treated as if
4459             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4460             used. If ``use_coordinate`` is a string, it specifies the name of a
4461             coordinate variariable to use as the index.
4462         limit : int, default: None
4463             Maximum number of consecutive NaNs to fill. Must be greater than 0
4464             or None for no limit. This filling is done regardless of the size of
4465             the gap in the data. To only interpolate over gaps less than a given length,
4466             see ``max_gap``.
4467         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
4468             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4469             Use None for no limit. When interpolating along a datetime64 dimension
4470             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4471 
4472             - a string that is valid input for pandas.to_timedelta
4473             - a :py:class:`numpy.timedelta64` object
4474             - a :py:class:`pandas.Timedelta` object
4475             - a :py:class:`datetime.timedelta` object
4476 
4477             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4478             dimensions has not been implemented yet. Gap length is defined as the difference
4479             between coordinate values at the first data point after a gap and the last value
4480             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4481             between coordinate values at the first (last) valid data point and the first (last) NaN.
4482             For example, consider::
4483 
4484                 <xarray.DataArray (x: 9)>
4485                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4486                 Coordinates:
4487                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4488 
4489             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4490         kwargs : dict, optional
4491             parameters passed verbatim to the underlying interpolation function
4492 
4493         Returns
4494         -------
4495         interpolated: Dataset
4496             Filled in Dataset.
4497 
4498         See Also
4499         --------
4500         numpy.interp
4501         scipy.interpolate
4502 
4503         Examples
4504         --------
4505         >>> ds = xr.Dataset(
4506         ...     {
4507         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
4508         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
4509         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
4510         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
4511         ...     },
4512         ...     coords={"x": [0, 1, 2, 3, 4]},
4513         ... )
4514         >>> ds
4515         <xarray.Dataset>
4516         Dimensions:  (x: 5)
4517         Coordinates:
4518           * x        (x) int64 0 1 2 3 4
4519         Data variables:
4520             A        (x) float64 nan 2.0 3.0 nan 0.0
4521             B        (x) float64 3.0 4.0 nan 1.0 7.0
4522             C        (x) float64 nan nan nan 5.0 0.0
4523             D        (x) float64 nan 3.0 nan -1.0 4.0
4524 
4525         >>> ds.interpolate_na(dim="x", method="linear")
4526         <xarray.Dataset>
4527         Dimensions:  (x: 5)
4528         Coordinates:
4529           * x        (x) int64 0 1 2 3 4
4530         Data variables:
4531             A        (x) float64 nan 2.0 3.0 1.5 0.0
4532             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4533             C        (x) float64 nan nan nan 5.0 0.0
4534             D        (x) float64 nan 3.0 1.0 -1.0 4.0
4535 
4536         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
4537         <xarray.Dataset>
4538         Dimensions:  (x: 5)
4539         Coordinates:
4540           * x        (x) int64 0 1 2 3 4
4541         Data variables:
4542             A        (x) float64 1.0 2.0 3.0 1.5 0.0
4543             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4544             C        (x) float64 20.0 15.0 10.0 5.0 0.0
4545             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
4546         """
4547         from .missing import _apply_over_vars_with_dim, interp_na
4548 
4549         new = _apply_over_vars_with_dim(
4550             interp_na,
4551             self,
4552             dim=dim,
4553             method=method,
4554             limit=limit,
4555             use_coordinate=use_coordinate,
4556             max_gap=max_gap,
4557             **kwargs,
4558         )
4559         return new
4560 
4561     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4562         """Fill NaN values by propogating values forward
4563 
4564         *Requires bottleneck.*
4565 
4566         Parameters
4567         ----------
4568         dim : Hashable
4569             Specifies the dimension along which to propagate values when
4570             filling.
4571         limit : int, default: None
4572             The maximum number of consecutive NaN values to forward fill. In
4573             other words, if there is a gap with more than this number of
4574             consecutive NaNs, it will only be partially filled. Must be greater
4575             than 0 or None for no limit.
4576 
4577         Returns
4578         -------
4579         Dataset
4580         """
4581         from .missing import _apply_over_vars_with_dim, ffill
4582 
4583         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4584         return new
4585 
4586     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4587         """Fill NaN values by propogating values backward
4588 
4589         *Requires bottleneck.*
4590 
4591         Parameters
4592         ----------
4593         dim : str
4594             Specifies the dimension along which to propagate values when
4595             filling.
4596         limit : int, default: None
4597             The maximum number of consecutive NaN values to backward fill. In
4598             other words, if there is a gap with more than this number of
4599             consecutive NaNs, it will only be partially filled. Must be greater
4600             than 0 or None for no limit.
4601 
4602         Returns
4603         -------
4604         Dataset
4605         """
4606         from .missing import _apply_over_vars_with_dim, bfill
4607 
4608         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4609         return new
4610 
4611     def combine_first(self, other: "Dataset") -> "Dataset":
4612         """Combine two Datasets, default to data_vars of self.
4613 
4614         The new coordinates follow the normal broadcasting and alignment rules
4615         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4616         filled with np.nan.
4617 
4618         Parameters
4619         ----------
4620         other : Dataset
4621             Used to fill all matching missing values in this array.
4622 
4623         Returns
4624         -------
4625         Dataset
4626         """
4627         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4628         return out
4629 
4630     def reduce(
4631         self,
4632         func: Callable,
4633         dim: Union[Hashable, Iterable[Hashable]] = None,
4634         keep_attrs: bool = None,
4635         keepdims: bool = False,
4636         numeric_only: bool = False,
4637         **kwargs: Any,
4638     ) -> "Dataset":
4639         """Reduce this dataset by applying `func` along some dimension(s).
4640 
4641         Parameters
4642         ----------
4643         func : callable
4644             Function which can be called in the form
4645             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4646             np.ndarray over an integer valued axis.
4647         dim : str or sequence of str, optional
4648             Dimension(s) over which to apply `func`.  By default `func` is
4649             applied over all dimensions.
4650         keep_attrs : bool, optional
4651             If True, the dataset's attributes (`attrs`) will be copied from
4652             the original object to the new one.  If False (default), the new
4653             object will be returned without attributes.
4654         keepdims : bool, default: False
4655             If True, the dimensions which are reduced are left in the result
4656             as dimensions of size one. Coordinates that use these dimensions
4657             are removed.
4658         numeric_only : bool, optional
4659             If True, only apply ``func`` to variables with a numeric dtype.
4660         **kwargs : Any
4661             Additional keyword arguments passed on to ``func``.
4662 
4663         Returns
4664         -------
4665         reduced : Dataset
4666             Dataset with this object's DataArrays replaced with new DataArrays
4667             of summarized data and the indicated dimension(s) removed.
4668         """
4669         if dim is None or dim is ...:
4670             dims = set(self.dims)
4671         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4672             dims = {dim}
4673         else:
4674             dims = set(dim)
4675 
4676         missing_dimensions = [d for d in dims if d not in self.dims]
4677         if missing_dimensions:
4678             raise ValueError(
4679                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4680             )
4681 
4682         if keep_attrs is None:
4683             keep_attrs = _get_keep_attrs(default=False)
4684 
4685         variables: Dict[Hashable, Variable] = {}
4686         for name, var in self._variables.items():
4687             reduce_dims = [d for d in var.dims if d in dims]
4688             if name in self.coords:
4689                 if not reduce_dims:
4690                     variables[name] = var
4691             else:
4692                 if (
4693                     not numeric_only
4694                     or np.issubdtype(var.dtype, np.number)
4695                     or (var.dtype == np.bool_)
4696                 ):
4697                     if len(reduce_dims) == 1:
4698                         # unpack dimensions for the benefit of functions
4699                         # like np.argmin which can't handle tuple arguments
4700                         (reduce_dims,) = reduce_dims
4701                     elif len(reduce_dims) == var.ndim:
4702                         # prefer to aggregate over axis=None rather than
4703                         # axis=(0, 1) if they will be equivalent, because
4704                         # the former is often more efficient
4705                         reduce_dims = None  # type: ignore
4706                     variables[name] = var.reduce(
4707                         func,
4708                         dim=reduce_dims,
4709                         keep_attrs=keep_attrs,
4710                         keepdims=keepdims,
4711                         **kwargs,
4712                     )
4713 
4714         coord_names = {k for k in self.coords if k in variables}
4715         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4716         attrs = self.attrs if keep_attrs else None
4717         return self._replace_with_new_dims(
4718             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4719         )
4720 
4721     def map(
4722         self,
4723         func: Callable,
4724         keep_attrs: bool = None,
4725         args: Iterable[Any] = (),
4726         **kwargs: Any,
4727     ) -> "Dataset":
4728         """Apply a function to each variable in this dataset
4729 
4730         Parameters
4731         ----------
4732         func : callable
4733             Function which can be called in the form `func(x, *args, **kwargs)`
4734             to transform each DataArray `x` in this dataset into another
4735             DataArray.
4736         keep_attrs : bool, optional
4737             If True, the dataset's attributes (`attrs`) will be copied from
4738             the original object to the new one. If False, the new object will
4739             be returned without attributes.
4740         args : tuple, optional
4741             Positional arguments passed on to `func`.
4742         **kwargs : Any
4743             Keyword arguments passed on to `func`.
4744 
4745         Returns
4746         -------
4747         applied : Dataset
4748             Resulting dataset from applying ``func`` to each data variable.
4749 
4750         Examples
4751         --------
4752         >>> da = xr.DataArray(np.random.randn(2, 3))
4753         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4754         >>> ds
4755         <xarray.Dataset>
4756         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4757         Dimensions without coordinates: dim_0, dim_1, x
4758         Data variables:
4759             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
4760             bar      (x) int64 -1 2
4761         >>> ds.map(np.fabs)
4762         <xarray.Dataset>
4763         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4764         Dimensions without coordinates: dim_0, dim_1, x
4765         Data variables:
4766             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
4767             bar      (x) float64 1.0 2.0
4768         """
4769         if keep_attrs is None:
4770             keep_attrs = _get_keep_attrs(default=False)
4771         variables = {
4772             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4773             for k, v in self.data_vars.items()
4774         }
4775         if keep_attrs:
4776             for k, v in variables.items():
4777                 v._copy_attrs_from(self.data_vars[k])
4778         attrs = self.attrs if keep_attrs else None
4779         return type(self)(variables, attrs=attrs)
4780 
4781     def apply(
4782         self,
4783         func: Callable,
4784         keep_attrs: bool = None,
4785         args: Iterable[Any] = (),
4786         **kwargs: Any,
4787     ) -> "Dataset":
4788         """
4789         Backward compatible implementation of ``map``
4790 
4791         See Also
4792         --------
4793         Dataset.map
4794         """
4795         warnings.warn(
4796             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4797             PendingDeprecationWarning,
4798             stacklevel=2,
4799         )
4800         return self.map(func, keep_attrs, args, **kwargs)
4801 
4802     def assign(
4803         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4804     ) -> "Dataset":
4805         """Assign new data variables to a Dataset, returning a new object
4806         with all the original variables in addition to the new ones.
4807 
4808         Parameters
4809         ----------
4810         variables : mapping of hashable to Any
4811             Mapping from variables names to the new values. If the new values
4812             are callable, they are computed on the Dataset and assigned to new
4813             data variables. If the values are not callable, (e.g. a DataArray,
4814             scalar, or array), they are simply assigned.
4815         **variables_kwargs
4816             The keyword arguments form of ``variables``.
4817             One of variables or variables_kwargs must be provided.
4818 
4819         Returns
4820         -------
4821         ds : Dataset
4822             A new Dataset with the new variables in addition to all the
4823             existing variables.
4824 
4825         Notes
4826         -----
4827         Since ``kwargs`` is a dictionary, the order of your arguments may not
4828         be preserved, and so the order of the new variables is not well
4829         defined. Assigning multiple variables within the same ``assign`` is
4830         possible, but you cannot reference other variables created within the
4831         same ``assign`` call.
4832 
4833         See Also
4834         --------
4835         pandas.DataFrame.assign
4836 
4837         Examples
4838         --------
4839         >>> x = xr.Dataset(
4840         ...     {
4841         ...         "temperature_c": (
4842         ...             ("lat", "lon"),
4843         ...             20 * np.random.rand(4).reshape(2, 2),
4844         ...         ),
4845         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4846         ...     },
4847         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4848         ... )
4849         >>> x
4850         <xarray.Dataset>
4851         Dimensions:        (lat: 2, lon: 2)
4852         Coordinates:
4853           * lat            (lat) int64 10 20
4854           * lon            (lon) int64 150 160
4855         Data variables:
4856             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4857             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4858 
4859         Where the value is a callable, evaluated on dataset:
4860 
4861         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4862         <xarray.Dataset>
4863         Dimensions:        (lat: 2, lon: 2)
4864         Coordinates:
4865           * lat            (lat) int64 10 20
4866           * lon            (lon) int64 150 160
4867         Data variables:
4868             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4869             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4870             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4871 
4872         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4873 
4874         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4875         <xarray.Dataset>
4876         Dimensions:        (lat: 2, lon: 2)
4877         Coordinates:
4878           * lat            (lat) int64 10 20
4879           * lon            (lon) int64 150 160
4880         Data variables:
4881             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4882             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4883             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4884 
4885         """
4886         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4887         data = self.copy()
4888         # do all calculations first...
4889         results = data._calc_assign_results(variables)
4890         # ... and then assign
4891         data.update(results)
4892         return data
4893 
4894     def to_array(self, dim="variable", name=None):
4895         """Convert this dataset into an xarray.DataArray
4896 
4897         The data variables of this dataset will be broadcast against each other
4898         and stacked along the first axis of the new array. All coordinates of
4899         this dataset will remain coordinates.
4900 
4901         Parameters
4902         ----------
4903         dim : str, optional
4904             Name of the new dimension.
4905         name : str, optional
4906             Name of the new data array.
4907 
4908         Returns
4909         -------
4910         array : xarray.DataArray
4911         """
4912         from .dataarray import DataArray
4913 
4914         data_vars = [self.variables[k] for k in self.data_vars]
4915         broadcast_vars = broadcast_variables(*data_vars)
4916         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4917 
4918         coords = dict(self.coords)
4919         coords[dim] = list(self.data_vars)
4920         indexes = propagate_indexes(self._indexes)
4921 
4922         dims = (dim,) + broadcast_vars[0].dims
4923 
4924         return DataArray(
4925             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4926         )
4927 
4928     def _normalize_dim_order(
4929         self, dim_order: List[Hashable] = None
4930     ) -> Dict[Hashable, int]:
4931         """
4932         Check the validity of the provided dimensions if any and return the mapping
4933         between dimension name and their size.
4934 
4935         Parameters
4936         ----------
4937         dim_order
4938             Dimension order to validate (default to the alphabetical order if None).
4939 
4940         Returns
4941         -------
4942         result
4943             Validated dimensions mapping.
4944 
4945         """
4946         if dim_order is None:
4947             dim_order = list(self.dims)
4948         elif set(dim_order) != set(self.dims):
4949             raise ValueError(
4950                 "dim_order {} does not match the set of dimensions of this "
4951                 "Dataset: {}".format(dim_order, list(self.dims))
4952             )
4953 
4954         ordered_dims = {k: self.dims[k] for k in dim_order}
4955 
4956         return ordered_dims
4957 
4958     def _to_dataframe(self, ordered_dims: Mapping[Hashable, int]):
4959         columns = [k for k in self.variables if k not in self.dims]
4960         data = [
4961             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4962             for k in columns
4963         ]
4964         index = self.coords.to_index([*ordered_dims])
4965         return pd.DataFrame(dict(zip(columns, data)), index=index)
4966 
4967     def to_dataframe(self, dim_order: List[Hashable] = None) -> pd.DataFrame:
4968         """Convert this dataset into a pandas.DataFrame.
4969 
4970         Non-index variables in this dataset form the columns of the
4971         DataFrame. The DataFrame is indexed by the Cartesian product of
4972         this dataset's indices.
4973 
4974         Parameters
4975         ----------
4976         dim_order
4977             Hierarchical dimension order for the resulting dataframe. All
4978             arrays are transposed to this order and then written out as flat
4979             vectors in contiguous order, so the last dimension in this list
4980             will be contiguous in the resulting DataFrame. This has a major
4981             influence on which operations are efficient on the resulting
4982             dataframe.
4983 
4984             If provided, must include all dimensions of this dataset. By
4985             default, dimensions are sorted alphabetically.
4986 
4987         Returns
4988         -------
4989         result
4990             Dataset as a pandas DataFrame.
4991 
4992         """
4993 
4994         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
4995 
4996         return self._to_dataframe(ordered_dims=ordered_dims)
4997 
4998     def _set_sparse_data_from_dataframe(
4999         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
5000     ) -> None:
5001         from sparse import COO
5002 
5003         if isinstance(idx, pd.MultiIndex):
5004             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
5005             is_sorted = idx.is_lexsorted()
5006             shape = tuple(lev.size for lev in idx.levels)
5007         else:
5008             coords = np.arange(idx.size).reshape(1, -1)
5009             is_sorted = True
5010             shape = (idx.size,)
5011 
5012         for name, values in arrays:
5013             # In virtually all real use cases, the sparse array will now have
5014             # missing values and needs a fill_value. For consistency, don't
5015             # special case the rare exceptions (e.g., dtype=int without a
5016             # MultiIndex).
5017             dtype, fill_value = dtypes.maybe_promote(values.dtype)
5018             values = np.asarray(values, dtype=dtype)
5019 
5020             data = COO(
5021                 coords,
5022                 values,
5023                 shape,
5024                 has_duplicates=False,
5025                 sorted=is_sorted,
5026                 fill_value=fill_value,
5027             )
5028             self[name] = (dims, data)
5029 
5030     def _set_numpy_data_from_dataframe(
5031         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
5032     ) -> None:
5033         if not isinstance(idx, pd.MultiIndex):
5034             for name, values in arrays:
5035                 self[name] = (dims, values)
5036             return
5037 
5038         # NB: similar, more general logic, now exists in
5039         # variable.unstack_once; we could consider combining them at some
5040         # point.
5041 
5042         shape = tuple(lev.size for lev in idx.levels)
5043         indexer = tuple(idx.codes)
5044 
5045         # We already verified that the MultiIndex has all unique values, so
5046         # there are missing values if and only if the size of output arrays is
5047         # larger that the index.
5048         missing_values = np.prod(shape) > idx.shape[0]
5049 
5050         for name, values in arrays:
5051             # NumPy indexing is much faster than using DataFrame.reindex() to
5052             # fill in missing values:
5053             # https://stackoverflow.com/a/35049899/809705
5054             if missing_values:
5055                 dtype, fill_value = dtypes.maybe_promote(values.dtype)
5056                 data = np.full(shape, fill_value, dtype)
5057             else:
5058                 # If there are no missing values, keep the existing dtype
5059                 # instead of promoting to support NA, e.g., keep integer
5060                 # columns as integers.
5061                 # TODO: consider removing this special case, which doesn't
5062                 # exist for sparse=True.
5063                 data = np.zeros(shape, values.dtype)
5064             data[indexer] = values
5065             self[name] = (dims, data)
5066 
5067     @classmethod
5068     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
5069         """Convert a pandas.DataFrame into an xarray.Dataset
5070 
5071         Each column will be converted into an independent variable in the
5072         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
5073         into a tensor product of one-dimensional indices (filling in missing
5074         values with NaN). This method will produce a Dataset very similar to
5075         that on which the 'to_dataframe' method was called, except with
5076         possibly redundant dimensions (since all dataset variables will have
5077         the same dimensionality)
5078 
5079         Parameters
5080         ----------
5081         dataframe : DataFrame
5082             DataFrame from which to copy data and indices.
5083         sparse : bool, default: False
5084             If true, create a sparse arrays instead of dense numpy arrays. This
5085             can potentially save a large amount of memory if the DataFrame has
5086             a MultiIndex. Requires the sparse package (sparse.pydata.org).
5087 
5088         Returns
5089         -------
5090         New Dataset.
5091 
5092         See Also
5093         --------
5094         xarray.DataArray.from_series
5095         pandas.DataFrame.to_xarray
5096         """
5097         # TODO: Add an option to remove dimensions along which the variables
5098         # are constant, to enable consistent serialization to/from a dataframe,
5099         # even if some variables have different dimensionality.
5100 
5101         if not dataframe.columns.is_unique:
5102             raise ValueError("cannot convert DataFrame with non-unique columns")
5103 
5104         idx = remove_unused_levels_categories(dataframe.index)
5105 
5106         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
5107             raise ValueError(
5108                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
5109             )
5110 
5111         # Cast to a NumPy array first, in case the Series is a pandas Extension
5112         # array (which doesn't have a valid NumPy dtype)
5113         # TODO: allow users to control how this casting happens, e.g., by
5114         # forwarding arguments to pandas.Series.to_numpy?
5115         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
5116 
5117         obj = cls()
5118 
5119         if isinstance(idx, pd.MultiIndex):
5120             dims = tuple(
5121                 name if name is not None else "level_%i" % n
5122                 for n, name in enumerate(idx.names)
5123             )
5124             for dim, lev in zip(dims, idx.levels):
5125                 obj[dim] = (dim, lev)
5126         else:
5127             index_name = idx.name if idx.name is not None else "index"
5128             dims = (index_name,)
5129             obj[index_name] = (dims, idx)
5130 
5131         if sparse:
5132             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
5133         else:
5134             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
5135         return obj
5136 
5137     def to_dask_dataframe(self, dim_order=None, set_index=False):
5138         """
5139         Convert this dataset into a dask.dataframe.DataFrame.
5140 
5141         The dimensions, coordinates and data variables in this dataset form
5142         the columns of the DataFrame.
5143 
5144         Parameters
5145         ----------
5146         dim_order : list, optional
5147             Hierarchical dimension order for the resulting dataframe. All
5148             arrays are transposed to this order and then written out as flat
5149             vectors in contiguous order, so the last dimension in this list
5150             will be contiguous in the resulting DataFrame. This has a major
5151             influence on which operations are efficient on the resulting dask
5152             dataframe.
5153 
5154             If provided, must include all dimensions of this dataset. By
5155             default, dimensions are sorted alphabetically.
5156         set_index : bool, optional
5157             If set_index=True, the dask DataFrame is indexed by this dataset's
5158             coordinate. Since dask DataFrames do not support multi-indexes,
5159             set_index only works if the dataset only contains one dimension.
5160 
5161         Returns
5162         -------
5163         dask.dataframe.DataFrame
5164         """
5165 
5166         import dask.array as da
5167         import dask.dataframe as dd
5168 
5169         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
5170 
5171         columns = list(ordered_dims)
5172         columns.extend(k for k in self.coords if k not in self.dims)
5173         columns.extend(self.data_vars)
5174 
5175         series_list = []
5176         for name in columns:
5177             try:
5178                 var = self.variables[name]
5179             except KeyError:
5180                 # dimension without a matching coordinate
5181                 size = self.dims[name]
5182                 data = da.arange(size, chunks=size, dtype=np.int64)
5183                 var = Variable((name,), data)
5184 
5185             # IndexVariable objects have a dummy .chunk() method
5186             if isinstance(var, IndexVariable):
5187                 var = var.to_base_variable()
5188 
5189             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
5190             series = dd.from_array(dask_array.reshape(-1), columns=[name])
5191             series_list.append(series)
5192 
5193         df = dd.concat(series_list, axis=1)
5194 
5195         if set_index:
5196             dim_order = [*ordered_dims]
5197 
5198             if len(dim_order) == 1:
5199                 (dim,) = dim_order
5200                 df = df.set_index(dim)
5201             else:
5202                 # triggers an error about multi-indexes, even if only one
5203                 # dimension is passed
5204                 df = df.set_index(dim_order)
5205 
5206         return df
5207 
5208     def to_dict(self, data=True):
5209         """
5210         Convert this dataset to a dictionary following xarray naming
5211         conventions.
5212 
5213         Converts all variables and attributes to native Python objects
5214         Useful for converting to json. To avoid datetime incompatibility
5215         use decode_times=False kwarg in xarrray.open_dataset.
5216 
5217         Parameters
5218         ----------
5219         data : bool, optional
5220             Whether to include the actual data in the dictionary. When set to
5221             False, returns just the schema.
5222 
5223         See Also
5224         --------
5225         Dataset.from_dict
5226         """
5227         d = {
5228             "coords": {},
5229             "attrs": decode_numpy_dict_values(self.attrs),
5230             "dims": dict(self.dims),
5231             "data_vars": {},
5232         }
5233         for k in self.coords:
5234             d["coords"].update({k: self[k].variable.to_dict(data=data)})
5235         for k in self.data_vars:
5236             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
5237         return d
5238 
5239     @classmethod
5240     def from_dict(cls, d):
5241         """
5242         Convert a dictionary into an xarray.Dataset.
5243 
5244         Input dict can take several forms:
5245 
5246         .. code:: python
5247 
5248             d = {
5249                 "t": {"dims": ("t"), "data": t},
5250                 "a": {"dims": ("t"), "data": x},
5251                 "b": {"dims": ("t"), "data": y},
5252             }
5253 
5254             d = {
5255                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
5256                 "attrs": {"title": "air temperature"},
5257                 "dims": "t",
5258                 "data_vars": {
5259                     "a": {"dims": "t", "data": x},
5260                     "b": {"dims": "t", "data": y},
5261                 },
5262             }
5263 
5264         where "t" is the name of the dimesion, "a" and "b" are names of data
5265         variables and t, x, and y are lists, numpy.arrays or pandas objects.
5266 
5267         Parameters
5268         ----------
5269         d : dict-like
5270             Mapping with a minimum structure of
5271                 ``{"var_0": {"dims": [..], "data": [..]}, \
5272                             ...}``
5273 
5274         Returns
5275         -------
5276         obj : xarray.Dataset
5277 
5278         See also
5279         --------
5280         Dataset.to_dict
5281         DataArray.from_dict
5282         """
5283 
5284         if not {"coords", "data_vars"}.issubset(set(d)):
5285             variables = d.items()
5286         else:
5287             import itertools
5288 
5289             variables = itertools.chain(
5290                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
5291             )
5292         try:
5293             variable_dict = {
5294                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
5295             }
5296         except KeyError as e:
5297             raise ValueError(
5298                 "cannot convert dict without the key "
5299                 "'{dims_data}'".format(dims_data=str(e.args[0]))
5300             )
5301         obj = cls(variable_dict)
5302 
5303         # what if coords aren't dims?
5304         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
5305         obj = obj.set_coords(coords)
5306 
5307         obj.attrs.update(d.get("attrs", {}))
5308 
5309         return obj
5310 
5311     @staticmethod
5312     def _unary_op(f):
5313         @functools.wraps(f)
5314         def func(self, *args, **kwargs):
5315             variables = {}
5316             keep_attrs = kwargs.pop("keep_attrs", None)
5317             if keep_attrs is None:
5318                 keep_attrs = _get_keep_attrs(default=True)
5319             for k, v in self._variables.items():
5320                 if k in self._coord_names:
5321                     variables[k] = v
5322                 else:
5323                     variables[k] = f(v, *args, **kwargs)
5324                     if keep_attrs:
5325                         variables[k].attrs = v._attrs
5326             attrs = self._attrs if keep_attrs else None
5327             return self._replace_with_new_dims(variables, attrs=attrs)
5328 
5329         return func
5330 
5331     @staticmethod
5332     def _binary_op(f, reflexive=False, join=None):
5333         @functools.wraps(f)
5334         def func(self, other):
5335             from .dataarray import DataArray
5336 
5337             if isinstance(other, groupby.GroupBy):
5338                 return NotImplemented
5339             align_type = OPTIONS["arithmetic_join"] if join is None else join
5340             if isinstance(other, (DataArray, Dataset)):
5341                 self, other = align(self, other, join=align_type, copy=False)
5342             g = f if not reflexive else lambda x, y: f(y, x)
5343             ds = self._calculate_binary_op(g, other, join=align_type)
5344             return ds
5345 
5346         return func
5347 
5348     @staticmethod
5349     def _inplace_binary_op(f):
5350         @functools.wraps(f)
5351         def func(self, other):
5352             from .dataarray import DataArray
5353 
5354             if isinstance(other, groupby.GroupBy):
5355                 raise TypeError(
5356                     "in-place operations between a Dataset and "
5357                     "a grouped object are not permitted"
5358                 )
5359             # we don't actually modify arrays in-place with in-place Dataset
5360             # arithmetic -- this lets us automatically align things
5361             if isinstance(other, (DataArray, Dataset)):
5362                 other = other.reindex_like(self, copy=False)
5363             g = ops.inplace_to_noninplace_op(f)
5364             ds = self._calculate_binary_op(g, other, inplace=True)
5365             self._replace_with_new_dims(
5366                 ds._variables,
5367                 ds._coord_names,
5368                 attrs=ds._attrs,
5369                 indexes=ds._indexes,
5370                 inplace=True,
5371             )
5372             return self
5373 
5374         return func
5375 
5376     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
5377         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
5378             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
5379                 raise ValueError(
5380                     "datasets must have the same data variables "
5381                     "for in-place arithmetic operations: %s, %s"
5382                     % (list(lhs_data_vars), list(rhs_data_vars))
5383                 )
5384 
5385             dest_vars = {}
5386 
5387             for k in lhs_data_vars:
5388                 if k in rhs_data_vars:
5389                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
5390                 elif join in ["left", "outer"]:
5391                     dest_vars[k] = f(lhs_vars[k], np.nan)
5392             for k in rhs_data_vars:
5393                 if k not in dest_vars and join in ["right", "outer"]:
5394                     dest_vars[k] = f(rhs_vars[k], np.nan)
5395             return dest_vars
5396 
5397         if utils.is_dict_like(other) and not isinstance(other, Dataset):
5398             # can't use our shortcut of doing the binary operation with
5399             # Variable objects, so apply over our data vars instead.
5400             new_data_vars = apply_over_both(
5401                 self.data_vars, other, self.data_vars, other
5402             )
5403             return Dataset(new_data_vars)
5404 
5405         other_coords = getattr(other, "coords", None)
5406         ds = self.coords.merge(other_coords)
5407 
5408         if isinstance(other, Dataset):
5409             new_vars = apply_over_both(
5410                 self.data_vars, other.data_vars, self.variables, other.variables
5411             )
5412         else:
5413             other_variable = getattr(other, "variable", other)
5414             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
5415         ds._variables.update(new_vars)
5416         ds._dims = calculate_dimensions(ds._variables)
5417         return ds
5418 
5419     def _copy_attrs_from(self, other):
5420         self.attrs = other.attrs
5421         for v in other.variables:
5422             if v in self.variables:
5423                 self.variables[v].attrs = other.variables[v].attrs
5424 
5425     def diff(self, dim, n=1, label="upper"):
5426         """Calculate the n-th order discrete difference along given axis.
5427 
5428         Parameters
5429         ----------
5430         dim : str
5431             Dimension over which to calculate the finite difference.
5432         n : int, optional
5433             The number of times values are differenced.
5434         label : str, optional
5435             The new coordinate in dimension ``dim`` will have the
5436             values of either the minuend's or subtrahend's coordinate
5437             for values 'upper' and 'lower', respectively.  Other
5438             values are not supported.
5439 
5440         Returns
5441         -------
5442         difference : same type as caller
5443             The n-th order finite difference of this object.
5444         .. note::
5445             `n` matches numpy's behavior and is different from pandas' first
5446             argument named `periods`.
5447 
5448         Examples
5449         --------
5450         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
5451         >>> ds.diff("x")
5452         <xarray.Dataset>
5453         Dimensions:  (x: 3)
5454         Dimensions without coordinates: x
5455         Data variables:
5456             foo      (x) int64 0 1 0
5457         >>> ds.diff("x", 2)
5458         <xarray.Dataset>
5459         Dimensions:  (x: 2)
5460         Dimensions without coordinates: x
5461         Data variables:
5462             foo      (x) int64 1 -1
5463 
5464         See Also
5465         --------
5466         Dataset.differentiate
5467         """
5468         if n == 0:
5469             return self
5470         if n < 0:
5471             raise ValueError(f"order `n` must be non-negative but got {n}")
5472 
5473         # prepare slices
5474         kwargs_start = {dim: slice(None, -1)}
5475         kwargs_end = {dim: slice(1, None)}
5476 
5477         # prepare new coordinate
5478         if label == "upper":
5479             kwargs_new = kwargs_end
5480         elif label == "lower":
5481             kwargs_new = kwargs_start
5482         else:
5483             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
5484 
5485         variables = {}
5486 
5487         for name, var in self.variables.items():
5488             if dim in var.dims:
5489                 if name in self.data_vars:
5490                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
5491                 else:
5492                     variables[name] = var.isel(**kwargs_new)
5493             else:
5494                 variables[name] = var
5495 
5496         indexes = dict(self.indexes)
5497         if dim in indexes:
5498             indexes[dim] = indexes[dim][kwargs_new[dim]]
5499 
5500         difference = self._replace_with_new_dims(variables, indexes=indexes)
5501 
5502         if n > 1:
5503             return difference.diff(dim, n - 1)
5504         else:
5505             return difference
5506 
5507     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
5508         """Shift this dataset by an offset along one or more dimensions.
5509 
5510         Only data variables are moved; coordinates stay in place. This is
5511         consistent with the behavior of ``shift`` in pandas.
5512 
5513         Parameters
5514         ----------
5515         shifts : mapping of hashable to int
5516             Integer offset to shift along each of the given dimensions.
5517             Positive offsets shift to the right; negative offsets shift to the
5518             left.
5519         fill_value : scalar or dict-like, optional
5520             Value to use for newly missing values. If a dict-like, maps
5521             variable names (including coordinates) to fill values.
5522         **shifts_kwargs
5523             The keyword arguments form of ``shifts``.
5524             One of shifts or shifts_kwargs must be provided.
5525 
5526         Returns
5527         -------
5528         shifted : Dataset
5529             Dataset with the same coordinates and attributes but shifted data
5530             variables.
5531 
5532         See Also
5533         --------
5534         roll
5535 
5536         Examples
5537         --------
5538         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5539         >>> ds.shift(x=2)
5540         <xarray.Dataset>
5541         Dimensions:  (x: 5)
5542         Dimensions without coordinates: x
5543         Data variables:
5544             foo      (x) object nan nan 'a' 'b' 'c'
5545         """
5546         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5547         invalid = [k for k in shifts if k not in self.dims]
5548         if invalid:
5549             raise ValueError("dimensions %r do not exist" % invalid)
5550 
5551         variables = {}
5552         for name, var in self.variables.items():
5553             if name in self.data_vars:
5554                 fill_value_ = (
5555                     fill_value.get(name, dtypes.NA)
5556                     if isinstance(fill_value, dict)
5557                     else fill_value
5558                 )
5559 
5560                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5561                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
5562             else:
5563                 variables[name] = var
5564 
5565         return self._replace(variables)
5566 
5567     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5568         """Roll this dataset by an offset along one or more dimensions.
5569 
5570         Unlike shift, roll may rotate all variables, including coordinates
5571         if specified. The direction of rotation is consistent with
5572         :py:func:`numpy.roll`.
5573 
5574         Parameters
5575         ----------
5576         shifts : dict, optional
5577             A dict with keys matching dimensions and values given
5578             by integers to rotate each of the given dimensions. Positive
5579             offsets roll to the right; negative offsets roll to the left.
5580         roll_coords : bool
5581             Indicates whether to  roll the coordinates by the offset
5582             The current default of roll_coords (None, equivalent to True) is
5583             deprecated and will change to False in a future version.
5584             Explicitly pass roll_coords to silence the warning.
5585         **shifts_kwargs : {dim: offset, ...}, optional
5586             The keyword arguments form of ``shifts``.
5587             One of shifts or shifts_kwargs must be provided.
5588         Returns
5589         -------
5590         rolled : Dataset
5591             Dataset with the same coordinates and attributes but rolled
5592             variables.
5593 
5594         See Also
5595         --------
5596         shift
5597 
5598         Examples
5599         --------
5600         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5601         >>> ds.roll(x=2)
5602         <xarray.Dataset>
5603         Dimensions:  (x: 5)
5604         Dimensions without coordinates: x
5605         Data variables:
5606             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
5607         """
5608         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5609         invalid = [k for k in shifts if k not in self.dims]
5610         if invalid:
5611             raise ValueError("dimensions %r do not exist" % invalid)
5612 
5613         if roll_coords is None:
5614             warnings.warn(
5615                 "roll_coords will be set to False in the future."
5616                 " Explicitly set roll_coords to silence warning.",
5617                 FutureWarning,
5618                 stacklevel=2,
5619             )
5620             roll_coords = True
5621 
5622         unrolled_vars = () if roll_coords else self.coords
5623 
5624         variables = {}
5625         for k, v in self.variables.items():
5626             if k not in unrolled_vars:
5627                 variables[k] = v.roll(
5628                     **{k: s for k, s in shifts.items() if k in v.dims}
5629                 )
5630             else:
5631                 variables[k] = v
5632 
5633         if roll_coords:
5634             indexes = {}
5635             for k, v in self.indexes.items():
5636                 (dim,) = self.variables[k].dims
5637                 if dim in shifts:
5638                     indexes[k] = roll_index(v, shifts[dim])
5639                 else:
5640                     indexes[k] = v
5641         else:
5642             indexes = dict(self.indexes)
5643 
5644         return self._replace(variables, indexes=indexes)
5645 
5646     def sortby(self, variables, ascending=True):
5647         """
5648         Sort object by labels or values (along an axis).
5649 
5650         Sorts the dataset, either along specified dimensions,
5651         or according to values of 1-D dataarrays that share dimension
5652         with calling object.
5653 
5654         If the input variables are dataarrays, then the dataarrays are aligned
5655         (via left-join) to the calling object prior to sorting by cell values.
5656         NaNs are sorted to the end, following Numpy convention.
5657 
5658         If multiple sorts along the same dimension is
5659         given, numpy's lexsort is performed along that dimension:
5660         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5661         and the FIRST key in the sequence is used as the primary sort key,
5662         followed by the 2nd key, etc.
5663 
5664         Parameters
5665         ----------
5666         variables : str, DataArray, or list of str or DataArray
5667             1D DataArray objects or name(s) of 1D variable(s) in
5668             coords/data_vars whose values are used to sort the dataset.
5669         ascending : bool, optional
5670             Whether to sort by ascending or descending order.
5671 
5672         Returns
5673         -------
5674         sorted : Dataset
5675             A new dataset where all the specified dims are sorted by dim
5676             labels.
5677         """
5678         from .dataarray import DataArray
5679 
5680         if not isinstance(variables, list):
5681             variables = [variables]
5682         else:
5683             variables = variables
5684         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5685         aligned_vars = align(self, *variables, join="left")
5686         aligned_self = aligned_vars[0]
5687         aligned_other_vars = aligned_vars[1:]
5688         vars_by_dim = defaultdict(list)
5689         for data_array in aligned_other_vars:
5690             if data_array.ndim != 1:
5691                 raise ValueError("Input DataArray is not 1-D.")
5692             (key,) = data_array.dims
5693             vars_by_dim[key].append(data_array)
5694 
5695         indices = {}
5696         for key, arrays in vars_by_dim.items():
5697             order = np.lexsort(tuple(reversed(arrays)))
5698             indices[key] = order if ascending else order[::-1]
5699         return aligned_self.isel(**indices)
5700 
5701     def quantile(
5702         self,
5703         q,
5704         dim=None,
5705         interpolation="linear",
5706         numeric_only=False,
5707         keep_attrs=None,
5708         skipna=True,
5709     ):
5710         """Compute the qth quantile of the data along the specified dimension.
5711 
5712         Returns the qth quantiles(s) of the array elements for each variable
5713         in the Dataset.
5714 
5715         Parameters
5716         ----------
5717         q : float or array-like of float
5718             Quantile to compute, which must be between 0 and 1 inclusive.
5719         dim : str or sequence of str, optional
5720             Dimension(s) over which to apply quantile.
5721         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
5722             This optional parameter specifies the interpolation method to
5723             use when the desired quantile lies between two data points
5724             ``i < j``:
5725 
5726                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5727                   the fractional part of the index surrounded by ``i`` and
5728                   ``j``.
5729                 * lower: ``i``.
5730                 * higher: ``j``.
5731                 * nearest: ``i`` or ``j``, whichever is nearest.
5732                 * midpoint: ``(i + j) / 2``.
5733         keep_attrs : bool, optional
5734             If True, the dataset's attributes (`attrs`) will be copied from
5735             the original object to the new one.  If False (default), the new
5736             object will be returned without attributes.
5737         numeric_only : bool, optional
5738             If True, only apply ``func`` to variables with a numeric dtype.
5739         skipna : bool, optional
5740             Whether to skip missing values when aggregating.
5741 
5742         Returns
5743         -------
5744         quantiles : Dataset
5745             If `q` is a single quantile, then the result is a scalar for each
5746             variable in data_vars. If multiple percentiles are given, first
5747             axis of the result corresponds to the quantile and a quantile
5748             dimension is added to the return Dataset. The other dimensions are
5749             the dimensions that remain after the reduction of the array.
5750 
5751         See Also
5752         --------
5753         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5754 
5755         Examples
5756         --------
5757         >>> ds = xr.Dataset(
5758         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5759         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5760         ... )
5761         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5762         <xarray.Dataset>
5763         Dimensions:   ()
5764         Coordinates:
5765             quantile  float64 0.0
5766         Data variables:
5767             a         float64 0.7
5768         >>> ds.quantile(0, dim="x")
5769         <xarray.Dataset>
5770         Dimensions:   (y: 4)
5771         Coordinates:
5772           * y         (y) float64 1.0 1.5 2.0 2.5
5773             quantile  float64 0.0
5774         Data variables:
5775             a         (y) float64 0.7 4.2 2.6 1.5
5776         >>> ds.quantile([0, 0.5, 1])
5777         <xarray.Dataset>
5778         Dimensions:   (quantile: 3)
5779         Coordinates:
5780           * quantile  (quantile) float64 0.0 0.5 1.0
5781         Data variables:
5782             a         (quantile) float64 0.7 3.4 9.4
5783         >>> ds.quantile([0, 0.5, 1], dim="x")
5784         <xarray.Dataset>
5785         Dimensions:   (quantile: 3, y: 4)
5786         Coordinates:
5787           * y         (y) float64 1.0 1.5 2.0 2.5
5788           * quantile  (quantile) float64 0.0 0.5 1.0
5789         Data variables:
5790             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5791         """
5792 
5793         if isinstance(dim, str):
5794             dims = {dim}
5795         elif dim in [None, ...]:
5796             dims = set(self.dims)
5797         else:
5798             dims = set(dim)
5799 
5800         _assert_empty(
5801             [d for d in dims if d not in self.dims],
5802             "Dataset does not contain the dimensions: %s",
5803         )
5804 
5805         q = np.asarray(q, dtype=np.float64)
5806 
5807         variables = {}
5808         for name, var in self.variables.items():
5809             reduce_dims = [d for d in var.dims if d in dims]
5810             if reduce_dims or not var.dims:
5811                 if name not in self.coords:
5812                     if (
5813                         not numeric_only
5814                         or np.issubdtype(var.dtype, np.number)
5815                         or var.dtype == np.bool_
5816                     ):
5817                         if len(reduce_dims) == var.ndim:
5818                             # prefer to aggregate over axis=None rather than
5819                             # axis=(0, 1) if they will be equivalent, because
5820                             # the former is often more efficient
5821                             reduce_dims = None
5822                         variables[name] = var.quantile(
5823                             q,
5824                             dim=reduce_dims,
5825                             interpolation=interpolation,
5826                             keep_attrs=keep_attrs,
5827                             skipna=skipna,
5828                         )
5829 
5830             else:
5831                 variables[name] = var
5832 
5833         # construct the new dataset
5834         coord_names = {k for k in self.coords if k in variables}
5835         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5836         if keep_attrs is None:
5837             keep_attrs = _get_keep_attrs(default=False)
5838         attrs = self.attrs if keep_attrs else None
5839         new = self._replace_with_new_dims(
5840             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5841         )
5842         return new.assign_coords(quantile=q)
5843 
5844     def rank(self, dim, pct=False, keep_attrs=None):
5845         """Ranks the data.
5846 
5847         Equal values are assigned a rank that is the average of the ranks that
5848         would have been otherwise assigned to all of the values within
5849         that set.
5850         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5851 
5852         NaNs in the input array are returned as NaNs.
5853 
5854         The `bottleneck` library is required.
5855 
5856         Parameters
5857         ----------
5858         dim : str
5859             Dimension over which to compute rank.
5860         pct : bool, optional
5861             If True, compute percentage ranks, otherwise compute integer ranks.
5862         keep_attrs : bool, optional
5863             If True, the dataset's attributes (`attrs`) will be copied from
5864             the original object to the new one.  If False (default), the new
5865             object will be returned without attributes.
5866 
5867         Returns
5868         -------
5869         ranked : Dataset
5870             Variables that do not depend on `dim` are dropped.
5871         """
5872         if dim not in self.dims:
5873             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5874 
5875         variables = {}
5876         for name, var in self.variables.items():
5877             if name in self.data_vars:
5878                 if dim in var.dims:
5879                     variables[name] = var.rank(dim, pct=pct)
5880             else:
5881                 variables[name] = var
5882 
5883         coord_names = set(self.coords)
5884         if keep_attrs is None:
5885             keep_attrs = _get_keep_attrs(default=False)
5886         attrs = self.attrs if keep_attrs else None
5887         return self._replace(variables, coord_names, attrs=attrs)
5888 
5889     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5890         """ Differentiate with the second order accurate central
5891         differences.
5892 
5893         .. note::
5894             This feature is limited to simple cartesian geometry, i.e. coord
5895             must be one dimensional.
5896 
5897         Parameters
5898         ----------
5899         coord : str
5900             The coordinate to be used to compute the gradient.
5901         edge_order : {1, 2}, default: 1
5902             N-th order accurate differences at the boundaries.
5903         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5904             "us", "ns", "ps", "fs", "as"}, default: None
5905             Unit to compute gradient. Only valid for datetime coordinate.
5906 
5907         Returns
5908         -------
5909         differentiated: Dataset
5910 
5911         See also
5912         --------
5913         numpy.gradient: corresponding numpy function
5914         """
5915         from .variable import Variable
5916 
5917         if coord not in self.variables and coord not in self.dims:
5918             raise ValueError(f"Coordinate {coord} does not exist.")
5919 
5920         coord_var = self[coord].variable
5921         if coord_var.ndim != 1:
5922             raise ValueError(
5923                 "Coordinate {} must be 1 dimensional but is {}"
5924                 " dimensional".format(coord, coord_var.ndim)
5925             )
5926 
5927         dim = coord_var.dims[0]
5928         if _contains_datetime_like_objects(coord_var):
5929             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5930                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5931             elif datetime_unit is None:
5932                 datetime_unit = "s"  # Default to seconds for cftime objects
5933             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5934 
5935         variables = {}
5936         for k, v in self.variables.items():
5937             if k in self.data_vars and dim in v.dims and k not in self.coords:
5938                 if _contains_datetime_like_objects(v):
5939                     v = v._to_numeric(datetime_unit=datetime_unit)
5940                 grad = duck_array_ops.gradient(
5941                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5942                 )
5943                 variables[k] = Variable(v.dims, grad)
5944             else:
5945                 variables[k] = v
5946         return self._replace(variables)
5947 
5948     def integrate(
5949         self, coord: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
5950     ) -> "Dataset":
5951         """Integrate along the given coordinate using the trapezoidal rule.
5952 
5953         .. note::
5954             This feature is limited to simple cartesian geometry, i.e. coord
5955             must be one dimensional.
5956 
5957         Parameters
5958         ----------
5959         coord : hashable, or sequence of hashable
5960             Coordinate(s) used for the integration.
5961         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5962                         'ps', 'fs', 'as'}, optional
5963             Specify the unit if datetime coordinate is used.
5964 
5965         Returns
5966         -------
5967         integrated : Dataset
5968 
5969         See also
5970         --------
5971         DataArray.integrate
5972         numpy.trapz : corresponding numpy function
5973 
5974         Examples
5975         --------
5976         >>> ds = xr.Dataset(
5977         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5978         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5979         ... )
5980         >>> ds
5981         <xarray.Dataset>
5982         Dimensions:  (x: 4)
5983         Coordinates:
5984           * x        (x) int64 0 1 2 3
5985             y        (x) int64 1 7 3 5
5986         Data variables:
5987             a        (x) int64 5 5 6 6
5988             b        (x) int64 1 2 1 0
5989         >>> ds.integrate("x")
5990         <xarray.Dataset>
5991         Dimensions:  ()
5992         Data variables:
5993             a        float64 16.5
5994             b        float64 3.5
5995         >>> ds.integrate("y")
5996         <xarray.Dataset>
5997         Dimensions:  ()
5998         Data variables:
5999             a        float64 20.0
6000             b        float64 4.0
6001         """
6002         if not isinstance(coord, (list, tuple)):
6003             coord = (coord,)
6004         result = self
6005         for c in coord:
6006             result = result._integrate_one(c, datetime_unit=datetime_unit)
6007         return result
6008 
6009     def _integrate_one(self, coord, datetime_unit=None):
6010         from .variable import Variable
6011 
6012         if coord not in self.variables and coord not in self.dims:
6013             raise ValueError(f"Coordinate {coord} does not exist.")
6014 
6015         coord_var = self[coord].variable
6016         if coord_var.ndim != 1:
6017             raise ValueError(
6018                 "Coordinate {} must be 1 dimensional but is {}"
6019                 " dimensional".format(coord, coord_var.ndim)
6020             )
6021 
6022         dim = coord_var.dims[0]
6023         if _contains_datetime_like_objects(coord_var):
6024             if coord_var.dtype.kind in "mM" and datetime_unit is None:
6025                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
6026             elif datetime_unit is None:
6027                 datetime_unit = "s"  # Default to seconds for cftime objects
6028             coord_var = coord_var._replace(
6029                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
6030             )
6031 
6032         variables = {}
6033         coord_names = set()
6034         for k, v in self.variables.items():
6035             if k in self.coords:
6036                 if dim not in v.dims:
6037                     variables[k] = v
6038                     coord_names.add(k)
6039             else:
6040                 if k in self.data_vars and dim in v.dims:
6041                     if _contains_datetime_like_objects(v):
6042                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
6043                     integ = duck_array_ops.trapz(
6044                         v.data, coord_var.data, axis=v.get_axis_num(dim)
6045                     )
6046                     v_dims = list(v.dims)
6047                     v_dims.remove(dim)
6048                     variables[k] = Variable(v_dims, integ)
6049                 else:
6050                     variables[k] = v
6051         indexes = {k: v for k, v in self.indexes.items() if k in variables}
6052         return self._replace_with_new_dims(
6053             variables, coord_names=coord_names, indexes=indexes
6054         )
6055 
6056     @property
6057     def real(self):
6058         return self.map(lambda x: x.real, keep_attrs=True)
6059 
6060     @property
6061     def imag(self):
6062         return self.map(lambda x: x.imag, keep_attrs=True)
6063 
6064     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
6065 
6066     def filter_by_attrs(self, **kwargs):
6067         """Returns a ``Dataset`` with variables that match specific conditions.
6068 
6069         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
6070         containing only the variables for which all the filter tests pass.
6071         These tests are either ``key=value`` for which the attribute ``key``
6072         has the exact value ``value`` or the callable passed into
6073         ``key=callable`` returns True. The callable will be passed a single
6074         value, either the value of the attribute ``key`` or ``None`` if the
6075         DataArray does not have an attribute with the name ``key``.
6076 
6077         Parameters
6078         ----------
6079         **kwargs
6080             key : str
6081                 Attribute name.
6082             value : callable or obj
6083                 If value is a callable, it should return a boolean in the form
6084                 of bool = func(attr) where attr is da.attrs[key].
6085                 Otherwise, value will be compared to the each
6086                 DataArray's attrs[key].
6087 
6088         Returns
6089         -------
6090         new : Dataset
6091             New dataset with variables filtered by attribute.
6092 
6093         Examples
6094         --------
6095         >>> # Create an example dataset:
6096         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
6097         >>> precip = 10 * np.random.rand(2, 2, 3)
6098         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
6099         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
6100         >>> dims = ["x", "y", "time"]
6101         >>> temp_attr = dict(standard_name="air_potential_temperature")
6102         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
6103         >>> ds = xr.Dataset(
6104         ...     {
6105         ...         "temperature": (dims, temp, temp_attr),
6106         ...         "precipitation": (dims, precip, precip_attr),
6107         ...     },
6108         ...     coords={
6109         ...         "lon": (["x", "y"], lon),
6110         ...         "lat": (["x", "y"], lat),
6111         ...         "time": pd.date_range("2014-09-06", periods=3),
6112         ...         "reference_time": pd.Timestamp("2014-09-05"),
6113         ...     },
6114         ... )
6115         >>> # Get variables matching a specific standard_name.
6116         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
6117         <xarray.Dataset>
6118         Dimensions:         (time: 3, x: 2, y: 2)
6119         Coordinates:
6120             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
6121             lat             (x, y) float64 42.25 42.21 42.63 42.59
6122           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
6123             reference_time  datetime64[ns] 2014-09-05
6124         Dimensions without coordinates: x, y
6125         Data variables:
6126             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
6127         >>> # Get all variables that have a standard_name attribute.
6128         >>> standard_name = lambda v: v is not None
6129         >>> ds.filter_by_attrs(standard_name=standard_name)
6130         <xarray.Dataset>
6131         Dimensions:         (time: 3, x: 2, y: 2)
6132         Coordinates:
6133             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
6134             lat             (x, y) float64 42.25 42.21 42.63 42.59
6135           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
6136             reference_time  datetime64[ns] 2014-09-05
6137         Dimensions without coordinates: x, y
6138         Data variables:
6139             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
6140             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
6141 
6142         """
6143         selection = []
6144         for var_name, variable in self.variables.items():
6145             has_value_flag = False
6146             for attr_name, pattern in kwargs.items():
6147                 attr_value = variable.attrs.get(attr_name)
6148                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
6149                     has_value_flag = True
6150                 else:
6151                     has_value_flag = False
6152                     break
6153             if has_value_flag is True:
6154                 selection.append(var_name)
6155         return self[selection]
6156 
6157     def unify_chunks(self) -> "Dataset":
6158         """Unify chunk size along all chunked dimensions of this Dataset.
6159 
6160         Returns
6161         -------
6162         Dataset with consistent chunk sizes for all dask-array variables
6163 
6164         See Also
6165         --------
6166         dask.array.core.unify_chunks
6167         """
6168 
6169         try:
6170             self.chunks
6171         except ValueError:  # "inconsistent chunks"
6172             pass
6173         else:
6174             # No variables with dask backend, or all chunks are already aligned
6175             return self.copy()
6176 
6177         # import dask is placed after the quick exit test above to allow
6178         # running this method if dask isn't installed and there are no chunks
6179         import dask.array
6180 
6181         ds = self.copy()
6182 
6183         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
6184 
6185         dask_array_names = []
6186         dask_unify_args = []
6187         for name, variable in ds.variables.items():
6188             if isinstance(variable.data, dask.array.Array):
6189                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
6190                 dask_array_names.append(name)
6191                 dask_unify_args.append(variable.data)
6192                 dask_unify_args.append(dims_tuple)
6193 
6194         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
6195 
6196         for name, new_array in zip(dask_array_names, rechunked_arrays):
6197             ds.variables[name]._data = new_array
6198 
6199         return ds
6200 
6201     def map_blocks(
6202         self,
6203         func: "Callable[..., T_DSorDA]",
6204         args: Sequence[Any] = (),
6205         kwargs: Mapping[str, Any] = None,
6206         template: Union["DataArray", "Dataset"] = None,
6207     ) -> "T_DSorDA":
6208         """
6209         Apply a function to each block of this Dataset.
6210 
6211         .. warning::
6212             This method is experimental and its signature may change.
6213 
6214         Parameters
6215         ----------
6216         func : callable
6217             User-provided function that accepts a Dataset as its first
6218             parameter. The function will receive a subset or 'block' of this Dataset (see below),
6219             corresponding to one chunk along each chunked dimension. ``func`` will be
6220             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
6221 
6222             This function must return either a single DataArray or a single Dataset.
6223 
6224             This function cannot add a new chunked dimension.
6225         args : sequence
6226             Passed to func after unpacking and subsetting any xarray objects by blocks.
6227             xarray objects in args must be aligned with obj, otherwise an error is raised.
6228         kwargs : mapping
6229             Passed verbatim to func after unpacking. xarray objects, if any, will not be
6230             subset to blocks. Passing dask collections in kwargs is not allowed.
6231         template : DataArray or Dataset, optional
6232             xarray object representing the final result after compute is called. If not provided,
6233             the function will be first run on mocked-up data, that looks like this object but
6234             has sizes 0, to determine properties of the returned object such as dtype,
6235             variable names, attributes, new dimensions and new indexes (if any).
6236             ``template`` must be provided if the function changes the size of existing dimensions.
6237             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
6238             ``attrs`` set by ``func`` will be ignored.
6239 
6240         Returns
6241         -------
6242         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
6243         function.
6244 
6245         Notes
6246         -----
6247         This function is designed for when ``func`` needs to manipulate a whole xarray object
6248         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
6249         recommended to use ``apply_ufunc``.
6250 
6251         If none of the variables in this object is backed by dask arrays, calling this function is
6252         equivalent to calling ``func(obj, *args, **kwargs)``.
6253 
6254         See Also
6255         --------
6256         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
6257         xarray.DataArray.map_blocks
6258 
6259         Examples
6260         --------
6261         Calculate an anomaly from climatology using ``.groupby()``. Using
6262         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
6263         its indices, and its methods like ``.groupby()``.
6264 
6265         >>> def calculate_anomaly(da, groupby_type="time.month"):
6266         ...     gb = da.groupby(groupby_type)
6267         ...     clim = gb.mean(dim="time")
6268         ...     return gb - clim
6269         ...
6270         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
6271         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
6272         >>> np.random.seed(123)
6273         >>> array = xr.DataArray(
6274         ...     np.random.rand(len(time)),
6275         ...     dims=["time"],
6276         ...     coords={"time": time, "month": month},
6277         ... ).chunk()
6278         >>> ds = xr.Dataset({"a": array})
6279         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
6280         <xarray.Dataset>
6281         Dimensions:  (time: 24)
6282         Coordinates:
6283           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6284             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
6285         Data variables:
6286             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
6287 
6288         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
6289         to the function being applied in ``xr.map_blocks()``:
6290 
6291         >>> ds.map_blocks(
6292         ...     calculate_anomaly,
6293         ...     kwargs={"groupby_type": "time.year"},
6294         ...     template=ds,
6295         ... )
6296         <xarray.Dataset>
6297         Dimensions:  (time: 24)
6298         Coordinates:
6299           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6300             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
6301         Data variables:
6302             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
6303         """
6304         from .parallel import map_blocks
6305 
6306         return map_blocks(func, self, args, kwargs, template)
6307 
6308     def polyfit(
6309         self,
6310         dim: Hashable,
6311         deg: int,
6312         skipna: bool = None,
6313         rcond: float = None,
6314         w: Union[Hashable, Any] = None,
6315         full: bool = False,
6316         cov: Union[bool, str] = False,
6317     ):
6318         """
6319         Least squares polynomial fit.
6320 
6321         This replicates the behaviour of `numpy.polyfit` but differs by skipping
6322         invalid values when `skipna = True`.
6323 
6324         Parameters
6325         ----------
6326         dim : hashable
6327             Coordinate along which to fit the polynomials.
6328         deg : int
6329             Degree of the fitting polynomial.
6330         skipna : bool, optional
6331             If True, removes all invalid values before fitting each 1D slices of the array.
6332             Default is True if data is stored in a dask.array or if there is any
6333             invalid values, False otherwise.
6334         rcond : float, optional
6335             Relative condition number to the fit.
6336         w : hashable or Any, optional
6337             Weights to apply to the y-coordinate of the sample points.
6338             Can be an array-like object or the name of a coordinate in the dataset.
6339         full : bool, optional
6340             Whether to return the residuals, matrix rank and singular values in addition
6341             to the coefficients.
6342         cov : bool or str, optional
6343             Whether to return to the covariance matrix in addition to the coefficients.
6344             The matrix is not scaled if `cov='unscaled'`.
6345 
6346         Returns
6347         -------
6348         polyfit_results : Dataset
6349             A single dataset which contains (for each "var" in the input dataset):
6350 
6351             [var]_polyfit_coefficients
6352                 The coefficients of the best fit for each variable in this dataset.
6353             [var]_polyfit_residuals
6354                 The residuals of the least-square computation for each variable (only included if `full=True`)
6355                 When the matrix rank is deficient, np.nan is returned.
6356             [dim]_matrix_rank
6357                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6358                 The rank is computed ignoring the NaN values that might be skipped.
6359             [dim]_singular_values
6360                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6361             [var]_polyfit_covariance
6362                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
6363 
6364         Warns
6365         -----
6366         RankWarning
6367             The rank of the coefficient matrix in the least-squares fit is deficient.
6368             The warning is not raised with in-memory (not dask) data and `full=True`.
6369 
6370         See Also
6371         --------
6372         numpy.polyfit
6373         """
6374         variables = {}
6375         skipna_da = skipna
6376 
6377         x = get_clean_interp_index(self, dim, strict=False)
6378         xname = "{}_".format(self[dim].name)
6379         order = int(deg) + 1
6380         lhs = np.vander(x, order)
6381 
6382         if rcond is None:
6383             rcond = x.shape[0] * np.core.finfo(x.dtype).eps
6384 
6385         # Weights:
6386         if w is not None:
6387             if isinstance(w, Hashable):
6388                 w = self.coords[w]
6389             w = np.asarray(w)
6390             if w.ndim != 1:
6391                 raise TypeError("Expected a 1-d array for weights.")
6392             if w.shape[0] != lhs.shape[0]:
6393                 raise TypeError("Expected w and {} to have the same length".format(dim))
6394             lhs *= w[:, np.newaxis]
6395 
6396         # Scaling
6397         scale = np.sqrt((lhs * lhs).sum(axis=0))
6398         lhs /= scale
6399 
6400         degree_dim = utils.get_temp_dimname(self.dims, "degree")
6401 
6402         rank = np.linalg.matrix_rank(lhs)
6403 
6404         if full:
6405             rank = xr.DataArray(rank, name=xname + "matrix_rank")
6406             variables[rank.name] = rank
6407             sing = np.linalg.svd(lhs, compute_uv=False)
6408             sing = xr.DataArray(
6409                 sing,
6410                 dims=(degree_dim,),
6411                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
6412                 name=xname + "singular_values",
6413             )
6414             variables[sing.name] = sing
6415 
6416         for name, da in self.data_vars.items():
6417             if dim not in da.dims:
6418                 continue
6419 
6420             if is_duck_dask_array(da.data) and (
6421                 rank != order or full or skipna is None
6422             ):
6423                 # Current algorithm with dask and skipna=False neither supports
6424                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
6425                 skipna_da = True
6426             elif skipna is None:
6427                 skipna_da = np.any(da.isnull())
6428 
6429             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
6430             stacked_coords: Dict[Hashable, DataArray] = {}
6431             if dims_to_stack:
6432                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
6433                 rhs = da.transpose(dim, *dims_to_stack).stack(
6434                     {stacked_dim: dims_to_stack}
6435                 )
6436                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
6437                 scale_da = scale[:, np.newaxis]
6438             else:
6439                 rhs = da
6440                 scale_da = scale
6441 
6442             if w is not None:
6443                 rhs *= w[:, np.newaxis]
6444 
6445             with warnings.catch_warnings():
6446                 if full:  # Copy np.polyfit behavior
6447                     warnings.simplefilter("ignore", np.RankWarning)
6448                 else:  # Raise only once per variable
6449                     warnings.simplefilter("once", np.RankWarning)
6450 
6451                 coeffs, residuals = duck_array_ops.least_squares(
6452                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
6453                 )
6454 
6455             if isinstance(name, str):
6456                 name = "{}_".format(name)
6457             else:
6458                 # Thus a ReprObject => polyfit was called on a DataArray
6459                 name = ""
6460 
6461             coeffs = xr.DataArray(
6462                 coeffs / scale_da,
6463                 dims=[degree_dim] + list(stacked_coords.keys()),
6464                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
6465                 name=name + "polyfit_coefficients",
6466             )
6467             if dims_to_stack:
6468                 coeffs = coeffs.unstack(stacked_dim)
6469             variables[coeffs.name] = coeffs
6470 
6471             if full or (cov is True):
6472                 residuals = xr.DataArray(
6473                     residuals if dims_to_stack else residuals.squeeze(),
6474                     dims=list(stacked_coords.keys()),
6475                     coords=stacked_coords,
6476                     name=name + "polyfit_residuals",
6477                 )
6478                 if dims_to_stack:
6479                     residuals = residuals.unstack(stacked_dim)
6480                 variables[residuals.name] = residuals
6481 
6482             if cov:
6483                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
6484                 Vbase /= np.outer(scale, scale)
6485                 if cov == "unscaled":
6486                     fac = 1
6487                 else:
6488                     if x.shape[0] <= order:
6489                         raise ValueError(
6490                             "The number of data points must exceed order to scale the covariance matrix."
6491                         )
6492                     fac = residuals / (x.shape[0] - order)
6493                 covariance = xr.DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
6494                 variables[name + "polyfit_covariance"] = covariance
6495 
6496         return Dataset(data_vars=variables, attrs=self.attrs.copy())
6497 
6498     def pad(
6499         self,
6500         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
6501         mode: str = "constant",
6502         stat_length: Union[
6503             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6504         ] = None,
6505         constant_values: Union[
6506             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6507         ] = None,
6508         end_values: Union[
6509             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6510         ] = None,
6511         reflect_type: str = None,
6512         **pad_width_kwargs: Any,
6513     ) -> "Dataset":
6514         """Pad this dataset along one or more dimensions.
6515 
6516         .. warning::
6517             This function is experimental and its behaviour is likely to change
6518             especially regarding padding of dimension coordinates (or IndexVariables).
6519 
6520         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
6521         coordinates will be padded with the same mode, otherwise coordinates
6522         are padded using the "constant" mode with fill_value dtypes.NA.
6523 
6524         Parameters
6525         ----------
6526         pad_width : mapping of hashable to tuple of int
6527             Mapping with the form of {dim: (pad_before, pad_after)}
6528             describing the number of values padded along each dimension.
6529             {dim: pad} is a shortcut for pad_before = pad_after = pad
6530         mode : str, default: "constant"
6531             One of the following string values (taken from numpy docs).
6532 
6533             'constant' (default)
6534                 Pads with a constant value.
6535             'edge'
6536                 Pads with the edge values of array.
6537             'linear_ramp'
6538                 Pads with the linear ramp between end_value and the
6539                 array edge value.
6540             'maximum'
6541                 Pads with the maximum value of all or part of the
6542                 vector along each axis.
6543             'mean'
6544                 Pads with the mean value of all or part of the
6545                 vector along each axis.
6546             'median'
6547                 Pads with the median value of all or part of the
6548                 vector along each axis.
6549             'minimum'
6550                 Pads with the minimum value of all or part of the
6551                 vector along each axis.
6552             'reflect'
6553                 Pads with the reflection of the vector mirrored on
6554                 the first and last values of the vector along each
6555                 axis.
6556             'symmetric'
6557                 Pads with the reflection of the vector mirrored
6558                 along the edge of the array.
6559             'wrap'
6560                 Pads with the wrap of the vector along the axis.
6561                 The first values are used to pad the end and the
6562                 end values are used to pad the beginning.
6563         stat_length : int, tuple or mapping of hashable to tuple, default: None
6564             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
6565             values at edge of each axis used to calculate the statistic value.
6566             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
6567             statistic lengths along each dimension.
6568             ((before, after),) yields same before and after statistic lengths
6569             for each dimension.
6570             (stat_length,) or int is a shortcut for before = after = statistic
6571             length for all axes.
6572             Default is ``None``, to use the entire axis.
6573         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
6574             Used in 'constant'.  The values to set the padded values for each
6575             axis.
6576             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6577             pad constants along each dimension.
6578             ``((before, after),)`` yields same before and after constants for each
6579             dimension.
6580             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6581             all dimensions.
6582             Default is 0.
6583         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
6584             Used in 'linear_ramp'.  The values used for the ending value of the
6585             linear_ramp and that will form the edge of the padded array.
6586             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6587             end values along each dimension.
6588             ``((before, after),)`` yields same before and after end values for each
6589             axis.
6590             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6591             all axes.
6592             Default is 0.
6593         reflect_type : {"even", "odd"}, optional
6594             Used in "reflect", and "symmetric".  The "even" style is the
6595             default with an unaltered reflection around the edge value.  For
6596             the "odd" style, the extended part of the array is created by
6597             subtracting the reflected values from two times the edge value.
6598         **pad_width_kwargs
6599             The keyword arguments form of ``pad_width``.
6600             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
6601 
6602         Returns
6603         -------
6604         padded : Dataset
6605             Dataset with the padded coordinates and data.
6606 
6607         See Also
6608         --------
6609         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
6610 
6611         Notes
6612         -----
6613         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
6614         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
6615         specify ``constant_values=np.nan``
6616 
6617         Examples
6618         --------
6619         >>> ds = xr.Dataset({"foo": ("x", range(5))})
6620         >>> ds.pad(x=(1, 2))
6621         <xarray.Dataset>
6622         Dimensions:  (x: 8)
6623         Dimensions without coordinates: x
6624         Data variables:
6625             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
6626         """
6627         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
6628 
6629         if mode in ("edge", "reflect", "symmetric", "wrap"):
6630             coord_pad_mode = mode
6631             coord_pad_options = {
6632                 "stat_length": stat_length,
6633                 "constant_values": constant_values,
6634                 "end_values": end_values,
6635                 "reflect_type": reflect_type,
6636             }
6637         else:
6638             coord_pad_mode = "constant"
6639             coord_pad_options = {}
6640 
6641         variables = {}
6642         for name, var in self.variables.items():
6643             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
6644             if not var_pad_width:
6645                 variables[name] = var
6646             elif name in self.data_vars:
6647                 variables[name] = var.pad(
6648                     pad_width=var_pad_width,
6649                     mode=mode,
6650                     stat_length=stat_length,
6651                     constant_values=constant_values,
6652                     end_values=end_values,
6653                     reflect_type=reflect_type,
6654                 )
6655             else:
6656                 variables[name] = var.pad(
6657                     pad_width=var_pad_width,
6658                     mode=coord_pad_mode,
6659                     **coord_pad_options,  # type: ignore
6660                 )
6661 
6662         return self._replace_vars_and_dims(variables)
6663 
6664     def idxmin(
6665         self,
6666         dim: Hashable = None,
6667         skipna: bool = None,
6668         fill_value: Any = dtypes.NA,
6669         keep_attrs: bool = None,
6670     ) -> "Dataset":
6671         """Return the coordinate label of the minimum value along a dimension.
6672 
6673         Returns a new `Dataset` named after the dimension with the values of
6674         the coordinate labels along that dimension corresponding to minimum
6675         values along that dimension.
6676 
6677         In comparison to :py:meth:`~Dataset.argmin`, this returns the
6678         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
6679 
6680         Parameters
6681         ----------
6682         dim : str, optional
6683             Dimension over which to apply `idxmin`.  This is optional for 1D
6684             variables, but required for variables with 2 or more dimensions.
6685         skipna : bool or None, default: None
6686             If True, skip missing values (as marked by NaN). By default, only
6687             skips missing values for ``float``, ``complex``, and ``object``
6688             dtypes; other dtypes either do not have a sentinel missing value
6689             (``int``) or ``skipna=True`` has not been implemented
6690             (``datetime64`` or ``timedelta64``).
6691         fill_value : Any, default: NaN
6692             Value to be filled in case all of the values along a dimension are
6693             null.  By default this is NaN.  The fill value and result are
6694             automatically converted to a compatible dtype if possible.
6695             Ignored if ``skipna`` is False.
6696         keep_attrs : bool, default: False
6697             If True, the attributes (``attrs``) will be copied from the
6698             original object to the new one.  If False (default), the new object
6699             will be returned without attributes.
6700 
6701         Returns
6702         -------
6703         reduced : Dataset
6704             New `Dataset` object with `idxmin` applied to its data and the
6705             indicated dimension removed.
6706 
6707         See Also
6708         --------
6709         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
6710 
6711         Examples
6712         --------
6713         >>> array1 = xr.DataArray(
6714         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6715         ... )
6716         >>> array2 = xr.DataArray(
6717         ...     [
6718         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6719         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6720         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6721         ...     ],
6722         ...     dims=["y", "x"],
6723         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6724         ... )
6725         >>> ds = xr.Dataset({"int": array1, "float": array2})
6726         >>> ds.min(dim="x")
6727         <xarray.Dataset>
6728         Dimensions:  (y: 3)
6729         Coordinates:
6730           * y        (y) int64 -1 0 1
6731         Data variables:
6732             int      int64 -2
6733             float    (y) float64 -2.0 -4.0 1.0
6734         >>> ds.argmin(dim="x")
6735         <xarray.Dataset>
6736         Dimensions:  (y: 3)
6737         Coordinates:
6738           * y        (y) int64 -1 0 1
6739         Data variables:
6740             int      int64 4
6741             float    (y) int64 4 0 2
6742         >>> ds.idxmin(dim="x")
6743         <xarray.Dataset>
6744         Dimensions:  (y: 3)
6745         Coordinates:
6746           * y        (y) int64 -1 0 1
6747         Data variables:
6748             int      <U1 'e'
6749             float    (y) object 'e' 'a' 'c'
6750         """
6751         return self.map(
6752             methodcaller(
6753                 "idxmin",
6754                 dim=dim,
6755                 skipna=skipna,
6756                 fill_value=fill_value,
6757                 keep_attrs=keep_attrs,
6758             )
6759         )
6760 
6761     def idxmax(
6762         self,
6763         dim: Hashable = None,
6764         skipna: bool = None,
6765         fill_value: Any = dtypes.NA,
6766         keep_attrs: bool = None,
6767     ) -> "Dataset":
6768         """Return the coordinate label of the maximum value along a dimension.
6769 
6770         Returns a new `Dataset` named after the dimension with the values of
6771         the coordinate labels along that dimension corresponding to maximum
6772         values along that dimension.
6773 
6774         In comparison to :py:meth:`~Dataset.argmax`, this returns the
6775         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
6776 
6777         Parameters
6778         ----------
6779         dim : str, optional
6780             Dimension over which to apply `idxmax`.  This is optional for 1D
6781             variables, but required for variables with 2 or more dimensions.
6782         skipna : bool or None, default: None
6783             If True, skip missing values (as marked by NaN). By default, only
6784             skips missing values for ``float``, ``complex``, and ``object``
6785             dtypes; other dtypes either do not have a sentinel missing value
6786             (``int``) or ``skipna=True`` has not been implemented
6787             (``datetime64`` or ``timedelta64``).
6788         fill_value : Any, default: NaN
6789             Value to be filled in case all of the values along a dimension are
6790             null.  By default this is NaN.  The fill value and result are
6791             automatically converted to a compatible dtype if possible.
6792             Ignored if ``skipna`` is False.
6793         keep_attrs : bool, default: False
6794             If True, the attributes (``attrs``) will be copied from the
6795             original object to the new one.  If False (default), the new object
6796             will be returned without attributes.
6797 
6798         Returns
6799         -------
6800         reduced : Dataset
6801             New `Dataset` object with `idxmax` applied to its data and the
6802             indicated dimension removed.
6803 
6804         See Also
6805         --------
6806         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
6807 
6808         Examples
6809         --------
6810         >>> array1 = xr.DataArray(
6811         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6812         ... )
6813         >>> array2 = xr.DataArray(
6814         ...     [
6815         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6816         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6817         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6818         ...     ],
6819         ...     dims=["y", "x"],
6820         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6821         ... )
6822         >>> ds = xr.Dataset({"int": array1, "float": array2})
6823         >>> ds.max(dim="x")
6824         <xarray.Dataset>
6825         Dimensions:  (y: 3)
6826         Coordinates:
6827           * y        (y) int64 -1 0 1
6828         Data variables:
6829             int      int64 2
6830             float    (y) float64 2.0 2.0 1.0
6831         >>> ds.argmax(dim="x")
6832         <xarray.Dataset>
6833         Dimensions:  (y: 3)
6834         Coordinates:
6835           * y        (y) int64 -1 0 1
6836         Data variables:
6837             int      int64 1
6838             float    (y) int64 0 2 2
6839         >>> ds.idxmax(dim="x")
6840         <xarray.Dataset>
6841         Dimensions:  (y: 3)
6842         Coordinates:
6843           * y        (y) int64 -1 0 1
6844         Data variables:
6845             int      <U1 'b'
6846             float    (y) object 'a' 'c' 'c'
6847         """
6848         return self.map(
6849             methodcaller(
6850                 "idxmax",
6851                 dim=dim,
6852                 skipna=skipna,
6853                 fill_value=fill_value,
6854                 keep_attrs=keep_attrs,
6855             )
6856         )
6857 
6858     def argmin(self, dim=None, axis=None, **kwargs):
6859         """Indices of the minima of the member variables.
6860 
6861         If there are multiple minima, the indices of the first one found will be
6862         returned.
6863 
6864         Parameters
6865         ----------
6866         dim : str, optional
6867             The dimension over which to find the minimum. By default, finds minimum over
6868             all dimensions - for now returning an int for backward compatibility, but
6869             this is deprecated, in future will be an error, since DataArray.argmin will
6870             return a dict with indices for all dimensions, which does not make sense for
6871             a Dataset.
6872         axis : int, optional
6873             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
6874             can be supplied.
6875         keep_attrs : bool, optional
6876             If True, the attributes (`attrs`) will be copied from the original
6877             object to the new one.  If False (default), the new object will be
6878             returned without attributes.
6879         skipna : bool, optional
6880             If True, skip missing values (as marked by NaN). By default, only
6881             skips missing values for float dtypes; other dtypes either do not
6882             have a sentinel missing value (int) or skipna=True has not been
6883             implemented (object, datetime64 or timedelta64).
6884 
6885         Returns
6886         -------
6887         result : Dataset
6888 
6889         See Also
6890         --------
6891         DataArray.argmin
6892 
6893         """
6894         if dim is None and axis is None:
6895             warnings.warn(
6896                 "Once the behaviour of DataArray.argmin() and Variable.argmin() with "
6897                 "neither dim nor axis argument changes to return a dict of indices of "
6898                 "each dimension, for consistency it will be an error to call "
6899                 "Dataset.argmin() with no argument, since we don't return a dict of "
6900                 "Datasets.",
6901                 DeprecationWarning,
6902                 stacklevel=2,
6903             )
6904         if (
6905             dim is None
6906             or axis is not None
6907             or (not isinstance(dim, Sequence) and dim is not ...)
6908             or isinstance(dim, str)
6909         ):
6910             # Return int index if single dimension is passed, and is not part of a
6911             # sequence
6912             argmin_func = getattr(duck_array_ops, "argmin")
6913             return self.reduce(argmin_func, dim=dim, axis=axis, **kwargs)
6914         else:
6915             raise ValueError(
6916                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6917                 "dicts cannot be contained in a Dataset, so cannot call "
6918                 "Dataset.argmin() with a sequence or ... for dim"
6919             )
6920 
6921     def argmax(self, dim=None, axis=None, **kwargs):
6922         """Indices of the maxima of the member variables.
6923 
6924         If there are multiple maxima, the indices of the first one found will be
6925         returned.
6926 
6927         Parameters
6928         ----------
6929         dim : str, optional
6930             The dimension over which to find the maximum. By default, finds maximum over
6931             all dimensions - for now returning an int for backward compatibility, but
6932             this is deprecated, in future will be an error, since DataArray.argmax will
6933             return a dict with indices for all dimensions, which does not make sense for
6934             a Dataset.
6935         axis : int, optional
6936             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
6937             can be supplied.
6938         keep_attrs : bool, optional
6939             If True, the attributes (`attrs`) will be copied from the original
6940             object to the new one.  If False (default), the new object will be
6941             returned without attributes.
6942         skipna : bool, optional
6943             If True, skip missing values (as marked by NaN). By default, only
6944             skips missing values for float dtypes; other dtypes either do not
6945             have a sentinel missing value (int) or skipna=True has not been
6946             implemented (object, datetime64 or timedelta64).
6947 
6948         Returns
6949         -------
6950         result : Dataset
6951 
6952         See Also
6953         --------
6954         DataArray.argmax
6955 
6956         """
6957         if dim is None and axis is None:
6958             warnings.warn(
6959                 "Once the behaviour of DataArray.argmax() and Variable.argmax() with "
6960                 "neither dim nor axis argument changes to return a dict of indices of "
6961                 "each dimension, for consistency it will be an error to call "
6962                 "Dataset.argmax() with no argument, since we don't return a dict of "
6963                 "Datasets.",
6964                 DeprecationWarning,
6965                 stacklevel=2,
6966             )
6967         if (
6968             dim is None
6969             or axis is not None
6970             or (not isinstance(dim, Sequence) and dim is not ...)
6971             or isinstance(dim, str)
6972         ):
6973             # Return int index if single dimension is passed, and is not part of a
6974             # sequence
6975             argmax_func = getattr(duck_array_ops, "argmax")
6976             return self.reduce(argmax_func, dim=dim, axis=axis, **kwargs)
6977         else:
6978             raise ValueError(
6979                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6980                 "dicts cannot be contained in a Dataset, so cannot call "
6981                 "Dataset.argmin() with a sequence or ... for dim"
6982             )
6983 
6984 
6985 ops.inject_all_ops_and_reduce_methods(Dataset, array_only=False)

</code><intended_behavior>Assuming the merge() method in this file calls into the core merge logic and handles dataset-level merging specifics, it might need to be updated to correctly pass through the new combine_attrs option and to handle the expected behavior of dropping conflicting attributes when combine_attrs="drop_conflicts" is specified. This ensures that dataset-level merge operations respect the new option, seamlessly integrating it into higher-level APIs. The existing functionality for handling other combine_attrs options must remain unaffected.</intended_behavior>]
2025-10-15 21:49:58.740 | INFO     | Search completed. Bug locations: [<file>xarray/core/merge.py</file>
<class>None</class>
<method>None</method>
<code>
1 from typing import (
2     TYPE_CHECKING,
3     AbstractSet,
4     Any,
5     Dict,
6     Hashable,
7     Iterable,
8     List,
9     Mapping,
10     NamedTuple,
11     Optional,
12     Sequence,
13     Set,
14     Tuple,
15     Union,
16 )
17 
18 import pandas as pd
19 
20 from . import dtypes, pdcompat
21 from .alignment import deep_align
22 from .duck_array_ops import lazy_array_equiv
23 from .utils import Frozen, compat_dict_union, dict_equiv
24 from .variable import Variable, as_variable, assert_unique_multiindex_level_names
25 
26 if TYPE_CHECKING:
27     from .coordinates import Coordinates
28     from .dataarray import DataArray
29     from .dataset import Dataset
30 
31     DimsLike = Union[Hashable, Sequence[Hashable]]
32     ArrayLike = Any
33     VariableLike = Union[
34         ArrayLike,
35         Tuple[DimsLike, ArrayLike],
36         Tuple[DimsLike, ArrayLike, Mapping],
37         Tuple[DimsLike, ArrayLike, Mapping, Mapping],
38     ]
39     XarrayValue = Union[DataArray, Variable, VariableLike]
40     DatasetLike = Union[Dataset, Mapping[Hashable, XarrayValue]]
41     CoercibleValue = Union[XarrayValue, pd.Series, pd.DataFrame]
42     CoercibleMapping = Union[Dataset, Mapping[Hashable, CoercibleValue]]
43 
44 
45 PANDAS_TYPES = (pd.Series, pd.DataFrame, pdcompat.Panel)
46 
47 _VALID_COMPAT = Frozen(
48     {
49         "identical": 0,
50         "equals": 1,
51         "broadcast_equals": 2,
52         "minimal": 3,
53         "no_conflicts": 4,
54         "override": 5,
55     }
56 )
57 
58 
59 def broadcast_dimension_size(variables: List[Variable]) -> Dict[Hashable, int]:
60     """Extract dimension sizes from a dictionary of variables.
61 
62     Raises ValueError if any dimensions have different sizes.
63     """
64     dims: Dict[Hashable, int] = {}
65     for var in variables:
66         for dim, size in zip(var.dims, var.shape):
67             if dim in dims and size != dims[dim]:
68                 raise ValueError("index %r not aligned" % dim)
69             dims[dim] = size
70     return dims
71 
72 
73 class MergeError(ValueError):
74     """Error class for merge failures due to incompatible arguments."""
75 
76     # inherits from ValueError for backward compatibility
77     # TODO: move this to an xarray.exceptions module?
78 
79 
80 def unique_variable(
81     name: Hashable,
82     variables: List[Variable],
83     compat: str = "broadcast_equals",
84     equals: bool = None,
85 ) -> Variable:
86     """Return the unique variable from a list of variables or raise MergeError.
87 
88     Parameters
89     ----------
90     name : hashable
91         Name for this variable.
92     variables : list of Variable
93         List of Variable objects, all of which go by the same name in different
94         inputs.
95     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
96         Type of equality check to use.
97     equals : None or bool, optional
98         corresponding to result of compat test
99 
100     Returns
101     -------
102     Variable to use in the result.
103 
104     Raises
105     ------
106     MergeError: if any of the variables are not equal.
107     """
108     out = variables[0]
109 
110     if len(variables) == 1 or compat == "override":
111         return out
112 
113     combine_method = None
114 
115     if compat == "minimal":
116         compat = "broadcast_equals"
117 
118     if compat == "broadcast_equals":
119         dim_lengths = broadcast_dimension_size(variables)
120         out = out.set_dims(dim_lengths)
121 
122     if compat == "no_conflicts":
123         combine_method = "fillna"
124 
125     if equals is None:
126         # first check without comparing values i.e. no computes
127         for var in variables[1:]:
128             equals = getattr(out, compat)(var, equiv=lazy_array_equiv)
129             if equals is not True:
130                 break
131 
132         if equals is None:
133             # now compare values with minimum number of computes
134             out = out.compute()
135             for var in variables[1:]:
136                 equals = getattr(out, compat)(var)
137                 if not equals:
138                     break
139 
140     if not equals:
141         raise MergeError(
142             f"conflicting values for variable {name!r} on objects to be combined. "
143             "You can skip this check by specifying compat='override'."
144         )
145 
146     if combine_method:
147         for var in variables[1:]:
148             out = getattr(out, combine_method)(var)
149 
150     return out
151 
152 
153 def _assert_compat_valid(compat):
154     if compat not in _VALID_COMPAT:
155         raise ValueError(
156             "compat={!r} invalid: must be {}".format(compat, set(_VALID_COMPAT))
157         )
158 
159 
160 MergeElement = Tuple[Variable, Optional[pd.Index]]
161 
162 
163 def merge_collected(
164     grouped: Dict[Hashable, List[MergeElement]],
165     prioritized: Mapping[Hashable, MergeElement] = None,
166     compat: str = "minimal",
167 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
168     """Merge dicts of variables, while resolving conflicts appropriately.
169 
170     Parameters
171     ----------
172     grouped : mapping
173     prioritized : mapping
174     compat : str
175         Type of equality check to use when checking for conflicts.
176 
177     Returns
178     -------
179     Dict with keys taken by the union of keys on list_of_mappings,
180     and Variable values corresponding to those that should be found on the
181     merged result.
182     """
183     if prioritized is None:
184         prioritized = {}
185 
186     _assert_compat_valid(compat)
187 
188     merged_vars: Dict[Hashable, Variable] = {}
189     merged_indexes: Dict[Hashable, pd.Index] = {}
190 
191     for name, elements_list in grouped.items():
192         if name in prioritized:
193             variable, index = prioritized[name]
194             merged_vars[name] = variable
195             if index is not None:
196                 merged_indexes[name] = index
197         else:
198             indexed_elements = [
199                 (variable, index)
200                 for variable, index in elements_list
201                 if index is not None
202             ]
203 
204             if indexed_elements:
205                 # TODO(shoyer): consider adjusting this logic. Are we really
206                 # OK throwing away variable without an index in favor of
207                 # indexed variables, without even checking if values match?
208                 variable, index = indexed_elements[0]
209                 for _, other_index in indexed_elements[1:]:
210                     if not index.equals(other_index):
211                         raise MergeError(
212                             "conflicting values for index %r on objects to be "
213                             "combined:\nfirst value: %r\nsecond value: %r"
214                             % (name, index, other_index)
215                         )
216                 if compat == "identical":
217                     for other_variable, _ in indexed_elements[1:]:
218                         if not dict_equiv(variable.attrs, other_variable.attrs):
219                             raise MergeError(
220                                 "conflicting attribute values on combined "
221                                 "variable %r:\nfirst value: %r\nsecond value: %r"
222                                 % (name, variable.attrs, other_variable.attrs)
223                             )
224                 merged_vars[name] = variable
225                 merged_indexes[name] = index
226             else:
227                 variables = [variable for variable, _ in elements_list]
228                 try:
229                     merged_vars[name] = unique_variable(name, variables, compat)
230                 except MergeError:
231                     if compat != "minimal":
232                         # we need more than "minimal" compatibility (for which
233                         # we drop conflicting coordinates)
234                         raise
235 
236     return merged_vars, merged_indexes
237 
238 
239 def collect_variables_and_indexes(
240     list_of_mappings: "List[DatasetLike]",
241 ) -> Dict[Hashable, List[MergeElement]]:
242     """Collect variables and indexes from list of mappings of xarray objects.
243 
244     Mappings must either be Dataset objects, or have values of one of the
245     following types:
246     - an xarray.Variable
247     - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in
248       an xarray.Variable
249     - or an xarray.DataArray
250     """
251     from .dataarray import DataArray
252     from .dataset import Dataset
253 
254     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}
255 
256     def append(name, variable, index):
257         values = grouped.setdefault(name, [])
258         values.append((variable, index))
259 
260     def append_all(variables, indexes):
261         for name, variable in variables.items():
262             append(name, variable, indexes.get(name))
263 
264     for mapping in list_of_mappings:
265         if isinstance(mapping, Dataset):
266             append_all(mapping.variables, mapping.indexes)
267             continue
268 
269         for name, variable in mapping.items():
270             if isinstance(variable, DataArray):
271                 coords = variable._coords.copy()  # use private API for speed
272                 indexes = dict(variable.indexes)
273                 # explicitly overwritten variables should take precedence
274                 coords.pop(name, None)
275                 indexes.pop(name, None)
276                 append_all(coords, indexes)
277 
278             variable = as_variable(variable, name=name)
279             if variable.dims == (name,):
280                 variable = variable.to_index_variable()
281                 index = variable.to_index()
282             else:
283                 index = None
284             append(name, variable, index)
285 
286     return grouped
287 
288 
289 def collect_from_coordinates(
290     list_of_coords: "List[Coordinates]",
291 ) -> Dict[Hashable, List[MergeElement]]:
292     """Collect variables and indexes to be merged from Coordinate objects."""
293     grouped: Dict[Hashable, List[Tuple[Variable, pd.Index]]] = {}
294 
295     for coords in list_of_coords:
296         variables = coords.variables
297         indexes = coords.indexes
298         for name, variable in variables.items():
299             value = grouped.setdefault(name, [])
300             value.append((variable, indexes.get(name)))
301     return grouped
302 
303 
304 def merge_coordinates_without_align(
305     objects: "List[Coordinates]",
306     prioritized: Mapping[Hashable, MergeElement] = None,
307     exclude_dims: AbstractSet = frozenset(),
308 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
309     """Merge variables/indexes from coordinates without automatic alignments.
310 
311     This function is used for merging coordinate from pre-existing xarray
312     objects.
313     """
314     collected = collect_from_coordinates(objects)
315 
316     if exclude_dims:
317         filtered: Dict[Hashable, List[MergeElement]] = {}
318         for name, elements in collected.items():
319             new_elements = [
320                 (variable, index)
321                 for variable, index in elements
322                 if exclude_dims.isdisjoint(variable.dims)
323             ]
324             if new_elements:
325                 filtered[name] = new_elements
326     else:
327         filtered = collected
328 
329     return merge_collected(filtered, prioritized)
330 
331 
332 def determine_coords(
333     list_of_mappings: Iterable["DatasetLike"],
334 ) -> Tuple[Set[Hashable], Set[Hashable]]:
335     """Given a list of dicts with xarray object values, identify coordinates.
336 
337     Parameters
338     ----------
339     list_of_mappings : list of dict or list of Dataset
340         Of the same form as the arguments to expand_variable_dicts.
341 
342     Returns
343     -------
344     coord_names : set of variable names
345     noncoord_names : set of variable names
346         All variable found in the input should appear in either the set of
347         coordinate or non-coordinate names.
348     """
349     from .dataarray import DataArray
350     from .dataset import Dataset
351 
352     coord_names: Set[Hashable] = set()
353     noncoord_names: Set[Hashable] = set()
354 
355     for mapping in list_of_mappings:
356         if isinstance(mapping, Dataset):
357             coord_names.update(mapping.coords)
358             noncoord_names.update(mapping.data_vars)
359         else:
360             for name, var in mapping.items():
361                 if isinstance(var, DataArray):
362                     coords = set(var._coords)  # use private API for speed
363                     # explicitly overwritten variables should take precedence
364                     coords.discard(name)
365                     coord_names.update(coords)
366 
367     return coord_names, noncoord_names
368 
369 
370 def coerce_pandas_values(objects: Iterable["CoercibleMapping"]) -> List["DatasetLike"]:
371     """Convert pandas values found in a list of labeled objects.
372 
373     Parameters
374     ----------
375     objects : list of Dataset or mapping
376         The mappings may contain any sort of objects coercible to
377         xarray.Variables as keys, including pandas objects.
378 
379     Returns
380     -------
381     List of Dataset or dictionary objects. Any inputs or values in the inputs
382     that were pandas objects have been converted into native xarray objects.
383     """
384     from .dataarray import DataArray
385     from .dataset import Dataset
386 
387     out = []
388     for obj in objects:
389         if isinstance(obj, Dataset):
390             variables: "DatasetLike" = obj
391         else:
392             variables = {}
393             if isinstance(obj, PANDAS_TYPES):
394                 obj = dict(obj.iteritems())
395             for k, v in obj.items():
396                 if isinstance(v, PANDAS_TYPES):
397                     v = DataArray(v)
398                 variables[k] = v
399         out.append(variables)
400     return out
401 
402 
403 def _get_priority_vars_and_indexes(
404     objects: List["DatasetLike"], priority_arg: Optional[int], compat: str = "equals"
405 ) -> Dict[Hashable, MergeElement]:
406     """Extract the priority variable from a list of mappings.
407 
408     We need this method because in some cases the priority argument itself
409     might have conflicting values (e.g., if it is a dict with two DataArray
410     values with conflicting coordinate values).
411 
412     Parameters
413     ----------
414     objects : list of dict-like of Variable
415         Dictionaries in which to find the priority variables.
416     priority_arg : int or None
417         Integer object whose variable should take priority.
418     compat : {"identical", "equals", "broadcast_equals", "no_conflicts"}, optional
419         Compatibility checks to use when merging variables.
420 
421     Returns
422     -------
423     A dictionary of variables and associated indexes (if any) to prioritize.
424     """
425     if priority_arg is None:
426         return {}
427 
428     collected = collect_variables_and_indexes([objects[priority_arg]])
429     variables, indexes = merge_collected(collected, compat=compat)
430     grouped: Dict[Hashable, MergeElement] = {}
431     for name, variable in variables.items():
432         grouped[name] = (variable, indexes.get(name))
433     return grouped
434 
435 
436 def merge_coords(
437     objects: Iterable["CoercibleMapping"],
438     compat: str = "minimal",
439     join: str = "outer",
440     priority_arg: Optional[int] = None,
441     indexes: Optional[Mapping[Hashable, pd.Index]] = None,
442     fill_value: object = dtypes.NA,
443 ) -> Tuple[Dict[Hashable, Variable], Dict[Hashable, pd.Index]]:
444     """Merge coordinate variables.
445 
446     See merge_core below for argument descriptions. This works similarly to
447     merge_core, except everything we don't worry about whether variables are
448     coordinates or not.
449     """
450     _assert_compat_valid(compat)
451     coerced = coerce_pandas_values(objects)
452     aligned = deep_align(
453         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value
454     )
455     collected = collect_variables_and_indexes(aligned)
456     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)
457     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)
458     assert_unique_multiindex_level_names(variables)
459     return variables, out_indexes
460 
461 
462 def merge_data_and_coords(data, coords, compat="broadcast_equals", join="outer"):
463     """Used in Dataset.__init__."""
464     objects = [data, coords]
465     explicit_coords = coords.keys()
466     indexes = dict(_extract_indexes_from_coords(coords))
467     return merge_core(
468         objects, compat, join, explicit_coords=explicit_coords, indexes=indexes
469     )
470 
471 
472 def _extract_indexes_from_coords(coords):
473     """Yields the name & index of valid indexes from a mapping of coords"""
474     for name, variable in coords.items():
475         variable = as_variable(variable, name=name)
476         if variable.dims == (name,):
477             yield name, variable.to_index()
478 
479 
480 def assert_valid_explicit_coords(variables, dims, explicit_coords):
481     """Validate explicit coordinate names/dims.
482 
483     Raise a MergeError if an explicit coord shares a name with a dimension
484     but is comprised of arbitrary dimensions.
485     """
486     for coord_name in explicit_coords:
487         if coord_name in dims and variables[coord_name].dims != (coord_name,):
488             raise MergeError(
489                 "coordinate %s shares a name with a dataset dimension, but is "
490                 "not a 1D variable along that dimension. This is disallowed "
491                 "by the xarray data model." % coord_name
492             )
493 
494 
495 def merge_attrs(variable_attrs, combine_attrs):
496     """Combine attributes from different variables according to combine_attrs"""
497     if not variable_attrs:
498         # no attributes to merge
499         return None
500 
501     if combine_attrs == "drop":
502         return {}
503     elif combine_attrs == "override":
504         return dict(variable_attrs[0])
505     elif combine_attrs == "no_conflicts":
506         result = dict(variable_attrs[0])
507         for attrs in variable_attrs[1:]:
508             try:
509                 result = compat_dict_union(result, attrs)
510             except ValueError:
511                 raise MergeError(
512                     "combine_attrs='no_conflicts', but some values are not "
513                     "the same. Merging %s with %s" % (str(result), str(attrs))
514                 )
515         return result
516     elif combine_attrs == "identical":
517         result = dict(variable_attrs[0])
518         for attrs in variable_attrs[1:]:
519             if not dict_equiv(result, attrs):
520                 raise MergeError(
521                     "combine_attrs='identical', but attrs differ. First is %s "
522                     ", other is %s." % (str(result), str(attrs))
523                 )
524         return result
525     else:
526         raise ValueError("Unrecognised value for combine_attrs=%s" % combine_attrs)
527 
528 
529 class _MergeResult(NamedTuple):
530     variables: Dict[Hashable, Variable]
531     coord_names: Set[Hashable]
532     dims: Dict[Hashable, int]
533     indexes: Dict[Hashable, pd.Index]
534     attrs: Dict[Hashable, Any]
535 
536 
537 def merge_core(
538     objects: Iterable["CoercibleMapping"],
539     compat: str = "broadcast_equals",
540     join: str = "outer",
541     combine_attrs: Optional[str] = "override",
542     priority_arg: Optional[int] = None,
543     explicit_coords: Optional[Sequence] = None,
544     indexes: Optional[Mapping[Hashable, pd.Index]] = None,
545     fill_value: object = dtypes.NA,
546 ) -> _MergeResult:
547     """Core logic for merging labeled objects.
548 
549     This is not public API.
550 
551     Parameters
552     ----------
553     objects : list of mapping
554         All values must be convertable to labeled arrays.
555     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
556         Compatibility checks to use when merging variables.
557     join : {"outer", "inner", "left", "right"}, optional
558         How to combine objects with different indexes.
559     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, optional
560         How to combine attributes of objects
561     priority_arg : int, optional
562         Optional argument in `objects` that takes precedence over the others.
563     explicit_coords : set, optional
564         An explicit list of variables from `objects` that are coordinates.
565     indexes : dict, optional
566         Dictionary with values given by pandas.Index objects.
567     fill_value : scalar, optional
568         Value to use for newly missing values
569 
570     Returns
571     -------
572     variables : dict
573         Dictionary of Variable objects.
574     coord_names : set
575         Set of coordinate names.
576     dims : dict
577         Dictionary mapping from dimension names to sizes.
578     attrs : dict
579         Dictionary of attributes
580 
581     Raises
582     ------
583     MergeError if the merge cannot be done successfully.
584     """
585     from .dataarray import DataArray
586     from .dataset import Dataset, calculate_dimensions
587 
588     _assert_compat_valid(compat)
589 
590     coerced = coerce_pandas_values(objects)
591     aligned = deep_align(
592         coerced, join=join, copy=False, indexes=indexes, fill_value=fill_value
593     )
594     collected = collect_variables_and_indexes(aligned)
595 
596     prioritized = _get_priority_vars_and_indexes(aligned, priority_arg, compat=compat)
597     variables, out_indexes = merge_collected(collected, prioritized, compat=compat)
598     assert_unique_multiindex_level_names(variables)
599 
600     dims = calculate_dimensions(variables)
601 
602     coord_names, noncoord_names = determine_coords(coerced)
603     if explicit_coords is not None:
604         assert_valid_explicit_coords(variables, dims, explicit_coords)
605         coord_names.update(explicit_coords)
606     for dim, size in dims.items():
607         if dim in variables:
608             coord_names.add(dim)
609     ambiguous_coords = coord_names.intersection(noncoord_names)
610     if ambiguous_coords:
611         raise MergeError(
612             "unable to determine if these variables should be "
613             "coordinates or not in the merged result: %s" % ambiguous_coords
614         )
615 
616     attrs = merge_attrs(
617         [
618             var.attrs
619             for var in coerced
620             if isinstance(var, Dataset) or isinstance(var, DataArray)
621         ],
622         combine_attrs,
623     )
624 
625     return _MergeResult(variables, coord_names, dims, out_indexes, attrs)
626 
627 
628 def merge(
629     objects: Iterable[Union["DataArray", "CoercibleMapping"]],
630     compat: str = "no_conflicts",
631     join: str = "outer",
632     fill_value: object = dtypes.NA,
633     combine_attrs: str = "drop",
634 ) -> "Dataset":
635     """Merge any number of xarray objects into a single Dataset as variables.
636 
637     Parameters
638     ----------
639     objects : iterable of Dataset or iterable of DataArray or iterable of dict-like
640         Merge together all variables from these objects. If any of them are
641         DataArray objects, they must have a name.
642     compat : {"identical", "equals", "broadcast_equals", "no_conflicts", "override"}, optional
643         String indicating how to compare variables of the same name for
644         potential conflicts:
645 
646         - "broadcast_equals": all values must be equal when variables are
647           broadcast against each other to ensure common dimensions.
648         - "equals": all values and dimensions must be the same.
649         - "identical": all values, dimensions and attributes must be the
650           same.
651         - "no_conflicts": only values which are not null in both datasets
652           must be equal. The returned dataset then contains the combination
653           of all non-null values.
654         - "override": skip comparing and pick variable from first dataset
655     join : {"outer", "inner", "left", "right", "exact"}, optional
656         String indicating how to combine differing indexes in objects.
657 
658         - "outer": use the union of object indexes
659         - "inner": use the intersection of object indexes
660         - "left": use indexes from the first object with each dimension
661         - "right": use indexes from the last object with each dimension
662         - "exact": instead of aligning, raise `ValueError` when indexes to be
663           aligned are not equal
664         - "override": if indexes are of same size, rewrite indexes to be
665           those of the first object with that dimension. Indexes for the same
666           dimension must have the same size in all objects.
667     fill_value : scalar or dict-like, optional
668         Value to use for newly missing values. If a dict-like, maps
669         variable names to fill values. Use a data array's name to
670         refer to its values.
671     combine_attrs : {"drop", "identical", "no_conflicts", "override"}, \
672                     default: "drop"
673         String indicating how to combine attrs of the objects being merged:
674 
675         - "drop": empty attrs on returned Dataset.
676         - "identical": all attrs must be the same on every object.
677         - "no_conflicts": attrs from all objects are combined, any that have
678           the same name must also have the same value.
679         - "override": skip comparing and copy attrs from the first dataset to
680           the result.
681 
682     Returns
683     -------
684     Dataset
685         Dataset with combined variables from each object.
686 
687     Examples
688     --------
689     >>> import xarray as xr
690     >>> x = xr.DataArray(
691     ...     [[1.0, 2.0], [3.0, 5.0]],
692     ...     dims=("lat", "lon"),
693     ...     coords={"lat": [35.0, 40.0], "lon": [100.0, 120.0]},
694     ...     name="var1",
695     ... )
696     >>> y = xr.DataArray(
697     ...     [[5.0, 6.0], [7.0, 8.0]],
698     ...     dims=("lat", "lon"),
699     ...     coords={"lat": [35.0, 42.0], "lon": [100.0, 150.0]},
700     ...     name="var2",
701     ... )
702     >>> z = xr.DataArray(
703     ...     [[0.0, 3.0], [4.0, 9.0]],
704     ...     dims=("time", "lon"),
705     ...     coords={"time": [30.0, 60.0], "lon": [100.0, 150.0]},
706     ...     name="var3",
707     ... )
708 
709     >>> x
710     <xarray.DataArray 'var1' (lat: 2, lon: 2)>
711     array([[1., 2.],
712            [3., 5.]])
713     Coordinates:
714       * lat      (lat) float64 35.0 40.0
715       * lon      (lon) float64 100.0 120.0
716 
717     >>> y
718     <xarray.DataArray 'var2' (lat: 2, lon: 2)>
719     array([[5., 6.],
720            [7., 8.]])
721     Coordinates:
722       * lat      (lat) float64 35.0 42.0
723       * lon      (lon) float64 100.0 150.0
724 
725     >>> z
726     <xarray.DataArray 'var3' (time: 2, lon: 2)>
727     array([[0., 3.],
728            [4., 9.]])
729     Coordinates:
730       * time     (time) float64 30.0 60.0
731       * lon      (lon) float64 100.0 150.0
732 
733     >>> xr.merge([x, y, z])
734     <xarray.Dataset>
735     Dimensions:  (lat: 3, lon: 3, time: 2)
736     Coordinates:
737       * lat      (lat) float64 35.0 40.0 42.0
738       * lon      (lon) float64 100.0 120.0 150.0
739       * time     (time) float64 30.0 60.0
740     Data variables:
741         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
742         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
743         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
744 
745     >>> xr.merge([x, y, z], compat="identical")
746     <xarray.Dataset>
747     Dimensions:  (lat: 3, lon: 3, time: 2)
748     Coordinates:
749       * lat      (lat) float64 35.0 40.0 42.0
750       * lon      (lon) float64 100.0 120.0 150.0
751       * time     (time) float64 30.0 60.0
752     Data variables:
753         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
754         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
755         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
756 
757     >>> xr.merge([x, y, z], compat="equals")
758     <xarray.Dataset>
759     Dimensions:  (lat: 3, lon: 3, time: 2)
760     Coordinates:
761       * lat      (lat) float64 35.0 40.0 42.0
762       * lon      (lon) float64 100.0 120.0 150.0
763       * time     (time) float64 30.0 60.0
764     Data variables:
765         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
766         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
767         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
768 
769     >>> xr.merge([x, y, z], compat="equals", fill_value=-999.0)
770     <xarray.Dataset>
771     Dimensions:  (lat: 3, lon: 3, time: 2)
772     Coordinates:
773       * lat      (lat) float64 35.0 40.0 42.0
774       * lon      (lon) float64 100.0 120.0 150.0
775       * time     (time) float64 30.0 60.0
776     Data variables:
777         var1     (lat, lon) float64 1.0 2.0 -999.0 3.0 ... -999.0 -999.0 -999.0
778         var2     (lat, lon) float64 5.0 -999.0 6.0 -999.0 ... -999.0 7.0 -999.0 8.0
779         var3     (time, lon) float64 0.0 -999.0 3.0 4.0 -999.0 9.0
780 
781     >>> xr.merge([x, y, z], join="override")
782     <xarray.Dataset>
783     Dimensions:  (lat: 2, lon: 2, time: 2)
784     Coordinates:
785       * lat      (lat) float64 35.0 40.0
786       * lon      (lon) float64 100.0 120.0
787       * time     (time) float64 30.0 60.0
788     Data variables:
789         var1     (lat, lon) float64 1.0 2.0 3.0 5.0
790         var2     (lat, lon) float64 5.0 6.0 7.0 8.0
791         var3     (time, lon) float64 0.0 3.0 4.0 9.0
792 
793     >>> xr.merge([x, y, z], join="inner")
794     <xarray.Dataset>
795     Dimensions:  (lat: 1, lon: 1, time: 2)
796     Coordinates:
797       * lat      (lat) float64 35.0
798       * lon      (lon) float64 100.0
799       * time     (time) float64 30.0 60.0
800     Data variables:
801         var1     (lat, lon) float64 1.0
802         var2     (lat, lon) float64 5.0
803         var3     (time, lon) float64 0.0 4.0
804 
805     >>> xr.merge([x, y, z], compat="identical", join="inner")
806     <xarray.Dataset>
807     Dimensions:  (lat: 1, lon: 1, time: 2)
808     Coordinates:
809       * lat      (lat) float64 35.0
810       * lon      (lon) float64 100.0
811       * time     (time) float64 30.0 60.0
812     Data variables:
813         var1     (lat, lon) float64 1.0
814         var2     (lat, lon) float64 5.0
815         var3     (time, lon) float64 0.0 4.0
816 
817     >>> xr.merge([x, y, z], compat="broadcast_equals", join="outer")
818     <xarray.Dataset>
819     Dimensions:  (lat: 3, lon: 3, time: 2)
820     Coordinates:
821       * lat      (lat) float64 35.0 40.0 42.0
822       * lon      (lon) float64 100.0 120.0 150.0
823       * time     (time) float64 30.0 60.0
824     Data variables:
825         var1     (lat, lon) float64 1.0 2.0 nan 3.0 5.0 nan nan nan nan
826         var2     (lat, lon) float64 5.0 nan 6.0 nan nan nan 7.0 nan 8.0
827         var3     (time, lon) float64 0.0 nan 3.0 4.0 nan 9.0
828 
829     >>> xr.merge([x, y, z], join="exact")
830     Traceback (most recent call last):
831     ...
832     ValueError: indexes along dimension 'lat' are not equal
833 
834     Raises
835     ------
836     xarray.MergeError
837         If any variables with the same name have conflicting values.
838 
839     See also
840     --------
841     concat
842     """
843     from .dataarray import DataArray
844     from .dataset import Dataset
845 
846     dict_like_objects = []
847     for obj in objects:
848         if not isinstance(obj, (DataArray, Dataset, dict)):
849             raise TypeError(
850                 "objects must be an iterable containing only "
851                 "Dataset(s), DataArray(s), and dictionaries."
852             )
853 
854         obj = obj.to_dataset(promote_attrs=True) if isinstance(obj, DataArray) else obj
855         dict_like_objects.append(obj)
856 
857     merge_result = merge_core(
858         dict_like_objects,
859         compat,
860         join,
861         combine_attrs=combine_attrs,
862         fill_value=fill_value,
863     )
864     merged = Dataset._construct_direct(**merge_result._asdict())
865     return merged
866 
867 
868 def dataset_merge_method(
869     dataset: "Dataset",
870     other: "CoercibleMapping",
871     overwrite_vars: Union[Hashable, Iterable[Hashable]],
872     compat: str,
873     join: str,
874     fill_value: Any,
875 ) -> _MergeResult:
876     """Guts of the Dataset.merge method."""
877     # we are locked into supporting overwrite_vars for the Dataset.merge
878     # method due for backwards compatibility
879     # TODO: consider deprecating it?
880 
881     if isinstance(overwrite_vars, Iterable) and not isinstance(overwrite_vars, str):
882         overwrite_vars = set(overwrite_vars)
883     else:
884         overwrite_vars = {overwrite_vars}
885 
886     if not overwrite_vars:
887         objs = [dataset, other]
888         priority_arg = None
889     elif overwrite_vars == set(other):
890         objs = [dataset, other]
891         priority_arg = 1
892     else:
893         other_overwrite: Dict[Hashable, CoercibleValue] = {}
894         other_no_overwrite: Dict[Hashable, CoercibleValue] = {}
895         for k, v in other.items():
896             if k in overwrite_vars:
897                 other_overwrite[k] = v
898             else:
899                 other_no_overwrite[k] = v
900         objs = [dataset, other_no_overwrite, other_overwrite]
901         priority_arg = 2
902 
903     return merge_core(
904         objs, compat, join, priority_arg=priority_arg, fill_value=fill_value
905     )
906 
907 
908 def dataset_update_method(
909     dataset: "Dataset", other: "CoercibleMapping"
910 ) -> _MergeResult:
911     """Guts of the Dataset.update method.
912 
913     This drops a duplicated coordinates from `other` if `other` is not an
914     `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,
915     GH2180).
916     """
917     from .dataarray import DataArray
918     from .dataset import Dataset
919 
920     if not isinstance(other, Dataset):
921         other = dict(other)
922         for key, value in other.items():
923             if isinstance(value, DataArray):
924                 # drop conflicting coordinates
925                 coord_names = [
926                     c
927                     for c in value.coords
928                     if c not in value.dims and c in dataset.coords
929                 ]
930                 if coord_names:
931                     other[key] = value.drop_vars(coord_names)
932 
933     # use ds.coords and not ds.indexes, else str coords are cast to object
934     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}
935     return merge_core(
936         [dataset, other],
937         priority_arg=1,
938         indexes=indexes,
939         combine_attrs="override",
940     )

</code><intended_behavior>This file should be modified to include the new combine_attrs="drop_conflicts" option. The merge function within this file currently processes combine_attrs to decide how to combine attributes from different Dataset objects being merged. The intended behavior is to extend this logic to support a new option, "drop_conflicts", which will combine attributes from all objects like the existing "no_conflicts" option but will drop any attribute where conflicting values are found instead of raising an error. This behavior will offer a more flexible approach to handling attributes during merges, fitting scenarios where users prefer to avoid errors over attribute conflicts. This adjustment should preserve all existing functionality and only extend additional behavior for managing attrs.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>None</class>
<method>None</method>
<code>
1 import copy
2 import datetime
3 import functools
4 import sys
5 import warnings
6 from collections import defaultdict
7 from distutils.version import LooseVersion
8 from html import escape
9 from numbers import Number
10 from operator import methodcaller
11 from pathlib import Path
12 from typing import (
13     TYPE_CHECKING,
14     Any,
15     Callable,
16     DefaultDict,
17     Dict,
18     Hashable,
19     Iterable,
20     Iterator,
21     List,
22     Mapping,
23     MutableMapping,
24     Optional,
25     Sequence,
26     Set,
27     Tuple,
28     TypeVar,
29     Union,
30     cast,
31     overload,
32 )
33 
34 import numpy as np
35 import pandas as pd
36 
37 import xarray as xr
38 
39 from ..coding.cftimeindex import _parse_array_of_cftime_strings
40 from ..plot.dataset_plot import _Dataset_PlotMethods
41 from . import (
42     alignment,
43     dtypes,
44     duck_array_ops,
45     formatting,
46     formatting_html,
47     groupby,
48     ops,
49     resample,
50     rolling,
51     utils,
52     weighted,
53 )
54 from .alignment import _broadcast_helper, _get_broadcast_dims_map_common_coords, align
55 from .common import (
56     DataWithCoords,
57     ImplementsDatasetReduce,
58     _contains_datetime_like_objects,
59 )
60 from .coordinates import (
61     DatasetCoordinates,
62     assert_coordinate_consistent,
63     remap_label_indexers,
64 )
65 from .duck_array_ops import datetime_to_numeric
66 from .indexes import (
67     Indexes,
68     default_indexes,
69     isel_variable_and_index,
70     propagate_indexes,
71     remove_unused_levels_categories,
72     roll_index,
73 )
74 from .indexing import is_fancy_indexer
75 from .merge import (
76     dataset_merge_method,
77     dataset_update_method,
78     merge_coordinates_without_align,
79     merge_data_and_coords,
80 )
81 from .missing import get_clean_interp_index
82 from .options import OPTIONS, _get_keep_attrs
83 from .pycompat import is_duck_dask_array, sparse_array_type
84 from .utils import (
85     Default,
86     Frozen,
87     HybridMappingProxy,
88     SortedKeysDict,
89     _default,
90     decode_numpy_dict_values,
91     drop_dims_from_indexers,
92     either_dict_or_kwargs,
93     hashable,
94     infix_dims,
95     is_dict_like,
96     is_scalar,
97     maybe_wrap_array,
98 )
99 from .variable import (
100     IndexVariable,
101     Variable,
102     as_variable,
103     assert_unique_multiindex_level_names,
104     broadcast_variables,
105 )
106 
107 if TYPE_CHECKING:
108     from ..backends import AbstractDataStore, ZarrStore
109     from .dataarray import DataArray
110     from .merge import CoercibleMapping
111 
112     T_DSorDA = TypeVar("T_DSorDA", DataArray, "Dataset")
113 
114     try:
115         from dask.delayed import Delayed
116     except ImportError:
117         Delayed = None
118 
119 
120 # list of attributes of pd.DatetimeIndex that are ndarrays of time info
121 _DATETIMEINDEX_COMPONENTS = [
122     "year",
123     "month",
124     "day",
125     "hour",
126     "minute",
127     "second",
128     "microsecond",
129     "nanosecond",
130     "date",
131     "time",
132     "dayofyear",
133     "weekofyear",
134     "dayofweek",
135     "quarter",
136 ]
137 
138 
139 def _get_virtual_variable(
140     variables, key: Hashable, level_vars: Mapping = None, dim_sizes: Mapping = None
141 ) -> Tuple[Hashable, Hashable, Variable]:
142     """Get a virtual variable (e.g., 'time.year' or a MultiIndex level)
143     from a dict of xarray.Variable objects (if possible)
144     """
145     if level_vars is None:
146         level_vars = {}
147     if dim_sizes is None:
148         dim_sizes = {}
149 
150     if key in dim_sizes:
151         data = pd.Index(range(dim_sizes[key]), name=key)
152         variable = IndexVariable((key,), data)
153         return key, key, variable
154 
155     if not isinstance(key, str):
156         raise KeyError(key)
157 
158     split_key = key.split(".", 1)
159     var_name: Optional[str]
160     if len(split_key) == 2:
161         ref_name, var_name = split_key
162     elif len(split_key) == 1:
163         ref_name, var_name = key, None
164     else:
165         raise KeyError(key)
166 
167     if ref_name in level_vars:
168         dim_var = variables[level_vars[ref_name]]
169         ref_var = dim_var.to_index_variable().get_level_variable(ref_name)
170     else:
171         ref_var = variables[ref_name]
172 
173     if var_name is None:
174         virtual_var = ref_var
175         var_name = key
176     else:
177         if _contains_datetime_like_objects(ref_var):
178             ref_var = xr.DataArray(ref_var)
179             data = getattr(ref_var.dt, var_name).data
180         else:
181             data = getattr(ref_var, var_name).data
182         virtual_var = Variable(ref_var.dims, data)
183 
184     return ref_name, var_name, virtual_var
185 
186 
187 def calculate_dimensions(variables: Mapping[Hashable, Variable]) -> Dict[Hashable, int]:
188     """Calculate the dimensions corresponding to a set of variables.
189 
190     Returns dictionary mapping from dimension names to sizes. Raises ValueError
191     if any of the dimension sizes conflict.
192     """
193     dims: Dict[Hashable, int] = {}
194     last_used = {}
195     scalar_vars = {k for k, v in variables.items() if not v.dims}
196     for k, var in variables.items():
197         for dim, size in zip(var.dims, var.shape):
198             if dim in scalar_vars:
199                 raise ValueError(
200                     "dimension %r already exists as a scalar variable" % dim
201                 )
202             if dim not in dims:
203                 dims[dim] = size
204                 last_used[dim] = k
205             elif dims[dim] != size:
206                 raise ValueError(
207                     "conflicting sizes for dimension %r: "
208                     "length %s on %r and length %s on %r"
209                     % (dim, size, k, dims[dim], last_used[dim])
210                 )
211     return dims
212 
213 
214 def merge_indexes(
215     indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]],
216     variables: Mapping[Hashable, Variable],
217     coord_names: Set[Hashable],
218     append: bool = False,
219 ) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:
220     """Merge variables into multi-indexes.
221 
222     Not public API. Used in Dataset and DataArray set_index
223     methods.
224     """
225     vars_to_replace: Dict[Hashable, Variable] = {}
226     vars_to_remove: List[Hashable] = []
227     dims_to_replace: Dict[Hashable, Hashable] = {}
228     error_msg = "{} is not the name of an existing variable."
229 
230     for dim, var_names in indexes.items():
231         if isinstance(var_names, str) or not isinstance(var_names, Sequence):
232             var_names = [var_names]
233 
234         names: List[Hashable] = []
235         codes: List[List[int]] = []
236         levels: List[List[int]] = []
237         current_index_variable = variables.get(dim)
238 
239         for n in var_names:
240             try:
241                 var = variables[n]
242             except KeyError:
243                 raise ValueError(error_msg.format(n))
244             if (
245                 current_index_variable is not None
246                 and var.dims != current_index_variable.dims
247             ):
248                 raise ValueError(
249                     "dimension mismatch between %r %s and %r %s"
250                     % (dim, current_index_variable.dims, n, var.dims)
251                 )
252 
253         if current_index_variable is not None and append:
254             current_index = current_index_variable.to_index()
255             if isinstance(current_index, pd.MultiIndex):
256                 names.extend(current_index.names)
257                 codes.extend(current_index.codes)
258                 levels.extend(current_index.levels)
259             else:
260                 names.append("%s_level_0" % dim)
261                 cat = pd.Categorical(current_index.values, ordered=True)
262                 codes.append(cat.codes)
263                 levels.append(cat.categories)
264 
265         if not len(names) and len(var_names) == 1:
266             idx = pd.Index(variables[var_names[0]].values)
267 
268         else:  # MultiIndex
269             for n in var_names:
270                 try:
271                     var = variables[n]
272                 except KeyError:
273                     raise ValueError(error_msg.format(n))
274                 names.append(n)
275                 cat = pd.Categorical(var.values, ordered=True)
276                 codes.append(cat.codes)
277                 levels.append(cat.categories)
278 
279             idx = pd.MultiIndex(levels, codes, names=names)
280             for n in names:
281                 dims_to_replace[n] = dim
282 
283         vars_to_replace[dim] = IndexVariable(dim, idx)
284         vars_to_remove.extend(var_names)
285 
286     new_variables = {k: v for k, v in variables.items() if k not in vars_to_remove}
287     new_variables.update(vars_to_replace)
288 
289     # update dimensions if necessary, GH: 3512
290     for k, v in new_variables.items():
291         if any(d in dims_to_replace for d in v.dims):
292             new_dims = [dims_to_replace.get(d, d) for d in v.dims]
293             new_variables[k] = v._replace(dims=new_dims)
294     new_coord_names = coord_names | set(vars_to_replace)
295     new_coord_names -= set(vars_to_remove)
296     return new_variables, new_coord_names
297 
298 
299 def split_indexes(
300     dims_or_levels: Union[Hashable, Sequence[Hashable]],
301     variables: Mapping[Hashable, Variable],
302     coord_names: Set[Hashable],
303     level_coords: Mapping[Hashable, Hashable],
304     drop: bool = False,
305 ) -> Tuple[Dict[Hashable, Variable], Set[Hashable]]:
306     """Extract (multi-)indexes (levels) as variables.
307 
308     Not public API. Used in Dataset and DataArray reset_index
309     methods.
310     """
311     if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
312         dims_or_levels = [dims_or_levels]
313 
314     dim_levels: DefaultDict[Any, List[Hashable]] = defaultdict(list)
315     dims = []
316     for k in dims_or_levels:
317         if k in level_coords:
318             dim_levels[level_coords[k]].append(k)
319         else:
320             dims.append(k)
321 
322     vars_to_replace = {}
323     vars_to_create: Dict[Hashable, Variable] = {}
324     vars_to_remove = []
325 
326     for d in dims:
327         index = variables[d].to_index()
328         if isinstance(index, pd.MultiIndex):
329             dim_levels[d] = index.names
330         else:
331             vars_to_remove.append(d)
332             if not drop:
333                 vars_to_create[str(d) + "_"] = Variable(d, index, variables[d].attrs)
334 
335     for d, levs in dim_levels.items():
336         index = variables[d].to_index()
337         if len(levs) == index.nlevels:
338             vars_to_remove.append(d)
339         else:
340             vars_to_replace[d] = IndexVariable(d, index.droplevel(levs))
341 
342         if not drop:
343             for lev in levs:
344                 idx = index.get_level_values(lev)
345                 vars_to_create[idx.name] = Variable(d, idx, variables[d].attrs)
346 
347     new_variables = dict(variables)
348     for v in set(vars_to_remove):
349         del new_variables[v]
350     new_variables.update(vars_to_replace)
351     new_variables.update(vars_to_create)
352     new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)
353 
354     return new_variables, new_coord_names
355 
356 
357 def _assert_empty(args: tuple, msg: str = "%s") -> None:
358     if args:
359         raise ValueError(msg % args)
360 
361 
362 def _check_chunks_compatibility(var, chunks, preferred_chunks):
363     for dim in var.dims:
364         if dim not in chunks or (dim not in preferred_chunks):
365             continue
366 
367         preferred_chunks_dim = preferred_chunks.get(dim)
368         chunks_dim = chunks.get(dim)
369 
370         if isinstance(chunks_dim, int):
371             chunks_dim = (chunks_dim,)
372         else:
373             chunks_dim = chunks_dim[:-1]
374 
375         if any(s % preferred_chunks_dim for s in chunks_dim):
376             warnings.warn(
377                 f"Specified Dask chunks {chunks[dim]} would separate "
378                 f"on disks chunk shape {preferred_chunks[dim]} for dimension {dim}. "
379                 "This could degrade performance. "
380                 "Consider rechunking after loading instead.",
381                 stacklevel=2,
382             )
383 
384 
385 def _get_chunk(var, chunks):
386     # chunks need to be explicity computed to take correctly into accout
387     # backend preferred chunking
388     import dask.array as da
389 
390     if isinstance(var, IndexVariable):
391         return {}
392 
393     if isinstance(chunks, int) or (chunks == "auto"):
394         chunks = dict.fromkeys(var.dims, chunks)
395 
396     preferred_chunks = var.encoding.get("preferred_chunks", {})
397     preferred_chunks_list = [
398         preferred_chunks.get(dim, shape) for dim, shape in zip(var.dims, var.shape)
399     ]
400 
401     chunks_list = [
402         chunks.get(dim, None) or preferred_chunks.get(dim, None) for dim in var.dims
403     ]
404 
405     output_chunks_list = da.core.normalize_chunks(
406         chunks_list,
407         shape=var.shape,
408         dtype=var.dtype,
409         previous_chunks=preferred_chunks_list,
410     )
411 
412     output_chunks = dict(zip(var.dims, output_chunks_list))
413     _check_chunks_compatibility(var, output_chunks, preferred_chunks)
414 
415     return output_chunks
416 
417 
418 def _maybe_chunk(
419     name,
420     var,
421     chunks,
422     token=None,
423     lock=None,
424     name_prefix="xarray-",
425     overwrite_encoded_chunks=False,
426 ):
427     from dask.base import tokenize
428 
429     if chunks is not None:
430         chunks = {dim: chunks[dim] for dim in var.dims if dim in chunks}
431     if var.ndim:
432         # when rechunking by different amounts, make sure dask names change
433         # by provinding chunks as an input to tokenize.
434         # subtle bugs result otherwise. see GH3350
435         token2 = tokenize(name, token if token else var._data, chunks)
436         name2 = f"{name_prefix}{name}-{token2}"
437         var = var.chunk(chunks, name=name2, lock=lock)
438 
439         if overwrite_encoded_chunks and var.chunks is not None:
440             var.encoding["chunks"] = tuple(x[0] for x in var.chunks)
441         return var
442     else:
443         return var
444 
445 
446 def as_dataset(obj: Any) -> "Dataset":
447     """Cast the given object to a Dataset.
448 
449     Handles Datasets, DataArrays and dictionaries of variables. A new Dataset
450     object is only created if the provided object is not already one.
451     """
452     if hasattr(obj, "to_dataset"):
453         obj = obj.to_dataset()
454     if not isinstance(obj, Dataset):
455         obj = Dataset(obj)
456     return obj
457 
458 
459 class DataVariables(Mapping[Hashable, "DataArray"]):
460     __slots__ = ("_dataset",)
461 
462     def __init__(self, dataset: "Dataset"):
463         self._dataset = dataset
464 
465     def __iter__(self) -> Iterator[Hashable]:
466         return (
467             key
468             for key in self._dataset._variables
469             if key not in self._dataset._coord_names
470         )
471 
472     def __len__(self) -> int:
473         return len(self._dataset._variables) - len(self._dataset._coord_names)
474 
475     def __contains__(self, key: Hashable) -> bool:
476         return key in self._dataset._variables and key not in self._dataset._coord_names
477 
478     def __getitem__(self, key: Hashable) -> "DataArray":
479         if key not in self._dataset._coord_names:
480             return cast("DataArray", self._dataset[key])
481         raise KeyError(key)
482 
483     def __repr__(self) -> str:
484         return formatting.data_vars_repr(self)
485 
486     @property
487     def variables(self) -> Mapping[Hashable, Variable]:
488         all_variables = self._dataset.variables
489         return Frozen({k: all_variables[k] for k in self})
490 
491     def _ipython_key_completions_(self):
492         """Provide method for the key-autocompletions in IPython. """
493         return [
494             key
495             for key in self._dataset._ipython_key_completions_()
496             if key not in self._dataset._coord_names
497         ]
498 
499 
500 class _LocIndexer:
501     __slots__ = ("dataset",)
502 
503     def __init__(self, dataset: "Dataset"):
504         self.dataset = dataset
505 
506     def __getitem__(self, key: Mapping[Hashable, Any]) -> "Dataset":
507         if not utils.is_dict_like(key):
508             raise TypeError("can only lookup dictionaries from Dataset.loc")
509         return self.dataset.sel(key)
510 
511 
512 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
513     """A multi-dimensional, in memory, array database.
514 
515     A dataset resembles an in-memory representation of a NetCDF file,
516     and consists of variables, coordinates and attributes which
517     together form a self describing dataset.
518 
519     Dataset implements the mapping interface with keys given by variable
520     names and values given by DataArray objects for each variable name.
521 
522     One dimensional variables with name equal to their dimension are
523     index coordinates used for label based indexing.
524 
525     To load data from a file or file-like object, use the `open_dataset`
526     function.
527 
528     Parameters
529     ----------
530     data_vars : dict-like, optional
531         A mapping from variable names to :py:class:`~xarray.DataArray`
532         objects, :py:class:`~xarray.Variable` objects or to tuples of
533         the form ``(dims, data[, attrs])`` which can be used as
534         arguments to create a new ``Variable``. Each dimension must
535         have the same length in all variables in which it appears.
536 
537         The following notations are accepted:
538 
539         - mapping {var name: DataArray}
540         - mapping {var name: Variable}
541         - mapping {var name: (dimension name, array-like)}
542         - mapping {var name: (tuple of dimension names, array-like)}
543         - mapping {dimension name: array-like}
544           (it will be automatically moved to coords, see below)
545 
546         Each dimension must have the same length in all variables in
547         which it appears.
548     coords : dict-like, optional
549         Another mapping in similar form as the `data_vars` argument,
550         except the each item is saved on the dataset as a "coordinate".
551         These variables have an associated meaning: they describe
552         constant/fixed/independent quantities, unlike the
553         varying/measured/dependent quantities that belong in
554         `variables`. Coordinates values may be given by 1-dimensional
555         arrays or scalars, in which case `dims` do not need to be
556         supplied: 1D arrays will be assumed to give index values along
557         the dimension with the same name.
558 
559         The following notations are accepted:
560 
561         - mapping {coord name: DataArray}
562         - mapping {coord name: Variable}
563         - mapping {coord name: (dimension name, array-like)}
564         - mapping {coord name: (tuple of dimension names, array-like)}
565         - mapping {dimension name: array-like}
566           (the dimension name is implicitly set to be the same as the
567           coord name)
568 
569         The last notation implies that the coord name is the same as
570         the dimension name.
571 
572     attrs : dict-like, optional
573         Global attributes to save on this dataset.
574 
575     Examples
576     --------
577     Create data:
578 
579     >>> np.random.seed(0)
580     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
581     >>> precipitation = 10 * np.random.rand(2, 2, 3)
582     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
583     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
584     >>> time = pd.date_range("2014-09-06", periods=3)
585     >>> reference_time = pd.Timestamp("2014-09-05")
586 
587     Initialize a dataset with multiple dimensions:
588 
589     >>> ds = xr.Dataset(
590     ...     data_vars=dict(
591     ...         temperature=(["x", "y", "time"], temperature),
592     ...         precipitation=(["x", "y", "time"], precipitation),
593     ...     ),
594     ...     coords=dict(
595     ...         lon=(["x", "y"], lon),
596     ...         lat=(["x", "y"], lat),
597     ...         time=time,
598     ...         reference_time=reference_time,
599     ...     ),
600     ...     attrs=dict(description="Weather related data."),
601     ... )
602     >>> ds
603     <xarray.Dataset>
604     Dimensions:         (time: 3, x: 2, y: 2)
605     Coordinates:
606         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
607         lat             (x, y) float64 42.25 42.21 42.63 42.59
608       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
609         reference_time  datetime64[ns] 2014-09-05
610     Dimensions without coordinates: x, y
611     Data variables:
612         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
613         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
614     Attributes:
615         description:  Weather related data.
616 
617     Find out where the coldest temperature was and what values the
618     other variables had:
619 
620     >>> ds.isel(ds.temperature.argmin(...))
621     <xarray.Dataset>
622     Dimensions:         ()
623     Coordinates:
624         lon             float64 -99.32
625         lat             float64 42.21
626         time            datetime64[ns] 2014-09-08
627         reference_time  datetime64[ns] 2014-09-05
628     Data variables:
629         temperature     float64 7.182
630         precipitation   float64 8.326
631     Attributes:
632         description:  Weather related data.
633     """
634 
635     _attrs: Optional[Dict[Hashable, Any]]
636     _cache: Dict[str, Any]
637     _coord_names: Set[Hashable]
638     _dims: Dict[Hashable, int]
639     _encoding: Optional[Dict[Hashable, Any]]
640     _close: Optional[Callable[[], None]]
641     _indexes: Optional[Dict[Hashable, pd.Index]]
642     _variables: Dict[Hashable, Variable]
643 
644     __slots__ = (
645         "_attrs",
646         "_cache",
647         "_coord_names",
648         "_dims",
649         "_encoding",
650         "_close",
651         "_indexes",
652         "_variables",
653         "__weakref__",
654     )
655 
656     _groupby_cls = groupby.DatasetGroupBy
657     _rolling_cls = rolling.DatasetRolling
658     _coarsen_cls = rolling.DatasetCoarsen
659     _resample_cls = resample.DatasetResample
660     _weighted_cls = weighted.DatasetWeighted
661 
662     def __init__(
663         self,
664         # could make a VariableArgs to use more generally, and refine these
665         # categories
666         data_vars: Mapping[Hashable, Any] = None,
667         coords: Mapping[Hashable, Any] = None,
668         attrs: Mapping[Hashable, Any] = None,
669     ):
670         # TODO(shoyer): expose indexes as a public argument in __init__
671 
672         if data_vars is None:
673             data_vars = {}
674         if coords is None:
675             coords = {}
676 
677         both_data_and_coords = set(data_vars) & set(coords)
678         if both_data_and_coords:
679             raise ValueError(
680                 "variables %r are found in both data_vars and coords"
681                 % both_data_and_coords
682             )
683 
684         if isinstance(coords, Dataset):
685             coords = coords.variables
686 
687         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
688             data_vars, coords, compat="broadcast_equals"
689         )
690 
691         self._attrs = dict(attrs) if attrs is not None else None
692         self._close = None
693         self._encoding = None
694         self._variables = variables
695         self._coord_names = coord_names
696         self._dims = dims
697         self._indexes = indexes
698 
699     @classmethod
700     def load_store(cls, store, decoder=None) -> "Dataset":
701         """Create a new dataset from the contents of a backends.*DataStore
702         object
703         """
704         variables, attributes = store.load()
705         if decoder:
706             variables, attributes = decoder(variables, attributes)
707         obj = cls(variables, attrs=attributes)
708         obj.set_close(store.close)
709         return obj
710 
711     @property
712     def variables(self) -> Mapping[Hashable, Variable]:
713         """Low level interface to Dataset contents as dict of Variable objects.
714 
715         This ordered dictionary is frozen to prevent mutation that could
716         violate Dataset invariants. It contains all variable objects
717         constituting the Dataset, including both data variables and
718         coordinates.
719         """
720         return Frozen(self._variables)
721 
722     @property
723     def attrs(self) -> Dict[Hashable, Any]:
724         """Dictionary of global attributes on this dataset"""
725         if self._attrs is None:
726             self._attrs = {}
727         return self._attrs
728 
729     @attrs.setter
730     def attrs(self, value: Mapping[Hashable, Any]) -> None:
731         self._attrs = dict(value)
732 
733     @property
734     def encoding(self) -> Dict:
735         """Dictionary of global encoding attributes on this dataset"""
736         if self._encoding is None:
737             self._encoding = {}
738         return self._encoding
739 
740     @encoding.setter
741     def encoding(self, value: Mapping) -> None:
742         self._encoding = dict(value)
743 
744     @property
745     def dims(self) -> Mapping[Hashable, int]:
746         """Mapping from dimension names to lengths.
747 
748         Cannot be modified directly, but is updated when adding new variables.
749 
750         Note that type of this object differs from `DataArray.dims`.
751         See `Dataset.sizes` and `DataArray.sizes` for consistently named
752         properties.
753         """
754         return Frozen(SortedKeysDict(self._dims))
755 
756     @property
757     def sizes(self) -> Mapping[Hashable, int]:
758         """Mapping from dimension names to lengths.
759 
760         Cannot be modified directly, but is updated when adding new variables.
761 
762         This is an alias for `Dataset.dims` provided for the benefit of
763         consistency with `DataArray.sizes`.
764 
765         See Also
766         --------
767         DataArray.sizes
768         """
769         return self.dims
770 
771     def load(self, **kwargs) -> "Dataset":
772         """Manually trigger loading and/or computation of this dataset's data
773         from disk or a remote source into memory and return this dataset.
774         Unlike compute, the original dataset is modified and returned.
775 
776         Normally, it should not be necessary to call this method in user code,
777         because all xarray functions should either work on deferred data or
778         load data automatically. However, this method can be necessary when
779         working with many file objects on disk.
780 
781         Parameters
782         ----------
783         **kwargs : dict
784             Additional keyword arguments passed on to ``dask.compute``.
785 
786         See Also
787         --------
788         dask.compute
789         """
790         # access .data to coerce everything to numpy or dask arrays
791         lazy_data = {
792             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
793         }
794         if lazy_data:
795             import dask.array as da
796 
797             # evaluate all the dask arrays simultaneously
798             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
799 
800             for k, data in zip(lazy_data, evaluated_data):
801                 self.variables[k].data = data
802 
803         # load everything else sequentially
804         for k, v in self.variables.items():
805             if k not in lazy_data:
806                 v.load()
807 
808         return self
809 
810     def __dask_tokenize__(self):
811         from dask.base import normalize_token
812 
813         return normalize_token(
814             (type(self), self._variables, self._coord_names, self._attrs)
815         )
816 
817     def __dask_graph__(self):
818         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
819         graphs = {k: v for k, v in graphs.items() if v is not None}
820         if not graphs:
821             return None
822         else:
823             try:
824                 from dask.highlevelgraph import HighLevelGraph
825 
826                 return HighLevelGraph.merge(*graphs.values())
827             except ImportError:
828                 from dask import sharedict
829 
830                 return sharedict.merge(*graphs.values())
831 
832     def __dask_keys__(self):
833         import dask
834 
835         return [
836             v.__dask_keys__()
837             for v in self.variables.values()
838             if dask.is_dask_collection(v)
839         ]
840 
841     def __dask_layers__(self):
842         import dask
843 
844         return sum(
845             [
846                 v.__dask_layers__()
847                 for v in self.variables.values()
848                 if dask.is_dask_collection(v)
849             ],
850             (),
851         )
852 
853     @property
854     def __dask_optimize__(self):
855         import dask.array as da
856 
857         return da.Array.__dask_optimize__
858 
859     @property
860     def __dask_scheduler__(self):
861         import dask.array as da
862 
863         return da.Array.__dask_scheduler__
864 
865     def __dask_postcompute__(self):
866         import dask
867 
868         info = [
869             (True, k, v.__dask_postcompute__())
870             if dask.is_dask_collection(v)
871             else (False, k, v)
872             for k, v in self._variables.items()
873         ]
874         args = (
875             info,
876             self._coord_names,
877             self._dims,
878             self._attrs,
879             self._indexes,
880             self._encoding,
881             self._close,
882         )
883         return self._dask_postcompute, args
884 
885     def __dask_postpersist__(self):
886         import dask
887 
888         info = [
889             (True, k, v.__dask_postpersist__())
890             if dask.is_dask_collection(v)
891             else (False, k, v)
892             for k, v in self._variables.items()
893         ]
894         args = (
895             info,
896             self._coord_names,
897             self._dims,
898             self._attrs,
899             self._indexes,
900             self._encoding,
901             self._close,
902         )
903         return self._dask_postpersist, args
904 
905     @staticmethod
906     def _dask_postcompute(results, info, *args):
907         variables = {}
908         results2 = list(results[::-1])
909         for is_dask, k, v in info:
910             if is_dask:
911                 func, args2 = v
912                 r = results2.pop()
913                 result = func(r, *args2)
914             else:
915                 result = v
916             variables[k] = result
917 
918         final = Dataset._construct_direct(variables, *args)
919         return final
920 
921     @staticmethod
922     def _dask_postpersist(dsk, info, *args):
923         variables = {}
924         # postpersist is called in both dask.optimize and dask.persist
925         # When persisting, we want to filter out unrelated keys for
926         # each Variable's task graph.
927         is_persist = len(dsk) == len(info)
928         for is_dask, k, v in info:
929             if is_dask:
930                 func, args2 = v
931                 if is_persist:
932                     name = args2[1][0]
933                     dsk2 = {k: v for k, v in dsk.items() if k[0] == name}
934                 else:
935                     dsk2 = dsk
936                 result = func(dsk2, *args2)
937             else:
938                 result = v
939             variables[k] = result
940 
941         return Dataset._construct_direct(variables, *args)
942 
943     def compute(self, **kwargs) -> "Dataset":
944         """Manually trigger loading and/or computation of this dataset's data
945         from disk or a remote source into memory and return a new dataset.
946         Unlike load, the original dataset is left unaltered.
947 
948         Normally, it should not be necessary to call this method in user code,
949         because all xarray functions should either work on deferred data or
950         load data automatically. However, this method can be necessary when
951         working with many file objects on disk.
952 
953         Parameters
954         ----------
955         **kwargs : dict
956             Additional keyword arguments passed on to ``dask.compute``.
957 
958         See Also
959         --------
960         dask.compute
961         """
962         new = self.copy(deep=False)
963         return new.load(**kwargs)
964 
965     def _persist_inplace(self, **kwargs) -> "Dataset":
966         """Persist all Dask arrays in memory"""
967         # access .data to coerce everything to numpy or dask arrays
968         lazy_data = {
969             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
970         }
971         if lazy_data:
972             import dask
973 
974             # evaluate all the dask arrays simultaneously
975             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
976 
977             for k, data in zip(lazy_data, evaluated_data):
978                 self.variables[k].data = data
979 
980         return self
981 
982     def persist(self, **kwargs) -> "Dataset":
983         """Trigger computation, keeping data as dask arrays
984 
985         This operation can be used to trigger computation on underlying dask
986         arrays, similar to ``.compute()`` or ``.load()``.  However this
987         operation keeps the data as dask arrays. This is particularly useful
988         when using the dask.distributed scheduler and you want to load a large
989         amount of data into distributed memory.
990 
991         Parameters
992         ----------
993         **kwargs : dict
994             Additional keyword arguments passed on to ``dask.persist``.
995 
996         See Also
997         --------
998         dask.persist
999         """
1000         new = self.copy(deep=False)
1001         return new._persist_inplace(**kwargs)
1002 
1003     @classmethod
1004     def _construct_direct(
1005         cls,
1006         variables,
1007         coord_names,
1008         dims=None,
1009         attrs=None,
1010         indexes=None,
1011         encoding=None,
1012         close=None,
1013     ):
1014         """Shortcut around __init__ for internal use when we want to skip
1015         costly validation
1016         """
1017         if dims is None:
1018             dims = calculate_dimensions(variables)
1019         obj = object.__new__(cls)
1020         obj._variables = variables
1021         obj._coord_names = coord_names
1022         obj._dims = dims
1023         obj._indexes = indexes
1024         obj._attrs = attrs
1025         obj._close = close
1026         obj._encoding = encoding
1027         return obj
1028 
1029     def _replace(
1030         self,
1031         variables: Dict[Hashable, Variable] = None,
1032         coord_names: Set[Hashable] = None,
1033         dims: Dict[Any, int] = None,
1034         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1035         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
1036         encoding: Union[dict, None, Default] = _default,
1037         inplace: bool = False,
1038     ) -> "Dataset":
1039         """Fastpath constructor for internal use.
1040 
1041         Returns an object with optionally with replaced attributes.
1042 
1043         Explicitly passed arguments are *not* copied when placed on the new
1044         dataset. It is up to the caller to ensure that they have the right type
1045         and are not used elsewhere.
1046         """
1047         if inplace:
1048             if variables is not None:
1049                 self._variables = variables
1050             if coord_names is not None:
1051                 self._coord_names = coord_names
1052             if dims is not None:
1053                 self._dims = dims
1054             if attrs is not _default:
1055                 self._attrs = attrs
1056             if indexes is not _default:
1057                 self._indexes = indexes
1058             if encoding is not _default:
1059                 self._encoding = encoding
1060             obj = self
1061         else:
1062             if variables is None:
1063                 variables = self._variables.copy()
1064             if coord_names is None:
1065                 coord_names = self._coord_names.copy()
1066             if dims is None:
1067                 dims = self._dims.copy()
1068             if attrs is _default:
1069                 attrs = copy.copy(self._attrs)
1070             if indexes is _default:
1071                 indexes = copy.copy(self._indexes)
1072             if encoding is _default:
1073                 encoding = copy.copy(self._encoding)
1074             obj = self._construct_direct(
1075                 variables, coord_names, dims, attrs, indexes, encoding
1076             )
1077         return obj
1078 
1079     def _replace_with_new_dims(
1080         self,
1081         variables: Dict[Hashable, Variable],
1082         coord_names: set = None,
1083         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1084         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
1085         inplace: bool = False,
1086     ) -> "Dataset":
1087         """Replace variables with recalculated dimensions."""
1088         dims = calculate_dimensions(variables)
1089         return self._replace(
1090             variables, coord_names, dims, attrs, indexes, inplace=inplace
1091         )
1092 
1093     def _replace_vars_and_dims(
1094         self,
1095         variables: Dict[Hashable, Variable],
1096         coord_names: set = None,
1097         dims: Dict[Hashable, int] = None,
1098         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1099         inplace: bool = False,
1100     ) -> "Dataset":
1101         """Deprecated version of _replace_with_new_dims().
1102 
1103         Unlike _replace_with_new_dims(), this method always recalculates
1104         indexes from variables.
1105         """
1106         if dims is None:
1107             dims = calculate_dimensions(variables)
1108         return self._replace(
1109             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1110         )
1111 
1112     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
1113         if not indexes:
1114             return self
1115 
1116         variables = self._variables.copy()
1117         new_indexes = dict(self.indexes)
1118         for name, idx in indexes.items():
1119             variables[name] = IndexVariable(name, idx)
1120             new_indexes[name] = idx
1121         obj = self._replace(variables, indexes=new_indexes)
1122 
1123         # switch from dimension to level names, if necessary
1124         dim_names: Dict[Hashable, str] = {}
1125         for dim, idx in indexes.items():
1126             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
1127                 dim_names[dim] = idx.name
1128         if dim_names:
1129             obj = obj.rename(dim_names)
1130         return obj
1131 
1132     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
1133         """Returns a copy of this dataset.
1134 
1135         If `deep=True`, a deep copy is made of each of the component variables.
1136         Otherwise, a shallow copy of each of the component variable is made, so
1137         that the underlying memory region of the new dataset is the same as in
1138         the original dataset.
1139 
1140         Use `data` to create a new object with the same structure as
1141         original but entirely new data.
1142 
1143         Parameters
1144         ----------
1145         deep : bool, optional
1146             Whether each component variable is loaded into memory and copied onto
1147             the new object. Default is False.
1148         data : dict-like, optional
1149             Data to use in the new object. Each item in `data` must have same
1150             shape as corresponding data variable in original. When `data` is
1151             used, `deep` is ignored for the data variables and only used for
1152             coords.
1153 
1154         Returns
1155         -------
1156         object : Dataset
1157             New object with dimensions, attributes, coordinates, name, encoding,
1158             and optionally data copied from original.
1159 
1160         Examples
1161         --------
1162         Shallow copy versus deep copy
1163 
1164         >>> da = xr.DataArray(np.random.randn(2, 3))
1165         >>> ds = xr.Dataset(
1166         ...     {"foo": da, "bar": ("x", [-1, 2])},
1167         ...     coords={"x": ["one", "two"]},
1168         ... )
1169         >>> ds.copy()
1170         <xarray.Dataset>
1171         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1172         Coordinates:
1173           * x        (x) <U3 'one' 'two'
1174         Dimensions without coordinates: dim_0, dim_1
1175         Data variables:
1176             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1177             bar      (x) int64 -1 2
1178 
1179         >>> ds_0 = ds.copy(deep=False)
1180         >>> ds_0["foo"][0, 0] = 7
1181         >>> ds_0
1182         <xarray.Dataset>
1183         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1184         Coordinates:
1185           * x        (x) <U3 'one' 'two'
1186         Dimensions without coordinates: dim_0, dim_1
1187         Data variables:
1188             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1189             bar      (x) int64 -1 2
1190 
1191         >>> ds
1192         <xarray.Dataset>
1193         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1194         Coordinates:
1195           * x        (x) <U3 'one' 'two'
1196         Dimensions without coordinates: dim_0, dim_1
1197         Data variables:
1198             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1199             bar      (x) int64 -1 2
1200 
1201         Changing the data using the ``data`` argument maintains the
1202         structure of the original object, but with the new data. Original
1203         object is unaffected.
1204 
1205         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1206         <xarray.Dataset>
1207         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1208         Coordinates:
1209           * x        (x) <U3 'one' 'two'
1210         Dimensions without coordinates: dim_0, dim_1
1211         Data variables:
1212             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1213             bar      (x) <U1 'a' 'b'
1214 
1215         >>> ds
1216         <xarray.Dataset>
1217         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1218         Coordinates:
1219           * x        (x) <U3 'one' 'two'
1220         Dimensions without coordinates: dim_0, dim_1
1221         Data variables:
1222             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1223             bar      (x) int64 -1 2
1224 
1225         See Also
1226         --------
1227         pandas.DataFrame.copy
1228         """
1229         if data is None:
1230             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1231         elif not utils.is_dict_like(data):
1232             raise ValueError("Data must be dict-like")
1233         else:
1234             var_keys = set(self.data_vars.keys())
1235             data_keys = set(data.keys())
1236             keys_not_in_vars = data_keys - var_keys
1237             if keys_not_in_vars:
1238                 raise ValueError(
1239                     "Data must only contain variables in original "
1240                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1241                 )
1242             keys_missing_from_data = var_keys - data_keys
1243             if keys_missing_from_data:
1244                 raise ValueError(
1245                     "Data must contain all variables in original "
1246                     "dataset. Data is missing {}".format(keys_missing_from_data)
1247                 )
1248             variables = {
1249                 k: v.copy(deep=deep, data=data.get(k))
1250                 for k, v in self._variables.items()
1251             }
1252 
1253         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1254 
1255         return self._replace(variables, attrs=attrs)
1256 
1257     @property
1258     def _level_coords(self) -> Dict[str, Hashable]:
1259         """Return a mapping of all MultiIndex levels and their corresponding
1260         coordinate name.
1261         """
1262         level_coords: Dict[str, Hashable] = {}
1263         for name, index in self.indexes.items():
1264             if isinstance(index, pd.MultiIndex):
1265                 level_names = index.names
1266                 (dim,) = self.variables[name].dims
1267                 level_coords.update({lname: dim for lname in level_names})
1268         return level_coords
1269 
1270     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: Dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: Dict[Hashable, pd.Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self._level_coords, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     indexes[var_name] = var.to_index()
1290 
1291         needed_dims: Set[Hashable] = set()
1292         for v in variables.values():
1293             needed_dims.update(v.dims)
1294 
1295         dims = {k: self.dims[k] for k in needed_dims}
1296 
1297         # preserves ordering of coordinates
1298         for k in self._variables:
1299             if k not in self._coord_names:
1300                 continue
1301 
1302             if set(self.variables[k].dims) <= needed_dims:
1303                 variables[k] = self._variables[k]
1304                 coord_names.add(k)
1305                 if k in self.indexes:
1306                     indexes[k] = self.indexes[k]
1307 
1308         return self._replace(variables, coord_names, dims, indexes=indexes)
1309 
1310     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1311         """Construct a DataArray by indexing this dataset"""
1312         from .dataarray import DataArray
1313 
1314         try:
1315             variable = self._variables[name]
1316         except KeyError:
1317             _, name, variable = _get_virtual_variable(
1318                 self._variables, name, self._level_coords, self.dims
1319             )
1320 
1321         needed_dims = set(variable.dims)
1322 
1323         coords: Dict[Hashable, Variable] = {}
1324         # preserve ordering
1325         for k in self._variables:
1326             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1327                 coords[k] = self.variables[k]
1328 
1329         if self._indexes is None:
1330             indexes = None
1331         else:
1332             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1333 
1334         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1335 
1336     def __copy__(self) -> "Dataset":
1337         return self.copy(deep=False)
1338 
1339     def __deepcopy__(self, memo=None) -> "Dataset":
1340         # memo does nothing but is required for compatibility with
1341         # copy.deepcopy
1342         return self.copy(deep=True)
1343 
1344     @property
1345     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1346         """Places to look-up items for attribute-style access"""
1347         yield from self._item_sources
1348         yield self.attrs
1349 
1350     @property
1351     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1352         """Places to look-up items for key-completion"""
1353         yield self.data_vars
1354         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1355 
1356         # virtual coordinates
1357         yield HybridMappingProxy(keys=self.dims, mapping=self)
1358 
1359         # uses empty dict -- everything here can already be found in self.coords.
1360         yield HybridMappingProxy(keys=self._level_coords, mapping={})
1361 
1362     def __contains__(self, key: object) -> bool:
1363         """The 'in' operator will return true or false depending on whether
1364         'key' is an array in the dataset or not.
1365         """
1366         return key in self._variables
1367 
1368     def __len__(self) -> int:
1369         return len(self.data_vars)
1370 
1371     def __bool__(self) -> bool:
1372         return bool(self.data_vars)
1373 
1374     def __iter__(self) -> Iterator[Hashable]:
1375         return iter(self.data_vars)
1376 
1377     def __array__(self, dtype=None):
1378         raise TypeError(
1379             "cannot directly convert an xarray.Dataset into a "
1380             "numpy array. Instead, create an xarray.DataArray "
1381             "first, either with indexing on the Dataset or by "
1382             "invoking the `to_array()` method."
1383         )
1384 
1385     @property
1386     def nbytes(self) -> int:
1387         return sum(v.nbytes for v in self.variables.values())
1388 
1389     @property
1390     def loc(self) -> _LocIndexer:
1391         """Attribute for location based indexing. Only supports __getitem__,
1392         and only when the key is a dict of the form {dim: labels}.
1393         """
1394         return _LocIndexer(self)
1395 
1396     # FIXME https://github.com/python/mypy/issues/7328
1397     @overload
1398     def __getitem__(self, key: Mapping) -> "Dataset":  # type: ignore
1399         ...
1400 
1401     @overload
1402     def __getitem__(self, key: Hashable) -> "DataArray":  # type: ignore
1403         ...
1404 
1405     @overload
1406     def __getitem__(self, key: Any) -> "Dataset":
1407         ...
1408 
1409     def __getitem__(self, key):
1410         """Access variables or coordinates this dataset as a
1411         :py:class:`~xarray.DataArray`.
1412 
1413         Indexing with a list of names will return a new ``Dataset`` object.
1414         """
1415         if utils.is_dict_like(key):
1416             return self.isel(**cast(Mapping, key))
1417 
1418         if hashable(key):
1419             return self._construct_dataarray(key)
1420         else:
1421             return self._copy_listed(np.asarray(key))
1422 
1423     def __setitem__(self, key: Hashable, value) -> None:
1424         """Add an array to this dataset.
1425 
1426         If value is a `DataArray`, call its `select_vars()` method, rename it
1427         to `key` and merge the contents of the resulting dataset into this
1428         dataset.
1429 
1430         If value is an `Variable` object (or tuple of form
1431         ``(dims, data[, attrs])``), add it to this dataset as a new
1432         variable.
1433         """
1434         if utils.is_dict_like(key):
1435             raise NotImplementedError(
1436                 "cannot yet use a dictionary as a key to set Dataset values"
1437             )
1438 
1439         self.update({key: value})
1440 
1441     def __delitem__(self, key: Hashable) -> None:
1442         """Remove a variable from this dataset."""
1443         del self._variables[key]
1444         self._coord_names.discard(key)
1445         if key in self.indexes:
1446             assert self._indexes is not None
1447             del self._indexes[key]
1448         self._dims = calculate_dimensions(self._variables)
1449 
1450     # mutable objects should not be hashable
1451     # https://github.com/python/mypy/issues/4266
1452     __hash__ = None  # type: ignore
1453 
1454     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1455         """Helper function for equals and identical"""
1456 
1457         # some stores (e.g., scipy) do not seem to preserve order, so don't
1458         # require matching order for equality
1459         def compat(x: Variable, y: Variable) -> bool:
1460             return getattr(x, compat_str)(y)
1461 
1462         return self._coord_names == other._coord_names and utils.dict_equiv(
1463             self._variables, other._variables, compat=compat
1464         )
1465 
1466     def broadcast_equals(self, other: "Dataset") -> bool:
1467         """Two Datasets are broadcast equal if they are equal after
1468         broadcasting all variables against each other.
1469 
1470         For example, variables that are scalar in one dataset but non-scalar in
1471         the other dataset can still be broadcast equal if the the non-scalar
1472         variable is a constant.
1473 
1474         See Also
1475         --------
1476         Dataset.equals
1477         Dataset.identical
1478         """
1479         try:
1480             return self._all_compat(other, "broadcast_equals")
1481         except (TypeError, AttributeError):
1482             return False
1483 
1484     def equals(self, other: "Dataset") -> bool:
1485         """Two Datasets are equal if they have matching variables and
1486         coordinates, all of which are equal.
1487 
1488         Datasets can still be equal (like pandas objects) if they have NaN
1489         values in the same locations.
1490 
1491         This method is necessary because `v1 == v2` for ``Dataset``
1492         does element-wise comparisons (like numpy.ndarrays).
1493 
1494         See Also
1495         --------
1496         Dataset.broadcast_equals
1497         Dataset.identical
1498         """
1499         try:
1500             return self._all_compat(other, "equals")
1501         except (TypeError, AttributeError):
1502             return False
1503 
1504     def identical(self, other: "Dataset") -> bool:
1505         """Like equals, but also checks all dataset attributes and the
1506         attributes on all variables and coordinates.
1507 
1508         See Also
1509         --------
1510         Dataset.broadcast_equals
1511         Dataset.equals
1512         """
1513         try:
1514             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1515                 other, "identical"
1516             )
1517         except (TypeError, AttributeError):
1518             return False
1519 
1520     @property
1521     def indexes(self) -> Indexes:
1522         """Mapping of pandas.Index objects used for label based indexing"""
1523         if self._indexes is None:
1524             self._indexes = default_indexes(self._variables, self._dims)
1525         return Indexes(self._indexes)
1526 
1527     @property
1528     def coords(self) -> DatasetCoordinates:
1529         """Dictionary of xarray.DataArray objects corresponding to coordinate
1530         variables
1531         """
1532         return DatasetCoordinates(self)
1533 
1534     @property
1535     def data_vars(self) -> DataVariables:
1536         """Dictionary of DataArray objects corresponding to data variables"""
1537         return DataVariables(self)
1538 
1539     def set_coords(self, names: "Union[Hashable, Iterable[Hashable]]") -> "Dataset":
1540         """Given names of one or more variables, set them as coordinates
1541 
1542         Parameters
1543         ----------
1544         names : hashable or iterable of hashable
1545             Name(s) of variables in this dataset to convert into coordinates.
1546 
1547         Returns
1548         -------
1549         Dataset
1550 
1551         See Also
1552         --------
1553         Dataset.swap_dims
1554         """
1555         # TODO: allow inserting new coordinates with this method, like
1556         # DataFrame.set_index?
1557         # nb. check in self._variables, not self.data_vars to insure that the
1558         # operation is idempotent
1559         if isinstance(names, str) or not isinstance(names, Iterable):
1560             names = [names]
1561         else:
1562             names = list(names)
1563         self._assert_all_in_dataset(names)
1564         obj = self.copy()
1565         obj._coord_names.update(names)
1566         return obj
1567 
1568     def reset_coords(
1569         self,
1570         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1571         drop: bool = False,
1572     ) -> "Dataset":
1573         """Given names of coordinates, reset them to become variables
1574 
1575         Parameters
1576         ----------
1577         names : hashable or iterable of hashable, optional
1578             Name(s) of non-index coordinates in this dataset to reset into
1579             variables. By default, all non-index coordinates are reset.
1580         drop : bool, optional
1581             If True, remove coordinates instead of converting them into
1582             variables.
1583 
1584         Returns
1585         -------
1586         Dataset
1587         """
1588         if names is None:
1589             names = self._coord_names - set(self.dims)
1590         else:
1591             if isinstance(names, str) or not isinstance(names, Iterable):
1592                 names = [names]
1593             else:
1594                 names = list(names)
1595             self._assert_all_in_dataset(names)
1596             bad_coords = set(names) & set(self.dims)
1597             if bad_coords:
1598                 raise ValueError(
1599                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1600                 )
1601         obj = self.copy()
1602         obj._coord_names.difference_update(names)
1603         if drop:
1604             for name in names:
1605                 del obj._variables[name]
1606         return obj
1607 
1608     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1609         """Store dataset contents to a backends.*DataStore object."""
1610         from ..backends.api import dump_to_store
1611 
1612         # TODO: rename and/or cleanup this method to make it more consistent
1613         # with to_netcdf()
1614         dump_to_store(self, store, **kwargs)
1615 
1616     def to_netcdf(
1617         self,
1618         path=None,
1619         mode: str = "w",
1620         format: str = None,
1621         group: str = None,
1622         engine: str = None,
1623         encoding: Mapping = None,
1624         unlimited_dims: Iterable[Hashable] = None,
1625         compute: bool = True,
1626         invalid_netcdf: bool = False,
1627     ) -> Union[bytes, "Delayed", None]:
1628         """Write dataset contents to a netCDF file.
1629 
1630         Parameters
1631         ----------
1632         path : str, Path or file-like, optional
1633             Path to which to save this dataset. File-like objects are only
1634             supported by the scipy engine. If no path is provided, this
1635             function returns the resulting netCDF file as bytes; in this case,
1636             we need to use scipy, which does not support netCDF version 4 (the
1637             default format becomes NETCDF3_64BIT).
1638         mode : {"w", "a"}, default: "w"
1639             Write ('w') or append ('a') mode. If mode='w', any existing file at
1640             this location will be overwritten. If mode='a', existing variables
1641             will be overwritten.
1642         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1643                   "NETCDF3_CLASSIC"}, optional
1644             File format for the resulting netCDF file:
1645 
1646             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1647               features.
1648             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1649               netCDF 3 compatible API features.
1650             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1651               which fully supports 2+ GB files, but is only compatible with
1652               clients linked against netCDF version 3.6.0 or later.
1653             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1654               handle 2+ GB files very well.
1655 
1656             All formats are supported by the netCDF4-python library.
1657             scipy.io.netcdf only supports the last two formats.
1658 
1659             The default format is NETCDF4 if you are saving a file to disk and
1660             have the netCDF4-python library available. Otherwise, xarray falls
1661             back to using scipy to write netCDF files and defaults to the
1662             NETCDF3_64BIT format (scipy does not support netCDF4).
1663         group : str, optional
1664             Path to the netCDF4 group in the given file to open (only works for
1665             format='NETCDF4'). The group(s) will be created if necessary.
1666         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1667             Engine to use when writing netCDF files. If not provided, the
1668             default engine is chosen based on available dependencies, with a
1669             preference for 'netcdf4' if writing to a file on disk.
1670         encoding : dict, optional
1671             Nested dictionary with variable names as keys and dictionaries of
1672             variable specific encodings as values, e.g.,
1673             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1674             "zlib": True}, ...}``
1675 
1676             The `h5netcdf` engine supports both the NetCDF4-style compression
1677             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1678             ones ``{"compression": "gzip", "compression_opts": 9}``.
1679             This allows using any compression plugin installed in the HDF5
1680             library, e.g. LZF.
1681 
1682         unlimited_dims : iterable of hashable, optional
1683             Dimension(s) that should be serialized as unlimited dimensions.
1684             By default, no dimensions are treated as unlimited dimensions.
1685             Note that unlimited_dims may also be set via
1686             ``dataset.encoding["unlimited_dims"]``.
1687         compute: bool, default: True
1688             If true compute immediately, otherwise return a
1689             ``dask.delayed.Delayed`` object that can be computed later.
1690         invalid_netcdf: bool, default: False
1691             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1692             hdf5 files which are invalid netcdf as described in
1693             https://github.com/shoyer/h5netcdf.
1694         """
1695         if encoding is None:
1696             encoding = {}
1697         from ..backends.api import to_netcdf
1698 
1699         return to_netcdf(
1700             self,
1701             path,
1702             mode,
1703             format=format,
1704             group=group,
1705             engine=engine,
1706             encoding=encoding,
1707             unlimited_dims=unlimited_dims,
1708             compute=compute,
1709             invalid_netcdf=invalid_netcdf,
1710         )
1711 
1712     def to_zarr(
1713         self,
1714         store: Union[MutableMapping, str, Path] = None,
1715         chunk_store: Union[MutableMapping, str, Path] = None,
1716         mode: str = None,
1717         synchronizer=None,
1718         group: str = None,
1719         encoding: Mapping = None,
1720         compute: bool = True,
1721         consolidated: bool = False,
1722         append_dim: Hashable = None,
1723         region: Mapping[str, slice] = None,
1724     ) -> "ZarrStore":
1725         """Write dataset contents to a zarr group.
1726 
1727         .. note:: Experimental
1728                   The Zarr backend is new and experimental. Please report any
1729                   unexpected behavior via github issues.
1730 
1731         Parameters
1732         ----------
1733         store : MutableMapping, str or Path, optional
1734             Store or path to directory in file system.
1735         chunk_store : MutableMapping, str or Path, optional
1736             Store or path to directory in file system only for Zarr array chunks.
1737             Requires zarr-python v2.4.0 or later.
1738         mode : {"w", "w-", "a", None}, optional
1739             Persistence mode: "w" means create (overwrite if exists);
1740             "w-" means create (fail if exists);
1741             "a" means override existing variables (create if does not exist).
1742             If ``append_dim`` is set, ``mode`` can be omitted as it is
1743             internally set to ``"a"``. Otherwise, ``mode`` will default to
1744             `w-` if not set.
1745         synchronizer : object, optional
1746             Zarr array synchronizer.
1747         group : str, optional
1748             Group path. (a.k.a. `path` in zarr terminology.)
1749         encoding : dict, optional
1750             Nested dictionary with variable names as keys and dictionaries of
1751             variable specific encodings as values, e.g.,
1752             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1753         compute : bool, optional
1754             If True write array data immediately, otherwise return a
1755             ``dask.delayed.Delayed`` object that can be computed to write
1756             array data later. Metadata is always updated eagerly.
1757         consolidated : bool, optional
1758             If True, apply zarr's `consolidate_metadata` function to the store
1759             after writing metadata.
1760         append_dim : hashable, optional
1761             If set, the dimension along which the data will be appended. All
1762             other dimensions on overriden variables must remain the same size.
1763         region : dict, optional
1764             Optional mapping from dimension names to integer slices along
1765             dataset dimensions to indicate the region of existing zarr array(s)
1766             in which to write this dataset's data. For example,
1767             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
1768             that values should be written to the region ``0:1000`` along ``x``
1769             and ``10000:11000`` along ``y``.
1770 
1771             Two restrictions apply to the use of ``region``:
1772 
1773             - If ``region`` is set, _all_ variables in a dataset must have at
1774               least one dimension in common with the region. Other variables
1775               should be written in a separate call to ``to_zarr()``.
1776             - Dimensions cannot be included in both ``region`` and
1777               ``append_dim`` at the same time. To create empty arrays to fill
1778               in with ``region``, use a separate call to ``to_zarr()`` with
1779               ``compute=False``. See "Appending to existing Zarr stores" in
1780               the reference documentation for full details.
1781 
1782         References
1783         ----------
1784         https://zarr.readthedocs.io/
1785 
1786         Notes
1787         -----
1788         Zarr chunking behavior:
1789             If chunks are found in the encoding argument or attribute
1790             corresponding to any DataArray, those chunks are used.
1791             If a DataArray is a dask array, it is written with those chunks.
1792             If not other chunks are found, Zarr uses its own heuristics to
1793             choose automatic chunk sizes.
1794         """
1795         from ..backends.api import to_zarr
1796 
1797         if encoding is None:
1798             encoding = {}
1799 
1800         return to_zarr(
1801             self,
1802             store=store,
1803             chunk_store=chunk_store,
1804             mode=mode,
1805             synchronizer=synchronizer,
1806             group=group,
1807             encoding=encoding,
1808             compute=compute,
1809             consolidated=consolidated,
1810             append_dim=append_dim,
1811             region=region,
1812         )
1813 
1814     def __repr__(self) -> str:
1815         return formatting.dataset_repr(self)
1816 
1817     def _repr_html_(self):
1818         if OPTIONS["display_style"] == "text":
1819             return f"<pre>{escape(repr(self))}</pre>"
1820         return formatting_html.dataset_repr(self)
1821 
1822     def info(self, buf=None) -> None:
1823         """
1824         Concise summary of a Dataset variables and attributes.
1825 
1826         Parameters
1827         ----------
1828         buf : file-like, default: sys.stdout
1829             writable buffer
1830 
1831         See Also
1832         --------
1833         pandas.DataFrame.assign
1834         ncdump : netCDF's ncdump
1835         """
1836         if buf is None:  # pragma: no cover
1837             buf = sys.stdout
1838 
1839         lines = []
1840         lines.append("xarray.Dataset {")
1841         lines.append("dimensions:")
1842         for name, size in self.dims.items():
1843             lines.append(f"\t{name} = {size} ;")
1844         lines.append("\nvariables:")
1845         for name, da in self.variables.items():
1846             dims = ", ".join(da.dims)
1847             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1848             for k, v in da.attrs.items():
1849                 lines.append(f"\t\t{name}:{k} = {v} ;")
1850         lines.append("\n// global attributes:")
1851         for k, v in self.attrs.items():
1852             lines.append(f"\t:{k} = {v} ;")
1853         lines.append("}")
1854 
1855         buf.write("\n".join(lines))
1856 
1857     @property
1858     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1859         """Block dimensions for this dataset's data or None if it's not a dask
1860         array.
1861         """
1862         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1863         for v in self.variables.values():
1864             if v.chunks is not None:
1865                 for dim, c in zip(v.dims, v.chunks):
1866                     if dim in chunks and c != chunks[dim]:
1867                         raise ValueError(
1868                             f"Object has inconsistent chunks along dimension {dim}. "
1869                             "This can be fixed by calling unify_chunks()."
1870                         )
1871                     chunks[dim] = c
1872         return Frozen(SortedKeysDict(chunks))
1873 
1874     def chunk(
1875         self,
1876         chunks: Union[
1877             Number,
1878             str,
1879             Mapping[Hashable, Union[None, Number, str, Tuple[Number, ...]]],
1880         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1881         name_prefix: str = "xarray-",
1882         token: str = None,
1883         lock: bool = False,
1884     ) -> "Dataset":
1885         """Coerce all arrays in this dataset into dask arrays with the given
1886         chunks.
1887 
1888         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1889         arrays will be rechunked to the given chunk sizes.
1890 
1891         If neither chunks is not provided for one or more dimensions, chunk
1892         sizes along that dimension will not be updated; non-dask arrays will be
1893         converted into dask arrays with a single block.
1894 
1895         Parameters
1896         ----------
1897         chunks : int, 'auto' or mapping, optional
1898             Chunk sizes along each dimension, e.g., ``5`` or
1899             ``{"x": 5, "y": 5}``.
1900         name_prefix : str, optional
1901             Prefix for the name of any new dask arrays.
1902         token : str, optional
1903             Token uniquely identifying this dataset.
1904         lock : optional
1905             Passed on to :py:func:`dask.array.from_array`, if the array is not
1906             already as dask array.
1907 
1908         Returns
1909         -------
1910         chunked : xarray.Dataset
1911         """
1912         if chunks is None:
1913             warnings.warn(
1914                 "None value for 'chunks' is deprecated. "
1915                 "It will raise an error in the future. Use instead '{}'",
1916                 category=FutureWarning,
1917             )
1918             chunks = {}
1919 
1920         if isinstance(chunks, (Number, str)):
1921             chunks = dict.fromkeys(self.dims, chunks)
1922 
1923         bad_dims = chunks.keys() - self.dims.keys()
1924         if bad_dims:
1925             raise ValueError(
1926                 "some chunks keys are not dimensions on this " "object: %s" % bad_dims
1927             )
1928 
1929         variables = {
1930             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
1931             for k, v in self.variables.items()
1932         }
1933         return self._replace(variables)
1934 
1935     def _validate_indexers(
1936         self, indexers: Mapping[Hashable, Any], missing_dims: str = "raise"
1937     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1938         """Here we make sure
1939         + indexer has a valid keys
1940         + indexer is in a valid data type
1941         + string indexers are cast to the appropriate date type if the
1942           associated index is a DatetimeIndex or CFTimeIndex
1943         """
1944         from .dataarray import DataArray
1945 
1946         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1947 
1948         # all indexers should be int, slice, np.ndarrays, or Variable
1949         for k, v in indexers.items():
1950             if isinstance(v, (int, slice, Variable)):
1951                 yield k, v
1952             elif isinstance(v, DataArray):
1953                 yield k, v.variable
1954             elif isinstance(v, tuple):
1955                 yield k, as_variable(v)
1956             elif isinstance(v, Dataset):
1957                 raise TypeError("cannot use a Dataset as an indexer")
1958             elif isinstance(v, Sequence) and len(v) == 0:
1959                 yield k, np.empty((0,), dtype="int64")
1960             else:
1961                 v = np.asarray(v)
1962 
1963                 if v.dtype.kind in "US":
1964                     index = self.indexes[k]
1965                     if isinstance(index, pd.DatetimeIndex):
1966                         v = v.astype("datetime64[ns]")
1967                     elif isinstance(index, xr.CFTimeIndex):
1968                         v = _parse_array_of_cftime_strings(v, index.date_type)
1969 
1970                 if v.ndim > 1:
1971                     raise IndexError(
1972                         "Unlabeled multi-dimensional array cannot be "
1973                         "used for indexing: {}".format(k)
1974                     )
1975                 yield k, v
1976 
1977     def _validate_interp_indexers(
1978         self, indexers: Mapping[Hashable, Any]
1979     ) -> Iterator[Tuple[Hashable, Variable]]:
1980         """Variant of _validate_indexers to be used for interpolation"""
1981         for k, v in self._validate_indexers(indexers):
1982             if isinstance(v, Variable):
1983                 if v.ndim == 1:
1984                     yield k, v.to_index_variable()
1985                 else:
1986                     yield k, v
1987             elif isinstance(v, int):
1988                 yield k, Variable((), v)
1989             elif isinstance(v, np.ndarray):
1990                 if v.ndim == 0:
1991                     yield k, Variable((), v)
1992                 elif v.ndim == 1:
1993                     yield k, IndexVariable((k,), v)
1994                 else:
1995                     raise AssertionError()  # Already tested by _validate_indexers
1996             else:
1997                 raise TypeError(type(v))
1998 
1999     def _get_indexers_coords_and_indexes(self, indexers):
2000         """Extract coordinates and indexes from indexers.
2001 
2002         Only coordinate with a name different from any of self.variables will
2003         be attached.
2004         """
2005         from .dataarray import DataArray
2006 
2007         coords_list = []
2008         for k, v in indexers.items():
2009             if isinstance(v, DataArray):
2010                 if v.dtype.kind == "b":
2011                     if v.ndim != 1:  # we only support 1-d boolean array
2012                         raise ValueError(
2013                             "{:d}d-boolean array is used for indexing along "
2014                             "dimension {!r}, but only 1d boolean arrays are "
2015                             "supported.".format(v.ndim, k)
2016                         )
2017                     # Make sure in case of boolean DataArray, its
2018                     # coordinate also should be indexed.
2019                     v_coords = v[v.values.nonzero()[0]].coords
2020                 else:
2021                     v_coords = v.coords
2022                 coords_list.append(v_coords)
2023 
2024         # we don't need to call align() explicitly or check indexes for
2025         # alignment, because merge_variables already checks for exact alignment
2026         # between dimension coordinates
2027         coords, indexes = merge_coordinates_without_align(coords_list)
2028         assert_coordinate_consistent(self, coords)
2029 
2030         # silently drop the conflicted variables.
2031         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2032         attached_indexes = {
2033             k: v for k, v in indexes.items() if k not in self._variables
2034         }
2035         return attached_coords, attached_indexes
2036 
2037     def isel(
2038         self,
2039         indexers: Mapping[Hashable, Any] = None,
2040         drop: bool = False,
2041         missing_dims: str = "raise",
2042         **indexers_kwargs: Any,
2043     ) -> "Dataset":
2044         """Returns a new dataset with each array indexed along the specified
2045         dimension(s).
2046 
2047         This method selects values from each array using its `__getitem__`
2048         method, except this method does not require knowing the order of
2049         each array's dimensions.
2050 
2051         Parameters
2052         ----------
2053         indexers : dict, optional
2054             A dict with keys matching dimensions and values given
2055             by integers, slice objects or arrays.
2056             indexer can be a integer, slice, array-like or DataArray.
2057             If DataArrays are passed as indexers, xarray-style indexing will be
2058             carried out. See :ref:`indexing` for the details.
2059             One of indexers or indexers_kwargs must be provided.
2060         drop : bool, optional
2061             If ``drop=True``, drop coordinates variables indexed by integers
2062             instead of making them scalar.
2063         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2064             What to do if dimensions that should be selected from are not present in the
2065             Dataset:
2066             - "raise": raise an exception
2067             - "warning": raise a warning, and ignore the missing dimensions
2068             - "ignore": ignore the missing dimensions
2069         **indexers_kwargs : {dim: indexer, ...}, optional
2070             The keyword arguments form of ``indexers``.
2071             One of indexers or indexers_kwargs must be provided.
2072 
2073         Returns
2074         -------
2075         obj : Dataset
2076             A new Dataset with the same contents as this dataset, except each
2077             array and dimension is indexed by the appropriate indexers.
2078             If indexer DataArrays have coordinates that do not conflict with
2079             this object, then these coordinates will be attached.
2080             In general, each array's data will be a view of the array's data
2081             in this dataset, unless vectorized indexing was triggered by using
2082             an array indexer, in which case the data will be a copy.
2083 
2084         See Also
2085         --------
2086         Dataset.sel
2087         DataArray.isel
2088         """
2089         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2090         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2091             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2092 
2093         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2094         # lists, or zero or one-dimensional np.ndarray's
2095         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2096 
2097         variables = {}
2098         dims: Dict[Hashable, Tuple[int, ...]] = {}
2099         coord_names = self._coord_names.copy()
2100         indexes = self._indexes.copy() if self._indexes is not None else None
2101 
2102         for var_name, var_value in self._variables.items():
2103             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
2104             if var_indexers:
2105                 var_value = var_value.isel(var_indexers)
2106                 if drop and var_value.ndim == 0 and var_name in coord_names:
2107                     coord_names.remove(var_name)
2108                     if indexes:
2109                         indexes.pop(var_name, None)
2110                     continue
2111                 if indexes and var_name in indexes:
2112                     if var_value.ndim == 1:
2113                         indexes[var_name] = var_value.to_index()
2114                     else:
2115                         del indexes[var_name]
2116             variables[var_name] = var_value
2117             dims.update(zip(var_value.dims, var_value.shape))
2118 
2119         return self._construct_direct(
2120             variables=variables,
2121             coord_names=coord_names,
2122             dims=dims,
2123             attrs=self._attrs,
2124             indexes=indexes,
2125             encoding=self._encoding,
2126             close=self._close,
2127         )
2128 
2129     def _isel_fancy(
2130         self,
2131         indexers: Mapping[Hashable, Any],
2132         *,
2133         drop: bool,
2134         missing_dims: str = "raise",
2135     ) -> "Dataset":
2136         # Note: we need to preserve the original indexers variable in order to merge the
2137         # coords below
2138         indexers_list = list(self._validate_indexers(indexers, missing_dims))
2139 
2140         variables: Dict[Hashable, Variable] = {}
2141         indexes: Dict[Hashable, pd.Index] = {}
2142 
2143         for name, var in self.variables.items():
2144             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
2145             if drop and name in var_indexers:
2146                 continue  # drop this variable
2147 
2148             if name in self.indexes:
2149                 new_var, new_index = isel_variable_and_index(
2150                     name, var, self.indexes[name], var_indexers
2151                 )
2152                 if new_index is not None:
2153                     indexes[name] = new_index
2154             elif var_indexers:
2155                 new_var = var.isel(indexers=var_indexers)
2156             else:
2157                 new_var = var.copy(deep=False)
2158 
2159             variables[name] = new_var
2160 
2161         coord_names = self._coord_names & variables.keys()
2162         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2163 
2164         # Extract coordinates from indexers
2165         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2166         variables.update(coord_vars)
2167         indexes.update(new_indexes)
2168         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2169         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2170 
2171     def sel(
2172         self,
2173         indexers: Mapping[Hashable, Any] = None,
2174         method: str = None,
2175         tolerance: Number = None,
2176         drop: bool = False,
2177         **indexers_kwargs: Any,
2178     ) -> "Dataset":
2179         """Returns a new dataset with each array indexed by tick labels
2180         along the specified dimension(s).
2181 
2182         In contrast to `Dataset.isel`, indexers for this method should use
2183         labels instead of integers.
2184 
2185         Under the hood, this method is powered by using pandas's powerful Index
2186         objects. This makes label based indexing essentially just as fast as
2187         using integer indexing.
2188 
2189         It also means this method uses pandas's (well documented) logic for
2190         indexing. This means you can use string shortcuts for datetime indexes
2191         (e.g., '2000-01' to select all values in January 2000). It also means
2192         that slices are treated as inclusive of both the start and stop values,
2193         unlike normal Python indexing.
2194 
2195         Parameters
2196         ----------
2197         indexers : dict, optional
2198             A dict with keys matching dimensions and values given
2199             by scalars, slices or arrays of tick labels. For dimensions with
2200             multi-index, the indexer may also be a dict-like object with keys
2201             matching index level names.
2202             If DataArrays are passed as indexers, xarray-style indexing will be
2203             carried out. See :ref:`indexing` for the details.
2204             One of indexers or indexers_kwargs must be provided.
2205         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2206             Method to use for inexact matches:
2207 
2208             * None (default): only exact matches
2209             * pad / ffill: propagate last valid index value forward
2210             * backfill / bfill: propagate next valid index value backward
2211             * nearest: use nearest valid index value
2212         tolerance : optional
2213             Maximum distance between original and new labels for inexact
2214             matches. The values of the index at the matching locations must
2215             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2216         drop : bool, optional
2217             If ``drop=True``, drop coordinates variables in `indexers` instead
2218             of making them scalar.
2219         **indexers_kwargs : {dim: indexer, ...}, optional
2220             The keyword arguments form of ``indexers``.
2221             One of indexers or indexers_kwargs must be provided.
2222 
2223         Returns
2224         -------
2225         obj : Dataset
2226             A new Dataset with the same contents as this dataset, except each
2227             variable and dimension is indexed by the appropriate indexers.
2228             If indexer DataArrays have coordinates that do not conflict with
2229             this object, then these coordinates will be attached.
2230             In general, each array's data will be a view of the array's data
2231             in this dataset, unless vectorized indexing was triggered by using
2232             an array indexer, in which case the data will be a copy.
2233 
2234         See Also
2235         --------
2236         Dataset.isel
2237         DataArray.sel
2238         """
2239         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2240         pos_indexers, new_indexes = remap_label_indexers(
2241             self, indexers=indexers, method=method, tolerance=tolerance
2242         )
2243         result = self.isel(indexers=pos_indexers, drop=drop)
2244         return result._overwrite_indexes(new_indexes)
2245 
2246     def head(
2247         self,
2248         indexers: Union[Mapping[Hashable, int], int] = None,
2249         **indexers_kwargs: Any,
2250     ) -> "Dataset":
2251         """Returns a new dataset with the first `n` values of each array
2252         for the specified dimension(s).
2253 
2254         Parameters
2255         ----------
2256         indexers : dict or int, default: 5
2257             A dict with keys matching dimensions and integer values `n`
2258             or a single integer `n` applied over all dimensions.
2259             One of indexers or indexers_kwargs must be provided.
2260         **indexers_kwargs : {dim: n, ...}, optional
2261             The keyword arguments form of ``indexers``.
2262             One of indexers or indexers_kwargs must be provided.
2263 
2264         See Also
2265         --------
2266         Dataset.tail
2267         Dataset.thin
2268         DataArray.head
2269         """
2270         if not indexers_kwargs:
2271             if indexers is None:
2272                 indexers = 5
2273             if not isinstance(indexers, int) and not is_dict_like(indexers):
2274                 raise TypeError("indexers must be either dict-like or a single integer")
2275         if isinstance(indexers, int):
2276             indexers = {dim: indexers for dim in self.dims}
2277         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2278         for k, v in indexers.items():
2279             if not isinstance(v, int):
2280                 raise TypeError(
2281                     "expected integer type indexer for "
2282                     "dimension %r, found %r" % (k, type(v))
2283                 )
2284             elif v < 0:
2285                 raise ValueError(
2286                     "expected positive integer as indexer "
2287                     "for dimension %r, found %s" % (k, v)
2288                 )
2289         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2290         return self.isel(indexers_slices)
2291 
2292     def tail(
2293         self,
2294         indexers: Union[Mapping[Hashable, int], int] = None,
2295         **indexers_kwargs: Any,
2296     ) -> "Dataset":
2297         """Returns a new dataset with the last `n` values of each array
2298         for the specified dimension(s).
2299 
2300         Parameters
2301         ----------
2302         indexers : dict or int, default: 5
2303             A dict with keys matching dimensions and integer values `n`
2304             or a single integer `n` applied over all dimensions.
2305             One of indexers or indexers_kwargs must be provided.
2306         **indexers_kwargs : {dim: n, ...}, optional
2307             The keyword arguments form of ``indexers``.
2308             One of indexers or indexers_kwargs must be provided.
2309 
2310         See Also
2311         --------
2312         Dataset.head
2313         Dataset.thin
2314         DataArray.tail
2315         """
2316         if not indexers_kwargs:
2317             if indexers is None:
2318                 indexers = 5
2319             if not isinstance(indexers, int) and not is_dict_like(indexers):
2320                 raise TypeError("indexers must be either dict-like or a single integer")
2321         if isinstance(indexers, int):
2322             indexers = {dim: indexers for dim in self.dims}
2323         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2324         for k, v in indexers.items():
2325             if not isinstance(v, int):
2326                 raise TypeError(
2327                     "expected integer type indexer for "
2328                     "dimension %r, found %r" % (k, type(v))
2329                 )
2330             elif v < 0:
2331                 raise ValueError(
2332                     "expected positive integer as indexer "
2333                     "for dimension %r, found %s" % (k, v)
2334                 )
2335         indexers_slices = {
2336             k: slice(-val, None) if val != 0 else slice(val)
2337             for k, val in indexers.items()
2338         }
2339         return self.isel(indexers_slices)
2340 
2341     def thin(
2342         self,
2343         indexers: Union[Mapping[Hashable, int], int] = None,
2344         **indexers_kwargs: Any,
2345     ) -> "Dataset":
2346         """Returns a new dataset with each array indexed along every `n`-th
2347         value for the specified dimension(s)
2348 
2349         Parameters
2350         ----------
2351         indexers : dict or int
2352             A dict with keys matching dimensions and integer values `n`
2353             or a single integer `n` applied over all dimensions.
2354             One of indexers or indexers_kwargs must be provided.
2355         **indexers_kwargs : {dim: n, ...}, optional
2356             The keyword arguments form of ``indexers``.
2357             One of indexers or indexers_kwargs must be provided.
2358 
2359         See Also
2360         --------
2361         Dataset.head
2362         Dataset.tail
2363         DataArray.thin
2364         """
2365         if (
2366             not indexers_kwargs
2367             and not isinstance(indexers, int)
2368             and not is_dict_like(indexers)
2369         ):
2370             raise TypeError("indexers must be either dict-like or a single integer")
2371         if isinstance(indexers, int):
2372             indexers = {dim: indexers for dim in self.dims}
2373         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2374         for k, v in indexers.items():
2375             if not isinstance(v, int):
2376                 raise TypeError(
2377                     "expected integer type indexer for "
2378                     "dimension %r, found %r" % (k, type(v))
2379                 )
2380             elif v < 0:
2381                 raise ValueError(
2382                     "expected positive integer as indexer "
2383                     "for dimension %r, found %s" % (k, v)
2384                 )
2385             elif v == 0:
2386                 raise ValueError("step cannot be zero")
2387         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2388         return self.isel(indexers_slices)
2389 
2390     def broadcast_like(
2391         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2392     ) -> "Dataset":
2393         """Broadcast this DataArray against another Dataset or DataArray.
2394         This is equivalent to xr.broadcast(other, self)[1]
2395 
2396         Parameters
2397         ----------
2398         other : Dataset or DataArray
2399             Object against which to broadcast this array.
2400         exclude : iterable of hashable, optional
2401             Dimensions that must not be broadcasted
2402 
2403         """
2404         if exclude is None:
2405             exclude = set()
2406         else:
2407             exclude = set(exclude)
2408         args = align(other, self, join="outer", copy=False, exclude=exclude)
2409 
2410         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2411 
2412         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2413 
2414     def reindex_like(
2415         self,
2416         other: Union["Dataset", "DataArray"],
2417         method: str = None,
2418         tolerance: Number = None,
2419         copy: bool = True,
2420         fill_value: Any = dtypes.NA,
2421     ) -> "Dataset":
2422         """Conform this object onto the indexes of another object, filling in
2423         missing values with ``fill_value``. The default fill value is NaN.
2424 
2425         Parameters
2426         ----------
2427         other : Dataset or DataArray
2428             Object with an 'indexes' attribute giving a mapping from dimension
2429             names to pandas.Index objects, which provides coordinates upon
2430             which to index the variables in this dataset. The indexes on this
2431             other object need not be the same as the indexes on this
2432             dataset. Any mis-matched index values will be filled in with
2433             NaN, and any mis-matched dimension names will simply be ignored.
2434         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2435             Method to use for filling index values from other not found in this
2436             dataset:
2437 
2438             * None (default): don't fill gaps
2439             * pad / ffill: propagate last valid index value forward
2440             * backfill / bfill: propagate next valid index value backward
2441             * nearest: use nearest valid index value
2442         tolerance : optional
2443             Maximum distance between original and new labels for inexact
2444             matches. The values of the index at the matching locations must
2445             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2446         copy : bool, optional
2447             If ``copy=True``, data in the return value is always copied. If
2448             ``copy=False`` and reindexing is unnecessary, or can be performed
2449             with only slice operations, then the output may share memory with
2450             the input. In either case, a new xarray object is always returned.
2451         fill_value : scalar or dict-like, optional
2452             Value to use for newly missing values. If a dict-like maps
2453             variable names to fill values.
2454 
2455         Returns
2456         -------
2457         reindexed : Dataset
2458             Another dataset, with this dataset's data but coordinates from the
2459             other object.
2460 
2461         See Also
2462         --------
2463         Dataset.reindex
2464         align
2465         """
2466         indexers = alignment.reindex_like_indexers(self, other)
2467         return self.reindex(
2468             indexers=indexers,
2469             method=method,
2470             copy=copy,
2471             fill_value=fill_value,
2472             tolerance=tolerance,
2473         )
2474 
2475     def reindex(
2476         self,
2477         indexers: Mapping[Hashable, Any] = None,
2478         method: str = None,
2479         tolerance: Number = None,
2480         copy: bool = True,
2481         fill_value: Any = dtypes.NA,
2482         **indexers_kwargs: Any,
2483     ) -> "Dataset":
2484         """Conform this object onto a new set of indexes, filling in
2485         missing values with ``fill_value``. The default fill value is NaN.
2486 
2487         Parameters
2488         ----------
2489         indexers : dict, optional
2490             Dictionary with keys given by dimension names and values given by
2491             arrays of coordinates tick labels. Any mis-matched coordinate
2492             values will be filled in with NaN, and any mis-matched dimension
2493             names will simply be ignored.
2494             One of indexers or indexers_kwargs must be provided.
2495         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2496             Method to use for filling index values in ``indexers`` not found in
2497             this dataset:
2498 
2499             * None (default): don't fill gaps
2500             * pad / ffill: propagate last valid index value forward
2501             * backfill / bfill: propagate next valid index value backward
2502             * nearest: use nearest valid index value
2503         tolerance : optional
2504             Maximum distance between original and new labels for inexact
2505             matches. The values of the index at the matching locations must
2506             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2507         copy : bool, optional
2508             If ``copy=True``, data in the return value is always copied. If
2509             ``copy=False`` and reindexing is unnecessary, or can be performed
2510             with only slice operations, then the output may share memory with
2511             the input. In either case, a new xarray object is always returned.
2512         fill_value : scalar or dict-like, optional
2513             Value to use for newly missing values. If a dict-like,
2514             maps variable names (including coordinates) to fill values.
2515         sparse : bool, default: False
2516             use sparse-array.
2517         **indexers_kwargs : {dim: indexer, ...}, optional
2518             Keyword arguments in the same form as ``indexers``.
2519             One of indexers or indexers_kwargs must be provided.
2520 
2521         Returns
2522         -------
2523         reindexed : Dataset
2524             Another dataset, with this dataset's data but replaced coordinates.
2525 
2526         See Also
2527         --------
2528         Dataset.reindex_like
2529         align
2530         pandas.Index.get_indexer
2531 
2532         Examples
2533         --------
2534         Create a dataset with some fictional data.
2535 
2536         >>> import xarray as xr
2537         >>> import pandas as pd
2538         >>> x = xr.Dataset(
2539         ...     {
2540         ...         "temperature": ("station", 20 * np.random.rand(4)),
2541         ...         "pressure": ("station", 500 * np.random.rand(4)),
2542         ...     },
2543         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2544         ... )
2545         >>> x
2546         <xarray.Dataset>
2547         Dimensions:      (station: 4)
2548         Coordinates:
2549           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2550         Data variables:
2551             temperature  (station) float64 10.98 14.3 12.06 10.9
2552             pressure     (station) float64 211.8 322.9 218.8 445.9
2553         >>> x.indexes
2554         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2555 
2556         Create a new index and reindex the dataset. By default values in the new index that
2557         do not have corresponding records in the dataset are assigned `NaN`.
2558 
2559         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2560         >>> x.reindex({"station": new_index})
2561         <xarray.Dataset>
2562         Dimensions:      (station: 4)
2563         Coordinates:
2564           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2565         Data variables:
2566             temperature  (station) float64 10.98 nan 12.06 nan
2567             pressure     (station) float64 211.8 nan 218.8 nan
2568 
2569         We can fill in the missing values by passing a value to the keyword `fill_value`.
2570 
2571         >>> x.reindex({"station": new_index}, fill_value=0)
2572         <xarray.Dataset>
2573         Dimensions:      (station: 4)
2574         Coordinates:
2575           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2576         Data variables:
2577             temperature  (station) float64 10.98 0.0 12.06 0.0
2578             pressure     (station) float64 211.8 0.0 218.8 0.0
2579 
2580         We can also use different fill values for each variable.
2581 
2582         >>> x.reindex(
2583         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
2584         ... )
2585         <xarray.Dataset>
2586         Dimensions:      (station: 4)
2587         Coordinates:
2588           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2589         Data variables:
2590             temperature  (station) float64 10.98 0.0 12.06 0.0
2591             pressure     (station) float64 211.8 100.0 218.8 100.0
2592 
2593         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2594         to the keyword method to fill the `NaN` values.
2595 
2596         >>> x.reindex({"station": new_index}, method="nearest")
2597         Traceback (most recent call last):
2598         ...
2599             raise ValueError('index must be monotonic increasing or decreasing')
2600         ValueError: index must be monotonic increasing or decreasing
2601 
2602         To further illustrate the filling functionality in reindex, we will create a
2603         dataset with a monotonically increasing index (for example, a sequence of dates).
2604 
2605         >>> x2 = xr.Dataset(
2606         ...     {
2607         ...         "temperature": (
2608         ...             "time",
2609         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2610         ...         ),
2611         ...         "pressure": ("time", 500 * np.random.rand(6)),
2612         ...     },
2613         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2614         ... )
2615         >>> x2
2616         <xarray.Dataset>
2617         Dimensions:      (time: 6)
2618         Coordinates:
2619           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2620         Data variables:
2621             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2622             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
2623 
2624         Suppose we decide to expand the dataset to cover a wider date range.
2625 
2626         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2627         >>> x2.reindex({"time": time_index2})
2628         <xarray.Dataset>
2629         Dimensions:      (time: 10)
2630         Coordinates:
2631           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2632         Data variables:
2633             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2634             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
2635 
2636         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2637         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2638 
2639         For example, to back-propagate the last valid value to fill the `NaN` values,
2640         pass `bfill` as an argument to the `method` keyword.
2641 
2642         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2643         >>> x3
2644         <xarray.Dataset>
2645         Dimensions:      (time: 10)
2646         Coordinates:
2647           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2648         Data variables:
2649             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2650             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
2651 
2652         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2653         will not be filled by any of the value propagation schemes.
2654 
2655         >>> x2.where(x2.temperature.isnull(), drop=True)
2656         <xarray.Dataset>
2657         Dimensions:      (time: 1)
2658         Coordinates:
2659           * time         (time) datetime64[ns] 2019-01-03
2660         Data variables:
2661             temperature  (time) float64 nan
2662             pressure     (time) float64 395.9
2663         >>> x3.where(x3.temperature.isnull(), drop=True)
2664         <xarray.Dataset>
2665         Dimensions:      (time: 2)
2666         Coordinates:
2667           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2668         Data variables:
2669             temperature  (time) float64 nan nan
2670             pressure     (time) float64 395.9 nan
2671 
2672         This is because filling while reindexing does not look at dataset values, but only compares
2673         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2674         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2675 
2676         """
2677         return self._reindex(
2678             indexers,
2679             method,
2680             tolerance,
2681             copy,
2682             fill_value,
2683             sparse=False,
2684             **indexers_kwargs,
2685         )
2686 
2687     def _reindex(
2688         self,
2689         indexers: Mapping[Hashable, Any] = None,
2690         method: str = None,
2691         tolerance: Number = None,
2692         copy: bool = True,
2693         fill_value: Any = dtypes.NA,
2694         sparse: bool = False,
2695         **indexers_kwargs: Any,
2696     ) -> "Dataset":
2697         """
2698         same to _reindex but support sparse option
2699         """
2700         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2701 
2702         bad_dims = [d for d in indexers if d not in self.dims]
2703         if bad_dims:
2704             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2705 
2706         variables, indexes = alignment.reindex_variables(
2707             self.variables,
2708             self.sizes,
2709             self.indexes,
2710             indexers,
2711             method,
2712             tolerance,
2713             copy=copy,
2714             fill_value=fill_value,
2715             sparse=sparse,
2716         )
2717         coord_names = set(self._coord_names)
2718         coord_names.update(indexers)
2719         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2720 
2721     def interp(
2722         self,
2723         coords: Mapping[Hashable, Any] = None,
2724         method: str = "linear",
2725         assume_sorted: bool = False,
2726         kwargs: Mapping[str, Any] = None,
2727         **coords_kwargs: Any,
2728     ) -> "Dataset":
2729         """Multidimensional interpolation of Dataset.
2730 
2731         Parameters
2732         ----------
2733         coords : dict, optional
2734             Mapping from dimension names to the new coordinates.
2735             New coordinate can be a scalar, array-like or DataArray.
2736             If DataArrays are passed as new coordinates, their dimensions are
2737             used for the broadcasting. Missing values are skipped.
2738         method : str, optional
2739             {"linear", "nearest"} for multidimensional array,
2740             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2741             for 1-dimensional array. "linear" is used by default.
2742         assume_sorted : bool, optional
2743             If False, values of coordinates that are interpolated over can be
2744             in any order and they are sorted first. If True, interpolated
2745             coordinates are assumed to be an array of monotonically increasing
2746             values.
2747         kwargs : dict, optional
2748             Additional keyword arguments passed to scipy's interpolator. Valid
2749             options and their behavior depend on if 1-dimensional or
2750             multi-dimensional interpolation is used.
2751         **coords_kwargs : {dim: coordinate, ...}, optional
2752             The keyword arguments form of ``coords``.
2753             One of coords or coords_kwargs must be provided.
2754 
2755         Returns
2756         -------
2757         interpolated : Dataset
2758             New dataset on the new coordinates.
2759 
2760         Notes
2761         -----
2762         scipy is required.
2763 
2764         See Also
2765         --------
2766         scipy.interpolate.interp1d
2767         scipy.interpolate.interpn
2768 
2769         Examples
2770         --------
2771         >>> ds = xr.Dataset(
2772         ...     data_vars={
2773         ...         "a": ("x", [5, 7, 4]),
2774         ...         "b": (
2775         ...             ("x", "y"),
2776         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2777         ...         ),
2778         ...     },
2779         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2780         ... )
2781         >>> ds
2782         <xarray.Dataset>
2783         Dimensions:  (x: 3, y: 4)
2784         Coordinates:
2785           * x        (x) int64 0 1 2
2786           * y        (y) int64 10 12 14 16
2787         Data variables:
2788             a        (x) int64 5 7 4
2789             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
2790 
2791         1D interpolation with the default method (linear):
2792 
2793         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
2794         <xarray.Dataset>
2795         Dimensions:  (x: 4, y: 4)
2796         Coordinates:
2797           * y        (y) int64 10 12 14 16
2798           * x        (x) float64 0.0 0.75 1.25 1.75
2799         Data variables:
2800             a        (x) float64 5.0 6.5 6.25 4.75
2801             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
2802 
2803         1D interpolation with a different method:
2804 
2805         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2806         <xarray.Dataset>
2807         Dimensions:  (x: 4, y: 4)
2808         Coordinates:
2809           * y        (y) int64 10 12 14 16
2810           * x        (x) float64 0.0 0.75 1.25 1.75
2811         Data variables:
2812             a        (x) float64 5.0 7.0 7.0 4.0
2813             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
2814 
2815         1D extrapolation:
2816 
2817         >>> ds.interp(
2818         ...     x=[1, 1.5, 2.5, 3.5],
2819         ...     method="linear",
2820         ...     kwargs={"fill_value": "extrapolate"},
2821         ... )
2822         <xarray.Dataset>
2823         Dimensions:  (x: 4, y: 4)
2824         Coordinates:
2825           * y        (y) int64 10 12 14 16
2826           * x        (x) float64 1.0 1.5 2.5 3.5
2827         Data variables:
2828             a        (x) float64 7.0 5.5 2.5 -0.5
2829             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
2830 
2831         2D interpolation:
2832 
2833         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2834         <xarray.Dataset>
2835         Dimensions:  (x: 4, y: 3)
2836         Coordinates:
2837           * x        (x) float64 0.0 0.75 1.25 1.75
2838           * y        (y) int64 11 13 15
2839         Data variables:
2840             a        (x) float64 5.0 6.5 6.25 4.75
2841             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
2842         """
2843         from . import missing
2844 
2845         if kwargs is None:
2846             kwargs = {}
2847 
2848         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2849         indexers = dict(self._validate_interp_indexers(coords))
2850 
2851         if coords:
2852             # This avoids broadcasting over coordinates that are both in
2853             # the original array AND in the indexing array. It essentially
2854             # forces interpolation along the shared coordinates.
2855             sdims = (
2856                 set(self.dims)
2857                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2858                 .difference(coords.keys())
2859             )
2860             indexers.update({d: self.variables[d] for d in sdims})
2861 
2862         obj = self if assume_sorted else self.sortby([k for k in coords])
2863 
2864         def maybe_variable(obj, k):
2865             # workaround to get variable for dimension without coordinate.
2866             try:
2867                 return obj._variables[k]
2868             except KeyError:
2869                 return as_variable((k, range(obj.dims[k])))
2870 
2871         def _validate_interp_indexer(x, new_x):
2872             # In the case of datetimes, the restrictions placed on indexers
2873             # used with interp are stronger than those which are placed on
2874             # isel, so we need an additional check after _validate_indexers.
2875             if _contains_datetime_like_objects(
2876                 x
2877             ) and not _contains_datetime_like_objects(new_x):
2878                 raise TypeError(
2879                     "When interpolating over a datetime-like "
2880                     "coordinate, the coordinates to "
2881                     "interpolate to must be either datetime "
2882                     "strings or datetimes. "
2883                     "Instead got\n{}".format(new_x)
2884                 )
2885             return x, new_x
2886 
2887         variables: Dict[Hashable, Variable] = {}
2888         for name, var in obj._variables.items():
2889             if name in indexers:
2890                 continue
2891 
2892             if var.dtype.kind in "uifc":
2893                 var_indexers = {
2894                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2895                     for k, v in indexers.items()
2896                     if k in var.dims
2897                 }
2898                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2899             elif all(d not in indexers for d in var.dims):
2900                 # keep unrelated object array
2901                 variables[name] = var
2902 
2903         coord_names = obj._coord_names & variables.keys()
2904         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2905         selected = self._replace_with_new_dims(
2906             variables.copy(), coord_names, indexes=indexes
2907         )
2908 
2909         # attach indexer as coordinate
2910         variables.update(indexers)
2911         for k, v in indexers.items():
2912             assert isinstance(v, Variable)
2913             if v.dims == (k,):
2914                 indexes[k] = v.to_index()
2915 
2916         # Extract coordinates from indexers
2917         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2918         variables.update(coord_vars)
2919         indexes.update(new_indexes)
2920 
2921         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2922         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2923 
2924     def interp_like(
2925         self,
2926         other: Union["Dataset", "DataArray"],
2927         method: str = "linear",
2928         assume_sorted: bool = False,
2929         kwargs: Mapping[str, Any] = None,
2930     ) -> "Dataset":
2931         """Interpolate this object onto the coordinates of another object,
2932         filling the out of range values with NaN.
2933 
2934         Parameters
2935         ----------
2936         other : Dataset or DataArray
2937             Object with an 'indexes' attribute giving a mapping from dimension
2938             names to an 1d array-like, which provides coordinates upon
2939             which to index the variables in this dataset. Missing values are skipped.
2940         method : str, optional
2941             {"linear", "nearest"} for multidimensional array,
2942             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2943             for 1-dimensional array. 'linear' is used by default.
2944         assume_sorted : bool, optional
2945             If False, values of coordinates that are interpolated over can be
2946             in any order and they are sorted first. If True, interpolated
2947             coordinates are assumed to be an array of monotonically increasing
2948             values.
2949         kwargs : dict, optional
2950             Additional keyword passed to scipy's interpolator.
2951 
2952         Returns
2953         -------
2954         interpolated : Dataset
2955             Another dataset by interpolating this dataset's data along the
2956             coordinates of the other object.
2957 
2958         Notes
2959         -----
2960         scipy is required.
2961         If the dataset has object-type coordinates, reindex is used for these
2962         coordinates instead of the interpolation.
2963 
2964         See Also
2965         --------
2966         Dataset.interp
2967         Dataset.reindex_like
2968         """
2969         if kwargs is None:
2970             kwargs = {}
2971         coords = alignment.reindex_like_indexers(self, other)
2972 
2973         numeric_coords: Dict[Hashable, pd.Index] = {}
2974         object_coords: Dict[Hashable, pd.Index] = {}
2975         for k, v in coords.items():
2976             if v.dtype.kind in "uifcMm":
2977                 numeric_coords[k] = v
2978             else:
2979                 object_coords[k] = v
2980 
2981         ds = self
2982         if object_coords:
2983             # We do not support interpolation along object coordinate.
2984             # reindex instead.
2985             ds = self.reindex(object_coords)
2986         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2987 
2988     # Helper methods for rename()
2989     def _rename_vars(self, name_dict, dims_dict):
2990         variables = {}
2991         coord_names = set()
2992         for k, v in self.variables.items():
2993             var = v.copy(deep=False)
2994             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2995             name = name_dict.get(k, k)
2996             if name in variables:
2997                 raise ValueError(f"the new name {name!r} conflicts")
2998             variables[name] = var
2999             if k in self._coord_names:
3000                 coord_names.add(name)
3001         return variables, coord_names
3002 
3003     def _rename_dims(self, name_dict):
3004         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3005 
3006     def _rename_indexes(self, name_dict, dims_set):
3007         if self._indexes is None:
3008             return None
3009         indexes = {}
3010         for k, v in self.indexes.items():
3011             new_name = name_dict.get(k, k)
3012             if new_name not in dims_set:
3013                 continue
3014             if isinstance(v, pd.MultiIndex):
3015                 new_names = [name_dict.get(k, k) for k in v.names]
3016                 index = v.rename(names=new_names)
3017             else:
3018                 index = v.rename(new_name)
3019             indexes[new_name] = index
3020         return indexes
3021 
3022     def _rename_all(self, name_dict, dims_dict):
3023         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3024         dims = self._rename_dims(dims_dict)
3025         indexes = self._rename_indexes(name_dict, dims.keys())
3026         return variables, coord_names, dims, indexes
3027 
3028     def rename(
3029         self,
3030         name_dict: Mapping[Hashable, Hashable] = None,
3031         **names: Hashable,
3032     ) -> "Dataset":
3033         """Returns a new object with renamed variables and dimensions.
3034 
3035         Parameters
3036         ----------
3037         name_dict : dict-like, optional
3038             Dictionary whose keys are current variable or dimension names and
3039             whose values are the desired names.
3040         **names : optional
3041             Keyword form of ``name_dict``.
3042             One of name_dict or names must be provided.
3043 
3044         Returns
3045         -------
3046         renamed : Dataset
3047             Dataset with renamed variables and dimensions.
3048 
3049         See Also
3050         --------
3051         Dataset.swap_dims
3052         Dataset.rename_vars
3053         Dataset.rename_dims
3054         DataArray.rename
3055         """
3056         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3057         for k in name_dict.keys():
3058             if k not in self and k not in self.dims:
3059                 raise ValueError(
3060                     "cannot rename %r because it is not a "
3061                     "variable or dimension in this dataset" % k
3062                 )
3063 
3064         variables, coord_names, dims, indexes = self._rename_all(
3065             name_dict=name_dict, dims_dict=name_dict
3066         )
3067         assert_unique_multiindex_level_names(variables)
3068         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3069 
3070     def rename_dims(
3071         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
3072     ) -> "Dataset":
3073         """Returns a new object with renamed dimensions only.
3074 
3075         Parameters
3076         ----------
3077         dims_dict : dict-like, optional
3078             Dictionary whose keys are current dimension names and
3079             whose values are the desired names. The desired names must
3080             not be the name of an existing dimension or Variable in the Dataset.
3081         **dims : optional
3082             Keyword form of ``dims_dict``.
3083             One of dims_dict or dims must be provided.
3084 
3085         Returns
3086         -------
3087         renamed : Dataset
3088             Dataset with renamed dimensions.
3089 
3090         See Also
3091         --------
3092         Dataset.swap_dims
3093         Dataset.rename
3094         Dataset.rename_vars
3095         DataArray.rename
3096         """
3097         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3098         for k, v in dims_dict.items():
3099             if k not in self.dims:
3100                 raise ValueError(
3101                     "cannot rename %r because it is not a "
3102                     "dimension in this dataset" % k
3103                 )
3104             if v in self.dims or v in self:
3105                 raise ValueError(
3106                     f"Cannot rename {k} to {v} because {v} already exists. "
3107                     "Try using swap_dims instead."
3108                 )
3109 
3110         variables, coord_names, sizes, indexes = self._rename_all(
3111             name_dict={}, dims_dict=dims_dict
3112         )
3113         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3114 
3115     def rename_vars(
3116         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
3117     ) -> "Dataset":
3118         """Returns a new object with renamed variables including coordinates
3119 
3120         Parameters
3121         ----------
3122         name_dict : dict-like, optional
3123             Dictionary whose keys are current variable or coordinate names and
3124             whose values are the desired names.
3125         **names : optional
3126             Keyword form of ``name_dict``.
3127             One of name_dict or names must be provided.
3128 
3129         Returns
3130         -------
3131         renamed : Dataset
3132             Dataset with renamed variables including coordinates
3133 
3134         See Also
3135         --------
3136         Dataset.swap_dims
3137         Dataset.rename
3138         Dataset.rename_dims
3139         DataArray.rename
3140         """
3141         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3142         for k in name_dict:
3143             if k not in self:
3144                 raise ValueError(
3145                     "cannot rename %r because it is not a "
3146                     "variable or coordinate in this dataset" % k
3147                 )
3148         variables, coord_names, dims, indexes = self._rename_all(
3149             name_dict=name_dict, dims_dict={}
3150         )
3151         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3152 
3153     def swap_dims(
3154         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs
3155     ) -> "Dataset":
3156         """Returns a new object with swapped dimensions.
3157 
3158         Parameters
3159         ----------
3160         dims_dict : dict-like
3161             Dictionary whose keys are current dimension names and whose values
3162             are new names.
3163         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3164             The keyword arguments form of ``dims_dict``.
3165             One of dims_dict or dims_kwargs must be provided.
3166 
3167         Returns
3168         -------
3169         swapped : Dataset
3170             Dataset with swapped dimensions.
3171 
3172         Examples
3173         --------
3174         >>> ds = xr.Dataset(
3175         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3176         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3177         ... )
3178         >>> ds
3179         <xarray.Dataset>
3180         Dimensions:  (x: 2)
3181         Coordinates:
3182           * x        (x) <U1 'a' 'b'
3183             y        (x) int64 0 1
3184         Data variables:
3185             a        (x) int64 5 7
3186             b        (x) float64 0.1 2.4
3187 
3188         >>> ds.swap_dims({"x": "y"})
3189         <xarray.Dataset>
3190         Dimensions:  (y: 2)
3191         Coordinates:
3192             x        (y) <U1 'a' 'b'
3193           * y        (y) int64 0 1
3194         Data variables:
3195             a        (y) int64 5 7
3196             b        (y) float64 0.1 2.4
3197 
3198         >>> ds.swap_dims({"x": "z"})
3199         <xarray.Dataset>
3200         Dimensions:  (z: 2)
3201         Coordinates:
3202             x        (z) <U1 'a' 'b'
3203             y        (z) int64 0 1
3204         Dimensions without coordinates: z
3205         Data variables:
3206             a        (z) int64 5 7
3207             b        (z) float64 0.1 2.4
3208 
3209         See Also
3210         --------
3211         Dataset.rename
3212         DataArray.swap_dims
3213         """
3214         # TODO: deprecate this method in favor of a (less confusing)
3215         # rename_dims() method that only renames dimensions.
3216 
3217         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3218         for k, v in dims_dict.items():
3219             if k not in self.dims:
3220                 raise ValueError(
3221                     "cannot swap from dimension %r because it is "
3222                     "not an existing dimension" % k
3223                 )
3224             if v in self.variables and self.variables[v].dims != (k,):
3225                 raise ValueError(
3226                     "replacement dimension %r is not a 1D "
3227                     "variable along the old dimension %r" % (v, k)
3228                 )
3229 
3230         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3231 
3232         coord_names = self._coord_names.copy()
3233         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3234 
3235         variables: Dict[Hashable, Variable] = {}
3236         indexes: Dict[Hashable, pd.Index] = {}
3237         for k, v in self.variables.items():
3238             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3239             if k in result_dims:
3240                 var = v.to_index_variable()
3241                 if k in self.indexes:
3242                     indexes[k] = self.indexes[k]
3243                 else:
3244                     new_index = var.to_index()
3245                     if new_index.nlevels == 1:
3246                         # make sure index name matches dimension name
3247                         new_index = new_index.rename(k)
3248                     indexes[k] = new_index
3249             else:
3250                 var = v.to_base_variable()
3251             var.dims = dims
3252             variables[k] = var
3253 
3254         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3255 
3256     def expand_dims(
3257         self,
3258         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
3259         axis: Union[None, int, Sequence[int]] = None,
3260         **dim_kwargs: Any,
3261     ) -> "Dataset":
3262         """Return a new object with an additional axis (or axes) inserted at
3263         the corresponding position in the array shape.  The new object is a
3264         view into the underlying array, not a copy.
3265 
3266         If dim is already a scalar coordinate, it will be promoted to a 1D
3267         coordinate consisting of a single value.
3268 
3269         Parameters
3270         ----------
3271         dim : hashable, sequence of hashable, mapping, or None
3272             Dimensions to include on the new variable. If provided as hashable
3273             or sequence of hashable, then dimensions are inserted with length
3274             1. If provided as a mapping, then the keys are the new dimensions
3275             and the values are either integers (giving the length of the new
3276             dimensions) or array-like (giving the coordinates of the new
3277             dimensions).
3278         axis : int, sequence of int, or None
3279             Axis position(s) where new axis is to be inserted (position(s) on
3280             the result array). If a list (or tuple) of integers is passed,
3281             multiple axes are inserted. In this case, dim arguments should be
3282             same length list. If axis=None is passed, all the axes will be
3283             inserted to the start of the result array.
3284         **dim_kwargs : int or sequence or ndarray
3285             The keywords are arbitrary dimensions being inserted and the values
3286             are either the lengths of the new dims (if int is given), or their
3287             coordinates. Note, this is an alternative to passing a dict to the
3288             dim kwarg and will only be used if dim is None.
3289 
3290         Returns
3291         -------
3292         expanded : same type as caller
3293             This object, but with an additional dimension(s).
3294         """
3295         if dim is None:
3296             pass
3297         elif isinstance(dim, Mapping):
3298             # We're later going to modify dim in place; don't tamper with
3299             # the input
3300             dim = dict(dim)
3301         elif isinstance(dim, int):
3302             raise TypeError(
3303                 "dim should be hashable or sequence of hashables or mapping"
3304             )
3305         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3306             dim = {dim: 1}
3307         elif isinstance(dim, Sequence):
3308             if len(dim) != len(set(dim)):
3309                 raise ValueError("dims should not contain duplicate values.")
3310             dim = {d: 1 for d in dim}
3311 
3312         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3313         assert isinstance(dim, MutableMapping)
3314 
3315         if axis is None:
3316             axis = list(range(len(dim)))
3317         elif not isinstance(axis, Sequence):
3318             axis = [axis]
3319 
3320         if len(dim) != len(axis):
3321             raise ValueError("lengths of dim and axis should be identical.")
3322         for d in dim:
3323             if d in self.dims:
3324                 raise ValueError(f"Dimension {d} already exists.")
3325             if d in self._variables and not utils.is_scalar(self._variables[d]):
3326                 raise ValueError(
3327                     "{dim} already exists as coordinate or"
3328                     " variable name.".format(dim=d)
3329                 )
3330 
3331         variables: Dict[Hashable, Variable] = {}
3332         coord_names = self._coord_names.copy()
3333         # If dim is a dict, then ensure that the values are either integers
3334         # or iterables.
3335         for k, v in dim.items():
3336             if hasattr(v, "__iter__"):
3337                 # If the value for the new dimension is an iterable, then
3338                 # save the coordinates to the variables dict, and set the
3339                 # value within the dim dict to the length of the iterable
3340                 # for later use.
3341                 variables[k] = xr.IndexVariable((k,), v)
3342                 coord_names.add(k)
3343                 dim[k] = variables[k].size
3344             elif isinstance(v, int):
3345                 pass  # Do nothing if the dimensions value is just an int
3346             else:
3347                 raise TypeError(
3348                     "The value of new dimension {k} must be "
3349                     "an iterable or an int".format(k=k)
3350                 )
3351 
3352         for k, v in self._variables.items():
3353             if k not in dim:
3354                 if k in coord_names:  # Do not change coordinates
3355                     variables[k] = v
3356                 else:
3357                     result_ndim = len(v.dims) + len(axis)
3358                     for a in axis:
3359                         if a < -result_ndim or result_ndim - 1 < a:
3360                             raise IndexError(
3361                                 f"Axis {a} of variable {k} is out of bounds of the "
3362                                 f"expanded dimension size {result_ndim}"
3363                             )
3364 
3365                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3366                     if len(axis_pos) != len(set(axis_pos)):
3367                         raise ValueError("axis should not contain duplicate values")
3368                     # We need to sort them to make sure `axis` equals to the
3369                     # axis positions of the result array.
3370                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3371 
3372                     all_dims = list(zip(v.dims, v.shape))
3373                     for d, c in zip_axis_dim:
3374                         all_dims.insert(d, c)
3375                     variables[k] = v.set_dims(dict(all_dims))
3376             else:
3377                 # If dims includes a label of a non-dimension coordinate,
3378                 # it will be promoted to a 1D coordinate with a single value.
3379                 variables[k] = v.set_dims(k).to_index_variable()
3380 
3381         new_dims = self._dims.copy()
3382         new_dims.update(dim)
3383 
3384         return self._replace_vars_and_dims(
3385             variables, dims=new_dims, coord_names=coord_names
3386         )
3387 
3388     def set_index(
3389         self,
3390         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3391         append: bool = False,
3392         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3393     ) -> "Dataset":
3394         """Set Dataset (multi-)indexes using one or more existing coordinates
3395         or variables.
3396 
3397         Parameters
3398         ----------
3399         indexes : {dim: index, ...}
3400             Mapping from names matching dimensions and values given
3401             by (lists of) the names of existing coordinates or variables to set
3402             as new (multi-)index.
3403         append : bool, optional
3404             If True, append the supplied index(es) to the existing index(es).
3405             Otherwise replace the existing index(es) (default).
3406         **indexes_kwargs : optional
3407             The keyword arguments form of ``indexes``.
3408             One of indexes or indexes_kwargs must be provided.
3409 
3410         Returns
3411         -------
3412         obj : Dataset
3413             Another dataset, with this dataset's data but replaced coordinates.
3414 
3415         Examples
3416         --------
3417         >>> arr = xr.DataArray(
3418         ...     data=np.ones((2, 3)),
3419         ...     dims=["x", "y"],
3420         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3421         ... )
3422         >>> ds = xr.Dataset({"v": arr})
3423         >>> ds
3424         <xarray.Dataset>
3425         Dimensions:  (x: 2, y: 3)
3426         Coordinates:
3427           * x        (x) int64 0 1
3428           * y        (y) int64 0 1 2
3429             a        (x) int64 3 4
3430         Data variables:
3431             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3432         >>> ds.set_index(x="a")
3433         <xarray.Dataset>
3434         Dimensions:  (x: 2, y: 3)
3435         Coordinates:
3436           * x        (x) int64 3 4
3437           * y        (y) int64 0 1 2
3438         Data variables:
3439             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3440 
3441         See Also
3442         --------
3443         Dataset.reset_index
3444         Dataset.swap_dims
3445         """
3446         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3447         variables, coord_names = merge_indexes(
3448             indexes, self._variables, self._coord_names, append=append
3449         )
3450         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3451 
3452     def reset_index(
3453         self,
3454         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3455         drop: bool = False,
3456     ) -> "Dataset":
3457         """Reset the specified index(es) or multi-index level(s).
3458 
3459         Parameters
3460         ----------
3461         dims_or_levels : str or list
3462             Name(s) of the dimension(s) and/or multi-index level(s) that will
3463             be reset.
3464         drop : bool, optional
3465             If True, remove the specified indexes and/or multi-index levels
3466             instead of extracting them as new coordinates (default: False).
3467 
3468         Returns
3469         -------
3470         obj : Dataset
3471             Another dataset, with this dataset's data but replaced coordinates.
3472 
3473         See Also
3474         --------
3475         Dataset.set_index
3476         """
3477         variables, coord_names = split_indexes(
3478             dims_or_levels,
3479             self._variables,
3480             self._coord_names,
3481             cast(Mapping[Hashable, Hashable], self._level_coords),
3482             drop=drop,
3483         )
3484         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3485 
3486     def reorder_levels(
3487         self,
3488         dim_order: Mapping[Hashable, Sequence[int]] = None,
3489         **dim_order_kwargs: Sequence[int],
3490     ) -> "Dataset":
3491         """Rearrange index levels using input order.
3492 
3493         Parameters
3494         ----------
3495         dim_order : optional
3496             Mapping from names matching dimensions and values given
3497             by lists representing new level orders. Every given dimension
3498             must have a multi-index.
3499         **dim_order_kwargs : optional
3500             The keyword arguments form of ``dim_order``.
3501             One of dim_order or dim_order_kwargs must be provided.
3502 
3503         Returns
3504         -------
3505         obj : Dataset
3506             Another dataset, with this dataset's data but replaced
3507             coordinates.
3508         """
3509         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3510         variables = self._variables.copy()
3511         indexes = dict(self.indexes)
3512         for dim, order in dim_order.items():
3513             coord = self._variables[dim]
3514             index = self.indexes[dim]
3515             if not isinstance(index, pd.MultiIndex):
3516                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3517             new_index = index.reorder_levels(order)
3518             variables[dim] = IndexVariable(coord.dims, new_index)
3519             indexes[dim] = new_index
3520 
3521         return self._replace(variables, indexes=indexes)
3522 
3523     def _stack_once(self, dims, new_dim):
3524         if ... in dims:
3525             dims = list(infix_dims(dims, self.dims))
3526         variables = {}
3527         for name, var in self.variables.items():
3528             if name not in dims:
3529                 if any(d in var.dims for d in dims):
3530                     add_dims = [d for d in dims if d not in var.dims]
3531                     vdims = list(var.dims) + add_dims
3532                     shape = [self.dims[d] for d in vdims]
3533                     exp_var = var.set_dims(vdims, shape)
3534                     stacked_var = exp_var.stack(**{new_dim: dims})
3535                     variables[name] = stacked_var
3536                 else:
3537                     variables[name] = var.copy(deep=False)
3538 
3539         # consider dropping levels that are unused?
3540         levels = [self.get_index(dim) for dim in dims]
3541         idx = utils.multiindex_from_product_levels(levels, names=dims)
3542         variables[new_dim] = IndexVariable(new_dim, idx)
3543 
3544         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3545 
3546         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3547         indexes[new_dim] = idx
3548 
3549         return self._replace_with_new_dims(
3550             variables, coord_names=coord_names, indexes=indexes
3551         )
3552 
3553     def stack(
3554         self,
3555         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3556         **dimensions_kwargs: Sequence[Hashable],
3557     ) -> "Dataset":
3558         """
3559         Stack any number of existing dimensions into a single new dimension.
3560 
3561         New dimensions will be added at the end, and the corresponding
3562         coordinate variables will be combined into a MultiIndex.
3563 
3564         Parameters
3565         ----------
3566         dimensions : mapping of hashable to sequence of hashable
3567             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
3568             dimensions, and the existing dimensions that they replace. An
3569             ellipsis (`...`) will be replaced by all unlisted dimensions.
3570             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
3571             all dimensions.
3572         **dimensions_kwargs
3573             The keyword arguments form of ``dimensions``.
3574             One of dimensions or dimensions_kwargs must be provided.
3575 
3576         Returns
3577         -------
3578         stacked : Dataset
3579             Dataset with stacked data.
3580 
3581         See Also
3582         --------
3583         Dataset.unstack
3584         """
3585         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3586         result = self
3587         for new_dim, dims in dimensions.items():
3588             result = result._stack_once(dims, new_dim)
3589         return result
3590 
3591     def to_stacked_array(
3592         self,
3593         new_dim: Hashable,
3594         sample_dims: Sequence[Hashable],
3595         variable_dim: str = "variable",
3596         name: Hashable = None,
3597     ) -> "DataArray":
3598         """Combine variables of differing dimensionality into a DataArray
3599         without broadcasting.
3600 
3601         This method is similar to Dataset.to_array but does not broadcast the
3602         variables.
3603 
3604         Parameters
3605         ----------
3606         new_dim : hashable
3607             Name of the new stacked coordinate
3608         sample_dims : sequence of hashable
3609             Dimensions that **will not** be stacked. Each array in the dataset
3610             must share these dimensions. For machine learning applications,
3611             these define the dimensions over which samples are drawn.
3612         variable_dim : str, optional
3613             Name of the level in the stacked coordinate which corresponds to
3614             the variables.
3615         name : str, optional
3616             Name of the new data array.
3617 
3618         Returns
3619         -------
3620         stacked : DataArray
3621             DataArray with the specified dimensions and data variables
3622             stacked together. The stacked coordinate is named ``new_dim``
3623             and represented by a MultiIndex object with a level containing the
3624             data variable names. The name of this level is controlled using
3625             the ``variable_dim`` argument.
3626 
3627         See Also
3628         --------
3629         Dataset.to_array
3630         Dataset.stack
3631         DataArray.to_unstacked_dataset
3632 
3633         Examples
3634         --------
3635         >>> data = xr.Dataset(
3636         ...     data_vars={
3637         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3638         ...         "b": ("x", [6, 7]),
3639         ...     },
3640         ...     coords={"y": ["u", "v", "w"]},
3641         ... )
3642 
3643         >>> data
3644         <xarray.Dataset>
3645         Dimensions:  (x: 2, y: 3)
3646         Coordinates:
3647           * y        (y) <U1 'u' 'v' 'w'
3648         Dimensions without coordinates: x
3649         Data variables:
3650             a        (x, y) int64 0 1 2 3 4 5
3651             b        (x) int64 6 7
3652 
3653         >>> data.to_stacked_array("z", sample_dims=["x"])
3654         <xarray.DataArray 'a' (x: 2, z: 4)>
3655         array([[0, 1, 2, 6],
3656                [3, 4, 5, 7]])
3657         Coordinates:
3658           * z         (z) MultiIndex
3659           - variable  (z) object 'a' 'a' 'a' 'b'
3660           - y         (z) object 'u' 'v' 'w' nan
3661         Dimensions without coordinates: x
3662 
3663         """
3664         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3665 
3666         for variable in self:
3667             dims = self[variable].dims
3668             dims_include_sample_dims = set(sample_dims) <= set(dims)
3669             if not dims_include_sample_dims:
3670                 raise ValueError(
3671                     "All variables in the dataset must contain the "
3672                     "dimensions {}.".format(dims)
3673                 )
3674 
3675         def ensure_stackable(val):
3676             assign_coords = {variable_dim: val.name}
3677             for dim in stacking_dims:
3678                 if dim not in val.dims:
3679                     assign_coords[dim] = None
3680 
3681             expand_dims = set(stacking_dims).difference(set(val.dims))
3682             expand_dims.add(variable_dim)
3683             # must be list for .expand_dims
3684             expand_dims = list(expand_dims)
3685 
3686             return (
3687                 val.assign_coords(**assign_coords)
3688                 .expand_dims(expand_dims)
3689                 .stack({new_dim: (variable_dim,) + stacking_dims})
3690             )
3691 
3692         # concatenate the arrays
3693         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3694         data_array = xr.concat(stackable_vars, dim=new_dim)
3695 
3696         # coerce the levels of the MultiIndex to have the same type as the
3697         # input dimensions. This code is messy, so it might be better to just
3698         # input a dummy value for the singleton dimension.
3699         idx = data_array.indexes[new_dim]
3700         levels = [idx.levels[0]] + [
3701             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3702         ]
3703         new_idx = idx.set_levels(levels)
3704         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3705 
3706         if name is not None:
3707             data_array.name = name
3708 
3709         return data_array
3710 
3711     def _unstack_once(self, dim: Hashable, fill_value) -> "Dataset":
3712         index = self.get_index(dim)
3713         index = remove_unused_levels_categories(index)
3714 
3715         variables: Dict[Hashable, Variable] = {}
3716         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3717 
3718         for name, var in self.variables.items():
3719             if name != dim:
3720                 if dim in var.dims:
3721                     if isinstance(fill_value, Mapping):
3722                         fill_value_ = fill_value[name]
3723                     else:
3724                         fill_value_ = fill_value
3725 
3726                     variables[name] = var._unstack_once(
3727                         index=index, dim=dim, fill_value=fill_value_
3728                     )
3729                 else:
3730                     variables[name] = var
3731 
3732         for name, lev in zip(index.names, index.levels):
3733             variables[name] = IndexVariable(name, lev)
3734             indexes[name] = lev
3735 
3736         coord_names = set(self._coord_names) - {dim} | set(index.names)
3737 
3738         return self._replace_with_new_dims(
3739             variables, coord_names=coord_names, indexes=indexes
3740         )
3741 
3742     def _unstack_full_reindex(
3743         self, dim: Hashable, fill_value, sparse: bool
3744     ) -> "Dataset":
3745         index = self.get_index(dim)
3746         index = remove_unused_levels_categories(index)
3747         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3748 
3749         # take a shortcut in case the MultiIndex was not modified.
3750         if index.equals(full_idx):
3751             obj = self
3752         else:
3753             obj = self._reindex(
3754                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3755             )
3756 
3757         new_dim_names = index.names
3758         new_dim_sizes = [lev.size for lev in index.levels]
3759 
3760         variables: Dict[Hashable, Variable] = {}
3761         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3762 
3763         for name, var in obj.variables.items():
3764             if name != dim:
3765                 if dim in var.dims:
3766                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3767                     variables[name] = var.unstack({dim: new_dims})
3768                 else:
3769                     variables[name] = var
3770 
3771         for name, lev in zip(new_dim_names, index.levels):
3772             variables[name] = IndexVariable(name, lev)
3773             indexes[name] = lev
3774 
3775         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3776 
3777         return self._replace_with_new_dims(
3778             variables, coord_names=coord_names, indexes=indexes
3779         )
3780 
3781     def unstack(
3782         self,
3783         dim: Union[Hashable, Iterable[Hashable]] = None,
3784         fill_value: Any = dtypes.NA,
3785         sparse: bool = False,
3786     ) -> "Dataset":
3787         """
3788         Unstack existing dimensions corresponding to MultiIndexes into
3789         multiple new dimensions.
3790 
3791         New dimensions will be added at the end.
3792 
3793         Parameters
3794         ----------
3795         dim : hashable or iterable of hashable, optional
3796             Dimension(s) over which to unstack. By default unstacks all
3797             MultiIndexes.
3798         fill_value : scalar or dict-like, default: nan
3799             value to be filled. If a dict-like, maps variable names to
3800             fill values. If not provided or if the dict-like does not
3801             contain all variables, the dtype's NA value will be used.
3802         sparse : bool, default: False
3803             use sparse-array if True
3804 
3805         Returns
3806         -------
3807         unstacked : Dataset
3808             Dataset with unstacked data.
3809 
3810         See Also
3811         --------
3812         Dataset.stack
3813         """
3814         if dim is None:
3815             dims = [
3816                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3817             ]
3818         else:
3819             if isinstance(dim, str) or not isinstance(dim, Iterable):
3820                 dims = [dim]
3821             else:
3822                 dims = list(dim)
3823 
3824             missing_dims = [d for d in dims if d not in self.dims]
3825             if missing_dims:
3826                 raise ValueError(
3827                     "Dataset does not contain the dimensions: %s" % missing_dims
3828                 )
3829 
3830             non_multi_dims = [
3831                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3832             ]
3833             if non_multi_dims:
3834                 raise ValueError(
3835                     "cannot unstack dimensions that do not "
3836                     "have a MultiIndex: %s" % non_multi_dims
3837                 )
3838 
3839         result = self.copy(deep=False)
3840         for dim in dims:
3841 
3842             if (
3843                 # Dask arrays don't support assignment by index, which the fast unstack
3844                 # function requires.
3845                 # https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
3846                 any(is_duck_dask_array(v.data) for v in self.variables.values())
3847                 # Sparse doesn't currently support (though we could special-case
3848                 # it)
3849                 # https://github.com/pydata/sparse/issues/422
3850                 or any(
3851                     isinstance(v.data, sparse_array_type)
3852                     for v in self.variables.values()
3853                 )
3854                 or sparse
3855                 # numpy full_like only added `shape` in 1.17
3856                 or LooseVersion(np.__version__) < LooseVersion("1.17")
3857                 # Until https://github.com/pydata/xarray/pull/4751 is resolved,
3858                 # we check explicitly whether it's a numpy array. Once that is
3859                 # resolved, explicitly exclude pint arrays.
3860                 # # pint doesn't implement `np.full_like` in a way that's
3861                 # # currently compatible.
3862                 # # https://github.com/pydata/xarray/pull/4746#issuecomment-753425173
3863                 # # or any(
3864                 # #     isinstance(v.data, pint_array_type) for v in self.variables.values()
3865                 # # )
3866                 or any(
3867                     not isinstance(v.data, np.ndarray) for v in self.variables.values()
3868                 )
3869             ):
3870                 result = result._unstack_full_reindex(dim, fill_value, sparse)
3871             else:
3872                 result = result._unstack_once(dim, fill_value)
3873         return result
3874 
3875     def update(self, other: "CoercibleMapping") -> "Dataset":
3876         """Update this dataset's variables with those from another dataset.
3877 
3878         Parameters
3879         ----------
3880         other : Dataset or mapping
3881             Variables with which to update this dataset. One of:
3882 
3883             - Dataset
3884             - mapping {var name: DataArray}
3885             - mapping {var name: Variable}
3886             - mapping {var name: (dimension name, array-like)}
3887             - mapping {var name: (tuple of dimension names, array-like)}
3888 
3889         Returns
3890         -------
3891         updated : Dataset
3892             Updated dataset.
3893 
3894         Raises
3895         ------
3896         ValueError
3897             If any dimensions would have inconsistent sizes in the updated
3898             dataset.
3899         """
3900         merge_result = dataset_update_method(self, other)
3901         return self._replace(inplace=True, **merge_result._asdict())
3902 
3903     def merge(
3904         self,
3905         other: Union["CoercibleMapping", "DataArray"],
3906         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3907         compat: str = "no_conflicts",
3908         join: str = "outer",
3909         fill_value: Any = dtypes.NA,
3910     ) -> "Dataset":
3911         """Merge the arrays of two datasets into a single dataset.
3912 
3913         This method generally does not allow for overriding data, with the
3914         exception of attributes, which are ignored on the second dataset.
3915         Variables with the same name are checked for conflicts via the equals
3916         or identical methods.
3917 
3918         Parameters
3919         ----------
3920         other : Dataset or mapping
3921             Dataset or variables to merge with this dataset.
3922         overwrite_vars : hashable or iterable of hashable, optional
3923             If provided, update variables of these name(s) without checking for
3924             conflicts in this dataset.
3925         compat : {"broadcast_equals", "equals", "identical", \
3926                   "no_conflicts"}, optional
3927             String indicating how to compare variables of the same name for
3928             potential conflicts:
3929 
3930             - 'broadcast_equals': all values must be equal when variables are
3931               broadcast against each other to ensure common dimensions.
3932             - 'equals': all values and dimensions must be the same.
3933             - 'identical': all values, dimensions and attributes must be the
3934               same.
3935             - 'no_conflicts': only values which are not null in both datasets
3936               must be equal. The returned dataset then contains the combination
3937               of all non-null values.
3938 
3939         join : {"outer", "inner", "left", "right", "exact"}, optional
3940             Method for joining ``self`` and ``other`` along shared dimensions:
3941 
3942             - 'outer': use the union of the indexes
3943             - 'inner': use the intersection of the indexes
3944             - 'left': use indexes from ``self``
3945             - 'right': use indexes from ``other``
3946             - 'exact': error instead of aligning non-equal indexes
3947         fill_value : scalar or dict-like, optional
3948             Value to use for newly missing values. If a dict-like, maps
3949             variable names (including coordinates) to fill values.
3950 
3951         Returns
3952         -------
3953         merged : Dataset
3954             Merged dataset.
3955 
3956         Raises
3957         ------
3958         MergeError
3959             If any variables conflict (see ``compat``).
3960         """
3961         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3962         merge_result = dataset_merge_method(
3963             self,
3964             other,
3965             overwrite_vars=overwrite_vars,
3966             compat=compat,
3967             join=join,
3968             fill_value=fill_value,
3969         )
3970         return self._replace(**merge_result._asdict())
3971 
3972     def _assert_all_in_dataset(
3973         self, names: Iterable[Hashable], virtual_okay: bool = False
3974     ) -> None:
3975         bad_names = set(names) - set(self._variables)
3976         if virtual_okay:
3977             bad_names -= self.virtual_variables
3978         if bad_names:
3979             raise ValueError(
3980                 "One or more of the specified variables "
3981                 "cannot be found in this dataset"
3982             )
3983 
3984     def drop_vars(
3985         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3986     ) -> "Dataset":
3987         """Drop variables from this dataset.
3988 
3989         Parameters
3990         ----------
3991         names : hashable or iterable of hashable
3992             Name(s) of variables to drop.
3993         errors : {"raise", "ignore"}, optional
3994             If 'raise' (default), raises a ValueError error if any of the variable
3995             passed are not in the dataset. If 'ignore', any given names that are in the
3996             dataset are dropped and no error is raised.
3997 
3998         Returns
3999         -------
4000         dropped : Dataset
4001 
4002         """
4003         # the Iterable check is required for mypy
4004         if is_scalar(names) or not isinstance(names, Iterable):
4005             names = {names}
4006         else:
4007             names = set(names)
4008         if errors == "raise":
4009             self._assert_all_in_dataset(names)
4010 
4011         variables = {k: v for k, v in self._variables.items() if k not in names}
4012         coord_names = {k for k in self._coord_names if k in variables}
4013         indexes = {k: v for k, v in self.indexes.items() if k not in names}
4014         return self._replace_with_new_dims(
4015             variables, coord_names=coord_names, indexes=indexes
4016         )
4017 
4018     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
4019         """Backward compatible method based on `drop_vars` and `drop_sel`
4020 
4021         Using either `drop_vars` or `drop_sel` is encouraged
4022 
4023         See Also
4024         --------
4025         Dataset.drop_vars
4026         Dataset.drop_sel
4027         """
4028         if errors not in ["raise", "ignore"]:
4029             raise ValueError('errors must be either "raise" or "ignore"')
4030 
4031         if is_dict_like(labels) and not isinstance(labels, dict):
4032             warnings.warn(
4033                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
4034                 FutureWarning,
4035                 stacklevel=2,
4036             )
4037             return self.drop_vars(labels, errors=errors)
4038 
4039         if labels_kwargs or isinstance(labels, dict):
4040             if dim is not None:
4041                 raise ValueError("cannot specify dim and dict-like arguments.")
4042             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
4043 
4044         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
4045             warnings.warn(
4046                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
4047                 PendingDeprecationWarning,
4048                 stacklevel=2,
4049             )
4050             return self.drop_vars(labels, errors=errors)
4051         if dim is not None:
4052             warnings.warn(
4053                 "dropping labels using list-like labels is deprecated; using "
4054                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
4055                 DeprecationWarning,
4056                 stacklevel=2,
4057             )
4058             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
4059 
4060         warnings.warn(
4061             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
4062             PendingDeprecationWarning,
4063             stacklevel=2,
4064         )
4065         return self.drop_sel(labels, errors=errors)
4066 
4067     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
4068         """Drop index labels from this dataset.
4069 
4070         Parameters
4071         ----------
4072         labels : mapping of hashable to Any
4073             Index labels to drop
4074         errors : {"raise", "ignore"}, optional
4075             If 'raise' (default), raises a ValueError error if
4076             any of the index labels passed are not
4077             in the dataset. If 'ignore', any given labels that are in the
4078             dataset are dropped and no error is raised.
4079         **labels_kwargs : {dim: label, ...}, optional
4080             The keyword arguments form of ``dim`` and ``labels``
4081 
4082         Returns
4083         -------
4084         dropped : Dataset
4085 
4086         Examples
4087         --------
4088         >>> data = np.arange(6).reshape(2, 3)
4089         >>> labels = ["a", "b", "c"]
4090         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4091         >>> ds
4092         <xarray.Dataset>
4093         Dimensions:  (x: 2, y: 3)
4094         Coordinates:
4095           * y        (y) <U1 'a' 'b' 'c'
4096         Dimensions without coordinates: x
4097         Data variables:
4098             A        (x, y) int64 0 1 2 3 4 5
4099         >>> ds.drop_sel(y=["a", "c"])
4100         <xarray.Dataset>
4101         Dimensions:  (x: 2, y: 1)
4102         Coordinates:
4103           * y        (y) <U1 'b'
4104         Dimensions without coordinates: x
4105         Data variables:
4106             A        (x, y) int64 1 4
4107         >>> ds.drop_sel(y="b")
4108         <xarray.Dataset>
4109         Dimensions:  (x: 2, y: 2)
4110         Coordinates:
4111           * y        (y) <U1 'a' 'c'
4112         Dimensions without coordinates: x
4113         Data variables:
4114             A        (x, y) int64 0 2 3 5
4115         """
4116         if errors not in ["raise", "ignore"]:
4117             raise ValueError('errors must be either "raise" or "ignore"')
4118 
4119         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
4120 
4121         ds = self
4122         for dim, labels_for_dim in labels.items():
4123             # Don't cast to set, as it would harm performance when labels
4124             # is a large numpy array
4125             if utils.is_scalar(labels_for_dim):
4126                 labels_for_dim = [labels_for_dim]
4127             labels_for_dim = np.asarray(labels_for_dim)
4128             try:
4129                 index = self.get_index(dim)
4130             except KeyError:
4131                 raise ValueError("dimension %r does not have coordinate labels" % dim)
4132             new_index = index.drop(labels_for_dim, errors=errors)
4133             ds = ds.loc[{dim: new_index}]
4134         return ds
4135 
4136     def drop_isel(self, indexers=None, **indexers_kwargs):
4137         """Drop index positions from this Dataset.
4138 
4139         Parameters
4140         ----------
4141         indexers : mapping of hashable to Any
4142             Index locations to drop
4143         **indexers_kwargs : {dim: position, ...}, optional
4144             The keyword arguments form of ``dim`` and ``positions``
4145 
4146         Returns
4147         -------
4148         dropped : Dataset
4149 
4150         Raises
4151         ------
4152         IndexError
4153 
4154         Examples
4155         --------
4156         >>> data = np.arange(6).reshape(2, 3)
4157         >>> labels = ["a", "b", "c"]
4158         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4159         >>> ds
4160         <xarray.Dataset>
4161         Dimensions:  (x: 2, y: 3)
4162         Coordinates:
4163           * y        (y) <U1 'a' 'b' 'c'
4164         Dimensions without coordinates: x
4165         Data variables:
4166             A        (x, y) int64 0 1 2 3 4 5
4167         >>> ds.drop_isel(y=[0, 2])
4168         <xarray.Dataset>
4169         Dimensions:  (x: 2, y: 1)
4170         Coordinates:
4171           * y        (y) <U1 'b'
4172         Dimensions without coordinates: x
4173         Data variables:
4174             A        (x, y) int64 1 4
4175         >>> ds.drop_isel(y=1)
4176         <xarray.Dataset>
4177         Dimensions:  (x: 2, y: 2)
4178         Coordinates:
4179           * y        (y) <U1 'a' 'c'
4180         Dimensions without coordinates: x
4181         Data variables:
4182             A        (x, y) int64 0 2 3 5
4183         """
4184 
4185         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
4186 
4187         ds = self
4188         dimension_index = {}
4189         for dim, pos_for_dim in indexers.items():
4190             # Don't cast to set, as it would harm performance when labels
4191             # is a large numpy array
4192             if utils.is_scalar(pos_for_dim):
4193                 pos_for_dim = [pos_for_dim]
4194             pos_for_dim = np.asarray(pos_for_dim)
4195             index = self.get_index(dim)
4196             new_index = index.delete(pos_for_dim)
4197             dimension_index[dim] = new_index
4198         ds = ds.loc[dimension_index]
4199         return ds
4200 
4201     def drop_dims(
4202         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
4203     ) -> "Dataset":
4204         """Drop dimensions and associated variables from this dataset.
4205 
4206         Parameters
4207         ----------
4208         drop_dims : hashable or iterable of hashable
4209             Dimension or dimensions to drop.
4210         errors : {"raise", "ignore"}, optional
4211             If 'raise' (default), raises a ValueError error if any of the
4212             dimensions passed are not in the dataset. If 'ignore', any given
4213             labels that are in the dataset are dropped and no error is raised.
4214 
4215         Returns
4216         -------
4217         obj : Dataset
4218             The dataset without the given dimensions (or any variables
4219             containing those dimensions)
4220         errors : {"raise", "ignore"}, optional
4221             If 'raise' (default), raises a ValueError error if
4222             any of the dimensions passed are not
4223             in the dataset. If 'ignore', any given dimensions that are in the
4224             dataset are dropped and no error is raised.
4225         """
4226         if errors not in ["raise", "ignore"]:
4227             raise ValueError('errors must be either "raise" or "ignore"')
4228 
4229         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
4230             drop_dims = {drop_dims}
4231         else:
4232             drop_dims = set(drop_dims)
4233 
4234         if errors == "raise":
4235             missing_dims = drop_dims - set(self.dims)
4236             if missing_dims:
4237                 raise ValueError(
4238                     "Dataset does not contain the dimensions: %s" % missing_dims
4239                 )
4240 
4241         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
4242         return self.drop_vars(drop_vars)
4243 
4244     def transpose(self, *dims: Hashable) -> "Dataset":
4245         """Return a new Dataset object with all array dimensions transposed.
4246 
4247         Although the order of dimensions on each array will change, the dataset
4248         dimensions themselves will remain in fixed (sorted) order.
4249 
4250         Parameters
4251         ----------
4252         *dims : hashable, optional
4253             By default, reverse the dimensions on each array. Otherwise,
4254             reorder the dimensions to this order.
4255 
4256         Returns
4257         -------
4258         transposed : Dataset
4259             Each array in the dataset (including) coordinates will be
4260             transposed to the given order.
4261 
4262         Notes
4263         -----
4264         This operation returns a view of each array's data. It is
4265         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
4266         -- the data will be fully loaded into memory.
4267 
4268         See Also
4269         --------
4270         numpy.transpose
4271         DataArray.transpose
4272         """
4273         if dims:
4274             if set(dims) ^ set(self.dims) and ... not in dims:
4275                 raise ValueError(
4276                     "arguments to transpose (%s) must be "
4277                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
4278                 )
4279         ds = self.copy()
4280         for name, var in self._variables.items():
4281             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
4282             ds._variables[name] = var.transpose(*var_dims)
4283         return ds
4284 
4285     def dropna(
4286         self,
4287         dim: Hashable,
4288         how: str = "any",
4289         thresh: int = None,
4290         subset: Iterable[Hashable] = None,
4291     ):
4292         """Returns a new dataset with dropped labels for missing values along
4293         the provided dimension.
4294 
4295         Parameters
4296         ----------
4297         dim : hashable
4298             Dimension along which to drop missing values. Dropping along
4299             multiple dimensions simultaneously is not yet supported.
4300         how : {"any", "all"}, default: "any"
4301             * any : if any NA values are present, drop that label
4302             * all : if all values are NA, drop that label
4303         thresh : int, default: None
4304             If supplied, require this many non-NA values.
4305         subset : iterable of hashable, optional
4306             Which variables to check for missing values. By default, all
4307             variables in the dataset are checked.
4308 
4309         Returns
4310         -------
4311         Dataset
4312         """
4313         # TODO: consider supporting multiple dimensions? Or not, given that
4314         # there are some ugly edge cases, e.g., pandas's dropna differs
4315         # depending on the order of the supplied axes.
4316 
4317         if dim not in self.dims:
4318             raise ValueError("%s must be a single dataset dimension" % dim)
4319 
4320         if subset is None:
4321             subset = iter(self.data_vars)
4322 
4323         count = np.zeros(self.dims[dim], dtype=np.int64)
4324         size = 0
4325 
4326         for k in subset:
4327             array = self._variables[k]
4328             if dim in array.dims:
4329                 dims = [d for d in array.dims if d != dim]
4330                 count += np.asarray(array.count(dims))  # type: ignore
4331                 size += np.prod([self.dims[d] for d in dims])
4332 
4333         if thresh is not None:
4334             mask = count >= thresh
4335         elif how == "any":
4336             mask = count == size
4337         elif how == "all":
4338             mask = count > 0
4339         elif how is not None:
4340             raise ValueError("invalid how option: %s" % how)
4341         else:
4342             raise TypeError("must specify how or thresh")
4343 
4344         return self.isel({dim: mask})
4345 
4346     def fillna(self, value: Any) -> "Dataset":
4347         """Fill missing values in this object.
4348 
4349         This operation follows the normal broadcasting and alignment rules that
4350         xarray uses for binary arithmetic, except the result is aligned to this
4351         object (``join='left'``) instead of aligned to the intersection of
4352         index coordinates (``join='inner'``).
4353 
4354         Parameters
4355         ----------
4356         value : scalar, ndarray, DataArray, dict or Dataset
4357             Used to fill all matching missing values in this dataset's data
4358             variables. Scalars, ndarrays or DataArrays arguments are used to
4359             fill all data with aligned coordinates (for DataArrays).
4360             Dictionaries or datasets match data variables and then align
4361             coordinates if necessary.
4362 
4363         Returns
4364         -------
4365         Dataset
4366 
4367         Examples
4368         --------
4369         >>> import numpy as np
4370         >>> import xarray as xr
4371         >>> ds = xr.Dataset(
4372         ...     {
4373         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
4374         ...         "B": ("x", [3, 4, np.nan, 1]),
4375         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
4376         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
4377         ...     },
4378         ...     coords={"x": [0, 1, 2, 3]},
4379         ... )
4380         >>> ds
4381         <xarray.Dataset>
4382         Dimensions:  (x: 4)
4383         Coordinates:
4384           * x        (x) int64 0 1 2 3
4385         Data variables:
4386             A        (x) float64 nan 2.0 nan 0.0
4387             B        (x) float64 3.0 4.0 nan 1.0
4388             C        (x) float64 nan nan nan 5.0
4389             D        (x) float64 nan 3.0 nan 4.0
4390 
4391         Replace all `NaN` values with 0s.
4392 
4393         >>> ds.fillna(0)
4394         <xarray.Dataset>
4395         Dimensions:  (x: 4)
4396         Coordinates:
4397           * x        (x) int64 0 1 2 3
4398         Data variables:
4399             A        (x) float64 0.0 2.0 0.0 0.0
4400             B        (x) float64 3.0 4.0 0.0 1.0
4401             C        (x) float64 0.0 0.0 0.0 5.0
4402             D        (x) float64 0.0 3.0 0.0 4.0
4403 
4404         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
4405 
4406         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4407         >>> ds.fillna(value=values)
4408         <xarray.Dataset>
4409         Dimensions:  (x: 4)
4410         Coordinates:
4411           * x        (x) int64 0 1 2 3
4412         Data variables:
4413             A        (x) float64 0.0 2.0 0.0 0.0
4414             B        (x) float64 3.0 4.0 1.0 1.0
4415             C        (x) float64 2.0 2.0 2.0 5.0
4416             D        (x) float64 3.0 3.0 3.0 4.0
4417         """
4418         if utils.is_dict_like(value):
4419             value_keys = getattr(value, "data_vars", value).keys()
4420             if not set(value_keys) <= set(self.data_vars.keys()):
4421                 raise ValueError(
4422                     "all variables in the argument to `fillna` "
4423                     "must be contained in the original dataset"
4424                 )
4425         out = ops.fillna(self, value)
4426         return out
4427 
4428     def interpolate_na(
4429         self,
4430         dim: Hashable = None,
4431         method: str = "linear",
4432         limit: int = None,
4433         use_coordinate: Union[bool, Hashable] = True,
4434         max_gap: Union[
4435             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4436         ] = None,
4437         **kwargs: Any,
4438     ) -> "Dataset":
4439         """Fill in NaNs by interpolating according to different methods.
4440 
4441         Parameters
4442         ----------
4443         dim : str
4444             Specifies the dimension along which to interpolate.
4445         method : str, optional
4446             String indicating which method to use for interpolation:
4447 
4448             - 'linear': linear interpolation (Default). Additional keyword
4449               arguments are passed to :py:func:`numpy.interp`
4450             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4451               are passed to :py:func:`scipy.interpolate.interp1d`. If
4452               ``method='polynomial'``, the ``order`` keyword argument must also be
4453               provided.
4454             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4455               respective :py:class:`scipy.interpolate` classes.
4456         use_coordinate : bool, str, default: True
4457             Specifies which index to use as the x values in the interpolation
4458             formulated as `y = f(x)`. If False, values are treated as if
4459             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4460             used. If ``use_coordinate`` is a string, it specifies the name of a
4461             coordinate variariable to use as the index.
4462         limit : int, default: None
4463             Maximum number of consecutive NaNs to fill. Must be greater than 0
4464             or None for no limit. This filling is done regardless of the size of
4465             the gap in the data. To only interpolate over gaps less than a given length,
4466             see ``max_gap``.
4467         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
4468             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4469             Use None for no limit. When interpolating along a datetime64 dimension
4470             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4471 
4472             - a string that is valid input for pandas.to_timedelta
4473             - a :py:class:`numpy.timedelta64` object
4474             - a :py:class:`pandas.Timedelta` object
4475             - a :py:class:`datetime.timedelta` object
4476 
4477             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4478             dimensions has not been implemented yet. Gap length is defined as the difference
4479             between coordinate values at the first data point after a gap and the last value
4480             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4481             between coordinate values at the first (last) valid data point and the first (last) NaN.
4482             For example, consider::
4483 
4484                 <xarray.DataArray (x: 9)>
4485                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4486                 Coordinates:
4487                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4488 
4489             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4490         kwargs : dict, optional
4491             parameters passed verbatim to the underlying interpolation function
4492 
4493         Returns
4494         -------
4495         interpolated: Dataset
4496             Filled in Dataset.
4497 
4498         See Also
4499         --------
4500         numpy.interp
4501         scipy.interpolate
4502 
4503         Examples
4504         --------
4505         >>> ds = xr.Dataset(
4506         ...     {
4507         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
4508         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
4509         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
4510         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
4511         ...     },
4512         ...     coords={"x": [0, 1, 2, 3, 4]},
4513         ... )
4514         >>> ds
4515         <xarray.Dataset>
4516         Dimensions:  (x: 5)
4517         Coordinates:
4518           * x        (x) int64 0 1 2 3 4
4519         Data variables:
4520             A        (x) float64 nan 2.0 3.0 nan 0.0
4521             B        (x) float64 3.0 4.0 nan 1.0 7.0
4522             C        (x) float64 nan nan nan 5.0 0.0
4523             D        (x) float64 nan 3.0 nan -1.0 4.0
4524 
4525         >>> ds.interpolate_na(dim="x", method="linear")
4526         <xarray.Dataset>
4527         Dimensions:  (x: 5)
4528         Coordinates:
4529           * x        (x) int64 0 1 2 3 4
4530         Data variables:
4531             A        (x) float64 nan 2.0 3.0 1.5 0.0
4532             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4533             C        (x) float64 nan nan nan 5.0 0.0
4534             D        (x) float64 nan 3.0 1.0 -1.0 4.0
4535 
4536         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
4537         <xarray.Dataset>
4538         Dimensions:  (x: 5)
4539         Coordinates:
4540           * x        (x) int64 0 1 2 3 4
4541         Data variables:
4542             A        (x) float64 1.0 2.0 3.0 1.5 0.0
4543             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4544             C        (x) float64 20.0 15.0 10.0 5.0 0.0
4545             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
4546         """
4547         from .missing import _apply_over_vars_with_dim, interp_na
4548 
4549         new = _apply_over_vars_with_dim(
4550             interp_na,
4551             self,
4552             dim=dim,
4553             method=method,
4554             limit=limit,
4555             use_coordinate=use_coordinate,
4556             max_gap=max_gap,
4557             **kwargs,
4558         )
4559         return new
4560 
4561     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4562         """Fill NaN values by propogating values forward
4563 
4564         *Requires bottleneck.*
4565 
4566         Parameters
4567         ----------
4568         dim : Hashable
4569             Specifies the dimension along which to propagate values when
4570             filling.
4571         limit : int, default: None
4572             The maximum number of consecutive NaN values to forward fill. In
4573             other words, if there is a gap with more than this number of
4574             consecutive NaNs, it will only be partially filled. Must be greater
4575             than 0 or None for no limit.
4576 
4577         Returns
4578         -------
4579         Dataset
4580         """
4581         from .missing import _apply_over_vars_with_dim, ffill
4582 
4583         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4584         return new
4585 
4586     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4587         """Fill NaN values by propogating values backward
4588 
4589         *Requires bottleneck.*
4590 
4591         Parameters
4592         ----------
4593         dim : str
4594             Specifies the dimension along which to propagate values when
4595             filling.
4596         limit : int, default: None
4597             The maximum number of consecutive NaN values to backward fill. In
4598             other words, if there is a gap with more than this number of
4599             consecutive NaNs, it will only be partially filled. Must be greater
4600             than 0 or None for no limit.
4601 
4602         Returns
4603         -------
4604         Dataset
4605         """
4606         from .missing import _apply_over_vars_with_dim, bfill
4607 
4608         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4609         return new
4610 
4611     def combine_first(self, other: "Dataset") -> "Dataset":
4612         """Combine two Datasets, default to data_vars of self.
4613 
4614         The new coordinates follow the normal broadcasting and alignment rules
4615         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4616         filled with np.nan.
4617 
4618         Parameters
4619         ----------
4620         other : Dataset
4621             Used to fill all matching missing values in this array.
4622 
4623         Returns
4624         -------
4625         Dataset
4626         """
4627         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4628         return out
4629 
4630     def reduce(
4631         self,
4632         func: Callable,
4633         dim: Union[Hashable, Iterable[Hashable]] = None,
4634         keep_attrs: bool = None,
4635         keepdims: bool = False,
4636         numeric_only: bool = False,
4637         **kwargs: Any,
4638     ) -> "Dataset":
4639         """Reduce this dataset by applying `func` along some dimension(s).
4640 
4641         Parameters
4642         ----------
4643         func : callable
4644             Function which can be called in the form
4645             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4646             np.ndarray over an integer valued axis.
4647         dim : str or sequence of str, optional
4648             Dimension(s) over which to apply `func`.  By default `func` is
4649             applied over all dimensions.
4650         keep_attrs : bool, optional
4651             If True, the dataset's attributes (`attrs`) will be copied from
4652             the original object to the new one.  If False (default), the new
4653             object will be returned without attributes.
4654         keepdims : bool, default: False
4655             If True, the dimensions which are reduced are left in the result
4656             as dimensions of size one. Coordinates that use these dimensions
4657             are removed.
4658         numeric_only : bool, optional
4659             If True, only apply ``func`` to variables with a numeric dtype.
4660         **kwargs : Any
4661             Additional keyword arguments passed on to ``func``.
4662 
4663         Returns
4664         -------
4665         reduced : Dataset
4666             Dataset with this object's DataArrays replaced with new DataArrays
4667             of summarized data and the indicated dimension(s) removed.
4668         """
4669         if dim is None or dim is ...:
4670             dims = set(self.dims)
4671         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4672             dims = {dim}
4673         else:
4674             dims = set(dim)
4675 
4676         missing_dimensions = [d for d in dims if d not in self.dims]
4677         if missing_dimensions:
4678             raise ValueError(
4679                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4680             )
4681 
4682         if keep_attrs is None:
4683             keep_attrs = _get_keep_attrs(default=False)
4684 
4685         variables: Dict[Hashable, Variable] = {}
4686         for name, var in self._variables.items():
4687             reduce_dims = [d for d in var.dims if d in dims]
4688             if name in self.coords:
4689                 if not reduce_dims:
4690                     variables[name] = var
4691             else:
4692                 if (
4693                     not numeric_only
4694                     or np.issubdtype(var.dtype, np.number)
4695                     or (var.dtype == np.bool_)
4696                 ):
4697                     if len(reduce_dims) == 1:
4698                         # unpack dimensions for the benefit of functions
4699                         # like np.argmin which can't handle tuple arguments
4700                         (reduce_dims,) = reduce_dims
4701                     elif len(reduce_dims) == var.ndim:
4702                         # prefer to aggregate over axis=None rather than
4703                         # axis=(0, 1) if they will be equivalent, because
4704                         # the former is often more efficient
4705                         reduce_dims = None  # type: ignore
4706                     variables[name] = var.reduce(
4707                         func,
4708                         dim=reduce_dims,
4709                         keep_attrs=keep_attrs,
4710                         keepdims=keepdims,
4711                         **kwargs,
4712                     )
4713 
4714         coord_names = {k for k in self.coords if k in variables}
4715         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4716         attrs = self.attrs if keep_attrs else None
4717         return self._replace_with_new_dims(
4718             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4719         )
4720 
4721     def map(
4722         self,
4723         func: Callable,
4724         keep_attrs: bool = None,
4725         args: Iterable[Any] = (),
4726         **kwargs: Any,
4727     ) -> "Dataset":
4728         """Apply a function to each variable in this dataset
4729 
4730         Parameters
4731         ----------
4732         func : callable
4733             Function which can be called in the form `func(x, *args, **kwargs)`
4734             to transform each DataArray `x` in this dataset into another
4735             DataArray.
4736         keep_attrs : bool, optional
4737             If True, the dataset's attributes (`attrs`) will be copied from
4738             the original object to the new one. If False, the new object will
4739             be returned without attributes.
4740         args : tuple, optional
4741             Positional arguments passed on to `func`.
4742         **kwargs : Any
4743             Keyword arguments passed on to `func`.
4744 
4745         Returns
4746         -------
4747         applied : Dataset
4748             Resulting dataset from applying ``func`` to each data variable.
4749 
4750         Examples
4751         --------
4752         >>> da = xr.DataArray(np.random.randn(2, 3))
4753         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4754         >>> ds
4755         <xarray.Dataset>
4756         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4757         Dimensions without coordinates: dim_0, dim_1, x
4758         Data variables:
4759             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
4760             bar      (x) int64 -1 2
4761         >>> ds.map(np.fabs)
4762         <xarray.Dataset>
4763         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4764         Dimensions without coordinates: dim_0, dim_1, x
4765         Data variables:
4766             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
4767             bar      (x) float64 1.0 2.0
4768         """
4769         if keep_attrs is None:
4770             keep_attrs = _get_keep_attrs(default=False)
4771         variables = {
4772             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4773             for k, v in self.data_vars.items()
4774         }
4775         if keep_attrs:
4776             for k, v in variables.items():
4777                 v._copy_attrs_from(self.data_vars[k])
4778         attrs = self.attrs if keep_attrs else None
4779         return type(self)(variables, attrs=attrs)
4780 
4781     def apply(
4782         self,
4783         func: Callable,
4784         keep_attrs: bool = None,
4785         args: Iterable[Any] = (),
4786         **kwargs: Any,
4787     ) -> "Dataset":
4788         """
4789         Backward compatible implementation of ``map``
4790 
4791         See Also
4792         --------
4793         Dataset.map
4794         """
4795         warnings.warn(
4796             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4797             PendingDeprecationWarning,
4798             stacklevel=2,
4799         )
4800         return self.map(func, keep_attrs, args, **kwargs)
4801 
4802     def assign(
4803         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4804     ) -> "Dataset":
4805         """Assign new data variables to a Dataset, returning a new object
4806         with all the original variables in addition to the new ones.
4807 
4808         Parameters
4809         ----------
4810         variables : mapping of hashable to Any
4811             Mapping from variables names to the new values. If the new values
4812             are callable, they are computed on the Dataset and assigned to new
4813             data variables. If the values are not callable, (e.g. a DataArray,
4814             scalar, or array), they are simply assigned.
4815         **variables_kwargs
4816             The keyword arguments form of ``variables``.
4817             One of variables or variables_kwargs must be provided.
4818 
4819         Returns
4820         -------
4821         ds : Dataset
4822             A new Dataset with the new variables in addition to all the
4823             existing variables.
4824 
4825         Notes
4826         -----
4827         Since ``kwargs`` is a dictionary, the order of your arguments may not
4828         be preserved, and so the order of the new variables is not well
4829         defined. Assigning multiple variables within the same ``assign`` is
4830         possible, but you cannot reference other variables created within the
4831         same ``assign`` call.
4832 
4833         See Also
4834         --------
4835         pandas.DataFrame.assign
4836 
4837         Examples
4838         --------
4839         >>> x = xr.Dataset(
4840         ...     {
4841         ...         "temperature_c": (
4842         ...             ("lat", "lon"),
4843         ...             20 * np.random.rand(4).reshape(2, 2),
4844         ...         ),
4845         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4846         ...     },
4847         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4848         ... )
4849         >>> x
4850         <xarray.Dataset>
4851         Dimensions:        (lat: 2, lon: 2)
4852         Coordinates:
4853           * lat            (lat) int64 10 20
4854           * lon            (lon) int64 150 160
4855         Data variables:
4856             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4857             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4858 
4859         Where the value is a callable, evaluated on dataset:
4860 
4861         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4862         <xarray.Dataset>
4863         Dimensions:        (lat: 2, lon: 2)
4864         Coordinates:
4865           * lat            (lat) int64 10 20
4866           * lon            (lon) int64 150 160
4867         Data variables:
4868             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4869             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4870             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4871 
4872         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4873 
4874         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4875         <xarray.Dataset>
4876         Dimensions:        (lat: 2, lon: 2)
4877         Coordinates:
4878           * lat            (lat) int64 10 20
4879           * lon            (lon) int64 150 160
4880         Data variables:
4881             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4882             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4883             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4884 
4885         """
4886         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4887         data = self.copy()
4888         # do all calculations first...
4889         results = data._calc_assign_results(variables)
4890         # ... and then assign
4891         data.update(results)
4892         return data
4893 
4894     def to_array(self, dim="variable", name=None):
4895         """Convert this dataset into an xarray.DataArray
4896 
4897         The data variables of this dataset will be broadcast against each other
4898         and stacked along the first axis of the new array. All coordinates of
4899         this dataset will remain coordinates.
4900 
4901         Parameters
4902         ----------
4903         dim : str, optional
4904             Name of the new dimension.
4905         name : str, optional
4906             Name of the new data array.
4907 
4908         Returns
4909         -------
4910         array : xarray.DataArray
4911         """
4912         from .dataarray import DataArray
4913 
4914         data_vars = [self.variables[k] for k in self.data_vars]
4915         broadcast_vars = broadcast_variables(*data_vars)
4916         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4917 
4918         coords = dict(self.coords)
4919         coords[dim] = list(self.data_vars)
4920         indexes = propagate_indexes(self._indexes)
4921 
4922         dims = (dim,) + broadcast_vars[0].dims
4923 
4924         return DataArray(
4925             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4926         )
4927 
4928     def _normalize_dim_order(
4929         self, dim_order: List[Hashable] = None
4930     ) -> Dict[Hashable, int]:
4931         """
4932         Check the validity of the provided dimensions if any and return the mapping
4933         between dimension name and their size.
4934 
4935         Parameters
4936         ----------
4937         dim_order
4938             Dimension order to validate (default to the alphabetical order if None).
4939 
4940         Returns
4941         -------
4942         result
4943             Validated dimensions mapping.
4944 
4945         """
4946         if dim_order is None:
4947             dim_order = list(self.dims)
4948         elif set(dim_order) != set(self.dims):
4949             raise ValueError(
4950                 "dim_order {} does not match the set of dimensions of this "
4951                 "Dataset: {}".format(dim_order, list(self.dims))
4952             )
4953 
4954         ordered_dims = {k: self.dims[k] for k in dim_order}
4955 
4956         return ordered_dims
4957 
4958     def _to_dataframe(self, ordered_dims: Mapping[Hashable, int]):
4959         columns = [k for k in self.variables if k not in self.dims]
4960         data = [
4961             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4962             for k in columns
4963         ]
4964         index = self.coords.to_index([*ordered_dims])
4965         return pd.DataFrame(dict(zip(columns, data)), index=index)
4966 
4967     def to_dataframe(self, dim_order: List[Hashable] = None) -> pd.DataFrame:
4968         """Convert this dataset into a pandas.DataFrame.
4969 
4970         Non-index variables in this dataset form the columns of the
4971         DataFrame. The DataFrame is indexed by the Cartesian product of
4972         this dataset's indices.
4973 
4974         Parameters
4975         ----------
4976         dim_order
4977             Hierarchical dimension order for the resulting dataframe. All
4978             arrays are transposed to this order and then written out as flat
4979             vectors in contiguous order, so the last dimension in this list
4980             will be contiguous in the resulting DataFrame. This has a major
4981             influence on which operations are efficient on the resulting
4982             dataframe.
4983 
4984             If provided, must include all dimensions of this dataset. By
4985             default, dimensions are sorted alphabetically.
4986 
4987         Returns
4988         -------
4989         result
4990             Dataset as a pandas DataFrame.
4991 
4992         """
4993 
4994         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
4995 
4996         return self._to_dataframe(ordered_dims=ordered_dims)
4997 
4998     def _set_sparse_data_from_dataframe(
4999         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
5000     ) -> None:
5001         from sparse import COO
5002 
5003         if isinstance(idx, pd.MultiIndex):
5004             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
5005             is_sorted = idx.is_lexsorted()
5006             shape = tuple(lev.size for lev in idx.levels)
5007         else:
5008             coords = np.arange(idx.size).reshape(1, -1)
5009             is_sorted = True
5010             shape = (idx.size,)
5011 
5012         for name, values in arrays:
5013             # In virtually all real use cases, the sparse array will now have
5014             # missing values and needs a fill_value. For consistency, don't
5015             # special case the rare exceptions (e.g., dtype=int without a
5016             # MultiIndex).
5017             dtype, fill_value = dtypes.maybe_promote(values.dtype)
5018             values = np.asarray(values, dtype=dtype)
5019 
5020             data = COO(
5021                 coords,
5022                 values,
5023                 shape,
5024                 has_duplicates=False,
5025                 sorted=is_sorted,
5026                 fill_value=fill_value,
5027             )
5028             self[name] = (dims, data)
5029 
5030     def _set_numpy_data_from_dataframe(
5031         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
5032     ) -> None:
5033         if not isinstance(idx, pd.MultiIndex):
5034             for name, values in arrays:
5035                 self[name] = (dims, values)
5036             return
5037 
5038         # NB: similar, more general logic, now exists in
5039         # variable.unstack_once; we could consider combining them at some
5040         # point.
5041 
5042         shape = tuple(lev.size for lev in idx.levels)
5043         indexer = tuple(idx.codes)
5044 
5045         # We already verified that the MultiIndex has all unique values, so
5046         # there are missing values if and only if the size of output arrays is
5047         # larger that the index.
5048         missing_values = np.prod(shape) > idx.shape[0]
5049 
5050         for name, values in arrays:
5051             # NumPy indexing is much faster than using DataFrame.reindex() to
5052             # fill in missing values:
5053             # https://stackoverflow.com/a/35049899/809705
5054             if missing_values:
5055                 dtype, fill_value = dtypes.maybe_promote(values.dtype)
5056                 data = np.full(shape, fill_value, dtype)
5057             else:
5058                 # If there are no missing values, keep the existing dtype
5059                 # instead of promoting to support NA, e.g., keep integer
5060                 # columns as integers.
5061                 # TODO: consider removing this special case, which doesn't
5062                 # exist for sparse=True.
5063                 data = np.zeros(shape, values.dtype)
5064             data[indexer] = values
5065             self[name] = (dims, data)
5066 
5067     @classmethod
5068     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
5069         """Convert a pandas.DataFrame into an xarray.Dataset
5070 
5071         Each column will be converted into an independent variable in the
5072         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
5073         into a tensor product of one-dimensional indices (filling in missing
5074         values with NaN). This method will produce a Dataset very similar to
5075         that on which the 'to_dataframe' method was called, except with
5076         possibly redundant dimensions (since all dataset variables will have
5077         the same dimensionality)
5078 
5079         Parameters
5080         ----------
5081         dataframe : DataFrame
5082             DataFrame from which to copy data and indices.
5083         sparse : bool, default: False
5084             If true, create a sparse arrays instead of dense numpy arrays. This
5085             can potentially save a large amount of memory if the DataFrame has
5086             a MultiIndex. Requires the sparse package (sparse.pydata.org).
5087 
5088         Returns
5089         -------
5090         New Dataset.
5091 
5092         See Also
5093         --------
5094         xarray.DataArray.from_series
5095         pandas.DataFrame.to_xarray
5096         """
5097         # TODO: Add an option to remove dimensions along which the variables
5098         # are constant, to enable consistent serialization to/from a dataframe,
5099         # even if some variables have different dimensionality.
5100 
5101         if not dataframe.columns.is_unique:
5102             raise ValueError("cannot convert DataFrame with non-unique columns")
5103 
5104         idx = remove_unused_levels_categories(dataframe.index)
5105 
5106         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
5107             raise ValueError(
5108                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
5109             )
5110 
5111         # Cast to a NumPy array first, in case the Series is a pandas Extension
5112         # array (which doesn't have a valid NumPy dtype)
5113         # TODO: allow users to control how this casting happens, e.g., by
5114         # forwarding arguments to pandas.Series.to_numpy?
5115         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
5116 
5117         obj = cls()
5118 
5119         if isinstance(idx, pd.MultiIndex):
5120             dims = tuple(
5121                 name if name is not None else "level_%i" % n
5122                 for n, name in enumerate(idx.names)
5123             )
5124             for dim, lev in zip(dims, idx.levels):
5125                 obj[dim] = (dim, lev)
5126         else:
5127             index_name = idx.name if idx.name is not None else "index"
5128             dims = (index_name,)
5129             obj[index_name] = (dims, idx)
5130 
5131         if sparse:
5132             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
5133         else:
5134             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
5135         return obj
5136 
5137     def to_dask_dataframe(self, dim_order=None, set_index=False):
5138         """
5139         Convert this dataset into a dask.dataframe.DataFrame.
5140 
5141         The dimensions, coordinates and data variables in this dataset form
5142         the columns of the DataFrame.
5143 
5144         Parameters
5145         ----------
5146         dim_order : list, optional
5147             Hierarchical dimension order for the resulting dataframe. All
5148             arrays are transposed to this order and then written out as flat
5149             vectors in contiguous order, so the last dimension in this list
5150             will be contiguous in the resulting DataFrame. This has a major
5151             influence on which operations are efficient on the resulting dask
5152             dataframe.
5153 
5154             If provided, must include all dimensions of this dataset. By
5155             default, dimensions are sorted alphabetically.
5156         set_index : bool, optional
5157             If set_index=True, the dask DataFrame is indexed by this dataset's
5158             coordinate. Since dask DataFrames do not support multi-indexes,
5159             set_index only works if the dataset only contains one dimension.
5160 
5161         Returns
5162         -------
5163         dask.dataframe.DataFrame
5164         """
5165 
5166         import dask.array as da
5167         import dask.dataframe as dd
5168 
5169         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
5170 
5171         columns = list(ordered_dims)
5172         columns.extend(k for k in self.coords if k not in self.dims)
5173         columns.extend(self.data_vars)
5174 
5175         series_list = []
5176         for name in columns:
5177             try:
5178                 var = self.variables[name]
5179             except KeyError:
5180                 # dimension without a matching coordinate
5181                 size = self.dims[name]
5182                 data = da.arange(size, chunks=size, dtype=np.int64)
5183                 var = Variable((name,), data)
5184 
5185             # IndexVariable objects have a dummy .chunk() method
5186             if isinstance(var, IndexVariable):
5187                 var = var.to_base_variable()
5188 
5189             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
5190             series = dd.from_array(dask_array.reshape(-1), columns=[name])
5191             series_list.append(series)
5192 
5193         df = dd.concat(series_list, axis=1)
5194 
5195         if set_index:
5196             dim_order = [*ordered_dims]
5197 
5198             if len(dim_order) == 1:
5199                 (dim,) = dim_order
5200                 df = df.set_index(dim)
5201             else:
5202                 # triggers an error about multi-indexes, even if only one
5203                 # dimension is passed
5204                 df = df.set_index(dim_order)
5205 
5206         return df
5207 
5208     def to_dict(self, data=True):
5209         """
5210         Convert this dataset to a dictionary following xarray naming
5211         conventions.
5212 
5213         Converts all variables and attributes to native Python objects
5214         Useful for converting to json. To avoid datetime incompatibility
5215         use decode_times=False kwarg in xarrray.open_dataset.
5216 
5217         Parameters
5218         ----------
5219         data : bool, optional
5220             Whether to include the actual data in the dictionary. When set to
5221             False, returns just the schema.
5222 
5223         See Also
5224         --------
5225         Dataset.from_dict
5226         """
5227         d = {
5228             "coords": {},
5229             "attrs": decode_numpy_dict_values(self.attrs),
5230             "dims": dict(self.dims),
5231             "data_vars": {},
5232         }
5233         for k in self.coords:
5234             d["coords"].update({k: self[k].variable.to_dict(data=data)})
5235         for k in self.data_vars:
5236             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
5237         return d
5238 
5239     @classmethod
5240     def from_dict(cls, d):
5241         """
5242         Convert a dictionary into an xarray.Dataset.
5243 
5244         Input dict can take several forms:
5245 
5246         .. code:: python
5247 
5248             d = {
5249                 "t": {"dims": ("t"), "data": t},
5250                 "a": {"dims": ("t"), "data": x},
5251                 "b": {"dims": ("t"), "data": y},
5252             }
5253 
5254             d = {
5255                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
5256                 "attrs": {"title": "air temperature"},
5257                 "dims": "t",
5258                 "data_vars": {
5259                     "a": {"dims": "t", "data": x},
5260                     "b": {"dims": "t", "data": y},
5261                 },
5262             }
5263 
5264         where "t" is the name of the dimesion, "a" and "b" are names of data
5265         variables and t, x, and y are lists, numpy.arrays or pandas objects.
5266 
5267         Parameters
5268         ----------
5269         d : dict-like
5270             Mapping with a minimum structure of
5271                 ``{"var_0": {"dims": [..], "data": [..]}, \
5272                             ...}``
5273 
5274         Returns
5275         -------
5276         obj : xarray.Dataset
5277 
5278         See also
5279         --------
5280         Dataset.to_dict
5281         DataArray.from_dict
5282         """
5283 
5284         if not {"coords", "data_vars"}.issubset(set(d)):
5285             variables = d.items()
5286         else:
5287             import itertools
5288 
5289             variables = itertools.chain(
5290                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
5291             )
5292         try:
5293             variable_dict = {
5294                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
5295             }
5296         except KeyError as e:
5297             raise ValueError(
5298                 "cannot convert dict without the key "
5299                 "'{dims_data}'".format(dims_data=str(e.args[0]))
5300             )
5301         obj = cls(variable_dict)
5302 
5303         # what if coords aren't dims?
5304         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
5305         obj = obj.set_coords(coords)
5306 
5307         obj.attrs.update(d.get("attrs", {}))
5308 
5309         return obj
5310 
5311     @staticmethod
5312     def _unary_op(f):
5313         @functools.wraps(f)
5314         def func(self, *args, **kwargs):
5315             variables = {}
5316             keep_attrs = kwargs.pop("keep_attrs", None)
5317             if keep_attrs is None:
5318                 keep_attrs = _get_keep_attrs(default=True)
5319             for k, v in self._variables.items():
5320                 if k in self._coord_names:
5321                     variables[k] = v
5322                 else:
5323                     variables[k] = f(v, *args, **kwargs)
5324                     if keep_attrs:
5325                         variables[k].attrs = v._attrs
5326             attrs = self._attrs if keep_attrs else None
5327             return self._replace_with_new_dims(variables, attrs=attrs)
5328 
5329         return func
5330 
5331     @staticmethod
5332     def _binary_op(f, reflexive=False, join=None):
5333         @functools.wraps(f)
5334         def func(self, other):
5335             from .dataarray import DataArray
5336 
5337             if isinstance(other, groupby.GroupBy):
5338                 return NotImplemented
5339             align_type = OPTIONS["arithmetic_join"] if join is None else join
5340             if isinstance(other, (DataArray, Dataset)):
5341                 self, other = align(self, other, join=align_type, copy=False)
5342             g = f if not reflexive else lambda x, y: f(y, x)
5343             ds = self._calculate_binary_op(g, other, join=align_type)
5344             return ds
5345 
5346         return func
5347 
5348     @staticmethod
5349     def _inplace_binary_op(f):
5350         @functools.wraps(f)
5351         def func(self, other):
5352             from .dataarray import DataArray
5353 
5354             if isinstance(other, groupby.GroupBy):
5355                 raise TypeError(
5356                     "in-place operations between a Dataset and "
5357                     "a grouped object are not permitted"
5358                 )
5359             # we don't actually modify arrays in-place with in-place Dataset
5360             # arithmetic -- this lets us automatically align things
5361             if isinstance(other, (DataArray, Dataset)):
5362                 other = other.reindex_like(self, copy=False)
5363             g = ops.inplace_to_noninplace_op(f)
5364             ds = self._calculate_binary_op(g, other, inplace=True)
5365             self._replace_with_new_dims(
5366                 ds._variables,
5367                 ds._coord_names,
5368                 attrs=ds._attrs,
5369                 indexes=ds._indexes,
5370                 inplace=True,
5371             )
5372             return self
5373 
5374         return func
5375 
5376     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
5377         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
5378             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
5379                 raise ValueError(
5380                     "datasets must have the same data variables "
5381                     "for in-place arithmetic operations: %s, %s"
5382                     % (list(lhs_data_vars), list(rhs_data_vars))
5383                 )
5384 
5385             dest_vars = {}
5386 
5387             for k in lhs_data_vars:
5388                 if k in rhs_data_vars:
5389                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
5390                 elif join in ["left", "outer"]:
5391                     dest_vars[k] = f(lhs_vars[k], np.nan)
5392             for k in rhs_data_vars:
5393                 if k not in dest_vars and join in ["right", "outer"]:
5394                     dest_vars[k] = f(rhs_vars[k], np.nan)
5395             return dest_vars
5396 
5397         if utils.is_dict_like(other) and not isinstance(other, Dataset):
5398             # can't use our shortcut of doing the binary operation with
5399             # Variable objects, so apply over our data vars instead.
5400             new_data_vars = apply_over_both(
5401                 self.data_vars, other, self.data_vars, other
5402             )
5403             return Dataset(new_data_vars)
5404 
5405         other_coords = getattr(other, "coords", None)
5406         ds = self.coords.merge(other_coords)
5407 
5408         if isinstance(other, Dataset):
5409             new_vars = apply_over_both(
5410                 self.data_vars, other.data_vars, self.variables, other.variables
5411             )
5412         else:
5413             other_variable = getattr(other, "variable", other)
5414             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
5415         ds._variables.update(new_vars)
5416         ds._dims = calculate_dimensions(ds._variables)
5417         return ds
5418 
5419     def _copy_attrs_from(self, other):
5420         self.attrs = other.attrs
5421         for v in other.variables:
5422             if v in self.variables:
5423                 self.variables[v].attrs = other.variables[v].attrs
5424 
5425     def diff(self, dim, n=1, label="upper"):
5426         """Calculate the n-th order discrete difference along given axis.
5427 
5428         Parameters
5429         ----------
5430         dim : str
5431             Dimension over which to calculate the finite difference.
5432         n : int, optional
5433             The number of times values are differenced.
5434         label : str, optional
5435             The new coordinate in dimension ``dim`` will have the
5436             values of either the minuend's or subtrahend's coordinate
5437             for values 'upper' and 'lower', respectively.  Other
5438             values are not supported.
5439 
5440         Returns
5441         -------
5442         difference : same type as caller
5443             The n-th order finite difference of this object.
5444         .. note::
5445             `n` matches numpy's behavior and is different from pandas' first
5446             argument named `periods`.
5447 
5448         Examples
5449         --------
5450         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
5451         >>> ds.diff("x")
5452         <xarray.Dataset>
5453         Dimensions:  (x: 3)
5454         Dimensions without coordinates: x
5455         Data variables:
5456             foo      (x) int64 0 1 0
5457         >>> ds.diff("x", 2)
5458         <xarray.Dataset>
5459         Dimensions:  (x: 2)
5460         Dimensions without coordinates: x
5461         Data variables:
5462             foo      (x) int64 1 -1
5463 
5464         See Also
5465         --------
5466         Dataset.differentiate
5467         """
5468         if n == 0:
5469             return self
5470         if n < 0:
5471             raise ValueError(f"order `n` must be non-negative but got {n}")
5472 
5473         # prepare slices
5474         kwargs_start = {dim: slice(None, -1)}
5475         kwargs_end = {dim: slice(1, None)}
5476 
5477         # prepare new coordinate
5478         if label == "upper":
5479             kwargs_new = kwargs_end
5480         elif label == "lower":
5481             kwargs_new = kwargs_start
5482         else:
5483             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
5484 
5485         variables = {}
5486 
5487         for name, var in self.variables.items():
5488             if dim in var.dims:
5489                 if name in self.data_vars:
5490                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
5491                 else:
5492                     variables[name] = var.isel(**kwargs_new)
5493             else:
5494                 variables[name] = var
5495 
5496         indexes = dict(self.indexes)
5497         if dim in indexes:
5498             indexes[dim] = indexes[dim][kwargs_new[dim]]
5499 
5500         difference = self._replace_with_new_dims(variables, indexes=indexes)
5501 
5502         if n > 1:
5503             return difference.diff(dim, n - 1)
5504         else:
5505             return difference
5506 
5507     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
5508         """Shift this dataset by an offset along one or more dimensions.
5509 
5510         Only data variables are moved; coordinates stay in place. This is
5511         consistent with the behavior of ``shift`` in pandas.
5512 
5513         Parameters
5514         ----------
5515         shifts : mapping of hashable to int
5516             Integer offset to shift along each of the given dimensions.
5517             Positive offsets shift to the right; negative offsets shift to the
5518             left.
5519         fill_value : scalar or dict-like, optional
5520             Value to use for newly missing values. If a dict-like, maps
5521             variable names (including coordinates) to fill values.
5522         **shifts_kwargs
5523             The keyword arguments form of ``shifts``.
5524             One of shifts or shifts_kwargs must be provided.
5525 
5526         Returns
5527         -------
5528         shifted : Dataset
5529             Dataset with the same coordinates and attributes but shifted data
5530             variables.
5531 
5532         See Also
5533         --------
5534         roll
5535 
5536         Examples
5537         --------
5538         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5539         >>> ds.shift(x=2)
5540         <xarray.Dataset>
5541         Dimensions:  (x: 5)
5542         Dimensions without coordinates: x
5543         Data variables:
5544             foo      (x) object nan nan 'a' 'b' 'c'
5545         """
5546         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5547         invalid = [k for k in shifts if k not in self.dims]
5548         if invalid:
5549             raise ValueError("dimensions %r do not exist" % invalid)
5550 
5551         variables = {}
5552         for name, var in self.variables.items():
5553             if name in self.data_vars:
5554                 fill_value_ = (
5555                     fill_value.get(name, dtypes.NA)
5556                     if isinstance(fill_value, dict)
5557                     else fill_value
5558                 )
5559 
5560                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5561                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
5562             else:
5563                 variables[name] = var
5564 
5565         return self._replace(variables)
5566 
5567     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5568         """Roll this dataset by an offset along one or more dimensions.
5569 
5570         Unlike shift, roll may rotate all variables, including coordinates
5571         if specified. The direction of rotation is consistent with
5572         :py:func:`numpy.roll`.
5573 
5574         Parameters
5575         ----------
5576         shifts : dict, optional
5577             A dict with keys matching dimensions and values given
5578             by integers to rotate each of the given dimensions. Positive
5579             offsets roll to the right; negative offsets roll to the left.
5580         roll_coords : bool
5581             Indicates whether to  roll the coordinates by the offset
5582             The current default of roll_coords (None, equivalent to True) is
5583             deprecated and will change to False in a future version.
5584             Explicitly pass roll_coords to silence the warning.
5585         **shifts_kwargs : {dim: offset, ...}, optional
5586             The keyword arguments form of ``shifts``.
5587             One of shifts or shifts_kwargs must be provided.
5588         Returns
5589         -------
5590         rolled : Dataset
5591             Dataset with the same coordinates and attributes but rolled
5592             variables.
5593 
5594         See Also
5595         --------
5596         shift
5597 
5598         Examples
5599         --------
5600         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5601         >>> ds.roll(x=2)
5602         <xarray.Dataset>
5603         Dimensions:  (x: 5)
5604         Dimensions without coordinates: x
5605         Data variables:
5606             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
5607         """
5608         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5609         invalid = [k for k in shifts if k not in self.dims]
5610         if invalid:
5611             raise ValueError("dimensions %r do not exist" % invalid)
5612 
5613         if roll_coords is None:
5614             warnings.warn(
5615                 "roll_coords will be set to False in the future."
5616                 " Explicitly set roll_coords to silence warning.",
5617                 FutureWarning,
5618                 stacklevel=2,
5619             )
5620             roll_coords = True
5621 
5622         unrolled_vars = () if roll_coords else self.coords
5623 
5624         variables = {}
5625         for k, v in self.variables.items():
5626             if k not in unrolled_vars:
5627                 variables[k] = v.roll(
5628                     **{k: s for k, s in shifts.items() if k in v.dims}
5629                 )
5630             else:
5631                 variables[k] = v
5632 
5633         if roll_coords:
5634             indexes = {}
5635             for k, v in self.indexes.items():
5636                 (dim,) = self.variables[k].dims
5637                 if dim in shifts:
5638                     indexes[k] = roll_index(v, shifts[dim])
5639                 else:
5640                     indexes[k] = v
5641         else:
5642             indexes = dict(self.indexes)
5643 
5644         return self._replace(variables, indexes=indexes)
5645 
5646     def sortby(self, variables, ascending=True):
5647         """
5648         Sort object by labels or values (along an axis).
5649 
5650         Sorts the dataset, either along specified dimensions,
5651         or according to values of 1-D dataarrays that share dimension
5652         with calling object.
5653 
5654         If the input variables are dataarrays, then the dataarrays are aligned
5655         (via left-join) to the calling object prior to sorting by cell values.
5656         NaNs are sorted to the end, following Numpy convention.
5657 
5658         If multiple sorts along the same dimension is
5659         given, numpy's lexsort is performed along that dimension:
5660         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5661         and the FIRST key in the sequence is used as the primary sort key,
5662         followed by the 2nd key, etc.
5663 
5664         Parameters
5665         ----------
5666         variables : str, DataArray, or list of str or DataArray
5667             1D DataArray objects or name(s) of 1D variable(s) in
5668             coords/data_vars whose values are used to sort the dataset.
5669         ascending : bool, optional
5670             Whether to sort by ascending or descending order.
5671 
5672         Returns
5673         -------
5674         sorted : Dataset
5675             A new dataset where all the specified dims are sorted by dim
5676             labels.
5677         """
5678         from .dataarray import DataArray
5679 
5680         if not isinstance(variables, list):
5681             variables = [variables]
5682         else:
5683             variables = variables
5684         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5685         aligned_vars = align(self, *variables, join="left")
5686         aligned_self = aligned_vars[0]
5687         aligned_other_vars = aligned_vars[1:]
5688         vars_by_dim = defaultdict(list)
5689         for data_array in aligned_other_vars:
5690             if data_array.ndim != 1:
5691                 raise ValueError("Input DataArray is not 1-D.")
5692             (key,) = data_array.dims
5693             vars_by_dim[key].append(data_array)
5694 
5695         indices = {}
5696         for key, arrays in vars_by_dim.items():
5697             order = np.lexsort(tuple(reversed(arrays)))
5698             indices[key] = order if ascending else order[::-1]
5699         return aligned_self.isel(**indices)
5700 
5701     def quantile(
5702         self,
5703         q,
5704         dim=None,
5705         interpolation="linear",
5706         numeric_only=False,
5707         keep_attrs=None,
5708         skipna=True,
5709     ):
5710         """Compute the qth quantile of the data along the specified dimension.
5711 
5712         Returns the qth quantiles(s) of the array elements for each variable
5713         in the Dataset.
5714 
5715         Parameters
5716         ----------
5717         q : float or array-like of float
5718             Quantile to compute, which must be between 0 and 1 inclusive.
5719         dim : str or sequence of str, optional
5720             Dimension(s) over which to apply quantile.
5721         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
5722             This optional parameter specifies the interpolation method to
5723             use when the desired quantile lies between two data points
5724             ``i < j``:
5725 
5726                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5727                   the fractional part of the index surrounded by ``i`` and
5728                   ``j``.
5729                 * lower: ``i``.
5730                 * higher: ``j``.
5731                 * nearest: ``i`` or ``j``, whichever is nearest.
5732                 * midpoint: ``(i + j) / 2``.
5733         keep_attrs : bool, optional
5734             If True, the dataset's attributes (`attrs`) will be copied from
5735             the original object to the new one.  If False (default), the new
5736             object will be returned without attributes.
5737         numeric_only : bool, optional
5738             If True, only apply ``func`` to variables with a numeric dtype.
5739         skipna : bool, optional
5740             Whether to skip missing values when aggregating.
5741 
5742         Returns
5743         -------
5744         quantiles : Dataset
5745             If `q` is a single quantile, then the result is a scalar for each
5746             variable in data_vars. If multiple percentiles are given, first
5747             axis of the result corresponds to the quantile and a quantile
5748             dimension is added to the return Dataset. The other dimensions are
5749             the dimensions that remain after the reduction of the array.
5750 
5751         See Also
5752         --------
5753         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5754 
5755         Examples
5756         --------
5757         >>> ds = xr.Dataset(
5758         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5759         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5760         ... )
5761         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5762         <xarray.Dataset>
5763         Dimensions:   ()
5764         Coordinates:
5765             quantile  float64 0.0
5766         Data variables:
5767             a         float64 0.7
5768         >>> ds.quantile(0, dim="x")
5769         <xarray.Dataset>
5770         Dimensions:   (y: 4)
5771         Coordinates:
5772           * y         (y) float64 1.0 1.5 2.0 2.5
5773             quantile  float64 0.0
5774         Data variables:
5775             a         (y) float64 0.7 4.2 2.6 1.5
5776         >>> ds.quantile([0, 0.5, 1])
5777         <xarray.Dataset>
5778         Dimensions:   (quantile: 3)
5779         Coordinates:
5780           * quantile  (quantile) float64 0.0 0.5 1.0
5781         Data variables:
5782             a         (quantile) float64 0.7 3.4 9.4
5783         >>> ds.quantile([0, 0.5, 1], dim="x")
5784         <xarray.Dataset>
5785         Dimensions:   (quantile: 3, y: 4)
5786         Coordinates:
5787           * y         (y) float64 1.0 1.5 2.0 2.5
5788           * quantile  (quantile) float64 0.0 0.5 1.0
5789         Data variables:
5790             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5791         """
5792 
5793         if isinstance(dim, str):
5794             dims = {dim}
5795         elif dim in [None, ...]:
5796             dims = set(self.dims)
5797         else:
5798             dims = set(dim)
5799 
5800         _assert_empty(
5801             [d for d in dims if d not in self.dims],
5802             "Dataset does not contain the dimensions: %s",
5803         )
5804 
5805         q = np.asarray(q, dtype=np.float64)
5806 
5807         variables = {}
5808         for name, var in self.variables.items():
5809             reduce_dims = [d for d in var.dims if d in dims]
5810             if reduce_dims or not var.dims:
5811                 if name not in self.coords:
5812                     if (
5813                         not numeric_only
5814                         or np.issubdtype(var.dtype, np.number)
5815                         or var.dtype == np.bool_
5816                     ):
5817                         if len(reduce_dims) == var.ndim:
5818                             # prefer to aggregate over axis=None rather than
5819                             # axis=(0, 1) if they will be equivalent, because
5820                             # the former is often more efficient
5821                             reduce_dims = None
5822                         variables[name] = var.quantile(
5823                             q,
5824                             dim=reduce_dims,
5825                             interpolation=interpolation,
5826                             keep_attrs=keep_attrs,
5827                             skipna=skipna,
5828                         )
5829 
5830             else:
5831                 variables[name] = var
5832 
5833         # construct the new dataset
5834         coord_names = {k for k in self.coords if k in variables}
5835         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5836         if keep_attrs is None:
5837             keep_attrs = _get_keep_attrs(default=False)
5838         attrs = self.attrs if keep_attrs else None
5839         new = self._replace_with_new_dims(
5840             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5841         )
5842         return new.assign_coords(quantile=q)
5843 
5844     def rank(self, dim, pct=False, keep_attrs=None):
5845         """Ranks the data.
5846 
5847         Equal values are assigned a rank that is the average of the ranks that
5848         would have been otherwise assigned to all of the values within
5849         that set.
5850         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5851 
5852         NaNs in the input array are returned as NaNs.
5853 
5854         The `bottleneck` library is required.
5855 
5856         Parameters
5857         ----------
5858         dim : str
5859             Dimension over which to compute rank.
5860         pct : bool, optional
5861             If True, compute percentage ranks, otherwise compute integer ranks.
5862         keep_attrs : bool, optional
5863             If True, the dataset's attributes (`attrs`) will be copied from
5864             the original object to the new one.  If False (default), the new
5865             object will be returned without attributes.
5866 
5867         Returns
5868         -------
5869         ranked : Dataset
5870             Variables that do not depend on `dim` are dropped.
5871         """
5872         if dim not in self.dims:
5873             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5874 
5875         variables = {}
5876         for name, var in self.variables.items():
5877             if name in self.data_vars:
5878                 if dim in var.dims:
5879                     variables[name] = var.rank(dim, pct=pct)
5880             else:
5881                 variables[name] = var
5882 
5883         coord_names = set(self.coords)
5884         if keep_attrs is None:
5885             keep_attrs = _get_keep_attrs(default=False)
5886         attrs = self.attrs if keep_attrs else None
5887         return self._replace(variables, coord_names, attrs=attrs)
5888 
5889     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5890         """ Differentiate with the second order accurate central
5891         differences.
5892 
5893         .. note::
5894             This feature is limited to simple cartesian geometry, i.e. coord
5895             must be one dimensional.
5896 
5897         Parameters
5898         ----------
5899         coord : str
5900             The coordinate to be used to compute the gradient.
5901         edge_order : {1, 2}, default: 1
5902             N-th order accurate differences at the boundaries.
5903         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5904             "us", "ns", "ps", "fs", "as"}, default: None
5905             Unit to compute gradient. Only valid for datetime coordinate.
5906 
5907         Returns
5908         -------
5909         differentiated: Dataset
5910 
5911         See also
5912         --------
5913         numpy.gradient: corresponding numpy function
5914         """
5915         from .variable import Variable
5916 
5917         if coord not in self.variables and coord not in self.dims:
5918             raise ValueError(f"Coordinate {coord} does not exist.")
5919 
5920         coord_var = self[coord].variable
5921         if coord_var.ndim != 1:
5922             raise ValueError(
5923                 "Coordinate {} must be 1 dimensional but is {}"
5924                 " dimensional".format(coord, coord_var.ndim)
5925             )
5926 
5927         dim = coord_var.dims[0]
5928         if _contains_datetime_like_objects(coord_var):
5929             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5930                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5931             elif datetime_unit is None:
5932                 datetime_unit = "s"  # Default to seconds for cftime objects
5933             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5934 
5935         variables = {}
5936         for k, v in self.variables.items():
5937             if k in self.data_vars and dim in v.dims and k not in self.coords:
5938                 if _contains_datetime_like_objects(v):
5939                     v = v._to_numeric(datetime_unit=datetime_unit)
5940                 grad = duck_array_ops.gradient(
5941                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5942                 )
5943                 variables[k] = Variable(v.dims, grad)
5944             else:
5945                 variables[k] = v
5946         return self._replace(variables)
5947 
5948     def integrate(
5949         self, coord: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None
5950     ) -> "Dataset":
5951         """Integrate along the given coordinate using the trapezoidal rule.
5952 
5953         .. note::
5954             This feature is limited to simple cartesian geometry, i.e. coord
5955             must be one dimensional.
5956 
5957         Parameters
5958         ----------
5959         coord : hashable, or sequence of hashable
5960             Coordinate(s) used for the integration.
5961         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5962                         'ps', 'fs', 'as'}, optional
5963             Specify the unit if datetime coordinate is used.
5964 
5965         Returns
5966         -------
5967         integrated : Dataset
5968 
5969         See also
5970         --------
5971         DataArray.integrate
5972         numpy.trapz : corresponding numpy function
5973 
5974         Examples
5975         --------
5976         >>> ds = xr.Dataset(
5977         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5978         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5979         ... )
5980         >>> ds
5981         <xarray.Dataset>
5982         Dimensions:  (x: 4)
5983         Coordinates:
5984           * x        (x) int64 0 1 2 3
5985             y        (x) int64 1 7 3 5
5986         Data variables:
5987             a        (x) int64 5 5 6 6
5988             b        (x) int64 1 2 1 0
5989         >>> ds.integrate("x")
5990         <xarray.Dataset>
5991         Dimensions:  ()
5992         Data variables:
5993             a        float64 16.5
5994             b        float64 3.5
5995         >>> ds.integrate("y")
5996         <xarray.Dataset>
5997         Dimensions:  ()
5998         Data variables:
5999             a        float64 20.0
6000             b        float64 4.0
6001         """
6002         if not isinstance(coord, (list, tuple)):
6003             coord = (coord,)
6004         result = self
6005         for c in coord:
6006             result = result._integrate_one(c, datetime_unit=datetime_unit)
6007         return result
6008 
6009     def _integrate_one(self, coord, datetime_unit=None):
6010         from .variable import Variable
6011 
6012         if coord not in self.variables and coord not in self.dims:
6013             raise ValueError(f"Coordinate {coord} does not exist.")
6014 
6015         coord_var = self[coord].variable
6016         if coord_var.ndim != 1:
6017             raise ValueError(
6018                 "Coordinate {} must be 1 dimensional but is {}"
6019                 " dimensional".format(coord, coord_var.ndim)
6020             )
6021 
6022         dim = coord_var.dims[0]
6023         if _contains_datetime_like_objects(coord_var):
6024             if coord_var.dtype.kind in "mM" and datetime_unit is None:
6025                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
6026             elif datetime_unit is None:
6027                 datetime_unit = "s"  # Default to seconds for cftime objects
6028             coord_var = coord_var._replace(
6029                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
6030             )
6031 
6032         variables = {}
6033         coord_names = set()
6034         for k, v in self.variables.items():
6035             if k in self.coords:
6036                 if dim not in v.dims:
6037                     variables[k] = v
6038                     coord_names.add(k)
6039             else:
6040                 if k in self.data_vars and dim in v.dims:
6041                     if _contains_datetime_like_objects(v):
6042                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
6043                     integ = duck_array_ops.trapz(
6044                         v.data, coord_var.data, axis=v.get_axis_num(dim)
6045                     )
6046                     v_dims = list(v.dims)
6047                     v_dims.remove(dim)
6048                     variables[k] = Variable(v_dims, integ)
6049                 else:
6050                     variables[k] = v
6051         indexes = {k: v for k, v in self.indexes.items() if k in variables}
6052         return self._replace_with_new_dims(
6053             variables, coord_names=coord_names, indexes=indexes
6054         )
6055 
6056     @property
6057     def real(self):
6058         return self.map(lambda x: x.real, keep_attrs=True)
6059 
6060     @property
6061     def imag(self):
6062         return self.map(lambda x: x.imag, keep_attrs=True)
6063 
6064     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
6065 
6066     def filter_by_attrs(self, **kwargs):
6067         """Returns a ``Dataset`` with variables that match specific conditions.
6068 
6069         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
6070         containing only the variables for which all the filter tests pass.
6071         These tests are either ``key=value`` for which the attribute ``key``
6072         has the exact value ``value`` or the callable passed into
6073         ``key=callable`` returns True. The callable will be passed a single
6074         value, either the value of the attribute ``key`` or ``None`` if the
6075         DataArray does not have an attribute with the name ``key``.
6076 
6077         Parameters
6078         ----------
6079         **kwargs
6080             key : str
6081                 Attribute name.
6082             value : callable or obj
6083                 If value is a callable, it should return a boolean in the form
6084                 of bool = func(attr) where attr is da.attrs[key].
6085                 Otherwise, value will be compared to the each
6086                 DataArray's attrs[key].
6087 
6088         Returns
6089         -------
6090         new : Dataset
6091             New dataset with variables filtered by attribute.
6092 
6093         Examples
6094         --------
6095         >>> # Create an example dataset:
6096         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
6097         >>> precip = 10 * np.random.rand(2, 2, 3)
6098         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
6099         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
6100         >>> dims = ["x", "y", "time"]
6101         >>> temp_attr = dict(standard_name="air_potential_temperature")
6102         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
6103         >>> ds = xr.Dataset(
6104         ...     {
6105         ...         "temperature": (dims, temp, temp_attr),
6106         ...         "precipitation": (dims, precip, precip_attr),
6107         ...     },
6108         ...     coords={
6109         ...         "lon": (["x", "y"], lon),
6110         ...         "lat": (["x", "y"], lat),
6111         ...         "time": pd.date_range("2014-09-06", periods=3),
6112         ...         "reference_time": pd.Timestamp("2014-09-05"),
6113         ...     },
6114         ... )
6115         >>> # Get variables matching a specific standard_name.
6116         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
6117         <xarray.Dataset>
6118         Dimensions:         (time: 3, x: 2, y: 2)
6119         Coordinates:
6120             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
6121             lat             (x, y) float64 42.25 42.21 42.63 42.59
6122           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
6123             reference_time  datetime64[ns] 2014-09-05
6124         Dimensions without coordinates: x, y
6125         Data variables:
6126             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
6127         >>> # Get all variables that have a standard_name attribute.
6128         >>> standard_name = lambda v: v is not None
6129         >>> ds.filter_by_attrs(standard_name=standard_name)
6130         <xarray.Dataset>
6131         Dimensions:         (time: 3, x: 2, y: 2)
6132         Coordinates:
6133             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
6134             lat             (x, y) float64 42.25 42.21 42.63 42.59
6135           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
6136             reference_time  datetime64[ns] 2014-09-05
6137         Dimensions without coordinates: x, y
6138         Data variables:
6139             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
6140             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
6141 
6142         """
6143         selection = []
6144         for var_name, variable in self.variables.items():
6145             has_value_flag = False
6146             for attr_name, pattern in kwargs.items():
6147                 attr_value = variable.attrs.get(attr_name)
6148                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
6149                     has_value_flag = True
6150                 else:
6151                     has_value_flag = False
6152                     break
6153             if has_value_flag is True:
6154                 selection.append(var_name)
6155         return self[selection]
6156 
6157     def unify_chunks(self) -> "Dataset":
6158         """Unify chunk size along all chunked dimensions of this Dataset.
6159 
6160         Returns
6161         -------
6162         Dataset with consistent chunk sizes for all dask-array variables
6163 
6164         See Also
6165         --------
6166         dask.array.core.unify_chunks
6167         """
6168 
6169         try:
6170             self.chunks
6171         except ValueError:  # "inconsistent chunks"
6172             pass
6173         else:
6174             # No variables with dask backend, or all chunks are already aligned
6175             return self.copy()
6176 
6177         # import dask is placed after the quick exit test above to allow
6178         # running this method if dask isn't installed and there are no chunks
6179         import dask.array
6180 
6181         ds = self.copy()
6182 
6183         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
6184 
6185         dask_array_names = []
6186         dask_unify_args = []
6187         for name, variable in ds.variables.items():
6188             if isinstance(variable.data, dask.array.Array):
6189                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
6190                 dask_array_names.append(name)
6191                 dask_unify_args.append(variable.data)
6192                 dask_unify_args.append(dims_tuple)
6193 
6194         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
6195 
6196         for name, new_array in zip(dask_array_names, rechunked_arrays):
6197             ds.variables[name]._data = new_array
6198 
6199         return ds
6200 
6201     def map_blocks(
6202         self,
6203         func: "Callable[..., T_DSorDA]",
6204         args: Sequence[Any] = (),
6205         kwargs: Mapping[str, Any] = None,
6206         template: Union["DataArray", "Dataset"] = None,
6207     ) -> "T_DSorDA":
6208         """
6209         Apply a function to each block of this Dataset.
6210 
6211         .. warning::
6212             This method is experimental and its signature may change.
6213 
6214         Parameters
6215         ----------
6216         func : callable
6217             User-provided function that accepts a Dataset as its first
6218             parameter. The function will receive a subset or 'block' of this Dataset (see below),
6219             corresponding to one chunk along each chunked dimension. ``func`` will be
6220             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
6221 
6222             This function must return either a single DataArray or a single Dataset.
6223 
6224             This function cannot add a new chunked dimension.
6225         args : sequence
6226             Passed to func after unpacking and subsetting any xarray objects by blocks.
6227             xarray objects in args must be aligned with obj, otherwise an error is raised.
6228         kwargs : mapping
6229             Passed verbatim to func after unpacking. xarray objects, if any, will not be
6230             subset to blocks. Passing dask collections in kwargs is not allowed.
6231         template : DataArray or Dataset, optional
6232             xarray object representing the final result after compute is called. If not provided,
6233             the function will be first run on mocked-up data, that looks like this object but
6234             has sizes 0, to determine properties of the returned object such as dtype,
6235             variable names, attributes, new dimensions and new indexes (if any).
6236             ``template`` must be provided if the function changes the size of existing dimensions.
6237             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
6238             ``attrs`` set by ``func`` will be ignored.
6239 
6240         Returns
6241         -------
6242         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
6243         function.
6244 
6245         Notes
6246         -----
6247         This function is designed for when ``func`` needs to manipulate a whole xarray object
6248         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
6249         recommended to use ``apply_ufunc``.
6250 
6251         If none of the variables in this object is backed by dask arrays, calling this function is
6252         equivalent to calling ``func(obj, *args, **kwargs)``.
6253 
6254         See Also
6255         --------
6256         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
6257         xarray.DataArray.map_blocks
6258 
6259         Examples
6260         --------
6261         Calculate an anomaly from climatology using ``.groupby()``. Using
6262         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
6263         its indices, and its methods like ``.groupby()``.
6264 
6265         >>> def calculate_anomaly(da, groupby_type="time.month"):
6266         ...     gb = da.groupby(groupby_type)
6267         ...     clim = gb.mean(dim="time")
6268         ...     return gb - clim
6269         ...
6270         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
6271         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
6272         >>> np.random.seed(123)
6273         >>> array = xr.DataArray(
6274         ...     np.random.rand(len(time)),
6275         ...     dims=["time"],
6276         ...     coords={"time": time, "month": month},
6277         ... ).chunk()
6278         >>> ds = xr.Dataset({"a": array})
6279         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
6280         <xarray.Dataset>
6281         Dimensions:  (time: 24)
6282         Coordinates:
6283           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6284             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
6285         Data variables:
6286             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
6287 
6288         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
6289         to the function being applied in ``xr.map_blocks()``:
6290 
6291         >>> ds.map_blocks(
6292         ...     calculate_anomaly,
6293         ...     kwargs={"groupby_type": "time.year"},
6294         ...     template=ds,
6295         ... )
6296         <xarray.Dataset>
6297         Dimensions:  (time: 24)
6298         Coordinates:
6299           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6300             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
6301         Data variables:
6302             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
6303         """
6304         from .parallel import map_blocks
6305 
6306         return map_blocks(func, self, args, kwargs, template)
6307 
6308     def polyfit(
6309         self,
6310         dim: Hashable,
6311         deg: int,
6312         skipna: bool = None,
6313         rcond: float = None,
6314         w: Union[Hashable, Any] = None,
6315         full: bool = False,
6316         cov: Union[bool, str] = False,
6317     ):
6318         """
6319         Least squares polynomial fit.
6320 
6321         This replicates the behaviour of `numpy.polyfit` but differs by skipping
6322         invalid values when `skipna = True`.
6323 
6324         Parameters
6325         ----------
6326         dim : hashable
6327             Coordinate along which to fit the polynomials.
6328         deg : int
6329             Degree of the fitting polynomial.
6330         skipna : bool, optional
6331             If True, removes all invalid values before fitting each 1D slices of the array.
6332             Default is True if data is stored in a dask.array or if there is any
6333             invalid values, False otherwise.
6334         rcond : float, optional
6335             Relative condition number to the fit.
6336         w : hashable or Any, optional
6337             Weights to apply to the y-coordinate of the sample points.
6338             Can be an array-like object or the name of a coordinate in the dataset.
6339         full : bool, optional
6340             Whether to return the residuals, matrix rank and singular values in addition
6341             to the coefficients.
6342         cov : bool or str, optional
6343             Whether to return to the covariance matrix in addition to the coefficients.
6344             The matrix is not scaled if `cov='unscaled'`.
6345 
6346         Returns
6347         -------
6348         polyfit_results : Dataset
6349             A single dataset which contains (for each "var" in the input dataset):
6350 
6351             [var]_polyfit_coefficients
6352                 The coefficients of the best fit for each variable in this dataset.
6353             [var]_polyfit_residuals
6354                 The residuals of the least-square computation for each variable (only included if `full=True`)
6355                 When the matrix rank is deficient, np.nan is returned.
6356             [dim]_matrix_rank
6357                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6358                 The rank is computed ignoring the NaN values that might be skipped.
6359             [dim]_singular_values
6360                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6361             [var]_polyfit_covariance
6362                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
6363 
6364         Warns
6365         -----
6366         RankWarning
6367             The rank of the coefficient matrix in the least-squares fit is deficient.
6368             The warning is not raised with in-memory (not dask) data and `full=True`.
6369 
6370         See Also
6371         --------
6372         numpy.polyfit
6373         """
6374         variables = {}
6375         skipna_da = skipna
6376 
6377         x = get_clean_interp_index(self, dim, strict=False)
6378         xname = "{}_".format(self[dim].name)
6379         order = int(deg) + 1
6380         lhs = np.vander(x, order)
6381 
6382         if rcond is None:
6383             rcond = x.shape[0] * np.core.finfo(x.dtype).eps
6384 
6385         # Weights:
6386         if w is not None:
6387             if isinstance(w, Hashable):
6388                 w = self.coords[w]
6389             w = np.asarray(w)
6390             if w.ndim != 1:
6391                 raise TypeError("Expected a 1-d array for weights.")
6392             if w.shape[0] != lhs.shape[0]:
6393                 raise TypeError("Expected w and {} to have the same length".format(dim))
6394             lhs *= w[:, np.newaxis]
6395 
6396         # Scaling
6397         scale = np.sqrt((lhs * lhs).sum(axis=0))
6398         lhs /= scale
6399 
6400         degree_dim = utils.get_temp_dimname(self.dims, "degree")
6401 
6402         rank = np.linalg.matrix_rank(lhs)
6403 
6404         if full:
6405             rank = xr.DataArray(rank, name=xname + "matrix_rank")
6406             variables[rank.name] = rank
6407             sing = np.linalg.svd(lhs, compute_uv=False)
6408             sing = xr.DataArray(
6409                 sing,
6410                 dims=(degree_dim,),
6411                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
6412                 name=xname + "singular_values",
6413             )
6414             variables[sing.name] = sing
6415 
6416         for name, da in self.data_vars.items():
6417             if dim not in da.dims:
6418                 continue
6419 
6420             if is_duck_dask_array(da.data) and (
6421                 rank != order or full or skipna is None
6422             ):
6423                 # Current algorithm with dask and skipna=False neither supports
6424                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
6425                 skipna_da = True
6426             elif skipna is None:
6427                 skipna_da = np.any(da.isnull())
6428 
6429             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
6430             stacked_coords: Dict[Hashable, DataArray] = {}
6431             if dims_to_stack:
6432                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
6433                 rhs = da.transpose(dim, *dims_to_stack).stack(
6434                     {stacked_dim: dims_to_stack}
6435                 )
6436                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
6437                 scale_da = scale[:, np.newaxis]
6438             else:
6439                 rhs = da
6440                 scale_da = scale
6441 
6442             if w is not None:
6443                 rhs *= w[:, np.newaxis]
6444 
6445             with warnings.catch_warnings():
6446                 if full:  # Copy np.polyfit behavior
6447                     warnings.simplefilter("ignore", np.RankWarning)
6448                 else:  # Raise only once per variable
6449                     warnings.simplefilter("once", np.RankWarning)
6450 
6451                 coeffs, residuals = duck_array_ops.least_squares(
6452                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
6453                 )
6454 
6455             if isinstance(name, str):
6456                 name = "{}_".format(name)
6457             else:
6458                 # Thus a ReprObject => polyfit was called on a DataArray
6459                 name = ""
6460 
6461             coeffs = xr.DataArray(
6462                 coeffs / scale_da,
6463                 dims=[degree_dim] + list(stacked_coords.keys()),
6464                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
6465                 name=name + "polyfit_coefficients",
6466             )
6467             if dims_to_stack:
6468                 coeffs = coeffs.unstack(stacked_dim)
6469             variables[coeffs.name] = coeffs
6470 
6471             if full or (cov is True):
6472                 residuals = xr.DataArray(
6473                     residuals if dims_to_stack else residuals.squeeze(),
6474                     dims=list(stacked_coords.keys()),
6475                     coords=stacked_coords,
6476                     name=name + "polyfit_residuals",
6477                 )
6478                 if dims_to_stack:
6479                     residuals = residuals.unstack(stacked_dim)
6480                 variables[residuals.name] = residuals
6481 
6482             if cov:
6483                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
6484                 Vbase /= np.outer(scale, scale)
6485                 if cov == "unscaled":
6486                     fac = 1
6487                 else:
6488                     if x.shape[0] <= order:
6489                         raise ValueError(
6490                             "The number of data points must exceed order to scale the covariance matrix."
6491                         )
6492                     fac = residuals / (x.shape[0] - order)
6493                 covariance = xr.DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
6494                 variables[name + "polyfit_covariance"] = covariance
6495 
6496         return Dataset(data_vars=variables, attrs=self.attrs.copy())
6497 
6498     def pad(
6499         self,
6500         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
6501         mode: str = "constant",
6502         stat_length: Union[
6503             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6504         ] = None,
6505         constant_values: Union[
6506             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6507         ] = None,
6508         end_values: Union[
6509             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6510         ] = None,
6511         reflect_type: str = None,
6512         **pad_width_kwargs: Any,
6513     ) -> "Dataset":
6514         """Pad this dataset along one or more dimensions.
6515 
6516         .. warning::
6517             This function is experimental and its behaviour is likely to change
6518             especially regarding padding of dimension coordinates (or IndexVariables).
6519 
6520         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
6521         coordinates will be padded with the same mode, otherwise coordinates
6522         are padded using the "constant" mode with fill_value dtypes.NA.
6523 
6524         Parameters
6525         ----------
6526         pad_width : mapping of hashable to tuple of int
6527             Mapping with the form of {dim: (pad_before, pad_after)}
6528             describing the number of values padded along each dimension.
6529             {dim: pad} is a shortcut for pad_before = pad_after = pad
6530         mode : str, default: "constant"
6531             One of the following string values (taken from numpy docs).
6532 
6533             'constant' (default)
6534                 Pads with a constant value.
6535             'edge'
6536                 Pads with the edge values of array.
6537             'linear_ramp'
6538                 Pads with the linear ramp between end_value and the
6539                 array edge value.
6540             'maximum'
6541                 Pads with the maximum value of all or part of the
6542                 vector along each axis.
6543             'mean'
6544                 Pads with the mean value of all or part of the
6545                 vector along each axis.
6546             'median'
6547                 Pads with the median value of all or part of the
6548                 vector along each axis.
6549             'minimum'
6550                 Pads with the minimum value of all or part of the
6551                 vector along each axis.
6552             'reflect'
6553                 Pads with the reflection of the vector mirrored on
6554                 the first and last values of the vector along each
6555                 axis.
6556             'symmetric'
6557                 Pads with the reflection of the vector mirrored
6558                 along the edge of the array.
6559             'wrap'
6560                 Pads with the wrap of the vector along the axis.
6561                 The first values are used to pad the end and the
6562                 end values are used to pad the beginning.
6563         stat_length : int, tuple or mapping of hashable to tuple, default: None
6564             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
6565             values at edge of each axis used to calculate the statistic value.
6566             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
6567             statistic lengths along each dimension.
6568             ((before, after),) yields same before and after statistic lengths
6569             for each dimension.
6570             (stat_length,) or int is a shortcut for before = after = statistic
6571             length for all axes.
6572             Default is ``None``, to use the entire axis.
6573         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
6574             Used in 'constant'.  The values to set the padded values for each
6575             axis.
6576             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6577             pad constants along each dimension.
6578             ``((before, after),)`` yields same before and after constants for each
6579             dimension.
6580             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6581             all dimensions.
6582             Default is 0.
6583         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
6584             Used in 'linear_ramp'.  The values used for the ending value of the
6585             linear_ramp and that will form the edge of the padded array.
6586             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6587             end values along each dimension.
6588             ``((before, after),)`` yields same before and after end values for each
6589             axis.
6590             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6591             all axes.
6592             Default is 0.
6593         reflect_type : {"even", "odd"}, optional
6594             Used in "reflect", and "symmetric".  The "even" style is the
6595             default with an unaltered reflection around the edge value.  For
6596             the "odd" style, the extended part of the array is created by
6597             subtracting the reflected values from two times the edge value.
6598         **pad_width_kwargs
6599             The keyword arguments form of ``pad_width``.
6600             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
6601 
6602         Returns
6603         -------
6604         padded : Dataset
6605             Dataset with the padded coordinates and data.
6606 
6607         See Also
6608         --------
6609         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
6610 
6611         Notes
6612         -----
6613         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
6614         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
6615         specify ``constant_values=np.nan``
6616 
6617         Examples
6618         --------
6619         >>> ds = xr.Dataset({"foo": ("x", range(5))})
6620         >>> ds.pad(x=(1, 2))
6621         <xarray.Dataset>
6622         Dimensions:  (x: 8)
6623         Dimensions without coordinates: x
6624         Data variables:
6625             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
6626         """
6627         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
6628 
6629         if mode in ("edge", "reflect", "symmetric", "wrap"):
6630             coord_pad_mode = mode
6631             coord_pad_options = {
6632                 "stat_length": stat_length,
6633                 "constant_values": constant_values,
6634                 "end_values": end_values,
6635                 "reflect_type": reflect_type,
6636             }
6637         else:
6638             coord_pad_mode = "constant"
6639             coord_pad_options = {}
6640 
6641         variables = {}
6642         for name, var in self.variables.items():
6643             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
6644             if not var_pad_width:
6645                 variables[name] = var
6646             elif name in self.data_vars:
6647                 variables[name] = var.pad(
6648                     pad_width=var_pad_width,
6649                     mode=mode,
6650                     stat_length=stat_length,
6651                     constant_values=constant_values,
6652                     end_values=end_values,
6653                     reflect_type=reflect_type,
6654                 )
6655             else:
6656                 variables[name] = var.pad(
6657                     pad_width=var_pad_width,
6658                     mode=coord_pad_mode,
6659                     **coord_pad_options,  # type: ignore
6660                 )
6661 
6662         return self._replace_vars_and_dims(variables)
6663 
6664     def idxmin(
6665         self,
6666         dim: Hashable = None,
6667         skipna: bool = None,
6668         fill_value: Any = dtypes.NA,
6669         keep_attrs: bool = None,
6670     ) -> "Dataset":
6671         """Return the coordinate label of the minimum value along a dimension.
6672 
6673         Returns a new `Dataset` named after the dimension with the values of
6674         the coordinate labels along that dimension corresponding to minimum
6675         values along that dimension.
6676 
6677         In comparison to :py:meth:`~Dataset.argmin`, this returns the
6678         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
6679 
6680         Parameters
6681         ----------
6682         dim : str, optional
6683             Dimension over which to apply `idxmin`.  This is optional for 1D
6684             variables, but required for variables with 2 or more dimensions.
6685         skipna : bool or None, default: None
6686             If True, skip missing values (as marked by NaN). By default, only
6687             skips missing values for ``float``, ``complex``, and ``object``
6688             dtypes; other dtypes either do not have a sentinel missing value
6689             (``int``) or ``skipna=True`` has not been implemented
6690             (``datetime64`` or ``timedelta64``).
6691         fill_value : Any, default: NaN
6692             Value to be filled in case all of the values along a dimension are
6693             null.  By default this is NaN.  The fill value and result are
6694             automatically converted to a compatible dtype if possible.
6695             Ignored if ``skipna`` is False.
6696         keep_attrs : bool, default: False
6697             If True, the attributes (``attrs``) will be copied from the
6698             original object to the new one.  If False (default), the new object
6699             will be returned without attributes.
6700 
6701         Returns
6702         -------
6703         reduced : Dataset
6704             New `Dataset` object with `idxmin` applied to its data and the
6705             indicated dimension removed.
6706 
6707         See Also
6708         --------
6709         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
6710 
6711         Examples
6712         --------
6713         >>> array1 = xr.DataArray(
6714         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6715         ... )
6716         >>> array2 = xr.DataArray(
6717         ...     [
6718         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6719         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6720         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6721         ...     ],
6722         ...     dims=["y", "x"],
6723         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6724         ... )
6725         >>> ds = xr.Dataset({"int": array1, "float": array2})
6726         >>> ds.min(dim="x")
6727         <xarray.Dataset>
6728         Dimensions:  (y: 3)
6729         Coordinates:
6730           * y        (y) int64 -1 0 1
6731         Data variables:
6732             int      int64 -2
6733             float    (y) float64 -2.0 -4.0 1.0
6734         >>> ds.argmin(dim="x")
6735         <xarray.Dataset>
6736         Dimensions:  (y: 3)
6737         Coordinates:
6738           * y        (y) int64 -1 0 1
6739         Data variables:
6740             int      int64 4
6741             float    (y) int64 4 0 2
6742         >>> ds.idxmin(dim="x")
6743         <xarray.Dataset>
6744         Dimensions:  (y: 3)
6745         Coordinates:
6746           * y        (y) int64 -1 0 1
6747         Data variables:
6748             int      <U1 'e'
6749             float    (y) object 'e' 'a' 'c'
6750         """
6751         return self.map(
6752             methodcaller(
6753                 "idxmin",
6754                 dim=dim,
6755                 skipna=skipna,
6756                 fill_value=fill_value,
6757                 keep_attrs=keep_attrs,
6758             )
6759         )
6760 
6761     def idxmax(
6762         self,
6763         dim: Hashable = None,
6764         skipna: bool = None,
6765         fill_value: Any = dtypes.NA,
6766         keep_attrs: bool = None,
6767     ) -> "Dataset":
6768         """Return the coordinate label of the maximum value along a dimension.
6769 
6770         Returns a new `Dataset` named after the dimension with the values of
6771         the coordinate labels along that dimension corresponding to maximum
6772         values along that dimension.
6773 
6774         In comparison to :py:meth:`~Dataset.argmax`, this returns the
6775         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
6776 
6777         Parameters
6778         ----------
6779         dim : str, optional
6780             Dimension over which to apply `idxmax`.  This is optional for 1D
6781             variables, but required for variables with 2 or more dimensions.
6782         skipna : bool or None, default: None
6783             If True, skip missing values (as marked by NaN). By default, only
6784             skips missing values for ``float``, ``complex``, and ``object``
6785             dtypes; other dtypes either do not have a sentinel missing value
6786             (``int``) or ``skipna=True`` has not been implemented
6787             (``datetime64`` or ``timedelta64``).
6788         fill_value : Any, default: NaN
6789             Value to be filled in case all of the values along a dimension are
6790             null.  By default this is NaN.  The fill value and result are
6791             automatically converted to a compatible dtype if possible.
6792             Ignored if ``skipna`` is False.
6793         keep_attrs : bool, default: False
6794             If True, the attributes (``attrs``) will be copied from the
6795             original object to the new one.  If False (default), the new object
6796             will be returned without attributes.
6797 
6798         Returns
6799         -------
6800         reduced : Dataset
6801             New `Dataset` object with `idxmax` applied to its data and the
6802             indicated dimension removed.
6803 
6804         See Also
6805         --------
6806         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
6807 
6808         Examples
6809         --------
6810         >>> array1 = xr.DataArray(
6811         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6812         ... )
6813         >>> array2 = xr.DataArray(
6814         ...     [
6815         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6816         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6817         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6818         ...     ],
6819         ...     dims=["y", "x"],
6820         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6821         ... )
6822         >>> ds = xr.Dataset({"int": array1, "float": array2})
6823         >>> ds.max(dim="x")
6824         <xarray.Dataset>
6825         Dimensions:  (y: 3)
6826         Coordinates:
6827           * y        (y) int64 -1 0 1
6828         Data variables:
6829             int      int64 2
6830             float    (y) float64 2.0 2.0 1.0
6831         >>> ds.argmax(dim="x")
6832         <xarray.Dataset>
6833         Dimensions:  (y: 3)
6834         Coordinates:
6835           * y        (y) int64 -1 0 1
6836         Data variables:
6837             int      int64 1
6838             float    (y) int64 0 2 2
6839         >>> ds.idxmax(dim="x")
6840         <xarray.Dataset>
6841         Dimensions:  (y: 3)
6842         Coordinates:
6843           * y        (y) int64 -1 0 1
6844         Data variables:
6845             int      <U1 'b'
6846             float    (y) object 'a' 'c' 'c'
6847         """
6848         return self.map(
6849             methodcaller(
6850                 "idxmax",
6851                 dim=dim,
6852                 skipna=skipna,
6853                 fill_value=fill_value,
6854                 keep_attrs=keep_attrs,
6855             )
6856         )
6857 
6858     def argmin(self, dim=None, axis=None, **kwargs):
6859         """Indices of the minima of the member variables.
6860 
6861         If there are multiple minima, the indices of the first one found will be
6862         returned.
6863 
6864         Parameters
6865         ----------
6866         dim : str, optional
6867             The dimension over which to find the minimum. By default, finds minimum over
6868             all dimensions - for now returning an int for backward compatibility, but
6869             this is deprecated, in future will be an error, since DataArray.argmin will
6870             return a dict with indices for all dimensions, which does not make sense for
6871             a Dataset.
6872         axis : int, optional
6873             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
6874             can be supplied.
6875         keep_attrs : bool, optional
6876             If True, the attributes (`attrs`) will be copied from the original
6877             object to the new one.  If False (default), the new object will be
6878             returned without attributes.
6879         skipna : bool, optional
6880             If True, skip missing values (as marked by NaN). By default, only
6881             skips missing values for float dtypes; other dtypes either do not
6882             have a sentinel missing value (int) or skipna=True has not been
6883             implemented (object, datetime64 or timedelta64).
6884 
6885         Returns
6886         -------
6887         result : Dataset
6888 
6889         See Also
6890         --------
6891         DataArray.argmin
6892 
6893         """
6894         if dim is None and axis is None:
6895             warnings.warn(
6896                 "Once the behaviour of DataArray.argmin() and Variable.argmin() with "
6897                 "neither dim nor axis argument changes to return a dict of indices of "
6898                 "each dimension, for consistency it will be an error to call "
6899                 "Dataset.argmin() with no argument, since we don't return a dict of "
6900                 "Datasets.",
6901                 DeprecationWarning,
6902                 stacklevel=2,
6903             )
6904         if (
6905             dim is None
6906             or axis is not None
6907             or (not isinstance(dim, Sequence) and dim is not ...)
6908             or isinstance(dim, str)
6909         ):
6910             # Return int index if single dimension is passed, and is not part of a
6911             # sequence
6912             argmin_func = getattr(duck_array_ops, "argmin")
6913             return self.reduce(argmin_func, dim=dim, axis=axis, **kwargs)
6914         else:
6915             raise ValueError(
6916                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6917                 "dicts cannot be contained in a Dataset, so cannot call "
6918                 "Dataset.argmin() with a sequence or ... for dim"
6919             )
6920 
6921     def argmax(self, dim=None, axis=None, **kwargs):
6922         """Indices of the maxima of the member variables.
6923 
6924         If there are multiple maxima, the indices of the first one found will be
6925         returned.
6926 
6927         Parameters
6928         ----------
6929         dim : str, optional
6930             The dimension over which to find the maximum. By default, finds maximum over
6931             all dimensions - for now returning an int for backward compatibility, but
6932             this is deprecated, in future will be an error, since DataArray.argmax will
6933             return a dict with indices for all dimensions, which does not make sense for
6934             a Dataset.
6935         axis : int, optional
6936             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
6937             can be supplied.
6938         keep_attrs : bool, optional
6939             If True, the attributes (`attrs`) will be copied from the original
6940             object to the new one.  If False (default), the new object will be
6941             returned without attributes.
6942         skipna : bool, optional
6943             If True, skip missing values (as marked by NaN). By default, only
6944             skips missing values for float dtypes; other dtypes either do not
6945             have a sentinel missing value (int) or skipna=True has not been
6946             implemented (object, datetime64 or timedelta64).
6947 
6948         Returns
6949         -------
6950         result : Dataset
6951 
6952         See Also
6953         --------
6954         DataArray.argmax
6955 
6956         """
6957         if dim is None and axis is None:
6958             warnings.warn(
6959                 "Once the behaviour of DataArray.argmax() and Variable.argmax() with "
6960                 "neither dim nor axis argument changes to return a dict of indices of "
6961                 "each dimension, for consistency it will be an error to call "
6962                 "Dataset.argmax() with no argument, since we don't return a dict of "
6963                 "Datasets.",
6964                 DeprecationWarning,
6965                 stacklevel=2,
6966             )
6967         if (
6968             dim is None
6969             or axis is not None
6970             or (not isinstance(dim, Sequence) and dim is not ...)
6971             or isinstance(dim, str)
6972         ):
6973             # Return int index if single dimension is passed, and is not part of a
6974             # sequence
6975             argmax_func = getattr(duck_array_ops, "argmax")
6976             return self.reduce(argmax_func, dim=dim, axis=axis, **kwargs)
6977         else:
6978             raise ValueError(
6979                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6980                 "dicts cannot be contained in a Dataset, so cannot call "
6981                 "Dataset.argmin() with a sequence or ... for dim"
6982             )
6983 
6984 
6985 ops.inject_all_ops_and_reduce_methods(Dataset, array_only=False)

</code><intended_behavior>Assuming the merge() method in this file calls into the core merge logic and handles dataset-level merging specifics, it might need to be updated to correctly pass through the new combine_attrs option and to handle the expected behavior of dropping conflicting attributes when combine_attrs="drop_conflicts" is specified. This ensures that dataset-level merge operations respect the new option, seamlessly integrating it into higher-level APIs. The existing functionality for handling other combine_attrs options must remain unaffected.</intended_behavior>]
2025-10-15 21:49:58.741 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 21:49:58.741 | INFO     | Search-only: skipping patch selection
2025-10-15 21:49:58.819 | INFO     | Task pydata__xarray-4827 completed successfully.
2025-10-15 21:49:58.820 | INFO     | No patch generated. You can try running ACR again.
