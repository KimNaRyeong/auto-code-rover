2025-10-15 22:04:43.552 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 22:04:45.696 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1495, output_tokens=12, cost=0.015310
2025-10-15 22:04:59.585 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1675, output_tokens=539, cost=0.032920
2025-10-15 22:05:00.834 | INFO     | registered non reproducing test 0
2025-10-15 22:05:15.874 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2972, output_tokens=625, cost=0.048470
2025-10-15 22:05:17.077 | INFO     | registered non reproducing test 1
2025-10-15 22:05:31.738 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3061, output_tokens=677, cost=0.050920
2025-10-15 22:05:32.940 | INFO     | registered non reproducing test 2
2025-10-15 22:05:32.940 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 22:05:32.943 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 22:05:43.166 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1766, output_tokens=393, cost=0.029450
2025-10-15 22:05:43.173 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 22:05:45.316 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=747, output_tokens=55, cost=0.009120
2025-10-15 22:05:45.318 | DEBUG    | Extracted a valid json.
2025-10-15 22:05:45.318 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class(\"assign_coords\", \"Dataset\")",
        "search_method_in_class(\"__len__\", \"Dataset\")",
        "search_code(\"_coord_names\")"
    ],
    "bug_locations": []
}
2025-10-15 22:05:45.440 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 22:05:45.441 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 22:05:59.629 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2539, output_tokens=633, cost=0.044380
2025-10-15 22:05:59.636 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 22:06:01.246 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3388, output_tokens=40, cost=0.035080
2025-10-15 22:06:01.249 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 22:06:03.549 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=394, output_tokens=51, cost=0.005470
2025-10-15 22:06:03.550 | DEBUG    | Extracted a valid json.
2025-10-15 22:06:03.550 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class(\"assign_coords\", \"DataArray\")",
        "search_code_in_file(\"_coord_names\", \"xarray/core/dataset.py\")"
    ],
    "bug_locations": []
}
2025-10-15 22:06:03.653 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 22:06:03.653 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 22:06:24.773 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4615, output_tokens=728, cost=0.067990
2025-10-15 22:06:24.785 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 22:06:37.061 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5559, output_tokens=497, cost=0.070500
2025-10-15 22:06:37.072 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 22:06:47.224 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=851, output_tokens=381, cost=0.019940
2025-10-15 22:06:47.225 | DEBUG    | Extracted a valid json.
2025-10-15 22:06:47.230 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "_replace",
            "intended_behavior": "This method should ensure when coordinates are reassigned (especially with operations that would lead to change in '_coord_names'), '_coord_names' should be accurately updated to reflect the current state of coordinates. After assigning new coordinates, a validation check should ensure that all elements in '_coord_names' accurately represent current available coordinates. If any discrepancies are found, '_coord_names' should be corrected to ensure consistency."
        },
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "set_index and reset_index",
            "intended_behavior": "For both 'set_index' and 'reset_index', there should be clear, additional logic to verify and update '_coord_names' appropriately post-operation. It involves adding or removing coordinate names based on how indices are restructured. If a coordinate is promoted to an index or vice versa, '_coord_names' should be updated accordingly to maintain accuracy."
        },
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "_stack_once",
            "intended_behavior": "Given that stacking can significantly alter the dataset's structure by combining multiple coordinates into a single new one, this method should include a final step where '_coord_names' is revisited. Any coordinates that have been combined into a new multiindex should have their names removed from '_coord_names', and the new coordinate name should be added. This would prevent the inconsistency between the physical structure of the dataset and its coordinate metadata, ensuring operations relying on '_coord_names' (like calculating dataset length) remain accurate."
        }
    ]
}
2025-10-15 22:06:47.296 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>_replace</method>
<code>
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj

</code><intended_behavior>This method should ensure when coordinates are reassigned (especially with operations that would lead to change in '_coord_names'), '_coord_names' should be accurately updated to reflect the current state of coordinates. After assigning new coordinates, a validation check should ensure that all elements in '_coord_names' accurately represent current available coordinates. If any discrepancies are found, '_coord_names' should be corrected to ensure consistency.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the indexs type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the indexs type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def _rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Also used internally by DataArray so that the warning (if any)
3569         is raised at the right stack level.
3570         """
3571         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3572         for k in name_dict.keys():
3573             if k not in self and k not in self.dims:
3574                 raise ValueError(
3575                     f"cannot rename {k!r} because it is not a "
3576                     "variable or dimension in this dataset"
3577                 )
3578 
3579             create_dim_coord = False
3580             new_k = name_dict[k]
3581 
3582             if k in self.dims and new_k in self._coord_names:
3583                 coord_dims = self._variables[name_dict[k]].dims
3584                 if coord_dims == (k,):
3585                     create_dim_coord = True
3586             elif k in self._coord_names and new_k in self.dims:
3587                 coord_dims = self._variables[k].dims
3588                 if coord_dims == (new_k,):
3589                     create_dim_coord = True
3590 
3591             if create_dim_coord:
3592                 warnings.warn(
3593                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3594                     "anymore. Try using swap_dims instead or use set_index "
3595                     "after rename to create an indexed coordinate.",
3596                     UserWarning,
3597                     stacklevel=3,
3598                 )
3599 
3600         variables, coord_names, dims, indexes = self._rename_all(
3601             name_dict=name_dict, dims_dict=name_dict
3602         )
3603         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3604 
3605     def rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Returns a new object with renamed variables, coordinates and dimensions.
3611 
3612         Parameters
3613         ----------
3614         name_dict : dict-like, optional
3615             Dictionary whose keys are current variable, coordinate or dimension names and
3616             whose values are the desired names.
3617         **names : optional
3618             Keyword form of ``name_dict``.
3619             One of name_dict or names must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed variables, coordinates and dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename_vars
3630         Dataset.rename_dims
3631         DataArray.rename
3632         """
3633         return self._rename(name_dict=name_dict, **names)
3634 
3635     def rename_dims(
3636         self: T_Dataset,
3637         dims_dict: Mapping[Any, Hashable] | None = None,
3638         **dims: Hashable,
3639     ) -> T_Dataset:
3640         """Returns a new object with renamed dimensions only.
3641 
3642         Parameters
3643         ----------
3644         dims_dict : dict-like, optional
3645             Dictionary whose keys are current dimension names and
3646             whose values are the desired names. The desired names must
3647             not be the name of an existing dimension or Variable in the Dataset.
3648         **dims : optional
3649             Keyword form of ``dims_dict``.
3650             One of dims_dict or dims must be provided.
3651 
3652         Returns
3653         -------
3654         renamed : Dataset
3655             Dataset with renamed dimensions.
3656 
3657         See Also
3658         --------
3659         Dataset.swap_dims
3660         Dataset.rename
3661         Dataset.rename_vars
3662         DataArray.rename
3663         """
3664         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3665         for k, v in dims_dict.items():
3666             if k not in self.dims:
3667                 raise ValueError(
3668                     f"cannot rename {k!r} because it is not a "
3669                     "dimension in this dataset"
3670                 )
3671             if v in self.dims or v in self:
3672                 raise ValueError(
3673                     f"Cannot rename {k} to {v} because {v} already exists. "
3674                     "Try using swap_dims instead."
3675                 )
3676 
3677         variables, coord_names, sizes, indexes = self._rename_all(
3678             name_dict={}, dims_dict=dims_dict
3679         )
3680         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3681 
3682     def rename_vars(
3683         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3684     ) -> T_Dataset:
3685         """Returns a new object with renamed variables including coordinates
3686 
3687         Parameters
3688         ----------
3689         name_dict : dict-like, optional
3690             Dictionary whose keys are current variable or coordinate names and
3691             whose values are the desired names.
3692         **names : optional
3693             Keyword form of ``name_dict``.
3694             One of name_dict or names must be provided.
3695 
3696         Returns
3697         -------
3698         renamed : Dataset
3699             Dataset with renamed variables including coordinates
3700 
3701         See Also
3702         --------
3703         Dataset.swap_dims
3704         Dataset.rename
3705         Dataset.rename_dims
3706         DataArray.rename
3707         """
3708         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3709         for k in name_dict:
3710             if k not in self:
3711                 raise ValueError(
3712                     f"cannot rename {k!r} because it is not a "
3713                     "variable or coordinate in this dataset"
3714                 )
3715         variables, coord_names, dims, indexes = self._rename_all(
3716             name_dict=name_dict, dims_dict={}
3717         )
3718         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3719 
3720     def swap_dims(
3721         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3722     ) -> T_Dataset:
3723         """Returns a new object with swapped dimensions.
3724 
3725         Parameters
3726         ----------
3727         dims_dict : dict-like
3728             Dictionary whose keys are current dimension names and whose values
3729             are new names.
3730         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3731             The keyword arguments form of ``dims_dict``.
3732             One of dims_dict or dims_kwargs must be provided.
3733 
3734         Returns
3735         -------
3736         swapped : Dataset
3737             Dataset with swapped dimensions.
3738 
3739         Examples
3740         --------
3741         >>> ds = xr.Dataset(
3742         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3743         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3744         ... )
3745         >>> ds
3746         <xarray.Dataset>
3747         Dimensions:  (x: 2)
3748         Coordinates:
3749           * x        (x) <U1 'a' 'b'
3750             y        (x) int64 0 1
3751         Data variables:
3752             a        (x) int64 5 7
3753             b        (x) float64 0.1 2.4
3754 
3755         >>> ds.swap_dims({"x": "y"})
3756         <xarray.Dataset>
3757         Dimensions:  (y: 2)
3758         Coordinates:
3759             x        (y) <U1 'a' 'b'
3760           * y        (y) int64 0 1
3761         Data variables:
3762             a        (y) int64 5 7
3763             b        (y) float64 0.1 2.4
3764 
3765         >>> ds.swap_dims({"x": "z"})
3766         <xarray.Dataset>
3767         Dimensions:  (z: 2)
3768         Coordinates:
3769             x        (z) <U1 'a' 'b'
3770             y        (z) int64 0 1
3771         Dimensions without coordinates: z
3772         Data variables:
3773             a        (z) int64 5 7
3774             b        (z) float64 0.1 2.4
3775 
3776         See Also
3777         --------
3778         Dataset.rename
3779         DataArray.swap_dims
3780         """
3781         # TODO: deprecate this method in favor of a (less confusing)
3782         # rename_dims() method that only renames dimensions.
3783 
3784         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3785         for k, v in dims_dict.items():
3786             if k not in self.dims:
3787                 raise ValueError(
3788                     f"cannot swap from dimension {k!r} because it is "
3789                     "not an existing dimension"
3790                 )
3791             if v in self.variables and self.variables[v].dims != (k,):
3792                 raise ValueError(
3793                     f"replacement dimension {v!r} is not a 1D "
3794                     f"variable along the old dimension {k!r}"
3795                 )
3796 
3797         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3798 
3799         coord_names = self._coord_names.copy()
3800         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3801 
3802         variables: dict[Hashable, Variable] = {}
3803         indexes: dict[Hashable, Index] = {}
3804         for k, v in self.variables.items():
3805             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3806             var: Variable
3807             if k in result_dims:
3808                 var = v.to_index_variable()
3809                 var.dims = dims
3810                 if k in self._indexes:
3811                     indexes[k] = self._indexes[k]
3812                     variables[k] = var
3813                 else:
3814                     index, index_vars = create_default_index_implicit(var)
3815                     indexes.update({name: index for name in index_vars})
3816                     variables.update(index_vars)
3817                     coord_names.update(index_vars)
3818             else:
3819                 var = v.to_base_variable()
3820                 var.dims = dims
3821                 variables[k] = var
3822 
3823         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3824 
3825     # change type of self and return to T_Dataset once
3826     # https://github.com/python/mypy/issues/12846 is resolved
3827     def expand_dims(
3828         self,
3829         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3830         axis: None | int | Sequence[int] = None,
3831         **dim_kwargs: Any,
3832     ) -> Dataset:
3833         """Return a new object with an additional axis (or axes) inserted at
3834         the corresponding position in the array shape.  The new object is a
3835         view into the underlying array, not a copy.
3836 
3837         If dim is already a scalar coordinate, it will be promoted to a 1D
3838         coordinate consisting of a single value.
3839 
3840         Parameters
3841         ----------
3842         dim : hashable, sequence of hashable, mapping, or None
3843             Dimensions to include on the new variable. If provided as hashable
3844             or sequence of hashable, then dimensions are inserted with length
3845             1. If provided as a mapping, then the keys are the new dimensions
3846             and the values are either integers (giving the length of the new
3847             dimensions) or array-like (giving the coordinates of the new
3848             dimensions).
3849         axis : int, sequence of int, or None, default: None
3850             Axis position(s) where new axis is to be inserted (position(s) on
3851             the result array). If a sequence of integers is passed,
3852             multiple axes are inserted. In this case, dim arguments should be
3853             same length list. If axis=None is passed, all the axes will be
3854             inserted to the start of the result array.
3855         **dim_kwargs : int or sequence or ndarray
3856             The keywords are arbitrary dimensions being inserted and the values
3857             are either the lengths of the new dims (if int is given), or their
3858             coordinates. Note, this is an alternative to passing a dict to the
3859             dim kwarg and will only be used if dim is None.
3860 
3861         Returns
3862         -------
3863         expanded : Dataset
3864             This object, but with additional dimension(s).
3865 
3866         See Also
3867         --------
3868         DataArray.expand_dims
3869         """
3870         if dim is None:
3871             pass
3872         elif isinstance(dim, Mapping):
3873             # We're later going to modify dim in place; don't tamper with
3874             # the input
3875             dim = dict(dim)
3876         elif isinstance(dim, int):
3877             raise TypeError(
3878                 "dim should be hashable or sequence of hashables or mapping"
3879             )
3880         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3881             dim = {dim: 1}
3882         elif isinstance(dim, Sequence):
3883             if len(dim) != len(set(dim)):
3884                 raise ValueError("dims should not contain duplicate values.")
3885             dim = {d: 1 for d in dim}
3886 
3887         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3888         assert isinstance(dim, MutableMapping)
3889 
3890         if axis is None:
3891             axis = list(range(len(dim)))
3892         elif not isinstance(axis, Sequence):
3893             axis = [axis]
3894 
3895         if len(dim) != len(axis):
3896             raise ValueError("lengths of dim and axis should be identical.")
3897         for d in dim:
3898             if d in self.dims:
3899                 raise ValueError(f"Dimension {d} already exists.")
3900             if d in self._variables and not utils.is_scalar(self._variables[d]):
3901                 raise ValueError(
3902                     "{dim} already exists as coordinate or"
3903                     " variable name.".format(dim=d)
3904                 )
3905 
3906         variables: dict[Hashable, Variable] = {}
3907         indexes: dict[Hashable, Index] = dict(self._indexes)
3908         coord_names = self._coord_names.copy()
3909         # If dim is a dict, then ensure that the values are either integers
3910         # or iterables.
3911         for k, v in dim.items():
3912             if hasattr(v, "__iter__"):
3913                 # If the value for the new dimension is an iterable, then
3914                 # save the coordinates to the variables dict, and set the
3915                 # value within the dim dict to the length of the iterable
3916                 # for later use.
3917                 index = PandasIndex(v, k)
3918                 indexes[k] = index
3919                 variables.update(index.create_variables())
3920                 coord_names.add(k)
3921                 dim[k] = variables[k].size
3922             elif isinstance(v, int):
3923                 pass  # Do nothing if the dimensions value is just an int
3924             else:
3925                 raise TypeError(
3926                     "The value of new dimension {k} must be "
3927                     "an iterable or an int".format(k=k)
3928                 )
3929 
3930         for k, v in self._variables.items():
3931             if k not in dim:
3932                 if k in coord_names:  # Do not change coordinates
3933                     variables[k] = v
3934                 else:
3935                     result_ndim = len(v.dims) + len(axis)
3936                     for a in axis:
3937                         if a < -result_ndim or result_ndim - 1 < a:
3938                             raise IndexError(
3939                                 f"Axis {a} of variable {k} is out of bounds of the "
3940                                 f"expanded dimension size {result_ndim}"
3941                             )
3942 
3943                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3944                     if len(axis_pos) != len(set(axis_pos)):
3945                         raise ValueError("axis should not contain duplicate values")
3946                     # We need to sort them to make sure `axis` equals to the
3947                     # axis positions of the result array.
3948                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3949 
3950                     all_dims = list(zip(v.dims, v.shape))
3951                     for d, c in zip_axis_dim:
3952                         all_dims.insert(d, c)
3953                     variables[k] = v.set_dims(dict(all_dims))
3954             else:
3955                 if k not in variables:
3956                     # If dims includes a label of a non-dimension coordinate,
3957                     # it will be promoted to a 1D coordinate with a single value.
3958                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3959                     indexes[k] = index
3960                     variables.update(index_vars)
3961 
3962         return self._replace_with_new_dims(
3963             variables, coord_names=coord_names, indexes=indexes
3964         )
3965 
3966     # change type of self and return to T_Dataset once
3967     # https://github.com/python/mypy/issues/12846 is resolved
3968     def set_index(
3969         self,
3970         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3971         append: bool = False,
3972         **indexes_kwargs: Hashable | Sequence[Hashable],
3973     ) -> Dataset:
3974         """Set Dataset (multi-)indexes using one or more existing coordinates
3975         or variables.
3976 
3977         This legacy method is limited to pandas (multi-)indexes and
3978         1-dimensional "dimension" coordinates. See
3979         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
3980         Xarray-compatible index from one or more arbitrary coordinates.
3981 
3982         Parameters
3983         ----------
3984         indexes : {dim: index, ...}
3985             Mapping from names matching dimensions and values given
3986             by (lists of) the names of existing coordinates or variables to set
3987             as new (multi-)index.
3988         append : bool, default: False
3989             If True, append the supplied index(es) to the existing index(es).
3990             Otherwise replace the existing index(es) (default).
3991         **indexes_kwargs : optional
3992             The keyword arguments form of ``indexes``.
3993             One of indexes or indexes_kwargs must be provided.
3994 
3995         Returns
3996         -------
3997         obj : Dataset
3998             Another dataset, with this dataset's data but replaced coordinates.
3999 
4000         Examples
4001         --------
4002         >>> arr = xr.DataArray(
4003         ...     data=np.ones((2, 3)),
4004         ...     dims=["x", "y"],
4005         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4006         ... )
4007         >>> ds = xr.Dataset({"v": arr})
4008         >>> ds
4009         <xarray.Dataset>
4010         Dimensions:  (x: 2, y: 3)
4011         Coordinates:
4012           * x        (x) int64 0 1
4013           * y        (y) int64 0 1 2
4014             a        (x) int64 3 4
4015         Data variables:
4016             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4017         >>> ds.set_index(x="a")
4018         <xarray.Dataset>
4019         Dimensions:  (x: 2, y: 3)
4020         Coordinates:
4021           * x        (x) int64 3 4
4022           * y        (y) int64 0 1 2
4023         Data variables:
4024             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4025 
4026         See Also
4027         --------
4028         Dataset.reset_index
4029         Dataset.set_xindex
4030         Dataset.swap_dims
4031         """
4032         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4033 
4034         new_indexes: dict[Hashable, Index] = {}
4035         new_variables: dict[Hashable, Variable] = {}
4036         drop_indexes: set[Hashable] = set()
4037         drop_variables: set[Hashable] = set()
4038         replace_dims: dict[Hashable, Hashable] = {}
4039         all_var_names: set[Hashable] = set()
4040 
4041         for dim, _var_names in dim_coords.items():
4042             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4043                 var_names = [_var_names]
4044             else:
4045                 var_names = list(_var_names)
4046 
4047             invalid_vars = set(var_names) - set(self._variables)
4048             if invalid_vars:
4049                 raise ValueError(
4050                     ", ".join([str(v) for v in invalid_vars])
4051                     + " variable(s) do not exist"
4052                 )
4053 
4054             all_var_names.update(var_names)
4055             drop_variables.update(var_names)
4056 
4057             # drop any pre-existing index involved and its corresponding coordinates
4058             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4059             all_index_coord_names = set(index_coord_names)
4060             for k in var_names:
4061                 all_index_coord_names.update(
4062                     self.xindexes.get_all_coords(k, errors="ignore")
4063                 )
4064 
4065             drop_indexes.update(all_index_coord_names)
4066             drop_variables.update(all_index_coord_names)
4067 
4068             if len(var_names) == 1 and (not append or dim not in self._indexes):
4069                 var_name = var_names[0]
4070                 var = self._variables[var_name]
4071                 if var.dims != (dim,):
4072                     raise ValueError(
4073                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4074                         f"variable {var_name!r} that has dimensions {var.dims}"
4075                     )
4076                 idx = PandasIndex.from_variables({dim: var}, options={})
4077                 idx_vars = idx.create_variables({var_name: var})
4078 
4079                 # trick to preserve coordinate order in this case
4080                 if dim in self._coord_names:
4081                     drop_variables.remove(dim)
4082             else:
4083                 if append:
4084                     current_variables = {
4085                         k: self._variables[k] for k in index_coord_names
4086                     }
4087                 else:
4088                     current_variables = {}
4089                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4090                     dim,
4091                     current_variables,
4092                     {k: self._variables[k] for k in var_names},
4093                 )
4094                 for n in idx.index.names:
4095                     replace_dims[n] = dim
4096 
4097             new_indexes.update({k: idx for k in idx_vars})
4098             new_variables.update(idx_vars)
4099 
4100         # re-add deindexed coordinates (convert to base variables)
4101         for k in drop_variables:
4102             if (
4103                 k not in new_variables
4104                 and k not in all_var_names
4105                 and k in self._coord_names
4106             ):
4107                 new_variables[k] = self._variables[k].to_base_variable()
4108 
4109         indexes_: dict[Any, Index] = {
4110             k: v for k, v in self._indexes.items() if k not in drop_indexes
4111         }
4112         indexes_.update(new_indexes)
4113 
4114         variables = {
4115             k: v for k, v in self._variables.items() if k not in drop_variables
4116         }
4117         variables.update(new_variables)
4118 
4119         # update dimensions if necessary, GH: 3512
4120         for k, v in variables.items():
4121             if any(d in replace_dims for d in v.dims):
4122                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4123                 variables[k] = v._replace(dims=new_dims)
4124 
4125         coord_names = self._coord_names - drop_variables | set(new_variables)
4126 
4127         return self._replace_with_new_dims(
4128             variables, coord_names=coord_names, indexes=indexes_
4129         )
4130 
4131     def reset_index(
4132         self: T_Dataset,
4133         dims_or_levels: Hashable | Sequence[Hashable],
4134         drop: bool = False,
4135     ) -> T_Dataset:
4136         """Reset the specified index(es) or multi-index level(s).
4137 
4138         This legacy method is specific to pandas (multi-)indexes and
4139         1-dimensional "dimension" coordinates. See the more generic
4140         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4141         method to respectively drop and set pandas or custom indexes for
4142         arbitrary coordinates.
4143 
4144         Parameters
4145         ----------
4146         dims_or_levels : Hashable or Sequence of Hashable
4147             Name(s) of the dimension(s) and/or multi-index level(s) that will
4148             be reset.
4149         drop : bool, default: False
4150             If True, remove the specified indexes and/or multi-index levels
4151             instead of extracting them as new coordinates (default: False).
4152 
4153         Returns
4154         -------
4155         obj : Dataset
4156             Another dataset, with this dataset's data but replaced coordinates.
4157 
4158         See Also
4159         --------
4160         Dataset.set_index
4161         Dataset.set_xindex
4162         Dataset.drop_indexes
4163         """
4164         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4165             dims_or_levels = [dims_or_levels]
4166 
4167         invalid_coords = set(dims_or_levels) - set(self._indexes)
4168         if invalid_coords:
4169             raise ValueError(
4170                 f"{tuple(invalid_coords)} are not coordinates with an index"
4171             )
4172 
4173         drop_indexes: set[Hashable] = set()
4174         drop_variables: set[Hashable] = set()
4175         seen: set[Index] = set()
4176         new_indexes: dict[Hashable, Index] = {}
4177         new_variables: dict[Hashable, Variable] = {}
4178 
4179         def drop_or_convert(var_names):
4180             if drop:
4181                 drop_variables.update(var_names)
4182             else:
4183                 base_vars = {
4184                     k: self._variables[k].to_base_variable() for k in var_names
4185                 }
4186                 new_variables.update(base_vars)
4187 
4188         for name in dims_or_levels:
4189             index = self._indexes[name]
4190 
4191             if index in seen:
4192                 continue
4193             seen.add(index)
4194 
4195             idx_var_names = set(self.xindexes.get_all_coords(name))
4196             drop_indexes.update(idx_var_names)
4197 
4198             if isinstance(index, PandasMultiIndex):
4199                 # special case for pd.MultiIndex
4200                 level_names = index.index.names
4201                 keep_level_vars = {
4202                     k: self._variables[k]
4203                     for k in level_names
4204                     if k not in dims_or_levels
4205                 }
4206 
4207                 if index.dim not in dims_or_levels and keep_level_vars:
4208                     # do not drop the multi-index completely
4209                     # instead replace it by a new (multi-)index with dropped level(s)
4210                     idx = index.keep_levels(keep_level_vars)
4211                     idx_vars = idx.create_variables(keep_level_vars)
4212                     new_indexes.update({k: idx for k in idx_vars})
4213                     new_variables.update(idx_vars)
4214                     if not isinstance(idx, PandasMultiIndex):
4215                         # multi-index reduced to single index
4216                         # backward compatibility: unique level coordinate renamed to dimension
4217                         drop_variables.update(keep_level_vars)
4218                     drop_or_convert(
4219                         [k for k in level_names if k not in keep_level_vars]
4220                     )
4221                 else:
4222                     # always drop the multi-index dimension variable
4223                     drop_variables.add(index.dim)
4224                     drop_or_convert(level_names)
4225             else:
4226                 drop_or_convert(idx_var_names)
4227 
4228         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4229         indexes.update(new_indexes)
4230 
4231         variables = {
4232             k: v for k, v in self._variables.items() if k not in drop_variables
4233         }
4234         variables.update(new_variables)
4235 
4236         coord_names = self._coord_names - drop_variables
4237 
4238         return self._replace_with_new_dims(
4239             variables, coord_names=coord_names, indexes=indexes
4240         )
4241 
4242     def set_xindex(
4243         self: T_Dataset,
4244         coord_names: str | Sequence[Hashable],
4245         index_cls: type[Index] | None = None,
4246         **options,
4247     ) -> T_Dataset:
4248         """Set a new, Xarray-compatible index from one or more existing
4249         coordinate(s).
4250 
4251         Parameters
4252         ----------
4253         coord_names : str or list
4254             Name(s) of the coordinate(s) used to build the index.
4255             If several names are given, their order matters.
4256         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4257             The type of index to create. By default, try setting
4258             a ``PandasIndex`` if ``len(coord_names) == 1``,
4259             otherwise a ``PandasMultiIndex``.
4260         **options
4261             Options passed to the index constructor.
4262 
4263         Returns
4264         -------
4265         obj : Dataset
4266             Another dataset, with this dataset's data and with a new index.
4267 
4268         """
4269         # the Sequence check is required for mypy
4270         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4271             coord_names = [coord_names]
4272 
4273         if index_cls is None:
4274             if len(coord_names) == 1:
4275                 index_cls = PandasIndex
4276             else:
4277                 index_cls = PandasMultiIndex
4278         else:
4279             if not issubclass(index_cls, Index):
4280                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4281 
4282         invalid_coords = set(coord_names) - self._coord_names
4283 
4284         if invalid_coords:
4285             msg = ["invalid coordinate(s)"]
4286             no_vars = invalid_coords - set(self._variables)
4287             data_vars = invalid_coords - no_vars
4288             if no_vars:
4289                 msg.append(f"those variables don't exist: {no_vars}")
4290             if data_vars:
4291                 msg.append(
4292                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4293                 )
4294             raise ValueError("\n".join(msg))
4295 
4296         # we could be more clever here (e.g., drop-in index replacement if index
4297         # coordinates do not conflict), but let's not allow this for now
4298         indexed_coords = set(coord_names) & set(self._indexes)
4299 
4300         if indexed_coords:
4301             raise ValueError(
4302                 f"those coordinates already have an index: {indexed_coords}"
4303             )
4304 
4305         coord_vars = {name: self._variables[name] for name in coord_names}
4306 
4307         index = index_cls.from_variables(coord_vars, options=options)
4308 
4309         new_coord_vars = index.create_variables(coord_vars)
4310 
4311         # special case for setting a pandas multi-index from level coordinates
4312         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4313         # elements) coordinate
4314         if isinstance(index, PandasMultiIndex):
4315             coord_names = [index.dim] + list(coord_names)
4316 
4317         variables: dict[Hashable, Variable]
4318         indexes: dict[Hashable, Index]
4319 
4320         if len(coord_names) == 1:
4321             variables = self._variables.copy()
4322             indexes = self._indexes.copy()
4323 
4324             name = list(coord_names).pop()
4325             if name in new_coord_vars:
4326                 variables[name] = new_coord_vars[name]
4327             indexes[name] = index
4328         else:
4329             # reorder variables and indexes so that coordinates having the same
4330             # index are next to each other
4331             variables = {}
4332             for name, var in self._variables.items():
4333                 if name not in coord_names:
4334                     variables[name] = var
4335 
4336             indexes = {}
4337             for name, idx in self._indexes.items():
4338                 if name not in coord_names:
4339                     indexes[name] = idx
4340 
4341             for name in coord_names:
4342                 try:
4343                     variables[name] = new_coord_vars[name]
4344                 except KeyError:
4345                     variables[name] = self._variables[name]
4346                 indexes[name] = index
4347 
4348         return self._replace(
4349             variables=variables,
4350             coord_names=self._coord_names | set(coord_names),
4351             indexes=indexes,
4352         )
4353 
4354     def reorder_levels(
4355         self: T_Dataset,
4356         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4357         **dim_order_kwargs: Sequence[int | Hashable],
4358     ) -> T_Dataset:
4359         """Rearrange index levels using input order.
4360 
4361         Parameters
4362         ----------
4363         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4364             Mapping from names matching dimensions and values given
4365             by lists representing new level orders. Every given dimension
4366             must have a multi-index.
4367         **dim_order_kwargs : Sequence of int or Hashable, optional
4368             The keyword arguments form of ``dim_order``.
4369             One of dim_order or dim_order_kwargs must be provided.
4370 
4371         Returns
4372         -------
4373         obj : Dataset
4374             Another dataset, with this dataset's data but replaced
4375             coordinates.
4376         """
4377         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4378         variables = self._variables.copy()
4379         indexes = dict(self._indexes)
4380         new_indexes: dict[Hashable, Index] = {}
4381         new_variables: dict[Hashable, IndexVariable] = {}
4382 
4383         for dim, order in dim_order.items():
4384             index = self._indexes[dim]
4385 
4386             if not isinstance(index, PandasMultiIndex):
4387                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4388 
4389             level_vars = {k: self._variables[k] for k in order}
4390             idx = index.reorder_levels(level_vars)
4391             idx_vars = idx.create_variables(level_vars)
4392             new_indexes.update({k: idx for k in idx_vars})
4393             new_variables.update(idx_vars)
4394 
4395         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4396         indexes.update(new_indexes)
4397 
4398         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4399         variables.update(new_variables)
4400 
4401         return self._replace(variables, indexes=indexes)
4402 
4403     def _get_stack_index(
4404         self,
4405         dim,
4406         multi=False,
4407         create_index=False,
4408     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4409         """Used by stack and unstack to get one pandas (multi-)index among
4410         the indexed coordinates along dimension `dim`.
4411 
4412         If exactly one index is found, return it with its corresponding
4413         coordinate variables(s), otherwise return None and an empty dict.
4414 
4415         If `create_index=True`, create a new index if none is found or raise
4416         an error if multiple indexes are found.
4417 
4418         """
4419         stack_index: Index | None = None
4420         stack_coords: dict[Hashable, Variable] = {}
4421 
4422         for name, index in self._indexes.items():
4423             var = self._variables[name]
4424             if (
4425                 var.ndim == 1
4426                 and var.dims[0] == dim
4427                 and (
4428                     # stack: must be a single coordinate index
4429                     not multi
4430                     and not self.xindexes.is_multi(name)
4431                     # unstack: must be an index that implements .unstack
4432                     or multi
4433                     and type(index).unstack is not Index.unstack
4434                 )
4435             ):
4436                 if stack_index is not None and index is not stack_index:
4437                     # more than one index found, stop
4438                     if create_index:
4439                         raise ValueError(
4440                             f"cannot stack dimension {dim!r} with `create_index=True` "
4441                             "and with more than one index found along that dimension"
4442                         )
4443                     return None, {}
4444                 stack_index = index
4445                 stack_coords[name] = var
4446 
4447         if create_index and stack_index is None:
4448             if dim in self._variables:
4449                 var = self._variables[dim]
4450             else:
4451                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4452             # dummy index (only `stack_coords` will be used to construct the multi-index)
4453             stack_index = PandasIndex([0], dim)
4454             stack_coords = {dim: var}
4455 
4456         return stack_index, stack_coords
4457 
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )
4516 
4517     def stack(
4518         self: T_Dataset,
4519         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4520         create_index: bool | None = True,
4521         index_cls: type[Index] = PandasMultiIndex,
4522         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4523     ) -> T_Dataset:
4524         """
4525         Stack any number of existing dimensions into a single new dimension.
4526 
4527         New dimensions will be added at the end, and by default the corresponding
4528         coordinate variables will be combined into a MultiIndex.
4529 
4530         Parameters
4531         ----------
4532         dimensions : mapping of hashable to sequence of hashable
4533             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4534             dimensions, and the existing dimensions that they replace. An
4535             ellipsis (`...`) will be replaced by all unlisted dimensions.
4536             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4537             all dimensions.
4538         create_index : bool or None, default: True
4539 
4540             - True: create a multi-index for each of the stacked dimensions.
4541             - False: don't create any index.
4542             - None. create a multi-index only if exactly one single (1-d) coordinate
4543               index is found for every dimension to stack.
4544 
4545         index_cls: Index-class, default: PandasMultiIndex
4546             Can be used to pass a custom multi-index type (must be an Xarray index that
4547             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4548         **dimensions_kwargs
4549             The keyword arguments form of ``dimensions``.
4550             One of dimensions or dimensions_kwargs must be provided.
4551 
4552         Returns
4553         -------
4554         stacked : Dataset
4555             Dataset with stacked data.
4556 
4557         See Also
4558         --------
4559         Dataset.unstack
4560         """
4561         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4562         result = self
4563         for new_dim, dims in dimensions.items():
4564             result = result._stack_once(dims, new_dim, index_cls, create_index)
4565         return result
4566 
4567     def to_stacked_array(
4568         self,
4569         new_dim: Hashable,
4570         sample_dims: Collection[Hashable],
4571         variable_dim: Hashable = "variable",
4572         name: Hashable | None = None,
4573     ) -> DataArray:
4574         """Combine variables of differing dimensionality into a DataArray
4575         without broadcasting.
4576 
4577         This method is similar to Dataset.to_array but does not broadcast the
4578         variables.
4579 
4580         Parameters
4581         ----------
4582         new_dim : hashable
4583             Name of the new stacked coordinate
4584         sample_dims : Collection of hashables
4585             List of dimensions that **will not** be stacked. Each array in the
4586             dataset must share these dimensions. For machine learning
4587             applications, these define the dimensions over which samples are
4588             drawn.
4589         variable_dim : hashable, default: "variable"
4590             Name of the level in the stacked coordinate which corresponds to
4591             the variables.
4592         name : hashable, optional
4593             Name of the new data array.
4594 
4595         Returns
4596         -------
4597         stacked : DataArray
4598             DataArray with the specified dimensions and data variables
4599             stacked together. The stacked coordinate is named ``new_dim``
4600             and represented by a MultiIndex object with a level containing the
4601             data variable names. The name of this level is controlled using
4602             the ``variable_dim`` argument.
4603 
4604         See Also
4605         --------
4606         Dataset.to_array
4607         Dataset.stack
4608         DataArray.to_unstacked_dataset
4609 
4610         Examples
4611         --------
4612         >>> data = xr.Dataset(
4613         ...     data_vars={
4614         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4615         ...         "b": ("x", [6, 7]),
4616         ...     },
4617         ...     coords={"y": ["u", "v", "w"]},
4618         ... )
4619 
4620         >>> data
4621         <xarray.Dataset>
4622         Dimensions:  (x: 2, y: 3)
4623         Coordinates:
4624           * y        (y) <U1 'u' 'v' 'w'
4625         Dimensions without coordinates: x
4626         Data variables:
4627             a        (x, y) int64 0 1 2 3 4 5
4628             b        (x) int64 6 7
4629 
4630         >>> data.to_stacked_array("z", sample_dims=["x"])
4631         <xarray.DataArray 'a' (x: 2, z: 4)>
4632         array([[0, 1, 2, 6],
4633                [3, 4, 5, 7]])
4634         Coordinates:
4635           * z         (z) object MultiIndex
4636           * variable  (z) object 'a' 'a' 'a' 'b'
4637           * y         (z) object 'u' 'v' 'w' nan
4638         Dimensions without coordinates: x
4639 
4640         """
4641         from .concat import concat
4642 
4643         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4644 
4645         for variable in self:
4646             dims = self[variable].dims
4647             dims_include_sample_dims = set(sample_dims) <= set(dims)
4648             if not dims_include_sample_dims:
4649                 raise ValueError(
4650                     "All variables in the dataset must contain the "
4651                     "dimensions {}.".format(dims)
4652                 )
4653 
4654         def ensure_stackable(val):
4655             assign_coords = {variable_dim: val.name}
4656             for dim in stacking_dims:
4657                 if dim not in val.dims:
4658                     assign_coords[dim] = None
4659 
4660             expand_dims = set(stacking_dims).difference(set(val.dims))
4661             expand_dims.add(variable_dim)
4662             # must be list for .expand_dims
4663             expand_dims = list(expand_dims)
4664 
4665             return (
4666                 val.assign_coords(**assign_coords)
4667                 .expand_dims(expand_dims)
4668                 .stack({new_dim: (variable_dim,) + stacking_dims})
4669             )
4670 
4671         # concatenate the arrays
4672         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4673         data_array = concat(stackable_vars, dim=new_dim)
4674 
4675         if name is not None:
4676             data_array.name = name
4677 
4678         return data_array
4679 
4680     def _unstack_once(
4681         self: T_Dataset,
4682         dim: Hashable,
4683         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4684         fill_value,
4685         sparse: bool = False,
4686     ) -> T_Dataset:
4687         index, index_vars = index_and_vars
4688         variables: dict[Hashable, Variable] = {}
4689         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4690 
4691         new_indexes, clean_index = index.unstack()
4692         indexes.update(new_indexes)
4693 
4694         for name, idx in new_indexes.items():
4695             variables.update(idx.create_variables(index_vars))
4696 
4697         for name, var in self.variables.items():
4698             if name not in index_vars:
4699                 if dim in var.dims:
4700                     if isinstance(fill_value, Mapping):
4701                         fill_value_ = fill_value[name]
4702                     else:
4703                         fill_value_ = fill_value
4704 
4705                     variables[name] = var._unstack_once(
4706                         index=clean_index,
4707                         dim=dim,
4708                         fill_value=fill_value_,
4709                         sparse=sparse,
4710                     )
4711                 else:
4712                     variables[name] = var
4713 
4714         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4715 
4716         return self._replace_with_new_dims(
4717             variables, coord_names=coord_names, indexes=indexes
4718         )
4719 
4720     def _unstack_full_reindex(
4721         self: T_Dataset,
4722         dim: Hashable,
4723         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4724         fill_value,
4725         sparse: bool,
4726     ) -> T_Dataset:
4727         index, index_vars = index_and_vars
4728         variables: dict[Hashable, Variable] = {}
4729         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4730 
4731         new_indexes, clean_index = index.unstack()
4732         indexes.update(new_indexes)
4733 
4734         new_index_variables = {}
4735         for name, idx in new_indexes.items():
4736             new_index_variables.update(idx.create_variables(index_vars))
4737 
4738         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4739         variables.update(new_index_variables)
4740 
4741         # take a shortcut in case the MultiIndex was not modified.
4742         full_idx = pd.MultiIndex.from_product(
4743             clean_index.levels, names=clean_index.names
4744         )
4745         if clean_index.equals(full_idx):
4746             obj = self
4747         else:
4748             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4749             xr_full_idx = PandasMultiIndex(full_idx, dim)
4750             indexers = Indexes(
4751                 {k: xr_full_idx for k in index_vars},
4752                 xr_full_idx.create_variables(index_vars),
4753             )
4754             obj = self._reindex(
4755                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4756             )
4757 
4758         for name, var in obj.variables.items():
4759             if name not in index_vars:
4760                 if dim in var.dims:
4761                     variables[name] = var.unstack({dim: new_dim_sizes})
4762                 else:
4763                     variables[name] = var
4764 
4765         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4766 
4767         return self._replace_with_new_dims(
4768             variables, coord_names=coord_names, indexes=indexes
4769         )
4770 
4771     def unstack(
4772         self: T_Dataset,
4773         dim: Hashable | Iterable[Hashable] | None = None,
4774         fill_value: Any = xrdtypes.NA,
4775         sparse: bool = False,
4776     ) -> T_Dataset:
4777         """
4778         Unstack existing dimensions corresponding to MultiIndexes into
4779         multiple new dimensions.
4780 
4781         New dimensions will be added at the end.
4782 
4783         Parameters
4784         ----------
4785         dim : hashable or iterable of hashable, optional
4786             Dimension(s) over which to unstack. By default unstacks all
4787             MultiIndexes.
4788         fill_value : scalar or dict-like, default: nan
4789             value to be filled. If a dict-like, maps variable names to
4790             fill values. If not provided or if the dict-like does not
4791             contain all variables, the dtype's NA value will be used.
4792         sparse : bool, default: False
4793             use sparse-array if True
4794 
4795         Returns
4796         -------
4797         unstacked : Dataset
4798             Dataset with unstacked data.
4799 
4800         See Also
4801         --------
4802         Dataset.stack
4803         """
4804 
4805         if dim is None:
4806             dims = list(self.dims)
4807         else:
4808             if isinstance(dim, str) or not isinstance(dim, Iterable):
4809                 dims = [dim]
4810             else:
4811                 dims = list(dim)
4812 
4813             missing_dims = [d for d in dims if d not in self.dims]
4814             if missing_dims:
4815                 raise ValueError(
4816                     f"Dataset does not contain the dimensions: {missing_dims}"
4817                 )
4818 
4819         # each specified dimension must have exactly one multi-index
4820         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4821         for d in dims:
4822             idx, idx_vars = self._get_stack_index(d, multi=True)
4823             if idx is not None:
4824                 stacked_indexes[d] = idx, idx_vars
4825 
4826         if dim is None:
4827             dims = list(stacked_indexes)
4828         else:
4829             non_multi_dims = set(dims) - set(stacked_indexes)
4830             if non_multi_dims:
4831                 raise ValueError(
4832                     "cannot unstack dimensions that do not "
4833                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4834                 )
4835 
4836         result = self.copy(deep=False)
4837 
4838         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4839         # so we can't just access self.variables[v].data for every variable.
4840         # We only check the non-index variables.
4841         # https://github.com/pydata/xarray/issues/5902
4842         nonindexes = [
4843             self.variables[k] for k in set(self.variables) - set(self._indexes)
4844         ]
4845         # Notes for each of these cases:
4846         # 1. Dask arrays don't support assignment by index, which the fast unstack
4847         #    function requires.
4848         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4849         # 2. Sparse doesn't currently support (though we could special-case it)
4850         #    https://github.com/pydata/sparse/issues/422
4851         # 3. pint requires checking if it's a NumPy array until
4852         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4853         #    Once that is resolved, explicitly exclude pint arrays.
4854         #    pint doesn't implement `np.full_like` in a way that's
4855         #    currently compatible.
4856         needs_full_reindex = any(
4857             is_duck_dask_array(v.data)
4858             or isinstance(v.data, sparse_array_type)
4859             or not isinstance(v.data, np.ndarray)
4860             for v in nonindexes
4861         )
4862 
4863         for dim in dims:
4864             if needs_full_reindex:
4865                 result = result._unstack_full_reindex(
4866                     dim, stacked_indexes[dim], fill_value, sparse
4867                 )
4868             else:
4869                 result = result._unstack_once(
4870                     dim, stacked_indexes[dim], fill_value, sparse
4871                 )
4872         return result
4873 
4874     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4875         """Update this dataset's variables with those from another dataset.
4876 
4877         Just like :py:meth:`dict.update` this is a in-place operation.
4878         For a non-inplace version, see :py:meth:`Dataset.merge`.
4879 
4880         Parameters
4881         ----------
4882         other : Dataset or mapping
4883             Variables with which to update this dataset. One of:
4884 
4885             - Dataset
4886             - mapping {var name: DataArray}
4887             - mapping {var name: Variable}
4888             - mapping {var name: (dimension name, array-like)}
4889             - mapping {var name: (tuple of dimension names, array-like)}
4890 
4891         Returns
4892         -------
4893         updated : Dataset
4894             Updated dataset. Note that since the update is in-place this is the input
4895             dataset.
4896 
4897             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4898 
4899         Raises
4900         ------
4901         ValueError
4902             If any dimensions would have inconsistent sizes in the updated
4903             dataset.
4904 
4905         See Also
4906         --------
4907         Dataset.assign
4908         Dataset.merge
4909         """
4910         merge_result = dataset_update_method(self, other)
4911         return self._replace(inplace=True, **merge_result._asdict())
4912 
4913     def merge(
4914         self: T_Dataset,
4915         other: CoercibleMapping | DataArray,
4916         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4917         compat: CompatOptions = "no_conflicts",
4918         join: JoinOptions = "outer",
4919         fill_value: Any = xrdtypes.NA,
4920         combine_attrs: CombineAttrsOptions = "override",
4921     ) -> T_Dataset:
4922         """Merge the arrays of two datasets into a single dataset.
4923 
4924         This method generally does not allow for overriding data, with the
4925         exception of attributes, which are ignored on the second dataset.
4926         Variables with the same name are checked for conflicts via the equals
4927         or identical methods.
4928 
4929         Parameters
4930         ----------
4931         other : Dataset or mapping
4932             Dataset or variables to merge with this dataset.
4933         overwrite_vars : hashable or iterable of hashable, optional
4934             If provided, update variables of these name(s) without checking for
4935             conflicts in this dataset.
4936         compat : {"identical", "equals", "broadcast_equals", \
4937                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4938             String indicating how to compare variables of the same name for
4939             potential conflicts:
4940 
4941             - 'identical': all values, dimensions and attributes must be the
4942               same.
4943             - 'equals': all values and dimensions must be the same.
4944             - 'broadcast_equals': all values must be equal when variables are
4945               broadcast against each other to ensure common dimensions.
4946             - 'no_conflicts': only values which are not null in both datasets
4947               must be equal. The returned dataset then contains the combination
4948               of all non-null values.
4949             - 'override': skip comparing and pick variable from first dataset
4950             - 'minimal': drop conflicting coordinates
4951 
4952         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4953                default: "outer"
4954             Method for joining ``self`` and ``other`` along shared dimensions:
4955 
4956             - 'outer': use the union of the indexes
4957             - 'inner': use the intersection of the indexes
4958             - 'left': use indexes from ``self``
4959             - 'right': use indexes from ``other``
4960             - 'exact': error instead of aligning non-equal indexes
4961             - 'override': use indexes from ``self`` that are the same size
4962               as those of ``other`` in that dimension
4963 
4964         fill_value : scalar or dict-like, optional
4965             Value to use for newly missing values. If a dict-like, maps
4966             variable names (including coordinates) to fill values.
4967         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4968                          "override"} or callable, default: "override"
4969             A callable or a string indicating how to combine attrs of the objects being
4970             merged:
4971 
4972             - "drop": empty attrs on returned Dataset.
4973             - "identical": all attrs must be the same on every object.
4974             - "no_conflicts": attrs from all objects are combined, any that have
4975               the same name must also have the same value.
4976             - "drop_conflicts": attrs from all objects are combined, any that have
4977               the same name but different values are dropped.
4978             - "override": skip comparing and copy attrs from the first dataset to
4979               the result.
4980 
4981             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4982             as its only parameters.
4983 
4984         Returns
4985         -------
4986         merged : Dataset
4987             Merged dataset.
4988 
4989         Raises
4990         ------
4991         MergeError
4992             If any variables conflict (see ``compat``).
4993 
4994         See Also
4995         --------
4996         Dataset.update
4997         """
4998         from .dataarray import DataArray
4999 
5000         other = other.to_dataset() if isinstance(other, DataArray) else other
5001         merge_result = dataset_merge_method(
5002             self,
5003             other,
5004             overwrite_vars=overwrite_vars,
5005             compat=compat,
5006             join=join,
5007             fill_value=fill_value,
5008             combine_attrs=combine_attrs,
5009         )
5010         return self._replace(**merge_result._asdict())
5011 
5012     def _assert_all_in_dataset(
5013         self, names: Iterable[Hashable], virtual_okay: bool = False
5014     ) -> None:
5015         bad_names = set(names) - set(self._variables)
5016         if virtual_okay:
5017             bad_names -= self.virtual_variables
5018         if bad_names:
5019             raise ValueError(
5020                 "One or more of the specified variables "
5021                 "cannot be found in this dataset"
5022             )
5023 
5024     def drop_vars(
5025         self: T_Dataset,
5026         names: Hashable | Iterable[Hashable],
5027         *,
5028         errors: ErrorOptions = "raise",
5029     ) -> T_Dataset:
5030         """Drop variables from this dataset.
5031 
5032         Parameters
5033         ----------
5034         names : hashable or iterable of hashable
5035             Name(s) of variables to drop.
5036         errors : {"raise", "ignore"}, default: "raise"
5037             If 'raise', raises a ValueError error if any of the variable
5038             passed are not in the dataset. If 'ignore', any given names that are in the
5039             dataset are dropped and no error is raised.
5040 
5041         Returns
5042         -------
5043         dropped : Dataset
5044 
5045         """
5046         # the Iterable check is required for mypy
5047         if is_scalar(names) or not isinstance(names, Iterable):
5048             names = {names}
5049         else:
5050             names = set(names)
5051         if errors == "raise":
5052             self._assert_all_in_dataset(names)
5053 
5054         # GH6505
5055         other_names = set()
5056         for var in names:
5057             maybe_midx = self._indexes.get(var, None)
5058             if isinstance(maybe_midx, PandasMultiIndex):
5059                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5060                 idx_other_names = idx_coord_names - set(names)
5061                 other_names.update(idx_other_names)
5062         if other_names:
5063             names |= set(other_names)
5064             warnings.warn(
5065                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5066                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5067                 DeprecationWarning,
5068                 stacklevel=2,
5069             )
5070 
5071         assert_no_index_corrupted(self.xindexes, names)
5072 
5073         variables = {k: v for k, v in self._variables.items() if k not in names}
5074         coord_names = {k for k in self._coord_names if k in variables}
5075         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5076         return self._replace_with_new_dims(
5077             variables, coord_names=coord_names, indexes=indexes
5078         )
5079 
5080     def drop_indexes(
5081         self: T_Dataset,
5082         coord_names: Hashable | Iterable[Hashable],
5083         *,
5084         errors: ErrorOptions = "raise",
5085     ) -> T_Dataset:
5086         """Drop the indexes assigned to the given coordinates.
5087 
5088         Parameters
5089         ----------
5090         coord_names : hashable or iterable of hashable
5091             Name(s) of the coordinate(s) for which to drop the index.
5092         errors : {"raise", "ignore"}, default: "raise"
5093             If 'raise', raises a ValueError error if any of the coordinates
5094             passed have no index or are not in the dataset.
5095             If 'ignore', no error is raised.
5096 
5097         Returns
5098         -------
5099         dropped : Dataset
5100             A new dataset with dropped indexes.
5101 
5102         """
5103         # the Iterable check is required for mypy
5104         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5105             coord_names = {coord_names}
5106         else:
5107             coord_names = set(coord_names)
5108 
5109         if errors == "raise":
5110             invalid_coords = coord_names - self._coord_names
5111             if invalid_coords:
5112                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5113 
5114             unindexed_coords = set(coord_names) - set(self._indexes)
5115             if unindexed_coords:
5116                 raise ValueError(
5117                     f"those coordinates do not have an index: {unindexed_coords}"
5118                 )
5119 
5120         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5121 
5122         variables = {}
5123         for name, var in self._variables.items():
5124             if name in coord_names:
5125                 variables[name] = var.to_base_variable()
5126             else:
5127                 variables[name] = var
5128 
5129         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5130 
5131         return self._replace(variables=variables, indexes=indexes)
5132 
5133     def drop(
5134         self: T_Dataset,
5135         labels=None,
5136         dim=None,
5137         *,
5138         errors: ErrorOptions = "raise",
5139         **labels_kwargs,
5140     ) -> T_Dataset:
5141         """Backward compatible method based on `drop_vars` and `drop_sel`
5142 
5143         Using either `drop_vars` or `drop_sel` is encouraged
5144 
5145         See Also
5146         --------
5147         Dataset.drop_vars
5148         Dataset.drop_sel
5149         """
5150         if errors not in ["raise", "ignore"]:
5151             raise ValueError('errors must be either "raise" or "ignore"')
5152 
5153         if is_dict_like(labels) and not isinstance(labels, dict):
5154             warnings.warn(
5155                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5156                 FutureWarning,
5157                 stacklevel=2,
5158             )
5159             return self.drop_vars(labels, errors=errors)
5160 
5161         if labels_kwargs or isinstance(labels, dict):
5162             if dim is not None:
5163                 raise ValueError("cannot specify dim and dict-like arguments.")
5164             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5165 
5166         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5167             warnings.warn(
5168                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5169                 PendingDeprecationWarning,
5170                 stacklevel=2,
5171             )
5172             return self.drop_vars(labels, errors=errors)
5173         if dim is not None:
5174             warnings.warn(
5175                 "dropping labels using list-like labels is deprecated; using "
5176                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5177                 DeprecationWarning,
5178                 stacklevel=2,
5179             )
5180             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5181 
5182         warnings.warn(
5183             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5184             PendingDeprecationWarning,
5185             stacklevel=2,
5186         )
5187         return self.drop_sel(labels, errors=errors)
5188 
5189     def drop_sel(
5190         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5191     ) -> T_Dataset:
5192         """Drop index labels from this dataset.
5193 
5194         Parameters
5195         ----------
5196         labels : mapping of hashable to Any
5197             Index labels to drop
5198         errors : {"raise", "ignore"}, default: "raise"
5199             If 'raise', raises a ValueError error if
5200             any of the index labels passed are not
5201             in the dataset. If 'ignore', any given labels that are in the
5202             dataset are dropped and no error is raised.
5203         **labels_kwargs : {dim: label, ...}, optional
5204             The keyword arguments form of ``dim`` and ``labels``
5205 
5206         Returns
5207         -------
5208         dropped : Dataset
5209 
5210         Examples
5211         --------
5212         >>> data = np.arange(6).reshape(2, 3)
5213         >>> labels = ["a", "b", "c"]
5214         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5215         >>> ds
5216         <xarray.Dataset>
5217         Dimensions:  (x: 2, y: 3)
5218         Coordinates:
5219           * y        (y) <U1 'a' 'b' 'c'
5220         Dimensions without coordinates: x
5221         Data variables:
5222             A        (x, y) int64 0 1 2 3 4 5
5223         >>> ds.drop_sel(y=["a", "c"])
5224         <xarray.Dataset>
5225         Dimensions:  (x: 2, y: 1)
5226         Coordinates:
5227           * y        (y) <U1 'b'
5228         Dimensions without coordinates: x
5229         Data variables:
5230             A        (x, y) int64 1 4
5231         >>> ds.drop_sel(y="b")
5232         <xarray.Dataset>
5233         Dimensions:  (x: 2, y: 2)
5234         Coordinates:
5235           * y        (y) <U1 'a' 'c'
5236         Dimensions without coordinates: x
5237         Data variables:
5238             A        (x, y) int64 0 2 3 5
5239         """
5240         if errors not in ["raise", "ignore"]:
5241             raise ValueError('errors must be either "raise" or "ignore"')
5242 
5243         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5244 
5245         ds = self
5246         for dim, labels_for_dim in labels.items():
5247             # Don't cast to set, as it would harm performance when labels
5248             # is a large numpy array
5249             if utils.is_scalar(labels_for_dim):
5250                 labels_for_dim = [labels_for_dim]
5251             labels_for_dim = np.asarray(labels_for_dim)
5252             try:
5253                 index = self.get_index(dim)
5254             except KeyError:
5255                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5256             new_index = index.drop(labels_for_dim, errors=errors)
5257             ds = ds.loc[{dim: new_index}]
5258         return ds
5259 
5260     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5261         """Drop index positions from this Dataset.
5262 
5263         Parameters
5264         ----------
5265         indexers : mapping of hashable to Any
5266             Index locations to drop
5267         **indexers_kwargs : {dim: position, ...}, optional
5268             The keyword arguments form of ``dim`` and ``positions``
5269 
5270         Returns
5271         -------
5272         dropped : Dataset
5273 
5274         Raises
5275         ------
5276         IndexError
5277 
5278         Examples
5279         --------
5280         >>> data = np.arange(6).reshape(2, 3)
5281         >>> labels = ["a", "b", "c"]
5282         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5283         >>> ds
5284         <xarray.Dataset>
5285         Dimensions:  (x: 2, y: 3)
5286         Coordinates:
5287           * y        (y) <U1 'a' 'b' 'c'
5288         Dimensions without coordinates: x
5289         Data variables:
5290             A        (x, y) int64 0 1 2 3 4 5
5291         >>> ds.drop_isel(y=[0, 2])
5292         <xarray.Dataset>
5293         Dimensions:  (x: 2, y: 1)
5294         Coordinates:
5295           * y        (y) <U1 'b'
5296         Dimensions without coordinates: x
5297         Data variables:
5298             A        (x, y) int64 1 4
5299         >>> ds.drop_isel(y=1)
5300         <xarray.Dataset>
5301         Dimensions:  (x: 2, y: 2)
5302         Coordinates:
5303           * y        (y) <U1 'a' 'c'
5304         Dimensions without coordinates: x
5305         Data variables:
5306             A        (x, y) int64 0 2 3 5
5307         """
5308 
5309         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5310 
5311         ds = self
5312         dimension_index = {}
5313         for dim, pos_for_dim in indexers.items():
5314             # Don't cast to set, as it would harm performance when labels
5315             # is a large numpy array
5316             if utils.is_scalar(pos_for_dim):
5317                 pos_for_dim = [pos_for_dim]
5318             pos_for_dim = np.asarray(pos_for_dim)
5319             index = self.get_index(dim)
5320             new_index = index.delete(pos_for_dim)
5321             dimension_index[dim] = new_index
5322         ds = ds.loc[dimension_index]
5323         return ds
5324 
5325     def drop_dims(
5326         self: T_Dataset,
5327         drop_dims: Hashable | Iterable[Hashable],
5328         *,
5329         errors: ErrorOptions = "raise",
5330     ) -> T_Dataset:
5331         """Drop dimensions and associated variables from this dataset.
5332 
5333         Parameters
5334         ----------
5335         drop_dims : hashable or iterable of hashable
5336             Dimension or dimensions to drop.
5337         errors : {"raise", "ignore"}, default: "raise"
5338             If 'raise', raises a ValueError error if any of the
5339             dimensions passed are not in the dataset. If 'ignore', any given
5340             dimensions that are in the dataset are dropped and no error is raised.
5341 
5342         Returns
5343         -------
5344         obj : Dataset
5345             The dataset without the given dimensions (or any variables
5346             containing those dimensions).
5347         """
5348         if errors not in ["raise", "ignore"]:
5349             raise ValueError('errors must be either "raise" or "ignore"')
5350 
5351         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5352             drop_dims = {drop_dims}
5353         else:
5354             drop_dims = set(drop_dims)
5355 
5356         if errors == "raise":
5357             missing_dims = drop_dims - set(self.dims)
5358             if missing_dims:
5359                 raise ValueError(
5360                     f"Dataset does not contain the dimensions: {missing_dims}"
5361                 )
5362 
5363         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5364         return self.drop_vars(drop_vars)
5365 
5366     def transpose(
5367         self: T_Dataset,
5368         *dims: Hashable,
5369         missing_dims: ErrorOptionsWithWarn = "raise",
5370     ) -> T_Dataset:
5371         """Return a new Dataset object with all array dimensions transposed.
5372 
5373         Although the order of dimensions on each array will change, the dataset
5374         dimensions themselves will remain in fixed (sorted) order.
5375 
5376         Parameters
5377         ----------
5378         *dims : hashable, optional
5379             By default, reverse the dimensions on each array. Otherwise,
5380             reorder the dimensions to this order.
5381         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5382             What to do if dimensions that should be selected from are not present in the
5383             Dataset:
5384             - "raise": raise an exception
5385             - "warn": raise a warning, and ignore the missing dimensions
5386             - "ignore": ignore the missing dimensions
5387 
5388         Returns
5389         -------
5390         transposed : Dataset
5391             Each array in the dataset (including) coordinates will be
5392             transposed to the given order.
5393 
5394         Notes
5395         -----
5396         This operation returns a view of each array's data. It is
5397         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5398         -- the data will be fully loaded into memory.
5399 
5400         See Also
5401         --------
5402         numpy.transpose
5403         DataArray.transpose
5404         """
5405         # Use infix_dims to check once for missing dimensions
5406         if len(dims) != 0:
5407             _ = list(infix_dims(dims, self.dims, missing_dims))
5408 
5409         ds = self.copy()
5410         for name, var in self._variables.items():
5411             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5412             ds._variables[name] = var.transpose(*var_dims)
5413         return ds
5414 
5415     def dropna(
5416         self: T_Dataset,
5417         dim: Hashable,
5418         how: Literal["any", "all"] = "any",
5419         thresh: int | None = None,
5420         subset: Iterable[Hashable] | None = None,
5421     ) -> T_Dataset:
5422         """Returns a new dataset with dropped labels for missing values along
5423         the provided dimension.
5424 
5425         Parameters
5426         ----------
5427         dim : hashable
5428             Dimension along which to drop missing values. Dropping along
5429             multiple dimensions simultaneously is not yet supported.
5430         how : {"any", "all"}, default: "any"
5431             - any : if any NA values are present, drop that label
5432             - all : if all values are NA, drop that label
5433 
5434         thresh : int or None, optional
5435             If supplied, require this many non-NA values.
5436         subset : iterable of hashable or None, optional
5437             Which variables to check for missing values. By default, all
5438             variables in the dataset are checked.
5439 
5440         Returns
5441         -------
5442         Dataset
5443         """
5444         # TODO: consider supporting multiple dimensions? Or not, given that
5445         # there are some ugly edge cases, e.g., pandas's dropna differs
5446         # depending on the order of the supplied axes.
5447 
5448         if dim not in self.dims:
5449             raise ValueError(f"{dim} must be a single dataset dimension")
5450 
5451         if subset is None:
5452             subset = iter(self.data_vars)
5453 
5454         count = np.zeros(self.dims[dim], dtype=np.int64)
5455         size = np.int_(0)  # for type checking
5456 
5457         for k in subset:
5458             array = self._variables[k]
5459             if dim in array.dims:
5460                 dims = [d for d in array.dims if d != dim]
5461                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5462                 size += math.prod([self.dims[d] for d in dims])
5463 
5464         if thresh is not None:
5465             mask = count >= thresh
5466         elif how == "any":
5467             mask = count == size
5468         elif how == "all":
5469             mask = count > 0
5470         elif how is not None:
5471             raise ValueError(f"invalid how option: {how}")
5472         else:
5473             raise TypeError("must specify how or thresh")
5474 
5475         return self.isel({dim: mask})
5476 
5477     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5478         """Fill missing values in this object.
5479 
5480         This operation follows the normal broadcasting and alignment rules that
5481         xarray uses for binary arithmetic, except the result is aligned to this
5482         object (``join='left'``) instead of aligned to the intersection of
5483         index coordinates (``join='inner'``).
5484 
5485         Parameters
5486         ----------
5487         value : scalar, ndarray, DataArray, dict or Dataset
5488             Used to fill all matching missing values in this dataset's data
5489             variables. Scalars, ndarrays or DataArrays arguments are used to
5490             fill all data with aligned coordinates (for DataArrays).
5491             Dictionaries or datasets match data variables and then align
5492             coordinates if necessary.
5493 
5494         Returns
5495         -------
5496         Dataset
5497 
5498         Examples
5499         --------
5500         >>> ds = xr.Dataset(
5501         ...     {
5502         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5503         ...         "B": ("x", [3, 4, np.nan, 1]),
5504         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5505         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5506         ...     },
5507         ...     coords={"x": [0, 1, 2, 3]},
5508         ... )
5509         >>> ds
5510         <xarray.Dataset>
5511         Dimensions:  (x: 4)
5512         Coordinates:
5513           * x        (x) int64 0 1 2 3
5514         Data variables:
5515             A        (x) float64 nan 2.0 nan 0.0
5516             B        (x) float64 3.0 4.0 nan 1.0
5517             C        (x) float64 nan nan nan 5.0
5518             D        (x) float64 nan 3.0 nan 4.0
5519 
5520         Replace all `NaN` values with 0s.
5521 
5522         >>> ds.fillna(0)
5523         <xarray.Dataset>
5524         Dimensions:  (x: 4)
5525         Coordinates:
5526           * x        (x) int64 0 1 2 3
5527         Data variables:
5528             A        (x) float64 0.0 2.0 0.0 0.0
5529             B        (x) float64 3.0 4.0 0.0 1.0
5530             C        (x) float64 0.0 0.0 0.0 5.0
5531             D        (x) float64 0.0 3.0 0.0 4.0
5532 
5533         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
5534 
5535         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5536         >>> ds.fillna(value=values)
5537         <xarray.Dataset>
5538         Dimensions:  (x: 4)
5539         Coordinates:
5540           * x        (x) int64 0 1 2 3
5541         Data variables:
5542             A        (x) float64 0.0 2.0 0.0 0.0
5543             B        (x) float64 3.0 4.0 1.0 1.0
5544             C        (x) float64 2.0 2.0 2.0 5.0
5545             D        (x) float64 3.0 3.0 3.0 4.0
5546         """
5547         if utils.is_dict_like(value):
5548             value_keys = getattr(value, "data_vars", value).keys()
5549             if not set(value_keys) <= set(self.data_vars.keys()):
5550                 raise ValueError(
5551                     "all variables in the argument to `fillna` "
5552                     "must be contained in the original dataset"
5553                 )
5554         out = ops.fillna(self, value)
5555         return out
5556 
5557     def interpolate_na(
5558         self: T_Dataset,
5559         dim: Hashable | None = None,
5560         method: InterpOptions = "linear",
5561         limit: int = None,
5562         use_coordinate: bool | Hashable = True,
5563         max_gap: (
5564             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5565         ) = None,
5566         **kwargs: Any,
5567     ) -> T_Dataset:
5568         """Fill in NaNs by interpolating according to different methods.
5569 
5570         Parameters
5571         ----------
5572         dim : Hashable or None, optional
5573             Specifies the dimension along which to interpolate.
5574         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5575             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5576             String indicating which method to use for interpolation:
5577 
5578             - 'linear': linear interpolation. Additional keyword
5579               arguments are passed to :py:func:`numpy.interp`
5580             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5581               are passed to :py:func:`scipy.interpolate.interp1d`. If
5582               ``method='polynomial'``, the ``order`` keyword argument must also be
5583               provided.
5584             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5585               respective :py:class:`scipy.interpolate` classes.
5586 
5587         use_coordinate : bool or Hashable, default: True
5588             Specifies which index to use as the x values in the interpolation
5589             formulated as `y = f(x)`. If False, values are treated as if
5590             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5591             used. If ``use_coordinate`` is a string, it specifies the name of a
5592             coordinate variariable to use as the index.
5593         limit : int, default: None
5594             Maximum number of consecutive NaNs to fill. Must be greater than 0
5595             or None for no limit. This filling is done regardless of the size of
5596             the gap in the data. To only interpolate over gaps less than a given length,
5597             see ``max_gap``.
5598         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5599             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5600             Use None for no limit. When interpolating along a datetime64 dimension
5601             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5602 
5603             - a string that is valid input for pandas.to_timedelta
5604             - a :py:class:`numpy.timedelta64` object
5605             - a :py:class:`pandas.Timedelta` object
5606             - a :py:class:`datetime.timedelta` object
5607 
5608             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5609             dimensions has not been implemented yet. Gap length is defined as the difference
5610             between coordinate values at the first data point after a gap and the last value
5611             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5612             between coordinate values at the first (last) valid data point and the first (last) NaN.
5613             For example, consider::
5614 
5615                 <xarray.DataArray (x: 9)>
5616                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5617                 Coordinates:
5618                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5619 
5620             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5621         **kwargs : dict, optional
5622             parameters passed verbatim to the underlying interpolation function
5623 
5624         Returns
5625         -------
5626         interpolated: Dataset
5627             Filled in Dataset.
5628 
5629         See Also
5630         --------
5631         numpy.interp
5632         scipy.interpolate
5633 
5634         Examples
5635         --------
5636         >>> ds = xr.Dataset(
5637         ...     {
5638         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5639         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5640         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5641         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5642         ...     },
5643         ...     coords={"x": [0, 1, 2, 3, 4]},
5644         ... )
5645         >>> ds
5646         <xarray.Dataset>
5647         Dimensions:  (x: 5)
5648         Coordinates:
5649           * x        (x) int64 0 1 2 3 4
5650         Data variables:
5651             A        (x) float64 nan 2.0 3.0 nan 0.0
5652             B        (x) float64 3.0 4.0 nan 1.0 7.0
5653             C        (x) float64 nan nan nan 5.0 0.0
5654             D        (x) float64 nan 3.0 nan -1.0 4.0
5655 
5656         >>> ds.interpolate_na(dim="x", method="linear")
5657         <xarray.Dataset>
5658         Dimensions:  (x: 5)
5659         Coordinates:
5660           * x        (x) int64 0 1 2 3 4
5661         Data variables:
5662             A        (x) float64 nan 2.0 3.0 1.5 0.0
5663             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5664             C        (x) float64 nan nan nan 5.0 0.0
5665             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5666 
5667         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5668         <xarray.Dataset>
5669         Dimensions:  (x: 5)
5670         Coordinates:
5671           * x        (x) int64 0 1 2 3 4
5672         Data variables:
5673             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5674             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5675             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5676             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5677         """
5678         from .missing import _apply_over_vars_with_dim, interp_na
5679 
5680         new = _apply_over_vars_with_dim(
5681             interp_na,
5682             self,
5683             dim=dim,
5684             method=method,
5685             limit=limit,
5686             use_coordinate=use_coordinate,
5687             max_gap=max_gap,
5688             **kwargs,
5689         )
5690         return new
5691 
5692     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5693         """Fill NaN values by propagating values forward
5694 
5695         *Requires bottleneck.*
5696 
5697         Parameters
5698         ----------
5699         dim : Hashable
5700             Specifies the dimension along which to propagate values when
5701             filling.
5702         limit : int or None, optional
5703             The maximum number of consecutive NaN values to forward fill. In
5704             other words, if there is a gap with more than this number of
5705             consecutive NaNs, it will only be partially filled. Must be greater
5706             than 0 or None for no limit. Must be None or greater than or equal
5707             to axis length if filling along chunked axes (dimensions).
5708 
5709         Returns
5710         -------
5711         Dataset
5712         """
5713         from .missing import _apply_over_vars_with_dim, ffill
5714 
5715         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5716         return new
5717 
5718     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5719         """Fill NaN values by propagating values backward
5720 
5721         *Requires bottleneck.*
5722 
5723         Parameters
5724         ----------
5725         dim : Hashable
5726             Specifies the dimension along which to propagate values when
5727             filling.
5728         limit : int or None, optional
5729             The maximum number of consecutive NaN values to backward fill. In
5730             other words, if there is a gap with more than this number of
5731             consecutive NaNs, it will only be partially filled. Must be greater
5732             than 0 or None for no limit. Must be None or greater than or equal
5733             to axis length if filling along chunked axes (dimensions).
5734 
5735         Returns
5736         -------
5737         Dataset
5738         """
5739         from .missing import _apply_over_vars_with_dim, bfill
5740 
5741         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5742         return new
5743 
5744     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5745         """Combine two Datasets, default to data_vars of self.
5746 
5747         The new coordinates follow the normal broadcasting and alignment rules
5748         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5749         filled with np.nan.
5750 
5751         Parameters
5752         ----------
5753         other : Dataset
5754             Used to fill all matching missing values in this array.
5755 
5756         Returns
5757         -------
5758         Dataset
5759         """
5760         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5761         return out
5762 
5763     def reduce(
5764         self: T_Dataset,
5765         func: Callable,
5766         dim: Hashable | Iterable[Hashable] = None,
5767         *,
5768         keep_attrs: bool | None = None,
5769         keepdims: bool = False,
5770         numeric_only: bool = False,
5771         **kwargs: Any,
5772     ) -> T_Dataset:
5773         """Reduce this dataset by applying `func` along some dimension(s).
5774 
5775         Parameters
5776         ----------
5777         func : callable
5778             Function which can be called in the form
5779             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5780             np.ndarray over an integer valued axis.
5781         dim : str or sequence of str, optional
5782             Dimension(s) over which to apply `func`.  By default `func` is
5783             applied over all dimensions.
5784         keep_attrs : bool or None, optional
5785             If True, the dataset's attributes (`attrs`) will be copied from
5786             the original object to the new one.  If False (default), the new
5787             object will be returned without attributes.
5788         keepdims : bool, default: False
5789             If True, the dimensions which are reduced are left in the result
5790             as dimensions of size one. Coordinates that use these dimensions
5791             are removed.
5792         numeric_only : bool, default: False
5793             If True, only apply ``func`` to variables with a numeric dtype.
5794         **kwargs : Any
5795             Additional keyword arguments passed on to ``func``.
5796 
5797         Returns
5798         -------
5799         reduced : Dataset
5800             Dataset with this object's DataArrays replaced with new DataArrays
5801             of summarized data and the indicated dimension(s) removed.
5802         """
5803         if kwargs.get("axis", None) is not None:
5804             raise ValueError(
5805                 "passing 'axis' to Dataset reduce methods is ambiguous."
5806                 " Please use 'dim' instead."
5807             )
5808 
5809         if dim is None or dim is ...:
5810             dims = set(self.dims)
5811         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5812             dims = {dim}
5813         else:
5814             dims = set(dim)
5815 
5816         missing_dimensions = [d for d in dims if d not in self.dims]
5817         if missing_dimensions:
5818             raise ValueError(
5819                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5820             )
5821 
5822         if keep_attrs is None:
5823             keep_attrs = _get_keep_attrs(default=False)
5824 
5825         variables: dict[Hashable, Variable] = {}
5826         for name, var in self._variables.items():
5827             reduce_dims = [d for d in var.dims if d in dims]
5828             if name in self.coords:
5829                 if not reduce_dims:
5830                     variables[name] = var
5831             else:
5832                 if (
5833                     # Some reduction functions (e.g. std, var) need to run on variables
5834                     # that don't have the reduce dims: PR5393
5835                     not reduce_dims
5836                     or not numeric_only
5837                     or np.issubdtype(var.dtype, np.number)
5838                     or (var.dtype == np.bool_)
5839                 ):
5840                     reduce_maybe_single: Hashable | None | list[Hashable]
5841                     if len(reduce_dims) == 1:
5842                         # unpack dimensions for the benefit of functions
5843                         # like np.argmin which can't handle tuple arguments
5844                         (reduce_maybe_single,) = reduce_dims
5845                     elif len(reduce_dims) == var.ndim:
5846                         # prefer to aggregate over axis=None rather than
5847                         # axis=(0, 1) if they will be equivalent, because
5848                         # the former is often more efficient
5849                         reduce_maybe_single = None
5850                     else:
5851                         reduce_maybe_single = reduce_dims
5852                     variables[name] = var.reduce(
5853                         func,
5854                         dim=reduce_maybe_single,
5855                         keep_attrs=keep_attrs,
5856                         keepdims=keepdims,
5857                         **kwargs,
5858                     )
5859 
5860         coord_names = {k for k in self.coords if k in variables}
5861         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5862         attrs = self.attrs if keep_attrs else None
5863         return self._replace_with_new_dims(
5864             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5865         )
5866 
5867     def map(
5868         self: T_Dataset,
5869         func: Callable,
5870         keep_attrs: bool | None = None,
5871         args: Iterable[Any] = (),
5872         **kwargs: Any,
5873     ) -> T_Dataset:
5874         """Apply a function to each data variable in this dataset
5875 
5876         Parameters
5877         ----------
5878         func : callable
5879             Function which can be called in the form `func(x, *args, **kwargs)`
5880             to transform each DataArray `x` in this dataset into another
5881             DataArray.
5882         keep_attrs : bool or None, optional
5883             If True, both the dataset's and variables' attributes (`attrs`) will be
5884             copied from the original objects to the new ones. If False, the new dataset
5885             and variables will be returned without copying the attributes.
5886         args : iterable, optional
5887             Positional arguments passed on to `func`.
5888         **kwargs : Any
5889             Keyword arguments passed on to `func`.
5890 
5891         Returns
5892         -------
5893         applied : Dataset
5894             Resulting dataset from applying ``func`` to each data variable.
5895 
5896         Examples
5897         --------
5898         >>> da = xr.DataArray(np.random.randn(2, 3))
5899         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5900         >>> ds
5901         <xarray.Dataset>
5902         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5903         Dimensions without coordinates: dim_0, dim_1, x
5904         Data variables:
5905             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5906             bar      (x) int64 -1 2
5907         >>> ds.map(np.fabs)
5908         <xarray.Dataset>
5909         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5910         Dimensions without coordinates: dim_0, dim_1, x
5911         Data variables:
5912             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5913             bar      (x) float64 1.0 2.0
5914         """
5915         if keep_attrs is None:
5916             keep_attrs = _get_keep_attrs(default=False)
5917         variables = {
5918             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5919             for k, v in self.data_vars.items()
5920         }
5921         if keep_attrs:
5922             for k, v in variables.items():
5923                 v._copy_attrs_from(self.data_vars[k])
5924         attrs = self.attrs if keep_attrs else None
5925         return type(self)(variables, attrs=attrs)
5926 
5927     def apply(
5928         self: T_Dataset,
5929         func: Callable,
5930         keep_attrs: bool | None = None,
5931         args: Iterable[Any] = (),
5932         **kwargs: Any,
5933     ) -> T_Dataset:
5934         """
5935         Backward compatible implementation of ``map``
5936 
5937         See Also
5938         --------
5939         Dataset.map
5940         """
5941         warnings.warn(
5942             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5943             PendingDeprecationWarning,
5944             stacklevel=2,
5945         )
5946         return self.map(func, keep_attrs, args, **kwargs)
5947 
5948     def assign(
5949         self: T_Dataset,
5950         variables: Mapping[Any, Any] | None = None,
5951         **variables_kwargs: Any,
5952     ) -> T_Dataset:
5953         """Assign new data variables to a Dataset, returning a new object
5954         with all the original variables in addition to the new ones.
5955 
5956         Parameters
5957         ----------
5958         variables : mapping of hashable to Any
5959             Mapping from variables names to the new values. If the new values
5960             are callable, they are computed on the Dataset and assigned to new
5961             data variables. If the values are not callable, (e.g. a DataArray,
5962             scalar, or array), they are simply assigned.
5963         **variables_kwargs
5964             The keyword arguments form of ``variables``.
5965             One of variables or variables_kwargs must be provided.
5966 
5967         Returns
5968         -------
5969         ds : Dataset
5970             A new Dataset with the new variables in addition to all the
5971             existing variables.
5972 
5973         Notes
5974         -----
5975         Since ``kwargs`` is a dictionary, the order of your arguments may not
5976         be preserved, and so the order of the new variables is not well
5977         defined. Assigning multiple variables within the same ``assign`` is
5978         possible, but you cannot reference other variables created within the
5979         same ``assign`` call.
5980 
5981         See Also
5982         --------
5983         pandas.DataFrame.assign
5984 
5985         Examples
5986         --------
5987         >>> x = xr.Dataset(
5988         ...     {
5989         ...         "temperature_c": (
5990         ...             ("lat", "lon"),
5991         ...             20 * np.random.rand(4).reshape(2, 2),
5992         ...         ),
5993         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5994         ...     },
5995         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5996         ... )
5997         >>> x
5998         <xarray.Dataset>
5999         Dimensions:        (lat: 2, lon: 2)
6000         Coordinates:
6001           * lat            (lat) int64 10 20
6002           * lon            (lon) int64 150 160
6003         Data variables:
6004             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6005             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6006 
6007         Where the value is a callable, evaluated on dataset:
6008 
6009         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6010         <xarray.Dataset>
6011         Dimensions:        (lat: 2, lon: 2)
6012         Coordinates:
6013           * lat            (lat) int64 10 20
6014           * lon            (lon) int64 150 160
6015         Data variables:
6016             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6017             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6018             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6019 
6020         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6021 
6022         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6023         <xarray.Dataset>
6024         Dimensions:        (lat: 2, lon: 2)
6025         Coordinates:
6026           * lat            (lat) int64 10 20
6027           * lon            (lon) int64 150 160
6028         Data variables:
6029             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6030             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6031             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6032 
6033         """
6034         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6035         data = self.copy()
6036         # do all calculations first...
6037         results: CoercibleMapping = data._calc_assign_results(variables)
6038         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6039         # ... and then assign
6040         data.update(results)
6041         return data
6042 
6043     def to_array(
6044         self, dim: Hashable = "variable", name: Hashable | None = None
6045     ) -> DataArray:
6046         """Convert this dataset into an xarray.DataArray
6047 
6048         The data variables of this dataset will be broadcast against each other
6049         and stacked along the first axis of the new array. All coordinates of
6050         this dataset will remain coordinates.
6051 
6052         Parameters
6053         ----------
6054         dim : Hashable, default: "variable"
6055             Name of the new dimension.
6056         name : Hashable or None, optional
6057             Name of the new data array.
6058 
6059         Returns
6060         -------
6061         array : xarray.DataArray
6062         """
6063         from .dataarray import DataArray
6064 
6065         data_vars = [self.variables[k] for k in self.data_vars]
6066         broadcast_vars = broadcast_variables(*data_vars)
6067         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6068 
6069         dims = (dim,) + broadcast_vars[0].dims
6070         variable = Variable(dims, data, self.attrs, fastpath=True)
6071 
6072         coords = {k: v.variable for k, v in self.coords.items()}
6073         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6074         new_dim_index = PandasIndex(list(self.data_vars), dim)
6075         indexes[dim] = new_dim_index
6076         coords.update(new_dim_index.create_variables())
6077 
6078         return DataArray._construct_direct(variable, coords, name, indexes)
6079 
6080     def _normalize_dim_order(
6081         self, dim_order: Sequence[Hashable] | None = None
6082     ) -> dict[Hashable, int]:
6083         """
6084         Check the validity of the provided dimensions if any and return the mapping
6085         between dimension name and their size.
6086 
6087         Parameters
6088         ----------
6089         dim_order: Sequence of Hashable or None, optional
6090             Dimension order to validate (default to the alphabetical order if None).
6091 
6092         Returns
6093         -------
6094         result : dict[Hashable, int]
6095             Validated dimensions mapping.
6096 
6097         """
6098         if dim_order is None:
6099             dim_order = list(self.dims)
6100         elif set(dim_order) != set(self.dims):
6101             raise ValueError(
6102                 "dim_order {} does not match the set of dimensions of this "
6103                 "Dataset: {}".format(dim_order, list(self.dims))
6104             )
6105 
6106         ordered_dims = {k: self.dims[k] for k in dim_order}
6107 
6108         return ordered_dims
6109 
6110     def to_pandas(self) -> pd.Series | pd.DataFrame:
6111         """Convert this dataset into a pandas object without changing the number of dimensions.
6112 
6113         The type of the returned object depends on the number of Dataset
6114         dimensions:
6115 
6116         * 0D -> `pandas.Series`
6117         * 1D -> `pandas.DataFrame`
6118 
6119         Only works for Datasets with 1 or fewer dimensions.
6120         """
6121         if len(self.dims) == 0:
6122             return pd.Series({k: v.item() for k, v in self.items()})
6123         if len(self.dims) == 1:
6124             return self.to_dataframe()
6125         raise ValueError(
6126             "cannot convert Datasets with %s dimensions into "
6127             "pandas objects without changing the number of dimensions. "
6128             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6129         )
6130 
6131     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6132         columns = [k for k in self.variables if k not in self.dims]
6133         data = [
6134             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6135             for k in columns
6136         ]
6137         index = self.coords.to_index([*ordered_dims])
6138         return pd.DataFrame(dict(zip(columns, data)), index=index)
6139 
6140     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6141         """Convert this dataset into a pandas.DataFrame.
6142 
6143         Non-index variables in this dataset form the columns of the
6144         DataFrame. The DataFrame is indexed by the Cartesian product of
6145         this dataset's indices.
6146 
6147         Parameters
6148         ----------
6149         dim_order: Sequence of Hashable or None, optional
6150             Hierarchical dimension order for the resulting dataframe. All
6151             arrays are transposed to this order and then written out as flat
6152             vectors in contiguous order, so the last dimension in this list
6153             will be contiguous in the resulting DataFrame. This has a major
6154             influence on which operations are efficient on the resulting
6155             dataframe.
6156 
6157             If provided, must include all dimensions of this dataset. By
6158             default, dimensions are sorted alphabetically.
6159 
6160         Returns
6161         -------
6162         result : DataFrame
6163             Dataset as a pandas DataFrame.
6164 
6165         """
6166 
6167         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6168 
6169         return self._to_dataframe(ordered_dims=ordered_dims)
6170 
6171     def _set_sparse_data_from_dataframe(
6172         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6173     ) -> None:
6174         from sparse import COO
6175 
6176         if isinstance(idx, pd.MultiIndex):
6177             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6178             is_sorted = idx.is_monotonic_increasing
6179             shape = tuple(lev.size for lev in idx.levels)
6180         else:
6181             coords = np.arange(idx.size).reshape(1, -1)
6182             is_sorted = True
6183             shape = (idx.size,)
6184 
6185         for name, values in arrays:
6186             # In virtually all real use cases, the sparse array will now have
6187             # missing values and needs a fill_value. For consistency, don't
6188             # special case the rare exceptions (e.g., dtype=int without a
6189             # MultiIndex).
6190             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6191             values = np.asarray(values, dtype=dtype)
6192 
6193             data = COO(
6194                 coords,
6195                 values,
6196                 shape,
6197                 has_duplicates=False,
6198                 sorted=is_sorted,
6199                 fill_value=fill_value,
6200             )
6201             self[name] = (dims, data)
6202 
6203     def _set_numpy_data_from_dataframe(
6204         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6205     ) -> None:
6206         if not isinstance(idx, pd.MultiIndex):
6207             for name, values in arrays:
6208                 self[name] = (dims, values)
6209             return
6210 
6211         # NB: similar, more general logic, now exists in
6212         # variable.unstack_once; we could consider combining them at some
6213         # point.
6214 
6215         shape = tuple(lev.size for lev in idx.levels)
6216         indexer = tuple(idx.codes)
6217 
6218         # We already verified that the MultiIndex has all unique values, so
6219         # there are missing values if and only if the size of output arrays is
6220         # larger that the index.
6221         missing_values = math.prod(shape) > idx.shape[0]
6222 
6223         for name, values in arrays:
6224             # NumPy indexing is much faster than using DataFrame.reindex() to
6225             # fill in missing values:
6226             # https://stackoverflow.com/a/35049899/809705
6227             if missing_values:
6228                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6229                 data = np.full(shape, fill_value, dtype)
6230             else:
6231                 # If there are no missing values, keep the existing dtype
6232                 # instead of promoting to support NA, e.g., keep integer
6233                 # columns as integers.
6234                 # TODO: consider removing this special case, which doesn't
6235                 # exist for sparse=True.
6236                 data = np.zeros(shape, values.dtype)
6237             data[indexer] = values
6238             self[name] = (dims, data)
6239 
6240     @classmethod
6241     def from_dataframe(
6242         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6243     ) -> T_Dataset:
6244         """Convert a pandas.DataFrame into an xarray.Dataset
6245 
6246         Each column will be converted into an independent variable in the
6247         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6248         into a tensor product of one-dimensional indices (filling in missing
6249         values with NaN). This method will produce a Dataset very similar to
6250         that on which the 'to_dataframe' method was called, except with
6251         possibly redundant dimensions (since all dataset variables will have
6252         the same dimensionality)
6253 
6254         Parameters
6255         ----------
6256         dataframe : DataFrame
6257             DataFrame from which to copy data and indices.
6258         sparse : bool, default: False
6259             If true, create a sparse arrays instead of dense numpy arrays. This
6260             can potentially save a large amount of memory if the DataFrame has
6261             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6262 
6263         Returns
6264         -------
6265         New Dataset.
6266 
6267         See Also
6268         --------
6269         xarray.DataArray.from_series
6270         pandas.DataFrame.to_xarray
6271         """
6272         # TODO: Add an option to remove dimensions along which the variables
6273         # are constant, to enable consistent serialization to/from a dataframe,
6274         # even if some variables have different dimensionality.
6275 
6276         if not dataframe.columns.is_unique:
6277             raise ValueError("cannot convert DataFrame with non-unique columns")
6278 
6279         idx = remove_unused_levels_categories(dataframe.index)
6280 
6281         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6282             raise ValueError(
6283                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6284             )
6285 
6286         # Cast to a NumPy array first, in case the Series is a pandas Extension
6287         # array (which doesn't have a valid NumPy dtype)
6288         # TODO: allow users to control how this casting happens, e.g., by
6289         # forwarding arguments to pandas.Series.to_numpy?
6290         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6291 
6292         indexes: dict[Hashable, Index] = {}
6293         index_vars: dict[Hashable, Variable] = {}
6294 
6295         if isinstance(idx, pd.MultiIndex):
6296             dims = tuple(
6297                 name if name is not None else "level_%i" % n
6298                 for n, name in enumerate(idx.names)
6299             )
6300             for dim, lev in zip(dims, idx.levels):
6301                 xr_idx = PandasIndex(lev, dim)
6302                 indexes[dim] = xr_idx
6303                 index_vars.update(xr_idx.create_variables())
6304         else:
6305             index_name = idx.name if idx.name is not None else "index"
6306             dims = (index_name,)
6307             xr_idx = PandasIndex(idx, index_name)
6308             indexes[index_name] = xr_idx
6309             index_vars.update(xr_idx.create_variables())
6310 
6311         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6312 
6313         if sparse:
6314             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6315         else:
6316             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6317         return obj
6318 
6319     def to_dask_dataframe(
6320         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6321     ) -> DaskDataFrame:
6322         """
6323         Convert this dataset into a dask.dataframe.DataFrame.
6324 
6325         The dimensions, coordinates and data variables in this dataset form
6326         the columns of the DataFrame.
6327 
6328         Parameters
6329         ----------
6330         dim_order : list, optional
6331             Hierarchical dimension order for the resulting dataframe. All
6332             arrays are transposed to this order and then written out as flat
6333             vectors in contiguous order, so the last dimension in this list
6334             will be contiguous in the resulting DataFrame. This has a major
6335             influence on which operations are efficient on the resulting dask
6336             dataframe.
6337 
6338             If provided, must include all dimensions of this dataset. By
6339             default, dimensions are sorted alphabetically.
6340         set_index : bool, default: False
6341             If set_index=True, the dask DataFrame is indexed by this dataset's
6342             coordinate. Since dask DataFrames do not support multi-indexes,
6343             set_index only works if the dataset only contains one dimension.
6344 
6345         Returns
6346         -------
6347         dask.dataframe.DataFrame
6348         """
6349 
6350         import dask.array as da
6351         import dask.dataframe as dd
6352 
6353         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6354 
6355         columns = list(ordered_dims)
6356         columns.extend(k for k in self.coords if k not in self.dims)
6357         columns.extend(self.data_vars)
6358 
6359         series_list = []
6360         for name in columns:
6361             try:
6362                 var = self.variables[name]
6363             except KeyError:
6364                 # dimension without a matching coordinate
6365                 size = self.dims[name]
6366                 data = da.arange(size, chunks=size, dtype=np.int64)
6367                 var = Variable((name,), data)
6368 
6369             # IndexVariable objects have a dummy .chunk() method
6370             if isinstance(var, IndexVariable):
6371                 var = var.to_base_variable()
6372 
6373             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6374             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6375             series_list.append(series)
6376 
6377         df = dd.concat(series_list, axis=1)
6378 
6379         if set_index:
6380             dim_order = [*ordered_dims]
6381 
6382             if len(dim_order) == 1:
6383                 (dim,) = dim_order
6384                 df = df.set_index(dim)
6385             else:
6386                 # triggers an error about multi-indexes, even if only one
6387                 # dimension is passed
6388                 df = df.set_index(dim_order)
6389 
6390         return df
6391 
6392     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6393         """
6394         Convert this dataset to a dictionary following xarray naming
6395         conventions.
6396 
6397         Converts all variables and attributes to native Python objects
6398         Useful for converting to json. To avoid datetime incompatibility
6399         use decode_times=False kwarg in xarrray.open_dataset.
6400 
6401         Parameters
6402         ----------
6403         data : bool, default: True
6404             Whether to include the actual data in the dictionary. When set to
6405             False, returns just the schema.
6406         encoding : bool, default: False
6407             Whether to include the Dataset's encoding in the dictionary.
6408 
6409         Returns
6410         -------
6411         d : dict
6412             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6413             "encoding".
6414 
6415         See Also
6416         --------
6417         Dataset.from_dict
6418         DataArray.to_dict
6419         """
6420         d: dict = {
6421             "coords": {},
6422             "attrs": decode_numpy_dict_values(self.attrs),
6423             "dims": dict(self.dims),
6424             "data_vars": {},
6425         }
6426         for k in self.coords:
6427             d["coords"].update(
6428                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6429             )
6430         for k in self.data_vars:
6431             d["data_vars"].update(
6432                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6433             )
6434         if encoding:
6435             d["encoding"] = dict(self.encoding)
6436         return d
6437 
6438     @classmethod
6439     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6440         """Convert a dictionary into an xarray.Dataset.
6441 
6442         Parameters
6443         ----------
6444         d : dict-like
6445             Mapping with a minimum structure of
6446                 ``{"var_0": {"dims": [..], "data": [..]}, \
6447                             ...}``
6448 
6449         Returns
6450         -------
6451         obj : Dataset
6452 
6453         See also
6454         --------
6455         Dataset.to_dict
6456         DataArray.from_dict
6457 
6458         Examples
6459         --------
6460         >>> d = {
6461         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6462         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6463         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6464         ... }
6465         >>> ds = xr.Dataset.from_dict(d)
6466         >>> ds
6467         <xarray.Dataset>
6468         Dimensions:  (t: 3)
6469         Coordinates:
6470           * t        (t) int64 0 1 2
6471         Data variables:
6472             a        (t) <U1 'a' 'b' 'c'
6473             b        (t) int64 10 20 30
6474 
6475         >>> d = {
6476         ...     "coords": {
6477         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6478         ...     },
6479         ...     "attrs": {"title": "air temperature"},
6480         ...     "dims": "t",
6481         ...     "data_vars": {
6482         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6483         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6484         ...     },
6485         ... }
6486         >>> ds = xr.Dataset.from_dict(d)
6487         >>> ds
6488         <xarray.Dataset>
6489         Dimensions:  (t: 3)
6490         Coordinates:
6491           * t        (t) int64 0 1 2
6492         Data variables:
6493             a        (t) int64 10 20 30
6494             b        (t) <U1 'a' 'b' 'c'
6495         Attributes:
6496             title:    air temperature
6497 
6498         """
6499 
6500         variables: Iterable[tuple[Hashable, Any]]
6501         if not {"coords", "data_vars"}.issubset(set(d)):
6502             variables = d.items()
6503         else:
6504             import itertools
6505 
6506             variables = itertools.chain(
6507                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6508             )
6509         try:
6510             variable_dict = {
6511                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6512             }
6513         except KeyError as e:
6514             raise ValueError(
6515                 "cannot convert dict without the key "
6516                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6517             )
6518         obj = cls(variable_dict)
6519 
6520         # what if coords aren't dims?
6521         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6522         obj = obj.set_coords(coords)
6523 
6524         obj.attrs.update(d.get("attrs", {}))
6525         obj.encoding.update(d.get("encoding", {}))
6526 
6527         return obj
6528 
6529     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6530         variables = {}
6531         keep_attrs = kwargs.pop("keep_attrs", None)
6532         if keep_attrs is None:
6533             keep_attrs = _get_keep_attrs(default=True)
6534         for k, v in self._variables.items():
6535             if k in self._coord_names:
6536                 variables[k] = v
6537             else:
6538                 variables[k] = f(v, *args, **kwargs)
6539                 if keep_attrs:
6540                     variables[k].attrs = v._attrs
6541         attrs = self._attrs if keep_attrs else None
6542         return self._replace_with_new_dims(variables, attrs=attrs)
6543 
6544     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6545         from .dataarray import DataArray
6546         from .groupby import GroupBy
6547 
6548         if isinstance(other, GroupBy):
6549             return NotImplemented
6550         align_type = OPTIONS["arithmetic_join"] if join is None else join
6551         if isinstance(other, (DataArray, Dataset)):
6552             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6553         g = f if not reflexive else lambda x, y: f(y, x)
6554         ds = self._calculate_binary_op(g, other, join=align_type)
6555         return ds
6556 
6557     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6558         from .dataarray import DataArray
6559         from .groupby import GroupBy
6560 
6561         if isinstance(other, GroupBy):
6562             raise TypeError(
6563                 "in-place operations between a Dataset and "
6564                 "a grouped object are not permitted"
6565             )
6566         # we don't actually modify arrays in-place with in-place Dataset
6567         # arithmetic -- this lets us automatically align things
6568         if isinstance(other, (DataArray, Dataset)):
6569             other = other.reindex_like(self, copy=False)
6570         g = ops.inplace_to_noninplace_op(f)
6571         ds = self._calculate_binary_op(g, other, inplace=True)
6572         self._replace_with_new_dims(
6573             ds._variables,
6574             ds._coord_names,
6575             attrs=ds._attrs,
6576             indexes=ds._indexes,
6577             inplace=True,
6578         )
6579         return self
6580 
6581     def _calculate_binary_op(
6582         self, f, other, join="inner", inplace: bool = False
6583     ) -> Dataset:
6584         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6585             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6586                 raise ValueError(
6587                     "datasets must have the same data variables "
6588                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6589                 )
6590 
6591             dest_vars = {}
6592 
6593             for k in lhs_data_vars:
6594                 if k in rhs_data_vars:
6595                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6596                 elif join in ["left", "outer"]:
6597                     dest_vars[k] = f(lhs_vars[k], np.nan)
6598             for k in rhs_data_vars:
6599                 if k not in dest_vars and join in ["right", "outer"]:
6600                     dest_vars[k] = f(rhs_vars[k], np.nan)
6601             return dest_vars
6602 
6603         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6604             # can't use our shortcut of doing the binary operation with
6605             # Variable objects, so apply over our data vars instead.
6606             new_data_vars = apply_over_both(
6607                 self.data_vars, other, self.data_vars, other
6608             )
6609             return type(self)(new_data_vars)
6610 
6611         other_coords: Coordinates | None = getattr(other, "coords", None)
6612         ds = self.coords.merge(other_coords)
6613 
6614         if isinstance(other, Dataset):
6615             new_vars = apply_over_both(
6616                 self.data_vars, other.data_vars, self.variables, other.variables
6617             )
6618         else:
6619             other_variable = getattr(other, "variable", other)
6620             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6621         ds._variables.update(new_vars)
6622         ds._dims = calculate_dimensions(ds._variables)
6623         return ds
6624 
6625     def _copy_attrs_from(self, other):
6626         self.attrs = other.attrs
6627         for v in other.variables:
6628             if v in self.variables:
6629                 self.variables[v].attrs = other.variables[v].attrs
6630 
6631     def diff(
6632         self: T_Dataset,
6633         dim: Hashable,
6634         n: int = 1,
6635         label: Literal["upper", "lower"] = "upper",
6636     ) -> T_Dataset:
6637         """Calculate the n-th order discrete difference along given axis.
6638 
6639         Parameters
6640         ----------
6641         dim : Hashable
6642             Dimension over which to calculate the finite difference.
6643         n : int, default: 1
6644             The number of times values are differenced.
6645         label : {"upper", "lower"}, default: "upper"
6646             The new coordinate in dimension ``dim`` will have the
6647             values of either the minuend's or subtrahend's coordinate
6648             for values 'upper' and 'lower', respectively.
6649 
6650         Returns
6651         -------
6652         difference : Dataset
6653             The n-th order finite difference of this object.
6654 
6655         Notes
6656         -----
6657         `n` matches numpy's behavior and is different from pandas' first argument named
6658         `periods`.
6659 
6660         Examples
6661         --------
6662         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6663         >>> ds.diff("x")
6664         <xarray.Dataset>
6665         Dimensions:  (x: 3)
6666         Dimensions without coordinates: x
6667         Data variables:
6668             foo      (x) int64 0 1 0
6669         >>> ds.diff("x", 2)
6670         <xarray.Dataset>
6671         Dimensions:  (x: 2)
6672         Dimensions without coordinates: x
6673         Data variables:
6674             foo      (x) int64 1 -1
6675 
6676         See Also
6677         --------
6678         Dataset.differentiate
6679         """
6680         if n == 0:
6681             return self
6682         if n < 0:
6683             raise ValueError(f"order `n` must be non-negative but got {n}")
6684 
6685         # prepare slices
6686         slice_start = {dim: slice(None, -1)}
6687         slice_end = {dim: slice(1, None)}
6688 
6689         # prepare new coordinate
6690         if label == "upper":
6691             slice_new = slice_end
6692         elif label == "lower":
6693             slice_new = slice_start
6694         else:
6695             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6696 
6697         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6698         variables = {}
6699 
6700         for name, var in self.variables.items():
6701             if name in index_vars:
6702                 variables[name] = index_vars[name]
6703             elif dim in var.dims:
6704                 if name in self.data_vars:
6705                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6706                 else:
6707                     variables[name] = var.isel(slice_new)
6708             else:
6709                 variables[name] = var
6710 
6711         difference = self._replace_with_new_dims(variables, indexes=indexes)
6712 
6713         if n > 1:
6714             return difference.diff(dim, n - 1)
6715         else:
6716             return difference
6717 
6718     def shift(
6719         self: T_Dataset,
6720         shifts: Mapping[Any, int] | None = None,
6721         fill_value: Any = xrdtypes.NA,
6722         **shifts_kwargs: int,
6723     ) -> T_Dataset:
6724 
6725         """Shift this dataset by an offset along one or more dimensions.
6726 
6727         Only data variables are moved; coordinates stay in place. This is
6728         consistent with the behavior of ``shift`` in pandas.
6729 
6730         Values shifted from beyond array bounds will appear at one end of
6731         each dimension, which are filled according to `fill_value`. For periodic
6732         offsets instead see `roll`.
6733 
6734         Parameters
6735         ----------
6736         shifts : mapping of hashable to int
6737             Integer offset to shift along each of the given dimensions.
6738             Positive offsets shift to the right; negative offsets shift to the
6739             left.
6740         fill_value : scalar or dict-like, optional
6741             Value to use for newly missing values. If a dict-like, maps
6742             variable names (including coordinates) to fill values.
6743         **shifts_kwargs
6744             The keyword arguments form of ``shifts``.
6745             One of shifts or shifts_kwargs must be provided.
6746 
6747         Returns
6748         -------
6749         shifted : Dataset
6750             Dataset with the same coordinates and attributes but shifted data
6751             variables.
6752 
6753         See Also
6754         --------
6755         roll
6756 
6757         Examples
6758         --------
6759         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6760         >>> ds.shift(x=2)
6761         <xarray.Dataset>
6762         Dimensions:  (x: 5)
6763         Dimensions without coordinates: x
6764         Data variables:
6765             foo      (x) object nan nan 'a' 'b' 'c'
6766         """
6767         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6768         invalid = [k for k in shifts if k not in self.dims]
6769         if invalid:
6770             raise ValueError(f"dimensions {invalid!r} do not exist")
6771 
6772         variables = {}
6773         for name, var in self.variables.items():
6774             if name in self.data_vars:
6775                 fill_value_ = (
6776                     fill_value.get(name, xrdtypes.NA)
6777                     if isinstance(fill_value, dict)
6778                     else fill_value
6779                 )
6780 
6781                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6782                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6783             else:
6784                 variables[name] = var
6785 
6786         return self._replace(variables)
6787 
6788     def roll(
6789         self: T_Dataset,
6790         shifts: Mapping[Any, int] | None = None,
6791         roll_coords: bool = False,
6792         **shifts_kwargs: int,
6793     ) -> T_Dataset:
6794         """Roll this dataset by an offset along one or more dimensions.
6795 
6796         Unlike shift, roll treats the given dimensions as periodic, so will not
6797         create any missing values to be filled.
6798 
6799         Also unlike shift, roll may rotate all variables, including coordinates
6800         if specified. The direction of rotation is consistent with
6801         :py:func:`numpy.roll`.
6802 
6803         Parameters
6804         ----------
6805         shifts : mapping of hashable to int, optional
6806             A dict with keys matching dimensions and values given
6807             by integers to rotate each of the given dimensions. Positive
6808             offsets roll to the right; negative offsets roll to the left.
6809         roll_coords : bool, default: False
6810             Indicates whether to roll the coordinates by the offset too.
6811         **shifts_kwargs : {dim: offset, ...}, optional
6812             The keyword arguments form of ``shifts``.
6813             One of shifts or shifts_kwargs must be provided.
6814 
6815         Returns
6816         -------
6817         rolled : Dataset
6818             Dataset with the same attributes but rolled data and coordinates.
6819 
6820         See Also
6821         --------
6822         shift
6823 
6824         Examples
6825         --------
6826         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6827         >>> ds.roll(x=2)
6828         <xarray.Dataset>
6829         Dimensions:  (x: 5)
6830         Coordinates:
6831           * x        (x) int64 0 1 2 3 4
6832         Data variables:
6833             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6834 
6835         >>> ds.roll(x=2, roll_coords=True)
6836         <xarray.Dataset>
6837         Dimensions:  (x: 5)
6838         Coordinates:
6839           * x        (x) int64 3 4 0 1 2
6840         Data variables:
6841             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6842 
6843         """
6844         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6845         invalid = [k for k in shifts if k not in self.dims]
6846         if invalid:
6847             raise ValueError(f"dimensions {invalid!r} do not exist")
6848 
6849         unrolled_vars: tuple[Hashable, ...]
6850 
6851         if roll_coords:
6852             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6853             unrolled_vars = ()
6854         else:
6855             indexes = dict(self._indexes)
6856             index_vars = dict(self.xindexes.variables)
6857             unrolled_vars = tuple(self.coords)
6858 
6859         variables = {}
6860         for k, var in self.variables.items():
6861             if k in index_vars:
6862                 variables[k] = index_vars[k]
6863             elif k not in unrolled_vars:
6864                 variables[k] = var.roll(
6865                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6866                 )
6867             else:
6868                 variables[k] = var
6869 
6870         return self._replace(variables, indexes=indexes)
6871 
6872     def sortby(
6873         self: T_Dataset,
6874         variables: Hashable | DataArray | list[Hashable | DataArray],
6875         ascending: bool = True,
6876     ) -> T_Dataset:
6877         """
6878         Sort object by labels or values (along an axis).
6879 
6880         Sorts the dataset, either along specified dimensions,
6881         or according to values of 1-D dataarrays that share dimension
6882         with calling object.
6883 
6884         If the input variables are dataarrays, then the dataarrays are aligned
6885         (via left-join) to the calling object prior to sorting by cell values.
6886         NaNs are sorted to the end, following Numpy convention.
6887 
6888         If multiple sorts along the same dimension is
6889         given, numpy's lexsort is performed along that dimension:
6890         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6891         and the FIRST key in the sequence is used as the primary sort key,
6892         followed by the 2nd key, etc.
6893 
6894         Parameters
6895         ----------
6896         variables : Hashable, DataArray, or list of hashable or DataArray
6897             1D DataArray objects or name(s) of 1D variable(s) in
6898             coords/data_vars whose values are used to sort the dataset.
6899         ascending : bool, default: True
6900             Whether to sort by ascending or descending order.
6901 
6902         Returns
6903         -------
6904         sorted : Dataset
6905             A new dataset where all the specified dims are sorted by dim
6906             labels.
6907 
6908         See Also
6909         --------
6910         DataArray.sortby
6911         numpy.sort
6912         pandas.sort_values
6913         pandas.sort_index
6914 
6915         Examples
6916         --------
6917         >>> ds = xr.Dataset(
6918         ...     {
6919         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6920         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6921         ...     },
6922         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6923         ... )
6924         >>> ds.sortby("x")
6925         <xarray.Dataset>
6926         Dimensions:  (x: 2, y: 2)
6927         Coordinates:
6928           * x        (x) <U1 'a' 'b'
6929           * y        (y) int64 1 0
6930         Data variables:
6931             A        (x, y) int64 3 4 1 2
6932             B        (x, y) int64 7 8 5 6
6933         """
6934         from .dataarray import DataArray
6935 
6936         if not isinstance(variables, list):
6937             variables = [variables]
6938         else:
6939             variables = variables
6940         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6941         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6942         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6943         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6944         vars_by_dim = defaultdict(list)
6945         for data_array in aligned_other_vars:
6946             if data_array.ndim != 1:
6947                 raise ValueError("Input DataArray is not 1-D.")
6948             (key,) = data_array.dims
6949             vars_by_dim[key].append(data_array)
6950 
6951         indices = {}
6952         for key, arrays in vars_by_dim.items():
6953             order = np.lexsort(tuple(reversed(arrays)))
6954             indices[key] = order if ascending else order[::-1]
6955         return aligned_self.isel(indices)
6956 
6957     def quantile(
6958         self: T_Dataset,
6959         q: ArrayLike,
6960         dim: str | Iterable[Hashable] | None = None,
6961         method: QUANTILE_METHODS = "linear",
6962         numeric_only: bool = False,
6963         keep_attrs: bool = None,
6964         skipna: bool = None,
6965         interpolation: QUANTILE_METHODS = None,
6966     ) -> T_Dataset:
6967         """Compute the qth quantile of the data along the specified dimension.
6968 
6969         Returns the qth quantiles(s) of the array elements for each variable
6970         in the Dataset.
6971 
6972         Parameters
6973         ----------
6974         q : float or array-like of float
6975             Quantile to compute, which must be between 0 and 1 inclusive.
6976         dim : str or Iterable of Hashable, optional
6977             Dimension(s) over which to apply quantile.
6978         method : str, default: "linear"
6979             This optional parameter specifies the interpolation method to use when the
6980             desired quantile lies between two data points. The options sorted by their R
6981             type as summarized in the H&F paper [1]_ are:
6982 
6983                 1. "inverted_cdf" (*)
6984                 2. "averaged_inverted_cdf" (*)
6985                 3. "closest_observation" (*)
6986                 4. "interpolated_inverted_cdf" (*)
6987                 5. "hazen" (*)
6988                 6. "weibull" (*)
6989                 7. "linear"  (default)
6990                 8. "median_unbiased" (*)
6991                 9. "normal_unbiased" (*)
6992 
6993             The first three methods are discontiuous.  The following discontinuous
6994             variations of the default "linear" (7.) option are also available:
6995 
6996                 * "lower"
6997                 * "higher"
6998                 * "midpoint"
6999                 * "nearest"
7000 
7001             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7002             was previously called "interpolation", renamed in accordance with numpy
7003             version 1.22.0.
7004 
7005             (*) These methods require numpy version 1.22 or newer.
7006 
7007         keep_attrs : bool, optional
7008             If True, the dataset's attributes (`attrs`) will be copied from
7009             the original object to the new one.  If False (default), the new
7010             object will be returned without attributes.
7011         numeric_only : bool, optional
7012             If True, only apply ``func`` to variables with a numeric dtype.
7013         skipna : bool, optional
7014             If True, skip missing values (as marked by NaN). By default, only
7015             skips missing values for float dtypes; other dtypes either do not
7016             have a sentinel missing value (int) or skipna=True has not been
7017             implemented (object, datetime64 or timedelta64).
7018 
7019         Returns
7020         -------
7021         quantiles : Dataset
7022             If `q` is a single quantile, then the result is a scalar for each
7023             variable in data_vars. If multiple percentiles are given, first
7024             axis of the result corresponds to the quantile and a quantile
7025             dimension is added to the return Dataset. The other dimensions are
7026             the dimensions that remain after the reduction of the array.
7027 
7028         See Also
7029         --------
7030         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7031 
7032         Examples
7033         --------
7034         >>> ds = xr.Dataset(
7035         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7036         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7037         ... )
7038         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7039         <xarray.Dataset>
7040         Dimensions:   ()
7041         Coordinates:
7042             quantile  float64 0.0
7043         Data variables:
7044             a         float64 0.7
7045         >>> ds.quantile(0, dim="x")
7046         <xarray.Dataset>
7047         Dimensions:   (y: 4)
7048         Coordinates:
7049           * y         (y) float64 1.0 1.5 2.0 2.5
7050             quantile  float64 0.0
7051         Data variables:
7052             a         (y) float64 0.7 4.2 2.6 1.5
7053         >>> ds.quantile([0, 0.5, 1])
7054         <xarray.Dataset>
7055         Dimensions:   (quantile: 3)
7056         Coordinates:
7057           * quantile  (quantile) float64 0.0 0.5 1.0
7058         Data variables:
7059             a         (quantile) float64 0.7 3.4 9.4
7060         >>> ds.quantile([0, 0.5, 1], dim="x")
7061         <xarray.Dataset>
7062         Dimensions:   (quantile: 3, y: 4)
7063         Coordinates:
7064           * y         (y) float64 1.0 1.5 2.0 2.5
7065           * quantile  (quantile) float64 0.0 0.5 1.0
7066         Data variables:
7067             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7068 
7069         References
7070         ----------
7071         .. [1] R. J. Hyndman and Y. Fan,
7072            "Sample quantiles in statistical packages,"
7073            The American Statistician, 50(4), pp. 361-365, 1996
7074         """
7075 
7076         # interpolation renamed to method in version 0.21.0
7077         # check here and in variable to avoid repeated warnings
7078         if interpolation is not None:
7079             warnings.warn(
7080                 "The `interpolation` argument to quantile was renamed to `method`.",
7081                 FutureWarning,
7082             )
7083 
7084             if method != "linear":
7085                 raise TypeError("Cannot pass interpolation and method keywords!")
7086 
7087             method = interpolation
7088 
7089         dims: set[Hashable]
7090         if isinstance(dim, str):
7091             dims = {dim}
7092         elif dim is None or dim is ...:
7093             dims = set(self.dims)
7094         else:
7095             dims = set(dim)
7096 
7097         _assert_empty(
7098             tuple(d for d in dims if d not in self.dims),
7099             "Dataset does not contain the dimensions: %s",
7100         )
7101 
7102         q = np.asarray(q, dtype=np.float64)
7103 
7104         variables = {}
7105         for name, var in self.variables.items():
7106             reduce_dims = [d for d in var.dims if d in dims]
7107             if reduce_dims or not var.dims:
7108                 if name not in self.coords:
7109                     if (
7110                         not numeric_only
7111                         or np.issubdtype(var.dtype, np.number)
7112                         or var.dtype == np.bool_
7113                     ):
7114                         variables[name] = var.quantile(
7115                             q,
7116                             dim=reduce_dims,
7117                             method=method,
7118                             keep_attrs=keep_attrs,
7119                             skipna=skipna,
7120                         )
7121 
7122             else:
7123                 variables[name] = var
7124 
7125         # construct the new dataset
7126         coord_names = {k for k in self.coords if k in variables}
7127         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7128         if keep_attrs is None:
7129             keep_attrs = _get_keep_attrs(default=False)
7130         attrs = self.attrs if keep_attrs else None
7131         new = self._replace_with_new_dims(
7132             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7133         )
7134         return new.assign_coords(quantile=q)
7135 
7136     def rank(
7137         self: T_Dataset,
7138         dim: Hashable,
7139         pct: bool = False,
7140         keep_attrs: bool | None = None,
7141     ) -> T_Dataset:
7142         """Ranks the data.
7143 
7144         Equal values are assigned a rank that is the average of the ranks that
7145         would have been otherwise assigned to all of the values within
7146         that set.
7147         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7148 
7149         NaNs in the input array are returned as NaNs.
7150 
7151         The `bottleneck` library is required.
7152 
7153         Parameters
7154         ----------
7155         dim : Hashable
7156             Dimension over which to compute rank.
7157         pct : bool, default: False
7158             If True, compute percentage ranks, otherwise compute integer ranks.
7159         keep_attrs : bool or None, optional
7160             If True, the dataset's attributes (`attrs`) will be copied from
7161             the original object to the new one.  If False, the new
7162             object will be returned without attributes.
7163 
7164         Returns
7165         -------
7166         ranked : Dataset
7167             Variables that do not depend on `dim` are dropped.
7168         """
7169         if not OPTIONS["use_bottleneck"]:
7170             raise RuntimeError(
7171                 "rank requires bottleneck to be enabled."
7172                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7173             )
7174 
7175         if dim not in self.dims:
7176             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7177 
7178         variables = {}
7179         for name, var in self.variables.items():
7180             if name in self.data_vars:
7181                 if dim in var.dims:
7182                     variables[name] = var.rank(dim, pct=pct)
7183             else:
7184                 variables[name] = var
7185 
7186         coord_names = set(self.coords)
7187         if keep_attrs is None:
7188             keep_attrs = _get_keep_attrs(default=False)
7189         attrs = self.attrs if keep_attrs else None
7190         return self._replace(variables, coord_names, attrs=attrs)
7191 
7192     def differentiate(
7193         self: T_Dataset,
7194         coord: Hashable,
7195         edge_order: Literal[1, 2] = 1,
7196         datetime_unit: DatetimeUnitOptions | None = None,
7197     ) -> T_Dataset:
7198         """ Differentiate with the second order accurate central
7199         differences.
7200 
7201         .. note::
7202             This feature is limited to simple cartesian geometry, i.e. coord
7203             must be one dimensional.
7204 
7205         Parameters
7206         ----------
7207         coord : Hashable
7208             The coordinate to be used to compute the gradient.
7209         edge_order : {1, 2}, default: 1
7210             N-th order accurate differences at the boundaries.
7211         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7212             "us", "ns", "ps", "fs", "as", None}, default: None
7213             Unit to compute gradient. Only valid for datetime coordinate.
7214 
7215         Returns
7216         -------
7217         differentiated: Dataset
7218 
7219         See also
7220         --------
7221         numpy.gradient: corresponding numpy function
7222         """
7223         from .variable import Variable
7224 
7225         if coord not in self.variables and coord not in self.dims:
7226             raise ValueError(f"Coordinate {coord} does not exist.")
7227 
7228         coord_var = self[coord].variable
7229         if coord_var.ndim != 1:
7230             raise ValueError(
7231                 "Coordinate {} must be 1 dimensional but is {}"
7232                 " dimensional".format(coord, coord_var.ndim)
7233             )
7234 
7235         dim = coord_var.dims[0]
7236         if _contains_datetime_like_objects(coord_var):
7237             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7238                 datetime_unit = cast(
7239                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7240                 )
7241             elif datetime_unit is None:
7242                 datetime_unit = "s"  # Default to seconds for cftime objects
7243             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7244 
7245         variables = {}
7246         for k, v in self.variables.items():
7247             if k in self.data_vars and dim in v.dims and k not in self.coords:
7248                 if _contains_datetime_like_objects(v):
7249                     v = v._to_numeric(datetime_unit=datetime_unit)
7250                 grad = duck_array_ops.gradient(
7251                     v.data,
7252                     coord_var.data,
7253                     edge_order=edge_order,
7254                     axis=v.get_axis_num(dim),
7255                 )
7256                 variables[k] = Variable(v.dims, grad)
7257             else:
7258                 variables[k] = v
7259         return self._replace(variables)
7260 
7261     def integrate(
7262         self: T_Dataset,
7263         coord: Hashable | Sequence[Hashable],
7264         datetime_unit: DatetimeUnitOptions = None,
7265     ) -> T_Dataset:
7266         """Integrate along the given coordinate using the trapezoidal rule.
7267 
7268         .. note::
7269             This feature is limited to simple cartesian geometry, i.e. coord
7270             must be one dimensional.
7271 
7272         Parameters
7273         ----------
7274         coord : hashable, or sequence of hashable
7275             Coordinate(s) used for the integration.
7276         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7277                         'ps', 'fs', 'as', None}, optional
7278             Specify the unit if datetime coordinate is used.
7279 
7280         Returns
7281         -------
7282         integrated : Dataset
7283 
7284         See also
7285         --------
7286         DataArray.integrate
7287         numpy.trapz : corresponding numpy function
7288 
7289         Examples
7290         --------
7291         >>> ds = xr.Dataset(
7292         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7293         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7294         ... )
7295         >>> ds
7296         <xarray.Dataset>
7297         Dimensions:  (x: 4)
7298         Coordinates:
7299           * x        (x) int64 0 1 2 3
7300             y        (x) int64 1 7 3 5
7301         Data variables:
7302             a        (x) int64 5 5 6 6
7303             b        (x) int64 1 2 1 0
7304         >>> ds.integrate("x")
7305         <xarray.Dataset>
7306         Dimensions:  ()
7307         Data variables:
7308             a        float64 16.5
7309             b        float64 3.5
7310         >>> ds.integrate("y")
7311         <xarray.Dataset>
7312         Dimensions:  ()
7313         Data variables:
7314             a        float64 20.0
7315             b        float64 4.0
7316         """
7317         if not isinstance(coord, (list, tuple)):
7318             coord = (coord,)
7319         result = self
7320         for c in coord:
7321             result = result._integrate_one(c, datetime_unit=datetime_unit)
7322         return result
7323 
7324     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7325         from .variable import Variable
7326 
7327         if coord not in self.variables and coord not in self.dims:
7328             raise ValueError(f"Coordinate {coord} does not exist.")
7329 
7330         coord_var = self[coord].variable
7331         if coord_var.ndim != 1:
7332             raise ValueError(
7333                 "Coordinate {} must be 1 dimensional but is {}"
7334                 " dimensional".format(coord, coord_var.ndim)
7335             )
7336 
7337         dim = coord_var.dims[0]
7338         if _contains_datetime_like_objects(coord_var):
7339             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7340                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7341             elif datetime_unit is None:
7342                 datetime_unit = "s"  # Default to seconds for cftime objects
7343             coord_var = coord_var._replace(
7344                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7345             )
7346 
7347         variables = {}
7348         coord_names = set()
7349         for k, v in self.variables.items():
7350             if k in self.coords:
7351                 if dim not in v.dims or cumulative:
7352                     variables[k] = v
7353                     coord_names.add(k)
7354             else:
7355                 if k in self.data_vars and dim in v.dims:
7356                     if _contains_datetime_like_objects(v):
7357                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7358                     if cumulative:
7359                         integ = duck_array_ops.cumulative_trapezoid(
7360                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7361                         )
7362                         v_dims = v.dims
7363                     else:
7364                         integ = duck_array_ops.trapz(
7365                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7366                         )
7367                         v_dims = list(v.dims)
7368                         v_dims.remove(dim)
7369                     variables[k] = Variable(v_dims, integ)
7370                 else:
7371                     variables[k] = v
7372         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7373         return self._replace_with_new_dims(
7374             variables, coord_names=coord_names, indexes=indexes
7375         )
7376 
7377     def cumulative_integrate(
7378         self: T_Dataset,
7379         coord: Hashable | Sequence[Hashable],
7380         datetime_unit: DatetimeUnitOptions = None,
7381     ) -> T_Dataset:
7382         """Integrate along the given coordinate using the trapezoidal rule.
7383 
7384         .. note::
7385             This feature is limited to simple cartesian geometry, i.e. coord
7386             must be one dimensional.
7387 
7388             The first entry of the cumulative integral of each variable is always 0, in
7389             order to keep the length of the dimension unchanged between input and
7390             output.
7391 
7392         Parameters
7393         ----------
7394         coord : hashable, or sequence of hashable
7395             Coordinate(s) used for the integration.
7396         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7397                         'ps', 'fs', 'as', None}, optional
7398             Specify the unit if datetime coordinate is used.
7399 
7400         Returns
7401         -------
7402         integrated : Dataset
7403 
7404         See also
7405         --------
7406         DataArray.cumulative_integrate
7407         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7408 
7409         Examples
7410         --------
7411         >>> ds = xr.Dataset(
7412         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7413         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7414         ... )
7415         >>> ds
7416         <xarray.Dataset>
7417         Dimensions:  (x: 4)
7418         Coordinates:
7419           * x        (x) int64 0 1 2 3
7420             y        (x) int64 1 7 3 5
7421         Data variables:
7422             a        (x) int64 5 5 6 6
7423             b        (x) int64 1 2 1 0
7424         >>> ds.cumulative_integrate("x")
7425         <xarray.Dataset>
7426         Dimensions:  (x: 4)
7427         Coordinates:
7428           * x        (x) int64 0 1 2 3
7429             y        (x) int64 1 7 3 5
7430         Data variables:
7431             a        (x) float64 0.0 5.0 10.5 16.5
7432             b        (x) float64 0.0 1.5 3.0 3.5
7433         >>> ds.cumulative_integrate("y")
7434         <xarray.Dataset>
7435         Dimensions:  (x: 4)
7436         Coordinates:
7437           * x        (x) int64 0 1 2 3
7438             y        (x) int64 1 7 3 5
7439         Data variables:
7440             a        (x) float64 0.0 30.0 8.0 20.0
7441             b        (x) float64 0.0 9.0 3.0 4.0
7442         """
7443         if not isinstance(coord, (list, tuple)):
7444             coord = (coord,)
7445         result = self
7446         for c in coord:
7447             result = result._integrate_one(
7448                 c, datetime_unit=datetime_unit, cumulative=True
7449             )
7450         return result
7451 
7452     @property
7453     def real(self: T_Dataset) -> T_Dataset:
7454         """
7455         The real part of each data variable.
7456 
7457         See Also
7458         --------
7459         numpy.ndarray.real
7460         """
7461         return self.map(lambda x: x.real, keep_attrs=True)
7462 
7463     @property
7464     def imag(self: T_Dataset) -> T_Dataset:
7465         """
7466         The imaginary part of each data variable.
7467 
7468         See Also
7469         --------
7470         numpy.ndarray.imag
7471         """
7472         return self.map(lambda x: x.imag, keep_attrs=True)
7473 
7474     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7475 
7476     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7477         """Returns a ``Dataset`` with variables that match specific conditions.
7478 
7479         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7480         containing only the variables for which all the filter tests pass.
7481         These tests are either ``key=value`` for which the attribute ``key``
7482         has the exact value ``value`` or the callable passed into
7483         ``key=callable`` returns True. The callable will be passed a single
7484         value, either the value of the attribute ``key`` or ``None`` if the
7485         DataArray does not have an attribute with the name ``key``.
7486 
7487         Parameters
7488         ----------
7489         **kwargs
7490             key : str
7491                 Attribute name.
7492             value : callable or obj
7493                 If value is a callable, it should return a boolean in the form
7494                 of bool = func(attr) where attr is da.attrs[key].
7495                 Otherwise, value will be compared to the each
7496                 DataArray's attrs[key].
7497 
7498         Returns
7499         -------
7500         new : Dataset
7501             New dataset with variables filtered by attribute.
7502 
7503         Examples
7504         --------
7505         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7506         >>> precip = 10 * np.random.rand(2, 2, 3)
7507         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7508         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7509         >>> dims = ["x", "y", "time"]
7510         >>> temp_attr = dict(standard_name="air_potential_temperature")
7511         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7512 
7513         >>> ds = xr.Dataset(
7514         ...     dict(
7515         ...         temperature=(dims, temp, temp_attr),
7516         ...         precipitation=(dims, precip, precip_attr),
7517         ...     ),
7518         ...     coords=dict(
7519         ...         lon=(["x", "y"], lon),
7520         ...         lat=(["x", "y"], lat),
7521         ...         time=pd.date_range("2014-09-06", periods=3),
7522         ...         reference_time=pd.Timestamp("2014-09-05"),
7523         ...     ),
7524         ... )
7525 
7526         Get variables matching a specific standard_name:
7527 
7528         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7529         <xarray.Dataset>
7530         Dimensions:         (x: 2, y: 2, time: 3)
7531         Coordinates:
7532             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7533             lat             (x, y) float64 42.25 42.21 42.63 42.59
7534           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7535             reference_time  datetime64[ns] 2014-09-05
7536         Dimensions without coordinates: x, y
7537         Data variables:
7538             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7539 
7540         Get all variables that have a standard_name attribute:
7541 
7542         >>> standard_name = lambda v: v is not None
7543         >>> ds.filter_by_attrs(standard_name=standard_name)
7544         <xarray.Dataset>
7545         Dimensions:         (x: 2, y: 2, time: 3)
7546         Coordinates:
7547             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7548             lat             (x, y) float64 42.25 42.21 42.63 42.59
7549           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7550             reference_time  datetime64[ns] 2014-09-05
7551         Dimensions without coordinates: x, y
7552         Data variables:
7553             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7554             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7555 
7556         """
7557         selection = []
7558         for var_name, variable in self.variables.items():
7559             has_value_flag = False
7560             for attr_name, pattern in kwargs.items():
7561                 attr_value = variable.attrs.get(attr_name)
7562                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7563                     has_value_flag = True
7564                 else:
7565                     has_value_flag = False
7566                     break
7567             if has_value_flag is True:
7568                 selection.append(var_name)
7569         return self[selection]
7570 
7571     def unify_chunks(self: T_Dataset) -> T_Dataset:
7572         """Unify chunk size along all chunked dimensions of this Dataset.
7573 
7574         Returns
7575         -------
7576         Dataset with consistent chunk sizes for all dask-array variables
7577 
7578         See Also
7579         --------
7580         dask.array.core.unify_chunks
7581         """
7582 
7583         return unify_chunks(self)[0]
7584 
7585     def map_blocks(
7586         self,
7587         func: Callable[..., T_Xarray],
7588         args: Sequence[Any] = (),
7589         kwargs: Mapping[str, Any] | None = None,
7590         template: DataArray | Dataset | None = None,
7591     ) -> T_Xarray:
7592         """
7593         Apply a function to each block of this Dataset.
7594 
7595         .. warning::
7596             This method is experimental and its signature may change.
7597 
7598         Parameters
7599         ----------
7600         func : callable
7601             User-provided function that accepts a Dataset as its first
7602             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7603             corresponding to one chunk along each chunked dimension. ``func`` will be
7604             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7605 
7606             This function must return either a single DataArray or a single Dataset.
7607 
7608             This function cannot add a new chunked dimension.
7609         args : sequence
7610             Passed to func after unpacking and subsetting any xarray objects by blocks.
7611             xarray objects in args must be aligned with obj, otherwise an error is raised.
7612         kwargs : Mapping or None
7613             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7614             subset to blocks. Passing dask collections in kwargs is not allowed.
7615         template : DataArray, Dataset or None, optional
7616             xarray object representing the final result after compute is called. If not provided,
7617             the function will be first run on mocked-up data, that looks like this object but
7618             has sizes 0, to determine properties of the returned object such as dtype,
7619             variable names, attributes, new dimensions and new indexes (if any).
7620             ``template`` must be provided if the function changes the size of existing dimensions.
7621             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7622             ``attrs`` set by ``func`` will be ignored.
7623 
7624         Returns
7625         -------
7626         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7627         function.
7628 
7629         Notes
7630         -----
7631         This function is designed for when ``func`` needs to manipulate a whole xarray object
7632         subset to each block. Each block is loaded into memory. In the more common case where
7633         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7634 
7635         If none of the variables in this object is backed by dask arrays, calling this function is
7636         equivalent to calling ``func(obj, *args, **kwargs)``.
7637 
7638         See Also
7639         --------
7640         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7641         xarray.DataArray.map_blocks
7642 
7643         Examples
7644         --------
7645         Calculate an anomaly from climatology using ``.groupby()``. Using
7646         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7647         its indices, and its methods like ``.groupby()``.
7648 
7649         >>> def calculate_anomaly(da, groupby_type="time.month"):
7650         ...     gb = da.groupby(groupby_type)
7651         ...     clim = gb.mean(dim="time")
7652         ...     return gb - clim
7653         ...
7654         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7655         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7656         >>> np.random.seed(123)
7657         >>> array = xr.DataArray(
7658         ...     np.random.rand(len(time)),
7659         ...     dims=["time"],
7660         ...     coords={"time": time, "month": month},
7661         ... ).chunk()
7662         >>> ds = xr.Dataset({"a": array})
7663         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7664         <xarray.Dataset>
7665         Dimensions:  (time: 24)
7666         Coordinates:
7667           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7668             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7669         Data variables:
7670             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7671 
7672         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7673         to the function being applied in ``xr.map_blocks()``:
7674 
7675         >>> ds.map_blocks(
7676         ...     calculate_anomaly,
7677         ...     kwargs={"groupby_type": "time.year"},
7678         ...     template=ds,
7679         ... )
7680         <xarray.Dataset>
7681         Dimensions:  (time: 24)
7682         Coordinates:
7683           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7684             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7685         Data variables:
7686             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7687         """
7688         from .parallel import map_blocks
7689 
7690         return map_blocks(func, self, args, kwargs, template)
7691 
7692     def polyfit(
7693         self: T_Dataset,
7694         dim: Hashable,
7695         deg: int,
7696         skipna: bool | None = None,
7697         rcond: float | None = None,
7698         w: Hashable | Any = None,
7699         full: bool = False,
7700         cov: bool | Literal["unscaled"] = False,
7701     ) -> T_Dataset:
7702         """
7703         Least squares polynomial fit.
7704 
7705         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7706         invalid values when `skipna = True`.
7707 
7708         Parameters
7709         ----------
7710         dim : hashable
7711             Coordinate along which to fit the polynomials.
7712         deg : int
7713             Degree of the fitting polynomial.
7714         skipna : bool or None, optional
7715             If True, removes all invalid values before fitting each 1D slices of the array.
7716             Default is True if data is stored in a dask.array or if there is any
7717             invalid values, False otherwise.
7718         rcond : float or None, optional
7719             Relative condition number to the fit.
7720         w : hashable or Any, optional
7721             Weights to apply to the y-coordinate of the sample points.
7722             Can be an array-like object or the name of a coordinate in the dataset.
7723         full : bool, default: False
7724             Whether to return the residuals, matrix rank and singular values in addition
7725             to the coefficients.
7726         cov : bool or "unscaled", default: False
7727             Whether to return to the covariance matrix in addition to the coefficients.
7728             The matrix is not scaled if `cov='unscaled'`.
7729 
7730         Returns
7731         -------
7732         polyfit_results : Dataset
7733             A single dataset which contains (for each "var" in the input dataset):
7734 
7735             [var]_polyfit_coefficients
7736                 The coefficients of the best fit for each variable in this dataset.
7737             [var]_polyfit_residuals
7738                 The residuals of the least-square computation for each variable (only included if `full=True`)
7739                 When the matrix rank is deficient, np.nan is returned.
7740             [dim]_matrix_rank
7741                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7742                 The rank is computed ignoring the NaN values that might be skipped.
7743             [dim]_singular_values
7744                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7745             [var]_polyfit_covariance
7746                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7747 
7748         Warns
7749         -----
7750         RankWarning
7751             The rank of the coefficient matrix in the least-squares fit is deficient.
7752             The warning is not raised with in-memory (not dask) data and `full=True`.
7753 
7754         See Also
7755         --------
7756         numpy.polyfit
7757         numpy.polyval
7758         xarray.polyval
7759         """
7760         from .dataarray import DataArray
7761 
7762         variables = {}
7763         skipna_da = skipna
7764 
7765         x = get_clean_interp_index(self, dim, strict=False)
7766         xname = f"{self[dim].name}_"
7767         order = int(deg) + 1
7768         lhs = np.vander(x, order)
7769 
7770         if rcond is None:
7771             rcond = (
7772                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7773             )
7774 
7775         # Weights:
7776         if w is not None:
7777             if isinstance(w, Hashable):
7778                 w = self.coords[w]
7779             w = np.asarray(w)
7780             if w.ndim != 1:
7781                 raise TypeError("Expected a 1-d array for weights.")
7782             if w.shape[0] != lhs.shape[0]:
7783                 raise TypeError(f"Expected w and {dim} to have the same length")
7784             lhs *= w[:, np.newaxis]
7785 
7786         # Scaling
7787         scale = np.sqrt((lhs * lhs).sum(axis=0))
7788         lhs /= scale
7789 
7790         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7791 
7792         rank = np.linalg.matrix_rank(lhs)
7793 
7794         if full:
7795             rank = DataArray(rank, name=xname + "matrix_rank")
7796             variables[rank.name] = rank
7797             _sing = np.linalg.svd(lhs, compute_uv=False)
7798             sing = DataArray(
7799                 _sing,
7800                 dims=(degree_dim,),
7801                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7802                 name=xname + "singular_values",
7803             )
7804             variables[sing.name] = sing
7805 
7806         for name, da in self.data_vars.items():
7807             if dim not in da.dims:
7808                 continue
7809 
7810             if is_duck_dask_array(da.data) and (
7811                 rank != order or full or skipna is None
7812             ):
7813                 # Current algorithm with dask and skipna=False neither supports
7814                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7815                 skipna_da = True
7816             elif skipna is None:
7817                 skipna_da = bool(np.any(da.isnull()))
7818 
7819             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7820             stacked_coords: dict[Hashable, DataArray] = {}
7821             if dims_to_stack:
7822                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7823                 rhs = da.transpose(dim, *dims_to_stack).stack(
7824                     {stacked_dim: dims_to_stack}
7825                 )
7826                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7827                 scale_da = scale[:, np.newaxis]
7828             else:
7829                 rhs = da
7830                 scale_da = scale
7831 
7832             if w is not None:
7833                 rhs *= w[:, np.newaxis]
7834 
7835             with warnings.catch_warnings():
7836                 if full:  # Copy np.polyfit behavior
7837                     warnings.simplefilter("ignore", np.RankWarning)
7838                 else:  # Raise only once per variable
7839                     warnings.simplefilter("once", np.RankWarning)
7840 
7841                 coeffs, residuals = duck_array_ops.least_squares(
7842                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7843                 )
7844 
7845             if isinstance(name, str):
7846                 name = f"{name}_"
7847             else:
7848                 # Thus a ReprObject => polyfit was called on a DataArray
7849                 name = ""
7850 
7851             coeffs = DataArray(
7852                 coeffs / scale_da,
7853                 dims=[degree_dim] + list(stacked_coords.keys()),
7854                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7855                 name=name + "polyfit_coefficients",
7856             )
7857             if dims_to_stack:
7858                 coeffs = coeffs.unstack(stacked_dim)
7859             variables[coeffs.name] = coeffs
7860 
7861             if full or (cov is True):
7862                 residuals = DataArray(
7863                     residuals if dims_to_stack else residuals.squeeze(),
7864                     dims=list(stacked_coords.keys()),
7865                     coords=stacked_coords,
7866                     name=name + "polyfit_residuals",
7867                 )
7868                 if dims_to_stack:
7869                     residuals = residuals.unstack(stacked_dim)
7870                 variables[residuals.name] = residuals
7871 
7872             if cov:
7873                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7874                 Vbase /= np.outer(scale, scale)
7875                 if cov == "unscaled":
7876                     fac = 1
7877                 else:
7878                     if x.shape[0] <= order:
7879                         raise ValueError(
7880                             "The number of data points must exceed order to scale the covariance matrix."
7881                         )
7882                     fac = residuals / (x.shape[0] - order)
7883                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7884                 variables[name + "polyfit_covariance"] = covariance
7885 
7886         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7887 
7888     def pad(
7889         self: T_Dataset,
7890         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7891         mode: PadModeOptions = "constant",
7892         stat_length: int
7893         | tuple[int, int]
7894         | Mapping[Any, tuple[int, int]]
7895         | None = None,
7896         constant_values: (
7897             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7898         ) = None,
7899         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7900         reflect_type: PadReflectOptions = None,
7901         **pad_width_kwargs: Any,
7902     ) -> T_Dataset:
7903         """Pad this dataset along one or more dimensions.
7904 
7905         .. warning::
7906             This function is experimental and its behaviour is likely to change
7907             especially regarding padding of dimension coordinates (or IndexVariables).
7908 
7909         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7910         coordinates will be padded with the same mode, otherwise coordinates
7911         are padded using the "constant" mode with fill_value dtypes.NA.
7912 
7913         Parameters
7914         ----------
7915         pad_width : mapping of hashable to tuple of int
7916             Mapping with the form of {dim: (pad_before, pad_after)}
7917             describing the number of values padded along each dimension.
7918             {dim: pad} is a shortcut for pad_before = pad_after = pad
7919         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7920             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7921             How to pad the DataArray (taken from numpy docs):
7922 
7923             - "constant": Pads with a constant value.
7924             - "edge": Pads with the edge values of array.
7925             - "linear_ramp": Pads with the linear ramp between end_value and the
7926               array edge value.
7927             - "maximum": Pads with the maximum value of all or part of the
7928               vector along each axis.
7929             - "mean": Pads with the mean value of all or part of the
7930               vector along each axis.
7931             - "median": Pads with the median value of all or part of the
7932               vector along each axis.
7933             - "minimum": Pads with the minimum value of all or part of the
7934               vector along each axis.
7935             - "reflect": Pads with the reflection of the vector mirrored on
7936               the first and last values of the vector along each axis.
7937             - "symmetric": Pads with the reflection of the vector mirrored
7938               along the edge of the array.
7939             - "wrap": Pads with the wrap of the vector along the axis.
7940               The first values are used to pad the end and the
7941               end values are used to pad the beginning.
7942 
7943         stat_length : int, tuple or mapping of hashable to tuple, default: None
7944             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7945             values at edge of each axis used to calculate the statistic value.
7946             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7947             statistic lengths along each dimension.
7948             ((before, after),) yields same before and after statistic lengths
7949             for each dimension.
7950             (stat_length,) or int is a shortcut for before = after = statistic
7951             length for all axes.
7952             Default is ``None``, to use the entire axis.
7953         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7954             Used in 'constant'.  The values to set the padded values for each
7955             axis.
7956             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7957             pad constants along each dimension.
7958             ``((before, after),)`` yields same before and after constants for each
7959             dimension.
7960             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7961             all dimensions.
7962             Default is 0.
7963         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7964             Used in 'linear_ramp'.  The values used for the ending value of the
7965             linear_ramp and that will form the edge of the padded array.
7966             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7967             end values along each dimension.
7968             ``((before, after),)`` yields same before and after end values for each
7969             axis.
7970             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7971             all axes.
7972             Default is 0.
7973         reflect_type : {"even", "odd", None}, optional
7974             Used in "reflect", and "symmetric".  The "even" style is the
7975             default with an unaltered reflection around the edge value.  For
7976             the "odd" style, the extended part of the array is created by
7977             subtracting the reflected values from two times the edge value.
7978         **pad_width_kwargs
7979             The keyword arguments form of ``pad_width``.
7980             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7981 
7982         Returns
7983         -------
7984         padded : Dataset
7985             Dataset with the padded coordinates and data.
7986 
7987         See Also
7988         --------
7989         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7990 
7991         Notes
7992         -----
7993         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7994         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7995         specify ``constant_values=np.nan``
7996 
7997         Padding coordinates will drop their corresponding index (if any) and will reset default
7998         indexes for dimension coordinates.
7999 
8000         Examples
8001         --------
8002         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8003         >>> ds.pad(x=(1, 2))
8004         <xarray.Dataset>
8005         Dimensions:  (x: 8)
8006         Dimensions without coordinates: x
8007         Data variables:
8008             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8009         """
8010         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8011 
8012         if mode in ("edge", "reflect", "symmetric", "wrap"):
8013             coord_pad_mode = mode
8014             coord_pad_options = {
8015                 "stat_length": stat_length,
8016                 "constant_values": constant_values,
8017                 "end_values": end_values,
8018                 "reflect_type": reflect_type,
8019             }
8020         else:
8021             coord_pad_mode = "constant"
8022             coord_pad_options = {}
8023 
8024         variables = {}
8025 
8026         # keep indexes that won't be affected by pad and drop all other indexes
8027         xindexes = self.xindexes
8028         pad_dims = set(pad_width)
8029         indexes = {}
8030         for k, idx in xindexes.items():
8031             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8032                 indexes[k] = idx
8033 
8034         for name, var in self.variables.items():
8035             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8036             if not var_pad_width:
8037                 variables[name] = var
8038             elif name in self.data_vars:
8039                 variables[name] = var.pad(
8040                     pad_width=var_pad_width,
8041                     mode=mode,
8042                     stat_length=stat_length,
8043                     constant_values=constant_values,
8044                     end_values=end_values,
8045                     reflect_type=reflect_type,
8046                 )
8047             else:
8048                 variables[name] = var.pad(
8049                     pad_width=var_pad_width,
8050                     mode=coord_pad_mode,
8051                     **coord_pad_options,  # type: ignore[arg-type]
8052                 )
8053                 # reset default index of dimension coordinates
8054                 if (name,) == var.dims:
8055                     dim_var = {name: variables[name]}
8056                     index = PandasIndex.from_variables(dim_var, options={})
8057                     index_vars = index.create_variables(dim_var)
8058                     indexes[name] = index
8059                     variables[name] = index_vars[name]
8060 
8061         return self._replace_with_new_dims(variables, indexes=indexes)
8062 
8063     def idxmin(
8064         self: T_Dataset,
8065         dim: Hashable | None = None,
8066         skipna: bool | None = None,
8067         fill_value: Any = xrdtypes.NA,
8068         keep_attrs: bool | None = None,
8069     ) -> T_Dataset:
8070         """Return the coordinate label of the minimum value along a dimension.
8071 
8072         Returns a new `Dataset` named after the dimension with the values of
8073         the coordinate labels along that dimension corresponding to minimum
8074         values along that dimension.
8075 
8076         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8077         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8078 
8079         Parameters
8080         ----------
8081         dim : Hashable, optional
8082             Dimension over which to apply `idxmin`.  This is optional for 1D
8083             variables, but required for variables with 2 or more dimensions.
8084         skipna : bool or None, optional
8085             If True, skip missing values (as marked by NaN). By default, only
8086             skips missing values for ``float``, ``complex``, and ``object``
8087             dtypes; other dtypes either do not have a sentinel missing value
8088             (``int``) or ``skipna=True`` has not been implemented
8089             (``datetime64`` or ``timedelta64``).
8090         fill_value : Any, default: NaN
8091             Value to be filled in case all of the values along a dimension are
8092             null.  By default this is NaN.  The fill value and result are
8093             automatically converted to a compatible dtype if possible.
8094             Ignored if ``skipna`` is False.
8095         keep_attrs : bool or None, optional
8096             If True, the attributes (``attrs``) will be copied from the
8097             original object to the new one. If False, the new object
8098             will be returned without attributes.
8099 
8100         Returns
8101         -------
8102         reduced : Dataset
8103             New `Dataset` object with `idxmin` applied to its data and the
8104             indicated dimension removed.
8105 
8106         See Also
8107         --------
8108         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8109 
8110         Examples
8111         --------
8112         >>> array1 = xr.DataArray(
8113         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8114         ... )
8115         >>> array2 = xr.DataArray(
8116         ...     [
8117         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8118         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8119         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8120         ...     ],
8121         ...     dims=["y", "x"],
8122         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8123         ... )
8124         >>> ds = xr.Dataset({"int": array1, "float": array2})
8125         >>> ds.min(dim="x")
8126         <xarray.Dataset>
8127         Dimensions:  (y: 3)
8128         Coordinates:
8129           * y        (y) int64 -1 0 1
8130         Data variables:
8131             int      int64 -2
8132             float    (y) float64 -2.0 -4.0 1.0
8133         >>> ds.argmin(dim="x")
8134         <xarray.Dataset>
8135         Dimensions:  (y: 3)
8136         Coordinates:
8137           * y        (y) int64 -1 0 1
8138         Data variables:
8139             int      int64 4
8140             float    (y) int64 4 0 2
8141         >>> ds.idxmin(dim="x")
8142         <xarray.Dataset>
8143         Dimensions:  (y: 3)
8144         Coordinates:
8145           * y        (y) int64 -1 0 1
8146         Data variables:
8147             int      <U1 'e'
8148             float    (y) object 'e' 'a' 'c'
8149         """
8150         return self.map(
8151             methodcaller(
8152                 "idxmin",
8153                 dim=dim,
8154                 skipna=skipna,
8155                 fill_value=fill_value,
8156                 keep_attrs=keep_attrs,
8157             )
8158         )
8159 
8160     def idxmax(
8161         self: T_Dataset,
8162         dim: Hashable | None = None,
8163         skipna: bool | None = None,
8164         fill_value: Any = xrdtypes.NA,
8165         keep_attrs: bool | None = None,
8166     ) -> T_Dataset:
8167         """Return the coordinate label of the maximum value along a dimension.
8168 
8169         Returns a new `Dataset` named after the dimension with the values of
8170         the coordinate labels along that dimension corresponding to maximum
8171         values along that dimension.
8172 
8173         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8174         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8175 
8176         Parameters
8177         ----------
8178         dim : str, optional
8179             Dimension over which to apply `idxmax`.  This is optional for 1D
8180             variables, but required for variables with 2 or more dimensions.
8181         skipna : bool or None, optional
8182             If True, skip missing values (as marked by NaN). By default, only
8183             skips missing values for ``float``, ``complex``, and ``object``
8184             dtypes; other dtypes either do not have a sentinel missing value
8185             (``int``) or ``skipna=True`` has not been implemented
8186             (``datetime64`` or ``timedelta64``).
8187         fill_value : Any, default: NaN
8188             Value to be filled in case all of the values along a dimension are
8189             null.  By default this is NaN.  The fill value and result are
8190             automatically converted to a compatible dtype if possible.
8191             Ignored if ``skipna`` is False.
8192         keep_attrs : bool or None, optional
8193             If True, the attributes (``attrs``) will be copied from the
8194             original object to the new one. If False, the new object
8195             will be returned without attributes.
8196 
8197         Returns
8198         -------
8199         reduced : Dataset
8200             New `Dataset` object with `idxmax` applied to its data and the
8201             indicated dimension removed.
8202 
8203         See Also
8204         --------
8205         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8206 
8207         Examples
8208         --------
8209         >>> array1 = xr.DataArray(
8210         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8211         ... )
8212         >>> array2 = xr.DataArray(
8213         ...     [
8214         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8215         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8216         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8217         ...     ],
8218         ...     dims=["y", "x"],
8219         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8220         ... )
8221         >>> ds = xr.Dataset({"int": array1, "float": array2})
8222         >>> ds.max(dim="x")
8223         <xarray.Dataset>
8224         Dimensions:  (y: 3)
8225         Coordinates:
8226           * y        (y) int64 -1 0 1
8227         Data variables:
8228             int      int64 2
8229             float    (y) float64 2.0 2.0 1.0
8230         >>> ds.argmax(dim="x")
8231         <xarray.Dataset>
8232         Dimensions:  (y: 3)
8233         Coordinates:
8234           * y        (y) int64 -1 0 1
8235         Data variables:
8236             int      int64 1
8237             float    (y) int64 0 2 2
8238         >>> ds.idxmax(dim="x")
8239         <xarray.Dataset>
8240         Dimensions:  (y: 3)
8241         Coordinates:
8242           * y        (y) int64 -1 0 1
8243         Data variables:
8244             int      <U1 'b'
8245             float    (y) object 'a' 'c' 'c'
8246         """
8247         return self.map(
8248             methodcaller(
8249                 "idxmax",
8250                 dim=dim,
8251                 skipna=skipna,
8252                 fill_value=fill_value,
8253                 keep_attrs=keep_attrs,
8254             )
8255         )
8256 
8257     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8258         """Indices of the minima of the member variables.
8259 
8260         If there are multiple minima, the indices of the first one found will be
8261         returned.
8262 
8263         Parameters
8264         ----------
8265         dim : Hashable, optional
8266             The dimension over which to find the minimum. By default, finds minimum over
8267             all dimensions - for now returning an int for backward compatibility, but
8268             this is deprecated, in future will be an error, since DataArray.argmin will
8269             return a dict with indices for all dimensions, which does not make sense for
8270             a Dataset.
8271         keep_attrs : bool, optional
8272             If True, the attributes (`attrs`) will be copied from the original
8273             object to the new one.  If False (default), the new object will be
8274             returned without attributes.
8275         skipna : bool, optional
8276             If True, skip missing values (as marked by NaN). By default, only
8277             skips missing values for float dtypes; other dtypes either do not
8278             have a sentinel missing value (int) or skipna=True has not been
8279             implemented (object, datetime64 or timedelta64).
8280 
8281         Returns
8282         -------
8283         result : Dataset
8284 
8285         See Also
8286         --------
8287         DataArray.argmin
8288         """
8289         if dim is None:
8290             warnings.warn(
8291                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8292                 "dim changes to return a dict of indices of each dimension, for "
8293                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8294                 "since we don't return a dict of Datasets.",
8295                 DeprecationWarning,
8296                 stacklevel=2,
8297             )
8298         if (
8299             dim is None
8300             or (not isinstance(dim, Sequence) and dim is not ...)
8301             or isinstance(dim, str)
8302         ):
8303             # Return int index if single dimension is passed, and is not part of a
8304             # sequence
8305             argmin_func = getattr(duck_array_ops, "argmin")
8306             return self.reduce(argmin_func, dim=dim, **kwargs)
8307         else:
8308             raise ValueError(
8309                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8310                 "dicts cannot be contained in a Dataset, so cannot call "
8311                 "Dataset.argmin() with a sequence or ... for dim"
8312             )
8313 
8314     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8315         """Indices of the maxima of the member variables.
8316 
8317         If there are multiple maxima, the indices of the first one found will be
8318         returned.
8319 
8320         Parameters
8321         ----------
8322         dim : str, optional
8323             The dimension over which to find the maximum. By default, finds maximum over
8324             all dimensions - for now returning an int for backward compatibility, but
8325             this is deprecated, in future will be an error, since DataArray.argmax will
8326             return a dict with indices for all dimensions, which does not make sense for
8327             a Dataset.
8328         keep_attrs : bool, optional
8329             If True, the attributes (`attrs`) will be copied from the original
8330             object to the new one.  If False (default), the new object will be
8331             returned without attributes.
8332         skipna : bool, optional
8333             If True, skip missing values (as marked by NaN). By default, only
8334             skips missing values for float dtypes; other dtypes either do not
8335             have a sentinel missing value (int) or skipna=True has not been
8336             implemented (object, datetime64 or timedelta64).
8337 
8338         Returns
8339         -------
8340         result : Dataset
8341 
8342         See Also
8343         --------
8344         DataArray.argmax
8345 
8346         """
8347         if dim is None:
8348             warnings.warn(
8349                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8350                 "dim changes to return a dict of indices of each dimension, for "
8351                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8352                 "since we don't return a dict of Datasets.",
8353                 DeprecationWarning,
8354                 stacklevel=2,
8355             )
8356         if (
8357             dim is None
8358             or (not isinstance(dim, Sequence) and dim is not ...)
8359             or isinstance(dim, str)
8360         ):
8361             # Return int index if single dimension is passed, and is not part of a
8362             # sequence
8363             argmax_func = getattr(duck_array_ops, "argmax")
8364             return self.reduce(argmax_func, dim=dim, **kwargs)
8365         else:
8366             raise ValueError(
8367                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8368                 "dicts cannot be contained in a Dataset, so cannot call "
8369                 "Dataset.argmin() with a sequence or ... for dim"
8370             )
8371 
8372     def query(
8373         self: T_Dataset,
8374         queries: Mapping[Any, Any] | None = None,
8375         parser: QueryParserOptions = "pandas",
8376         engine: QueryEngineOptions = None,
8377         missing_dims: ErrorOptionsWithWarn = "raise",
8378         **queries_kwargs: Any,
8379     ) -> T_Dataset:
8380         """Return a new dataset with each array indexed along the specified
8381         dimension(s), where the indexers are given as strings containing
8382         Python expressions to be evaluated against the data variables in the
8383         dataset.
8384 
8385         Parameters
8386         ----------
8387         queries : dict-like, optional
8388             A dict-like with keys matching dimensions and values given by strings
8389             containing Python expressions to be evaluated against the data variables
8390             in the dataset. The expressions will be evaluated using the pandas
8391             eval() function, and can contain any valid Python expressions but cannot
8392             contain any Python statements.
8393         parser : {"pandas", "python"}, default: "pandas"
8394             The parser to use to construct the syntax tree from the expression.
8395             The default of 'pandas' parses code slightly different than standard
8396             Python. Alternatively, you can parse an expression using the 'python'
8397             parser to retain strict Python semantics.
8398         engine : {"python", "numexpr", None}, default: None
8399             The engine used to evaluate the expression. Supported engines are:
8400 
8401             - None: tries to use numexpr, falls back to python
8402             - "numexpr": evaluates expressions using numexpr
8403             - "python": performs operations as if you had evald in top level python
8404 
8405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8406             What to do if dimensions that should be selected from are not present in the
8407             Dataset:
8408 
8409             - "raise": raise an exception
8410             - "warn": raise a warning, and ignore the missing dimensions
8411             - "ignore": ignore the missing dimensions
8412 
8413         **queries_kwargs : {dim: query, ...}, optional
8414             The keyword arguments form of ``queries``.
8415             One of queries or queries_kwargs must be provided.
8416 
8417         Returns
8418         -------
8419         obj : Dataset
8420             A new Dataset with the same contents as this dataset, except each
8421             array and dimension is indexed by the results of the appropriate
8422             queries.
8423 
8424         See Also
8425         --------
8426         Dataset.isel
8427         pandas.eval
8428 
8429         Examples
8430         --------
8431         >>> a = np.arange(0, 5, 1)
8432         >>> b = np.linspace(0, 1, 5)
8433         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8434         >>> ds
8435         <xarray.Dataset>
8436         Dimensions:  (x: 5)
8437         Dimensions without coordinates: x
8438         Data variables:
8439             a        (x) int64 0 1 2 3 4
8440             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8441         >>> ds.query(x="a > 2")
8442         <xarray.Dataset>
8443         Dimensions:  (x: 2)
8444         Dimensions without coordinates: x
8445         Data variables:
8446             a        (x) int64 3 4
8447             b        (x) float64 0.75 1.0
8448         """
8449 
8450         # allow queries to be given either as a dict or as kwargs
8451         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8452 
8453         # check queries
8454         for dim, expr in queries.items():
8455             if not isinstance(expr, str):
8456                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8457                 raise ValueError(msg)
8458 
8459         # evaluate the queries to create the indexers
8460         indexers = {
8461             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8462             for dim, expr in queries.items()
8463         }
8464 
8465         # apply the selection
8466         return self.isel(indexers, missing_dims=missing_dims)
8467 
8468     def curvefit(
8469         self: T_Dataset,
8470         coords: str | DataArray | Iterable[str | DataArray],
8471         func: Callable[..., Any],
8472         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8473         skipna: bool = True,
8474         p0: dict[str, Any] | None = None,
8475         bounds: dict[str, Any] | None = None,
8476         param_names: Sequence[str] | None = None,
8477         kwargs: dict[str, Any] | None = None,
8478     ) -> T_Dataset:
8479         """
8480         Curve fitting optimization for arbitrary functions.
8481 
8482         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8483 
8484         Parameters
8485         ----------
8486         coords : hashable, DataArray, or sequence of hashable or DataArray
8487             Independent coordinate(s) over which to perform the curve fitting. Must share
8488             at least one dimension with the calling object. When fitting multi-dimensional
8489             functions, supply `coords` as a sequence in the same order as arguments in
8490             `func`. To fit along existing dimensions of the calling object, `coords` can
8491             also be specified as a str or sequence of strs.
8492         func : callable
8493             User specified function in the form `f(x, *params)` which returns a numpy
8494             array of length `len(x)`. `params` are the fittable parameters which are optimized
8495             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8496             coordinates, e.g. `f((x0, x1), *params)`.
8497         reduce_dims : hashable or sequence of hashable
8498             Additional dimension(s) over which to aggregate while fitting. For example,
8499             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8500             aggregate all lat and lon points and fit the specified function along the
8501             time dimension.
8502         skipna : bool, default: True
8503             Whether to skip missing values when fitting. Default is True.
8504         p0 : dict-like, optional
8505             Optional dictionary of parameter names to initial guesses passed to the
8506             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8507             be assigned initial values following the default scipy behavior.
8508         bounds : dict-like, optional
8509             Optional dictionary of parameter names to bounding values passed to the
8510             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8511             will be unbounded following the default scipy behavior.
8512         param_names : sequence of hashable, optional
8513             Sequence of names for the fittable parameters of `func`. If not supplied,
8514             this will be automatically determined by arguments of `func`. `param_names`
8515             should be manually supplied when fitting a function that takes a variable
8516             number of parameters.
8517         **kwargs : optional
8518             Additional keyword arguments to passed to scipy curve_fit.
8519 
8520         Returns
8521         -------
8522         curvefit_results : Dataset
8523             A single dataset which contains:
8524 
8525             [var]_curvefit_coefficients
8526                 The coefficients of the best fit.
8527             [var]_curvefit_covariance
8528                 The covariance matrix of the coefficient estimates.
8529 
8530         See Also
8531         --------
8532         Dataset.polyfit
8533         scipy.optimize.curve_fit
8534         """
8535         from scipy.optimize import curve_fit
8536 
8537         from .alignment import broadcast
8538         from .computation import apply_ufunc
8539         from .dataarray import _THIS_ARRAY, DataArray
8540 
8541         if p0 is None:
8542             p0 = {}
8543         if bounds is None:
8544             bounds = {}
8545         if kwargs is None:
8546             kwargs = {}
8547 
8548         if not reduce_dims:
8549             reduce_dims_ = []
8550         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8551             reduce_dims_ = [reduce_dims]
8552         else:
8553             reduce_dims_ = list(reduce_dims)
8554 
8555         if (
8556             isinstance(coords, str)
8557             or isinstance(coords, DataArray)
8558             or not isinstance(coords, Iterable)
8559         ):
8560             coords = [coords]
8561         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8562 
8563         # Determine whether any coords are dims on self
8564         for coord in coords_:
8565             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8566         reduce_dims_ = list(set(reduce_dims_))
8567         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8568         if not reduce_dims_:
8569             raise ValueError(
8570                 "No arguments to `coords` were identified as a dimension on the calling "
8571                 "object, and no dims were supplied to `reduce_dims`. This would result "
8572                 "in fitting on scalar data."
8573             )
8574 
8575         # Broadcast all coords with each other
8576         coords_ = broadcast(*coords_)
8577         coords_ = [
8578             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8579         ]
8580 
8581         params, func_args = _get_func_args(func, param_names)
8582         param_defaults, bounds_defaults = _initialize_curvefit_params(
8583             params, p0, bounds, func_args
8584         )
8585         n_params = len(params)
8586         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8587         kwargs.setdefault(
8588             "bounds",
8589             [
8590                 [bounds_defaults[p][0] for p in params],
8591                 [bounds_defaults[p][1] for p in params],
8592             ],
8593         )
8594 
8595         def _wrapper(Y, *coords_, **kwargs):
8596             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8597             x = np.vstack([c.ravel() for c in coords_])
8598             y = Y.ravel()
8599             if skipna:
8600                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8601                 x = x[:, mask]
8602                 y = y[mask]
8603                 if not len(y):
8604                     popt = np.full([n_params], np.nan)
8605                     pcov = np.full([n_params, n_params], np.nan)
8606                     return popt, pcov
8607             x = np.squeeze(x)
8608             popt, pcov = curve_fit(func, x, y, **kwargs)
8609             return popt, pcov
8610 
8611         result = type(self)()
8612         for name, da in self.data_vars.items():
8613             if name is _THIS_ARRAY:
8614                 name = ""
8615             else:
8616                 name = f"{str(name)}_"
8617 
8618             popt, pcov = apply_ufunc(
8619                 _wrapper,
8620                 da,
8621                 *coords_,
8622                 vectorize=True,
8623                 dask="parallelized",
8624                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8625                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8626                 dask_gufunc_kwargs={
8627                     "output_sizes": {
8628                         "param": n_params,
8629                         "cov_i": n_params,
8630                         "cov_j": n_params,
8631                     },
8632                 },
8633                 output_dtypes=(np.float64, np.float64),
8634                 exclude_dims=set(reduce_dims_),
8635                 kwargs=kwargs,
8636             )
8637             result[name + "curvefit_coefficients"] = popt
8638             result[name + "curvefit_covariance"] = pcov
8639 
8640         result = result.assign_coords(
8641             {"param": params, "cov_i": params, "cov_j": params}
8642         )
8643         result.attrs = self.attrs.copy()
8644 
8645         return result
8646 
8647     def drop_duplicates(
8648         self: T_Dataset,
8649         dim: Hashable | Iterable[Hashable],
8650         keep: Literal["first", "last", False] = "first",
8651     ) -> T_Dataset:
8652         """Returns a new Dataset with duplicate dimension values removed.
8653 
8654         Parameters
8655         ----------
8656         dim : dimension label or labels
8657             Pass `...` to drop duplicates along all dimensions.
8658         keep : {"first", "last", False}, default: "first"
8659             Determines which duplicates (if any) to keep.
8660             - ``"first"`` : Drop duplicates except for the first occurrence.
8661             - ``"last"`` : Drop duplicates except for the last occurrence.
8662             - False : Drop all duplicates.
8663 
8664         Returns
8665         -------
8666         Dataset
8667 
8668         See Also
8669         --------
8670         DataArray.drop_duplicates
8671         """
8672         if isinstance(dim, str):
8673             dims: Iterable = (dim,)
8674         elif dim is ...:
8675             dims = self.dims
8676         elif not isinstance(dim, Iterable):
8677             dims = [dim]
8678         else:
8679             dims = dim
8680 
8681         missing_dims = set(dims) - set(self.dims)
8682         if missing_dims:
8683             raise ValueError(f"'{missing_dims}' not found in dimensions")
8684 
8685         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8686         return self.isel(indexes)
8687 
8688     def convert_calendar(
8689         self: T_Dataset,
8690         calendar: CFCalendar,
8691         dim: Hashable = "time",
8692         align_on: Literal["date", "year", None] = None,
8693         missing: Any | None = None,
8694         use_cftime: bool | None = None,
8695     ) -> T_Dataset:
8696         """Convert the Dataset to another calendar.
8697 
8698         Only converts the individual timestamps, does not modify any data except
8699         in dropping invalid/surplus dates or inserting missing dates.
8700 
8701         If the source and target calendars are either no_leap, all_leap or a
8702         standard type, only the type of the time array is modified.
8703         When converting to a leap year from a non-leap year, the 29th of February
8704         is removed from the array. In the other direction the 29th of February
8705         will be missing in the output, unless `missing` is specified,
8706         in which case that value is inserted.
8707 
8708         For conversions involving `360_day` calendars, see Notes.
8709 
8710         This method is safe to use with sub-daily data as it doesn't touch the
8711         time part of the timestamps.
8712 
8713         Parameters
8714         ---------
8715         calendar : str
8716             The target calendar name.
8717         dim : Hashable, default: "time"
8718             Name of the time coordinate.
8719         align_on : {None, 'date', 'year'}, optional
8720             Must be specified when either source or target is a `360_day` calendar,
8721             ignored otherwise. See Notes.
8722         missing : Any or None, optional
8723             By default, i.e. if the value is None, this method will simply attempt
8724             to convert the dates in the source calendar to the same dates in the
8725             target calendar, and drop any of those that are not possible to
8726             represent.  If a value is provided, a new time coordinate will be
8727             created in the target calendar with the same frequency as the original
8728             time coordinate; for any dates that are not present in the source, the
8729             data will be filled with this value.  Note that using this mode requires
8730             that the source data have an inferable frequency; for more information
8731             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8732             target calendar combinations, this could result in many missing values, see notes.
8733         use_cftime : bool or None, optional
8734             Whether to use cftime objects in the output, only used if `calendar`
8735             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8736             If True, the new time axis uses cftime objects.
8737             If None (default), it uses :py:class:`numpy.datetime64` values if the
8738             date range permits it, and :py:class:`cftime.datetime` objects if not.
8739             If False, it uses :py:class:`numpy.datetime64`  or fails.
8740 
8741         Returns
8742         -------
8743         Dataset
8744             Copy of the dataarray with the time coordinate converted to the
8745             target calendar. If 'missing' was None (default), invalid dates in
8746             the new calendar are dropped, but missing dates are not inserted.
8747             If `missing` was given, the new data is reindexed to have a time axis
8748             with the same frequency as the source, but in the new calendar; any
8749             missing datapoints are filled with `missing`.
8750 
8751         Notes
8752         -----
8753         Passing a value to `missing` is only usable if the source's time coordinate as an
8754         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8755         if the target coordinate, generated from this frequency, has dates equivalent to the
8756         source. It is usually **not** appropriate to use this mode with:
8757 
8758         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8759         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8760             or 'mH' where 24 % m != 0).
8761 
8762         If one of the source or target calendars is `"360_day"`, `align_on` must
8763         be specified and two options are offered.
8764 
8765         - "year"
8766             The dates are translated according to their relative position in the year,
8767             ignoring their original month and day information, meaning that the
8768             missing/surplus days are added/removed at regular intervals.
8769 
8770             From a `360_day` to a standard calendar, the output will be missing the
8771             following dates (day of year in parentheses):
8772 
8773             To a leap year:
8774                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8775                 September 31st (275) and November 30th (335).
8776             To a non-leap year:
8777                 February 6th (36), April 19th (109), July 2nd (183),
8778                 September 12th (255), November 25th (329).
8779 
8780             From a standard calendar to a `"360_day"`, the following dates in the
8781             source array will be dropped:
8782 
8783             From a leap year:
8784                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8785                 September 31st (275), December 1st (336)
8786             From a non-leap year:
8787                 February 6th (37), April 20th (110), July 2nd (183),
8788                 September 13th (256), November 25th (329)
8789 
8790             This option is best used on daily and subdaily data.
8791 
8792         - "date"
8793             The month/day information is conserved and invalid dates are dropped
8794             from the output. This means that when converting from a `"360_day"` to a
8795             standard calendar, all 31st (Jan, March, May, July, August, October and
8796             December) will be missing as there is no equivalent dates in the
8797             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8798             will be dropped as there are no equivalent dates in a standard calendar.
8799 
8800             This option is best used with data on a frequency coarser than daily.
8801         """
8802         return convert_calendar(
8803             self,
8804             calendar,
8805             dim=dim,
8806             align_on=align_on,
8807             missing=missing,
8808             use_cftime=use_cftime,
8809         )
8810 
8811     def interp_calendar(
8812         self: T_Dataset,
8813         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8814         dim: Hashable = "time",
8815     ) -> T_Dataset:
8816         """Interpolates the Dataset to another calendar based on decimal year measure.
8817 
8818         Each timestamp in `source` and `target` are first converted to their decimal
8819         year equivalent then `source` is interpolated on the target coordinate.
8820         The decimal year of a timestamp is its year plus its sub-year component
8821         converted to the fraction of its year. For example "2000-03-01 12:00" is
8822         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8823 
8824         This method should only be used when the time (HH:MM:SS) information of
8825         time coordinate is not important.
8826 
8827         Parameters
8828         ----------
8829         target: DataArray or DatetimeIndex or CFTimeIndex
8830             The target time coordinate of a valid dtype
8831             (np.datetime64 or cftime objects)
8832         dim : Hashable, default: "time"
8833             The time coordinate name.
8834 
8835         Return
8836         ------
8837         DataArray
8838             The source interpolated on the decimal years of target,
8839         """
8840         return interp_calendar(self, target, dim=dim)
8841 
8842     def groupby(
8843         self,
8844         group: Hashable | DataArray | IndexVariable,
8845         squeeze: bool = True,
8846         restore_coord_dims: bool = False,
8847     ) -> DatasetGroupBy:
8848         """Returns a DatasetGroupBy object for performing grouped operations.
8849 
8850         Parameters
8851         ----------
8852         group : Hashable, DataArray or IndexVariable
8853             Array whose unique values should be used to group this array. If a
8854             string, must be the name of a variable contained in this dataset.
8855         squeeze : bool, default: True
8856             If "group" is a dimension of any arrays in this dataset, `squeeze`
8857             controls whether the subarrays have a dimension of length 1 along
8858             that dimension or if the dimension is squeezed out.
8859         restore_coord_dims : bool, default: False
8860             If True, also restore the dimension order of multi-dimensional
8861             coordinates.
8862 
8863         Returns
8864         -------
8865         grouped : DatasetGroupBy
8866             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8867             iterated over in the form of `(unique_value, grouped_array)` pairs.
8868 
8869         See Also
8870         --------
8871         Dataset.groupby_bins
8872         DataArray.groupby
8873         core.groupby.DatasetGroupBy
8874         pandas.DataFrame.groupby
8875         """
8876         from .groupby import DatasetGroupBy
8877 
8878         # While we don't generally check the type of every arg, passing
8879         # multiple dimensions as multiple arguments is common enough, and the
8880         # consequences hidden enough (strings evaluate as true) to warrant
8881         # checking here.
8882         # A future version could make squeeze kwarg only, but would face
8883         # backward-compat issues.
8884         if not isinstance(squeeze, bool):
8885             raise TypeError(
8886                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8887             )
8888 
8889         return DatasetGroupBy(
8890             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8891         )
8892 
8893     def groupby_bins(
8894         self,
8895         group: Hashable | DataArray | IndexVariable,
8896         bins: ArrayLike,
8897         right: bool = True,
8898         labels: ArrayLike | None = None,
8899         precision: int = 3,
8900         include_lowest: bool = False,
8901         squeeze: bool = True,
8902         restore_coord_dims: bool = False,
8903     ) -> DatasetGroupBy:
8904         """Returns a DatasetGroupBy object for performing grouped operations.
8905 
8906         Rather than using all unique values of `group`, the values are discretized
8907         first by applying `pandas.cut` [1]_ to `group`.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose binned values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         bins : int or array-like
8915             If bins is an int, it defines the number of equal-width bins in the
8916             range of x. However, in this case, the range of x is extended by .1%
8917             on each side to include the min or max values of x. If bins is a
8918             sequence it defines the bin edges allowing for non-uniform bin
8919             width. No extension of the range of x is done in this case.
8920         right : bool, default: True
8921             Indicates whether the bins include the rightmost edge or not. If
8922             right == True (the default), then the bins [1,2,3,4] indicate
8923             (1,2], (2,3], (3,4].
8924         labels : array-like or bool, default: None
8925             Used as labels for the resulting bins. Must be of the same length as
8926             the resulting bins. If False, string bin labels are assigned by
8927             `pandas.cut`.
8928         precision : int, default: 3
8929             The precision at which to store and display the bins labels.
8930         include_lowest : bool, default: False
8931             Whether the first interval should be left-inclusive or not.
8932         squeeze : bool, default: True
8933             If "group" is a dimension of any arrays in this dataset, `squeeze`
8934             controls whether the subarrays have a dimension of length 1 along
8935             that dimension or if the dimension is squeezed out.
8936         restore_coord_dims : bool, default: False
8937             If True, also restore the dimension order of multi-dimensional
8938             coordinates.
8939 
8940         Returns
8941         -------
8942         grouped : DatasetGroupBy
8943             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8944             iterated over in the form of `(unique_value, grouped_array)` pairs.
8945             The name of the group has the added suffix `_bins` in order to
8946             distinguish it from the original variable.
8947 
8948         See Also
8949         --------
8950         Dataset.groupby
8951         DataArray.groupby_bins
8952         core.groupby.DatasetGroupBy
8953         pandas.DataFrame.groupby
8954 
8955         References
8956         ----------
8957         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8958         """
8959         from .groupby import DatasetGroupBy
8960 
8961         return DatasetGroupBy(
8962             self,
8963             group,
8964             squeeze=squeeze,
8965             bins=bins,
8966             restore_coord_dims=restore_coord_dims,
8967             cut_kwargs={
8968                 "right": right,
8969                 "labels": labels,
8970                 "precision": precision,
8971                 "include_lowest": include_lowest,
8972             },
8973         )
8974 
8975     def weighted(self, weights: DataArray) -> DatasetWeighted:
8976         """
8977         Weighted Dataset operations.
8978 
8979         Parameters
8980         ----------
8981         weights : DataArray
8982             An array of weights associated with the values in this Dataset.
8983             Each value in the data contributes to the reduction operation
8984             according to its associated weight.
8985 
8986         Notes
8987         -----
8988         ``weights`` must be a DataArray and cannot contain missing values.
8989         Missing values can be replaced by ``weights.fillna(0)``.
8990 
8991         Returns
8992         -------
8993         core.weighted.DatasetWeighted
8994 
8995         See Also
8996         --------
8997         DataArray.weighted
8998         """
8999         from .weighted import DatasetWeighted
9000 
9001         return DatasetWeighted(self, weights)
9002 
9003     def rolling(
9004         self,
9005         dim: Mapping[Any, int] | None = None,
9006         min_periods: int | None = None,
9007         center: bool | Mapping[Any, bool] = False,
9008         **window_kwargs: int,
9009     ) -> DatasetRolling:
9010         """
9011         Rolling window object for Datasets.
9012 
9013         Parameters
9014         ----------
9015         dim : dict, optional
9016             Mapping from the dimension name to create the rolling iterator
9017             along (e.g. `time`) to its moving window size.
9018         min_periods : int or None, default: None
9019             Minimum number of observations in window required to have a value
9020             (otherwise result is NA). The default, None, is equivalent to
9021             setting min_periods equal to the size of the window.
9022         center : bool or Mapping to int, default: False
9023             Set the labels at the center of the window.
9024         **window_kwargs : optional
9025             The keyword arguments form of ``dim``.
9026             One of dim or window_kwargs must be provided.
9027 
9028         Returns
9029         -------
9030         core.rolling.DatasetRolling
9031 
9032         See Also
9033         --------
9034         core.rolling.DatasetRolling
9035         DataArray.rolling
9036         """
9037         from .rolling import DatasetRolling
9038 
9039         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9040         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9041 
9042     def coarsen(
9043         self,
9044         dim: Mapping[Any, int] | None = None,
9045         boundary: CoarsenBoundaryOptions = "exact",
9046         side: SideOptions | Mapping[Any, SideOptions] = "left",
9047         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9048         **window_kwargs: int,
9049     ) -> DatasetCoarsen:
9050         """
9051         Coarsen object for Datasets.
9052 
9053         Parameters
9054         ----------
9055         dim : mapping of hashable to int, optional
9056             Mapping from the dimension name to the window size.
9057         boundary : {"exact", "trim", "pad"}, default: "exact"
9058             If 'exact', a ValueError will be raised if dimension size is not a
9059             multiple of the window size. If 'trim', the excess entries are
9060             dropped. If 'pad', NA will be padded.
9061         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9062         coord_func : str or mapping of hashable to str, default: "mean"
9063             function (name) that is applied to the coordinates,
9064             or a mapping from coordinate name to function (name).
9065 
9066         Returns
9067         -------
9068         core.rolling.DatasetCoarsen
9069 
9070         See Also
9071         --------
9072         core.rolling.DatasetCoarsen
9073         DataArray.coarsen
9074         """
9075         from .rolling import DatasetCoarsen
9076 
9077         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9078         return DatasetCoarsen(
9079             self,
9080             dim,
9081             boundary=boundary,
9082             side=side,
9083             coord_func=coord_func,
9084         )
9085 
9086     def resample(
9087         self,
9088         indexer: Mapping[Any, str] | None = None,
9089         skipna: bool | None = None,
9090         closed: SideOptions | None = None,
9091         label: SideOptions | None = None,
9092         base: int = 0,
9093         keep_attrs: bool | None = None,
9094         loffset: datetime.timedelta | str | None = None,
9095         restore_coord_dims: bool | None = None,
9096         **indexer_kwargs: str,
9097     ) -> DatasetResample:
9098         """Returns a Resample object for performing resampling operations.
9099 
9100         Handles both downsampling and upsampling. The resampled
9101         dimension must be a datetime-like coordinate. If any intervals
9102         contain no values from the original object, they will be given
9103         the value ``NaN``.
9104 
9105         Parameters
9106         ----------
9107         indexer : Mapping of Hashable to str, optional
9108             Mapping from the dimension name to resample frequency [1]_. The
9109             dimension must be datetime-like.
9110         skipna : bool, optional
9111             Whether to skip missing values when aggregating in downsampling.
9112         closed : {"left", "right"}, optional
9113             Side of each interval to treat as closed.
9114         label : {"left", "right"}, optional
9115             Side of each interval to use for labeling.
9116         base : int, default = 0
9117             For frequencies that evenly subdivide 1 day, the "origin" of the
9118             aggregated intervals. For example, for "24H" frequency, base could
9119             range from 0 through 23.
9120         loffset : timedelta or str, optional
9121             Offset used to adjust the resampled time labels. Some pandas date
9122             offset strings are supported.
9123         restore_coord_dims : bool, optional
9124             If True, also restore the dimension order of multi-dimensional
9125             coordinates.
9126         **indexer_kwargs : str
9127             The keyword arguments form of ``indexer``.
9128             One of indexer or indexer_kwargs must be provided.
9129 
9130         Returns
9131         -------
9132         resampled : core.resample.DataArrayResample
9133             This object resampled.
9134 
9135         See Also
9136         --------
9137         DataArray.resample
9138         pandas.Series.resample
9139         pandas.DataFrame.resample
9140 
9141         References
9142         ----------
9143         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9144         """
9145         from .resample import DatasetResample
9146 
9147         return self._resample(
9148             resample_cls=DatasetResample,
9149             indexer=indexer,
9150             skipna=skipna,
9151             closed=closed,
9152             label=label,
9153             base=base,
9154             keep_attrs=keep_attrs,
9155             loffset=loffset,
9156             restore_coord_dims=restore_coord_dims,
9157             **indexer_kwargs,
9158         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the indexs type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the indexs type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def _rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Also used internally by DataArray so that the warning (if any)
3569         is raised at the right stack level.
3570         """
3571         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3572         for k in name_dict.keys():
3573             if k not in self and k not in self.dims:
3574                 raise ValueError(
3575                     f"cannot rename {k!r} because it is not a "
3576                     "variable or dimension in this dataset"
3577                 )
3578 
3579             create_dim_coord = False
3580             new_k = name_dict[k]
3581 
3582             if k in self.dims and new_k in self._coord_names:
3583                 coord_dims = self._variables[name_dict[k]].dims
3584                 if coord_dims == (k,):
3585                     create_dim_coord = True
3586             elif k in self._coord_names and new_k in self.dims:
3587                 coord_dims = self._variables[k].dims
3588                 if coord_dims == (new_k,):
3589                     create_dim_coord = True
3590 
3591             if create_dim_coord:
3592                 warnings.warn(
3593                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3594                     "anymore. Try using swap_dims instead or use set_index "
3595                     "after rename to create an indexed coordinate.",
3596                     UserWarning,
3597                     stacklevel=3,
3598                 )
3599 
3600         variables, coord_names, dims, indexes = self._rename_all(
3601             name_dict=name_dict, dims_dict=name_dict
3602         )
3603         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3604 
3605     def rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Returns a new object with renamed variables, coordinates and dimensions.
3611 
3612         Parameters
3613         ----------
3614         name_dict : dict-like, optional
3615             Dictionary whose keys are current variable, coordinate or dimension names and
3616             whose values are the desired names.
3617         **names : optional
3618             Keyword form of ``name_dict``.
3619             One of name_dict or names must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed variables, coordinates and dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename_vars
3630         Dataset.rename_dims
3631         DataArray.rename
3632         """
3633         return self._rename(name_dict=name_dict, **names)
3634 
3635     def rename_dims(
3636         self: T_Dataset,
3637         dims_dict: Mapping[Any, Hashable] | None = None,
3638         **dims: Hashable,
3639     ) -> T_Dataset:
3640         """Returns a new object with renamed dimensions only.
3641 
3642         Parameters
3643         ----------
3644         dims_dict : dict-like, optional
3645             Dictionary whose keys are current dimension names and
3646             whose values are the desired names. The desired names must
3647             not be the name of an existing dimension or Variable in the Dataset.
3648         **dims : optional
3649             Keyword form of ``dims_dict``.
3650             One of dims_dict or dims must be provided.
3651 
3652         Returns
3653         -------
3654         renamed : Dataset
3655             Dataset with renamed dimensions.
3656 
3657         See Also
3658         --------
3659         Dataset.swap_dims
3660         Dataset.rename
3661         Dataset.rename_vars
3662         DataArray.rename
3663         """
3664         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3665         for k, v in dims_dict.items():
3666             if k not in self.dims:
3667                 raise ValueError(
3668                     f"cannot rename {k!r} because it is not a "
3669                     "dimension in this dataset"
3670                 )
3671             if v in self.dims or v in self:
3672                 raise ValueError(
3673                     f"Cannot rename {k} to {v} because {v} already exists. "
3674                     "Try using swap_dims instead."
3675                 )
3676 
3677         variables, coord_names, sizes, indexes = self._rename_all(
3678             name_dict={}, dims_dict=dims_dict
3679         )
3680         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3681 
3682     def rename_vars(
3683         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3684     ) -> T_Dataset:
3685         """Returns a new object with renamed variables including coordinates
3686 
3687         Parameters
3688         ----------
3689         name_dict : dict-like, optional
3690             Dictionary whose keys are current variable or coordinate names and
3691             whose values are the desired names.
3692         **names : optional
3693             Keyword form of ``name_dict``.
3694             One of name_dict or names must be provided.
3695 
3696         Returns
3697         -------
3698         renamed : Dataset
3699             Dataset with renamed variables including coordinates
3700 
3701         See Also
3702         --------
3703         Dataset.swap_dims
3704         Dataset.rename
3705         Dataset.rename_dims
3706         DataArray.rename
3707         """
3708         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3709         for k in name_dict:
3710             if k not in self:
3711                 raise ValueError(
3712                     f"cannot rename {k!r} because it is not a "
3713                     "variable or coordinate in this dataset"
3714                 )
3715         variables, coord_names, dims, indexes = self._rename_all(
3716             name_dict=name_dict, dims_dict={}
3717         )
3718         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3719 
3720     def swap_dims(
3721         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3722     ) -> T_Dataset:
3723         """Returns a new object with swapped dimensions.
3724 
3725         Parameters
3726         ----------
3727         dims_dict : dict-like
3728             Dictionary whose keys are current dimension names and whose values
3729             are new names.
3730         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3731             The keyword arguments form of ``dims_dict``.
3732             One of dims_dict or dims_kwargs must be provided.
3733 
3734         Returns
3735         -------
3736         swapped : Dataset
3737             Dataset with swapped dimensions.
3738 
3739         Examples
3740         --------
3741         >>> ds = xr.Dataset(
3742         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3743         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3744         ... )
3745         >>> ds
3746         <xarray.Dataset>
3747         Dimensions:  (x: 2)
3748         Coordinates:
3749           * x        (x) <U1 'a' 'b'
3750             y        (x) int64 0 1
3751         Data variables:
3752             a        (x) int64 5 7
3753             b        (x) float64 0.1 2.4
3754 
3755         >>> ds.swap_dims({"x": "y"})
3756         <xarray.Dataset>
3757         Dimensions:  (y: 2)
3758         Coordinates:
3759             x        (y) <U1 'a' 'b'
3760           * y        (y) int64 0 1
3761         Data variables:
3762             a        (y) int64 5 7
3763             b        (y) float64 0.1 2.4
3764 
3765         >>> ds.swap_dims({"x": "z"})
3766         <xarray.Dataset>
3767         Dimensions:  (z: 2)
3768         Coordinates:
3769             x        (z) <U1 'a' 'b'
3770             y        (z) int64 0 1
3771         Dimensions without coordinates: z
3772         Data variables:
3773             a        (z) int64 5 7
3774             b        (z) float64 0.1 2.4
3775 
3776         See Also
3777         --------
3778         Dataset.rename
3779         DataArray.swap_dims
3780         """
3781         # TODO: deprecate this method in favor of a (less confusing)
3782         # rename_dims() method that only renames dimensions.
3783 
3784         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3785         for k, v in dims_dict.items():
3786             if k not in self.dims:
3787                 raise ValueError(
3788                     f"cannot swap from dimension {k!r} because it is "
3789                     "not an existing dimension"
3790                 )
3791             if v in self.variables and self.variables[v].dims != (k,):
3792                 raise ValueError(
3793                     f"replacement dimension {v!r} is not a 1D "
3794                     f"variable along the old dimension {k!r}"
3795                 )
3796 
3797         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3798 
3799         coord_names = self._coord_names.copy()
3800         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3801 
3802         variables: dict[Hashable, Variable] = {}
3803         indexes: dict[Hashable, Index] = {}
3804         for k, v in self.variables.items():
3805             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3806             var: Variable
3807             if k in result_dims:
3808                 var = v.to_index_variable()
3809                 var.dims = dims
3810                 if k in self._indexes:
3811                     indexes[k] = self._indexes[k]
3812                     variables[k] = var
3813                 else:
3814                     index, index_vars = create_default_index_implicit(var)
3815                     indexes.update({name: index for name in index_vars})
3816                     variables.update(index_vars)
3817                     coord_names.update(index_vars)
3818             else:
3819                 var = v.to_base_variable()
3820                 var.dims = dims
3821                 variables[k] = var
3822 
3823         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3824 
3825     # change type of self and return to T_Dataset once
3826     # https://github.com/python/mypy/issues/12846 is resolved
3827     def expand_dims(
3828         self,
3829         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3830         axis: None | int | Sequence[int] = None,
3831         **dim_kwargs: Any,
3832     ) -> Dataset:
3833         """Return a new object with an additional axis (or axes) inserted at
3834         the corresponding position in the array shape.  The new object is a
3835         view into the underlying array, not a copy.
3836 
3837         If dim is already a scalar coordinate, it will be promoted to a 1D
3838         coordinate consisting of a single value.
3839 
3840         Parameters
3841         ----------
3842         dim : hashable, sequence of hashable, mapping, or None
3843             Dimensions to include on the new variable. If provided as hashable
3844             or sequence of hashable, then dimensions are inserted with length
3845             1. If provided as a mapping, then the keys are the new dimensions
3846             and the values are either integers (giving the length of the new
3847             dimensions) or array-like (giving the coordinates of the new
3848             dimensions).
3849         axis : int, sequence of int, or None, default: None
3850             Axis position(s) where new axis is to be inserted (position(s) on
3851             the result array). If a sequence of integers is passed,
3852             multiple axes are inserted. In this case, dim arguments should be
3853             same length list. If axis=None is passed, all the axes will be
3854             inserted to the start of the result array.
3855         **dim_kwargs : int or sequence or ndarray
3856             The keywords are arbitrary dimensions being inserted and the values
3857             are either the lengths of the new dims (if int is given), or their
3858             coordinates. Note, this is an alternative to passing a dict to the
3859             dim kwarg and will only be used if dim is None.
3860 
3861         Returns
3862         -------
3863         expanded : Dataset
3864             This object, but with additional dimension(s).
3865 
3866         See Also
3867         --------
3868         DataArray.expand_dims
3869         """
3870         if dim is None:
3871             pass
3872         elif isinstance(dim, Mapping):
3873             # We're later going to modify dim in place; don't tamper with
3874             # the input
3875             dim = dict(dim)
3876         elif isinstance(dim, int):
3877             raise TypeError(
3878                 "dim should be hashable or sequence of hashables or mapping"
3879             )
3880         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3881             dim = {dim: 1}
3882         elif isinstance(dim, Sequence):
3883             if len(dim) != len(set(dim)):
3884                 raise ValueError("dims should not contain duplicate values.")
3885             dim = {d: 1 for d in dim}
3886 
3887         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3888         assert isinstance(dim, MutableMapping)
3889 
3890         if axis is None:
3891             axis = list(range(len(dim)))
3892         elif not isinstance(axis, Sequence):
3893             axis = [axis]
3894 
3895         if len(dim) != len(axis):
3896             raise ValueError("lengths of dim and axis should be identical.")
3897         for d in dim:
3898             if d in self.dims:
3899                 raise ValueError(f"Dimension {d} already exists.")
3900             if d in self._variables and not utils.is_scalar(self._variables[d]):
3901                 raise ValueError(
3902                     "{dim} already exists as coordinate or"
3903                     " variable name.".format(dim=d)
3904                 )
3905 
3906         variables: dict[Hashable, Variable] = {}
3907         indexes: dict[Hashable, Index] = dict(self._indexes)
3908         coord_names = self._coord_names.copy()
3909         # If dim is a dict, then ensure that the values are either integers
3910         # or iterables.
3911         for k, v in dim.items():
3912             if hasattr(v, "__iter__"):
3913                 # If the value for the new dimension is an iterable, then
3914                 # save the coordinates to the variables dict, and set the
3915                 # value within the dim dict to the length of the iterable
3916                 # for later use.
3917                 index = PandasIndex(v, k)
3918                 indexes[k] = index
3919                 variables.update(index.create_variables())
3920                 coord_names.add(k)
3921                 dim[k] = variables[k].size
3922             elif isinstance(v, int):
3923                 pass  # Do nothing if the dimensions value is just an int
3924             else:
3925                 raise TypeError(
3926                     "The value of new dimension {k} must be "
3927                     "an iterable or an int".format(k=k)
3928                 )
3929 
3930         for k, v in self._variables.items():
3931             if k not in dim:
3932                 if k in coord_names:  # Do not change coordinates
3933                     variables[k] = v
3934                 else:
3935                     result_ndim = len(v.dims) + len(axis)
3936                     for a in axis:
3937                         if a < -result_ndim or result_ndim - 1 < a:
3938                             raise IndexError(
3939                                 f"Axis {a} of variable {k} is out of bounds of the "
3940                                 f"expanded dimension size {result_ndim}"
3941                             )
3942 
3943                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3944                     if len(axis_pos) != len(set(axis_pos)):
3945                         raise ValueError("axis should not contain duplicate values")
3946                     # We need to sort them to make sure `axis` equals to the
3947                     # axis positions of the result array.
3948                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3949 
3950                     all_dims = list(zip(v.dims, v.shape))
3951                     for d, c in zip_axis_dim:
3952                         all_dims.insert(d, c)
3953                     variables[k] = v.set_dims(dict(all_dims))
3954             else:
3955                 if k not in variables:
3956                     # If dims includes a label of a non-dimension coordinate,
3957                     # it will be promoted to a 1D coordinate with a single value.
3958                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3959                     indexes[k] = index
3960                     variables.update(index_vars)
3961 
3962         return self._replace_with_new_dims(
3963             variables, coord_names=coord_names, indexes=indexes
3964         )
3965 
3966     # change type of self and return to T_Dataset once
3967     # https://github.com/python/mypy/issues/12846 is resolved
3968     def set_index(
3969         self,
3970         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3971         append: bool = False,
3972         **indexes_kwargs: Hashable | Sequence[Hashable],
3973     ) -> Dataset:
3974         """Set Dataset (multi-)indexes using one or more existing coordinates
3975         or variables.
3976 
3977         This legacy method is limited to pandas (multi-)indexes and
3978         1-dimensional "dimension" coordinates. See
3979         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
3980         Xarray-compatible index from one or more arbitrary coordinates.
3981 
3982         Parameters
3983         ----------
3984         indexes : {dim: index, ...}
3985             Mapping from names matching dimensions and values given
3986             by (lists of) the names of existing coordinates or variables to set
3987             as new (multi-)index.
3988         append : bool, default: False
3989             If True, append the supplied index(es) to the existing index(es).
3990             Otherwise replace the existing index(es) (default).
3991         **indexes_kwargs : optional
3992             The keyword arguments form of ``indexes``.
3993             One of indexes or indexes_kwargs must be provided.
3994 
3995         Returns
3996         -------
3997         obj : Dataset
3998             Another dataset, with this dataset's data but replaced coordinates.
3999 
4000         Examples
4001         --------
4002         >>> arr = xr.DataArray(
4003         ...     data=np.ones((2, 3)),
4004         ...     dims=["x", "y"],
4005         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4006         ... )
4007         >>> ds = xr.Dataset({"v": arr})
4008         >>> ds
4009         <xarray.Dataset>
4010         Dimensions:  (x: 2, y: 3)
4011         Coordinates:
4012           * x        (x) int64 0 1
4013           * y        (y) int64 0 1 2
4014             a        (x) int64 3 4
4015         Data variables:
4016             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4017         >>> ds.set_index(x="a")
4018         <xarray.Dataset>
4019         Dimensions:  (x: 2, y: 3)
4020         Coordinates:
4021           * x        (x) int64 3 4
4022           * y        (y) int64 0 1 2
4023         Data variables:
4024             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4025 
4026         See Also
4027         --------
4028         Dataset.reset_index
4029         Dataset.set_xindex
4030         Dataset.swap_dims
4031         """
4032         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4033 
4034         new_indexes: dict[Hashable, Index] = {}
4035         new_variables: dict[Hashable, Variable] = {}
4036         drop_indexes: set[Hashable] = set()
4037         drop_variables: set[Hashable] = set()
4038         replace_dims: dict[Hashable, Hashable] = {}
4039         all_var_names: set[Hashable] = set()
4040 
4041         for dim, _var_names in dim_coords.items():
4042             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4043                 var_names = [_var_names]
4044             else:
4045                 var_names = list(_var_names)
4046 
4047             invalid_vars = set(var_names) - set(self._variables)
4048             if invalid_vars:
4049                 raise ValueError(
4050                     ", ".join([str(v) for v in invalid_vars])
4051                     + " variable(s) do not exist"
4052                 )
4053 
4054             all_var_names.update(var_names)
4055             drop_variables.update(var_names)
4056 
4057             # drop any pre-existing index involved and its corresponding coordinates
4058             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4059             all_index_coord_names = set(index_coord_names)
4060             for k in var_names:
4061                 all_index_coord_names.update(
4062                     self.xindexes.get_all_coords(k, errors="ignore")
4063                 )
4064 
4065             drop_indexes.update(all_index_coord_names)
4066             drop_variables.update(all_index_coord_names)
4067 
4068             if len(var_names) == 1 and (not append or dim not in self._indexes):
4069                 var_name = var_names[0]
4070                 var = self._variables[var_name]
4071                 if var.dims != (dim,):
4072                     raise ValueError(
4073                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4074                         f"variable {var_name!r} that has dimensions {var.dims}"
4075                     )
4076                 idx = PandasIndex.from_variables({dim: var}, options={})
4077                 idx_vars = idx.create_variables({var_name: var})
4078 
4079                 # trick to preserve coordinate order in this case
4080                 if dim in self._coord_names:
4081                     drop_variables.remove(dim)
4082             else:
4083                 if append:
4084                     current_variables = {
4085                         k: self._variables[k] for k in index_coord_names
4086                     }
4087                 else:
4088                     current_variables = {}
4089                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4090                     dim,
4091                     current_variables,
4092                     {k: self._variables[k] for k in var_names},
4093                 )
4094                 for n in idx.index.names:
4095                     replace_dims[n] = dim
4096 
4097             new_indexes.update({k: idx for k in idx_vars})
4098             new_variables.update(idx_vars)
4099 
4100         # re-add deindexed coordinates (convert to base variables)
4101         for k in drop_variables:
4102             if (
4103                 k not in new_variables
4104                 and k not in all_var_names
4105                 and k in self._coord_names
4106             ):
4107                 new_variables[k] = self._variables[k].to_base_variable()
4108 
4109         indexes_: dict[Any, Index] = {
4110             k: v for k, v in self._indexes.items() if k not in drop_indexes
4111         }
4112         indexes_.update(new_indexes)
4113 
4114         variables = {
4115             k: v for k, v in self._variables.items() if k not in drop_variables
4116         }
4117         variables.update(new_variables)
4118 
4119         # update dimensions if necessary, GH: 3512
4120         for k, v in variables.items():
4121             if any(d in replace_dims for d in v.dims):
4122                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4123                 variables[k] = v._replace(dims=new_dims)
4124 
4125         coord_names = self._coord_names - drop_variables | set(new_variables)
4126 
4127         return self._replace_with_new_dims(
4128             variables, coord_names=coord_names, indexes=indexes_
4129         )
4130 
4131     def reset_index(
4132         self: T_Dataset,
4133         dims_or_levels: Hashable | Sequence[Hashable],
4134         drop: bool = False,
4135     ) -> T_Dataset:
4136         """Reset the specified index(es) or multi-index level(s).
4137 
4138         This legacy method is specific to pandas (multi-)indexes and
4139         1-dimensional "dimension" coordinates. See the more generic
4140         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4141         method to respectively drop and set pandas or custom indexes for
4142         arbitrary coordinates.
4143 
4144         Parameters
4145         ----------
4146         dims_or_levels : Hashable or Sequence of Hashable
4147             Name(s) of the dimension(s) and/or multi-index level(s) that will
4148             be reset.
4149         drop : bool, default: False
4150             If True, remove the specified indexes and/or multi-index levels
4151             instead of extracting them as new coordinates (default: False).
4152 
4153         Returns
4154         -------
4155         obj : Dataset
4156             Another dataset, with this dataset's data but replaced coordinates.
4157 
4158         See Also
4159         --------
4160         Dataset.set_index
4161         Dataset.set_xindex
4162         Dataset.drop_indexes
4163         """
4164         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4165             dims_or_levels = [dims_or_levels]
4166 
4167         invalid_coords = set(dims_or_levels) - set(self._indexes)
4168         if invalid_coords:
4169             raise ValueError(
4170                 f"{tuple(invalid_coords)} are not coordinates with an index"
4171             )
4172 
4173         drop_indexes: set[Hashable] = set()
4174         drop_variables: set[Hashable] = set()
4175         seen: set[Index] = set()
4176         new_indexes: dict[Hashable, Index] = {}
4177         new_variables: dict[Hashable, Variable] = {}
4178 
4179         def drop_or_convert(var_names):
4180             if drop:
4181                 drop_variables.update(var_names)
4182             else:
4183                 base_vars = {
4184                     k: self._variables[k].to_base_variable() for k in var_names
4185                 }
4186                 new_variables.update(base_vars)
4187 
4188         for name in dims_or_levels:
4189             index = self._indexes[name]
4190 
4191             if index in seen:
4192                 continue
4193             seen.add(index)
4194 
4195             idx_var_names = set(self.xindexes.get_all_coords(name))
4196             drop_indexes.update(idx_var_names)
4197 
4198             if isinstance(index, PandasMultiIndex):
4199                 # special case for pd.MultiIndex
4200                 level_names = index.index.names
4201                 keep_level_vars = {
4202                     k: self._variables[k]
4203                     for k in level_names
4204                     if k not in dims_or_levels
4205                 }
4206 
4207                 if index.dim not in dims_or_levels and keep_level_vars:
4208                     # do not drop the multi-index completely
4209                     # instead replace it by a new (multi-)index with dropped level(s)
4210                     idx = index.keep_levels(keep_level_vars)
4211                     idx_vars = idx.create_variables(keep_level_vars)
4212                     new_indexes.update({k: idx for k in idx_vars})
4213                     new_variables.update(idx_vars)
4214                     if not isinstance(idx, PandasMultiIndex):
4215                         # multi-index reduced to single index
4216                         # backward compatibility: unique level coordinate renamed to dimension
4217                         drop_variables.update(keep_level_vars)
4218                     drop_or_convert(
4219                         [k for k in level_names if k not in keep_level_vars]
4220                     )
4221                 else:
4222                     # always drop the multi-index dimension variable
4223                     drop_variables.add(index.dim)
4224                     drop_or_convert(level_names)
4225             else:
4226                 drop_or_convert(idx_var_names)
4227 
4228         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4229         indexes.update(new_indexes)
4230 
4231         variables = {
4232             k: v for k, v in self._variables.items() if k not in drop_variables
4233         }
4234         variables.update(new_variables)
4235 
4236         coord_names = self._coord_names - drop_variables
4237 
4238         return self._replace_with_new_dims(
4239             variables, coord_names=coord_names, indexes=indexes
4240         )
4241 
4242     def set_xindex(
4243         self: T_Dataset,
4244         coord_names: str | Sequence[Hashable],
4245         index_cls: type[Index] | None = None,
4246         **options,
4247     ) -> T_Dataset:
4248         """Set a new, Xarray-compatible index from one or more existing
4249         coordinate(s).
4250 
4251         Parameters
4252         ----------
4253         coord_names : str or list
4254             Name(s) of the coordinate(s) used to build the index.
4255             If several names are given, their order matters.
4256         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4257             The type of index to create. By default, try setting
4258             a ``PandasIndex`` if ``len(coord_names) == 1``,
4259             otherwise a ``PandasMultiIndex``.
4260         **options
4261             Options passed to the index constructor.
4262 
4263         Returns
4264         -------
4265         obj : Dataset
4266             Another dataset, with this dataset's data and with a new index.
4267 
4268         """
4269         # the Sequence check is required for mypy
4270         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4271             coord_names = [coord_names]
4272 
4273         if index_cls is None:
4274             if len(coord_names) == 1:
4275                 index_cls = PandasIndex
4276             else:
4277                 index_cls = PandasMultiIndex
4278         else:
4279             if not issubclass(index_cls, Index):
4280                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4281 
4282         invalid_coords = set(coord_names) - self._coord_names
4283 
4284         if invalid_coords:
4285             msg = ["invalid coordinate(s)"]
4286             no_vars = invalid_coords - set(self._variables)
4287             data_vars = invalid_coords - no_vars
4288             if no_vars:
4289                 msg.append(f"those variables don't exist: {no_vars}")
4290             if data_vars:
4291                 msg.append(
4292                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4293                 )
4294             raise ValueError("\n".join(msg))
4295 
4296         # we could be more clever here (e.g., drop-in index replacement if index
4297         # coordinates do not conflict), but let's not allow this for now
4298         indexed_coords = set(coord_names) & set(self._indexes)
4299 
4300         if indexed_coords:
4301             raise ValueError(
4302                 f"those coordinates already have an index: {indexed_coords}"
4303             )
4304 
4305         coord_vars = {name: self._variables[name] for name in coord_names}
4306 
4307         index = index_cls.from_variables(coord_vars, options=options)
4308 
4309         new_coord_vars = index.create_variables(coord_vars)
4310 
4311         # special case for setting a pandas multi-index from level coordinates
4312         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4313         # elements) coordinate
4314         if isinstance(index, PandasMultiIndex):
4315             coord_names = [index.dim] + list(coord_names)
4316 
4317         variables: dict[Hashable, Variable]
4318         indexes: dict[Hashable, Index]
4319 
4320         if len(coord_names) == 1:
4321             variables = self._variables.copy()
4322             indexes = self._indexes.copy()
4323 
4324             name = list(coord_names).pop()
4325             if name in new_coord_vars:
4326                 variables[name] = new_coord_vars[name]
4327             indexes[name] = index
4328         else:
4329             # reorder variables and indexes so that coordinates having the same
4330             # index are next to each other
4331             variables = {}
4332             for name, var in self._variables.items():
4333                 if name not in coord_names:
4334                     variables[name] = var
4335 
4336             indexes = {}
4337             for name, idx in self._indexes.items():
4338                 if name not in coord_names:
4339                     indexes[name] = idx
4340 
4341             for name in coord_names:
4342                 try:
4343                     variables[name] = new_coord_vars[name]
4344                 except KeyError:
4345                     variables[name] = self._variables[name]
4346                 indexes[name] = index
4347 
4348         return self._replace(
4349             variables=variables,
4350             coord_names=self._coord_names | set(coord_names),
4351             indexes=indexes,
4352         )
4353 
4354     def reorder_levels(
4355         self: T_Dataset,
4356         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4357         **dim_order_kwargs: Sequence[int | Hashable],
4358     ) -> T_Dataset:
4359         """Rearrange index levels using input order.
4360 
4361         Parameters
4362         ----------
4363         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4364             Mapping from names matching dimensions and values given
4365             by lists representing new level orders. Every given dimension
4366             must have a multi-index.
4367         **dim_order_kwargs : Sequence of int or Hashable, optional
4368             The keyword arguments form of ``dim_order``.
4369             One of dim_order or dim_order_kwargs must be provided.
4370 
4371         Returns
4372         -------
4373         obj : Dataset
4374             Another dataset, with this dataset's data but replaced
4375             coordinates.
4376         """
4377         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4378         variables = self._variables.copy()
4379         indexes = dict(self._indexes)
4380         new_indexes: dict[Hashable, Index] = {}
4381         new_variables: dict[Hashable, IndexVariable] = {}
4382 
4383         for dim, order in dim_order.items():
4384             index = self._indexes[dim]
4385 
4386             if not isinstance(index, PandasMultiIndex):
4387                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4388 
4389             level_vars = {k: self._variables[k] for k in order}
4390             idx = index.reorder_levels(level_vars)
4391             idx_vars = idx.create_variables(level_vars)
4392             new_indexes.update({k: idx for k in idx_vars})
4393             new_variables.update(idx_vars)
4394 
4395         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4396         indexes.update(new_indexes)
4397 
4398         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4399         variables.update(new_variables)
4400 
4401         return self._replace(variables, indexes=indexes)
4402 
4403     def _get_stack_index(
4404         self,
4405         dim,
4406         multi=False,
4407         create_index=False,
4408     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4409         """Used by stack and unstack to get one pandas (multi-)index among
4410         the indexed coordinates along dimension `dim`.
4411 
4412         If exactly one index is found, return it with its corresponding
4413         coordinate variables(s), otherwise return None and an empty dict.
4414 
4415         If `create_index=True`, create a new index if none is found or raise
4416         an error if multiple indexes are found.
4417 
4418         """
4419         stack_index: Index | None = None
4420         stack_coords: dict[Hashable, Variable] = {}
4421 
4422         for name, index in self._indexes.items():
4423             var = self._variables[name]
4424             if (
4425                 var.ndim == 1
4426                 and var.dims[0] == dim
4427                 and (
4428                     # stack: must be a single coordinate index
4429                     not multi
4430                     and not self.xindexes.is_multi(name)
4431                     # unstack: must be an index that implements .unstack
4432                     or multi
4433                     and type(index).unstack is not Index.unstack
4434                 )
4435             ):
4436                 if stack_index is not None and index is not stack_index:
4437                     # more than one index found, stop
4438                     if create_index:
4439                         raise ValueError(
4440                             f"cannot stack dimension {dim!r} with `create_index=True` "
4441                             "and with more than one index found along that dimension"
4442                         )
4443                     return None, {}
4444                 stack_index = index
4445                 stack_coords[name] = var
4446 
4447         if create_index and stack_index is None:
4448             if dim in self._variables:
4449                 var = self._variables[dim]
4450             else:
4451                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4452             # dummy index (only `stack_coords` will be used to construct the multi-index)
4453             stack_index = PandasIndex([0], dim)
4454             stack_coords = {dim: var}
4455 
4456         return stack_index, stack_coords
4457 
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )
4516 
4517     def stack(
4518         self: T_Dataset,
4519         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4520         create_index: bool | None = True,
4521         index_cls: type[Index] = PandasMultiIndex,
4522         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4523     ) -> T_Dataset:
4524         """
4525         Stack any number of existing dimensions into a single new dimension.
4526 
4527         New dimensions will be added at the end, and by default the corresponding
4528         coordinate variables will be combined into a MultiIndex.
4529 
4530         Parameters
4531         ----------
4532         dimensions : mapping of hashable to sequence of hashable
4533             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4534             dimensions, and the existing dimensions that they replace. An
4535             ellipsis (`...`) will be replaced by all unlisted dimensions.
4536             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4537             all dimensions.
4538         create_index : bool or None, default: True
4539 
4540             - True: create a multi-index for each of the stacked dimensions.
4541             - False: don't create any index.
4542             - None. create a multi-index only if exactly one single (1-d) coordinate
4543               index is found for every dimension to stack.
4544 
4545         index_cls: Index-class, default: PandasMultiIndex
4546             Can be used to pass a custom multi-index type (must be an Xarray index that
4547             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4548         **dimensions_kwargs
4549             The keyword arguments form of ``dimensions``.
4550             One of dimensions or dimensions_kwargs must be provided.
4551 
4552         Returns
4553         -------
4554         stacked : Dataset
4555             Dataset with stacked data.
4556 
4557         See Also
4558         --------
4559         Dataset.unstack
4560         """
4561         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4562         result = self
4563         for new_dim, dims in dimensions.items():
4564             result = result._stack_once(dims, new_dim, index_cls, create_index)
4565         return result
4566 
4567     def to_stacked_array(
4568         self,
4569         new_dim: Hashable,
4570         sample_dims: Collection[Hashable],
4571         variable_dim: Hashable = "variable",
4572         name: Hashable | None = None,
4573     ) -> DataArray:
4574         """Combine variables of differing dimensionality into a DataArray
4575         without broadcasting.
4576 
4577         This method is similar to Dataset.to_array but does not broadcast the
4578         variables.
4579 
4580         Parameters
4581         ----------
4582         new_dim : hashable
4583             Name of the new stacked coordinate
4584         sample_dims : Collection of hashables
4585             List of dimensions that **will not** be stacked. Each array in the
4586             dataset must share these dimensions. For machine learning
4587             applications, these define the dimensions over which samples are
4588             drawn.
4589         variable_dim : hashable, default: "variable"
4590             Name of the level in the stacked coordinate which corresponds to
4591             the variables.
4592         name : hashable, optional
4593             Name of the new data array.
4594 
4595         Returns
4596         -------
4597         stacked : DataArray
4598             DataArray with the specified dimensions and data variables
4599             stacked together. The stacked coordinate is named ``new_dim``
4600             and represented by a MultiIndex object with a level containing the
4601             data variable names. The name of this level is controlled using
4602             the ``variable_dim`` argument.
4603 
4604         See Also
4605         --------
4606         Dataset.to_array
4607         Dataset.stack
4608         DataArray.to_unstacked_dataset
4609 
4610         Examples
4611         --------
4612         >>> data = xr.Dataset(
4613         ...     data_vars={
4614         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4615         ...         "b": ("x", [6, 7]),
4616         ...     },
4617         ...     coords={"y": ["u", "v", "w"]},
4618         ... )
4619 
4620         >>> data
4621         <xarray.Dataset>
4622         Dimensions:  (x: 2, y: 3)
4623         Coordinates:
4624           * y        (y) <U1 'u' 'v' 'w'
4625         Dimensions without coordinates: x
4626         Data variables:
4627             a        (x, y) int64 0 1 2 3 4 5
4628             b        (x) int64 6 7
4629 
4630         >>> data.to_stacked_array("z", sample_dims=["x"])
4631         <xarray.DataArray 'a' (x: 2, z: 4)>
4632         array([[0, 1, 2, 6],
4633                [3, 4, 5, 7]])
4634         Coordinates:
4635           * z         (z) object MultiIndex
4636           * variable  (z) object 'a' 'a' 'a' 'b'
4637           * y         (z) object 'u' 'v' 'w' nan
4638         Dimensions without coordinates: x
4639 
4640         """
4641         from .concat import concat
4642 
4643         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4644 
4645         for variable in self:
4646             dims = self[variable].dims
4647             dims_include_sample_dims = set(sample_dims) <= set(dims)
4648             if not dims_include_sample_dims:
4649                 raise ValueError(
4650                     "All variables in the dataset must contain the "
4651                     "dimensions {}.".format(dims)
4652                 )
4653 
4654         def ensure_stackable(val):
4655             assign_coords = {variable_dim: val.name}
4656             for dim in stacking_dims:
4657                 if dim not in val.dims:
4658                     assign_coords[dim] = None
4659 
4660             expand_dims = set(stacking_dims).difference(set(val.dims))
4661             expand_dims.add(variable_dim)
4662             # must be list for .expand_dims
4663             expand_dims = list(expand_dims)
4664 
4665             return (
4666                 val.assign_coords(**assign_coords)
4667                 .expand_dims(expand_dims)
4668                 .stack({new_dim: (variable_dim,) + stacking_dims})
4669             )
4670 
4671         # concatenate the arrays
4672         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4673         data_array = concat(stackable_vars, dim=new_dim)
4674 
4675         if name is not None:
4676             data_array.name = name
4677 
4678         return data_array
4679 
4680     def _unstack_once(
4681         self: T_Dataset,
4682         dim: Hashable,
4683         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4684         fill_value,
4685         sparse: bool = False,
4686     ) -> T_Dataset:
4687         index, index_vars = index_and_vars
4688         variables: dict[Hashable, Variable] = {}
4689         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4690 
4691         new_indexes, clean_index = index.unstack()
4692         indexes.update(new_indexes)
4693 
4694         for name, idx in new_indexes.items():
4695             variables.update(idx.create_variables(index_vars))
4696 
4697         for name, var in self.variables.items():
4698             if name not in index_vars:
4699                 if dim in var.dims:
4700                     if isinstance(fill_value, Mapping):
4701                         fill_value_ = fill_value[name]
4702                     else:
4703                         fill_value_ = fill_value
4704 
4705                     variables[name] = var._unstack_once(
4706                         index=clean_index,
4707                         dim=dim,
4708                         fill_value=fill_value_,
4709                         sparse=sparse,
4710                     )
4711                 else:
4712                     variables[name] = var
4713 
4714         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4715 
4716         return self._replace_with_new_dims(
4717             variables, coord_names=coord_names, indexes=indexes
4718         )
4719 
4720     def _unstack_full_reindex(
4721         self: T_Dataset,
4722         dim: Hashable,
4723         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4724         fill_value,
4725         sparse: bool,
4726     ) -> T_Dataset:
4727         index, index_vars = index_and_vars
4728         variables: dict[Hashable, Variable] = {}
4729         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4730 
4731         new_indexes, clean_index = index.unstack()
4732         indexes.update(new_indexes)
4733 
4734         new_index_variables = {}
4735         for name, idx in new_indexes.items():
4736             new_index_variables.update(idx.create_variables(index_vars))
4737 
4738         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4739         variables.update(new_index_variables)
4740 
4741         # take a shortcut in case the MultiIndex was not modified.
4742         full_idx = pd.MultiIndex.from_product(
4743             clean_index.levels, names=clean_index.names
4744         )
4745         if clean_index.equals(full_idx):
4746             obj = self
4747         else:
4748             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4749             xr_full_idx = PandasMultiIndex(full_idx, dim)
4750             indexers = Indexes(
4751                 {k: xr_full_idx for k in index_vars},
4752                 xr_full_idx.create_variables(index_vars),
4753             )
4754             obj = self._reindex(
4755                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4756             )
4757 
4758         for name, var in obj.variables.items():
4759             if name not in index_vars:
4760                 if dim in var.dims:
4761                     variables[name] = var.unstack({dim: new_dim_sizes})
4762                 else:
4763                     variables[name] = var
4764 
4765         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4766 
4767         return self._replace_with_new_dims(
4768             variables, coord_names=coord_names, indexes=indexes
4769         )
4770 
4771     def unstack(
4772         self: T_Dataset,
4773         dim: Hashable | Iterable[Hashable] | None = None,
4774         fill_value: Any = xrdtypes.NA,
4775         sparse: bool = False,
4776     ) -> T_Dataset:
4777         """
4778         Unstack existing dimensions corresponding to MultiIndexes into
4779         multiple new dimensions.
4780 
4781         New dimensions will be added at the end.
4782 
4783         Parameters
4784         ----------
4785         dim : hashable or iterable of hashable, optional
4786             Dimension(s) over which to unstack. By default unstacks all
4787             MultiIndexes.
4788         fill_value : scalar or dict-like, default: nan
4789             value to be filled. If a dict-like, maps variable names to
4790             fill values. If not provided or if the dict-like does not
4791             contain all variables, the dtype's NA value will be used.
4792         sparse : bool, default: False
4793             use sparse-array if True
4794 
4795         Returns
4796         -------
4797         unstacked : Dataset
4798             Dataset with unstacked data.
4799 
4800         See Also
4801         --------
4802         Dataset.stack
4803         """
4804 
4805         if dim is None:
4806             dims = list(self.dims)
4807         else:
4808             if isinstance(dim, str) or not isinstance(dim, Iterable):
4809                 dims = [dim]
4810             else:
4811                 dims = list(dim)
4812 
4813             missing_dims = [d for d in dims if d not in self.dims]
4814             if missing_dims:
4815                 raise ValueError(
4816                     f"Dataset does not contain the dimensions: {missing_dims}"
4817                 )
4818 
4819         # each specified dimension must have exactly one multi-index
4820         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4821         for d in dims:
4822             idx, idx_vars = self._get_stack_index(d, multi=True)
4823             if idx is not None:
4824                 stacked_indexes[d] = idx, idx_vars
4825 
4826         if dim is None:
4827             dims = list(stacked_indexes)
4828         else:
4829             non_multi_dims = set(dims) - set(stacked_indexes)
4830             if non_multi_dims:
4831                 raise ValueError(
4832                     "cannot unstack dimensions that do not "
4833                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4834                 )
4835 
4836         result = self.copy(deep=False)
4837 
4838         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4839         # so we can't just access self.variables[v].data for every variable.
4840         # We only check the non-index variables.
4841         # https://github.com/pydata/xarray/issues/5902
4842         nonindexes = [
4843             self.variables[k] for k in set(self.variables) - set(self._indexes)
4844         ]
4845         # Notes for each of these cases:
4846         # 1. Dask arrays don't support assignment by index, which the fast unstack
4847         #    function requires.
4848         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4849         # 2. Sparse doesn't currently support (though we could special-case it)
4850         #    https://github.com/pydata/sparse/issues/422
4851         # 3. pint requires checking if it's a NumPy array until
4852         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4853         #    Once that is resolved, explicitly exclude pint arrays.
4854         #    pint doesn't implement `np.full_like` in a way that's
4855         #    currently compatible.
4856         needs_full_reindex = any(
4857             is_duck_dask_array(v.data)
4858             or isinstance(v.data, sparse_array_type)
4859             or not isinstance(v.data, np.ndarray)
4860             for v in nonindexes
4861         )
4862 
4863         for dim in dims:
4864             if needs_full_reindex:
4865                 result = result._unstack_full_reindex(
4866                     dim, stacked_indexes[dim], fill_value, sparse
4867                 )
4868             else:
4869                 result = result._unstack_once(
4870                     dim, stacked_indexes[dim], fill_value, sparse
4871                 )
4872         return result
4873 
4874     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4875         """Update this dataset's variables with those from another dataset.
4876 
4877         Just like :py:meth:`dict.update` this is a in-place operation.
4878         For a non-inplace version, see :py:meth:`Dataset.merge`.
4879 
4880         Parameters
4881         ----------
4882         other : Dataset or mapping
4883             Variables with which to update this dataset. One of:
4884 
4885             - Dataset
4886             - mapping {var name: DataArray}
4887             - mapping {var name: Variable}
4888             - mapping {var name: (dimension name, array-like)}
4889             - mapping {var name: (tuple of dimension names, array-like)}
4890 
4891         Returns
4892         -------
4893         updated : Dataset
4894             Updated dataset. Note that since the update is in-place this is the input
4895             dataset.
4896 
4897             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4898 
4899         Raises
4900         ------
4901         ValueError
4902             If any dimensions would have inconsistent sizes in the updated
4903             dataset.
4904 
4905         See Also
4906         --------
4907         Dataset.assign
4908         Dataset.merge
4909         """
4910         merge_result = dataset_update_method(self, other)
4911         return self._replace(inplace=True, **merge_result._asdict())
4912 
4913     def merge(
4914         self: T_Dataset,
4915         other: CoercibleMapping | DataArray,
4916         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4917         compat: CompatOptions = "no_conflicts",
4918         join: JoinOptions = "outer",
4919         fill_value: Any = xrdtypes.NA,
4920         combine_attrs: CombineAttrsOptions = "override",
4921     ) -> T_Dataset:
4922         """Merge the arrays of two datasets into a single dataset.
4923 
4924         This method generally does not allow for overriding data, with the
4925         exception of attributes, which are ignored on the second dataset.
4926         Variables with the same name are checked for conflicts via the equals
4927         or identical methods.
4928 
4929         Parameters
4930         ----------
4931         other : Dataset or mapping
4932             Dataset or variables to merge with this dataset.
4933         overwrite_vars : hashable or iterable of hashable, optional
4934             If provided, update variables of these name(s) without checking for
4935             conflicts in this dataset.
4936         compat : {"identical", "equals", "broadcast_equals", \
4937                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4938             String indicating how to compare variables of the same name for
4939             potential conflicts:
4940 
4941             - 'identical': all values, dimensions and attributes must be the
4942               same.
4943             - 'equals': all values and dimensions must be the same.
4944             - 'broadcast_equals': all values must be equal when variables are
4945               broadcast against each other to ensure common dimensions.
4946             - 'no_conflicts': only values which are not null in both datasets
4947               must be equal. The returned dataset then contains the combination
4948               of all non-null values.
4949             - 'override': skip comparing and pick variable from first dataset
4950             - 'minimal': drop conflicting coordinates
4951 
4952         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4953                default: "outer"
4954             Method for joining ``self`` and ``other`` along shared dimensions:
4955 
4956             - 'outer': use the union of the indexes
4957             - 'inner': use the intersection of the indexes
4958             - 'left': use indexes from ``self``
4959             - 'right': use indexes from ``other``
4960             - 'exact': error instead of aligning non-equal indexes
4961             - 'override': use indexes from ``self`` that are the same size
4962               as those of ``other`` in that dimension
4963 
4964         fill_value : scalar or dict-like, optional
4965             Value to use for newly missing values. If a dict-like, maps
4966             variable names (including coordinates) to fill values.
4967         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4968                          "override"} or callable, default: "override"
4969             A callable or a string indicating how to combine attrs of the objects being
4970             merged:
4971 
4972             - "drop": empty attrs on returned Dataset.
4973             - "identical": all attrs must be the same on every object.
4974             - "no_conflicts": attrs from all objects are combined, any that have
4975               the same name must also have the same value.
4976             - "drop_conflicts": attrs from all objects are combined, any that have
4977               the same name but different values are dropped.
4978             - "override": skip comparing and copy attrs from the first dataset to
4979               the result.
4980 
4981             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4982             as its only parameters.
4983 
4984         Returns
4985         -------
4986         merged : Dataset
4987             Merged dataset.
4988 
4989         Raises
4990         ------
4991         MergeError
4992             If any variables conflict (see ``compat``).
4993 
4994         See Also
4995         --------
4996         Dataset.update
4997         """
4998         from .dataarray import DataArray
4999 
5000         other = other.to_dataset() if isinstance(other, DataArray) else other
5001         merge_result = dataset_merge_method(
5002             self,
5003             other,
5004             overwrite_vars=overwrite_vars,
5005             compat=compat,
5006             join=join,
5007             fill_value=fill_value,
5008             combine_attrs=combine_attrs,
5009         )
5010         return self._replace(**merge_result._asdict())
5011 
5012     def _assert_all_in_dataset(
5013         self, names: Iterable[Hashable], virtual_okay: bool = False
5014     ) -> None:
5015         bad_names = set(names) - set(self._variables)
5016         if virtual_okay:
5017             bad_names -= self.virtual_variables
5018         if bad_names:
5019             raise ValueError(
5020                 "One or more of the specified variables "
5021                 "cannot be found in this dataset"
5022             )
5023 
5024     def drop_vars(
5025         self: T_Dataset,
5026         names: Hashable | Iterable[Hashable],
5027         *,
5028         errors: ErrorOptions = "raise",
5029     ) -> T_Dataset:
5030         """Drop variables from this dataset.
5031 
5032         Parameters
5033         ----------
5034         names : hashable or iterable of hashable
5035             Name(s) of variables to drop.
5036         errors : {"raise", "ignore"}, default: "raise"
5037             If 'raise', raises a ValueError error if any of the variable
5038             passed are not in the dataset. If 'ignore', any given names that are in the
5039             dataset are dropped and no error is raised.
5040 
5041         Returns
5042         -------
5043         dropped : Dataset
5044 
5045         """
5046         # the Iterable check is required for mypy
5047         if is_scalar(names) or not isinstance(names, Iterable):
5048             names = {names}
5049         else:
5050             names = set(names)
5051         if errors == "raise":
5052             self._assert_all_in_dataset(names)
5053 
5054         # GH6505
5055         other_names = set()
5056         for var in names:
5057             maybe_midx = self._indexes.get(var, None)
5058             if isinstance(maybe_midx, PandasMultiIndex):
5059                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5060                 idx_other_names = idx_coord_names - set(names)
5061                 other_names.update(idx_other_names)
5062         if other_names:
5063             names |= set(other_names)
5064             warnings.warn(
5065                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5066                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5067                 DeprecationWarning,
5068                 stacklevel=2,
5069             )
5070 
5071         assert_no_index_corrupted(self.xindexes, names)
5072 
5073         variables = {k: v for k, v in self._variables.items() if k not in names}
5074         coord_names = {k for k in self._coord_names if k in variables}
5075         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5076         return self._replace_with_new_dims(
5077             variables, coord_names=coord_names, indexes=indexes
5078         )
5079 
5080     def drop_indexes(
5081         self: T_Dataset,
5082         coord_names: Hashable | Iterable[Hashable],
5083         *,
5084         errors: ErrorOptions = "raise",
5085     ) -> T_Dataset:
5086         """Drop the indexes assigned to the given coordinates.
5087 
5088         Parameters
5089         ----------
5090         coord_names : hashable or iterable of hashable
5091             Name(s) of the coordinate(s) for which to drop the index.
5092         errors : {"raise", "ignore"}, default: "raise"
5093             If 'raise', raises a ValueError error if any of the coordinates
5094             passed have no index or are not in the dataset.
5095             If 'ignore', no error is raised.
5096 
5097         Returns
5098         -------
5099         dropped : Dataset
5100             A new dataset with dropped indexes.
5101 
5102         """
5103         # the Iterable check is required for mypy
5104         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5105             coord_names = {coord_names}
5106         else:
5107             coord_names = set(coord_names)
5108 
5109         if errors == "raise":
5110             invalid_coords = coord_names - self._coord_names
5111             if invalid_coords:
5112                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5113 
5114             unindexed_coords = set(coord_names) - set(self._indexes)
5115             if unindexed_coords:
5116                 raise ValueError(
5117                     f"those coordinates do not have an index: {unindexed_coords}"
5118                 )
5119 
5120         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5121 
5122         variables = {}
5123         for name, var in self._variables.items():
5124             if name in coord_names:
5125                 variables[name] = var.to_base_variable()
5126             else:
5127                 variables[name] = var
5128 
5129         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5130 
5131         return self._replace(variables=variables, indexes=indexes)
5132 
5133     def drop(
5134         self: T_Dataset,
5135         labels=None,
5136         dim=None,
5137         *,
5138         errors: ErrorOptions = "raise",
5139         **labels_kwargs,
5140     ) -> T_Dataset:
5141         """Backward compatible method based on `drop_vars` and `drop_sel`
5142 
5143         Using either `drop_vars` or `drop_sel` is encouraged
5144 
5145         See Also
5146         --------
5147         Dataset.drop_vars
5148         Dataset.drop_sel
5149         """
5150         if errors not in ["raise", "ignore"]:
5151             raise ValueError('errors must be either "raise" or "ignore"')
5152 
5153         if is_dict_like(labels) and not isinstance(labels, dict):
5154             warnings.warn(
5155                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5156                 FutureWarning,
5157                 stacklevel=2,
5158             )
5159             return self.drop_vars(labels, errors=errors)
5160 
5161         if labels_kwargs or isinstance(labels, dict):
5162             if dim is not None:
5163                 raise ValueError("cannot specify dim and dict-like arguments.")
5164             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5165 
5166         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5167             warnings.warn(
5168                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5169                 PendingDeprecationWarning,
5170                 stacklevel=2,
5171             )
5172             return self.drop_vars(labels, errors=errors)
5173         if dim is not None:
5174             warnings.warn(
5175                 "dropping labels using list-like labels is deprecated; using "
5176                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5177                 DeprecationWarning,
5178                 stacklevel=2,
5179             )
5180             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5181 
5182         warnings.warn(
5183             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5184             PendingDeprecationWarning,
5185             stacklevel=2,
5186         )
5187         return self.drop_sel(labels, errors=errors)
5188 
5189     def drop_sel(
5190         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5191     ) -> T_Dataset:
5192         """Drop index labels from this dataset.
5193 
5194         Parameters
5195         ----------
5196         labels : mapping of hashable to Any
5197             Index labels to drop
5198         errors : {"raise", "ignore"}, default: "raise"
5199             If 'raise', raises a ValueError error if
5200             any of the index labels passed are not
5201             in the dataset. If 'ignore', any given labels that are in the
5202             dataset are dropped and no error is raised.
5203         **labels_kwargs : {dim: label, ...}, optional
5204             The keyword arguments form of ``dim`` and ``labels``
5205 
5206         Returns
5207         -------
5208         dropped : Dataset
5209 
5210         Examples
5211         --------
5212         >>> data = np.arange(6).reshape(2, 3)
5213         >>> labels = ["a", "b", "c"]
5214         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5215         >>> ds
5216         <xarray.Dataset>
5217         Dimensions:  (x: 2, y: 3)
5218         Coordinates:
5219           * y        (y) <U1 'a' 'b' 'c'
5220         Dimensions without coordinates: x
5221         Data variables:
5222             A        (x, y) int64 0 1 2 3 4 5
5223         >>> ds.drop_sel(y=["a", "c"])
5224         <xarray.Dataset>
5225         Dimensions:  (x: 2, y: 1)
5226         Coordinates:
5227           * y        (y) <U1 'b'
5228         Dimensions without coordinates: x
5229         Data variables:
5230             A        (x, y) int64 1 4
5231         >>> ds.drop_sel(y="b")
5232         <xarray.Dataset>
5233         Dimensions:  (x: 2, y: 2)
5234         Coordinates:
5235           * y        (y) <U1 'a' 'c'
5236         Dimensions without coordinates: x
5237         Data variables:
5238             A        (x, y) int64 0 2 3 5
5239         """
5240         if errors not in ["raise", "ignore"]:
5241             raise ValueError('errors must be either "raise" or "ignore"')
5242 
5243         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5244 
5245         ds = self
5246         for dim, labels_for_dim in labels.items():
5247             # Don't cast to set, as it would harm performance when labels
5248             # is a large numpy array
5249             if utils.is_scalar(labels_for_dim):
5250                 labels_for_dim = [labels_for_dim]
5251             labels_for_dim = np.asarray(labels_for_dim)
5252             try:
5253                 index = self.get_index(dim)
5254             except KeyError:
5255                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5256             new_index = index.drop(labels_for_dim, errors=errors)
5257             ds = ds.loc[{dim: new_index}]
5258         return ds
5259 
5260     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5261         """Drop index positions from this Dataset.
5262 
5263         Parameters
5264         ----------
5265         indexers : mapping of hashable to Any
5266             Index locations to drop
5267         **indexers_kwargs : {dim: position, ...}, optional
5268             The keyword arguments form of ``dim`` and ``positions``
5269 
5270         Returns
5271         -------
5272         dropped : Dataset
5273 
5274         Raises
5275         ------
5276         IndexError
5277 
5278         Examples
5279         --------
5280         >>> data = np.arange(6).reshape(2, 3)
5281         >>> labels = ["a", "b", "c"]
5282         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5283         >>> ds
5284         <xarray.Dataset>
5285         Dimensions:  (x: 2, y: 3)
5286         Coordinates:
5287           * y        (y) <U1 'a' 'b' 'c'
5288         Dimensions without coordinates: x
5289         Data variables:
5290             A        (x, y) int64 0 1 2 3 4 5
5291         >>> ds.drop_isel(y=[0, 2])
5292         <xarray.Dataset>
5293         Dimensions:  (x: 2, y: 1)
5294         Coordinates:
5295           * y        (y) <U1 'b'
5296         Dimensions without coordinates: x
5297         Data variables:
5298             A        (x, y) int64 1 4
5299         >>> ds.drop_isel(y=1)
5300         <xarray.Dataset>
5301         Dimensions:  (x: 2, y: 2)
5302         Coordinates:
5303           * y        (y) <U1 'a' 'c'
5304         Dimensions without coordinates: x
5305         Data variables:
5306             A        (x, y) int64 0 2 3 5
5307         """
5308 
5309         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5310 
5311         ds = self
5312         dimension_index = {}
5313         for dim, pos_for_dim in indexers.items():
5314             # Don't cast to set, as it would harm performance when labels
5315             # is a large numpy array
5316             if utils.is_scalar(pos_for_dim):
5317                 pos_for_dim = [pos_for_dim]
5318             pos_for_dim = np.asarray(pos_for_dim)
5319             index = self.get_index(dim)
5320             new_index = index.delete(pos_for_dim)
5321             dimension_index[dim] = new_index
5322         ds = ds.loc[dimension_index]
5323         return ds
5324 
5325     def drop_dims(
5326         self: T_Dataset,
5327         drop_dims: Hashable | Iterable[Hashable],
5328         *,
5329         errors: ErrorOptions = "raise",
5330     ) -> T_Dataset:
5331         """Drop dimensions and associated variables from this dataset.
5332 
5333         Parameters
5334         ----------
5335         drop_dims : hashable or iterable of hashable
5336             Dimension or dimensions to drop.
5337         errors : {"raise", "ignore"}, default: "raise"
5338             If 'raise', raises a ValueError error if any of the
5339             dimensions passed are not in the dataset. If 'ignore', any given
5340             dimensions that are in the dataset are dropped and no error is raised.
5341 
5342         Returns
5343         -------
5344         obj : Dataset
5345             The dataset without the given dimensions (or any variables
5346             containing those dimensions).
5347         """
5348         if errors not in ["raise", "ignore"]:
5349             raise ValueError('errors must be either "raise" or "ignore"')
5350 
5351         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5352             drop_dims = {drop_dims}
5353         else:
5354             drop_dims = set(drop_dims)
5355 
5356         if errors == "raise":
5357             missing_dims = drop_dims - set(self.dims)
5358             if missing_dims:
5359                 raise ValueError(
5360                     f"Dataset does not contain the dimensions: {missing_dims}"
5361                 )
5362 
5363         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5364         return self.drop_vars(drop_vars)
5365 
5366     def transpose(
5367         self: T_Dataset,
5368         *dims: Hashable,
5369         missing_dims: ErrorOptionsWithWarn = "raise",
5370     ) -> T_Dataset:
5371         """Return a new Dataset object with all array dimensions transposed.
5372 
5373         Although the order of dimensions on each array will change, the dataset
5374         dimensions themselves will remain in fixed (sorted) order.
5375 
5376         Parameters
5377         ----------
5378         *dims : hashable, optional
5379             By default, reverse the dimensions on each array. Otherwise,
5380             reorder the dimensions to this order.
5381         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5382             What to do if dimensions that should be selected from are not present in the
5383             Dataset:
5384             - "raise": raise an exception
5385             - "warn": raise a warning, and ignore the missing dimensions
5386             - "ignore": ignore the missing dimensions
5387 
5388         Returns
5389         -------
5390         transposed : Dataset
5391             Each array in the dataset (including) coordinates will be
5392             transposed to the given order.
5393 
5394         Notes
5395         -----
5396         This operation returns a view of each array's data. It is
5397         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5398         -- the data will be fully loaded into memory.
5399 
5400         See Also
5401         --------
5402         numpy.transpose
5403         DataArray.transpose
5404         """
5405         # Use infix_dims to check once for missing dimensions
5406         if len(dims) != 0:
5407             _ = list(infix_dims(dims, self.dims, missing_dims))
5408 
5409         ds = self.copy()
5410         for name, var in self._variables.items():
5411             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5412             ds._variables[name] = var.transpose(*var_dims)
5413         return ds
5414 
5415     def dropna(
5416         self: T_Dataset,
5417         dim: Hashable,
5418         how: Literal["any", "all"] = "any",
5419         thresh: int | None = None,
5420         subset: Iterable[Hashable] | None = None,
5421     ) -> T_Dataset:
5422         """Returns a new dataset with dropped labels for missing values along
5423         the provided dimension.
5424 
5425         Parameters
5426         ----------
5427         dim : hashable
5428             Dimension along which to drop missing values. Dropping along
5429             multiple dimensions simultaneously is not yet supported.
5430         how : {"any", "all"}, default: "any"
5431             - any : if any NA values are present, drop that label
5432             - all : if all values are NA, drop that label
5433 
5434         thresh : int or None, optional
5435             If supplied, require this many non-NA values.
5436         subset : iterable of hashable or None, optional
5437             Which variables to check for missing values. By default, all
5438             variables in the dataset are checked.
5439 
5440         Returns
5441         -------
5442         Dataset
5443         """
5444         # TODO: consider supporting multiple dimensions? Or not, given that
5445         # there are some ugly edge cases, e.g., pandas's dropna differs
5446         # depending on the order of the supplied axes.
5447 
5448         if dim not in self.dims:
5449             raise ValueError(f"{dim} must be a single dataset dimension")
5450 
5451         if subset is None:
5452             subset = iter(self.data_vars)
5453 
5454         count = np.zeros(self.dims[dim], dtype=np.int64)
5455         size = np.int_(0)  # for type checking
5456 
5457         for k in subset:
5458             array = self._variables[k]
5459             if dim in array.dims:
5460                 dims = [d for d in array.dims if d != dim]
5461                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5462                 size += math.prod([self.dims[d] for d in dims])
5463 
5464         if thresh is not None:
5465             mask = count >= thresh
5466         elif how == "any":
5467             mask = count == size
5468         elif how == "all":
5469             mask = count > 0
5470         elif how is not None:
5471             raise ValueError(f"invalid how option: {how}")
5472         else:
5473             raise TypeError("must specify how or thresh")
5474 
5475         return self.isel({dim: mask})
5476 
5477     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5478         """Fill missing values in this object.
5479 
5480         This operation follows the normal broadcasting and alignment rules that
5481         xarray uses for binary arithmetic, except the result is aligned to this
5482         object (``join='left'``) instead of aligned to the intersection of
5483         index coordinates (``join='inner'``).
5484 
5485         Parameters
5486         ----------
5487         value : scalar, ndarray, DataArray, dict or Dataset
5488             Used to fill all matching missing values in this dataset's data
5489             variables. Scalars, ndarrays or DataArrays arguments are used to
5490             fill all data with aligned coordinates (for DataArrays).
5491             Dictionaries or datasets match data variables and then align
5492             coordinates if necessary.
5493 
5494         Returns
5495         -------
5496         Dataset
5497 
5498         Examples
5499         --------
5500         >>> ds = xr.Dataset(
5501         ...     {
5502         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5503         ...         "B": ("x", [3, 4, np.nan, 1]),
5504         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5505         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5506         ...     },
5507         ...     coords={"x": [0, 1, 2, 3]},
5508         ... )
5509         >>> ds
5510         <xarray.Dataset>
5511         Dimensions:  (x: 4)
5512         Coordinates:
5513           * x        (x) int64 0 1 2 3
5514         Data variables:
5515             A        (x) float64 nan 2.0 nan 0.0
5516             B        (x) float64 3.0 4.0 nan 1.0
5517             C        (x) float64 nan nan nan 5.0
5518             D        (x) float64 nan 3.0 nan 4.0
5519 
5520         Replace all `NaN` values with 0s.
5521 
5522         >>> ds.fillna(0)
5523         <xarray.Dataset>
5524         Dimensions:  (x: 4)
5525         Coordinates:
5526           * x        (x) int64 0 1 2 3
5527         Data variables:
5528             A        (x) float64 0.0 2.0 0.0 0.0
5529             B        (x) float64 3.0 4.0 0.0 1.0
5530             C        (x) float64 0.0 0.0 0.0 5.0
5531             D        (x) float64 0.0 3.0 0.0 4.0
5532 
5533         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
5534 
5535         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5536         >>> ds.fillna(value=values)
5537         <xarray.Dataset>
5538         Dimensions:  (x: 4)
5539         Coordinates:
5540           * x        (x) int64 0 1 2 3
5541         Data variables:
5542             A        (x) float64 0.0 2.0 0.0 0.0
5543             B        (x) float64 3.0 4.0 1.0 1.0
5544             C        (x) float64 2.0 2.0 2.0 5.0
5545             D        (x) float64 3.0 3.0 3.0 4.0
5546         """
5547         if utils.is_dict_like(value):
5548             value_keys = getattr(value, "data_vars", value).keys()
5549             if not set(value_keys) <= set(self.data_vars.keys()):
5550                 raise ValueError(
5551                     "all variables in the argument to `fillna` "
5552                     "must be contained in the original dataset"
5553                 )
5554         out = ops.fillna(self, value)
5555         return out
5556 
5557     def interpolate_na(
5558         self: T_Dataset,
5559         dim: Hashable | None = None,
5560         method: InterpOptions = "linear",
5561         limit: int = None,
5562         use_coordinate: bool | Hashable = True,
5563         max_gap: (
5564             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5565         ) = None,
5566         **kwargs: Any,
5567     ) -> T_Dataset:
5568         """Fill in NaNs by interpolating according to different methods.
5569 
5570         Parameters
5571         ----------
5572         dim : Hashable or None, optional
5573             Specifies the dimension along which to interpolate.
5574         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5575             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5576             String indicating which method to use for interpolation:
5577 
5578             - 'linear': linear interpolation. Additional keyword
5579               arguments are passed to :py:func:`numpy.interp`
5580             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5581               are passed to :py:func:`scipy.interpolate.interp1d`. If
5582               ``method='polynomial'``, the ``order`` keyword argument must also be
5583               provided.
5584             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5585               respective :py:class:`scipy.interpolate` classes.
5586 
5587         use_coordinate : bool or Hashable, default: True
5588             Specifies which index to use as the x values in the interpolation
5589             formulated as `y = f(x)`. If False, values are treated as if
5590             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5591             used. If ``use_coordinate`` is a string, it specifies the name of a
5592             coordinate variariable to use as the index.
5593         limit : int, default: None
5594             Maximum number of consecutive NaNs to fill. Must be greater than 0
5595             or None for no limit. This filling is done regardless of the size of
5596             the gap in the data. To only interpolate over gaps less than a given length,
5597             see ``max_gap``.
5598         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5599             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5600             Use None for no limit. When interpolating along a datetime64 dimension
5601             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5602 
5603             - a string that is valid input for pandas.to_timedelta
5604             - a :py:class:`numpy.timedelta64` object
5605             - a :py:class:`pandas.Timedelta` object
5606             - a :py:class:`datetime.timedelta` object
5607 
5608             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5609             dimensions has not been implemented yet. Gap length is defined as the difference
5610             between coordinate values at the first data point after a gap and the last value
5611             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5612             between coordinate values at the first (last) valid data point and the first (last) NaN.
5613             For example, consider::
5614 
5615                 <xarray.DataArray (x: 9)>
5616                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5617                 Coordinates:
5618                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5619 
5620             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5621         **kwargs : dict, optional
5622             parameters passed verbatim to the underlying interpolation function
5623 
5624         Returns
5625         -------
5626         interpolated: Dataset
5627             Filled in Dataset.
5628 
5629         See Also
5630         --------
5631         numpy.interp
5632         scipy.interpolate
5633 
5634         Examples
5635         --------
5636         >>> ds = xr.Dataset(
5637         ...     {
5638         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5639         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5640         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5641         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5642         ...     },
5643         ...     coords={"x": [0, 1, 2, 3, 4]},
5644         ... )
5645         >>> ds
5646         <xarray.Dataset>
5647         Dimensions:  (x: 5)
5648         Coordinates:
5649           * x        (x) int64 0 1 2 3 4
5650         Data variables:
5651             A        (x) float64 nan 2.0 3.0 nan 0.0
5652             B        (x) float64 3.0 4.0 nan 1.0 7.0
5653             C        (x) float64 nan nan nan 5.0 0.0
5654             D        (x) float64 nan 3.0 nan -1.0 4.0
5655 
5656         >>> ds.interpolate_na(dim="x", method="linear")
5657         <xarray.Dataset>
5658         Dimensions:  (x: 5)
5659         Coordinates:
5660           * x        (x) int64 0 1 2 3 4
5661         Data variables:
5662             A        (x) float64 nan 2.0 3.0 1.5 0.0
5663             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5664             C        (x) float64 nan nan nan 5.0 0.0
5665             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5666 
5667         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5668         <xarray.Dataset>
5669         Dimensions:  (x: 5)
5670         Coordinates:
5671           * x        (x) int64 0 1 2 3 4
5672         Data variables:
5673             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5674             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5675             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5676             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5677         """
5678         from .missing import _apply_over_vars_with_dim, interp_na
5679 
5680         new = _apply_over_vars_with_dim(
5681             interp_na,
5682             self,
5683             dim=dim,
5684             method=method,
5685             limit=limit,
5686             use_coordinate=use_coordinate,
5687             max_gap=max_gap,
5688             **kwargs,
5689         )
5690         return new
5691 
5692     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5693         """Fill NaN values by propagating values forward
5694 
5695         *Requires bottleneck.*
5696 
5697         Parameters
5698         ----------
5699         dim : Hashable
5700             Specifies the dimension along which to propagate values when
5701             filling.
5702         limit : int or None, optional
5703             The maximum number of consecutive NaN values to forward fill. In
5704             other words, if there is a gap with more than this number of
5705             consecutive NaNs, it will only be partially filled. Must be greater
5706             than 0 or None for no limit. Must be None or greater than or equal
5707             to axis length if filling along chunked axes (dimensions).
5708 
5709         Returns
5710         -------
5711         Dataset
5712         """
5713         from .missing import _apply_over_vars_with_dim, ffill
5714 
5715         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5716         return new
5717 
5718     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5719         """Fill NaN values by propagating values backward
5720 
5721         *Requires bottleneck.*
5722 
5723         Parameters
5724         ----------
5725         dim : Hashable
5726             Specifies the dimension along which to propagate values when
5727             filling.
5728         limit : int or None, optional
5729             The maximum number of consecutive NaN values to backward fill. In
5730             other words, if there is a gap with more than this number of
5731             consecutive NaNs, it will only be partially filled. Must be greater
5732             than 0 or None for no limit. Must be None or greater than or equal
5733             to axis length if filling along chunked axes (dimensions).
5734 
5735         Returns
5736         -------
5737         Dataset
5738         """
5739         from .missing import _apply_over_vars_with_dim, bfill
5740 
5741         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5742         return new
5743 
5744     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5745         """Combine two Datasets, default to data_vars of self.
5746 
5747         The new coordinates follow the normal broadcasting and alignment rules
5748         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5749         filled with np.nan.
5750 
5751         Parameters
5752         ----------
5753         other : Dataset
5754             Used to fill all matching missing values in this array.
5755 
5756         Returns
5757         -------
5758         Dataset
5759         """
5760         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5761         return out
5762 
5763     def reduce(
5764         self: T_Dataset,
5765         func: Callable,
5766         dim: Hashable | Iterable[Hashable] = None,
5767         *,
5768         keep_attrs: bool | None = None,
5769         keepdims: bool = False,
5770         numeric_only: bool = False,
5771         **kwargs: Any,
5772     ) -> T_Dataset:
5773         """Reduce this dataset by applying `func` along some dimension(s).
5774 
5775         Parameters
5776         ----------
5777         func : callable
5778             Function which can be called in the form
5779             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5780             np.ndarray over an integer valued axis.
5781         dim : str or sequence of str, optional
5782             Dimension(s) over which to apply `func`.  By default `func` is
5783             applied over all dimensions.
5784         keep_attrs : bool or None, optional
5785             If True, the dataset's attributes (`attrs`) will be copied from
5786             the original object to the new one.  If False (default), the new
5787             object will be returned without attributes.
5788         keepdims : bool, default: False
5789             If True, the dimensions which are reduced are left in the result
5790             as dimensions of size one. Coordinates that use these dimensions
5791             are removed.
5792         numeric_only : bool, default: False
5793             If True, only apply ``func`` to variables with a numeric dtype.
5794         **kwargs : Any
5795             Additional keyword arguments passed on to ``func``.
5796 
5797         Returns
5798         -------
5799         reduced : Dataset
5800             Dataset with this object's DataArrays replaced with new DataArrays
5801             of summarized data and the indicated dimension(s) removed.
5802         """
5803         if kwargs.get("axis", None) is not None:
5804             raise ValueError(
5805                 "passing 'axis' to Dataset reduce methods is ambiguous."
5806                 " Please use 'dim' instead."
5807             )
5808 
5809         if dim is None or dim is ...:
5810             dims = set(self.dims)
5811         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5812             dims = {dim}
5813         else:
5814             dims = set(dim)
5815 
5816         missing_dimensions = [d for d in dims if d not in self.dims]
5817         if missing_dimensions:
5818             raise ValueError(
5819                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5820             )
5821 
5822         if keep_attrs is None:
5823             keep_attrs = _get_keep_attrs(default=False)
5824 
5825         variables: dict[Hashable, Variable] = {}
5826         for name, var in self._variables.items():
5827             reduce_dims = [d for d in var.dims if d in dims]
5828             if name in self.coords:
5829                 if not reduce_dims:
5830                     variables[name] = var
5831             else:
5832                 if (
5833                     # Some reduction functions (e.g. std, var) need to run on variables
5834                     # that don't have the reduce dims: PR5393
5835                     not reduce_dims
5836                     or not numeric_only
5837                     or np.issubdtype(var.dtype, np.number)
5838                     or (var.dtype == np.bool_)
5839                 ):
5840                     reduce_maybe_single: Hashable | None | list[Hashable]
5841                     if len(reduce_dims) == 1:
5842                         # unpack dimensions for the benefit of functions
5843                         # like np.argmin which can't handle tuple arguments
5844                         (reduce_maybe_single,) = reduce_dims
5845                     elif len(reduce_dims) == var.ndim:
5846                         # prefer to aggregate over axis=None rather than
5847                         # axis=(0, 1) if they will be equivalent, because
5848                         # the former is often more efficient
5849                         reduce_maybe_single = None
5850                     else:
5851                         reduce_maybe_single = reduce_dims
5852                     variables[name] = var.reduce(
5853                         func,
5854                         dim=reduce_maybe_single,
5855                         keep_attrs=keep_attrs,
5856                         keepdims=keepdims,
5857                         **kwargs,
5858                     )
5859 
5860         coord_names = {k for k in self.coords if k in variables}
5861         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5862         attrs = self.attrs if keep_attrs else None
5863         return self._replace_with_new_dims(
5864             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5865         )
5866 
5867     def map(
5868         self: T_Dataset,
5869         func: Callable,
5870         keep_attrs: bool | None = None,
5871         args: Iterable[Any] = (),
5872         **kwargs: Any,
5873     ) -> T_Dataset:
5874         """Apply a function to each data variable in this dataset
5875 
5876         Parameters
5877         ----------
5878         func : callable
5879             Function which can be called in the form `func(x, *args, **kwargs)`
5880             to transform each DataArray `x` in this dataset into another
5881             DataArray.
5882         keep_attrs : bool or None, optional
5883             If True, both the dataset's and variables' attributes (`attrs`) will be
5884             copied from the original objects to the new ones. If False, the new dataset
5885             and variables will be returned without copying the attributes.
5886         args : iterable, optional
5887             Positional arguments passed on to `func`.
5888         **kwargs : Any
5889             Keyword arguments passed on to `func`.
5890 
5891         Returns
5892         -------
5893         applied : Dataset
5894             Resulting dataset from applying ``func`` to each data variable.
5895 
5896         Examples
5897         --------
5898         >>> da = xr.DataArray(np.random.randn(2, 3))
5899         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5900         >>> ds
5901         <xarray.Dataset>
5902         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5903         Dimensions without coordinates: dim_0, dim_1, x
5904         Data variables:
5905             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5906             bar      (x) int64 -1 2
5907         >>> ds.map(np.fabs)
5908         <xarray.Dataset>
5909         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5910         Dimensions without coordinates: dim_0, dim_1, x
5911         Data variables:
5912             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5913             bar      (x) float64 1.0 2.0
5914         """
5915         if keep_attrs is None:
5916             keep_attrs = _get_keep_attrs(default=False)
5917         variables = {
5918             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5919             for k, v in self.data_vars.items()
5920         }
5921         if keep_attrs:
5922             for k, v in variables.items():
5923                 v._copy_attrs_from(self.data_vars[k])
5924         attrs = self.attrs if keep_attrs else None
5925         return type(self)(variables, attrs=attrs)
5926 
5927     def apply(
5928         self: T_Dataset,
5929         func: Callable,
5930         keep_attrs: bool | None = None,
5931         args: Iterable[Any] = (),
5932         **kwargs: Any,
5933     ) -> T_Dataset:
5934         """
5935         Backward compatible implementation of ``map``
5936 
5937         See Also
5938         --------
5939         Dataset.map
5940         """
5941         warnings.warn(
5942             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5943             PendingDeprecationWarning,
5944             stacklevel=2,
5945         )
5946         return self.map(func, keep_attrs, args, **kwargs)
5947 
5948     def assign(
5949         self: T_Dataset,
5950         variables: Mapping[Any, Any] | None = None,
5951         **variables_kwargs: Any,
5952     ) -> T_Dataset:
5953         """Assign new data variables to a Dataset, returning a new object
5954         with all the original variables in addition to the new ones.
5955 
5956         Parameters
5957         ----------
5958         variables : mapping of hashable to Any
5959             Mapping from variables names to the new values. If the new values
5960             are callable, they are computed on the Dataset and assigned to new
5961             data variables. If the values are not callable, (e.g. a DataArray,
5962             scalar, or array), they are simply assigned.
5963         **variables_kwargs
5964             The keyword arguments form of ``variables``.
5965             One of variables or variables_kwargs must be provided.
5966 
5967         Returns
5968         -------
5969         ds : Dataset
5970             A new Dataset with the new variables in addition to all the
5971             existing variables.
5972 
5973         Notes
5974         -----
5975         Since ``kwargs`` is a dictionary, the order of your arguments may not
5976         be preserved, and so the order of the new variables is not well
5977         defined. Assigning multiple variables within the same ``assign`` is
5978         possible, but you cannot reference other variables created within the
5979         same ``assign`` call.
5980 
5981         See Also
5982         --------
5983         pandas.DataFrame.assign
5984 
5985         Examples
5986         --------
5987         >>> x = xr.Dataset(
5988         ...     {
5989         ...         "temperature_c": (
5990         ...             ("lat", "lon"),
5991         ...             20 * np.random.rand(4).reshape(2, 2),
5992         ...         ),
5993         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5994         ...     },
5995         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5996         ... )
5997         >>> x
5998         <xarray.Dataset>
5999         Dimensions:        (lat: 2, lon: 2)
6000         Coordinates:
6001           * lat            (lat) int64 10 20
6002           * lon            (lon) int64 150 160
6003         Data variables:
6004             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6005             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6006 
6007         Where the value is a callable, evaluated on dataset:
6008 
6009         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6010         <xarray.Dataset>
6011         Dimensions:        (lat: 2, lon: 2)
6012         Coordinates:
6013           * lat            (lat) int64 10 20
6014           * lon            (lon) int64 150 160
6015         Data variables:
6016             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6017             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6018             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6019 
6020         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6021 
6022         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6023         <xarray.Dataset>
6024         Dimensions:        (lat: 2, lon: 2)
6025         Coordinates:
6026           * lat            (lat) int64 10 20
6027           * lon            (lon) int64 150 160
6028         Data variables:
6029             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6030             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6031             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6032 
6033         """
6034         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6035         data = self.copy()
6036         # do all calculations first...
6037         results: CoercibleMapping = data._calc_assign_results(variables)
6038         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6039         # ... and then assign
6040         data.update(results)
6041         return data
6042 
6043     def to_array(
6044         self, dim: Hashable = "variable", name: Hashable | None = None
6045     ) -> DataArray:
6046         """Convert this dataset into an xarray.DataArray
6047 
6048         The data variables of this dataset will be broadcast against each other
6049         and stacked along the first axis of the new array. All coordinates of
6050         this dataset will remain coordinates.
6051 
6052         Parameters
6053         ----------
6054         dim : Hashable, default: "variable"
6055             Name of the new dimension.
6056         name : Hashable or None, optional
6057             Name of the new data array.
6058 
6059         Returns
6060         -------
6061         array : xarray.DataArray
6062         """
6063         from .dataarray import DataArray
6064 
6065         data_vars = [self.variables[k] for k in self.data_vars]
6066         broadcast_vars = broadcast_variables(*data_vars)
6067         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6068 
6069         dims = (dim,) + broadcast_vars[0].dims
6070         variable = Variable(dims, data, self.attrs, fastpath=True)
6071 
6072         coords = {k: v.variable for k, v in self.coords.items()}
6073         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6074         new_dim_index = PandasIndex(list(self.data_vars), dim)
6075         indexes[dim] = new_dim_index
6076         coords.update(new_dim_index.create_variables())
6077 
6078         return DataArray._construct_direct(variable, coords, name, indexes)
6079 
6080     def _normalize_dim_order(
6081         self, dim_order: Sequence[Hashable] | None = None
6082     ) -> dict[Hashable, int]:
6083         """
6084         Check the validity of the provided dimensions if any and return the mapping
6085         between dimension name and their size.
6086 
6087         Parameters
6088         ----------
6089         dim_order: Sequence of Hashable or None, optional
6090             Dimension order to validate (default to the alphabetical order if None).
6091 
6092         Returns
6093         -------
6094         result : dict[Hashable, int]
6095             Validated dimensions mapping.
6096 
6097         """
6098         if dim_order is None:
6099             dim_order = list(self.dims)
6100         elif set(dim_order) != set(self.dims):
6101             raise ValueError(
6102                 "dim_order {} does not match the set of dimensions of this "
6103                 "Dataset: {}".format(dim_order, list(self.dims))
6104             )
6105 
6106         ordered_dims = {k: self.dims[k] for k in dim_order}
6107 
6108         return ordered_dims
6109 
6110     def to_pandas(self) -> pd.Series | pd.DataFrame:
6111         """Convert this dataset into a pandas object without changing the number of dimensions.
6112 
6113         The type of the returned object depends on the number of Dataset
6114         dimensions:
6115 
6116         * 0D -> `pandas.Series`
6117         * 1D -> `pandas.DataFrame`
6118 
6119         Only works for Datasets with 1 or fewer dimensions.
6120         """
6121         if len(self.dims) == 0:
6122             return pd.Series({k: v.item() for k, v in self.items()})
6123         if len(self.dims) == 1:
6124             return self.to_dataframe()
6125         raise ValueError(
6126             "cannot convert Datasets with %s dimensions into "
6127             "pandas objects without changing the number of dimensions. "
6128             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6129         )
6130 
6131     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6132         columns = [k for k in self.variables if k not in self.dims]
6133         data = [
6134             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6135             for k in columns
6136         ]
6137         index = self.coords.to_index([*ordered_dims])
6138         return pd.DataFrame(dict(zip(columns, data)), index=index)
6139 
6140     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6141         """Convert this dataset into a pandas.DataFrame.
6142 
6143         Non-index variables in this dataset form the columns of the
6144         DataFrame. The DataFrame is indexed by the Cartesian product of
6145         this dataset's indices.
6146 
6147         Parameters
6148         ----------
6149         dim_order: Sequence of Hashable or None, optional
6150             Hierarchical dimension order for the resulting dataframe. All
6151             arrays are transposed to this order and then written out as flat
6152             vectors in contiguous order, so the last dimension in this list
6153             will be contiguous in the resulting DataFrame. This has a major
6154             influence on which operations are efficient on the resulting
6155             dataframe.
6156 
6157             If provided, must include all dimensions of this dataset. By
6158             default, dimensions are sorted alphabetically.
6159 
6160         Returns
6161         -------
6162         result : DataFrame
6163             Dataset as a pandas DataFrame.
6164 
6165         """
6166 
6167         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6168 
6169         return self._to_dataframe(ordered_dims=ordered_dims)
6170 
6171     def _set_sparse_data_from_dataframe(
6172         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6173     ) -> None:
6174         from sparse import COO
6175 
6176         if isinstance(idx, pd.MultiIndex):
6177             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6178             is_sorted = idx.is_monotonic_increasing
6179             shape = tuple(lev.size for lev in idx.levels)
6180         else:
6181             coords = np.arange(idx.size).reshape(1, -1)
6182             is_sorted = True
6183             shape = (idx.size,)
6184 
6185         for name, values in arrays:
6186             # In virtually all real use cases, the sparse array will now have
6187             # missing values and needs a fill_value. For consistency, don't
6188             # special case the rare exceptions (e.g., dtype=int without a
6189             # MultiIndex).
6190             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6191             values = np.asarray(values, dtype=dtype)
6192 
6193             data = COO(
6194                 coords,
6195                 values,
6196                 shape,
6197                 has_duplicates=False,
6198                 sorted=is_sorted,
6199                 fill_value=fill_value,
6200             )
6201             self[name] = (dims, data)
6202 
6203     def _set_numpy_data_from_dataframe(
6204         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6205     ) -> None:
6206         if not isinstance(idx, pd.MultiIndex):
6207             for name, values in arrays:
6208                 self[name] = (dims, values)
6209             return
6210 
6211         # NB: similar, more general logic, now exists in
6212         # variable.unstack_once; we could consider combining them at some
6213         # point.
6214 
6215         shape = tuple(lev.size for lev in idx.levels)
6216         indexer = tuple(idx.codes)
6217 
6218         # We already verified that the MultiIndex has all unique values, so
6219         # there are missing values if and only if the size of output arrays is
6220         # larger that the index.
6221         missing_values = math.prod(shape) > idx.shape[0]
6222 
6223         for name, values in arrays:
6224             # NumPy indexing is much faster than using DataFrame.reindex() to
6225             # fill in missing values:
6226             # https://stackoverflow.com/a/35049899/809705
6227             if missing_values:
6228                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6229                 data = np.full(shape, fill_value, dtype)
6230             else:
6231                 # If there are no missing values, keep the existing dtype
6232                 # instead of promoting to support NA, e.g., keep integer
6233                 # columns as integers.
6234                 # TODO: consider removing this special case, which doesn't
6235                 # exist for sparse=True.
6236                 data = np.zeros(shape, values.dtype)
6237             data[indexer] = values
6238             self[name] = (dims, data)
6239 
6240     @classmethod
6241     def from_dataframe(
6242         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6243     ) -> T_Dataset:
6244         """Convert a pandas.DataFrame into an xarray.Dataset
6245 
6246         Each column will be converted into an independent variable in the
6247         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6248         into a tensor product of one-dimensional indices (filling in missing
6249         values with NaN). This method will produce a Dataset very similar to
6250         that on which the 'to_dataframe' method was called, except with
6251         possibly redundant dimensions (since all dataset variables will have
6252         the same dimensionality)
6253 
6254         Parameters
6255         ----------
6256         dataframe : DataFrame
6257             DataFrame from which to copy data and indices.
6258         sparse : bool, default: False
6259             If true, create a sparse arrays instead of dense numpy arrays. This
6260             can potentially save a large amount of memory if the DataFrame has
6261             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6262 
6263         Returns
6264         -------
6265         New Dataset.
6266 
6267         See Also
6268         --------
6269         xarray.DataArray.from_series
6270         pandas.DataFrame.to_xarray
6271         """
6272         # TODO: Add an option to remove dimensions along which the variables
6273         # are constant, to enable consistent serialization to/from a dataframe,
6274         # even if some variables have different dimensionality.
6275 
6276         if not dataframe.columns.is_unique:
6277             raise ValueError("cannot convert DataFrame with non-unique columns")
6278 
6279         idx = remove_unused_levels_categories(dataframe.index)
6280 
6281         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6282             raise ValueError(
6283                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6284             )
6285 
6286         # Cast to a NumPy array first, in case the Series is a pandas Extension
6287         # array (which doesn't have a valid NumPy dtype)
6288         # TODO: allow users to control how this casting happens, e.g., by
6289         # forwarding arguments to pandas.Series.to_numpy?
6290         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6291 
6292         indexes: dict[Hashable, Index] = {}
6293         index_vars: dict[Hashable, Variable] = {}
6294 
6295         if isinstance(idx, pd.MultiIndex):
6296             dims = tuple(
6297                 name if name is not None else "level_%i" % n
6298                 for n, name in enumerate(idx.names)
6299             )
6300             for dim, lev in zip(dims, idx.levels):
6301                 xr_idx = PandasIndex(lev, dim)
6302                 indexes[dim] = xr_idx
6303                 index_vars.update(xr_idx.create_variables())
6304         else:
6305             index_name = idx.name if idx.name is not None else "index"
6306             dims = (index_name,)
6307             xr_idx = PandasIndex(idx, index_name)
6308             indexes[index_name] = xr_idx
6309             index_vars.update(xr_idx.create_variables())
6310 
6311         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6312 
6313         if sparse:
6314             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6315         else:
6316             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6317         return obj
6318 
6319     def to_dask_dataframe(
6320         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6321     ) -> DaskDataFrame:
6322         """
6323         Convert this dataset into a dask.dataframe.DataFrame.
6324 
6325         The dimensions, coordinates and data variables in this dataset form
6326         the columns of the DataFrame.
6327 
6328         Parameters
6329         ----------
6330         dim_order : list, optional
6331             Hierarchical dimension order for the resulting dataframe. All
6332             arrays are transposed to this order and then written out as flat
6333             vectors in contiguous order, so the last dimension in this list
6334             will be contiguous in the resulting DataFrame. This has a major
6335             influence on which operations are efficient on the resulting dask
6336             dataframe.
6337 
6338             If provided, must include all dimensions of this dataset. By
6339             default, dimensions are sorted alphabetically.
6340         set_index : bool, default: False
6341             If set_index=True, the dask DataFrame is indexed by this dataset's
6342             coordinate. Since dask DataFrames do not support multi-indexes,
6343             set_index only works if the dataset only contains one dimension.
6344 
6345         Returns
6346         -------
6347         dask.dataframe.DataFrame
6348         """
6349 
6350         import dask.array as da
6351         import dask.dataframe as dd
6352 
6353         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6354 
6355         columns = list(ordered_dims)
6356         columns.extend(k for k in self.coords if k not in self.dims)
6357         columns.extend(self.data_vars)
6358 
6359         series_list = []
6360         for name in columns:
6361             try:
6362                 var = self.variables[name]
6363             except KeyError:
6364                 # dimension without a matching coordinate
6365                 size = self.dims[name]
6366                 data = da.arange(size, chunks=size, dtype=np.int64)
6367                 var = Variable((name,), data)
6368 
6369             # IndexVariable objects have a dummy .chunk() method
6370             if isinstance(var, IndexVariable):
6371                 var = var.to_base_variable()
6372 
6373             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6374             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6375             series_list.append(series)
6376 
6377         df = dd.concat(series_list, axis=1)
6378 
6379         if set_index:
6380             dim_order = [*ordered_dims]
6381 
6382             if len(dim_order) == 1:
6383                 (dim,) = dim_order
6384                 df = df.set_index(dim)
6385             else:
6386                 # triggers an error about multi-indexes, even if only one
6387                 # dimension is passed
6388                 df = df.set_index(dim_order)
6389 
6390         return df
6391 
6392     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6393         """
6394         Convert this dataset to a dictionary following xarray naming
6395         conventions.
6396 
6397         Converts all variables and attributes to native Python objects
6398         Useful for converting to json. To avoid datetime incompatibility
6399         use decode_times=False kwarg in xarrray.open_dataset.
6400 
6401         Parameters
6402         ----------
6403         data : bool, default: True
6404             Whether to include the actual data in the dictionary. When set to
6405             False, returns just the schema.
6406         encoding : bool, default: False
6407             Whether to include the Dataset's encoding in the dictionary.
6408 
6409         Returns
6410         -------
6411         d : dict
6412             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6413             "encoding".
6414 
6415         See Also
6416         --------
6417         Dataset.from_dict
6418         DataArray.to_dict
6419         """
6420         d: dict = {
6421             "coords": {},
6422             "attrs": decode_numpy_dict_values(self.attrs),
6423             "dims": dict(self.dims),
6424             "data_vars": {},
6425         }
6426         for k in self.coords:
6427             d["coords"].update(
6428                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6429             )
6430         for k in self.data_vars:
6431             d["data_vars"].update(
6432                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6433             )
6434         if encoding:
6435             d["encoding"] = dict(self.encoding)
6436         return d
6437 
6438     @classmethod
6439     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6440         """Convert a dictionary into an xarray.Dataset.
6441 
6442         Parameters
6443         ----------
6444         d : dict-like
6445             Mapping with a minimum structure of
6446                 ``{"var_0": {"dims": [..], "data": [..]}, \
6447                             ...}``
6448 
6449         Returns
6450         -------
6451         obj : Dataset
6452 
6453         See also
6454         --------
6455         Dataset.to_dict
6456         DataArray.from_dict
6457 
6458         Examples
6459         --------
6460         >>> d = {
6461         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6462         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6463         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6464         ... }
6465         >>> ds = xr.Dataset.from_dict(d)
6466         >>> ds
6467         <xarray.Dataset>
6468         Dimensions:  (t: 3)
6469         Coordinates:
6470           * t        (t) int64 0 1 2
6471         Data variables:
6472             a        (t) <U1 'a' 'b' 'c'
6473             b        (t) int64 10 20 30
6474 
6475         >>> d = {
6476         ...     "coords": {
6477         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6478         ...     },
6479         ...     "attrs": {"title": "air temperature"},
6480         ...     "dims": "t",
6481         ...     "data_vars": {
6482         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6483         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6484         ...     },
6485         ... }
6486         >>> ds = xr.Dataset.from_dict(d)
6487         >>> ds
6488         <xarray.Dataset>
6489         Dimensions:  (t: 3)
6490         Coordinates:
6491           * t        (t) int64 0 1 2
6492         Data variables:
6493             a        (t) int64 10 20 30
6494             b        (t) <U1 'a' 'b' 'c'
6495         Attributes:
6496             title:    air temperature
6497 
6498         """
6499 
6500         variables: Iterable[tuple[Hashable, Any]]
6501         if not {"coords", "data_vars"}.issubset(set(d)):
6502             variables = d.items()
6503         else:
6504             import itertools
6505 
6506             variables = itertools.chain(
6507                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6508             )
6509         try:
6510             variable_dict = {
6511                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6512             }
6513         except KeyError as e:
6514             raise ValueError(
6515                 "cannot convert dict without the key "
6516                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6517             )
6518         obj = cls(variable_dict)
6519 
6520         # what if coords aren't dims?
6521         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6522         obj = obj.set_coords(coords)
6523 
6524         obj.attrs.update(d.get("attrs", {}))
6525         obj.encoding.update(d.get("encoding", {}))
6526 
6527         return obj
6528 
6529     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6530         variables = {}
6531         keep_attrs = kwargs.pop("keep_attrs", None)
6532         if keep_attrs is None:
6533             keep_attrs = _get_keep_attrs(default=True)
6534         for k, v in self._variables.items():
6535             if k in self._coord_names:
6536                 variables[k] = v
6537             else:
6538                 variables[k] = f(v, *args, **kwargs)
6539                 if keep_attrs:
6540                     variables[k].attrs = v._attrs
6541         attrs = self._attrs if keep_attrs else None
6542         return self._replace_with_new_dims(variables, attrs=attrs)
6543 
6544     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6545         from .dataarray import DataArray
6546         from .groupby import GroupBy
6547 
6548         if isinstance(other, GroupBy):
6549             return NotImplemented
6550         align_type = OPTIONS["arithmetic_join"] if join is None else join
6551         if isinstance(other, (DataArray, Dataset)):
6552             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6553         g = f if not reflexive else lambda x, y: f(y, x)
6554         ds = self._calculate_binary_op(g, other, join=align_type)
6555         return ds
6556 
6557     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6558         from .dataarray import DataArray
6559         from .groupby import GroupBy
6560 
6561         if isinstance(other, GroupBy):
6562             raise TypeError(
6563                 "in-place operations between a Dataset and "
6564                 "a grouped object are not permitted"
6565             )
6566         # we don't actually modify arrays in-place with in-place Dataset
6567         # arithmetic -- this lets us automatically align things
6568         if isinstance(other, (DataArray, Dataset)):
6569             other = other.reindex_like(self, copy=False)
6570         g = ops.inplace_to_noninplace_op(f)
6571         ds = self._calculate_binary_op(g, other, inplace=True)
6572         self._replace_with_new_dims(
6573             ds._variables,
6574             ds._coord_names,
6575             attrs=ds._attrs,
6576             indexes=ds._indexes,
6577             inplace=True,
6578         )
6579         return self
6580 
6581     def _calculate_binary_op(
6582         self, f, other, join="inner", inplace: bool = False
6583     ) -> Dataset:
6584         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6585             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6586                 raise ValueError(
6587                     "datasets must have the same data variables "
6588                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6589                 )
6590 
6591             dest_vars = {}
6592 
6593             for k in lhs_data_vars:
6594                 if k in rhs_data_vars:
6595                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6596                 elif join in ["left", "outer"]:
6597                     dest_vars[k] = f(lhs_vars[k], np.nan)
6598             for k in rhs_data_vars:
6599                 if k not in dest_vars and join in ["right", "outer"]:
6600                     dest_vars[k] = f(rhs_vars[k], np.nan)
6601             return dest_vars
6602 
6603         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6604             # can't use our shortcut of doing the binary operation with
6605             # Variable objects, so apply over our data vars instead.
6606             new_data_vars = apply_over_both(
6607                 self.data_vars, other, self.data_vars, other
6608             )
6609             return type(self)(new_data_vars)
6610 
6611         other_coords: Coordinates | None = getattr(other, "coords", None)
6612         ds = self.coords.merge(other_coords)
6613 
6614         if isinstance(other, Dataset):
6615             new_vars = apply_over_both(
6616                 self.data_vars, other.data_vars, self.variables, other.variables
6617             )
6618         else:
6619             other_variable = getattr(other, "variable", other)
6620             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6621         ds._variables.update(new_vars)
6622         ds._dims = calculate_dimensions(ds._variables)
6623         return ds
6624 
6625     def _copy_attrs_from(self, other):
6626         self.attrs = other.attrs
6627         for v in other.variables:
6628             if v in self.variables:
6629                 self.variables[v].attrs = other.variables[v].attrs
6630 
6631     def diff(
6632         self: T_Dataset,
6633         dim: Hashable,
6634         n: int = 1,
6635         label: Literal["upper", "lower"] = "upper",
6636     ) -> T_Dataset:
6637         """Calculate the n-th order discrete difference along given axis.
6638 
6639         Parameters
6640         ----------
6641         dim : Hashable
6642             Dimension over which to calculate the finite difference.
6643         n : int, default: 1
6644             The number of times values are differenced.
6645         label : {"upper", "lower"}, default: "upper"
6646             The new coordinate in dimension ``dim`` will have the
6647             values of either the minuend's or subtrahend's coordinate
6648             for values 'upper' and 'lower', respectively.
6649 
6650         Returns
6651         -------
6652         difference : Dataset
6653             The n-th order finite difference of this object.
6654 
6655         Notes
6656         -----
6657         `n` matches numpy's behavior and is different from pandas' first argument named
6658         `periods`.
6659 
6660         Examples
6661         --------
6662         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6663         >>> ds.diff("x")
6664         <xarray.Dataset>
6665         Dimensions:  (x: 3)
6666         Dimensions without coordinates: x
6667         Data variables:
6668             foo      (x) int64 0 1 0
6669         >>> ds.diff("x", 2)
6670         <xarray.Dataset>
6671         Dimensions:  (x: 2)
6672         Dimensions without coordinates: x
6673         Data variables:
6674             foo      (x) int64 1 -1
6675 
6676         See Also
6677         --------
6678         Dataset.differentiate
6679         """
6680         if n == 0:
6681             return self
6682         if n < 0:
6683             raise ValueError(f"order `n` must be non-negative but got {n}")
6684 
6685         # prepare slices
6686         slice_start = {dim: slice(None, -1)}
6687         slice_end = {dim: slice(1, None)}
6688 
6689         # prepare new coordinate
6690         if label == "upper":
6691             slice_new = slice_end
6692         elif label == "lower":
6693             slice_new = slice_start
6694         else:
6695             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6696 
6697         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6698         variables = {}
6699 
6700         for name, var in self.variables.items():
6701             if name in index_vars:
6702                 variables[name] = index_vars[name]
6703             elif dim in var.dims:
6704                 if name in self.data_vars:
6705                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6706                 else:
6707                     variables[name] = var.isel(slice_new)
6708             else:
6709                 variables[name] = var
6710 
6711         difference = self._replace_with_new_dims(variables, indexes=indexes)
6712 
6713         if n > 1:
6714             return difference.diff(dim, n - 1)
6715         else:
6716             return difference
6717 
6718     def shift(
6719         self: T_Dataset,
6720         shifts: Mapping[Any, int] | None = None,
6721         fill_value: Any = xrdtypes.NA,
6722         **shifts_kwargs: int,
6723     ) -> T_Dataset:
6724 
6725         """Shift this dataset by an offset along one or more dimensions.
6726 
6727         Only data variables are moved; coordinates stay in place. This is
6728         consistent with the behavior of ``shift`` in pandas.
6729 
6730         Values shifted from beyond array bounds will appear at one end of
6731         each dimension, which are filled according to `fill_value`. For periodic
6732         offsets instead see `roll`.
6733 
6734         Parameters
6735         ----------
6736         shifts : mapping of hashable to int
6737             Integer offset to shift along each of the given dimensions.
6738             Positive offsets shift to the right; negative offsets shift to the
6739             left.
6740         fill_value : scalar or dict-like, optional
6741             Value to use for newly missing values. If a dict-like, maps
6742             variable names (including coordinates) to fill values.
6743         **shifts_kwargs
6744             The keyword arguments form of ``shifts``.
6745             One of shifts or shifts_kwargs must be provided.
6746 
6747         Returns
6748         -------
6749         shifted : Dataset
6750             Dataset with the same coordinates and attributes but shifted data
6751             variables.
6752 
6753         See Also
6754         --------
6755         roll
6756 
6757         Examples
6758         --------
6759         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6760         >>> ds.shift(x=2)
6761         <xarray.Dataset>
6762         Dimensions:  (x: 5)
6763         Dimensions without coordinates: x
6764         Data variables:
6765             foo      (x) object nan nan 'a' 'b' 'c'
6766         """
6767         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6768         invalid = [k for k in shifts if k not in self.dims]
6769         if invalid:
6770             raise ValueError(f"dimensions {invalid!r} do not exist")
6771 
6772         variables = {}
6773         for name, var in self.variables.items():
6774             if name in self.data_vars:
6775                 fill_value_ = (
6776                     fill_value.get(name, xrdtypes.NA)
6777                     if isinstance(fill_value, dict)
6778                     else fill_value
6779                 )
6780 
6781                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6782                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6783             else:
6784                 variables[name] = var
6785 
6786         return self._replace(variables)
6787 
6788     def roll(
6789         self: T_Dataset,
6790         shifts: Mapping[Any, int] | None = None,
6791         roll_coords: bool = False,
6792         **shifts_kwargs: int,
6793     ) -> T_Dataset:
6794         """Roll this dataset by an offset along one or more dimensions.
6795 
6796         Unlike shift, roll treats the given dimensions as periodic, so will not
6797         create any missing values to be filled.
6798 
6799         Also unlike shift, roll may rotate all variables, including coordinates
6800         if specified. The direction of rotation is consistent with
6801         :py:func:`numpy.roll`.
6802 
6803         Parameters
6804         ----------
6805         shifts : mapping of hashable to int, optional
6806             A dict with keys matching dimensions and values given
6807             by integers to rotate each of the given dimensions. Positive
6808             offsets roll to the right; negative offsets roll to the left.
6809         roll_coords : bool, default: False
6810             Indicates whether to roll the coordinates by the offset too.
6811         **shifts_kwargs : {dim: offset, ...}, optional
6812             The keyword arguments form of ``shifts``.
6813             One of shifts or shifts_kwargs must be provided.
6814 
6815         Returns
6816         -------
6817         rolled : Dataset
6818             Dataset with the same attributes but rolled data and coordinates.
6819 
6820         See Also
6821         --------
6822         shift
6823 
6824         Examples
6825         --------
6826         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6827         >>> ds.roll(x=2)
6828         <xarray.Dataset>
6829         Dimensions:  (x: 5)
6830         Coordinates:
6831           * x        (x) int64 0 1 2 3 4
6832         Data variables:
6833             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6834 
6835         >>> ds.roll(x=2, roll_coords=True)
6836         <xarray.Dataset>
6837         Dimensions:  (x: 5)
6838         Coordinates:
6839           * x        (x) int64 3 4 0 1 2
6840         Data variables:
6841             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6842 
6843         """
6844         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6845         invalid = [k for k in shifts if k not in self.dims]
6846         if invalid:
6847             raise ValueError(f"dimensions {invalid!r} do not exist")
6848 
6849         unrolled_vars: tuple[Hashable, ...]
6850 
6851         if roll_coords:
6852             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6853             unrolled_vars = ()
6854         else:
6855             indexes = dict(self._indexes)
6856             index_vars = dict(self.xindexes.variables)
6857             unrolled_vars = tuple(self.coords)
6858 
6859         variables = {}
6860         for k, var in self.variables.items():
6861             if k in index_vars:
6862                 variables[k] = index_vars[k]
6863             elif k not in unrolled_vars:
6864                 variables[k] = var.roll(
6865                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6866                 )
6867             else:
6868                 variables[k] = var
6869 
6870         return self._replace(variables, indexes=indexes)
6871 
6872     def sortby(
6873         self: T_Dataset,
6874         variables: Hashable | DataArray | list[Hashable | DataArray],
6875         ascending: bool = True,
6876     ) -> T_Dataset:
6877         """
6878         Sort object by labels or values (along an axis).
6879 
6880         Sorts the dataset, either along specified dimensions,
6881         or according to values of 1-D dataarrays that share dimension
6882         with calling object.
6883 
6884         If the input variables are dataarrays, then the dataarrays are aligned
6885         (via left-join) to the calling object prior to sorting by cell values.
6886         NaNs are sorted to the end, following Numpy convention.
6887 
6888         If multiple sorts along the same dimension is
6889         given, numpy's lexsort is performed along that dimension:
6890         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6891         and the FIRST key in the sequence is used as the primary sort key,
6892         followed by the 2nd key, etc.
6893 
6894         Parameters
6895         ----------
6896         variables : Hashable, DataArray, or list of hashable or DataArray
6897             1D DataArray objects or name(s) of 1D variable(s) in
6898             coords/data_vars whose values are used to sort the dataset.
6899         ascending : bool, default: True
6900             Whether to sort by ascending or descending order.
6901 
6902         Returns
6903         -------
6904         sorted : Dataset
6905             A new dataset where all the specified dims are sorted by dim
6906             labels.
6907 
6908         See Also
6909         --------
6910         DataArray.sortby
6911         numpy.sort
6912         pandas.sort_values
6913         pandas.sort_index
6914 
6915         Examples
6916         --------
6917         >>> ds = xr.Dataset(
6918         ...     {
6919         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6920         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6921         ...     },
6922         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6923         ... )
6924         >>> ds.sortby("x")
6925         <xarray.Dataset>
6926         Dimensions:  (x: 2, y: 2)
6927         Coordinates:
6928           * x        (x) <U1 'a' 'b'
6929           * y        (y) int64 1 0
6930         Data variables:
6931             A        (x, y) int64 3 4 1 2
6932             B        (x, y) int64 7 8 5 6
6933         """
6934         from .dataarray import DataArray
6935 
6936         if not isinstance(variables, list):
6937             variables = [variables]
6938         else:
6939             variables = variables
6940         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6941         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6942         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6943         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6944         vars_by_dim = defaultdict(list)
6945         for data_array in aligned_other_vars:
6946             if data_array.ndim != 1:
6947                 raise ValueError("Input DataArray is not 1-D.")
6948             (key,) = data_array.dims
6949             vars_by_dim[key].append(data_array)
6950 
6951         indices = {}
6952         for key, arrays in vars_by_dim.items():
6953             order = np.lexsort(tuple(reversed(arrays)))
6954             indices[key] = order if ascending else order[::-1]
6955         return aligned_self.isel(indices)
6956 
6957     def quantile(
6958         self: T_Dataset,
6959         q: ArrayLike,
6960         dim: str | Iterable[Hashable] | None = None,
6961         method: QUANTILE_METHODS = "linear",
6962         numeric_only: bool = False,
6963         keep_attrs: bool = None,
6964         skipna: bool = None,
6965         interpolation: QUANTILE_METHODS = None,
6966     ) -> T_Dataset:
6967         """Compute the qth quantile of the data along the specified dimension.
6968 
6969         Returns the qth quantiles(s) of the array elements for each variable
6970         in the Dataset.
6971 
6972         Parameters
6973         ----------
6974         q : float or array-like of float
6975             Quantile to compute, which must be between 0 and 1 inclusive.
6976         dim : str or Iterable of Hashable, optional
6977             Dimension(s) over which to apply quantile.
6978         method : str, default: "linear"
6979             This optional parameter specifies the interpolation method to use when the
6980             desired quantile lies between two data points. The options sorted by their R
6981             type as summarized in the H&F paper [1]_ are:
6982 
6983                 1. "inverted_cdf" (*)
6984                 2. "averaged_inverted_cdf" (*)
6985                 3. "closest_observation" (*)
6986                 4. "interpolated_inverted_cdf" (*)
6987                 5. "hazen" (*)
6988                 6. "weibull" (*)
6989                 7. "linear"  (default)
6990                 8. "median_unbiased" (*)
6991                 9. "normal_unbiased" (*)
6992 
6993             The first three methods are discontiuous.  The following discontinuous
6994             variations of the default "linear" (7.) option are also available:
6995 
6996                 * "lower"
6997                 * "higher"
6998                 * "midpoint"
6999                 * "nearest"
7000 
7001             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7002             was previously called "interpolation", renamed in accordance with numpy
7003             version 1.22.0.
7004 
7005             (*) These methods require numpy version 1.22 or newer.
7006 
7007         keep_attrs : bool, optional
7008             If True, the dataset's attributes (`attrs`) will be copied from
7009             the original object to the new one.  If False (default), the new
7010             object will be returned without attributes.
7011         numeric_only : bool, optional
7012             If True, only apply ``func`` to variables with a numeric dtype.
7013         skipna : bool, optional
7014             If True, skip missing values (as marked by NaN). By default, only
7015             skips missing values for float dtypes; other dtypes either do not
7016             have a sentinel missing value (int) or skipna=True has not been
7017             implemented (object, datetime64 or timedelta64).
7018 
7019         Returns
7020         -------
7021         quantiles : Dataset
7022             If `q` is a single quantile, then the result is a scalar for each
7023             variable in data_vars. If multiple percentiles are given, first
7024             axis of the result corresponds to the quantile and a quantile
7025             dimension is added to the return Dataset. The other dimensions are
7026             the dimensions that remain after the reduction of the array.
7027 
7028         See Also
7029         --------
7030         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7031 
7032         Examples
7033         --------
7034         >>> ds = xr.Dataset(
7035         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7036         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7037         ... )
7038         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7039         <xarray.Dataset>
7040         Dimensions:   ()
7041         Coordinates:
7042             quantile  float64 0.0
7043         Data variables:
7044             a         float64 0.7
7045         >>> ds.quantile(0, dim="x")
7046         <xarray.Dataset>
7047         Dimensions:   (y: 4)
7048         Coordinates:
7049           * y         (y) float64 1.0 1.5 2.0 2.5
7050             quantile  float64 0.0
7051         Data variables:
7052             a         (y) float64 0.7 4.2 2.6 1.5
7053         >>> ds.quantile([0, 0.5, 1])
7054         <xarray.Dataset>
7055         Dimensions:   (quantile: 3)
7056         Coordinates:
7057           * quantile  (quantile) float64 0.0 0.5 1.0
7058         Data variables:
7059             a         (quantile) float64 0.7 3.4 9.4
7060         >>> ds.quantile([0, 0.5, 1], dim="x")
7061         <xarray.Dataset>
7062         Dimensions:   (quantile: 3, y: 4)
7063         Coordinates:
7064           * y         (y) float64 1.0 1.5 2.0 2.5
7065           * quantile  (quantile) float64 0.0 0.5 1.0
7066         Data variables:
7067             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7068 
7069         References
7070         ----------
7071         .. [1] R. J. Hyndman and Y. Fan,
7072            "Sample quantiles in statistical packages,"
7073            The American Statistician, 50(4), pp. 361-365, 1996
7074         """
7075 
7076         # interpolation renamed to method in version 0.21.0
7077         # check here and in variable to avoid repeated warnings
7078         if interpolation is not None:
7079             warnings.warn(
7080                 "The `interpolation` argument to quantile was renamed to `method`.",
7081                 FutureWarning,
7082             )
7083 
7084             if method != "linear":
7085                 raise TypeError("Cannot pass interpolation and method keywords!")
7086 
7087             method = interpolation
7088 
7089         dims: set[Hashable]
7090         if isinstance(dim, str):
7091             dims = {dim}
7092         elif dim is None or dim is ...:
7093             dims = set(self.dims)
7094         else:
7095             dims = set(dim)
7096 
7097         _assert_empty(
7098             tuple(d for d in dims if d not in self.dims),
7099             "Dataset does not contain the dimensions: %s",
7100         )
7101 
7102         q = np.asarray(q, dtype=np.float64)
7103 
7104         variables = {}
7105         for name, var in self.variables.items():
7106             reduce_dims = [d for d in var.dims if d in dims]
7107             if reduce_dims or not var.dims:
7108                 if name not in self.coords:
7109                     if (
7110                         not numeric_only
7111                         or np.issubdtype(var.dtype, np.number)
7112                         or var.dtype == np.bool_
7113                     ):
7114                         variables[name] = var.quantile(
7115                             q,
7116                             dim=reduce_dims,
7117                             method=method,
7118                             keep_attrs=keep_attrs,
7119                             skipna=skipna,
7120                         )
7121 
7122             else:
7123                 variables[name] = var
7124 
7125         # construct the new dataset
7126         coord_names = {k for k in self.coords if k in variables}
7127         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7128         if keep_attrs is None:
7129             keep_attrs = _get_keep_attrs(default=False)
7130         attrs = self.attrs if keep_attrs else None
7131         new = self._replace_with_new_dims(
7132             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7133         )
7134         return new.assign_coords(quantile=q)
7135 
7136     def rank(
7137         self: T_Dataset,
7138         dim: Hashable,
7139         pct: bool = False,
7140         keep_attrs: bool | None = None,
7141     ) -> T_Dataset:
7142         """Ranks the data.
7143 
7144         Equal values are assigned a rank that is the average of the ranks that
7145         would have been otherwise assigned to all of the values within
7146         that set.
7147         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7148 
7149         NaNs in the input array are returned as NaNs.
7150 
7151         The `bottleneck` library is required.
7152 
7153         Parameters
7154         ----------
7155         dim : Hashable
7156             Dimension over which to compute rank.
7157         pct : bool, default: False
7158             If True, compute percentage ranks, otherwise compute integer ranks.
7159         keep_attrs : bool or None, optional
7160             If True, the dataset's attributes (`attrs`) will be copied from
7161             the original object to the new one.  If False, the new
7162             object will be returned without attributes.
7163 
7164         Returns
7165         -------
7166         ranked : Dataset
7167             Variables that do not depend on `dim` are dropped.
7168         """
7169         if not OPTIONS["use_bottleneck"]:
7170             raise RuntimeError(
7171                 "rank requires bottleneck to be enabled."
7172                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7173             )
7174 
7175         if dim not in self.dims:
7176             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7177 
7178         variables = {}
7179         for name, var in self.variables.items():
7180             if name in self.data_vars:
7181                 if dim in var.dims:
7182                     variables[name] = var.rank(dim, pct=pct)
7183             else:
7184                 variables[name] = var
7185 
7186         coord_names = set(self.coords)
7187         if keep_attrs is None:
7188             keep_attrs = _get_keep_attrs(default=False)
7189         attrs = self.attrs if keep_attrs else None
7190         return self._replace(variables, coord_names, attrs=attrs)
7191 
7192     def differentiate(
7193         self: T_Dataset,
7194         coord: Hashable,
7195         edge_order: Literal[1, 2] = 1,
7196         datetime_unit: DatetimeUnitOptions | None = None,
7197     ) -> T_Dataset:
7198         """ Differentiate with the second order accurate central
7199         differences.
7200 
7201         .. note::
7202             This feature is limited to simple cartesian geometry, i.e. coord
7203             must be one dimensional.
7204 
7205         Parameters
7206         ----------
7207         coord : Hashable
7208             The coordinate to be used to compute the gradient.
7209         edge_order : {1, 2}, default: 1
7210             N-th order accurate differences at the boundaries.
7211         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7212             "us", "ns", "ps", "fs", "as", None}, default: None
7213             Unit to compute gradient. Only valid for datetime coordinate.
7214 
7215         Returns
7216         -------
7217         differentiated: Dataset
7218 
7219         See also
7220         --------
7221         numpy.gradient: corresponding numpy function
7222         """
7223         from .variable import Variable
7224 
7225         if coord not in self.variables and coord not in self.dims:
7226             raise ValueError(f"Coordinate {coord} does not exist.")
7227 
7228         coord_var = self[coord].variable
7229         if coord_var.ndim != 1:
7230             raise ValueError(
7231                 "Coordinate {} must be 1 dimensional but is {}"
7232                 " dimensional".format(coord, coord_var.ndim)
7233             )
7234 
7235         dim = coord_var.dims[0]
7236         if _contains_datetime_like_objects(coord_var):
7237             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7238                 datetime_unit = cast(
7239                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7240                 )
7241             elif datetime_unit is None:
7242                 datetime_unit = "s"  # Default to seconds for cftime objects
7243             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7244 
7245         variables = {}
7246         for k, v in self.variables.items():
7247             if k in self.data_vars and dim in v.dims and k not in self.coords:
7248                 if _contains_datetime_like_objects(v):
7249                     v = v._to_numeric(datetime_unit=datetime_unit)
7250                 grad = duck_array_ops.gradient(
7251                     v.data,
7252                     coord_var.data,
7253                     edge_order=edge_order,
7254                     axis=v.get_axis_num(dim),
7255                 )
7256                 variables[k] = Variable(v.dims, grad)
7257             else:
7258                 variables[k] = v
7259         return self._replace(variables)
7260 
7261     def integrate(
7262         self: T_Dataset,
7263         coord: Hashable | Sequence[Hashable],
7264         datetime_unit: DatetimeUnitOptions = None,
7265     ) -> T_Dataset:
7266         """Integrate along the given coordinate using the trapezoidal rule.
7267 
7268         .. note::
7269             This feature is limited to simple cartesian geometry, i.e. coord
7270             must be one dimensional.
7271 
7272         Parameters
7273         ----------
7274         coord : hashable, or sequence of hashable
7275             Coordinate(s) used for the integration.
7276         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7277                         'ps', 'fs', 'as', None}, optional
7278             Specify the unit if datetime coordinate is used.
7279 
7280         Returns
7281         -------
7282         integrated : Dataset
7283 
7284         See also
7285         --------
7286         DataArray.integrate
7287         numpy.trapz : corresponding numpy function
7288 
7289         Examples
7290         --------
7291         >>> ds = xr.Dataset(
7292         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7293         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7294         ... )
7295         >>> ds
7296         <xarray.Dataset>
7297         Dimensions:  (x: 4)
7298         Coordinates:
7299           * x        (x) int64 0 1 2 3
7300             y        (x) int64 1 7 3 5
7301         Data variables:
7302             a        (x) int64 5 5 6 6
7303             b        (x) int64 1 2 1 0
7304         >>> ds.integrate("x")
7305         <xarray.Dataset>
7306         Dimensions:  ()
7307         Data variables:
7308             a        float64 16.5
7309             b        float64 3.5
7310         >>> ds.integrate("y")
7311         <xarray.Dataset>
7312         Dimensions:  ()
7313         Data variables:
7314             a        float64 20.0
7315             b        float64 4.0
7316         """
7317         if not isinstance(coord, (list, tuple)):
7318             coord = (coord,)
7319         result = self
7320         for c in coord:
7321             result = result._integrate_one(c, datetime_unit=datetime_unit)
7322         return result
7323 
7324     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7325         from .variable import Variable
7326 
7327         if coord not in self.variables and coord not in self.dims:
7328             raise ValueError(f"Coordinate {coord} does not exist.")
7329 
7330         coord_var = self[coord].variable
7331         if coord_var.ndim != 1:
7332             raise ValueError(
7333                 "Coordinate {} must be 1 dimensional but is {}"
7334                 " dimensional".format(coord, coord_var.ndim)
7335             )
7336 
7337         dim = coord_var.dims[0]
7338         if _contains_datetime_like_objects(coord_var):
7339             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7340                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7341             elif datetime_unit is None:
7342                 datetime_unit = "s"  # Default to seconds for cftime objects
7343             coord_var = coord_var._replace(
7344                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7345             )
7346 
7347         variables = {}
7348         coord_names = set()
7349         for k, v in self.variables.items():
7350             if k in self.coords:
7351                 if dim not in v.dims or cumulative:
7352                     variables[k] = v
7353                     coord_names.add(k)
7354             else:
7355                 if k in self.data_vars and dim in v.dims:
7356                     if _contains_datetime_like_objects(v):
7357                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7358                     if cumulative:
7359                         integ = duck_array_ops.cumulative_trapezoid(
7360                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7361                         )
7362                         v_dims = v.dims
7363                     else:
7364                         integ = duck_array_ops.trapz(
7365                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7366                         )
7367                         v_dims = list(v.dims)
7368                         v_dims.remove(dim)
7369                     variables[k] = Variable(v_dims, integ)
7370                 else:
7371                     variables[k] = v
7372         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7373         return self._replace_with_new_dims(
7374             variables, coord_names=coord_names, indexes=indexes
7375         )
7376 
7377     def cumulative_integrate(
7378         self: T_Dataset,
7379         coord: Hashable | Sequence[Hashable],
7380         datetime_unit: DatetimeUnitOptions = None,
7381     ) -> T_Dataset:
7382         """Integrate along the given coordinate using the trapezoidal rule.
7383 
7384         .. note::
7385             This feature is limited to simple cartesian geometry, i.e. coord
7386             must be one dimensional.
7387 
7388             The first entry of the cumulative integral of each variable is always 0, in
7389             order to keep the length of the dimension unchanged between input and
7390             output.
7391 
7392         Parameters
7393         ----------
7394         coord : hashable, or sequence of hashable
7395             Coordinate(s) used for the integration.
7396         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7397                         'ps', 'fs', 'as', None}, optional
7398             Specify the unit if datetime coordinate is used.
7399 
7400         Returns
7401         -------
7402         integrated : Dataset
7403 
7404         See also
7405         --------
7406         DataArray.cumulative_integrate
7407         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7408 
7409         Examples
7410         --------
7411         >>> ds = xr.Dataset(
7412         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7413         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7414         ... )
7415         >>> ds
7416         <xarray.Dataset>
7417         Dimensions:  (x: 4)
7418         Coordinates:
7419           * x        (x) int64 0 1 2 3
7420             y        (x) int64 1 7 3 5
7421         Data variables:
7422             a        (x) int64 5 5 6 6
7423             b        (x) int64 1 2 1 0
7424         >>> ds.cumulative_integrate("x")
7425         <xarray.Dataset>
7426         Dimensions:  (x: 4)
7427         Coordinates:
7428           * x        (x) int64 0 1 2 3
7429             y        (x) int64 1 7 3 5
7430         Data variables:
7431             a        (x) float64 0.0 5.0 10.5 16.5
7432             b        (x) float64 0.0 1.5 3.0 3.5
7433         >>> ds.cumulative_integrate("y")
7434         <xarray.Dataset>
7435         Dimensions:  (x: 4)
7436         Coordinates:
7437           * x        (x) int64 0 1 2 3
7438             y        (x) int64 1 7 3 5
7439         Data variables:
7440             a        (x) float64 0.0 30.0 8.0 20.0
7441             b        (x) float64 0.0 9.0 3.0 4.0
7442         """
7443         if not isinstance(coord, (list, tuple)):
7444             coord = (coord,)
7445         result = self
7446         for c in coord:
7447             result = result._integrate_one(
7448                 c, datetime_unit=datetime_unit, cumulative=True
7449             )
7450         return result
7451 
7452     @property
7453     def real(self: T_Dataset) -> T_Dataset:
7454         """
7455         The real part of each data variable.
7456 
7457         See Also
7458         --------
7459         numpy.ndarray.real
7460         """
7461         return self.map(lambda x: x.real, keep_attrs=True)
7462 
7463     @property
7464     def imag(self: T_Dataset) -> T_Dataset:
7465         """
7466         The imaginary part of each data variable.
7467 
7468         See Also
7469         --------
7470         numpy.ndarray.imag
7471         """
7472         return self.map(lambda x: x.imag, keep_attrs=True)
7473 
7474     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7475 
7476     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7477         """Returns a ``Dataset`` with variables that match specific conditions.
7478 
7479         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7480         containing only the variables for which all the filter tests pass.
7481         These tests are either ``key=value`` for which the attribute ``key``
7482         has the exact value ``value`` or the callable passed into
7483         ``key=callable`` returns True. The callable will be passed a single
7484         value, either the value of the attribute ``key`` or ``None`` if the
7485         DataArray does not have an attribute with the name ``key``.
7486 
7487         Parameters
7488         ----------
7489         **kwargs
7490             key : str
7491                 Attribute name.
7492             value : callable or obj
7493                 If value is a callable, it should return a boolean in the form
7494                 of bool = func(attr) where attr is da.attrs[key].
7495                 Otherwise, value will be compared to the each
7496                 DataArray's attrs[key].
7497 
7498         Returns
7499         -------
7500         new : Dataset
7501             New dataset with variables filtered by attribute.
7502 
7503         Examples
7504         --------
7505         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7506         >>> precip = 10 * np.random.rand(2, 2, 3)
7507         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7508         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7509         >>> dims = ["x", "y", "time"]
7510         >>> temp_attr = dict(standard_name="air_potential_temperature")
7511         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7512 
7513         >>> ds = xr.Dataset(
7514         ...     dict(
7515         ...         temperature=(dims, temp, temp_attr),
7516         ...         precipitation=(dims, precip, precip_attr),
7517         ...     ),
7518         ...     coords=dict(
7519         ...         lon=(["x", "y"], lon),
7520         ...         lat=(["x", "y"], lat),
7521         ...         time=pd.date_range("2014-09-06", periods=3),
7522         ...         reference_time=pd.Timestamp("2014-09-05"),
7523         ...     ),
7524         ... )
7525 
7526         Get variables matching a specific standard_name:
7527 
7528         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7529         <xarray.Dataset>
7530         Dimensions:         (x: 2, y: 2, time: 3)
7531         Coordinates:
7532             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7533             lat             (x, y) float64 42.25 42.21 42.63 42.59
7534           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7535             reference_time  datetime64[ns] 2014-09-05
7536         Dimensions without coordinates: x, y
7537         Data variables:
7538             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7539 
7540         Get all variables that have a standard_name attribute:
7541 
7542         >>> standard_name = lambda v: v is not None
7543         >>> ds.filter_by_attrs(standard_name=standard_name)
7544         <xarray.Dataset>
7545         Dimensions:         (x: 2, y: 2, time: 3)
7546         Coordinates:
7547             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7548             lat             (x, y) float64 42.25 42.21 42.63 42.59
7549           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7550             reference_time  datetime64[ns] 2014-09-05
7551         Dimensions without coordinates: x, y
7552         Data variables:
7553             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7554             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7555 
7556         """
7557         selection = []
7558         for var_name, variable in self.variables.items():
7559             has_value_flag = False
7560             for attr_name, pattern in kwargs.items():
7561                 attr_value = variable.attrs.get(attr_name)
7562                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7563                     has_value_flag = True
7564                 else:
7565                     has_value_flag = False
7566                     break
7567             if has_value_flag is True:
7568                 selection.append(var_name)
7569         return self[selection]
7570 
7571     def unify_chunks(self: T_Dataset) -> T_Dataset:
7572         """Unify chunk size along all chunked dimensions of this Dataset.
7573 
7574         Returns
7575         -------
7576         Dataset with consistent chunk sizes for all dask-array variables
7577 
7578         See Also
7579         --------
7580         dask.array.core.unify_chunks
7581         """
7582 
7583         return unify_chunks(self)[0]
7584 
7585     def map_blocks(
7586         self,
7587         func: Callable[..., T_Xarray],
7588         args: Sequence[Any] = (),
7589         kwargs: Mapping[str, Any] | None = None,
7590         template: DataArray | Dataset | None = None,
7591     ) -> T_Xarray:
7592         """
7593         Apply a function to each block of this Dataset.
7594 
7595         .. warning::
7596             This method is experimental and its signature may change.
7597 
7598         Parameters
7599         ----------
7600         func : callable
7601             User-provided function that accepts a Dataset as its first
7602             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7603             corresponding to one chunk along each chunked dimension. ``func`` will be
7604             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7605 
7606             This function must return either a single DataArray or a single Dataset.
7607 
7608             This function cannot add a new chunked dimension.
7609         args : sequence
7610             Passed to func after unpacking and subsetting any xarray objects by blocks.
7611             xarray objects in args must be aligned with obj, otherwise an error is raised.
7612         kwargs : Mapping or None
7613             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7614             subset to blocks. Passing dask collections in kwargs is not allowed.
7615         template : DataArray, Dataset or None, optional
7616             xarray object representing the final result after compute is called. If not provided,
7617             the function will be first run on mocked-up data, that looks like this object but
7618             has sizes 0, to determine properties of the returned object such as dtype,
7619             variable names, attributes, new dimensions and new indexes (if any).
7620             ``template`` must be provided if the function changes the size of existing dimensions.
7621             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7622             ``attrs`` set by ``func`` will be ignored.
7623 
7624         Returns
7625         -------
7626         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7627         function.
7628 
7629         Notes
7630         -----
7631         This function is designed for when ``func`` needs to manipulate a whole xarray object
7632         subset to each block. Each block is loaded into memory. In the more common case where
7633         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7634 
7635         If none of the variables in this object is backed by dask arrays, calling this function is
7636         equivalent to calling ``func(obj, *args, **kwargs)``.
7637 
7638         See Also
7639         --------
7640         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7641         xarray.DataArray.map_blocks
7642 
7643         Examples
7644         --------
7645         Calculate an anomaly from climatology using ``.groupby()``. Using
7646         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7647         its indices, and its methods like ``.groupby()``.
7648 
7649         >>> def calculate_anomaly(da, groupby_type="time.month"):
7650         ...     gb = da.groupby(groupby_type)
7651         ...     clim = gb.mean(dim="time")
7652         ...     return gb - clim
7653         ...
7654         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7655         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7656         >>> np.random.seed(123)
7657         >>> array = xr.DataArray(
7658         ...     np.random.rand(len(time)),
7659         ...     dims=["time"],
7660         ...     coords={"time": time, "month": month},
7661         ... ).chunk()
7662         >>> ds = xr.Dataset({"a": array})
7663         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7664         <xarray.Dataset>
7665         Dimensions:  (time: 24)
7666         Coordinates:
7667           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7668             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7669         Data variables:
7670             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7671 
7672         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7673         to the function being applied in ``xr.map_blocks()``:
7674 
7675         >>> ds.map_blocks(
7676         ...     calculate_anomaly,
7677         ...     kwargs={"groupby_type": "time.year"},
7678         ...     template=ds,
7679         ... )
7680         <xarray.Dataset>
7681         Dimensions:  (time: 24)
7682         Coordinates:
7683           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7684             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7685         Data variables:
7686             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7687         """
7688         from .parallel import map_blocks
7689 
7690         return map_blocks(func, self, args, kwargs, template)
7691 
7692     def polyfit(
7693         self: T_Dataset,
7694         dim: Hashable,
7695         deg: int,
7696         skipna: bool | None = None,
7697         rcond: float | None = None,
7698         w: Hashable | Any = None,
7699         full: bool = False,
7700         cov: bool | Literal["unscaled"] = False,
7701     ) -> T_Dataset:
7702         """
7703         Least squares polynomial fit.
7704 
7705         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7706         invalid values when `skipna = True`.
7707 
7708         Parameters
7709         ----------
7710         dim : hashable
7711             Coordinate along which to fit the polynomials.
7712         deg : int
7713             Degree of the fitting polynomial.
7714         skipna : bool or None, optional
7715             If True, removes all invalid values before fitting each 1D slices of the array.
7716             Default is True if data is stored in a dask.array or if there is any
7717             invalid values, False otherwise.
7718         rcond : float or None, optional
7719             Relative condition number to the fit.
7720         w : hashable or Any, optional
7721             Weights to apply to the y-coordinate of the sample points.
7722             Can be an array-like object or the name of a coordinate in the dataset.
7723         full : bool, default: False
7724             Whether to return the residuals, matrix rank and singular values in addition
7725             to the coefficients.
7726         cov : bool or "unscaled", default: False
7727             Whether to return to the covariance matrix in addition to the coefficients.
7728             The matrix is not scaled if `cov='unscaled'`.
7729 
7730         Returns
7731         -------
7732         polyfit_results : Dataset
7733             A single dataset which contains (for each "var" in the input dataset):
7734 
7735             [var]_polyfit_coefficients
7736                 The coefficients of the best fit for each variable in this dataset.
7737             [var]_polyfit_residuals
7738                 The residuals of the least-square computation for each variable (only included if `full=True`)
7739                 When the matrix rank is deficient, np.nan is returned.
7740             [dim]_matrix_rank
7741                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7742                 The rank is computed ignoring the NaN values that might be skipped.
7743             [dim]_singular_values
7744                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7745             [var]_polyfit_covariance
7746                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7747 
7748         Warns
7749         -----
7750         RankWarning
7751             The rank of the coefficient matrix in the least-squares fit is deficient.
7752             The warning is not raised with in-memory (not dask) data and `full=True`.
7753 
7754         See Also
7755         --------
7756         numpy.polyfit
7757         numpy.polyval
7758         xarray.polyval
7759         """
7760         from .dataarray import DataArray
7761 
7762         variables = {}
7763         skipna_da = skipna
7764 
7765         x = get_clean_interp_index(self, dim, strict=False)
7766         xname = f"{self[dim].name}_"
7767         order = int(deg) + 1
7768         lhs = np.vander(x, order)
7769 
7770         if rcond is None:
7771             rcond = (
7772                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7773             )
7774 
7775         # Weights:
7776         if w is not None:
7777             if isinstance(w, Hashable):
7778                 w = self.coords[w]
7779             w = np.asarray(w)
7780             if w.ndim != 1:
7781                 raise TypeError("Expected a 1-d array for weights.")
7782             if w.shape[0] != lhs.shape[0]:
7783                 raise TypeError(f"Expected w and {dim} to have the same length")
7784             lhs *= w[:, np.newaxis]
7785 
7786         # Scaling
7787         scale = np.sqrt((lhs * lhs).sum(axis=0))
7788         lhs /= scale
7789 
7790         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7791 
7792         rank = np.linalg.matrix_rank(lhs)
7793 
7794         if full:
7795             rank = DataArray(rank, name=xname + "matrix_rank")
7796             variables[rank.name] = rank
7797             _sing = np.linalg.svd(lhs, compute_uv=False)
7798             sing = DataArray(
7799                 _sing,
7800                 dims=(degree_dim,),
7801                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7802                 name=xname + "singular_values",
7803             )
7804             variables[sing.name] = sing
7805 
7806         for name, da in self.data_vars.items():
7807             if dim not in da.dims:
7808                 continue
7809 
7810             if is_duck_dask_array(da.data) and (
7811                 rank != order or full or skipna is None
7812             ):
7813                 # Current algorithm with dask and skipna=False neither supports
7814                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7815                 skipna_da = True
7816             elif skipna is None:
7817                 skipna_da = bool(np.any(da.isnull()))
7818 
7819             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7820             stacked_coords: dict[Hashable, DataArray] = {}
7821             if dims_to_stack:
7822                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7823                 rhs = da.transpose(dim, *dims_to_stack).stack(
7824                     {stacked_dim: dims_to_stack}
7825                 )
7826                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7827                 scale_da = scale[:, np.newaxis]
7828             else:
7829                 rhs = da
7830                 scale_da = scale
7831 
7832             if w is not None:
7833                 rhs *= w[:, np.newaxis]
7834 
7835             with warnings.catch_warnings():
7836                 if full:  # Copy np.polyfit behavior
7837                     warnings.simplefilter("ignore", np.RankWarning)
7838                 else:  # Raise only once per variable
7839                     warnings.simplefilter("once", np.RankWarning)
7840 
7841                 coeffs, residuals = duck_array_ops.least_squares(
7842                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7843                 )
7844 
7845             if isinstance(name, str):
7846                 name = f"{name}_"
7847             else:
7848                 # Thus a ReprObject => polyfit was called on a DataArray
7849                 name = ""
7850 
7851             coeffs = DataArray(
7852                 coeffs / scale_da,
7853                 dims=[degree_dim] + list(stacked_coords.keys()),
7854                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7855                 name=name + "polyfit_coefficients",
7856             )
7857             if dims_to_stack:
7858                 coeffs = coeffs.unstack(stacked_dim)
7859             variables[coeffs.name] = coeffs
7860 
7861             if full or (cov is True):
7862                 residuals = DataArray(
7863                     residuals if dims_to_stack else residuals.squeeze(),
7864                     dims=list(stacked_coords.keys()),
7865                     coords=stacked_coords,
7866                     name=name + "polyfit_residuals",
7867                 )
7868                 if dims_to_stack:
7869                     residuals = residuals.unstack(stacked_dim)
7870                 variables[residuals.name] = residuals
7871 
7872             if cov:
7873                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7874                 Vbase /= np.outer(scale, scale)
7875                 if cov == "unscaled":
7876                     fac = 1
7877                 else:
7878                     if x.shape[0] <= order:
7879                         raise ValueError(
7880                             "The number of data points must exceed order to scale the covariance matrix."
7881                         )
7882                     fac = residuals / (x.shape[0] - order)
7883                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7884                 variables[name + "polyfit_covariance"] = covariance
7885 
7886         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7887 
7888     def pad(
7889         self: T_Dataset,
7890         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7891         mode: PadModeOptions = "constant",
7892         stat_length: int
7893         | tuple[int, int]
7894         | Mapping[Any, tuple[int, int]]
7895         | None = None,
7896         constant_values: (
7897             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7898         ) = None,
7899         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7900         reflect_type: PadReflectOptions = None,
7901         **pad_width_kwargs: Any,
7902     ) -> T_Dataset:
7903         """Pad this dataset along one or more dimensions.
7904 
7905         .. warning::
7906             This function is experimental and its behaviour is likely to change
7907             especially regarding padding of dimension coordinates (or IndexVariables).
7908 
7909         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7910         coordinates will be padded with the same mode, otherwise coordinates
7911         are padded using the "constant" mode with fill_value dtypes.NA.
7912 
7913         Parameters
7914         ----------
7915         pad_width : mapping of hashable to tuple of int
7916             Mapping with the form of {dim: (pad_before, pad_after)}
7917             describing the number of values padded along each dimension.
7918             {dim: pad} is a shortcut for pad_before = pad_after = pad
7919         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7920             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7921             How to pad the DataArray (taken from numpy docs):
7922 
7923             - "constant": Pads with a constant value.
7924             - "edge": Pads with the edge values of array.
7925             - "linear_ramp": Pads with the linear ramp between end_value and the
7926               array edge value.
7927             - "maximum": Pads with the maximum value of all or part of the
7928               vector along each axis.
7929             - "mean": Pads with the mean value of all or part of the
7930               vector along each axis.
7931             - "median": Pads with the median value of all or part of the
7932               vector along each axis.
7933             - "minimum": Pads with the minimum value of all or part of the
7934               vector along each axis.
7935             - "reflect": Pads with the reflection of the vector mirrored on
7936               the first and last values of the vector along each axis.
7937             - "symmetric": Pads with the reflection of the vector mirrored
7938               along the edge of the array.
7939             - "wrap": Pads with the wrap of the vector along the axis.
7940               The first values are used to pad the end and the
7941               end values are used to pad the beginning.
7942 
7943         stat_length : int, tuple or mapping of hashable to tuple, default: None
7944             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7945             values at edge of each axis used to calculate the statistic value.
7946             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7947             statistic lengths along each dimension.
7948             ((before, after),) yields same before and after statistic lengths
7949             for each dimension.
7950             (stat_length,) or int is a shortcut for before = after = statistic
7951             length for all axes.
7952             Default is ``None``, to use the entire axis.
7953         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7954             Used in 'constant'.  The values to set the padded values for each
7955             axis.
7956             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7957             pad constants along each dimension.
7958             ``((before, after),)`` yields same before and after constants for each
7959             dimension.
7960             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7961             all dimensions.
7962             Default is 0.
7963         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7964             Used in 'linear_ramp'.  The values used for the ending value of the
7965             linear_ramp and that will form the edge of the padded array.
7966             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7967             end values along each dimension.
7968             ``((before, after),)`` yields same before and after end values for each
7969             axis.
7970             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7971             all axes.
7972             Default is 0.
7973         reflect_type : {"even", "odd", None}, optional
7974             Used in "reflect", and "symmetric".  The "even" style is the
7975             default with an unaltered reflection around the edge value.  For
7976             the "odd" style, the extended part of the array is created by
7977             subtracting the reflected values from two times the edge value.
7978         **pad_width_kwargs
7979             The keyword arguments form of ``pad_width``.
7980             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7981 
7982         Returns
7983         -------
7984         padded : Dataset
7985             Dataset with the padded coordinates and data.
7986 
7987         See Also
7988         --------
7989         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7990 
7991         Notes
7992         -----
7993         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7994         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7995         specify ``constant_values=np.nan``
7996 
7997         Padding coordinates will drop their corresponding index (if any) and will reset default
7998         indexes for dimension coordinates.
7999 
8000         Examples
8001         --------
8002         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8003         >>> ds.pad(x=(1, 2))
8004         <xarray.Dataset>
8005         Dimensions:  (x: 8)
8006         Dimensions without coordinates: x
8007         Data variables:
8008             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8009         """
8010         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8011 
8012         if mode in ("edge", "reflect", "symmetric", "wrap"):
8013             coord_pad_mode = mode
8014             coord_pad_options = {
8015                 "stat_length": stat_length,
8016                 "constant_values": constant_values,
8017                 "end_values": end_values,
8018                 "reflect_type": reflect_type,
8019             }
8020         else:
8021             coord_pad_mode = "constant"
8022             coord_pad_options = {}
8023 
8024         variables = {}
8025 
8026         # keep indexes that won't be affected by pad and drop all other indexes
8027         xindexes = self.xindexes
8028         pad_dims = set(pad_width)
8029         indexes = {}
8030         for k, idx in xindexes.items():
8031             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8032                 indexes[k] = idx
8033 
8034         for name, var in self.variables.items():
8035             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8036             if not var_pad_width:
8037                 variables[name] = var
8038             elif name in self.data_vars:
8039                 variables[name] = var.pad(
8040                     pad_width=var_pad_width,
8041                     mode=mode,
8042                     stat_length=stat_length,
8043                     constant_values=constant_values,
8044                     end_values=end_values,
8045                     reflect_type=reflect_type,
8046                 )
8047             else:
8048                 variables[name] = var.pad(
8049                     pad_width=var_pad_width,
8050                     mode=coord_pad_mode,
8051                     **coord_pad_options,  # type: ignore[arg-type]
8052                 )
8053                 # reset default index of dimension coordinates
8054                 if (name,) == var.dims:
8055                     dim_var = {name: variables[name]}
8056                     index = PandasIndex.from_variables(dim_var, options={})
8057                     index_vars = index.create_variables(dim_var)
8058                     indexes[name] = index
8059                     variables[name] = index_vars[name]
8060 
8061         return self._replace_with_new_dims(variables, indexes=indexes)
8062 
8063     def idxmin(
8064         self: T_Dataset,
8065         dim: Hashable | None = None,
8066         skipna: bool | None = None,
8067         fill_value: Any = xrdtypes.NA,
8068         keep_attrs: bool | None = None,
8069     ) -> T_Dataset:
8070         """Return the coordinate label of the minimum value along a dimension.
8071 
8072         Returns a new `Dataset` named after the dimension with the values of
8073         the coordinate labels along that dimension corresponding to minimum
8074         values along that dimension.
8075 
8076         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8077         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8078 
8079         Parameters
8080         ----------
8081         dim : Hashable, optional
8082             Dimension over which to apply `idxmin`.  This is optional for 1D
8083             variables, but required for variables with 2 or more dimensions.
8084         skipna : bool or None, optional
8085             If True, skip missing values (as marked by NaN). By default, only
8086             skips missing values for ``float``, ``complex``, and ``object``
8087             dtypes; other dtypes either do not have a sentinel missing value
8088             (``int``) or ``skipna=True`` has not been implemented
8089             (``datetime64`` or ``timedelta64``).
8090         fill_value : Any, default: NaN
8091             Value to be filled in case all of the values along a dimension are
8092             null.  By default this is NaN.  The fill value and result are
8093             automatically converted to a compatible dtype if possible.
8094             Ignored if ``skipna`` is False.
8095         keep_attrs : bool or None, optional
8096             If True, the attributes (``attrs``) will be copied from the
8097             original object to the new one. If False, the new object
8098             will be returned without attributes.
8099 
8100         Returns
8101         -------
8102         reduced : Dataset
8103             New `Dataset` object with `idxmin` applied to its data and the
8104             indicated dimension removed.
8105 
8106         See Also
8107         --------
8108         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8109 
8110         Examples
8111         --------
8112         >>> array1 = xr.DataArray(
8113         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8114         ... )
8115         >>> array2 = xr.DataArray(
8116         ...     [
8117         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8118         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8119         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8120         ...     ],
8121         ...     dims=["y", "x"],
8122         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8123         ... )
8124         >>> ds = xr.Dataset({"int": array1, "float": array2})
8125         >>> ds.min(dim="x")
8126         <xarray.Dataset>
8127         Dimensions:  (y: 3)
8128         Coordinates:
8129           * y        (y) int64 -1 0 1
8130         Data variables:
8131             int      int64 -2
8132             float    (y) float64 -2.0 -4.0 1.0
8133         >>> ds.argmin(dim="x")
8134         <xarray.Dataset>
8135         Dimensions:  (y: 3)
8136         Coordinates:
8137           * y        (y) int64 -1 0 1
8138         Data variables:
8139             int      int64 4
8140             float    (y) int64 4 0 2
8141         >>> ds.idxmin(dim="x")
8142         <xarray.Dataset>
8143         Dimensions:  (y: 3)
8144         Coordinates:
8145           * y        (y) int64 -1 0 1
8146         Data variables:
8147             int      <U1 'e'
8148             float    (y) object 'e' 'a' 'c'
8149         """
8150         return self.map(
8151             methodcaller(
8152                 "idxmin",
8153                 dim=dim,
8154                 skipna=skipna,
8155                 fill_value=fill_value,
8156                 keep_attrs=keep_attrs,
8157             )
8158         )
8159 
8160     def idxmax(
8161         self: T_Dataset,
8162         dim: Hashable | None = None,
8163         skipna: bool | None = None,
8164         fill_value: Any = xrdtypes.NA,
8165         keep_attrs: bool | None = None,
8166     ) -> T_Dataset:
8167         """Return the coordinate label of the maximum value along a dimension.
8168 
8169         Returns a new `Dataset` named after the dimension with the values of
8170         the coordinate labels along that dimension corresponding to maximum
8171         values along that dimension.
8172 
8173         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8174         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8175 
8176         Parameters
8177         ----------
8178         dim : str, optional
8179             Dimension over which to apply `idxmax`.  This is optional for 1D
8180             variables, but required for variables with 2 or more dimensions.
8181         skipna : bool or None, optional
8182             If True, skip missing values (as marked by NaN). By default, only
8183             skips missing values for ``float``, ``complex``, and ``object``
8184             dtypes; other dtypes either do not have a sentinel missing value
8185             (``int``) or ``skipna=True`` has not been implemented
8186             (``datetime64`` or ``timedelta64``).
8187         fill_value : Any, default: NaN
8188             Value to be filled in case all of the values along a dimension are
8189             null.  By default this is NaN.  The fill value and result are
8190             automatically converted to a compatible dtype if possible.
8191             Ignored if ``skipna`` is False.
8192         keep_attrs : bool or None, optional
8193             If True, the attributes (``attrs``) will be copied from the
8194             original object to the new one. If False, the new object
8195             will be returned without attributes.
8196 
8197         Returns
8198         -------
8199         reduced : Dataset
8200             New `Dataset` object with `idxmax` applied to its data and the
8201             indicated dimension removed.
8202 
8203         See Also
8204         --------
8205         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8206 
8207         Examples
8208         --------
8209         >>> array1 = xr.DataArray(
8210         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8211         ... )
8212         >>> array2 = xr.DataArray(
8213         ...     [
8214         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8215         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8216         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8217         ...     ],
8218         ...     dims=["y", "x"],
8219         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8220         ... )
8221         >>> ds = xr.Dataset({"int": array1, "float": array2})
8222         >>> ds.max(dim="x")
8223         <xarray.Dataset>
8224         Dimensions:  (y: 3)
8225         Coordinates:
8226           * y        (y) int64 -1 0 1
8227         Data variables:
8228             int      int64 2
8229             float    (y) float64 2.0 2.0 1.0
8230         >>> ds.argmax(dim="x")
8231         <xarray.Dataset>
8232         Dimensions:  (y: 3)
8233         Coordinates:
8234           * y        (y) int64 -1 0 1
8235         Data variables:
8236             int      int64 1
8237             float    (y) int64 0 2 2
8238         >>> ds.idxmax(dim="x")
8239         <xarray.Dataset>
8240         Dimensions:  (y: 3)
8241         Coordinates:
8242           * y        (y) int64 -1 0 1
8243         Data variables:
8244             int      <U1 'b'
8245             float    (y) object 'a' 'c' 'c'
8246         """
8247         return self.map(
8248             methodcaller(
8249                 "idxmax",
8250                 dim=dim,
8251                 skipna=skipna,
8252                 fill_value=fill_value,
8253                 keep_attrs=keep_attrs,
8254             )
8255         )
8256 
8257     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8258         """Indices of the minima of the member variables.
8259 
8260         If there are multiple minima, the indices of the first one found will be
8261         returned.
8262 
8263         Parameters
8264         ----------
8265         dim : Hashable, optional
8266             The dimension over which to find the minimum. By default, finds minimum over
8267             all dimensions - for now returning an int for backward compatibility, but
8268             this is deprecated, in future will be an error, since DataArray.argmin will
8269             return a dict with indices for all dimensions, which does not make sense for
8270             a Dataset.
8271         keep_attrs : bool, optional
8272             If True, the attributes (`attrs`) will be copied from the original
8273             object to the new one.  If False (default), the new object will be
8274             returned without attributes.
8275         skipna : bool, optional
8276             If True, skip missing values (as marked by NaN). By default, only
8277             skips missing values for float dtypes; other dtypes either do not
8278             have a sentinel missing value (int) or skipna=True has not been
8279             implemented (object, datetime64 or timedelta64).
8280 
8281         Returns
8282         -------
8283         result : Dataset
8284 
8285         See Also
8286         --------
8287         DataArray.argmin
8288         """
8289         if dim is None:
8290             warnings.warn(
8291                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8292                 "dim changes to return a dict of indices of each dimension, for "
8293                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8294                 "since we don't return a dict of Datasets.",
8295                 DeprecationWarning,
8296                 stacklevel=2,
8297             )
8298         if (
8299             dim is None
8300             or (not isinstance(dim, Sequence) and dim is not ...)
8301             or isinstance(dim, str)
8302         ):
8303             # Return int index if single dimension is passed, and is not part of a
8304             # sequence
8305             argmin_func = getattr(duck_array_ops, "argmin")
8306             return self.reduce(argmin_func, dim=dim, **kwargs)
8307         else:
8308             raise ValueError(
8309                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8310                 "dicts cannot be contained in a Dataset, so cannot call "
8311                 "Dataset.argmin() with a sequence or ... for dim"
8312             )
8313 
8314     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8315         """Indices of the maxima of the member variables.
8316 
8317         If there are multiple maxima, the indices of the first one found will be
8318         returned.
8319 
8320         Parameters
8321         ----------
8322         dim : str, optional
8323             The dimension over which to find the maximum. By default, finds maximum over
8324             all dimensions - for now returning an int for backward compatibility, but
8325             this is deprecated, in future will be an error, since DataArray.argmax will
8326             return a dict with indices for all dimensions, which does not make sense for
8327             a Dataset.
8328         keep_attrs : bool, optional
8329             If True, the attributes (`attrs`) will be copied from the original
8330             object to the new one.  If False (default), the new object will be
8331             returned without attributes.
8332         skipna : bool, optional
8333             If True, skip missing values (as marked by NaN). By default, only
8334             skips missing values for float dtypes; other dtypes either do not
8335             have a sentinel missing value (int) or skipna=True has not been
8336             implemented (object, datetime64 or timedelta64).
8337 
8338         Returns
8339         -------
8340         result : Dataset
8341 
8342         See Also
8343         --------
8344         DataArray.argmax
8345 
8346         """
8347         if dim is None:
8348             warnings.warn(
8349                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8350                 "dim changes to return a dict of indices of each dimension, for "
8351                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8352                 "since we don't return a dict of Datasets.",
8353                 DeprecationWarning,
8354                 stacklevel=2,
8355             )
8356         if (
8357             dim is None
8358             or (not isinstance(dim, Sequence) and dim is not ...)
8359             or isinstance(dim, str)
8360         ):
8361             # Return int index if single dimension is passed, and is not part of a
8362             # sequence
8363             argmax_func = getattr(duck_array_ops, "argmax")
8364             return self.reduce(argmax_func, dim=dim, **kwargs)
8365         else:
8366             raise ValueError(
8367                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8368                 "dicts cannot be contained in a Dataset, so cannot call "
8369                 "Dataset.argmin() with a sequence or ... for dim"
8370             )
8371 
8372     def query(
8373         self: T_Dataset,
8374         queries: Mapping[Any, Any] | None = None,
8375         parser: QueryParserOptions = "pandas",
8376         engine: QueryEngineOptions = None,
8377         missing_dims: ErrorOptionsWithWarn = "raise",
8378         **queries_kwargs: Any,
8379     ) -> T_Dataset:
8380         """Return a new dataset with each array indexed along the specified
8381         dimension(s), where the indexers are given as strings containing
8382         Python expressions to be evaluated against the data variables in the
8383         dataset.
8384 
8385         Parameters
8386         ----------
8387         queries : dict-like, optional
8388             A dict-like with keys matching dimensions and values given by strings
8389             containing Python expressions to be evaluated against the data variables
8390             in the dataset. The expressions will be evaluated using the pandas
8391             eval() function, and can contain any valid Python expressions but cannot
8392             contain any Python statements.
8393         parser : {"pandas", "python"}, default: "pandas"
8394             The parser to use to construct the syntax tree from the expression.
8395             The default of 'pandas' parses code slightly different than standard
8396             Python. Alternatively, you can parse an expression using the 'python'
8397             parser to retain strict Python semantics.
8398         engine : {"python", "numexpr", None}, default: None
8399             The engine used to evaluate the expression. Supported engines are:
8400 
8401             - None: tries to use numexpr, falls back to python
8402             - "numexpr": evaluates expressions using numexpr
8403             - "python": performs operations as if you had evald in top level python
8404 
8405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8406             What to do if dimensions that should be selected from are not present in the
8407             Dataset:
8408 
8409             - "raise": raise an exception
8410             - "warn": raise a warning, and ignore the missing dimensions
8411             - "ignore": ignore the missing dimensions
8412 
8413         **queries_kwargs : {dim: query, ...}, optional
8414             The keyword arguments form of ``queries``.
8415             One of queries or queries_kwargs must be provided.
8416 
8417         Returns
8418         -------
8419         obj : Dataset
8420             A new Dataset with the same contents as this dataset, except each
8421             array and dimension is indexed by the results of the appropriate
8422             queries.
8423 
8424         See Also
8425         --------
8426         Dataset.isel
8427         pandas.eval
8428 
8429         Examples
8430         --------
8431         >>> a = np.arange(0, 5, 1)
8432         >>> b = np.linspace(0, 1, 5)
8433         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8434         >>> ds
8435         <xarray.Dataset>
8436         Dimensions:  (x: 5)
8437         Dimensions without coordinates: x
8438         Data variables:
8439             a        (x) int64 0 1 2 3 4
8440             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8441         >>> ds.query(x="a > 2")
8442         <xarray.Dataset>
8443         Dimensions:  (x: 2)
8444         Dimensions without coordinates: x
8445         Data variables:
8446             a        (x) int64 3 4
8447             b        (x) float64 0.75 1.0
8448         """
8449 
8450         # allow queries to be given either as a dict or as kwargs
8451         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8452 
8453         # check queries
8454         for dim, expr in queries.items():
8455             if not isinstance(expr, str):
8456                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8457                 raise ValueError(msg)
8458 
8459         # evaluate the queries to create the indexers
8460         indexers = {
8461             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8462             for dim, expr in queries.items()
8463         }
8464 
8465         # apply the selection
8466         return self.isel(indexers, missing_dims=missing_dims)
8467 
8468     def curvefit(
8469         self: T_Dataset,
8470         coords: str | DataArray | Iterable[str | DataArray],
8471         func: Callable[..., Any],
8472         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8473         skipna: bool = True,
8474         p0: dict[str, Any] | None = None,
8475         bounds: dict[str, Any] | None = None,
8476         param_names: Sequence[str] | None = None,
8477         kwargs: dict[str, Any] | None = None,
8478     ) -> T_Dataset:
8479         """
8480         Curve fitting optimization for arbitrary functions.
8481 
8482         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8483 
8484         Parameters
8485         ----------
8486         coords : hashable, DataArray, or sequence of hashable or DataArray
8487             Independent coordinate(s) over which to perform the curve fitting. Must share
8488             at least one dimension with the calling object. When fitting multi-dimensional
8489             functions, supply `coords` as a sequence in the same order as arguments in
8490             `func`. To fit along existing dimensions of the calling object, `coords` can
8491             also be specified as a str or sequence of strs.
8492         func : callable
8493             User specified function in the form `f(x, *params)` which returns a numpy
8494             array of length `len(x)`. `params` are the fittable parameters which are optimized
8495             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8496             coordinates, e.g. `f((x0, x1), *params)`.
8497         reduce_dims : hashable or sequence of hashable
8498             Additional dimension(s) over which to aggregate while fitting. For example,
8499             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8500             aggregate all lat and lon points and fit the specified function along the
8501             time dimension.
8502         skipna : bool, default: True
8503             Whether to skip missing values when fitting. Default is True.
8504         p0 : dict-like, optional
8505             Optional dictionary of parameter names to initial guesses passed to the
8506             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8507             be assigned initial values following the default scipy behavior.
8508         bounds : dict-like, optional
8509             Optional dictionary of parameter names to bounding values passed to the
8510             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8511             will be unbounded following the default scipy behavior.
8512         param_names : sequence of hashable, optional
8513             Sequence of names for the fittable parameters of `func`. If not supplied,
8514             this will be automatically determined by arguments of `func`. `param_names`
8515             should be manually supplied when fitting a function that takes a variable
8516             number of parameters.
8517         **kwargs : optional
8518             Additional keyword arguments to passed to scipy curve_fit.
8519 
8520         Returns
8521         -------
8522         curvefit_results : Dataset
8523             A single dataset which contains:
8524 
8525             [var]_curvefit_coefficients
8526                 The coefficients of the best fit.
8527             [var]_curvefit_covariance
8528                 The covariance matrix of the coefficient estimates.
8529 
8530         See Also
8531         --------
8532         Dataset.polyfit
8533         scipy.optimize.curve_fit
8534         """
8535         from scipy.optimize import curve_fit
8536 
8537         from .alignment import broadcast
8538         from .computation import apply_ufunc
8539         from .dataarray import _THIS_ARRAY, DataArray
8540 
8541         if p0 is None:
8542             p0 = {}
8543         if bounds is None:
8544             bounds = {}
8545         if kwargs is None:
8546             kwargs = {}
8547 
8548         if not reduce_dims:
8549             reduce_dims_ = []
8550         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8551             reduce_dims_ = [reduce_dims]
8552         else:
8553             reduce_dims_ = list(reduce_dims)
8554 
8555         if (
8556             isinstance(coords, str)
8557             or isinstance(coords, DataArray)
8558             or not isinstance(coords, Iterable)
8559         ):
8560             coords = [coords]
8561         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8562 
8563         # Determine whether any coords are dims on self
8564         for coord in coords_:
8565             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8566         reduce_dims_ = list(set(reduce_dims_))
8567         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8568         if not reduce_dims_:
8569             raise ValueError(
8570                 "No arguments to `coords` were identified as a dimension on the calling "
8571                 "object, and no dims were supplied to `reduce_dims`. This would result "
8572                 "in fitting on scalar data."
8573             )
8574 
8575         # Broadcast all coords with each other
8576         coords_ = broadcast(*coords_)
8577         coords_ = [
8578             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8579         ]
8580 
8581         params, func_args = _get_func_args(func, param_names)
8582         param_defaults, bounds_defaults = _initialize_curvefit_params(
8583             params, p0, bounds, func_args
8584         )
8585         n_params = len(params)
8586         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8587         kwargs.setdefault(
8588             "bounds",
8589             [
8590                 [bounds_defaults[p][0] for p in params],
8591                 [bounds_defaults[p][1] for p in params],
8592             ],
8593         )
8594 
8595         def _wrapper(Y, *coords_, **kwargs):
8596             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8597             x = np.vstack([c.ravel() for c in coords_])
8598             y = Y.ravel()
8599             if skipna:
8600                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8601                 x = x[:, mask]
8602                 y = y[mask]
8603                 if not len(y):
8604                     popt = np.full([n_params], np.nan)
8605                     pcov = np.full([n_params, n_params], np.nan)
8606                     return popt, pcov
8607             x = np.squeeze(x)
8608             popt, pcov = curve_fit(func, x, y, **kwargs)
8609             return popt, pcov
8610 
8611         result = type(self)()
8612         for name, da in self.data_vars.items():
8613             if name is _THIS_ARRAY:
8614                 name = ""
8615             else:
8616                 name = f"{str(name)}_"
8617 
8618             popt, pcov = apply_ufunc(
8619                 _wrapper,
8620                 da,
8621                 *coords_,
8622                 vectorize=True,
8623                 dask="parallelized",
8624                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8625                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8626                 dask_gufunc_kwargs={
8627                     "output_sizes": {
8628                         "param": n_params,
8629                         "cov_i": n_params,
8630                         "cov_j": n_params,
8631                     },
8632                 },
8633                 output_dtypes=(np.float64, np.float64),
8634                 exclude_dims=set(reduce_dims_),
8635                 kwargs=kwargs,
8636             )
8637             result[name + "curvefit_coefficients"] = popt
8638             result[name + "curvefit_covariance"] = pcov
8639 
8640         result = result.assign_coords(
8641             {"param": params, "cov_i": params, "cov_j": params}
8642         )
8643         result.attrs = self.attrs.copy()
8644 
8645         return result
8646 
8647     def drop_duplicates(
8648         self: T_Dataset,
8649         dim: Hashable | Iterable[Hashable],
8650         keep: Literal["first", "last", False] = "first",
8651     ) -> T_Dataset:
8652         """Returns a new Dataset with duplicate dimension values removed.
8653 
8654         Parameters
8655         ----------
8656         dim : dimension label or labels
8657             Pass `...` to drop duplicates along all dimensions.
8658         keep : {"first", "last", False}, default: "first"
8659             Determines which duplicates (if any) to keep.
8660             - ``"first"`` : Drop duplicates except for the first occurrence.
8661             - ``"last"`` : Drop duplicates except for the last occurrence.
8662             - False : Drop all duplicates.
8663 
8664         Returns
8665         -------
8666         Dataset
8667 
8668         See Also
8669         --------
8670         DataArray.drop_duplicates
8671         """
8672         if isinstance(dim, str):
8673             dims: Iterable = (dim,)
8674         elif dim is ...:
8675             dims = self.dims
8676         elif not isinstance(dim, Iterable):
8677             dims = [dim]
8678         else:
8679             dims = dim
8680 
8681         missing_dims = set(dims) - set(self.dims)
8682         if missing_dims:
8683             raise ValueError(f"'{missing_dims}' not found in dimensions")
8684 
8685         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8686         return self.isel(indexes)
8687 
8688     def convert_calendar(
8689         self: T_Dataset,
8690         calendar: CFCalendar,
8691         dim: Hashable = "time",
8692         align_on: Literal["date", "year", None] = None,
8693         missing: Any | None = None,
8694         use_cftime: bool | None = None,
8695     ) -> T_Dataset:
8696         """Convert the Dataset to another calendar.
8697 
8698         Only converts the individual timestamps, does not modify any data except
8699         in dropping invalid/surplus dates or inserting missing dates.
8700 
8701         If the source and target calendars are either no_leap, all_leap or a
8702         standard type, only the type of the time array is modified.
8703         When converting to a leap year from a non-leap year, the 29th of February
8704         is removed from the array. In the other direction the 29th of February
8705         will be missing in the output, unless `missing` is specified,
8706         in which case that value is inserted.
8707 
8708         For conversions involving `360_day` calendars, see Notes.
8709 
8710         This method is safe to use with sub-daily data as it doesn't touch the
8711         time part of the timestamps.
8712 
8713         Parameters
8714         ---------
8715         calendar : str
8716             The target calendar name.
8717         dim : Hashable, default: "time"
8718             Name of the time coordinate.
8719         align_on : {None, 'date', 'year'}, optional
8720             Must be specified when either source or target is a `360_day` calendar,
8721             ignored otherwise. See Notes.
8722         missing : Any or None, optional
8723             By default, i.e. if the value is None, this method will simply attempt
8724             to convert the dates in the source calendar to the same dates in the
8725             target calendar, and drop any of those that are not possible to
8726             represent.  If a value is provided, a new time coordinate will be
8727             created in the target calendar with the same frequency as the original
8728             time coordinate; for any dates that are not present in the source, the
8729             data will be filled with this value.  Note that using this mode requires
8730             that the source data have an inferable frequency; for more information
8731             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8732             target calendar combinations, this could result in many missing values, see notes.
8733         use_cftime : bool or None, optional
8734             Whether to use cftime objects in the output, only used if `calendar`
8735             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8736             If True, the new time axis uses cftime objects.
8737             If None (default), it uses :py:class:`numpy.datetime64` values if the
8738             date range permits it, and :py:class:`cftime.datetime` objects if not.
8739             If False, it uses :py:class:`numpy.datetime64`  or fails.
8740 
8741         Returns
8742         -------
8743         Dataset
8744             Copy of the dataarray with the time coordinate converted to the
8745             target calendar. If 'missing' was None (default), invalid dates in
8746             the new calendar are dropped, but missing dates are not inserted.
8747             If `missing` was given, the new data is reindexed to have a time axis
8748             with the same frequency as the source, but in the new calendar; any
8749             missing datapoints are filled with `missing`.
8750 
8751         Notes
8752         -----
8753         Passing a value to `missing` is only usable if the source's time coordinate as an
8754         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8755         if the target coordinate, generated from this frequency, has dates equivalent to the
8756         source. It is usually **not** appropriate to use this mode with:
8757 
8758         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8759         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8760             or 'mH' where 24 % m != 0).
8761 
8762         If one of the source or target calendars is `"360_day"`, `align_on` must
8763         be specified and two options are offered.
8764 
8765         - "year"
8766             The dates are translated according to their relative position in the year,
8767             ignoring their original month and day information, meaning that the
8768             missing/surplus days are added/removed at regular intervals.
8769 
8770             From a `360_day` to a standard calendar, the output will be missing the
8771             following dates (day of year in parentheses):
8772 
8773             To a leap year:
8774                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8775                 September 31st (275) and November 30th (335).
8776             To a non-leap year:
8777                 February 6th (36), April 19th (109), July 2nd (183),
8778                 September 12th (255), November 25th (329).
8779 
8780             From a standard calendar to a `"360_day"`, the following dates in the
8781             source array will be dropped:
8782 
8783             From a leap year:
8784                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8785                 September 31st (275), December 1st (336)
8786             From a non-leap year:
8787                 February 6th (37), April 20th (110), July 2nd (183),
8788                 September 13th (256), November 25th (329)
8789 
8790             This option is best used on daily and subdaily data.
8791 
8792         - "date"
8793             The month/day information is conserved and invalid dates are dropped
8794             from the output. This means that when converting from a `"360_day"` to a
8795             standard calendar, all 31st (Jan, March, May, July, August, October and
8796             December) will be missing as there is no equivalent dates in the
8797             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8798             will be dropped as there are no equivalent dates in a standard calendar.
8799 
8800             This option is best used with data on a frequency coarser than daily.
8801         """
8802         return convert_calendar(
8803             self,
8804             calendar,
8805             dim=dim,
8806             align_on=align_on,
8807             missing=missing,
8808             use_cftime=use_cftime,
8809         )
8810 
8811     def interp_calendar(
8812         self: T_Dataset,
8813         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8814         dim: Hashable = "time",
8815     ) -> T_Dataset:
8816         """Interpolates the Dataset to another calendar based on decimal year measure.
8817 
8818         Each timestamp in `source` and `target` are first converted to their decimal
8819         year equivalent then `source` is interpolated on the target coordinate.
8820         The decimal year of a timestamp is its year plus its sub-year component
8821         converted to the fraction of its year. For example "2000-03-01 12:00" is
8822         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8823 
8824         This method should only be used when the time (HH:MM:SS) information of
8825         time coordinate is not important.
8826 
8827         Parameters
8828         ----------
8829         target: DataArray or DatetimeIndex or CFTimeIndex
8830             The target time coordinate of a valid dtype
8831             (np.datetime64 or cftime objects)
8832         dim : Hashable, default: "time"
8833             The time coordinate name.
8834 
8835         Return
8836         ------
8837         DataArray
8838             The source interpolated on the decimal years of target,
8839         """
8840         return interp_calendar(self, target, dim=dim)
8841 
8842     def groupby(
8843         self,
8844         group: Hashable | DataArray | IndexVariable,
8845         squeeze: bool = True,
8846         restore_coord_dims: bool = False,
8847     ) -> DatasetGroupBy:
8848         """Returns a DatasetGroupBy object for performing grouped operations.
8849 
8850         Parameters
8851         ----------
8852         group : Hashable, DataArray or IndexVariable
8853             Array whose unique values should be used to group this array. If a
8854             string, must be the name of a variable contained in this dataset.
8855         squeeze : bool, default: True
8856             If "group" is a dimension of any arrays in this dataset, `squeeze`
8857             controls whether the subarrays have a dimension of length 1 along
8858             that dimension or if the dimension is squeezed out.
8859         restore_coord_dims : bool, default: False
8860             If True, also restore the dimension order of multi-dimensional
8861             coordinates.
8862 
8863         Returns
8864         -------
8865         grouped : DatasetGroupBy
8866             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8867             iterated over in the form of `(unique_value, grouped_array)` pairs.
8868 
8869         See Also
8870         --------
8871         Dataset.groupby_bins
8872         DataArray.groupby
8873         core.groupby.DatasetGroupBy
8874         pandas.DataFrame.groupby
8875         """
8876         from .groupby import DatasetGroupBy
8877 
8878         # While we don't generally check the type of every arg, passing
8879         # multiple dimensions as multiple arguments is common enough, and the
8880         # consequences hidden enough (strings evaluate as true) to warrant
8881         # checking here.
8882         # A future version could make squeeze kwarg only, but would face
8883         # backward-compat issues.
8884         if not isinstance(squeeze, bool):
8885             raise TypeError(
8886                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8887             )
8888 
8889         return DatasetGroupBy(
8890             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8891         )
8892 
8893     def groupby_bins(
8894         self,
8895         group: Hashable | DataArray | IndexVariable,
8896         bins: ArrayLike,
8897         right: bool = True,
8898         labels: ArrayLike | None = None,
8899         precision: int = 3,
8900         include_lowest: bool = False,
8901         squeeze: bool = True,
8902         restore_coord_dims: bool = False,
8903     ) -> DatasetGroupBy:
8904         """Returns a DatasetGroupBy object for performing grouped operations.
8905 
8906         Rather than using all unique values of `group`, the values are discretized
8907         first by applying `pandas.cut` [1]_ to `group`.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose binned values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         bins : int or array-like
8915             If bins is an int, it defines the number of equal-width bins in the
8916             range of x. However, in this case, the range of x is extended by .1%
8917             on each side to include the min or max values of x. If bins is a
8918             sequence it defines the bin edges allowing for non-uniform bin
8919             width. No extension of the range of x is done in this case.
8920         right : bool, default: True
8921             Indicates whether the bins include the rightmost edge or not. If
8922             right == True (the default), then the bins [1,2,3,4] indicate
8923             (1,2], (2,3], (3,4].
8924         labels : array-like or bool, default: None
8925             Used as labels for the resulting bins. Must be of the same length as
8926             the resulting bins. If False, string bin labels are assigned by
8927             `pandas.cut`.
8928         precision : int, default: 3
8929             The precision at which to store and display the bins labels.
8930         include_lowest : bool, default: False
8931             Whether the first interval should be left-inclusive or not.
8932         squeeze : bool, default: True
8933             If "group" is a dimension of any arrays in this dataset, `squeeze`
8934             controls whether the subarrays have a dimension of length 1 along
8935             that dimension or if the dimension is squeezed out.
8936         restore_coord_dims : bool, default: False
8937             If True, also restore the dimension order of multi-dimensional
8938             coordinates.
8939 
8940         Returns
8941         -------
8942         grouped : DatasetGroupBy
8943             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8944             iterated over in the form of `(unique_value, grouped_array)` pairs.
8945             The name of the group has the added suffix `_bins` in order to
8946             distinguish it from the original variable.
8947 
8948         See Also
8949         --------
8950         Dataset.groupby
8951         DataArray.groupby_bins
8952         core.groupby.DatasetGroupBy
8953         pandas.DataFrame.groupby
8954 
8955         References
8956         ----------
8957         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8958         """
8959         from .groupby import DatasetGroupBy
8960 
8961         return DatasetGroupBy(
8962             self,
8963             group,
8964             squeeze=squeeze,
8965             bins=bins,
8966             restore_coord_dims=restore_coord_dims,
8967             cut_kwargs={
8968                 "right": right,
8969                 "labels": labels,
8970                 "precision": precision,
8971                 "include_lowest": include_lowest,
8972             },
8973         )
8974 
8975     def weighted(self, weights: DataArray) -> DatasetWeighted:
8976         """
8977         Weighted Dataset operations.
8978 
8979         Parameters
8980         ----------
8981         weights : DataArray
8982             An array of weights associated with the values in this Dataset.
8983             Each value in the data contributes to the reduction operation
8984             according to its associated weight.
8985 
8986         Notes
8987         -----
8988         ``weights`` must be a DataArray and cannot contain missing values.
8989         Missing values can be replaced by ``weights.fillna(0)``.
8990 
8991         Returns
8992         -------
8993         core.weighted.DatasetWeighted
8994 
8995         See Also
8996         --------
8997         DataArray.weighted
8998         """
8999         from .weighted import DatasetWeighted
9000 
9001         return DatasetWeighted(self, weights)
9002 
9003     def rolling(
9004         self,
9005         dim: Mapping[Any, int] | None = None,
9006         min_periods: int | None = None,
9007         center: bool | Mapping[Any, bool] = False,
9008         **window_kwargs: int,
9009     ) -> DatasetRolling:
9010         """
9011         Rolling window object for Datasets.
9012 
9013         Parameters
9014         ----------
9015         dim : dict, optional
9016             Mapping from the dimension name to create the rolling iterator
9017             along (e.g. `time`) to its moving window size.
9018         min_periods : int or None, default: None
9019             Minimum number of observations in window required to have a value
9020             (otherwise result is NA). The default, None, is equivalent to
9021             setting min_periods equal to the size of the window.
9022         center : bool or Mapping to int, default: False
9023             Set the labels at the center of the window.
9024         **window_kwargs : optional
9025             The keyword arguments form of ``dim``.
9026             One of dim or window_kwargs must be provided.
9027 
9028         Returns
9029         -------
9030         core.rolling.DatasetRolling
9031 
9032         See Also
9033         --------
9034         core.rolling.DatasetRolling
9035         DataArray.rolling
9036         """
9037         from .rolling import DatasetRolling
9038 
9039         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9040         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9041 
9042     def coarsen(
9043         self,
9044         dim: Mapping[Any, int] | None = None,
9045         boundary: CoarsenBoundaryOptions = "exact",
9046         side: SideOptions | Mapping[Any, SideOptions] = "left",
9047         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9048         **window_kwargs: int,
9049     ) -> DatasetCoarsen:
9050         """
9051         Coarsen object for Datasets.
9052 
9053         Parameters
9054         ----------
9055         dim : mapping of hashable to int, optional
9056             Mapping from the dimension name to the window size.
9057         boundary : {"exact", "trim", "pad"}, default: "exact"
9058             If 'exact', a ValueError will be raised if dimension size is not a
9059             multiple of the window size. If 'trim', the excess entries are
9060             dropped. If 'pad', NA will be padded.
9061         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9062         coord_func : str or mapping of hashable to str, default: "mean"
9063             function (name) that is applied to the coordinates,
9064             or a mapping from coordinate name to function (name).
9065 
9066         Returns
9067         -------
9068         core.rolling.DatasetCoarsen
9069 
9070         See Also
9071         --------
9072         core.rolling.DatasetCoarsen
9073         DataArray.coarsen
9074         """
9075         from .rolling import DatasetCoarsen
9076 
9077         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9078         return DatasetCoarsen(
9079             self,
9080             dim,
9081             boundary=boundary,
9082             side=side,
9083             coord_func=coord_func,
9084         )
9085 
9086     def resample(
9087         self,
9088         indexer: Mapping[Any, str] | None = None,
9089         skipna: bool | None = None,
9090         closed: SideOptions | None = None,
9091         label: SideOptions | None = None,
9092         base: int = 0,
9093         keep_attrs: bool | None = None,
9094         loffset: datetime.timedelta | str | None = None,
9095         restore_coord_dims: bool | None = None,
9096         **indexer_kwargs: str,
9097     ) -> DatasetResample:
9098         """Returns a Resample object for performing resampling operations.
9099 
9100         Handles both downsampling and upsampling. The resampled
9101         dimension must be a datetime-like coordinate. If any intervals
9102         contain no values from the original object, they will be given
9103         the value ``NaN``.
9104 
9105         Parameters
9106         ----------
9107         indexer : Mapping of Hashable to str, optional
9108             Mapping from the dimension name to resample frequency [1]_. The
9109             dimension must be datetime-like.
9110         skipna : bool, optional
9111             Whether to skip missing values when aggregating in downsampling.
9112         closed : {"left", "right"}, optional
9113             Side of each interval to treat as closed.
9114         label : {"left", "right"}, optional
9115             Side of each interval to use for labeling.
9116         base : int, default = 0
9117             For frequencies that evenly subdivide 1 day, the "origin" of the
9118             aggregated intervals. For example, for "24H" frequency, base could
9119             range from 0 through 23.
9120         loffset : timedelta or str, optional
9121             Offset used to adjust the resampled time labels. Some pandas date
9122             offset strings are supported.
9123         restore_coord_dims : bool, optional
9124             If True, also restore the dimension order of multi-dimensional
9125             coordinates.
9126         **indexer_kwargs : str
9127             The keyword arguments form of ``indexer``.
9128             One of indexer or indexer_kwargs must be provided.
9129 
9130         Returns
9131         -------
9132         resampled : core.resample.DataArrayResample
9133             This object resampled.
9134 
9135         See Also
9136         --------
9137         DataArray.resample
9138         pandas.Series.resample
9139         pandas.DataFrame.resample
9140 
9141         References
9142         ----------
9143         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9144         """
9145         from .resample import DatasetResample
9146 
9147         return self._resample(
9148             resample_cls=DatasetResample,
9149             indexer=indexer,
9150             skipna=skipna,
9151             closed=closed,
9152             label=label,
9153             base=base,
9154             keep_attrs=keep_attrs,
9155             loffset=loffset,
9156             restore_coord_dims=restore_coord_dims,
9157             **indexer_kwargs,
9158         )

</code><intended_behavior>For both 'set_index' and 'reset_index', there should be clear, additional logic to verify and update '_coord_names' appropriately post-operation. It involves adding or removing coordinate names based on how indices are restructured. If a coordinate is promoted to an index or vice versa, '_coord_names' should be updated accordingly to maintain accuracy.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>_stack_once</method>
<code>
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )

</code><intended_behavior>Given that stacking can significantly alter the dataset's structure by combining multiple coordinates into a single new one, this method should include a final step where '_coord_names' is revisited. Any coordinates that have been combined into a new multiindex should have their names removed from '_coord_names', and the new coordinate name should be added. This would prevent the inconsistency between the physical structure of the dataset and its coordinate metadata, ensuring operations relying on '_coord_names' (like calculating dataset length) remain accurate.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the indexs type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the indexs type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def _rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Also used internally by DataArray so that the warning (if any)
3569         is raised at the right stack level.
3570         """
3571         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3572         for k in name_dict.keys():
3573             if k not in self and k not in self.dims:
3574                 raise ValueError(
3575                     f"cannot rename {k!r} because it is not a "
3576                     "variable or dimension in this dataset"
3577                 )
3578 
3579             create_dim_coord = False
3580             new_k = name_dict[k]
3581 
3582             if k in self.dims and new_k in self._coord_names:
3583                 coord_dims = self._variables[name_dict[k]].dims
3584                 if coord_dims == (k,):
3585                     create_dim_coord = True
3586             elif k in self._coord_names and new_k in self.dims:
3587                 coord_dims = self._variables[k].dims
3588                 if coord_dims == (new_k,):
3589                     create_dim_coord = True
3590 
3591             if create_dim_coord:
3592                 warnings.warn(
3593                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3594                     "anymore. Try using swap_dims instead or use set_index "
3595                     "after rename to create an indexed coordinate.",
3596                     UserWarning,
3597                     stacklevel=3,
3598                 )
3599 
3600         variables, coord_names, dims, indexes = self._rename_all(
3601             name_dict=name_dict, dims_dict=name_dict
3602         )
3603         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3604 
3605     def rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Returns a new object with renamed variables, coordinates and dimensions.
3611 
3612         Parameters
3613         ----------
3614         name_dict : dict-like, optional
3615             Dictionary whose keys are current variable, coordinate or dimension names and
3616             whose values are the desired names.
3617         **names : optional
3618             Keyword form of ``name_dict``.
3619             One of name_dict or names must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed variables, coordinates and dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename_vars
3630         Dataset.rename_dims
3631         DataArray.rename
3632         """
3633         return self._rename(name_dict=name_dict, **names)
3634 
3635     def rename_dims(
3636         self: T_Dataset,
3637         dims_dict: Mapping[Any, Hashable] | None = None,
3638         **dims: Hashable,
3639     ) -> T_Dataset:
3640         """Returns a new object with renamed dimensions only.
3641 
3642         Parameters
3643         ----------
3644         dims_dict : dict-like, optional
3645             Dictionary whose keys are current dimension names and
3646             whose values are the desired names. The desired names must
3647             not be the name of an existing dimension or Variable in the Dataset.
3648         **dims : optional
3649             Keyword form of ``dims_dict``.
3650             One of dims_dict or dims must be provided.
3651 
3652         Returns
3653         -------
3654         renamed : Dataset
3655             Dataset with renamed dimensions.
3656 
3657         See Also
3658         --------
3659         Dataset.swap_dims
3660         Dataset.rename
3661         Dataset.rename_vars
3662         DataArray.rename
3663         """
3664         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3665         for k, v in dims_dict.items():
3666             if k not in self.dims:
3667                 raise ValueError(
3668                     f"cannot rename {k!r} because it is not a "
3669                     "dimension in this dataset"
3670                 )
3671             if v in self.dims or v in self:
3672                 raise ValueError(
3673                     f"Cannot rename {k} to {v} because {v} already exists. "
3674                     "Try using swap_dims instead."
3675                 )
3676 
3677         variables, coord_names, sizes, indexes = self._rename_all(
3678             name_dict={}, dims_dict=dims_dict
3679         )
3680         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3681 
3682     def rename_vars(
3683         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3684     ) -> T_Dataset:
3685         """Returns a new object with renamed variables including coordinates
3686 
3687         Parameters
3688         ----------
3689         name_dict : dict-like, optional
3690             Dictionary whose keys are current variable or coordinate names and
3691             whose values are the desired names.
3692         **names : optional
3693             Keyword form of ``name_dict``.
3694             One of name_dict or names must be provided.
3695 
3696         Returns
3697         -------
3698         renamed : Dataset
3699             Dataset with renamed variables including coordinates
3700 
3701         See Also
3702         --------
3703         Dataset.swap_dims
3704         Dataset.rename
3705         Dataset.rename_dims
3706         DataArray.rename
3707         """
3708         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3709         for k in name_dict:
3710             if k not in self:
3711                 raise ValueError(
3712                     f"cannot rename {k!r} because it is not a "
3713                     "variable or coordinate in this dataset"
3714                 )
3715         variables, coord_names, dims, indexes = self._rename_all(
3716             name_dict=name_dict, dims_dict={}
3717         )
3718         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3719 
3720     def swap_dims(
3721         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3722     ) -> T_Dataset:
3723         """Returns a new object with swapped dimensions.
3724 
3725         Parameters
3726         ----------
3727         dims_dict : dict-like
3728             Dictionary whose keys are current dimension names and whose values
3729             are new names.
3730         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3731             The keyword arguments form of ``dims_dict``.
3732             One of dims_dict or dims_kwargs must be provided.
3733 
3734         Returns
3735         -------
3736         swapped : Dataset
3737             Dataset with swapped dimensions.
3738 
3739         Examples
3740         --------
3741         >>> ds = xr.Dataset(
3742         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3743         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3744         ... )
3745         >>> ds
3746         <xarray.Dataset>
3747         Dimensions:  (x: 2)
3748         Coordinates:
3749           * x        (x) <U1 'a' 'b'
3750             y        (x) int64 0 1
3751         Data variables:
3752             a        (x) int64 5 7
3753             b        (x) float64 0.1 2.4
3754 
3755         >>> ds.swap_dims({"x": "y"})
3756         <xarray.Dataset>
3757         Dimensions:  (y: 2)
3758         Coordinates:
3759             x        (y) <U1 'a' 'b'
3760           * y        (y) int64 0 1
3761         Data variables:
3762             a        (y) int64 5 7
3763             b        (y) float64 0.1 2.4
3764 
3765         >>> ds.swap_dims({"x": "z"})
3766         <xarray.Dataset>
3767         Dimensions:  (z: 2)
3768         Coordinates:
3769             x        (z) <U1 'a' 'b'
3770             y        (z) int64 0 1
3771         Dimensions without coordinates: z
3772         Data variables:
3773             a        (z) int64 5 7
3774             b        (z) float64 0.1 2.4
3775 
3776         See Also
3777         --------
3778         Dataset.rename
3779         DataArray.swap_dims
3780         """
3781         # TODO: deprecate this method in favor of a (less confusing)
3782         # rename_dims() method that only renames dimensions.
3783 
3784         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3785         for k, v in dims_dict.items():
3786             if k not in self.dims:
3787                 raise ValueError(
3788                     f"cannot swap from dimension {k!r} because it is "
3789                     "not an existing dimension"
3790                 )
3791             if v in self.variables and self.variables[v].dims != (k,):
3792                 raise ValueError(
3793                     f"replacement dimension {v!r} is not a 1D "
3794                     f"variable along the old dimension {k!r}"
3795                 )
3796 
3797         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3798 
3799         coord_names = self._coord_names.copy()
3800         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3801 
3802         variables: dict[Hashable, Variable] = {}
3803         indexes: dict[Hashable, Index] = {}
3804         for k, v in self.variables.items():
3805             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3806             var: Variable
3807             if k in result_dims:
3808                 var = v.to_index_variable()
3809                 var.dims = dims
3810                 if k in self._indexes:
3811                     indexes[k] = self._indexes[k]
3812                     variables[k] = var
3813                 else:
3814                     index, index_vars = create_default_index_implicit(var)
3815                     indexes.update({name: index for name in index_vars})
3816                     variables.update(index_vars)
3817                     coord_names.update(index_vars)
3818             else:
3819                 var = v.to_base_variable()
3820                 var.dims = dims
3821                 variables[k] = var
3822 
3823         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3824 
3825     # change type of self and return to T_Dataset once
3826     # https://github.com/python/mypy/issues/12846 is resolved
3827     def expand_dims(
3828         self,
3829         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3830         axis: None | int | Sequence[int] = None,
3831         **dim_kwargs: Any,
3832     ) -> Dataset:
3833         """Return a new object with an additional axis (or axes) inserted at
3834         the corresponding position in the array shape.  The new object is a
3835         view into the underlying array, not a copy.
3836 
3837         If dim is already a scalar coordinate, it will be promoted to a 1D
3838         coordinate consisting of a single value.
3839 
3840         Parameters
3841         ----------
3842         dim : hashable, sequence of hashable, mapping, or None
3843             Dimensions to include on the new variable. If provided as hashable
3844             or sequence of hashable, then dimensions are inserted with length
3845             1. If provided as a mapping, then the keys are the new dimensions
3846             and the values are either integers (giving the length of the new
3847             dimensions) or array-like (giving the coordinates of the new
3848             dimensions).
3849         axis : int, sequence of int, or None, default: None
3850             Axis position(s) where new axis is to be inserted (position(s) on
3851             the result array). If a sequence of integers is passed,
3852             multiple axes are inserted. In this case, dim arguments should be
3853             same length list. If axis=None is passed, all the axes will be
3854             inserted to the start of the result array.
3855         **dim_kwargs : int or sequence or ndarray
3856             The keywords are arbitrary dimensions being inserted and the values
3857             are either the lengths of the new dims (if int is given), or their
3858             coordinates. Note, this is an alternative to passing a dict to the
3859             dim kwarg and will only be used if dim is None.
3860 
3861         Returns
3862         -------
3863         expanded : Dataset
3864             This object, but with additional dimension(s).
3865 
3866         See Also
3867         --------
3868         DataArray.expand_dims
3869         """
3870         if dim is None:
3871             pass
3872         elif isinstance(dim, Mapping):
3873             # We're later going to modify dim in place; don't tamper with
3874             # the input
3875             dim = dict(dim)
3876         elif isinstance(dim, int):
3877             raise TypeError(
3878                 "dim should be hashable or sequence of hashables or mapping"
3879             )
3880         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3881             dim = {dim: 1}
3882         elif isinstance(dim, Sequence):
3883             if len(dim) != len(set(dim)):
3884                 raise ValueError("dims should not contain duplicate values.")
3885             dim = {d: 1 for d in dim}
3886 
3887         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3888         assert isinstance(dim, MutableMapping)
3889 
3890         if axis is None:
3891             axis = list(range(len(dim)))
3892         elif not isinstance(axis, Sequence):
3893             axis = [axis]
3894 
3895         if len(dim) != len(axis):
3896             raise ValueError("lengths of dim and axis should be identical.")
3897         for d in dim:
3898             if d in self.dims:
3899                 raise ValueError(f"Dimension {d} already exists.")
3900             if d in self._variables and not utils.is_scalar(self._variables[d]):
3901                 raise ValueError(
3902                     "{dim} already exists as coordinate or"
3903                     " variable name.".format(dim=d)
3904                 )
3905 
3906         variables: dict[Hashable, Variable] = {}
3907         indexes: dict[Hashable, Index] = dict(self._indexes)
3908         coord_names = self._coord_names.copy()
3909         # If dim is a dict, then ensure that the values are either integers
3910         # or iterables.
3911         for k, v in dim.items():
3912             if hasattr(v, "__iter__"):
3913                 # If the value for the new dimension is an iterable, then
3914                 # save the coordinates to the variables dict, and set the
3915                 # value within the dim dict to the length of the iterable
3916                 # for later use.
3917                 index = PandasIndex(v, k)
3918                 indexes[k] = index
3919                 variables.update(index.create_variables())
3920                 coord_names.add(k)
3921                 dim[k] = variables[k].size
3922             elif isinstance(v, int):
3923                 pass  # Do nothing if the dimensions value is just an int
3924             else:
3925                 raise TypeError(
3926                     "The value of new dimension {k} must be "
3927                     "an iterable or an int".format(k=k)
3928                 )
3929 
3930         for k, v in self._variables.items():
3931             if k not in dim:
3932                 if k in coord_names:  # Do not change coordinates
3933                     variables[k] = v
3934                 else:
3935                     result_ndim = len(v.dims) + len(axis)
3936                     for a in axis:
3937                         if a < -result_ndim or result_ndim - 1 < a:
3938                             raise IndexError(
3939                                 f"Axis {a} of variable {k} is out of bounds of the "
3940                                 f"expanded dimension size {result_ndim}"
3941                             )
3942 
3943                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3944                     if len(axis_pos) != len(set(axis_pos)):
3945                         raise ValueError("axis should not contain duplicate values")
3946                     # We need to sort them to make sure `axis` equals to the
3947                     # axis positions of the result array.
3948                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3949 
3950                     all_dims = list(zip(v.dims, v.shape))
3951                     for d, c in zip_axis_dim:
3952                         all_dims.insert(d, c)
3953                     variables[k] = v.set_dims(dict(all_dims))
3954             else:
3955                 if k not in variables:
3956                     # If dims includes a label of a non-dimension coordinate,
3957                     # it will be promoted to a 1D coordinate with a single value.
3958                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3959                     indexes[k] = index
3960                     variables.update(index_vars)
3961 
3962         return self._replace_with_new_dims(
3963             variables, coord_names=coord_names, indexes=indexes
3964         )
3965 
3966     # change type of self and return to T_Dataset once
3967     # https://github.com/python/mypy/issues/12846 is resolved
3968     def set_index(
3969         self,
3970         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3971         append: bool = False,
3972         **indexes_kwargs: Hashable | Sequence[Hashable],
3973     ) -> Dataset:
3974         """Set Dataset (multi-)indexes using one or more existing coordinates
3975         or variables.
3976 
3977         This legacy method is limited to pandas (multi-)indexes and
3978         1-dimensional "dimension" coordinates. See
3979         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
3980         Xarray-compatible index from one or more arbitrary coordinates.
3981 
3982         Parameters
3983         ----------
3984         indexes : {dim: index, ...}
3985             Mapping from names matching dimensions and values given
3986             by (lists of) the names of existing coordinates or variables to set
3987             as new (multi-)index.
3988         append : bool, default: False
3989             If True, append the supplied index(es) to the existing index(es).
3990             Otherwise replace the existing index(es) (default).
3991         **indexes_kwargs : optional
3992             The keyword arguments form of ``indexes``.
3993             One of indexes or indexes_kwargs must be provided.
3994 
3995         Returns
3996         -------
3997         obj : Dataset
3998             Another dataset, with this dataset's data but replaced coordinates.
3999 
4000         Examples
4001         --------
4002         >>> arr = xr.DataArray(
4003         ...     data=np.ones((2, 3)),
4004         ...     dims=["x", "y"],
4005         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4006         ... )
4007         >>> ds = xr.Dataset({"v": arr})
4008         >>> ds
4009         <xarray.Dataset>
4010         Dimensions:  (x: 2, y: 3)
4011         Coordinates:
4012           * x        (x) int64 0 1
4013           * y        (y) int64 0 1 2
4014             a        (x) int64 3 4
4015         Data variables:
4016             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4017         >>> ds.set_index(x="a")
4018         <xarray.Dataset>
4019         Dimensions:  (x: 2, y: 3)
4020         Coordinates:
4021           * x        (x) int64 3 4
4022           * y        (y) int64 0 1 2
4023         Data variables:
4024             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4025 
4026         See Also
4027         --------
4028         Dataset.reset_index
4029         Dataset.set_xindex
4030         Dataset.swap_dims
4031         """
4032         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4033 
4034         new_indexes: dict[Hashable, Index] = {}
4035         new_variables: dict[Hashable, Variable] = {}
4036         drop_indexes: set[Hashable] = set()
4037         drop_variables: set[Hashable] = set()
4038         replace_dims: dict[Hashable, Hashable] = {}
4039         all_var_names: set[Hashable] = set()
4040 
4041         for dim, _var_names in dim_coords.items():
4042             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4043                 var_names = [_var_names]
4044             else:
4045                 var_names = list(_var_names)
4046 
4047             invalid_vars = set(var_names) - set(self._variables)
4048             if invalid_vars:
4049                 raise ValueError(
4050                     ", ".join([str(v) for v in invalid_vars])
4051                     + " variable(s) do not exist"
4052                 )
4053 
4054             all_var_names.update(var_names)
4055             drop_variables.update(var_names)
4056 
4057             # drop any pre-existing index involved and its corresponding coordinates
4058             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4059             all_index_coord_names = set(index_coord_names)
4060             for k in var_names:
4061                 all_index_coord_names.update(
4062                     self.xindexes.get_all_coords(k, errors="ignore")
4063                 )
4064 
4065             drop_indexes.update(all_index_coord_names)
4066             drop_variables.update(all_index_coord_names)
4067 
4068             if len(var_names) == 1 and (not append or dim not in self._indexes):
4069                 var_name = var_names[0]
4070                 var = self._variables[var_name]
4071                 if var.dims != (dim,):
4072                     raise ValueError(
4073                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4074                         f"variable {var_name!r} that has dimensions {var.dims}"
4075                     )
4076                 idx = PandasIndex.from_variables({dim: var}, options={})
4077                 idx_vars = idx.create_variables({var_name: var})
4078 
4079                 # trick to preserve coordinate order in this case
4080                 if dim in self._coord_names:
4081                     drop_variables.remove(dim)
4082             else:
4083                 if append:
4084                     current_variables = {
4085                         k: self._variables[k] for k in index_coord_names
4086                     }
4087                 else:
4088                     current_variables = {}
4089                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4090                     dim,
4091                     current_variables,
4092                     {k: self._variables[k] for k in var_names},
4093                 )
4094                 for n in idx.index.names:
4095                     replace_dims[n] = dim
4096 
4097             new_indexes.update({k: idx for k in idx_vars})
4098             new_variables.update(idx_vars)
4099 
4100         # re-add deindexed coordinates (convert to base variables)
4101         for k in drop_variables:
4102             if (
4103                 k not in new_variables
4104                 and k not in all_var_names
4105                 and k in self._coord_names
4106             ):
4107                 new_variables[k] = self._variables[k].to_base_variable()
4108 
4109         indexes_: dict[Any, Index] = {
4110             k: v for k, v in self._indexes.items() if k not in drop_indexes
4111         }
4112         indexes_.update(new_indexes)
4113 
4114         variables = {
4115             k: v for k, v in self._variables.items() if k not in drop_variables
4116         }
4117         variables.update(new_variables)
4118 
4119         # update dimensions if necessary, GH: 3512
4120         for k, v in variables.items():
4121             if any(d in replace_dims for d in v.dims):
4122                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4123                 variables[k] = v._replace(dims=new_dims)
4124 
4125         coord_names = self._coord_names - drop_variables | set(new_variables)
4126 
4127         return self._replace_with_new_dims(
4128             variables, coord_names=coord_names, indexes=indexes_
4129         )
4130 
4131     def reset_index(
4132         self: T_Dataset,
4133         dims_or_levels: Hashable | Sequence[Hashable],
4134         drop: bool = False,
4135     ) -> T_Dataset:
4136         """Reset the specified index(es) or multi-index level(s).
4137 
4138         This legacy method is specific to pandas (multi-)indexes and
4139         1-dimensional "dimension" coordinates. See the more generic
4140         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4141         method to respectively drop and set pandas or custom indexes for
4142         arbitrary coordinates.
4143 
4144         Parameters
4145         ----------
4146         dims_or_levels : Hashable or Sequence of Hashable
4147             Name(s) of the dimension(s) and/or multi-index level(s) that will
4148             be reset.
4149         drop : bool, default: False
4150             If True, remove the specified indexes and/or multi-index levels
4151             instead of extracting them as new coordinates (default: False).
4152 
4153         Returns
4154         -------
4155         obj : Dataset
4156             Another dataset, with this dataset's data but replaced coordinates.
4157 
4158         See Also
4159         --------
4160         Dataset.set_index
4161         Dataset.set_xindex
4162         Dataset.drop_indexes
4163         """
4164         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4165             dims_or_levels = [dims_or_levels]
4166 
4167         invalid_coords = set(dims_or_levels) - set(self._indexes)
4168         if invalid_coords:
4169             raise ValueError(
4170                 f"{tuple(invalid_coords)} are not coordinates with an index"
4171             )
4172 
4173         drop_indexes: set[Hashable] = set()
4174         drop_variables: set[Hashable] = set()
4175         seen: set[Index] = set()
4176         new_indexes: dict[Hashable, Index] = {}
4177         new_variables: dict[Hashable, Variable] = {}
4178 
4179         def drop_or_convert(var_names):
4180             if drop:
4181                 drop_variables.update(var_names)
4182             else:
4183                 base_vars = {
4184                     k: self._variables[k].to_base_variable() for k in var_names
4185                 }
4186                 new_variables.update(base_vars)
4187 
4188         for name in dims_or_levels:
4189             index = self._indexes[name]
4190 
4191             if index in seen:
4192                 continue
4193             seen.add(index)
4194 
4195             idx_var_names = set(self.xindexes.get_all_coords(name))
4196             drop_indexes.update(idx_var_names)
4197 
4198             if isinstance(index, PandasMultiIndex):
4199                 # special case for pd.MultiIndex
4200                 level_names = index.index.names
4201                 keep_level_vars = {
4202                     k: self._variables[k]
4203                     for k in level_names
4204                     if k not in dims_or_levels
4205                 }
4206 
4207                 if index.dim not in dims_or_levels and keep_level_vars:
4208                     # do not drop the multi-index completely
4209                     # instead replace it by a new (multi-)index with dropped level(s)
4210                     idx = index.keep_levels(keep_level_vars)
4211                     idx_vars = idx.create_variables(keep_level_vars)
4212                     new_indexes.update({k: idx for k in idx_vars})
4213                     new_variables.update(idx_vars)
4214                     if not isinstance(idx, PandasMultiIndex):
4215                         # multi-index reduced to single index
4216                         # backward compatibility: unique level coordinate renamed to dimension
4217                         drop_variables.update(keep_level_vars)
4218                     drop_or_convert(
4219                         [k for k in level_names if k not in keep_level_vars]
4220                     )
4221                 else:
4222                     # always drop the multi-index dimension variable
4223                     drop_variables.add(index.dim)
4224                     drop_or_convert(level_names)
4225             else:
4226                 drop_or_convert(idx_var_names)
4227 
4228         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4229         indexes.update(new_indexes)
4230 
4231         variables = {
4232             k: v for k, v in self._variables.items() if k not in drop_variables
4233         }
4234         variables.update(new_variables)
4235 
4236         coord_names = self._coord_names - drop_variables
4237 
4238         return self._replace_with_new_dims(
4239             variables, coord_names=coord_names, indexes=indexes
4240         )
4241 
4242     def set_xindex(
4243         self: T_Dataset,
4244         coord_names: str | Sequence[Hashable],
4245         index_cls: type[Index] | None = None,
4246         **options,
4247     ) -> T_Dataset:
4248         """Set a new, Xarray-compatible index from one or more existing
4249         coordinate(s).
4250 
4251         Parameters
4252         ----------
4253         coord_names : str or list
4254             Name(s) of the coordinate(s) used to build the index.
4255             If several names are given, their order matters.
4256         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4257             The type of index to create. By default, try setting
4258             a ``PandasIndex`` if ``len(coord_names) == 1``,
4259             otherwise a ``PandasMultiIndex``.
4260         **options
4261             Options passed to the index constructor.
4262 
4263         Returns
4264         -------
4265         obj : Dataset
4266             Another dataset, with this dataset's data and with a new index.
4267 
4268         """
4269         # the Sequence check is required for mypy
4270         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4271             coord_names = [coord_names]
4272 
4273         if index_cls is None:
4274             if len(coord_names) == 1:
4275                 index_cls = PandasIndex
4276             else:
4277                 index_cls = PandasMultiIndex
4278         else:
4279             if not issubclass(index_cls, Index):
4280                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4281 
4282         invalid_coords = set(coord_names) - self._coord_names
4283 
4284         if invalid_coords:
4285             msg = ["invalid coordinate(s)"]
4286             no_vars = invalid_coords - set(self._variables)
4287             data_vars = invalid_coords - no_vars
4288             if no_vars:
4289                 msg.append(f"those variables don't exist: {no_vars}")
4290             if data_vars:
4291                 msg.append(
4292                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4293                 )
4294             raise ValueError("\n".join(msg))
4295 
4296         # we could be more clever here (e.g., drop-in index replacement if index
4297         # coordinates do not conflict), but let's not allow this for now
4298         indexed_coords = set(coord_names) & set(self._indexes)
4299 
4300         if indexed_coords:
4301             raise ValueError(
4302                 f"those coordinates already have an index: {indexed_coords}"
4303             )
4304 
4305         coord_vars = {name: self._variables[name] for name in coord_names}
4306 
4307         index = index_cls.from_variables(coord_vars, options=options)
4308 
4309         new_coord_vars = index.create_variables(coord_vars)
4310 
4311         # special case for setting a pandas multi-index from level coordinates
4312         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4313         # elements) coordinate
4314         if isinstance(index, PandasMultiIndex):
4315             coord_names = [index.dim] + list(coord_names)
4316 
4317         variables: dict[Hashable, Variable]
4318         indexes: dict[Hashable, Index]
4319 
4320         if len(coord_names) == 1:
4321             variables = self._variables.copy()
4322             indexes = self._indexes.copy()
4323 
4324             name = list(coord_names).pop()
4325             if name in new_coord_vars:
4326                 variables[name] = new_coord_vars[name]
4327             indexes[name] = index
4328         else:
4329             # reorder variables and indexes so that coordinates having the same
4330             # index are next to each other
4331             variables = {}
4332             for name, var in self._variables.items():
4333                 if name not in coord_names:
4334                     variables[name] = var
4335 
4336             indexes = {}
4337             for name, idx in self._indexes.items():
4338                 if name not in coord_names:
4339                     indexes[name] = idx
4340 
4341             for name in coord_names:
4342                 try:
4343                     variables[name] = new_coord_vars[name]
4344                 except KeyError:
4345                     variables[name] = self._variables[name]
4346                 indexes[name] = index
4347 
4348         return self._replace(
4349             variables=variables,
4350             coord_names=self._coord_names | set(coord_names),
4351             indexes=indexes,
4352         )
4353 
4354     def reorder_levels(
4355         self: T_Dataset,
4356         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4357         **dim_order_kwargs: Sequence[int | Hashable],
4358     ) -> T_Dataset:
4359         """Rearrange index levels using input order.
4360 
4361         Parameters
4362         ----------
4363         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4364             Mapping from names matching dimensions and values given
4365             by lists representing new level orders. Every given dimension
4366             must have a multi-index.
4367         **dim_order_kwargs : Sequence of int or Hashable, optional
4368             The keyword arguments form of ``dim_order``.
4369             One of dim_order or dim_order_kwargs must be provided.
4370 
4371         Returns
4372         -------
4373         obj : Dataset
4374             Another dataset, with this dataset's data but replaced
4375             coordinates.
4376         """
4377         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4378         variables = self._variables.copy()
4379         indexes = dict(self._indexes)
4380         new_indexes: dict[Hashable, Index] = {}
4381         new_variables: dict[Hashable, IndexVariable] = {}
4382 
4383         for dim, order in dim_order.items():
4384             index = self._indexes[dim]
4385 
4386             if not isinstance(index, PandasMultiIndex):
4387                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4388 
4389             level_vars = {k: self._variables[k] for k in order}
4390             idx = index.reorder_levels(level_vars)
4391             idx_vars = idx.create_variables(level_vars)
4392             new_indexes.update({k: idx for k in idx_vars})
4393             new_variables.update(idx_vars)
4394 
4395         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4396         indexes.update(new_indexes)
4397 
4398         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4399         variables.update(new_variables)
4400 
4401         return self._replace(variables, indexes=indexes)
4402 
4403     def _get_stack_index(
4404         self,
4405         dim,
4406         multi=False,
4407         create_index=False,
4408     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4409         """Used by stack and unstack to get one pandas (multi-)index among
4410         the indexed coordinates along dimension `dim`.
4411 
4412         If exactly one index is found, return it with its corresponding
4413         coordinate variables(s), otherwise return None and an empty dict.
4414 
4415         If `create_index=True`, create a new index if none is found or raise
4416         an error if multiple indexes are found.
4417 
4418         """
4419         stack_index: Index | None = None
4420         stack_coords: dict[Hashable, Variable] = {}
4421 
4422         for name, index in self._indexes.items():
4423             var = self._variables[name]
4424             if (
4425                 var.ndim == 1
4426                 and var.dims[0] == dim
4427                 and (
4428                     # stack: must be a single coordinate index
4429                     not multi
4430                     and not self.xindexes.is_multi(name)
4431                     # unstack: must be an index that implements .unstack
4432                     or multi
4433                     and type(index).unstack is not Index.unstack
4434                 )
4435             ):
4436                 if stack_index is not None and index is not stack_index:
4437                     # more than one index found, stop
4438                     if create_index:
4439                         raise ValueError(
4440                             f"cannot stack dimension {dim!r} with `create_index=True` "
4441                             "and with more than one index found along that dimension"
4442                         )
4443                     return None, {}
4444                 stack_index = index
4445                 stack_coords[name] = var
4446 
4447         if create_index and stack_index is None:
4448             if dim in self._variables:
4449                 var = self._variables[dim]
4450             else:
4451                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4452             # dummy index (only `stack_coords` will be used to construct the multi-index)
4453             stack_index = PandasIndex([0], dim)
4454             stack_coords = {dim: var}
4455 
4456         return stack_index, stack_coords
4457 
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )
4516 
4517     def stack(
4518         self: T_Dataset,
4519         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4520         create_index: bool | None = True,
4521         index_cls: type[Index] = PandasMultiIndex,
4522         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4523     ) -> T_Dataset:
4524         """
4525         Stack any number of existing dimensions into a single new dimension.
4526 
4527         New dimensions will be added at the end, and by default the corresponding
4528         coordinate variables will be combined into a MultiIndex.
4529 
4530         Parameters
4531         ----------
4532         dimensions : mapping of hashable to sequence of hashable
4533             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4534             dimensions, and the existing dimensions that they replace. An
4535             ellipsis (`...`) will be replaced by all unlisted dimensions.
4536             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4537             all dimensions.
4538         create_index : bool or None, default: True
4539 
4540             - True: create a multi-index for each of the stacked dimensions.
4541             - False: don't create any index.
4542             - None. create a multi-index only if exactly one single (1-d) coordinate
4543               index is found for every dimension to stack.
4544 
4545         index_cls: Index-class, default: PandasMultiIndex
4546             Can be used to pass a custom multi-index type (must be an Xarray index that
4547             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4548         **dimensions_kwargs
4549             The keyword arguments form of ``dimensions``.
4550             One of dimensions or dimensions_kwargs must be provided.
4551 
4552         Returns
4553         -------
4554         stacked : Dataset
4555             Dataset with stacked data.
4556 
4557         See Also
4558         --------
4559         Dataset.unstack
4560         """
4561         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4562         result = self
4563         for new_dim, dims in dimensions.items():
4564             result = result._stack_once(dims, new_dim, index_cls, create_index)
4565         return result
4566 
4567     def to_stacked_array(
4568         self,
4569         new_dim: Hashable,
4570         sample_dims: Collection[Hashable],
4571         variable_dim: Hashable = "variable",
4572         name: Hashable | None = None,
4573     ) -> DataArray:
4574         """Combine variables of differing dimensionality into a DataArray
4575         without broadcasting.
4576 
4577         This method is similar to Dataset.to_array but does not broadcast the
4578         variables.
4579 
4580         Parameters
4581         ----------
4582         new_dim : hashable
4583             Name of the new stacked coordinate
4584         sample_dims : Collection of hashables
4585             List of dimensions that **will not** be stacked. Each array in the
4586             dataset must share these dimensions. For machine learning
4587             applications, these define the dimensions over which samples are
4588             drawn.
4589         variable_dim : hashable, default: "variable"
4590             Name of the level in the stacked coordinate which corresponds to
4591             the variables.
4592         name : hashable, optional
4593             Name of the new data array.
4594 
4595         Returns
4596         -------
4597         stacked : DataArray
4598             DataArray with the specified dimensions and data variables
4599             stacked together. The stacked coordinate is named ``new_dim``
4600             and represented by a MultiIndex object with a level containing the
4601             data variable names. The name of this level is controlled using
4602             the ``variable_dim`` argument.
4603 
4604         See Also
4605         --------
4606         Dataset.to_array
4607         Dataset.stack
4608         DataArray.to_unstacked_dataset
4609 
4610         Examples
4611         --------
4612         >>> data = xr.Dataset(
4613         ...     data_vars={
4614         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4615         ...         "b": ("x", [6, 7]),
4616         ...     },
4617         ...     coords={"y": ["u", "v", "w"]},
4618         ... )
4619 
4620         >>> data
4621         <xarray.Dataset>
4622         Dimensions:  (x: 2, y: 3)
4623         Coordinates:
4624           * y        (y) <U1 'u' 'v' 'w'
4625         Dimensions without coordinates: x
4626         Data variables:
4627             a        (x, y) int64 0 1 2 3 4 5
4628             b        (x) int64 6 7
4629 
4630         >>> data.to_stacked_array("z", sample_dims=["x"])
4631         <xarray.DataArray 'a' (x: 2, z: 4)>
4632         array([[0, 1, 2, 6],
4633                [3, 4, 5, 7]])
4634         Coordinates:
4635           * z         (z) object MultiIndex
4636           * variable  (z) object 'a' 'a' 'a' 'b'
4637           * y         (z) object 'u' 'v' 'w' nan
4638         Dimensions without coordinates: x
4639 
4640         """
4641         from .concat import concat
4642 
4643         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4644 
4645         for variable in self:
4646             dims = self[variable].dims
4647             dims_include_sample_dims = set(sample_dims) <= set(dims)
4648             if not dims_include_sample_dims:
4649                 raise ValueError(
4650                     "All variables in the dataset must contain the "
4651                     "dimensions {}.".format(dims)
4652                 )
4653 
4654         def ensure_stackable(val):
4655             assign_coords = {variable_dim: val.name}
4656             for dim in stacking_dims:
4657                 if dim not in val.dims:
4658                     assign_coords[dim] = None
4659 
4660             expand_dims = set(stacking_dims).difference(set(val.dims))
4661             expand_dims.add(variable_dim)
4662             # must be list for .expand_dims
4663             expand_dims = list(expand_dims)
4664 
4665             return (
4666                 val.assign_coords(**assign_coords)
4667                 .expand_dims(expand_dims)
4668                 .stack({new_dim: (variable_dim,) + stacking_dims})
4669             )
4670 
4671         # concatenate the arrays
4672         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4673         data_array = concat(stackable_vars, dim=new_dim)
4674 
4675         if name is not None:
4676             data_array.name = name
4677 
4678         return data_array
4679 
4680     def _unstack_once(
4681         self: T_Dataset,
4682         dim: Hashable,
4683         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4684         fill_value,
4685         sparse: bool = False,
4686     ) -> T_Dataset:
4687         index, index_vars = index_and_vars
4688         variables: dict[Hashable, Variable] = {}
4689         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4690 
4691         new_indexes, clean_index = index.unstack()
4692         indexes.update(new_indexes)
4693 
4694         for name, idx in new_indexes.items():
4695             variables.update(idx.create_variables(index_vars))
4696 
4697         for name, var in self.variables.items():
4698             if name not in index_vars:
4699                 if dim in var.dims:
4700                     if isinstance(fill_value, Mapping):
4701                         fill_value_ = fill_value[name]
4702                     else:
4703                         fill_value_ = fill_value
4704 
4705                     variables[name] = var._unstack_once(
4706                         index=clean_index,
4707                         dim=dim,
4708                         fill_value=fill_value_,
4709                         sparse=sparse,
4710                     )
4711                 else:
4712                     variables[name] = var
4713 
4714         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4715 
4716         return self._replace_with_new_dims(
4717             variables, coord_names=coord_names, indexes=indexes
4718         )
4719 
4720     def _unstack_full_reindex(
4721         self: T_Dataset,
4722         dim: Hashable,
4723         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4724         fill_value,
4725         sparse: bool,
4726     ) -> T_Dataset:
4727         index, index_vars = index_and_vars
4728         variables: dict[Hashable, Variable] = {}
4729         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4730 
4731         new_indexes, clean_index = index.unstack()
4732         indexes.update(new_indexes)
4733 
4734         new_index_variables = {}
4735         for name, idx in new_indexes.items():
4736             new_index_variables.update(idx.create_variables(index_vars))
4737 
4738         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4739         variables.update(new_index_variables)
4740 
4741         # take a shortcut in case the MultiIndex was not modified.
4742         full_idx = pd.MultiIndex.from_product(
4743             clean_index.levels, names=clean_index.names
4744         )
4745         if clean_index.equals(full_idx):
4746             obj = self
4747         else:
4748             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4749             xr_full_idx = PandasMultiIndex(full_idx, dim)
4750             indexers = Indexes(
4751                 {k: xr_full_idx for k in index_vars},
4752                 xr_full_idx.create_variables(index_vars),
4753             )
4754             obj = self._reindex(
4755                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4756             )
4757 
4758         for name, var in obj.variables.items():
4759             if name not in index_vars:
4760                 if dim in var.dims:
4761                     variables[name] = var.unstack({dim: new_dim_sizes})
4762                 else:
4763                     variables[name] = var
4764 
4765         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4766 
4767         return self._replace_with_new_dims(
4768             variables, coord_names=coord_names, indexes=indexes
4769         )
4770 
4771     def unstack(
4772         self: T_Dataset,
4773         dim: Hashable | Iterable[Hashable] | None = None,
4774         fill_value: Any = xrdtypes.NA,
4775         sparse: bool = False,
4776     ) -> T_Dataset:
4777         """
4778         Unstack existing dimensions corresponding to MultiIndexes into
4779         multiple new dimensions.
4780 
4781         New dimensions will be added at the end.
4782 
4783         Parameters
4784         ----------
4785         dim : hashable or iterable of hashable, optional
4786             Dimension(s) over which to unstack. By default unstacks all
4787             MultiIndexes.
4788         fill_value : scalar or dict-like, default: nan
4789             value to be filled. If a dict-like, maps variable names to
4790             fill values. If not provided or if the dict-like does not
4791             contain all variables, the dtype's NA value will be used.
4792         sparse : bool, default: False
4793             use sparse-array if True
4794 
4795         Returns
4796         -------
4797         unstacked : Dataset
4798             Dataset with unstacked data.
4799 
4800         See Also
4801         --------
4802         Dataset.stack
4803         """
4804 
4805         if dim is None:
4806             dims = list(self.dims)
4807         else:
4808             if isinstance(dim, str) or not isinstance(dim, Iterable):
4809                 dims = [dim]
4810             else:
4811                 dims = list(dim)
4812 
4813             missing_dims = [d for d in dims if d not in self.dims]
4814             if missing_dims:
4815                 raise ValueError(
4816                     f"Dataset does not contain the dimensions: {missing_dims}"
4817                 )
4818 
4819         # each specified dimension must have exactly one multi-index
4820         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4821         for d in dims:
4822             idx, idx_vars = self._get_stack_index(d, multi=True)
4823             if idx is not None:
4824                 stacked_indexes[d] = idx, idx_vars
4825 
4826         if dim is None:
4827             dims = list(stacked_indexes)
4828         else:
4829             non_multi_dims = set(dims) - set(stacked_indexes)
4830             if non_multi_dims:
4831                 raise ValueError(
4832                     "cannot unstack dimensions that do not "
4833                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4834                 )
4835 
4836         result = self.copy(deep=False)
4837 
4838         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4839         # so we can't just access self.variables[v].data for every variable.
4840         # We only check the non-index variables.
4841         # https://github.com/pydata/xarray/issues/5902
4842         nonindexes = [
4843             self.variables[k] for k in set(self.variables) - set(self._indexes)
4844         ]
4845         # Notes for each of these cases:
4846         # 1. Dask arrays don't support assignment by index, which the fast unstack
4847         #    function requires.
4848         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4849         # 2. Sparse doesn't currently support (though we could special-case it)
4850         #    https://github.com/pydata/sparse/issues/422
4851         # 3. pint requires checking if it's a NumPy array until
4852         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4853         #    Once that is resolved, explicitly exclude pint arrays.
4854         #    pint doesn't implement `np.full_like` in a way that's
4855         #    currently compatible.
4856         needs_full_reindex = any(
4857             is_duck_dask_array(v.data)
4858             or isinstance(v.data, sparse_array_type)
4859             or not isinstance(v.data, np.ndarray)
4860             for v in nonindexes
4861         )
4862 
4863         for dim in dims:
4864             if needs_full_reindex:
4865                 result = result._unstack_full_reindex(
4866                     dim, stacked_indexes[dim], fill_value, sparse
4867                 )
4868             else:
4869                 result = result._unstack_once(
4870                     dim, stacked_indexes[dim], fill_value, sparse
4871                 )
4872         return result
4873 
4874     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4875         """Update this dataset's variables with those from another dataset.
4876 
4877         Just like :py:meth:`dict.update` this is a in-place operation.
4878         For a non-inplace version, see :py:meth:`Dataset.merge`.
4879 
4880         Parameters
4881         ----------
4882         other : Dataset or mapping
4883             Variables with which to update this dataset. One of:
4884 
4885             - Dataset
4886             - mapping {var name: DataArray}
4887             - mapping {var name: Variable}
4888             - mapping {var name: (dimension name, array-like)}
4889             - mapping {var name: (tuple of dimension names, array-like)}
4890 
4891         Returns
4892         -------
4893         updated : Dataset
4894             Updated dataset. Note that since the update is in-place this is the input
4895             dataset.
4896 
4897             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4898 
4899         Raises
4900         ------
4901         ValueError
4902             If any dimensions would have inconsistent sizes in the updated
4903             dataset.
4904 
4905         See Also
4906         --------
4907         Dataset.assign
4908         Dataset.merge
4909         """
4910         merge_result = dataset_update_method(self, other)
4911         return self._replace(inplace=True, **merge_result._asdict())
4912 
4913     def merge(
4914         self: T_Dataset,
4915         other: CoercibleMapping | DataArray,
4916         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4917         compat: CompatOptions = "no_conflicts",
4918         join: JoinOptions = "outer",
4919         fill_value: Any = xrdtypes.NA,
4920         combine_attrs: CombineAttrsOptions = "override",
4921     ) -> T_Dataset:
4922         """Merge the arrays of two datasets into a single dataset.
4923 
4924         This method generally does not allow for overriding data, with the
4925         exception of attributes, which are ignored on the second dataset.
4926         Variables with the same name are checked for conflicts via the equals
4927         or identical methods.
4928 
4929         Parameters
4930         ----------
4931         other : Dataset or mapping
4932             Dataset or variables to merge with this dataset.
4933         overwrite_vars : hashable or iterable of hashable, optional
4934             If provided, update variables of these name(s) without checking for
4935             conflicts in this dataset.
4936         compat : {"identical", "equals", "broadcast_equals", \
4937                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4938             String indicating how to compare variables of the same name for
4939             potential conflicts:
4940 
4941             - 'identical': all values, dimensions and attributes must be the
4942               same.
4943             - 'equals': all values and dimensions must be the same.
4944             - 'broadcast_equals': all values must be equal when variables are
4945               broadcast against each other to ensure common dimensions.
4946             - 'no_conflicts': only values which are not null in both datasets
4947               must be equal. The returned dataset then contains the combination
4948               of all non-null values.
4949             - 'override': skip comparing and pick variable from first dataset
4950             - 'minimal': drop conflicting coordinates
4951 
4952         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4953                default: "outer"
4954             Method for joining ``self`` and ``other`` along shared dimensions:
4955 
4956             - 'outer': use the union of the indexes
4957             - 'inner': use the intersection of the indexes
4958             - 'left': use indexes from ``self``
4959             - 'right': use indexes from ``other``
4960             - 'exact': error instead of aligning non-equal indexes
4961             - 'override': use indexes from ``self`` that are the same size
4962               as those of ``other`` in that dimension
4963 
4964         fill_value : scalar or dict-like, optional
4965             Value to use for newly missing values. If a dict-like, maps
4966             variable names (including coordinates) to fill values.
4967         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4968                          "override"} or callable, default: "override"
4969             A callable or a string indicating how to combine attrs of the objects being
4970             merged:
4971 
4972             - "drop": empty attrs on returned Dataset.
4973             - "identical": all attrs must be the same on every object.
4974             - "no_conflicts": attrs from all objects are combined, any that have
4975               the same name must also have the same value.
4976             - "drop_conflicts": attrs from all objects are combined, any that have
4977               the same name but different values are dropped.
4978             - "override": skip comparing and copy attrs from the first dataset to
4979               the result.
4980 
4981             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4982             as its only parameters.
4983 
4984         Returns
4985         -------
4986         merged : Dataset
4987             Merged dataset.
4988 
4989         Raises
4990         ------
4991         MergeError
4992             If any variables conflict (see ``compat``).
4993 
4994         See Also
4995         --------
4996         Dataset.update
4997         """
4998         from .dataarray import DataArray
4999 
5000         other = other.to_dataset() if isinstance(other, DataArray) else other
5001         merge_result = dataset_merge_method(
5002             self,
5003             other,
5004             overwrite_vars=overwrite_vars,
5005             compat=compat,
5006             join=join,
5007             fill_value=fill_value,
5008             combine_attrs=combine_attrs,
5009         )
5010         return self._replace(**merge_result._asdict())
5011 
5012     def _assert_all_in_dataset(
5013         self, names: Iterable[Hashable], virtual_okay: bool = False
5014     ) -> None:
5015         bad_names = set(names) - set(self._variables)
5016         if virtual_okay:
5017             bad_names -= self.virtual_variables
5018         if bad_names:
5019             raise ValueError(
5020                 "One or more of the specified variables "
5021                 "cannot be found in this dataset"
5022             )
5023 
5024     def drop_vars(
5025         self: T_Dataset,
5026         names: Hashable | Iterable[Hashable],
5027         *,
5028         errors: ErrorOptions = "raise",
5029     ) -> T_Dataset:
5030         """Drop variables from this dataset.
5031 
5032         Parameters
5033         ----------
5034         names : hashable or iterable of hashable
5035             Name(s) of variables to drop.
5036         errors : {"raise", "ignore"}, default: "raise"
5037             If 'raise', raises a ValueError error if any of the variable
5038             passed are not in the dataset. If 'ignore', any given names that are in the
5039             dataset are dropped and no error is raised.
5040 
5041         Returns
5042         -------
5043         dropped : Dataset
5044 
5045         """
5046         # the Iterable check is required for mypy
5047         if is_scalar(names) or not isinstance(names, Iterable):
5048             names = {names}
5049         else:
5050             names = set(names)
5051         if errors == "raise":
5052             self._assert_all_in_dataset(names)
5053 
5054         # GH6505
5055         other_names = set()
5056         for var in names:
5057             maybe_midx = self._indexes.get(var, None)
5058             if isinstance(maybe_midx, PandasMultiIndex):
5059                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5060                 idx_other_names = idx_coord_names - set(names)
5061                 other_names.update(idx_other_names)
5062         if other_names:
5063             names |= set(other_names)
5064             warnings.warn(
5065                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5066                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5067                 DeprecationWarning,
5068                 stacklevel=2,
5069             )
5070 
5071         assert_no_index_corrupted(self.xindexes, names)
5072 
5073         variables = {k: v for k, v in self._variables.items() if k not in names}
5074         coord_names = {k for k in self._coord_names if k in variables}
5075         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5076         return self._replace_with_new_dims(
5077             variables, coord_names=coord_names, indexes=indexes
5078         )
5079 
5080     def drop_indexes(
5081         self: T_Dataset,
5082         coord_names: Hashable | Iterable[Hashable],
5083         *,
5084         errors: ErrorOptions = "raise",
5085     ) -> T_Dataset:
5086         """Drop the indexes assigned to the given coordinates.
5087 
5088         Parameters
5089         ----------
5090         coord_names : hashable or iterable of hashable
5091             Name(s) of the coordinate(s) for which to drop the index.
5092         errors : {"raise", "ignore"}, default: "raise"
5093             If 'raise', raises a ValueError error if any of the coordinates
5094             passed have no index or are not in the dataset.
5095             If 'ignore', no error is raised.
5096 
5097         Returns
5098         -------
5099         dropped : Dataset
5100             A new dataset with dropped indexes.
5101 
5102         """
5103         # the Iterable check is required for mypy
5104         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5105             coord_names = {coord_names}
5106         else:
5107             coord_names = set(coord_names)
5108 
5109         if errors == "raise":
5110             invalid_coords = coord_names - self._coord_names
5111             if invalid_coords:
5112                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5113 
5114             unindexed_coords = set(coord_names) - set(self._indexes)
5115             if unindexed_coords:
5116                 raise ValueError(
5117                     f"those coordinates do not have an index: {unindexed_coords}"
5118                 )
5119 
5120         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5121 
5122         variables = {}
5123         for name, var in self._variables.items():
5124             if name in coord_names:
5125                 variables[name] = var.to_base_variable()
5126             else:
5127                 variables[name] = var
5128 
5129         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5130 
5131         return self._replace(variables=variables, indexes=indexes)
5132 
5133     def drop(
5134         self: T_Dataset,
5135         labels=None,
5136         dim=None,
5137         *,
5138         errors: ErrorOptions = "raise",
5139         **labels_kwargs,
5140     ) -> T_Dataset:
5141         """Backward compatible method based on `drop_vars` and `drop_sel`
5142 
5143         Using either `drop_vars` or `drop_sel` is encouraged
5144 
5145         See Also
5146         --------
5147         Dataset.drop_vars
5148         Dataset.drop_sel
5149         """
5150         if errors not in ["raise", "ignore"]:
5151             raise ValueError('errors must be either "raise" or "ignore"')
5152 
5153         if is_dict_like(labels) and not isinstance(labels, dict):
5154             warnings.warn(
5155                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5156                 FutureWarning,
5157                 stacklevel=2,
5158             )
5159             return self.drop_vars(labels, errors=errors)
5160 
5161         if labels_kwargs or isinstance(labels, dict):
5162             if dim is not None:
5163                 raise ValueError("cannot specify dim and dict-like arguments.")
5164             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5165 
5166         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5167             warnings.warn(
5168                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5169                 PendingDeprecationWarning,
5170                 stacklevel=2,
5171             )
5172             return self.drop_vars(labels, errors=errors)
5173         if dim is not None:
5174             warnings.warn(
5175                 "dropping labels using list-like labels is deprecated; using "
5176                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5177                 DeprecationWarning,
5178                 stacklevel=2,
5179             )
5180             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5181 
5182         warnings.warn(
5183             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5184             PendingDeprecationWarning,
5185             stacklevel=2,
5186         )
5187         return self.drop_sel(labels, errors=errors)
5188 
5189     def drop_sel(
5190         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5191     ) -> T_Dataset:
5192         """Drop index labels from this dataset.
5193 
5194         Parameters
5195         ----------
5196         labels : mapping of hashable to Any
5197             Index labels to drop
5198         errors : {"raise", "ignore"}, default: "raise"
5199             If 'raise', raises a ValueError error if
5200             any of the index labels passed are not
5201             in the dataset. If 'ignore', any given labels that are in the
5202             dataset are dropped and no error is raised.
5203         **labels_kwargs : {dim: label, ...}, optional
5204             The keyword arguments form of ``dim`` and ``labels``
5205 
5206         Returns
5207         -------
5208         dropped : Dataset
5209 
5210         Examples
5211         --------
5212         >>> data = np.arange(6).reshape(2, 3)
5213         >>> labels = ["a", "b", "c"]
5214         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5215         >>> ds
5216         <xarray.Dataset>
5217         Dimensions:  (x: 2, y: 3)
5218         Coordinates:
5219           * y        (y) <U1 'a' 'b' 'c'
5220         Dimensions without coordinates: x
5221         Data variables:
5222             A        (x, y) int64 0 1 2 3 4 5
5223         >>> ds.drop_sel(y=["a", "c"])
5224         <xarray.Dataset>
5225         Dimensions:  (x: 2, y: 1)
5226         Coordinates:
5227           * y        (y) <U1 'b'
5228         Dimensions without coordinates: x
5229         Data variables:
5230             A        (x, y) int64 1 4
5231         >>> ds.drop_sel(y="b")
5232         <xarray.Dataset>
5233         Dimensions:  (x: 2, y: 2)
5234         Coordinates:
5235           * y        (y) <U1 'a' 'c'
5236         Dimensions without coordinates: x
5237         Data variables:
5238             A        (x, y) int64 0 2 3 5
5239         """
5240         if errors not in ["raise", "ignore"]:
5241             raise ValueError('errors must be either "raise" or "ignore"')
5242 
5243         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5244 
5245         ds = self
5246         for dim, labels_for_dim in labels.items():
5247             # Don't cast to set, as it would harm performance when labels
5248             # is a large numpy array
5249             if utils.is_scalar(labels_for_dim):
5250                 labels_for_dim = [labels_for_dim]
5251             labels_for_dim = np.asarray(labels_for_dim)
5252             try:
5253                 index = self.get_index(dim)
5254             except KeyError:
5255                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5256             new_index = index.drop(labels_for_dim, errors=errors)
5257             ds = ds.loc[{dim: new_index}]
5258         return ds
5259 
5260     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5261         """Drop index positions from this Dataset.
5262 
5263         Parameters
5264         ----------
5265         indexers : mapping of hashable to Any
5266             Index locations to drop
5267         **indexers_kwargs : {dim: position, ...}, optional
5268             The keyword arguments form of ``dim`` and ``positions``
5269 
5270         Returns
5271         -------
5272         dropped : Dataset
5273 
5274         Raises
5275         ------
5276         IndexError
5277 
5278         Examples
5279         --------
5280         >>> data = np.arange(6).reshape(2, 3)
5281         >>> labels = ["a", "b", "c"]
5282         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5283         >>> ds
5284         <xarray.Dataset>
5285         Dimensions:  (x: 2, y: 3)
5286         Coordinates:
5287           * y        (y) <U1 'a' 'b' 'c'
5288         Dimensions without coordinates: x
5289         Data variables:
5290             A        (x, y) int64 0 1 2 3 4 5
5291         >>> ds.drop_isel(y=[0, 2])
5292         <xarray.Dataset>
5293         Dimensions:  (x: 2, y: 1)
5294         Coordinates:
5295           * y        (y) <U1 'b'
5296         Dimensions without coordinates: x
5297         Data variables:
5298             A        (x, y) int64 1 4
5299         >>> ds.drop_isel(y=1)
5300         <xarray.Dataset>
5301         Dimensions:  (x: 2, y: 2)
5302         Coordinates:
5303           * y        (y) <U1 'a' 'c'
5304         Dimensions without coordinates: x
5305         Data variables:
5306             A        (x, y) int64 0 2 3 5
5307         """
5308 
5309         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5310 
5311         ds = self
5312         dimension_index = {}
5313         for dim, pos_for_dim in indexers.items():
5314             # Don't cast to set, as it would harm performance when labels
5315             # is a large numpy array
5316             if utils.is_scalar(pos_for_dim):
5317                 pos_for_dim = [pos_for_dim]
5318             pos_for_dim = np.asarray(pos_for_dim)
5319             index = self.get_index(dim)
5320             new_index = index.delete(pos_for_dim)
5321             dimension_index[dim] = new_index
5322         ds = ds.loc[dimension_index]
5323         return ds
5324 
5325     def drop_dims(
5326         self: T_Dataset,
5327         drop_dims: Hashable | Iterable[Hashable],
5328         *,
5329         errors: ErrorOptions = "raise",
5330     ) -> T_Dataset:
5331         """Drop dimensions and associated variables from this dataset.
5332 
5333         Parameters
5334         ----------
5335         drop_dims : hashable or iterable of hashable
5336             Dimension or dimensions to drop.
5337         errors : {"raise", "ignore"}, default: "raise"
5338             If 'raise', raises a ValueError error if any of the
5339             dimensions passed are not in the dataset. If 'ignore', any given
5340             dimensions that are in the dataset are dropped and no error is raised.
5341 
5342         Returns
5343         -------
5344         obj : Dataset
5345             The dataset without the given dimensions (or any variables
5346             containing those dimensions).
5347         """
5348         if errors not in ["raise", "ignore"]:
5349             raise ValueError('errors must be either "raise" or "ignore"')
5350 
5351         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5352             drop_dims = {drop_dims}
5353         else:
5354             drop_dims = set(drop_dims)
5355 
5356         if errors == "raise":
5357             missing_dims = drop_dims - set(self.dims)
5358             if missing_dims:
5359                 raise ValueError(
5360                     f"Dataset does not contain the dimensions: {missing_dims}"
5361                 )
5362 
5363         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5364         return self.drop_vars(drop_vars)
5365 
5366     def transpose(
5367         self: T_Dataset,
5368         *dims: Hashable,
5369         missing_dims: ErrorOptionsWithWarn = "raise",
5370     ) -> T_Dataset:
5371         """Return a new Dataset object with all array dimensions transposed.
5372 
5373         Although the order of dimensions on each array will change, the dataset
5374         dimensions themselves will remain in fixed (sorted) order.
5375 
5376         Parameters
5377         ----------
5378         *dims : hashable, optional
5379             By default, reverse the dimensions on each array. Otherwise,
5380             reorder the dimensions to this order.
5381         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5382             What to do if dimensions that should be selected from are not present in the
5383             Dataset:
5384             - "raise": raise an exception
5385             - "warn": raise a warning, and ignore the missing dimensions
5386             - "ignore": ignore the missing dimensions
5387 
5388         Returns
5389         -------
5390         transposed : Dataset
5391             Each array in the dataset (including) coordinates will be
5392             transposed to the given order.
5393 
5394         Notes
5395         -----
5396         This operation returns a view of each array's data. It is
5397         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5398         -- the data will be fully loaded into memory.
5399 
5400         See Also
5401         --------
5402         numpy.transpose
5403         DataArray.transpose
5404         """
5405         # Use infix_dims to check once for missing dimensions
5406         if len(dims) != 0:
5407             _ = list(infix_dims(dims, self.dims, missing_dims))
5408 
5409         ds = self.copy()
5410         for name, var in self._variables.items():
5411             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5412             ds._variables[name] = var.transpose(*var_dims)
5413         return ds
5414 
5415     def dropna(
5416         self: T_Dataset,
5417         dim: Hashable,
5418         how: Literal["any", "all"] = "any",
5419         thresh: int | None = None,
5420         subset: Iterable[Hashable] | None = None,
5421     ) -> T_Dataset:
5422         """Returns a new dataset with dropped labels for missing values along
5423         the provided dimension.
5424 
5425         Parameters
5426         ----------
5427         dim : hashable
5428             Dimension along which to drop missing values. Dropping along
5429             multiple dimensions simultaneously is not yet supported.
5430         how : {"any", "all"}, default: "any"
5431             - any : if any NA values are present, drop that label
5432             - all : if all values are NA, drop that label
5433 
5434         thresh : int or None, optional
5435             If supplied, require this many non-NA values.
5436         subset : iterable of hashable or None, optional
5437             Which variables to check for missing values. By default, all
5438             variables in the dataset are checked.
5439 
5440         Returns
5441         -------
5442         Dataset
5443         """
5444         # TODO: consider supporting multiple dimensions? Or not, given that
5445         # there are some ugly edge cases, e.g., pandas's dropna differs
5446         # depending on the order of the supplied axes.
5447 
5448         if dim not in self.dims:
5449             raise ValueError(f"{dim} must be a single dataset dimension")
5450 
5451         if subset is None:
5452             subset = iter(self.data_vars)
5453 
5454         count = np.zeros(self.dims[dim], dtype=np.int64)
5455         size = np.int_(0)  # for type checking
5456 
5457         for k in subset:
5458             array = self._variables[k]
5459             if dim in array.dims:
5460                 dims = [d for d in array.dims if d != dim]
5461                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5462                 size += math.prod([self.dims[d] for d in dims])
5463 
5464         if thresh is not None:
5465             mask = count >= thresh
5466         elif how == "any":
5467             mask = count == size
5468         elif how == "all":
5469             mask = count > 0
5470         elif how is not None:
5471             raise ValueError(f"invalid how option: {how}")
5472         else:
5473             raise TypeError("must specify how or thresh")
5474 
5475         return self.isel({dim: mask})
5476 
5477     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5478         """Fill missing values in this object.
5479 
5480         This operation follows the normal broadcasting and alignment rules that
5481         xarray uses for binary arithmetic, except the result is aligned to this
5482         object (``join='left'``) instead of aligned to the intersection of
5483         index coordinates (``join='inner'``).
5484 
5485         Parameters
5486         ----------
5487         value : scalar, ndarray, DataArray, dict or Dataset
5488             Used to fill all matching missing values in this dataset's data
5489             variables. Scalars, ndarrays or DataArrays arguments are used to
5490             fill all data with aligned coordinates (for DataArrays).
5491             Dictionaries or datasets match data variables and then align
5492             coordinates if necessary.
5493 
5494         Returns
5495         -------
5496         Dataset
5497 
5498         Examples
5499         --------
5500         >>> ds = xr.Dataset(
5501         ...     {
5502         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5503         ...         "B": ("x", [3, 4, np.nan, 1]),
5504         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5505         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5506         ...     },
5507         ...     coords={"x": [0, 1, 2, 3]},
5508         ... )
5509         >>> ds
5510         <xarray.Dataset>
5511         Dimensions:  (x: 4)
5512         Coordinates:
5513           * x        (x) int64 0 1 2 3
5514         Data variables:
5515             A        (x) float64 nan 2.0 nan 0.0
5516             B        (x) float64 3.0 4.0 nan 1.0
5517             C        (x) float64 nan nan nan 5.0
5518             D        (x) float64 nan 3.0 nan 4.0
5519 
5520         Replace all `NaN` values with 0s.
5521 
5522         >>> ds.fillna(0)
5523         <xarray.Dataset>
5524         Dimensions:  (x: 4)
5525         Coordinates:
5526           * x        (x) int64 0 1 2 3
5527         Data variables:
5528             A        (x) float64 0.0 2.0 0.0 0.0
5529             B        (x) float64 3.0 4.0 0.0 1.0
5530             C        (x) float64 0.0 0.0 0.0 5.0
5531             D        (x) float64 0.0 3.0 0.0 4.0
5532 
5533         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
5534 
5535         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5536         >>> ds.fillna(value=values)
5537         <xarray.Dataset>
5538         Dimensions:  (x: 4)
5539         Coordinates:
5540           * x        (x) int64 0 1 2 3
5541         Data variables:
5542             A        (x) float64 0.0 2.0 0.0 0.0
5543             B        (x) float64 3.0 4.0 1.0 1.0
5544             C        (x) float64 2.0 2.0 2.0 5.0
5545             D        (x) float64 3.0 3.0 3.0 4.0
5546         """
5547         if utils.is_dict_like(value):
5548             value_keys = getattr(value, "data_vars", value).keys()
5549             if not set(value_keys) <= set(self.data_vars.keys()):
5550                 raise ValueError(
5551                     "all variables in the argument to `fillna` "
5552                     "must be contained in the original dataset"
5553                 )
5554         out = ops.fillna(self, value)
5555         return out
5556 
5557     def interpolate_na(
5558         self: T_Dataset,
5559         dim: Hashable | None = None,
5560         method: InterpOptions = "linear",
5561         limit: int = None,
5562         use_coordinate: bool | Hashable = True,
5563         max_gap: (
5564             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5565         ) = None,
5566         **kwargs: Any,
5567     ) -> T_Dataset:
5568         """Fill in NaNs by interpolating according to different methods.
5569 
5570         Parameters
5571         ----------
5572         dim : Hashable or None, optional
5573             Specifies the dimension along which to interpolate.
5574         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5575             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5576             String indicating which method to use for interpolation:
5577 
5578             - 'linear': linear interpolation. Additional keyword
5579               arguments are passed to :py:func:`numpy.interp`
5580             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5581               are passed to :py:func:`scipy.interpolate.interp1d`. If
5582               ``method='polynomial'``, the ``order`` keyword argument must also be
5583               provided.
5584             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5585               respective :py:class:`scipy.interpolate` classes.
5586 
5587         use_coordinate : bool or Hashable, default: True
5588             Specifies which index to use as the x values in the interpolation
5589             formulated as `y = f(x)`. If False, values are treated as if
5590             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5591             used. If ``use_coordinate`` is a string, it specifies the name of a
5592             coordinate variariable to use as the index.
5593         limit : int, default: None
5594             Maximum number of consecutive NaNs to fill. Must be greater than 0
5595             or None for no limit. This filling is done regardless of the size of
5596             the gap in the data. To only interpolate over gaps less than a given length,
5597             see ``max_gap``.
5598         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5599             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5600             Use None for no limit. When interpolating along a datetime64 dimension
5601             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5602 
5603             - a string that is valid input for pandas.to_timedelta
5604             - a :py:class:`numpy.timedelta64` object
5605             - a :py:class:`pandas.Timedelta` object
5606             - a :py:class:`datetime.timedelta` object
5607 
5608             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5609             dimensions has not been implemented yet. Gap length is defined as the difference
5610             between coordinate values at the first data point after a gap and the last value
5611             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5612             between coordinate values at the first (last) valid data point and the first (last) NaN.
5613             For example, consider::
5614 
5615                 <xarray.DataArray (x: 9)>
5616                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5617                 Coordinates:
5618                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5619 
5620             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5621         **kwargs : dict, optional
5622             parameters passed verbatim to the underlying interpolation function
5623 
5624         Returns
5625         -------
5626         interpolated: Dataset
5627             Filled in Dataset.
5628 
5629         See Also
5630         --------
5631         numpy.interp
5632         scipy.interpolate
5633 
5634         Examples
5635         --------
5636         >>> ds = xr.Dataset(
5637         ...     {
5638         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5639         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5640         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5641         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5642         ...     },
5643         ...     coords={"x": [0, 1, 2, 3, 4]},
5644         ... )
5645         >>> ds
5646         <xarray.Dataset>
5647         Dimensions:  (x: 5)
5648         Coordinates:
5649           * x        (x) int64 0 1 2 3 4
5650         Data variables:
5651             A        (x) float64 nan 2.0 3.0 nan 0.0
5652             B        (x) float64 3.0 4.0 nan 1.0 7.0
5653             C        (x) float64 nan nan nan 5.0 0.0
5654             D        (x) float64 nan 3.0 nan -1.0 4.0
5655 
5656         >>> ds.interpolate_na(dim="x", method="linear")
5657         <xarray.Dataset>
5658         Dimensions:  (x: 5)
5659         Coordinates:
5660           * x        (x) int64 0 1 2 3 4
5661         Data variables:
5662             A        (x) float64 nan 2.0 3.0 1.5 0.0
5663             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5664             C        (x) float64 nan nan nan 5.0 0.0
5665             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5666 
5667         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5668         <xarray.Dataset>
5669         Dimensions:  (x: 5)
5670         Coordinates:
5671           * x        (x) int64 0 1 2 3 4
5672         Data variables:
5673             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5674             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5675             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5676             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5677         """
5678         from .missing import _apply_over_vars_with_dim, interp_na
5679 
5680         new = _apply_over_vars_with_dim(
5681             interp_na,
5682             self,
5683             dim=dim,
5684             method=method,
5685             limit=limit,
5686             use_coordinate=use_coordinate,
5687             max_gap=max_gap,
5688             **kwargs,
5689         )
5690         return new
5691 
5692     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5693         """Fill NaN values by propagating values forward
5694 
5695         *Requires bottleneck.*
5696 
5697         Parameters
5698         ----------
5699         dim : Hashable
5700             Specifies the dimension along which to propagate values when
5701             filling.
5702         limit : int or None, optional
5703             The maximum number of consecutive NaN values to forward fill. In
5704             other words, if there is a gap with more than this number of
5705             consecutive NaNs, it will only be partially filled. Must be greater
5706             than 0 or None for no limit. Must be None or greater than or equal
5707             to axis length if filling along chunked axes (dimensions).
5708 
5709         Returns
5710         -------
5711         Dataset
5712         """
5713         from .missing import _apply_over_vars_with_dim, ffill
5714 
5715         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5716         return new
5717 
5718     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5719         """Fill NaN values by propagating values backward
5720 
5721         *Requires bottleneck.*
5722 
5723         Parameters
5724         ----------
5725         dim : Hashable
5726             Specifies the dimension along which to propagate values when
5727             filling.
5728         limit : int or None, optional
5729             The maximum number of consecutive NaN values to backward fill. In
5730             other words, if there is a gap with more than this number of
5731             consecutive NaNs, it will only be partially filled. Must be greater
5732             than 0 or None for no limit. Must be None or greater than or equal
5733             to axis length if filling along chunked axes (dimensions).
5734 
5735         Returns
5736         -------
5737         Dataset
5738         """
5739         from .missing import _apply_over_vars_with_dim, bfill
5740 
5741         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5742         return new
5743 
5744     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5745         """Combine two Datasets, default to data_vars of self.
5746 
5747         The new coordinates follow the normal broadcasting and alignment rules
5748         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5749         filled with np.nan.
5750 
5751         Parameters
5752         ----------
5753         other : Dataset
5754             Used to fill all matching missing values in this array.
5755 
5756         Returns
5757         -------
5758         Dataset
5759         """
5760         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5761         return out
5762 
5763     def reduce(
5764         self: T_Dataset,
5765         func: Callable,
5766         dim: Hashable | Iterable[Hashable] = None,
5767         *,
5768         keep_attrs: bool | None = None,
5769         keepdims: bool = False,
5770         numeric_only: bool = False,
5771         **kwargs: Any,
5772     ) -> T_Dataset:
5773         """Reduce this dataset by applying `func` along some dimension(s).
5774 
5775         Parameters
5776         ----------
5777         func : callable
5778             Function which can be called in the form
5779             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5780             np.ndarray over an integer valued axis.
5781         dim : str or sequence of str, optional
5782             Dimension(s) over which to apply `func`.  By default `func` is
5783             applied over all dimensions.
5784         keep_attrs : bool or None, optional
5785             If True, the dataset's attributes (`attrs`) will be copied from
5786             the original object to the new one.  If False (default), the new
5787             object will be returned without attributes.
5788         keepdims : bool, default: False
5789             If True, the dimensions which are reduced are left in the result
5790             as dimensions of size one. Coordinates that use these dimensions
5791             are removed.
5792         numeric_only : bool, default: False
5793             If True, only apply ``func`` to variables with a numeric dtype.
5794         **kwargs : Any
5795             Additional keyword arguments passed on to ``func``.
5796 
5797         Returns
5798         -------
5799         reduced : Dataset
5800             Dataset with this object's DataArrays replaced with new DataArrays
5801             of summarized data and the indicated dimension(s) removed.
5802         """
5803         if kwargs.get("axis", None) is not None:
5804             raise ValueError(
5805                 "passing 'axis' to Dataset reduce methods is ambiguous."
5806                 " Please use 'dim' instead."
5807             )
5808 
5809         if dim is None or dim is ...:
5810             dims = set(self.dims)
5811         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5812             dims = {dim}
5813         else:
5814             dims = set(dim)
5815 
5816         missing_dimensions = [d for d in dims if d not in self.dims]
5817         if missing_dimensions:
5818             raise ValueError(
5819                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5820             )
5821 
5822         if keep_attrs is None:
5823             keep_attrs = _get_keep_attrs(default=False)
5824 
5825         variables: dict[Hashable, Variable] = {}
5826         for name, var in self._variables.items():
5827             reduce_dims = [d for d in var.dims if d in dims]
5828             if name in self.coords:
5829                 if not reduce_dims:
5830                     variables[name] = var
5831             else:
5832                 if (
5833                     # Some reduction functions (e.g. std, var) need to run on variables
5834                     # that don't have the reduce dims: PR5393
5835                     not reduce_dims
5836                     or not numeric_only
5837                     or np.issubdtype(var.dtype, np.number)
5838                     or (var.dtype == np.bool_)
5839                 ):
5840                     reduce_maybe_single: Hashable | None | list[Hashable]
5841                     if len(reduce_dims) == 1:
5842                         # unpack dimensions for the benefit of functions
5843                         # like np.argmin which can't handle tuple arguments
5844                         (reduce_maybe_single,) = reduce_dims
5845                     elif len(reduce_dims) == var.ndim:
5846                         # prefer to aggregate over axis=None rather than
5847                         # axis=(0, 1) if they will be equivalent, because
5848                         # the former is often more efficient
5849                         reduce_maybe_single = None
5850                     else:
5851                         reduce_maybe_single = reduce_dims
5852                     variables[name] = var.reduce(
5853                         func,
5854                         dim=reduce_maybe_single,
5855                         keep_attrs=keep_attrs,
5856                         keepdims=keepdims,
5857                         **kwargs,
5858                     )
5859 
5860         coord_names = {k for k in self.coords if k in variables}
5861         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5862         attrs = self.attrs if keep_attrs else None
5863         return self._replace_with_new_dims(
5864             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5865         )
5866 
5867     def map(
5868         self: T_Dataset,
5869         func: Callable,
5870         keep_attrs: bool | None = None,
5871         args: Iterable[Any] = (),
5872         **kwargs: Any,
5873     ) -> T_Dataset:
5874         """Apply a function to each data variable in this dataset
5875 
5876         Parameters
5877         ----------
5878         func : callable
5879             Function which can be called in the form `func(x, *args, **kwargs)`
5880             to transform each DataArray `x` in this dataset into another
5881             DataArray.
5882         keep_attrs : bool or None, optional
5883             If True, both the dataset's and variables' attributes (`attrs`) will be
5884             copied from the original objects to the new ones. If False, the new dataset
5885             and variables will be returned without copying the attributes.
5886         args : iterable, optional
5887             Positional arguments passed on to `func`.
5888         **kwargs : Any
5889             Keyword arguments passed on to `func`.
5890 
5891         Returns
5892         -------
5893         applied : Dataset
5894             Resulting dataset from applying ``func`` to each data variable.
5895 
5896         Examples
5897         --------
5898         >>> da = xr.DataArray(np.random.randn(2, 3))
5899         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5900         >>> ds
5901         <xarray.Dataset>
5902         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5903         Dimensions without coordinates: dim_0, dim_1, x
5904         Data variables:
5905             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5906             bar      (x) int64 -1 2
5907         >>> ds.map(np.fabs)
5908         <xarray.Dataset>
5909         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5910         Dimensions without coordinates: dim_0, dim_1, x
5911         Data variables:
5912             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5913             bar      (x) float64 1.0 2.0
5914         """
5915         if keep_attrs is None:
5916             keep_attrs = _get_keep_attrs(default=False)
5917         variables = {
5918             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5919             for k, v in self.data_vars.items()
5920         }
5921         if keep_attrs:
5922             for k, v in variables.items():
5923                 v._copy_attrs_from(self.data_vars[k])
5924         attrs = self.attrs if keep_attrs else None
5925         return type(self)(variables, attrs=attrs)
5926 
5927     def apply(
5928         self: T_Dataset,
5929         func: Callable,
5930         keep_attrs: bool | None = None,
5931         args: Iterable[Any] = (),
5932         **kwargs: Any,
5933     ) -> T_Dataset:
5934         """
5935         Backward compatible implementation of ``map``
5936 
5937         See Also
5938         --------
5939         Dataset.map
5940         """
5941         warnings.warn(
5942             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5943             PendingDeprecationWarning,
5944             stacklevel=2,
5945         )
5946         return self.map(func, keep_attrs, args, **kwargs)
5947 
5948     def assign(
5949         self: T_Dataset,
5950         variables: Mapping[Any, Any] | None = None,
5951         **variables_kwargs: Any,
5952     ) -> T_Dataset:
5953         """Assign new data variables to a Dataset, returning a new object
5954         with all the original variables in addition to the new ones.
5955 
5956         Parameters
5957         ----------
5958         variables : mapping of hashable to Any
5959             Mapping from variables names to the new values. If the new values
5960             are callable, they are computed on the Dataset and assigned to new
5961             data variables. If the values are not callable, (e.g. a DataArray,
5962             scalar, or array), they are simply assigned.
5963         **variables_kwargs
5964             The keyword arguments form of ``variables``.
5965             One of variables or variables_kwargs must be provided.
5966 
5967         Returns
5968         -------
5969         ds : Dataset
5970             A new Dataset with the new variables in addition to all the
5971             existing variables.
5972 
5973         Notes
5974         -----
5975         Since ``kwargs`` is a dictionary, the order of your arguments may not
5976         be preserved, and so the order of the new variables is not well
5977         defined. Assigning multiple variables within the same ``assign`` is
5978         possible, but you cannot reference other variables created within the
5979         same ``assign`` call.
5980 
5981         See Also
5982         --------
5983         pandas.DataFrame.assign
5984 
5985         Examples
5986         --------
5987         >>> x = xr.Dataset(
5988         ...     {
5989         ...         "temperature_c": (
5990         ...             ("lat", "lon"),
5991         ...             20 * np.random.rand(4).reshape(2, 2),
5992         ...         ),
5993         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5994         ...     },
5995         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5996         ... )
5997         >>> x
5998         <xarray.Dataset>
5999         Dimensions:        (lat: 2, lon: 2)
6000         Coordinates:
6001           * lat            (lat) int64 10 20
6002           * lon            (lon) int64 150 160
6003         Data variables:
6004             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6005             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6006 
6007         Where the value is a callable, evaluated on dataset:
6008 
6009         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6010         <xarray.Dataset>
6011         Dimensions:        (lat: 2, lon: 2)
6012         Coordinates:
6013           * lat            (lat) int64 10 20
6014           * lon            (lon) int64 150 160
6015         Data variables:
6016             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6017             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6018             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6019 
6020         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6021 
6022         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6023         <xarray.Dataset>
6024         Dimensions:        (lat: 2, lon: 2)
6025         Coordinates:
6026           * lat            (lat) int64 10 20
6027           * lon            (lon) int64 150 160
6028         Data variables:
6029             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6030             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6031             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6032 
6033         """
6034         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6035         data = self.copy()
6036         # do all calculations first...
6037         results: CoercibleMapping = data._calc_assign_results(variables)
6038         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6039         # ... and then assign
6040         data.update(results)
6041         return data
6042 
6043     def to_array(
6044         self, dim: Hashable = "variable", name: Hashable | None = None
6045     ) -> DataArray:
6046         """Convert this dataset into an xarray.DataArray
6047 
6048         The data variables of this dataset will be broadcast against each other
6049         and stacked along the first axis of the new array. All coordinates of
6050         this dataset will remain coordinates.
6051 
6052         Parameters
6053         ----------
6054         dim : Hashable, default: "variable"
6055             Name of the new dimension.
6056         name : Hashable or None, optional
6057             Name of the new data array.
6058 
6059         Returns
6060         -------
6061         array : xarray.DataArray
6062         """
6063         from .dataarray import DataArray
6064 
6065         data_vars = [self.variables[k] for k in self.data_vars]
6066         broadcast_vars = broadcast_variables(*data_vars)
6067         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6068 
6069         dims = (dim,) + broadcast_vars[0].dims
6070         variable = Variable(dims, data, self.attrs, fastpath=True)
6071 
6072         coords = {k: v.variable for k, v in self.coords.items()}
6073         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6074         new_dim_index = PandasIndex(list(self.data_vars), dim)
6075         indexes[dim] = new_dim_index
6076         coords.update(new_dim_index.create_variables())
6077 
6078         return DataArray._construct_direct(variable, coords, name, indexes)
6079 
6080     def _normalize_dim_order(
6081         self, dim_order: Sequence[Hashable] | None = None
6082     ) -> dict[Hashable, int]:
6083         """
6084         Check the validity of the provided dimensions if any and return the mapping
6085         between dimension name and their size.
6086 
6087         Parameters
6088         ----------
6089         dim_order: Sequence of Hashable or None, optional
6090             Dimension order to validate (default to the alphabetical order if None).
6091 
6092         Returns
6093         -------
6094         result : dict[Hashable, int]
6095             Validated dimensions mapping.
6096 
6097         """
6098         if dim_order is None:
6099             dim_order = list(self.dims)
6100         elif set(dim_order) != set(self.dims):
6101             raise ValueError(
6102                 "dim_order {} does not match the set of dimensions of this "
6103                 "Dataset: {}".format(dim_order, list(self.dims))
6104             )
6105 
6106         ordered_dims = {k: self.dims[k] for k in dim_order}
6107 
6108         return ordered_dims
6109 
6110     def to_pandas(self) -> pd.Series | pd.DataFrame:
6111         """Convert this dataset into a pandas object without changing the number of dimensions.
6112 
6113         The type of the returned object depends on the number of Dataset
6114         dimensions:
6115 
6116         * 0D -> `pandas.Series`
6117         * 1D -> `pandas.DataFrame`
6118 
6119         Only works for Datasets with 1 or fewer dimensions.
6120         """
6121         if len(self.dims) == 0:
6122             return pd.Series({k: v.item() for k, v in self.items()})
6123         if len(self.dims) == 1:
6124             return self.to_dataframe()
6125         raise ValueError(
6126             "cannot convert Datasets with %s dimensions into "
6127             "pandas objects without changing the number of dimensions. "
6128             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6129         )
6130 
6131     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6132         columns = [k for k in self.variables if k not in self.dims]
6133         data = [
6134             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6135             for k in columns
6136         ]
6137         index = self.coords.to_index([*ordered_dims])
6138         return pd.DataFrame(dict(zip(columns, data)), index=index)
6139 
6140     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6141         """Convert this dataset into a pandas.DataFrame.
6142 
6143         Non-index variables in this dataset form the columns of the
6144         DataFrame. The DataFrame is indexed by the Cartesian product of
6145         this dataset's indices.
6146 
6147         Parameters
6148         ----------
6149         dim_order: Sequence of Hashable or None, optional
6150             Hierarchical dimension order for the resulting dataframe. All
6151             arrays are transposed to this order and then written out as flat
6152             vectors in contiguous order, so the last dimension in this list
6153             will be contiguous in the resulting DataFrame. This has a major
6154             influence on which operations are efficient on the resulting
6155             dataframe.
6156 
6157             If provided, must include all dimensions of this dataset. By
6158             default, dimensions are sorted alphabetically.
6159 
6160         Returns
6161         -------
6162         result : DataFrame
6163             Dataset as a pandas DataFrame.
6164 
6165         """
6166 
6167         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6168 
6169         return self._to_dataframe(ordered_dims=ordered_dims)
6170 
6171     def _set_sparse_data_from_dataframe(
6172         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6173     ) -> None:
6174         from sparse import COO
6175 
6176         if isinstance(idx, pd.MultiIndex):
6177             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6178             is_sorted = idx.is_monotonic_increasing
6179             shape = tuple(lev.size for lev in idx.levels)
6180         else:
6181             coords = np.arange(idx.size).reshape(1, -1)
6182             is_sorted = True
6183             shape = (idx.size,)
6184 
6185         for name, values in arrays:
6186             # In virtually all real use cases, the sparse array will now have
6187             # missing values and needs a fill_value. For consistency, don't
6188             # special case the rare exceptions (e.g., dtype=int without a
6189             # MultiIndex).
6190             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6191             values = np.asarray(values, dtype=dtype)
6192 
6193             data = COO(
6194                 coords,
6195                 values,
6196                 shape,
6197                 has_duplicates=False,
6198                 sorted=is_sorted,
6199                 fill_value=fill_value,
6200             )
6201             self[name] = (dims, data)
6202 
6203     def _set_numpy_data_from_dataframe(
6204         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6205     ) -> None:
6206         if not isinstance(idx, pd.MultiIndex):
6207             for name, values in arrays:
6208                 self[name] = (dims, values)
6209             return
6210 
6211         # NB: similar, more general logic, now exists in
6212         # variable.unstack_once; we could consider combining them at some
6213         # point.
6214 
6215         shape = tuple(lev.size for lev in idx.levels)
6216         indexer = tuple(idx.codes)
6217 
6218         # We already verified that the MultiIndex has all unique values, so
6219         # there are missing values if and only if the size of output arrays is
6220         # larger that the index.
6221         missing_values = math.prod(shape) > idx.shape[0]
6222 
6223         for name, values in arrays:
6224             # NumPy indexing is much faster than using DataFrame.reindex() to
6225             # fill in missing values:
6226             # https://stackoverflow.com/a/35049899/809705
6227             if missing_values:
6228                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6229                 data = np.full(shape, fill_value, dtype)
6230             else:
6231                 # If there are no missing values, keep the existing dtype
6232                 # instead of promoting to support NA, e.g., keep integer
6233                 # columns as integers.
6234                 # TODO: consider removing this special case, which doesn't
6235                 # exist for sparse=True.
6236                 data = np.zeros(shape, values.dtype)
6237             data[indexer] = values
6238             self[name] = (dims, data)
6239 
6240     @classmethod
6241     def from_dataframe(
6242         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6243     ) -> T_Dataset:
6244         """Convert a pandas.DataFrame into an xarray.Dataset
6245 
6246         Each column will be converted into an independent variable in the
6247         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6248         into a tensor product of one-dimensional indices (filling in missing
6249         values with NaN). This method will produce a Dataset very similar to
6250         that on which the 'to_dataframe' method was called, except with
6251         possibly redundant dimensions (since all dataset variables will have
6252         the same dimensionality)
6253 
6254         Parameters
6255         ----------
6256         dataframe : DataFrame
6257             DataFrame from which to copy data and indices.
6258         sparse : bool, default: False
6259             If true, create a sparse arrays instead of dense numpy arrays. This
6260             can potentially save a large amount of memory if the DataFrame has
6261             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6262 
6263         Returns
6264         -------
6265         New Dataset.
6266 
6267         See Also
6268         --------
6269         xarray.DataArray.from_series
6270         pandas.DataFrame.to_xarray
6271         """
6272         # TODO: Add an option to remove dimensions along which the variables
6273         # are constant, to enable consistent serialization to/from a dataframe,
6274         # even if some variables have different dimensionality.
6275 
6276         if not dataframe.columns.is_unique:
6277             raise ValueError("cannot convert DataFrame with non-unique columns")
6278 
6279         idx = remove_unused_levels_categories(dataframe.index)
6280 
6281         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6282             raise ValueError(
6283                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6284             )
6285 
6286         # Cast to a NumPy array first, in case the Series is a pandas Extension
6287         # array (which doesn't have a valid NumPy dtype)
6288         # TODO: allow users to control how this casting happens, e.g., by
6289         # forwarding arguments to pandas.Series.to_numpy?
6290         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6291 
6292         indexes: dict[Hashable, Index] = {}
6293         index_vars: dict[Hashable, Variable] = {}
6294 
6295         if isinstance(idx, pd.MultiIndex):
6296             dims = tuple(
6297                 name if name is not None else "level_%i" % n
6298                 for n, name in enumerate(idx.names)
6299             )
6300             for dim, lev in zip(dims, idx.levels):
6301                 xr_idx = PandasIndex(lev, dim)
6302                 indexes[dim] = xr_idx
6303                 index_vars.update(xr_idx.create_variables())
6304         else:
6305             index_name = idx.name if idx.name is not None else "index"
6306             dims = (index_name,)
6307             xr_idx = PandasIndex(idx, index_name)
6308             indexes[index_name] = xr_idx
6309             index_vars.update(xr_idx.create_variables())
6310 
6311         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6312 
6313         if sparse:
6314             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6315         else:
6316             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6317         return obj
6318 
6319     def to_dask_dataframe(
6320         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6321     ) -> DaskDataFrame:
6322         """
6323         Convert this dataset into a dask.dataframe.DataFrame.
6324 
6325         The dimensions, coordinates and data variables in this dataset form
6326         the columns of the DataFrame.
6327 
6328         Parameters
6329         ----------
6330         dim_order : list, optional
6331             Hierarchical dimension order for the resulting dataframe. All
6332             arrays are transposed to this order and then written out as flat
6333             vectors in contiguous order, so the last dimension in this list
6334             will be contiguous in the resulting DataFrame. This has a major
6335             influence on which operations are efficient on the resulting dask
6336             dataframe.
6337 
6338             If provided, must include all dimensions of this dataset. By
6339             default, dimensions are sorted alphabetically.
6340         set_index : bool, default: False
6341             If set_index=True, the dask DataFrame is indexed by this dataset's
6342             coordinate. Since dask DataFrames do not support multi-indexes,
6343             set_index only works if the dataset only contains one dimension.
6344 
6345         Returns
6346         -------
6347         dask.dataframe.DataFrame
6348         """
6349 
6350         import dask.array as da
6351         import dask.dataframe as dd
6352 
6353         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6354 
6355         columns = list(ordered_dims)
6356         columns.extend(k for k in self.coords if k not in self.dims)
6357         columns.extend(self.data_vars)
6358 
6359         series_list = []
6360         for name in columns:
6361             try:
6362                 var = self.variables[name]
6363             except KeyError:
6364                 # dimension without a matching coordinate
6365                 size = self.dims[name]
6366                 data = da.arange(size, chunks=size, dtype=np.int64)
6367                 var = Variable((name,), data)
6368 
6369             # IndexVariable objects have a dummy .chunk() method
6370             if isinstance(var, IndexVariable):
6371                 var = var.to_base_variable()
6372 
6373             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6374             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6375             series_list.append(series)
6376 
6377         df = dd.concat(series_list, axis=1)
6378 
6379         if set_index:
6380             dim_order = [*ordered_dims]
6381 
6382             if len(dim_order) == 1:
6383                 (dim,) = dim_order
6384                 df = df.set_index(dim)
6385             else:
6386                 # triggers an error about multi-indexes, even if only one
6387                 # dimension is passed
6388                 df = df.set_index(dim_order)
6389 
6390         return df
6391 
6392     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6393         """
6394         Convert this dataset to a dictionary following xarray naming
6395         conventions.
6396 
6397         Converts all variables and attributes to native Python objects
6398         Useful for converting to json. To avoid datetime incompatibility
6399         use decode_times=False kwarg in xarrray.open_dataset.
6400 
6401         Parameters
6402         ----------
6403         data : bool, default: True
6404             Whether to include the actual data in the dictionary. When set to
6405             False, returns just the schema.
6406         encoding : bool, default: False
6407             Whether to include the Dataset's encoding in the dictionary.
6408 
6409         Returns
6410         -------
6411         d : dict
6412             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6413             "encoding".
6414 
6415         See Also
6416         --------
6417         Dataset.from_dict
6418         DataArray.to_dict
6419         """
6420         d: dict = {
6421             "coords": {},
6422             "attrs": decode_numpy_dict_values(self.attrs),
6423             "dims": dict(self.dims),
6424             "data_vars": {},
6425         }
6426         for k in self.coords:
6427             d["coords"].update(
6428                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6429             )
6430         for k in self.data_vars:
6431             d["data_vars"].update(
6432                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6433             )
6434         if encoding:
6435             d["encoding"] = dict(self.encoding)
6436         return d
6437 
6438     @classmethod
6439     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6440         """Convert a dictionary into an xarray.Dataset.
6441 
6442         Parameters
6443         ----------
6444         d : dict-like
6445             Mapping with a minimum structure of
6446                 ``{"var_0": {"dims": [..], "data": [..]}, \
6447                             ...}``
6448 
6449         Returns
6450         -------
6451         obj : Dataset
6452 
6453         See also
6454         --------
6455         Dataset.to_dict
6456         DataArray.from_dict
6457 
6458         Examples
6459         --------
6460         >>> d = {
6461         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6462         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6463         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6464         ... }
6465         >>> ds = xr.Dataset.from_dict(d)
6466         >>> ds
6467         <xarray.Dataset>
6468         Dimensions:  (t: 3)
6469         Coordinates:
6470           * t        (t) int64 0 1 2
6471         Data variables:
6472             a        (t) <U1 'a' 'b' 'c'
6473             b        (t) int64 10 20 30
6474 
6475         >>> d = {
6476         ...     "coords": {
6477         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6478         ...     },
6479         ...     "attrs": {"title": "air temperature"},
6480         ...     "dims": "t",
6481         ...     "data_vars": {
6482         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6483         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6484         ...     },
6485         ... }
6486         >>> ds = xr.Dataset.from_dict(d)
6487         >>> ds
6488         <xarray.Dataset>
6489         Dimensions:  (t: 3)
6490         Coordinates:
6491           * t        (t) int64 0 1 2
6492         Data variables:
6493             a        (t) int64 10 20 30
6494             b        (t) <U1 'a' 'b' 'c'
6495         Attributes:
6496             title:    air temperature
6497 
6498         """
6499 
6500         variables: Iterable[tuple[Hashable, Any]]
6501         if not {"coords", "data_vars"}.issubset(set(d)):
6502             variables = d.items()
6503         else:
6504             import itertools
6505 
6506             variables = itertools.chain(
6507                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6508             )
6509         try:
6510             variable_dict = {
6511                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6512             }
6513         except KeyError as e:
6514             raise ValueError(
6515                 "cannot convert dict without the key "
6516                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6517             )
6518         obj = cls(variable_dict)
6519 
6520         # what if coords aren't dims?
6521         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6522         obj = obj.set_coords(coords)
6523 
6524         obj.attrs.update(d.get("attrs", {}))
6525         obj.encoding.update(d.get("encoding", {}))
6526 
6527         return obj
6528 
6529     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6530         variables = {}
6531         keep_attrs = kwargs.pop("keep_attrs", None)
6532         if keep_attrs is None:
6533             keep_attrs = _get_keep_attrs(default=True)
6534         for k, v in self._variables.items():
6535             if k in self._coord_names:
6536                 variables[k] = v
6537             else:
6538                 variables[k] = f(v, *args, **kwargs)
6539                 if keep_attrs:
6540                     variables[k].attrs = v._attrs
6541         attrs = self._attrs if keep_attrs else None
6542         return self._replace_with_new_dims(variables, attrs=attrs)
6543 
6544     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6545         from .dataarray import DataArray
6546         from .groupby import GroupBy
6547 
6548         if isinstance(other, GroupBy):
6549             return NotImplemented
6550         align_type = OPTIONS["arithmetic_join"] if join is None else join
6551         if isinstance(other, (DataArray, Dataset)):
6552             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6553         g = f if not reflexive else lambda x, y: f(y, x)
6554         ds = self._calculate_binary_op(g, other, join=align_type)
6555         return ds
6556 
6557     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6558         from .dataarray import DataArray
6559         from .groupby import GroupBy
6560 
6561         if isinstance(other, GroupBy):
6562             raise TypeError(
6563                 "in-place operations between a Dataset and "
6564                 "a grouped object are not permitted"
6565             )
6566         # we don't actually modify arrays in-place with in-place Dataset
6567         # arithmetic -- this lets us automatically align things
6568         if isinstance(other, (DataArray, Dataset)):
6569             other = other.reindex_like(self, copy=False)
6570         g = ops.inplace_to_noninplace_op(f)
6571         ds = self._calculate_binary_op(g, other, inplace=True)
6572         self._replace_with_new_dims(
6573             ds._variables,
6574             ds._coord_names,
6575             attrs=ds._attrs,
6576             indexes=ds._indexes,
6577             inplace=True,
6578         )
6579         return self
6580 
6581     def _calculate_binary_op(
6582         self, f, other, join="inner", inplace: bool = False
6583     ) -> Dataset:
6584         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6585             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6586                 raise ValueError(
6587                     "datasets must have the same data variables "
6588                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6589                 )
6590 
6591             dest_vars = {}
6592 
6593             for k in lhs_data_vars:
6594                 if k in rhs_data_vars:
6595                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6596                 elif join in ["left", "outer"]:
6597                     dest_vars[k] = f(lhs_vars[k], np.nan)
6598             for k in rhs_data_vars:
6599                 if k not in dest_vars and join in ["right", "outer"]:
6600                     dest_vars[k] = f(rhs_vars[k], np.nan)
6601             return dest_vars
6602 
6603         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6604             # can't use our shortcut of doing the binary operation with
6605             # Variable objects, so apply over our data vars instead.
6606             new_data_vars = apply_over_both(
6607                 self.data_vars, other, self.data_vars, other
6608             )
6609             return type(self)(new_data_vars)
6610 
6611         other_coords: Coordinates | None = getattr(other, "coords", None)
6612         ds = self.coords.merge(other_coords)
6613 
6614         if isinstance(other, Dataset):
6615             new_vars = apply_over_both(
6616                 self.data_vars, other.data_vars, self.variables, other.variables
6617             )
6618         else:
6619             other_variable = getattr(other, "variable", other)
6620             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6621         ds._variables.update(new_vars)
6622         ds._dims = calculate_dimensions(ds._variables)
6623         return ds
6624 
6625     def _copy_attrs_from(self, other):
6626         self.attrs = other.attrs
6627         for v in other.variables:
6628             if v in self.variables:
6629                 self.variables[v].attrs = other.variables[v].attrs
6630 
6631     def diff(
6632         self: T_Dataset,
6633         dim: Hashable,
6634         n: int = 1,
6635         label: Literal["upper", "lower"] = "upper",
6636     ) -> T_Dataset:
6637         """Calculate the n-th order discrete difference along given axis.
6638 
6639         Parameters
6640         ----------
6641         dim : Hashable
6642             Dimension over which to calculate the finite difference.
6643         n : int, default: 1
6644             The number of times values are differenced.
6645         label : {"upper", "lower"}, default: "upper"
6646             The new coordinate in dimension ``dim`` will have the
6647             values of either the minuend's or subtrahend's coordinate
6648             for values 'upper' and 'lower', respectively.
6649 
6650         Returns
6651         -------
6652         difference : Dataset
6653             The n-th order finite difference of this object.
6654 
6655         Notes
6656         -----
6657         `n` matches numpy's behavior and is different from pandas' first argument named
6658         `periods`.
6659 
6660         Examples
6661         --------
6662         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6663         >>> ds.diff("x")
6664         <xarray.Dataset>
6665         Dimensions:  (x: 3)
6666         Dimensions without coordinates: x
6667         Data variables:
6668             foo      (x) int64 0 1 0
6669         >>> ds.diff("x", 2)
6670         <xarray.Dataset>
6671         Dimensions:  (x: 2)
6672         Dimensions without coordinates: x
6673         Data variables:
6674             foo      (x) int64 1 -1
6675 
6676         See Also
6677         --------
6678         Dataset.differentiate
6679         """
6680         if n == 0:
6681             return self
6682         if n < 0:
6683             raise ValueError(f"order `n` must be non-negative but got {n}")
6684 
6685         # prepare slices
6686         slice_start = {dim: slice(None, -1)}
6687         slice_end = {dim: slice(1, None)}
6688 
6689         # prepare new coordinate
6690         if label == "upper":
6691             slice_new = slice_end
6692         elif label == "lower":
6693             slice_new = slice_start
6694         else:
6695             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6696 
6697         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6698         variables = {}
6699 
6700         for name, var in self.variables.items():
6701             if name in index_vars:
6702                 variables[name] = index_vars[name]
6703             elif dim in var.dims:
6704                 if name in self.data_vars:
6705                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6706                 else:
6707                     variables[name] = var.isel(slice_new)
6708             else:
6709                 variables[name] = var
6710 
6711         difference = self._replace_with_new_dims(variables, indexes=indexes)
6712 
6713         if n > 1:
6714             return difference.diff(dim, n - 1)
6715         else:
6716             return difference
6717 
6718     def shift(
6719         self: T_Dataset,
6720         shifts: Mapping[Any, int] | None = None,
6721         fill_value: Any = xrdtypes.NA,
6722         **shifts_kwargs: int,
6723     ) -> T_Dataset:
6724 
6725         """Shift this dataset by an offset along one or more dimensions.
6726 
6727         Only data variables are moved; coordinates stay in place. This is
6728         consistent with the behavior of ``shift`` in pandas.
6729 
6730         Values shifted from beyond array bounds will appear at one end of
6731         each dimension, which are filled according to `fill_value`. For periodic
6732         offsets instead see `roll`.
6733 
6734         Parameters
6735         ----------
6736         shifts : mapping of hashable to int
6737             Integer offset to shift along each of the given dimensions.
6738             Positive offsets shift to the right; negative offsets shift to the
6739             left.
6740         fill_value : scalar or dict-like, optional
6741             Value to use for newly missing values. If a dict-like, maps
6742             variable names (including coordinates) to fill values.
6743         **shifts_kwargs
6744             The keyword arguments form of ``shifts``.
6745             One of shifts or shifts_kwargs must be provided.
6746 
6747         Returns
6748         -------
6749         shifted : Dataset
6750             Dataset with the same coordinates and attributes but shifted data
6751             variables.
6752 
6753         See Also
6754         --------
6755         roll
6756 
6757         Examples
6758         --------
6759         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6760         >>> ds.shift(x=2)
6761         <xarray.Dataset>
6762         Dimensions:  (x: 5)
6763         Dimensions without coordinates: x
6764         Data variables:
6765             foo      (x) object nan nan 'a' 'b' 'c'
6766         """
6767         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6768         invalid = [k for k in shifts if k not in self.dims]
6769         if invalid:
6770             raise ValueError(f"dimensions {invalid!r} do not exist")
6771 
6772         variables = {}
6773         for name, var in self.variables.items():
6774             if name in self.data_vars:
6775                 fill_value_ = (
6776                     fill_value.get(name, xrdtypes.NA)
6777                     if isinstance(fill_value, dict)
6778                     else fill_value
6779                 )
6780 
6781                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6782                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6783             else:
6784                 variables[name] = var
6785 
6786         return self._replace(variables)
6787 
6788     def roll(
6789         self: T_Dataset,
6790         shifts: Mapping[Any, int] | None = None,
6791         roll_coords: bool = False,
6792         **shifts_kwargs: int,
6793     ) -> T_Dataset:
6794         """Roll this dataset by an offset along one or more dimensions.
6795 
6796         Unlike shift, roll treats the given dimensions as periodic, so will not
6797         create any missing values to be filled.
6798 
6799         Also unlike shift, roll may rotate all variables, including coordinates
6800         if specified. The direction of rotation is consistent with
6801         :py:func:`numpy.roll`.
6802 
6803         Parameters
6804         ----------
6805         shifts : mapping of hashable to int, optional
6806             A dict with keys matching dimensions and values given
6807             by integers to rotate each of the given dimensions. Positive
6808             offsets roll to the right; negative offsets roll to the left.
6809         roll_coords : bool, default: False
6810             Indicates whether to roll the coordinates by the offset too.
6811         **shifts_kwargs : {dim: offset, ...}, optional
6812             The keyword arguments form of ``shifts``.
6813             One of shifts or shifts_kwargs must be provided.
6814 
6815         Returns
6816         -------
6817         rolled : Dataset
6818             Dataset with the same attributes but rolled data and coordinates.
6819 
6820         See Also
6821         --------
6822         shift
6823 
6824         Examples
6825         --------
6826         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6827         >>> ds.roll(x=2)
6828         <xarray.Dataset>
6829         Dimensions:  (x: 5)
6830         Coordinates:
6831           * x        (x) int64 0 1 2 3 4
6832         Data variables:
6833             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6834 
6835         >>> ds.roll(x=2, roll_coords=True)
6836         <xarray.Dataset>
6837         Dimensions:  (x: 5)
6838         Coordinates:
6839           * x        (x) int64 3 4 0 1 2
6840         Data variables:
6841             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6842 
6843         """
6844         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6845         invalid = [k for k in shifts if k not in self.dims]
6846         if invalid:
6847             raise ValueError(f"dimensions {invalid!r} do not exist")
6848 
6849         unrolled_vars: tuple[Hashable, ...]
6850 
6851         if roll_coords:
6852             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6853             unrolled_vars = ()
6854         else:
6855             indexes = dict(self._indexes)
6856             index_vars = dict(self.xindexes.variables)
6857             unrolled_vars = tuple(self.coords)
6858 
6859         variables = {}
6860         for k, var in self.variables.items():
6861             if k in index_vars:
6862                 variables[k] = index_vars[k]
6863             elif k not in unrolled_vars:
6864                 variables[k] = var.roll(
6865                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6866                 )
6867             else:
6868                 variables[k] = var
6869 
6870         return self._replace(variables, indexes=indexes)
6871 
6872     def sortby(
6873         self: T_Dataset,
6874         variables: Hashable | DataArray | list[Hashable | DataArray],
6875         ascending: bool = True,
6876     ) -> T_Dataset:
6877         """
6878         Sort object by labels or values (along an axis).
6879 
6880         Sorts the dataset, either along specified dimensions,
6881         or according to values of 1-D dataarrays that share dimension
6882         with calling object.
6883 
6884         If the input variables are dataarrays, then the dataarrays are aligned
6885         (via left-join) to the calling object prior to sorting by cell values.
6886         NaNs are sorted to the end, following Numpy convention.
6887 
6888         If multiple sorts along the same dimension is
6889         given, numpy's lexsort is performed along that dimension:
6890         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6891         and the FIRST key in the sequence is used as the primary sort key,
6892         followed by the 2nd key, etc.
6893 
6894         Parameters
6895         ----------
6896         variables : Hashable, DataArray, or list of hashable or DataArray
6897             1D DataArray objects or name(s) of 1D variable(s) in
6898             coords/data_vars whose values are used to sort the dataset.
6899         ascending : bool, default: True
6900             Whether to sort by ascending or descending order.
6901 
6902         Returns
6903         -------
6904         sorted : Dataset
6905             A new dataset where all the specified dims are sorted by dim
6906             labels.
6907 
6908         See Also
6909         --------
6910         DataArray.sortby
6911         numpy.sort
6912         pandas.sort_values
6913         pandas.sort_index
6914 
6915         Examples
6916         --------
6917         >>> ds = xr.Dataset(
6918         ...     {
6919         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6920         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6921         ...     },
6922         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6923         ... )
6924         >>> ds.sortby("x")
6925         <xarray.Dataset>
6926         Dimensions:  (x: 2, y: 2)
6927         Coordinates:
6928           * x        (x) <U1 'a' 'b'
6929           * y        (y) int64 1 0
6930         Data variables:
6931             A        (x, y) int64 3 4 1 2
6932             B        (x, y) int64 7 8 5 6
6933         """
6934         from .dataarray import DataArray
6935 
6936         if not isinstance(variables, list):
6937             variables = [variables]
6938         else:
6939             variables = variables
6940         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6941         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6942         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6943         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6944         vars_by_dim = defaultdict(list)
6945         for data_array in aligned_other_vars:
6946             if data_array.ndim != 1:
6947                 raise ValueError("Input DataArray is not 1-D.")
6948             (key,) = data_array.dims
6949             vars_by_dim[key].append(data_array)
6950 
6951         indices = {}
6952         for key, arrays in vars_by_dim.items():
6953             order = np.lexsort(tuple(reversed(arrays)))
6954             indices[key] = order if ascending else order[::-1]
6955         return aligned_self.isel(indices)
6956 
6957     def quantile(
6958         self: T_Dataset,
6959         q: ArrayLike,
6960         dim: str | Iterable[Hashable] | None = None,
6961         method: QUANTILE_METHODS = "linear",
6962         numeric_only: bool = False,
6963         keep_attrs: bool = None,
6964         skipna: bool = None,
6965         interpolation: QUANTILE_METHODS = None,
6966     ) -> T_Dataset:
6967         """Compute the qth quantile of the data along the specified dimension.
6968 
6969         Returns the qth quantiles(s) of the array elements for each variable
6970         in the Dataset.
6971 
6972         Parameters
6973         ----------
6974         q : float or array-like of float
6975             Quantile to compute, which must be between 0 and 1 inclusive.
6976         dim : str or Iterable of Hashable, optional
6977             Dimension(s) over which to apply quantile.
6978         method : str, default: "linear"
6979             This optional parameter specifies the interpolation method to use when the
6980             desired quantile lies between two data points. The options sorted by their R
6981             type as summarized in the H&F paper [1]_ are:
6982 
6983                 1. "inverted_cdf" (*)
6984                 2. "averaged_inverted_cdf" (*)
6985                 3. "closest_observation" (*)
6986                 4. "interpolated_inverted_cdf" (*)
6987                 5. "hazen" (*)
6988                 6. "weibull" (*)
6989                 7. "linear"  (default)
6990                 8. "median_unbiased" (*)
6991                 9. "normal_unbiased" (*)
6992 
6993             The first three methods are discontiuous.  The following discontinuous
6994             variations of the default "linear" (7.) option are also available:
6995 
6996                 * "lower"
6997                 * "higher"
6998                 * "midpoint"
6999                 * "nearest"
7000 
7001             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7002             was previously called "interpolation", renamed in accordance with numpy
7003             version 1.22.0.
7004 
7005             (*) These methods require numpy version 1.22 or newer.
7006 
7007         keep_attrs : bool, optional
7008             If True, the dataset's attributes (`attrs`) will be copied from
7009             the original object to the new one.  If False (default), the new
7010             object will be returned without attributes.
7011         numeric_only : bool, optional
7012             If True, only apply ``func`` to variables with a numeric dtype.
7013         skipna : bool, optional
7014             If True, skip missing values (as marked by NaN). By default, only
7015             skips missing values for float dtypes; other dtypes either do not
7016             have a sentinel missing value (int) or skipna=True has not been
7017             implemented (object, datetime64 or timedelta64).
7018 
7019         Returns
7020         -------
7021         quantiles : Dataset
7022             If `q` is a single quantile, then the result is a scalar for each
7023             variable in data_vars. If multiple percentiles are given, first
7024             axis of the result corresponds to the quantile and a quantile
7025             dimension is added to the return Dataset. The other dimensions are
7026             the dimensions that remain after the reduction of the array.
7027 
7028         See Also
7029         --------
7030         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7031 
7032         Examples
7033         --------
7034         >>> ds = xr.Dataset(
7035         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7036         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7037         ... )
7038         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7039         <xarray.Dataset>
7040         Dimensions:   ()
7041         Coordinates:
7042             quantile  float64 0.0
7043         Data variables:
7044             a         float64 0.7
7045         >>> ds.quantile(0, dim="x")
7046         <xarray.Dataset>
7047         Dimensions:   (y: 4)
7048         Coordinates:
7049           * y         (y) float64 1.0 1.5 2.0 2.5
7050             quantile  float64 0.0
7051         Data variables:
7052             a         (y) float64 0.7 4.2 2.6 1.5
7053         >>> ds.quantile([0, 0.5, 1])
7054         <xarray.Dataset>
7055         Dimensions:   (quantile: 3)
7056         Coordinates:
7057           * quantile  (quantile) float64 0.0 0.5 1.0
7058         Data variables:
7059             a         (quantile) float64 0.7 3.4 9.4
7060         >>> ds.quantile([0, 0.5, 1], dim="x")
7061         <xarray.Dataset>
7062         Dimensions:   (quantile: 3, y: 4)
7063         Coordinates:
7064           * y         (y) float64 1.0 1.5 2.0 2.5
7065           * quantile  (quantile) float64 0.0 0.5 1.0
7066         Data variables:
7067             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7068 
7069         References
7070         ----------
7071         .. [1] R. J. Hyndman and Y. Fan,
7072            "Sample quantiles in statistical packages,"
7073            The American Statistician, 50(4), pp. 361-365, 1996
7074         """
7075 
7076         # interpolation renamed to method in version 0.21.0
7077         # check here and in variable to avoid repeated warnings
7078         if interpolation is not None:
7079             warnings.warn(
7080                 "The `interpolation` argument to quantile was renamed to `method`.",
7081                 FutureWarning,
7082             )
7083 
7084             if method != "linear":
7085                 raise TypeError("Cannot pass interpolation and method keywords!")
7086 
7087             method = interpolation
7088 
7089         dims: set[Hashable]
7090         if isinstance(dim, str):
7091             dims = {dim}
7092         elif dim is None or dim is ...:
7093             dims = set(self.dims)
7094         else:
7095             dims = set(dim)
7096 
7097         _assert_empty(
7098             tuple(d for d in dims if d not in self.dims),
7099             "Dataset does not contain the dimensions: %s",
7100         )
7101 
7102         q = np.asarray(q, dtype=np.float64)
7103 
7104         variables = {}
7105         for name, var in self.variables.items():
7106             reduce_dims = [d for d in var.dims if d in dims]
7107             if reduce_dims or not var.dims:
7108                 if name not in self.coords:
7109                     if (
7110                         not numeric_only
7111                         or np.issubdtype(var.dtype, np.number)
7112                         or var.dtype == np.bool_
7113                     ):
7114                         variables[name] = var.quantile(
7115                             q,
7116                             dim=reduce_dims,
7117                             method=method,
7118                             keep_attrs=keep_attrs,
7119                             skipna=skipna,
7120                         )
7121 
7122             else:
7123                 variables[name] = var
7124 
7125         # construct the new dataset
7126         coord_names = {k for k in self.coords if k in variables}
7127         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7128         if keep_attrs is None:
7129             keep_attrs = _get_keep_attrs(default=False)
7130         attrs = self.attrs if keep_attrs else None
7131         new = self._replace_with_new_dims(
7132             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7133         )
7134         return new.assign_coords(quantile=q)
7135 
7136     def rank(
7137         self: T_Dataset,
7138         dim: Hashable,
7139         pct: bool = False,
7140         keep_attrs: bool | None = None,
7141     ) -> T_Dataset:
7142         """Ranks the data.
7143 
7144         Equal values are assigned a rank that is the average of the ranks that
7145         would have been otherwise assigned to all of the values within
7146         that set.
7147         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7148 
7149         NaNs in the input array are returned as NaNs.
7150 
7151         The `bottleneck` library is required.
7152 
7153         Parameters
7154         ----------
7155         dim : Hashable
7156             Dimension over which to compute rank.
7157         pct : bool, default: False
7158             If True, compute percentage ranks, otherwise compute integer ranks.
7159         keep_attrs : bool or None, optional
7160             If True, the dataset's attributes (`attrs`) will be copied from
7161             the original object to the new one.  If False, the new
7162             object will be returned without attributes.
7163 
7164         Returns
7165         -------
7166         ranked : Dataset
7167             Variables that do not depend on `dim` are dropped.
7168         """
7169         if not OPTIONS["use_bottleneck"]:
7170             raise RuntimeError(
7171                 "rank requires bottleneck to be enabled."
7172                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7173             )
7174 
7175         if dim not in self.dims:
7176             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7177 
7178         variables = {}
7179         for name, var in self.variables.items():
7180             if name in self.data_vars:
7181                 if dim in var.dims:
7182                     variables[name] = var.rank(dim, pct=pct)
7183             else:
7184                 variables[name] = var
7185 
7186         coord_names = set(self.coords)
7187         if keep_attrs is None:
7188             keep_attrs = _get_keep_attrs(default=False)
7189         attrs = self.attrs if keep_attrs else None
7190         return self._replace(variables, coord_names, attrs=attrs)
7191 
7192     def differentiate(
7193         self: T_Dataset,
7194         coord: Hashable,
7195         edge_order: Literal[1, 2] = 1,
7196         datetime_unit: DatetimeUnitOptions | None = None,
7197     ) -> T_Dataset:
7198         """ Differentiate with the second order accurate central
7199         differences.
7200 
7201         .. note::
7202             This feature is limited to simple cartesian geometry, i.e. coord
7203             must be one dimensional.
7204 
7205         Parameters
7206         ----------
7207         coord : Hashable
7208             The coordinate to be used to compute the gradient.
7209         edge_order : {1, 2}, default: 1
7210             N-th order accurate differences at the boundaries.
7211         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7212             "us", "ns", "ps", "fs", "as", None}, default: None
7213             Unit to compute gradient. Only valid for datetime coordinate.
7214 
7215         Returns
7216         -------
7217         differentiated: Dataset
7218 
7219         See also
7220         --------
7221         numpy.gradient: corresponding numpy function
7222         """
7223         from .variable import Variable
7224 
7225         if coord not in self.variables and coord not in self.dims:
7226             raise ValueError(f"Coordinate {coord} does not exist.")
7227 
7228         coord_var = self[coord].variable
7229         if coord_var.ndim != 1:
7230             raise ValueError(
7231                 "Coordinate {} must be 1 dimensional but is {}"
7232                 " dimensional".format(coord, coord_var.ndim)
7233             )
7234 
7235         dim = coord_var.dims[0]
7236         if _contains_datetime_like_objects(coord_var):
7237             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7238                 datetime_unit = cast(
7239                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7240                 )
7241             elif datetime_unit is None:
7242                 datetime_unit = "s"  # Default to seconds for cftime objects
7243             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7244 
7245         variables = {}
7246         for k, v in self.variables.items():
7247             if k in self.data_vars and dim in v.dims and k not in self.coords:
7248                 if _contains_datetime_like_objects(v):
7249                     v = v._to_numeric(datetime_unit=datetime_unit)
7250                 grad = duck_array_ops.gradient(
7251                     v.data,
7252                     coord_var.data,
7253                     edge_order=edge_order,
7254                     axis=v.get_axis_num(dim),
7255                 )
7256                 variables[k] = Variable(v.dims, grad)
7257             else:
7258                 variables[k] = v
7259         return self._replace(variables)
7260 
7261     def integrate(
7262         self: T_Dataset,
7263         coord: Hashable | Sequence[Hashable],
7264         datetime_unit: DatetimeUnitOptions = None,
7265     ) -> T_Dataset:
7266         """Integrate along the given coordinate using the trapezoidal rule.
7267 
7268         .. note::
7269             This feature is limited to simple cartesian geometry, i.e. coord
7270             must be one dimensional.
7271 
7272         Parameters
7273         ----------
7274         coord : hashable, or sequence of hashable
7275             Coordinate(s) used for the integration.
7276         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7277                         'ps', 'fs', 'as', None}, optional
7278             Specify the unit if datetime coordinate is used.
7279 
7280         Returns
7281         -------
7282         integrated : Dataset
7283 
7284         See also
7285         --------
7286         DataArray.integrate
7287         numpy.trapz : corresponding numpy function
7288 
7289         Examples
7290         --------
7291         >>> ds = xr.Dataset(
7292         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7293         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7294         ... )
7295         >>> ds
7296         <xarray.Dataset>
7297         Dimensions:  (x: 4)
7298         Coordinates:
7299           * x        (x) int64 0 1 2 3
7300             y        (x) int64 1 7 3 5
7301         Data variables:
7302             a        (x) int64 5 5 6 6
7303             b        (x) int64 1 2 1 0
7304         >>> ds.integrate("x")
7305         <xarray.Dataset>
7306         Dimensions:  ()
7307         Data variables:
7308             a        float64 16.5
7309             b        float64 3.5
7310         >>> ds.integrate("y")
7311         <xarray.Dataset>
7312         Dimensions:  ()
7313         Data variables:
7314             a        float64 20.0
7315             b        float64 4.0
7316         """
7317         if not isinstance(coord, (list, tuple)):
7318             coord = (coord,)
7319         result = self
7320         for c in coord:
7321             result = result._integrate_one(c, datetime_unit=datetime_unit)
7322         return result
7323 
7324     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7325         from .variable import Variable
7326 
7327         if coord not in self.variables and coord not in self.dims:
7328             raise ValueError(f"Coordinate {coord} does not exist.")
7329 
7330         coord_var = self[coord].variable
7331         if coord_var.ndim != 1:
7332             raise ValueError(
7333                 "Coordinate {} must be 1 dimensional but is {}"
7334                 " dimensional".format(coord, coord_var.ndim)
7335             )
7336 
7337         dim = coord_var.dims[0]
7338         if _contains_datetime_like_objects(coord_var):
7339             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7340                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7341             elif datetime_unit is None:
7342                 datetime_unit = "s"  # Default to seconds for cftime objects
7343             coord_var = coord_var._replace(
7344                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7345             )
7346 
7347         variables = {}
7348         coord_names = set()
7349         for k, v in self.variables.items():
7350             if k in self.coords:
7351                 if dim not in v.dims or cumulative:
7352                     variables[k] = v
7353                     coord_names.add(k)
7354             else:
7355                 if k in self.data_vars and dim in v.dims:
7356                     if _contains_datetime_like_objects(v):
7357                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7358                     if cumulative:
7359                         integ = duck_array_ops.cumulative_trapezoid(
7360                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7361                         )
7362                         v_dims = v.dims
7363                     else:
7364                         integ = duck_array_ops.trapz(
7365                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7366                         )
7367                         v_dims = list(v.dims)
7368                         v_dims.remove(dim)
7369                     variables[k] = Variable(v_dims, integ)
7370                 else:
7371                     variables[k] = v
7372         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7373         return self._replace_with_new_dims(
7374             variables, coord_names=coord_names, indexes=indexes
7375         )
7376 
7377     def cumulative_integrate(
7378         self: T_Dataset,
7379         coord: Hashable | Sequence[Hashable],
7380         datetime_unit: DatetimeUnitOptions = None,
7381     ) -> T_Dataset:
7382         """Integrate along the given coordinate using the trapezoidal rule.
7383 
7384         .. note::
7385             This feature is limited to simple cartesian geometry, i.e. coord
7386             must be one dimensional.
7387 
7388             The first entry of the cumulative integral of each variable is always 0, in
7389             order to keep the length of the dimension unchanged between input and
7390             output.
7391 
7392         Parameters
7393         ----------
7394         coord : hashable, or sequence of hashable
7395             Coordinate(s) used for the integration.
7396         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7397                         'ps', 'fs', 'as', None}, optional
7398             Specify the unit if datetime coordinate is used.
7399 
7400         Returns
7401         -------
7402         integrated : Dataset
7403 
7404         See also
7405         --------
7406         DataArray.cumulative_integrate
7407         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7408 
7409         Examples
7410         --------
7411         >>> ds = xr.Dataset(
7412         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7413         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7414         ... )
7415         >>> ds
7416         <xarray.Dataset>
7417         Dimensions:  (x: 4)
7418         Coordinates:
7419           * x        (x) int64 0 1 2 3
7420             y        (x) int64 1 7 3 5
7421         Data variables:
7422             a        (x) int64 5 5 6 6
7423             b        (x) int64 1 2 1 0
7424         >>> ds.cumulative_integrate("x")
7425         <xarray.Dataset>
7426         Dimensions:  (x: 4)
7427         Coordinates:
7428           * x        (x) int64 0 1 2 3
7429             y        (x) int64 1 7 3 5
7430         Data variables:
7431             a        (x) float64 0.0 5.0 10.5 16.5
7432             b        (x) float64 0.0 1.5 3.0 3.5
7433         >>> ds.cumulative_integrate("y")
7434         <xarray.Dataset>
7435         Dimensions:  (x: 4)
7436         Coordinates:
7437           * x        (x) int64 0 1 2 3
7438             y        (x) int64 1 7 3 5
7439         Data variables:
7440             a        (x) float64 0.0 30.0 8.0 20.0
7441             b        (x) float64 0.0 9.0 3.0 4.0
7442         """
7443         if not isinstance(coord, (list, tuple)):
7444             coord = (coord,)
7445         result = self
7446         for c in coord:
7447             result = result._integrate_one(
7448                 c, datetime_unit=datetime_unit, cumulative=True
7449             )
7450         return result
7451 
7452     @property
7453     def real(self: T_Dataset) -> T_Dataset:
7454         """
7455         The real part of each data variable.
7456 
7457         See Also
7458         --------
7459         numpy.ndarray.real
7460         """
7461         return self.map(lambda x: x.real, keep_attrs=True)
7462 
7463     @property
7464     def imag(self: T_Dataset) -> T_Dataset:
7465         """
7466         The imaginary part of each data variable.
7467 
7468         See Also
7469         --------
7470         numpy.ndarray.imag
7471         """
7472         return self.map(lambda x: x.imag, keep_attrs=True)
7473 
7474     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7475 
7476     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7477         """Returns a ``Dataset`` with variables that match specific conditions.
7478 
7479         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7480         containing only the variables for which all the filter tests pass.
7481         These tests are either ``key=value`` for which the attribute ``key``
7482         has the exact value ``value`` or the callable passed into
7483         ``key=callable`` returns True. The callable will be passed a single
7484         value, either the value of the attribute ``key`` or ``None`` if the
7485         DataArray does not have an attribute with the name ``key``.
7486 
7487         Parameters
7488         ----------
7489         **kwargs
7490             key : str
7491                 Attribute name.
7492             value : callable or obj
7493                 If value is a callable, it should return a boolean in the form
7494                 of bool = func(attr) where attr is da.attrs[key].
7495                 Otherwise, value will be compared to the each
7496                 DataArray's attrs[key].
7497 
7498         Returns
7499         -------
7500         new : Dataset
7501             New dataset with variables filtered by attribute.
7502 
7503         Examples
7504         --------
7505         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7506         >>> precip = 10 * np.random.rand(2, 2, 3)
7507         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7508         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7509         >>> dims = ["x", "y", "time"]
7510         >>> temp_attr = dict(standard_name="air_potential_temperature")
7511         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7512 
7513         >>> ds = xr.Dataset(
7514         ...     dict(
7515         ...         temperature=(dims, temp, temp_attr),
7516         ...         precipitation=(dims, precip, precip_attr),
7517         ...     ),
7518         ...     coords=dict(
7519         ...         lon=(["x", "y"], lon),
7520         ...         lat=(["x", "y"], lat),
7521         ...         time=pd.date_range("2014-09-06", periods=3),
7522         ...         reference_time=pd.Timestamp("2014-09-05"),
7523         ...     ),
7524         ... )
7525 
7526         Get variables matching a specific standard_name:
7527 
7528         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7529         <xarray.Dataset>
7530         Dimensions:         (x: 2, y: 2, time: 3)
7531         Coordinates:
7532             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7533             lat             (x, y) float64 42.25 42.21 42.63 42.59
7534           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7535             reference_time  datetime64[ns] 2014-09-05
7536         Dimensions without coordinates: x, y
7537         Data variables:
7538             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7539 
7540         Get all variables that have a standard_name attribute:
7541 
7542         >>> standard_name = lambda v: v is not None
7543         >>> ds.filter_by_attrs(standard_name=standard_name)
7544         <xarray.Dataset>
7545         Dimensions:         (x: 2, y: 2, time: 3)
7546         Coordinates:
7547             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7548             lat             (x, y) float64 42.25 42.21 42.63 42.59
7549           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7550             reference_time  datetime64[ns] 2014-09-05
7551         Dimensions without coordinates: x, y
7552         Data variables:
7553             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7554             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7555 
7556         """
7557         selection = []
7558         for var_name, variable in self.variables.items():
7559             has_value_flag = False
7560             for attr_name, pattern in kwargs.items():
7561                 attr_value = variable.attrs.get(attr_name)
7562                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7563                     has_value_flag = True
7564                 else:
7565                     has_value_flag = False
7566                     break
7567             if has_value_flag is True:
7568                 selection.append(var_name)
7569         return self[selection]
7570 
7571     def unify_chunks(self: T_Dataset) -> T_Dataset:
7572         """Unify chunk size along all chunked dimensions of this Dataset.
7573 
7574         Returns
7575         -------
7576         Dataset with consistent chunk sizes for all dask-array variables
7577 
7578         See Also
7579         --------
7580         dask.array.core.unify_chunks
7581         """
7582 
7583         return unify_chunks(self)[0]
7584 
7585     def map_blocks(
7586         self,
7587         func: Callable[..., T_Xarray],
7588         args: Sequence[Any] = (),
7589         kwargs: Mapping[str, Any] | None = None,
7590         template: DataArray | Dataset | None = None,
7591     ) -> T_Xarray:
7592         """
7593         Apply a function to each block of this Dataset.
7594 
7595         .. warning::
7596             This method is experimental and its signature may change.
7597 
7598         Parameters
7599         ----------
7600         func : callable
7601             User-provided function that accepts a Dataset as its first
7602             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7603             corresponding to one chunk along each chunked dimension. ``func`` will be
7604             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7605 
7606             This function must return either a single DataArray or a single Dataset.
7607 
7608             This function cannot add a new chunked dimension.
7609         args : sequence
7610             Passed to func after unpacking and subsetting any xarray objects by blocks.
7611             xarray objects in args must be aligned with obj, otherwise an error is raised.
7612         kwargs : Mapping or None
7613             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7614             subset to blocks. Passing dask collections in kwargs is not allowed.
7615         template : DataArray, Dataset or None, optional
7616             xarray object representing the final result after compute is called. If not provided,
7617             the function will be first run on mocked-up data, that looks like this object but
7618             has sizes 0, to determine properties of the returned object such as dtype,
7619             variable names, attributes, new dimensions and new indexes (if any).
7620             ``template`` must be provided if the function changes the size of existing dimensions.
7621             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7622             ``attrs`` set by ``func`` will be ignored.
7623 
7624         Returns
7625         -------
7626         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7627         function.
7628 
7629         Notes
7630         -----
7631         This function is designed for when ``func`` needs to manipulate a whole xarray object
7632         subset to each block. Each block is loaded into memory. In the more common case where
7633         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7634 
7635         If none of the variables in this object is backed by dask arrays, calling this function is
7636         equivalent to calling ``func(obj, *args, **kwargs)``.
7637 
7638         See Also
7639         --------
7640         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7641         xarray.DataArray.map_blocks
7642 
7643         Examples
7644         --------
7645         Calculate an anomaly from climatology using ``.groupby()``. Using
7646         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7647         its indices, and its methods like ``.groupby()``.
7648 
7649         >>> def calculate_anomaly(da, groupby_type="time.month"):
7650         ...     gb = da.groupby(groupby_type)
7651         ...     clim = gb.mean(dim="time")
7652         ...     return gb - clim
7653         ...
7654         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7655         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7656         >>> np.random.seed(123)
7657         >>> array = xr.DataArray(
7658         ...     np.random.rand(len(time)),
7659         ...     dims=["time"],
7660         ...     coords={"time": time, "month": month},
7661         ... ).chunk()
7662         >>> ds = xr.Dataset({"a": array})
7663         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7664         <xarray.Dataset>
7665         Dimensions:  (time: 24)
7666         Coordinates:
7667           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7668             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7669         Data variables:
7670             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7671 
7672         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7673         to the function being applied in ``xr.map_blocks()``:
7674 
7675         >>> ds.map_blocks(
7676         ...     calculate_anomaly,
7677         ...     kwargs={"groupby_type": "time.year"},
7678         ...     template=ds,
7679         ... )
7680         <xarray.Dataset>
7681         Dimensions:  (time: 24)
7682         Coordinates:
7683           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7684             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7685         Data variables:
7686             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7687         """
7688         from .parallel import map_blocks
7689 
7690         return map_blocks(func, self, args, kwargs, template)
7691 
7692     def polyfit(
7693         self: T_Dataset,
7694         dim: Hashable,
7695         deg: int,
7696         skipna: bool | None = None,
7697         rcond: float | None = None,
7698         w: Hashable | Any = None,
7699         full: bool = False,
7700         cov: bool | Literal["unscaled"] = False,
7701     ) -> T_Dataset:
7702         """
7703         Least squares polynomial fit.
7704 
7705         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7706         invalid values when `skipna = True`.
7707 
7708         Parameters
7709         ----------
7710         dim : hashable
7711             Coordinate along which to fit the polynomials.
7712         deg : int
7713             Degree of the fitting polynomial.
7714         skipna : bool or None, optional
7715             If True, removes all invalid values before fitting each 1D slices of the array.
7716             Default is True if data is stored in a dask.array or if there is any
7717             invalid values, False otherwise.
7718         rcond : float or None, optional
7719             Relative condition number to the fit.
7720         w : hashable or Any, optional
7721             Weights to apply to the y-coordinate of the sample points.
7722             Can be an array-like object or the name of a coordinate in the dataset.
7723         full : bool, default: False
7724             Whether to return the residuals, matrix rank and singular values in addition
7725             to the coefficients.
7726         cov : bool or "unscaled", default: False
7727             Whether to return to the covariance matrix in addition to the coefficients.
7728             The matrix is not scaled if `cov='unscaled'`.
7729 
7730         Returns
7731         -------
7732         polyfit_results : Dataset
7733             A single dataset which contains (for each "var" in the input dataset):
7734 
7735             [var]_polyfit_coefficients
7736                 The coefficients of the best fit for each variable in this dataset.
7737             [var]_polyfit_residuals
7738                 The residuals of the least-square computation for each variable (only included if `full=True`)
7739                 When the matrix rank is deficient, np.nan is returned.
7740             [dim]_matrix_rank
7741                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7742                 The rank is computed ignoring the NaN values that might be skipped.
7743             [dim]_singular_values
7744                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7745             [var]_polyfit_covariance
7746                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7747 
7748         Warns
7749         -----
7750         RankWarning
7751             The rank of the coefficient matrix in the least-squares fit is deficient.
7752             The warning is not raised with in-memory (not dask) data and `full=True`.
7753 
7754         See Also
7755         --------
7756         numpy.polyfit
7757         numpy.polyval
7758         xarray.polyval
7759         """
7760         from .dataarray import DataArray
7761 
7762         variables = {}
7763         skipna_da = skipna
7764 
7765         x = get_clean_interp_index(self, dim, strict=False)
7766         xname = f"{self[dim].name}_"
7767         order = int(deg) + 1
7768         lhs = np.vander(x, order)
7769 
7770         if rcond is None:
7771             rcond = (
7772                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7773             )
7774 
7775         # Weights:
7776         if w is not None:
7777             if isinstance(w, Hashable):
7778                 w = self.coords[w]
7779             w = np.asarray(w)
7780             if w.ndim != 1:
7781                 raise TypeError("Expected a 1-d array for weights.")
7782             if w.shape[0] != lhs.shape[0]:
7783                 raise TypeError(f"Expected w and {dim} to have the same length")
7784             lhs *= w[:, np.newaxis]
7785 
7786         # Scaling
7787         scale = np.sqrt((lhs * lhs).sum(axis=0))
7788         lhs /= scale
7789 
7790         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7791 
7792         rank = np.linalg.matrix_rank(lhs)
7793 
7794         if full:
7795             rank = DataArray(rank, name=xname + "matrix_rank")
7796             variables[rank.name] = rank
7797             _sing = np.linalg.svd(lhs, compute_uv=False)
7798             sing = DataArray(
7799                 _sing,
7800                 dims=(degree_dim,),
7801                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7802                 name=xname + "singular_values",
7803             )
7804             variables[sing.name] = sing
7805 
7806         for name, da in self.data_vars.items():
7807             if dim not in da.dims:
7808                 continue
7809 
7810             if is_duck_dask_array(da.data) and (
7811                 rank != order or full or skipna is None
7812             ):
7813                 # Current algorithm with dask and skipna=False neither supports
7814                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7815                 skipna_da = True
7816             elif skipna is None:
7817                 skipna_da = bool(np.any(da.isnull()))
7818 
7819             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7820             stacked_coords: dict[Hashable, DataArray] = {}
7821             if dims_to_stack:
7822                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7823                 rhs = da.transpose(dim, *dims_to_stack).stack(
7824                     {stacked_dim: dims_to_stack}
7825                 )
7826                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7827                 scale_da = scale[:, np.newaxis]
7828             else:
7829                 rhs = da
7830                 scale_da = scale
7831 
7832             if w is not None:
7833                 rhs *= w[:, np.newaxis]
7834 
7835             with warnings.catch_warnings():
7836                 if full:  # Copy np.polyfit behavior
7837                     warnings.simplefilter("ignore", np.RankWarning)
7838                 else:  # Raise only once per variable
7839                     warnings.simplefilter("once", np.RankWarning)
7840 
7841                 coeffs, residuals = duck_array_ops.least_squares(
7842                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7843                 )
7844 
7845             if isinstance(name, str):
7846                 name = f"{name}_"
7847             else:
7848                 # Thus a ReprObject => polyfit was called on a DataArray
7849                 name = ""
7850 
7851             coeffs = DataArray(
7852                 coeffs / scale_da,
7853                 dims=[degree_dim] + list(stacked_coords.keys()),
7854                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7855                 name=name + "polyfit_coefficients",
7856             )
7857             if dims_to_stack:
7858                 coeffs = coeffs.unstack(stacked_dim)
7859             variables[coeffs.name] = coeffs
7860 
7861             if full or (cov is True):
7862                 residuals = DataArray(
7863                     residuals if dims_to_stack else residuals.squeeze(),
7864                     dims=list(stacked_coords.keys()),
7865                     coords=stacked_coords,
7866                     name=name + "polyfit_residuals",
7867                 )
7868                 if dims_to_stack:
7869                     residuals = residuals.unstack(stacked_dim)
7870                 variables[residuals.name] = residuals
7871 
7872             if cov:
7873                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7874                 Vbase /= np.outer(scale, scale)
7875                 if cov == "unscaled":
7876                     fac = 1
7877                 else:
7878                     if x.shape[0] <= order:
7879                         raise ValueError(
7880                             "The number of data points must exceed order to scale the covariance matrix."
7881                         )
7882                     fac = residuals / (x.shape[0] - order)
7883                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7884                 variables[name + "polyfit_covariance"] = covariance
7885 
7886         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7887 
7888     def pad(
7889         self: T_Dataset,
7890         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7891         mode: PadModeOptions = "constant",
7892         stat_length: int
7893         | tuple[int, int]
7894         | Mapping[Any, tuple[int, int]]
7895         | None = None,
7896         constant_values: (
7897             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7898         ) = None,
7899         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7900         reflect_type: PadReflectOptions = None,
7901         **pad_width_kwargs: Any,
7902     ) -> T_Dataset:
7903         """Pad this dataset along one or more dimensions.
7904 
7905         .. warning::
7906             This function is experimental and its behaviour is likely to change
7907             especially regarding padding of dimension coordinates (or IndexVariables).
7908 
7909         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7910         coordinates will be padded with the same mode, otherwise coordinates
7911         are padded using the "constant" mode with fill_value dtypes.NA.
7912 
7913         Parameters
7914         ----------
7915         pad_width : mapping of hashable to tuple of int
7916             Mapping with the form of {dim: (pad_before, pad_after)}
7917             describing the number of values padded along each dimension.
7918             {dim: pad} is a shortcut for pad_before = pad_after = pad
7919         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7920             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7921             How to pad the DataArray (taken from numpy docs):
7922 
7923             - "constant": Pads with a constant value.
7924             - "edge": Pads with the edge values of array.
7925             - "linear_ramp": Pads with the linear ramp between end_value and the
7926               array edge value.
7927             - "maximum": Pads with the maximum value of all or part of the
7928               vector along each axis.
7929             - "mean": Pads with the mean value of all or part of the
7930               vector along each axis.
7931             - "median": Pads with the median value of all or part of the
7932               vector along each axis.
7933             - "minimum": Pads with the minimum value of all or part of the
7934               vector along each axis.
7935             - "reflect": Pads with the reflection of the vector mirrored on
7936               the first and last values of the vector along each axis.
7937             - "symmetric": Pads with the reflection of the vector mirrored
7938               along the edge of the array.
7939             - "wrap": Pads with the wrap of the vector along the axis.
7940               The first values are used to pad the end and the
7941               end values are used to pad the beginning.
7942 
7943         stat_length : int, tuple or mapping of hashable to tuple, default: None
7944             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7945             values at edge of each axis used to calculate the statistic value.
7946             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7947             statistic lengths along each dimension.
7948             ((before, after),) yields same before and after statistic lengths
7949             for each dimension.
7950             (stat_length,) or int is a shortcut for before = after = statistic
7951             length for all axes.
7952             Default is ``None``, to use the entire axis.
7953         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7954             Used in 'constant'.  The values to set the padded values for each
7955             axis.
7956             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7957             pad constants along each dimension.
7958             ``((before, after),)`` yields same before and after constants for each
7959             dimension.
7960             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7961             all dimensions.
7962             Default is 0.
7963         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7964             Used in 'linear_ramp'.  The values used for the ending value of the
7965             linear_ramp and that will form the edge of the padded array.
7966             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7967             end values along each dimension.
7968             ``((before, after),)`` yields same before and after end values for each
7969             axis.
7970             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7971             all axes.
7972             Default is 0.
7973         reflect_type : {"even", "odd", None}, optional
7974             Used in "reflect", and "symmetric".  The "even" style is the
7975             default with an unaltered reflection around the edge value.  For
7976             the "odd" style, the extended part of the array is created by
7977             subtracting the reflected values from two times the edge value.
7978         **pad_width_kwargs
7979             The keyword arguments form of ``pad_width``.
7980             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7981 
7982         Returns
7983         -------
7984         padded : Dataset
7985             Dataset with the padded coordinates and data.
7986 
7987         See Also
7988         --------
7989         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7990 
7991         Notes
7992         -----
7993         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7994         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7995         specify ``constant_values=np.nan``
7996 
7997         Padding coordinates will drop their corresponding index (if any) and will reset default
7998         indexes for dimension coordinates.
7999 
8000         Examples
8001         --------
8002         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8003         >>> ds.pad(x=(1, 2))
8004         <xarray.Dataset>
8005         Dimensions:  (x: 8)
8006         Dimensions without coordinates: x
8007         Data variables:
8008             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8009         """
8010         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8011 
8012         if mode in ("edge", "reflect", "symmetric", "wrap"):
8013             coord_pad_mode = mode
8014             coord_pad_options = {
8015                 "stat_length": stat_length,
8016                 "constant_values": constant_values,
8017                 "end_values": end_values,
8018                 "reflect_type": reflect_type,
8019             }
8020         else:
8021             coord_pad_mode = "constant"
8022             coord_pad_options = {}
8023 
8024         variables = {}
8025 
8026         # keep indexes that won't be affected by pad and drop all other indexes
8027         xindexes = self.xindexes
8028         pad_dims = set(pad_width)
8029         indexes = {}
8030         for k, idx in xindexes.items():
8031             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8032                 indexes[k] = idx
8033 
8034         for name, var in self.variables.items():
8035             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8036             if not var_pad_width:
8037                 variables[name] = var
8038             elif name in self.data_vars:
8039                 variables[name] = var.pad(
8040                     pad_width=var_pad_width,
8041                     mode=mode,
8042                     stat_length=stat_length,
8043                     constant_values=constant_values,
8044                     end_values=end_values,
8045                     reflect_type=reflect_type,
8046                 )
8047             else:
8048                 variables[name] = var.pad(
8049                     pad_width=var_pad_width,
8050                     mode=coord_pad_mode,
8051                     **coord_pad_options,  # type: ignore[arg-type]
8052                 )
8053                 # reset default index of dimension coordinates
8054                 if (name,) == var.dims:
8055                     dim_var = {name: variables[name]}
8056                     index = PandasIndex.from_variables(dim_var, options={})
8057                     index_vars = index.create_variables(dim_var)
8058                     indexes[name] = index
8059                     variables[name] = index_vars[name]
8060 
8061         return self._replace_with_new_dims(variables, indexes=indexes)
8062 
8063     def idxmin(
8064         self: T_Dataset,
8065         dim: Hashable | None = None,
8066         skipna: bool | None = None,
8067         fill_value: Any = xrdtypes.NA,
8068         keep_attrs: bool | None = None,
8069     ) -> T_Dataset:
8070         """Return the coordinate label of the minimum value along a dimension.
8071 
8072         Returns a new `Dataset` named after the dimension with the values of
8073         the coordinate labels along that dimension corresponding to minimum
8074         values along that dimension.
8075 
8076         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8077         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8078 
8079         Parameters
8080         ----------
8081         dim : Hashable, optional
8082             Dimension over which to apply `idxmin`.  This is optional for 1D
8083             variables, but required for variables with 2 or more dimensions.
8084         skipna : bool or None, optional
8085             If True, skip missing values (as marked by NaN). By default, only
8086             skips missing values for ``float``, ``complex``, and ``object``
8087             dtypes; other dtypes either do not have a sentinel missing value
8088             (``int``) or ``skipna=True`` has not been implemented
8089             (``datetime64`` or ``timedelta64``).
8090         fill_value : Any, default: NaN
8091             Value to be filled in case all of the values along a dimension are
8092             null.  By default this is NaN.  The fill value and result are
8093             automatically converted to a compatible dtype if possible.
8094             Ignored if ``skipna`` is False.
8095         keep_attrs : bool or None, optional
8096             If True, the attributes (``attrs``) will be copied from the
8097             original object to the new one. If False, the new object
8098             will be returned without attributes.
8099 
8100         Returns
8101         -------
8102         reduced : Dataset
8103             New `Dataset` object with `idxmin` applied to its data and the
8104             indicated dimension removed.
8105 
8106         See Also
8107         --------
8108         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8109 
8110         Examples
8111         --------
8112         >>> array1 = xr.DataArray(
8113         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8114         ... )
8115         >>> array2 = xr.DataArray(
8116         ...     [
8117         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8118         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8119         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8120         ...     ],
8121         ...     dims=["y", "x"],
8122         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8123         ... )
8124         >>> ds = xr.Dataset({"int": array1, "float": array2})
8125         >>> ds.min(dim="x")
8126         <xarray.Dataset>
8127         Dimensions:  (y: 3)
8128         Coordinates:
8129           * y        (y) int64 -1 0 1
8130         Data variables:
8131             int      int64 -2
8132             float    (y) float64 -2.0 -4.0 1.0
8133         >>> ds.argmin(dim="x")
8134         <xarray.Dataset>
8135         Dimensions:  (y: 3)
8136         Coordinates:
8137           * y        (y) int64 -1 0 1
8138         Data variables:
8139             int      int64 4
8140             float    (y) int64 4 0 2
8141         >>> ds.idxmin(dim="x")
8142         <xarray.Dataset>
8143         Dimensions:  (y: 3)
8144         Coordinates:
8145           * y        (y) int64 -1 0 1
8146         Data variables:
8147             int      <U1 'e'
8148             float    (y) object 'e' 'a' 'c'
8149         """
8150         return self.map(
8151             methodcaller(
8152                 "idxmin",
8153                 dim=dim,
8154                 skipna=skipna,
8155                 fill_value=fill_value,
8156                 keep_attrs=keep_attrs,
8157             )
8158         )
8159 
8160     def idxmax(
8161         self: T_Dataset,
8162         dim: Hashable | None = None,
8163         skipna: bool | None = None,
8164         fill_value: Any = xrdtypes.NA,
8165         keep_attrs: bool | None = None,
8166     ) -> T_Dataset:
8167         """Return the coordinate label of the maximum value along a dimension.
8168 
8169         Returns a new `Dataset` named after the dimension with the values of
8170         the coordinate labels along that dimension corresponding to maximum
8171         values along that dimension.
8172 
8173         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8174         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8175 
8176         Parameters
8177         ----------
8178         dim : str, optional
8179             Dimension over which to apply `idxmax`.  This is optional for 1D
8180             variables, but required for variables with 2 or more dimensions.
8181         skipna : bool or None, optional
8182             If True, skip missing values (as marked by NaN). By default, only
8183             skips missing values for ``float``, ``complex``, and ``object``
8184             dtypes; other dtypes either do not have a sentinel missing value
8185             (``int``) or ``skipna=True`` has not been implemented
8186             (``datetime64`` or ``timedelta64``).
8187         fill_value : Any, default: NaN
8188             Value to be filled in case all of the values along a dimension are
8189             null.  By default this is NaN.  The fill value and result are
8190             automatically converted to a compatible dtype if possible.
8191             Ignored if ``skipna`` is False.
8192         keep_attrs : bool or None, optional
8193             If True, the attributes (``attrs``) will be copied from the
8194             original object to the new one. If False, the new object
8195             will be returned without attributes.
8196 
8197         Returns
8198         -------
8199         reduced : Dataset
8200             New `Dataset` object with `idxmax` applied to its data and the
8201             indicated dimension removed.
8202 
8203         See Also
8204         --------
8205         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8206 
8207         Examples
8208         --------
8209         >>> array1 = xr.DataArray(
8210         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8211         ... )
8212         >>> array2 = xr.DataArray(
8213         ...     [
8214         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8215         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8216         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8217         ...     ],
8218         ...     dims=["y", "x"],
8219         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8220         ... )
8221         >>> ds = xr.Dataset({"int": array1, "float": array2})
8222         >>> ds.max(dim="x")
8223         <xarray.Dataset>
8224         Dimensions:  (y: 3)
8225         Coordinates:
8226           * y        (y) int64 -1 0 1
8227         Data variables:
8228             int      int64 2
8229             float    (y) float64 2.0 2.0 1.0
8230         >>> ds.argmax(dim="x")
8231         <xarray.Dataset>
8232         Dimensions:  (y: 3)
8233         Coordinates:
8234           * y        (y) int64 -1 0 1
8235         Data variables:
8236             int      int64 1
8237             float    (y) int64 0 2 2
8238         >>> ds.idxmax(dim="x")
8239         <xarray.Dataset>
8240         Dimensions:  (y: 3)
8241         Coordinates:
8242           * y        (y) int64 -1 0 1
8243         Data variables:
8244             int      <U1 'b'
8245             float    (y) object 'a' 'c' 'c'
8246         """
8247         return self.map(
8248             methodcaller(
8249                 "idxmax",
8250                 dim=dim,
8251                 skipna=skipna,
8252                 fill_value=fill_value,
8253                 keep_attrs=keep_attrs,
8254             )
8255         )
8256 
8257     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8258         """Indices of the minima of the member variables.
8259 
8260         If there are multiple minima, the indices of the first one found will be
8261         returned.
8262 
8263         Parameters
8264         ----------
8265         dim : Hashable, optional
8266             The dimension over which to find the minimum. By default, finds minimum over
8267             all dimensions - for now returning an int for backward compatibility, but
8268             this is deprecated, in future will be an error, since DataArray.argmin will
8269             return a dict with indices for all dimensions, which does not make sense for
8270             a Dataset.
8271         keep_attrs : bool, optional
8272             If True, the attributes (`attrs`) will be copied from the original
8273             object to the new one.  If False (default), the new object will be
8274             returned without attributes.
8275         skipna : bool, optional
8276             If True, skip missing values (as marked by NaN). By default, only
8277             skips missing values for float dtypes; other dtypes either do not
8278             have a sentinel missing value (int) or skipna=True has not been
8279             implemented (object, datetime64 or timedelta64).
8280 
8281         Returns
8282         -------
8283         result : Dataset
8284 
8285         See Also
8286         --------
8287         DataArray.argmin
8288         """
8289         if dim is None:
8290             warnings.warn(
8291                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8292                 "dim changes to return a dict of indices of each dimension, for "
8293                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8294                 "since we don't return a dict of Datasets.",
8295                 DeprecationWarning,
8296                 stacklevel=2,
8297             )
8298         if (
8299             dim is None
8300             or (not isinstance(dim, Sequence) and dim is not ...)
8301             or isinstance(dim, str)
8302         ):
8303             # Return int index if single dimension is passed, and is not part of a
8304             # sequence
8305             argmin_func = getattr(duck_array_ops, "argmin")
8306             return self.reduce(argmin_func, dim=dim, **kwargs)
8307         else:
8308             raise ValueError(
8309                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8310                 "dicts cannot be contained in a Dataset, so cannot call "
8311                 "Dataset.argmin() with a sequence or ... for dim"
8312             )
8313 
8314     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8315         """Indices of the maxima of the member variables.
8316 
8317         If there are multiple maxima, the indices of the first one found will be
8318         returned.
8319 
8320         Parameters
8321         ----------
8322         dim : str, optional
8323             The dimension over which to find the maximum. By default, finds maximum over
8324             all dimensions - for now returning an int for backward compatibility, but
8325             this is deprecated, in future will be an error, since DataArray.argmax will
8326             return a dict with indices for all dimensions, which does not make sense for
8327             a Dataset.
8328         keep_attrs : bool, optional
8329             If True, the attributes (`attrs`) will be copied from the original
8330             object to the new one.  If False (default), the new object will be
8331             returned without attributes.
8332         skipna : bool, optional
8333             If True, skip missing values (as marked by NaN). By default, only
8334             skips missing values for float dtypes; other dtypes either do not
8335             have a sentinel missing value (int) or skipna=True has not been
8336             implemented (object, datetime64 or timedelta64).
8337 
8338         Returns
8339         -------
8340         result : Dataset
8341 
8342         See Also
8343         --------
8344         DataArray.argmax
8345 
8346         """
8347         if dim is None:
8348             warnings.warn(
8349                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8350                 "dim changes to return a dict of indices of each dimension, for "
8351                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8352                 "since we don't return a dict of Datasets.",
8353                 DeprecationWarning,
8354                 stacklevel=2,
8355             )
8356         if (
8357             dim is None
8358             or (not isinstance(dim, Sequence) and dim is not ...)
8359             or isinstance(dim, str)
8360         ):
8361             # Return int index if single dimension is passed, and is not part of a
8362             # sequence
8363             argmax_func = getattr(duck_array_ops, "argmax")
8364             return self.reduce(argmax_func, dim=dim, **kwargs)
8365         else:
8366             raise ValueError(
8367                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8368                 "dicts cannot be contained in a Dataset, so cannot call "
8369                 "Dataset.argmin() with a sequence or ... for dim"
8370             )
8371 
8372     def query(
8373         self: T_Dataset,
8374         queries: Mapping[Any, Any] | None = None,
8375         parser: QueryParserOptions = "pandas",
8376         engine: QueryEngineOptions = None,
8377         missing_dims: ErrorOptionsWithWarn = "raise",
8378         **queries_kwargs: Any,
8379     ) -> T_Dataset:
8380         """Return a new dataset with each array indexed along the specified
8381         dimension(s), where the indexers are given as strings containing
8382         Python expressions to be evaluated against the data variables in the
8383         dataset.
8384 
8385         Parameters
8386         ----------
8387         queries : dict-like, optional
8388             A dict-like with keys matching dimensions and values given by strings
8389             containing Python expressions to be evaluated against the data variables
8390             in the dataset. The expressions will be evaluated using the pandas
8391             eval() function, and can contain any valid Python expressions but cannot
8392             contain any Python statements.
8393         parser : {"pandas", "python"}, default: "pandas"
8394             The parser to use to construct the syntax tree from the expression.
8395             The default of 'pandas' parses code slightly different than standard
8396             Python. Alternatively, you can parse an expression using the 'python'
8397             parser to retain strict Python semantics.
8398         engine : {"python", "numexpr", None}, default: None
8399             The engine used to evaluate the expression. Supported engines are:
8400 
8401             - None: tries to use numexpr, falls back to python
8402             - "numexpr": evaluates expressions using numexpr
8403             - "python": performs operations as if you had evald in top level python
8404 
8405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8406             What to do if dimensions that should be selected from are not present in the
8407             Dataset:
8408 
8409             - "raise": raise an exception
8410             - "warn": raise a warning, and ignore the missing dimensions
8411             - "ignore": ignore the missing dimensions
8412 
8413         **queries_kwargs : {dim: query, ...}, optional
8414             The keyword arguments form of ``queries``.
8415             One of queries or queries_kwargs must be provided.
8416 
8417         Returns
8418         -------
8419         obj : Dataset
8420             A new Dataset with the same contents as this dataset, except each
8421             array and dimension is indexed by the results of the appropriate
8422             queries.
8423 
8424         See Also
8425         --------
8426         Dataset.isel
8427         pandas.eval
8428 
8429         Examples
8430         --------
8431         >>> a = np.arange(0, 5, 1)
8432         >>> b = np.linspace(0, 1, 5)
8433         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8434         >>> ds
8435         <xarray.Dataset>
8436         Dimensions:  (x: 5)
8437         Dimensions without coordinates: x
8438         Data variables:
8439             a        (x) int64 0 1 2 3 4
8440             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8441         >>> ds.query(x="a > 2")
8442         <xarray.Dataset>
8443         Dimensions:  (x: 2)
8444         Dimensions without coordinates: x
8445         Data variables:
8446             a        (x) int64 3 4
8447             b        (x) float64 0.75 1.0
8448         """
8449 
8450         # allow queries to be given either as a dict or as kwargs
8451         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8452 
8453         # check queries
8454         for dim, expr in queries.items():
8455             if not isinstance(expr, str):
8456                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8457                 raise ValueError(msg)
8458 
8459         # evaluate the queries to create the indexers
8460         indexers = {
8461             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8462             for dim, expr in queries.items()
8463         }
8464 
8465         # apply the selection
8466         return self.isel(indexers, missing_dims=missing_dims)
8467 
8468     def curvefit(
8469         self: T_Dataset,
8470         coords: str | DataArray | Iterable[str | DataArray],
8471         func: Callable[..., Any],
8472         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8473         skipna: bool = True,
8474         p0: dict[str, Any] | None = None,
8475         bounds: dict[str, Any] | None = None,
8476         param_names: Sequence[str] | None = None,
8477         kwargs: dict[str, Any] | None = None,
8478     ) -> T_Dataset:
8479         """
8480         Curve fitting optimization for arbitrary functions.
8481 
8482         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8483 
8484         Parameters
8485         ----------
8486         coords : hashable, DataArray, or sequence of hashable or DataArray
8487             Independent coordinate(s) over which to perform the curve fitting. Must share
8488             at least one dimension with the calling object. When fitting multi-dimensional
8489             functions, supply `coords` as a sequence in the same order as arguments in
8490             `func`. To fit along existing dimensions of the calling object, `coords` can
8491             also be specified as a str or sequence of strs.
8492         func : callable
8493             User specified function in the form `f(x, *params)` which returns a numpy
8494             array of length `len(x)`. `params` are the fittable parameters which are optimized
8495             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8496             coordinates, e.g. `f((x0, x1), *params)`.
8497         reduce_dims : hashable or sequence of hashable
8498             Additional dimension(s) over which to aggregate while fitting. For example,
8499             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8500             aggregate all lat and lon points and fit the specified function along the
8501             time dimension.
8502         skipna : bool, default: True
8503             Whether to skip missing values when fitting. Default is True.
8504         p0 : dict-like, optional
8505             Optional dictionary of parameter names to initial guesses passed to the
8506             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8507             be assigned initial values following the default scipy behavior.
8508         bounds : dict-like, optional
8509             Optional dictionary of parameter names to bounding values passed to the
8510             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8511             will be unbounded following the default scipy behavior.
8512         param_names : sequence of hashable, optional
8513             Sequence of names for the fittable parameters of `func`. If not supplied,
8514             this will be automatically determined by arguments of `func`. `param_names`
8515             should be manually supplied when fitting a function that takes a variable
8516             number of parameters.
8517         **kwargs : optional
8518             Additional keyword arguments to passed to scipy curve_fit.
8519 
8520         Returns
8521         -------
8522         curvefit_results : Dataset
8523             A single dataset which contains:
8524 
8525             [var]_curvefit_coefficients
8526                 The coefficients of the best fit.
8527             [var]_curvefit_covariance
8528                 The covariance matrix of the coefficient estimates.
8529 
8530         See Also
8531         --------
8532         Dataset.polyfit
8533         scipy.optimize.curve_fit
8534         """
8535         from scipy.optimize import curve_fit
8536 
8537         from .alignment import broadcast
8538         from .computation import apply_ufunc
8539         from .dataarray import _THIS_ARRAY, DataArray
8540 
8541         if p0 is None:
8542             p0 = {}
8543         if bounds is None:
8544             bounds = {}
8545         if kwargs is None:
8546             kwargs = {}
8547 
8548         if not reduce_dims:
8549             reduce_dims_ = []
8550         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8551             reduce_dims_ = [reduce_dims]
8552         else:
8553             reduce_dims_ = list(reduce_dims)
8554 
8555         if (
8556             isinstance(coords, str)
8557             or isinstance(coords, DataArray)
8558             or not isinstance(coords, Iterable)
8559         ):
8560             coords = [coords]
8561         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8562 
8563         # Determine whether any coords are dims on self
8564         for coord in coords_:
8565             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8566         reduce_dims_ = list(set(reduce_dims_))
8567         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8568         if not reduce_dims_:
8569             raise ValueError(
8570                 "No arguments to `coords` were identified as a dimension on the calling "
8571                 "object, and no dims were supplied to `reduce_dims`. This would result "
8572                 "in fitting on scalar data."
8573             )
8574 
8575         # Broadcast all coords with each other
8576         coords_ = broadcast(*coords_)
8577         coords_ = [
8578             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8579         ]
8580 
8581         params, func_args = _get_func_args(func, param_names)
8582         param_defaults, bounds_defaults = _initialize_curvefit_params(
8583             params, p0, bounds, func_args
8584         )
8585         n_params = len(params)
8586         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8587         kwargs.setdefault(
8588             "bounds",
8589             [
8590                 [bounds_defaults[p][0] for p in params],
8591                 [bounds_defaults[p][1] for p in params],
8592             ],
8593         )
8594 
8595         def _wrapper(Y, *coords_, **kwargs):
8596             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8597             x = np.vstack([c.ravel() for c in coords_])
8598             y = Y.ravel()
8599             if skipna:
8600                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8601                 x = x[:, mask]
8602                 y = y[mask]
8603                 if not len(y):
8604                     popt = np.full([n_params], np.nan)
8605                     pcov = np.full([n_params, n_params], np.nan)
8606                     return popt, pcov
8607             x = np.squeeze(x)
8608             popt, pcov = curve_fit(func, x, y, **kwargs)
8609             return popt, pcov
8610 
8611         result = type(self)()
8612         for name, da in self.data_vars.items():
8613             if name is _THIS_ARRAY:
8614                 name = ""
8615             else:
8616                 name = f"{str(name)}_"
8617 
8618             popt, pcov = apply_ufunc(
8619                 _wrapper,
8620                 da,
8621                 *coords_,
8622                 vectorize=True,
8623                 dask="parallelized",
8624                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8625                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8626                 dask_gufunc_kwargs={
8627                     "output_sizes": {
8628                         "param": n_params,
8629                         "cov_i": n_params,
8630                         "cov_j": n_params,
8631                     },
8632                 },
8633                 output_dtypes=(np.float64, np.float64),
8634                 exclude_dims=set(reduce_dims_),
8635                 kwargs=kwargs,
8636             )
8637             result[name + "curvefit_coefficients"] = popt
8638             result[name + "curvefit_covariance"] = pcov
8639 
8640         result = result.assign_coords(
8641             {"param": params, "cov_i": params, "cov_j": params}
8642         )
8643         result.attrs = self.attrs.copy()
8644 
8645         return result
8646 
8647     def drop_duplicates(
8648         self: T_Dataset,
8649         dim: Hashable | Iterable[Hashable],
8650         keep: Literal["first", "last", False] = "first",
8651     ) -> T_Dataset:
8652         """Returns a new Dataset with duplicate dimension values removed.
8653 
8654         Parameters
8655         ----------
8656         dim : dimension label or labels
8657             Pass `...` to drop duplicates along all dimensions.
8658         keep : {"first", "last", False}, default: "first"
8659             Determines which duplicates (if any) to keep.
8660             - ``"first"`` : Drop duplicates except for the first occurrence.
8661             - ``"last"`` : Drop duplicates except for the last occurrence.
8662             - False : Drop all duplicates.
8663 
8664         Returns
8665         -------
8666         Dataset
8667 
8668         See Also
8669         --------
8670         DataArray.drop_duplicates
8671         """
8672         if isinstance(dim, str):
8673             dims: Iterable = (dim,)
8674         elif dim is ...:
8675             dims = self.dims
8676         elif not isinstance(dim, Iterable):
8677             dims = [dim]
8678         else:
8679             dims = dim
8680 
8681         missing_dims = set(dims) - set(self.dims)
8682         if missing_dims:
8683             raise ValueError(f"'{missing_dims}' not found in dimensions")
8684 
8685         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8686         return self.isel(indexes)
8687 
8688     def convert_calendar(
8689         self: T_Dataset,
8690         calendar: CFCalendar,
8691         dim: Hashable = "time",
8692         align_on: Literal["date", "year", None] = None,
8693         missing: Any | None = None,
8694         use_cftime: bool | None = None,
8695     ) -> T_Dataset:
8696         """Convert the Dataset to another calendar.
8697 
8698         Only converts the individual timestamps, does not modify any data except
8699         in dropping invalid/surplus dates or inserting missing dates.
8700 
8701         If the source and target calendars are either no_leap, all_leap or a
8702         standard type, only the type of the time array is modified.
8703         When converting to a leap year from a non-leap year, the 29th of February
8704         is removed from the array. In the other direction the 29th of February
8705         will be missing in the output, unless `missing` is specified,
8706         in which case that value is inserted.
8707 
8708         For conversions involving `360_day` calendars, see Notes.
8709 
8710         This method is safe to use with sub-daily data as it doesn't touch the
8711         time part of the timestamps.
8712 
8713         Parameters
8714         ---------
8715         calendar : str
8716             The target calendar name.
8717         dim : Hashable, default: "time"
8718             Name of the time coordinate.
8719         align_on : {None, 'date', 'year'}, optional
8720             Must be specified when either source or target is a `360_day` calendar,
8721             ignored otherwise. See Notes.
8722         missing : Any or None, optional
8723             By default, i.e. if the value is None, this method will simply attempt
8724             to convert the dates in the source calendar to the same dates in the
8725             target calendar, and drop any of those that are not possible to
8726             represent.  If a value is provided, a new time coordinate will be
8727             created in the target calendar with the same frequency as the original
8728             time coordinate; for any dates that are not present in the source, the
8729             data will be filled with this value.  Note that using this mode requires
8730             that the source data have an inferable frequency; for more information
8731             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8732             target calendar combinations, this could result in many missing values, see notes.
8733         use_cftime : bool or None, optional
8734             Whether to use cftime objects in the output, only used if `calendar`
8735             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8736             If True, the new time axis uses cftime objects.
8737             If None (default), it uses :py:class:`numpy.datetime64` values if the
8738             date range permits it, and :py:class:`cftime.datetime` objects if not.
8739             If False, it uses :py:class:`numpy.datetime64`  or fails.
8740 
8741         Returns
8742         -------
8743         Dataset
8744             Copy of the dataarray with the time coordinate converted to the
8745             target calendar. If 'missing' was None (default), invalid dates in
8746             the new calendar are dropped, but missing dates are not inserted.
8747             If `missing` was given, the new data is reindexed to have a time axis
8748             with the same frequency as the source, but in the new calendar; any
8749             missing datapoints are filled with `missing`.
8750 
8751         Notes
8752         -----
8753         Passing a value to `missing` is only usable if the source's time coordinate as an
8754         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8755         if the target coordinate, generated from this frequency, has dates equivalent to the
8756         source. It is usually **not** appropriate to use this mode with:
8757 
8758         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8759         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8760             or 'mH' where 24 % m != 0).
8761 
8762         If one of the source or target calendars is `"360_day"`, `align_on` must
8763         be specified and two options are offered.
8764 
8765         - "year"
8766             The dates are translated according to their relative position in the year,
8767             ignoring their original month and day information, meaning that the
8768             missing/surplus days are added/removed at regular intervals.
8769 
8770             From a `360_day` to a standard calendar, the output will be missing the
8771             following dates (day of year in parentheses):
8772 
8773             To a leap year:
8774                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8775                 September 31st (275) and November 30th (335).
8776             To a non-leap year:
8777                 February 6th (36), April 19th (109), July 2nd (183),
8778                 September 12th (255), November 25th (329).
8779 
8780             From a standard calendar to a `"360_day"`, the following dates in the
8781             source array will be dropped:
8782 
8783             From a leap year:
8784                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8785                 September 31st (275), December 1st (336)
8786             From a non-leap year:
8787                 February 6th (37), April 20th (110), July 2nd (183),
8788                 September 13th (256), November 25th (329)
8789 
8790             This option is best used on daily and subdaily data.
8791 
8792         - "date"
8793             The month/day information is conserved and invalid dates are dropped
8794             from the output. This means that when converting from a `"360_day"` to a
8795             standard calendar, all 31st (Jan, March, May, July, August, October and
8796             December) will be missing as there is no equivalent dates in the
8797             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8798             will be dropped as there are no equivalent dates in a standard calendar.
8799 
8800             This option is best used with data on a frequency coarser than daily.
8801         """
8802         return convert_calendar(
8803             self,
8804             calendar,
8805             dim=dim,
8806             align_on=align_on,
8807             missing=missing,
8808             use_cftime=use_cftime,
8809         )
8810 
8811     def interp_calendar(
8812         self: T_Dataset,
8813         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8814         dim: Hashable = "time",
8815     ) -> T_Dataset:
8816         """Interpolates the Dataset to another calendar based on decimal year measure.
8817 
8818         Each timestamp in `source` and `target` are first converted to their decimal
8819         year equivalent then `source` is interpolated on the target coordinate.
8820         The decimal year of a timestamp is its year plus its sub-year component
8821         converted to the fraction of its year. For example "2000-03-01 12:00" is
8822         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8823 
8824         This method should only be used when the time (HH:MM:SS) information of
8825         time coordinate is not important.
8826 
8827         Parameters
8828         ----------
8829         target: DataArray or DatetimeIndex or CFTimeIndex
8830             The target time coordinate of a valid dtype
8831             (np.datetime64 or cftime objects)
8832         dim : Hashable, default: "time"
8833             The time coordinate name.
8834 
8835         Return
8836         ------
8837         DataArray
8838             The source interpolated on the decimal years of target,
8839         """
8840         return interp_calendar(self, target, dim=dim)
8841 
8842     def groupby(
8843         self,
8844         group: Hashable | DataArray | IndexVariable,
8845         squeeze: bool = True,
8846         restore_coord_dims: bool = False,
8847     ) -> DatasetGroupBy:
8848         """Returns a DatasetGroupBy object for performing grouped operations.
8849 
8850         Parameters
8851         ----------
8852         group : Hashable, DataArray or IndexVariable
8853             Array whose unique values should be used to group this array. If a
8854             string, must be the name of a variable contained in this dataset.
8855         squeeze : bool, default: True
8856             If "group" is a dimension of any arrays in this dataset, `squeeze`
8857             controls whether the subarrays have a dimension of length 1 along
8858             that dimension or if the dimension is squeezed out.
8859         restore_coord_dims : bool, default: False
8860             If True, also restore the dimension order of multi-dimensional
8861             coordinates.
8862 
8863         Returns
8864         -------
8865         grouped : DatasetGroupBy
8866             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8867             iterated over in the form of `(unique_value, grouped_array)` pairs.
8868 
8869         See Also
8870         --------
8871         Dataset.groupby_bins
8872         DataArray.groupby
8873         core.groupby.DatasetGroupBy
8874         pandas.DataFrame.groupby
8875         """
8876         from .groupby import DatasetGroupBy
8877 
8878         # While we don't generally check the type of every arg, passing
8879         # multiple dimensions as multiple arguments is common enough, and the
8880         # consequences hidden enough (strings evaluate as true) to warrant
8881         # checking here.
8882         # A future version could make squeeze kwarg only, but would face
8883         # backward-compat issues.
8884         if not isinstance(squeeze, bool):
8885             raise TypeError(
8886                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8887             )
8888 
8889         return DatasetGroupBy(
8890             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8891         )
8892 
8893     def groupby_bins(
8894         self,
8895         group: Hashable | DataArray | IndexVariable,
8896         bins: ArrayLike,
8897         right: bool = True,
8898         labels: ArrayLike | None = None,
8899         precision: int = 3,
8900         include_lowest: bool = False,
8901         squeeze: bool = True,
8902         restore_coord_dims: bool = False,
8903     ) -> DatasetGroupBy:
8904         """Returns a DatasetGroupBy object for performing grouped operations.
8905 
8906         Rather than using all unique values of `group`, the values are discretized
8907         first by applying `pandas.cut` [1]_ to `group`.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose binned values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         bins : int or array-like
8915             If bins is an int, it defines the number of equal-width bins in the
8916             range of x. However, in this case, the range of x is extended by .1%
8917             on each side to include the min or max values of x. If bins is a
8918             sequence it defines the bin edges allowing for non-uniform bin
8919             width. No extension of the range of x is done in this case.
8920         right : bool, default: True
8921             Indicates whether the bins include the rightmost edge or not. If
8922             right == True (the default), then the bins [1,2,3,4] indicate
8923             (1,2], (2,3], (3,4].
8924         labels : array-like or bool, default: None
8925             Used as labels for the resulting bins. Must be of the same length as
8926             the resulting bins. If False, string bin labels are assigned by
8927             `pandas.cut`.
8928         precision : int, default: 3
8929             The precision at which to store and display the bins labels.
8930         include_lowest : bool, default: False
8931             Whether the first interval should be left-inclusive or not.
8932         squeeze : bool, default: True
8933             If "group" is a dimension of any arrays in this dataset, `squeeze`
8934             controls whether the subarrays have a dimension of length 1 along
8935             that dimension or if the dimension is squeezed out.
8936         restore_coord_dims : bool, default: False
8937             If True, also restore the dimension order of multi-dimensional
8938             coordinates.
8939 
8940         Returns
8941         -------
8942         grouped : DatasetGroupBy
8943             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8944             iterated over in the form of `(unique_value, grouped_array)` pairs.
8945             The name of the group has the added suffix `_bins` in order to
8946             distinguish it from the original variable.
8947 
8948         See Also
8949         --------
8950         Dataset.groupby
8951         DataArray.groupby_bins
8952         core.groupby.DatasetGroupBy
8953         pandas.DataFrame.groupby
8954 
8955         References
8956         ----------
8957         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8958         """
8959         from .groupby import DatasetGroupBy
8960 
8961         return DatasetGroupBy(
8962             self,
8963             group,
8964             squeeze=squeeze,
8965             bins=bins,
8966             restore_coord_dims=restore_coord_dims,
8967             cut_kwargs={
8968                 "right": right,
8969                 "labels": labels,
8970                 "precision": precision,
8971                 "include_lowest": include_lowest,
8972             },
8973         )
8974 
8975     def weighted(self, weights: DataArray) -> DatasetWeighted:
8976         """
8977         Weighted Dataset operations.
8978 
8979         Parameters
8980         ----------
8981         weights : DataArray
8982             An array of weights associated with the values in this Dataset.
8983             Each value in the data contributes to the reduction operation
8984             according to its associated weight.
8985 
8986         Notes
8987         -----
8988         ``weights`` must be a DataArray and cannot contain missing values.
8989         Missing values can be replaced by ``weights.fillna(0)``.
8990 
8991         Returns
8992         -------
8993         core.weighted.DatasetWeighted
8994 
8995         See Also
8996         --------
8997         DataArray.weighted
8998         """
8999         from .weighted import DatasetWeighted
9000 
9001         return DatasetWeighted(self, weights)
9002 
9003     def rolling(
9004         self,
9005         dim: Mapping[Any, int] | None = None,
9006         min_periods: int | None = None,
9007         center: bool | Mapping[Any, bool] = False,
9008         **window_kwargs: int,
9009     ) -> DatasetRolling:
9010         """
9011         Rolling window object for Datasets.
9012 
9013         Parameters
9014         ----------
9015         dim : dict, optional
9016             Mapping from the dimension name to create the rolling iterator
9017             along (e.g. `time`) to its moving window size.
9018         min_periods : int or None, default: None
9019             Minimum number of observations in window required to have a value
9020             (otherwise result is NA). The default, None, is equivalent to
9021             setting min_periods equal to the size of the window.
9022         center : bool or Mapping to int, default: False
9023             Set the labels at the center of the window.
9024         **window_kwargs : optional
9025             The keyword arguments form of ``dim``.
9026             One of dim or window_kwargs must be provided.
9027 
9028         Returns
9029         -------
9030         core.rolling.DatasetRolling
9031 
9032         See Also
9033         --------
9034         core.rolling.DatasetRolling
9035         DataArray.rolling
9036         """
9037         from .rolling import DatasetRolling
9038 
9039         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9040         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9041 
9042     def coarsen(
9043         self,
9044         dim: Mapping[Any, int] | None = None,
9045         boundary: CoarsenBoundaryOptions = "exact",
9046         side: SideOptions | Mapping[Any, SideOptions] = "left",
9047         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9048         **window_kwargs: int,
9049     ) -> DatasetCoarsen:
9050         """
9051         Coarsen object for Datasets.
9052 
9053         Parameters
9054         ----------
9055         dim : mapping of hashable to int, optional
9056             Mapping from the dimension name to the window size.
9057         boundary : {"exact", "trim", "pad"}, default: "exact"
9058             If 'exact', a ValueError will be raised if dimension size is not a
9059             multiple of the window size. If 'trim', the excess entries are
9060             dropped. If 'pad', NA will be padded.
9061         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9062         coord_func : str or mapping of hashable to str, default: "mean"
9063             function (name) that is applied to the coordinates,
9064             or a mapping from coordinate name to function (name).
9065 
9066         Returns
9067         -------
9068         core.rolling.DatasetCoarsen
9069 
9070         See Also
9071         --------
9072         core.rolling.DatasetCoarsen
9073         DataArray.coarsen
9074         """
9075         from .rolling import DatasetCoarsen
9076 
9077         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9078         return DatasetCoarsen(
9079             self,
9080             dim,
9081             boundary=boundary,
9082             side=side,
9083             coord_func=coord_func,
9084         )
9085 
9086     def resample(
9087         self,
9088         indexer: Mapping[Any, str] | None = None,
9089         skipna: bool | None = None,
9090         closed: SideOptions | None = None,
9091         label: SideOptions | None = None,
9092         base: int = 0,
9093         keep_attrs: bool | None = None,
9094         loffset: datetime.timedelta | str | None = None,
9095         restore_coord_dims: bool | None = None,
9096         **indexer_kwargs: str,
9097     ) -> DatasetResample:
9098         """Returns a Resample object for performing resampling operations.
9099 
9100         Handles both downsampling and upsampling. The resampled
9101         dimension must be a datetime-like coordinate. If any intervals
9102         contain no values from the original object, they will be given
9103         the value ``NaN``.
9104 
9105         Parameters
9106         ----------
9107         indexer : Mapping of Hashable to str, optional
9108             Mapping from the dimension name to resample frequency [1]_. The
9109             dimension must be datetime-like.
9110         skipna : bool, optional
9111             Whether to skip missing values when aggregating in downsampling.
9112         closed : {"left", "right"}, optional
9113             Side of each interval to treat as closed.
9114         label : {"left", "right"}, optional
9115             Side of each interval to use for labeling.
9116         base : int, default = 0
9117             For frequencies that evenly subdivide 1 day, the "origin" of the
9118             aggregated intervals. For example, for "24H" frequency, base could
9119             range from 0 through 23.
9120         loffset : timedelta or str, optional
9121             Offset used to adjust the resampled time labels. Some pandas date
9122             offset strings are supported.
9123         restore_coord_dims : bool, optional
9124             If True, also restore the dimension order of multi-dimensional
9125             coordinates.
9126         **indexer_kwargs : str
9127             The keyword arguments form of ``indexer``.
9128             One of indexer or indexer_kwargs must be provided.
9129 
9130         Returns
9131         -------
9132         resampled : core.resample.DataArrayResample
9133             This object resampled.
9134 
9135         See Also
9136         --------
9137         DataArray.resample
9138         pandas.Series.resample
9139         pandas.DataFrame.resample
9140 
9141         References
9142         ----------
9143         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9144         """
9145         from .resample import DatasetResample
9146 
9147         return self._resample(
9148             resample_cls=DatasetResample,
9149             indexer=indexer,
9150             skipna=skipna,
9151             closed=closed,
9152             label=label,
9153             base=base,
9154             keep_attrs=keep_attrs,
9155             loffset=loffset,
9156             restore_coord_dims=restore_coord_dims,
9157             **indexer_kwargs,
9158         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 22:06:47.299 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>_replace</method>
<code>
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj

</code><intended_behavior>This method should ensure when coordinates are reassigned (especially with operations that would lead to change in '_coord_names'), '_coord_names' should be accurately updated to reflect the current state of coordinates. After assigning new coordinates, a validation check should ensure that all elements in '_coord_names' accurately represent current available coordinates. If any discrepancies are found, '_coord_names' should be corrected to ensure consistency.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the indexs type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the indexs type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def _rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Also used internally by DataArray so that the warning (if any)
3569         is raised at the right stack level.
3570         """
3571         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3572         for k in name_dict.keys():
3573             if k not in self and k not in self.dims:
3574                 raise ValueError(
3575                     f"cannot rename {k!r} because it is not a "
3576                     "variable or dimension in this dataset"
3577                 )
3578 
3579             create_dim_coord = False
3580             new_k = name_dict[k]
3581 
3582             if k in self.dims and new_k in self._coord_names:
3583                 coord_dims = self._variables[name_dict[k]].dims
3584                 if coord_dims == (k,):
3585                     create_dim_coord = True
3586             elif k in self._coord_names and new_k in self.dims:
3587                 coord_dims = self._variables[k].dims
3588                 if coord_dims == (new_k,):
3589                     create_dim_coord = True
3590 
3591             if create_dim_coord:
3592                 warnings.warn(
3593                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3594                     "anymore. Try using swap_dims instead or use set_index "
3595                     "after rename to create an indexed coordinate.",
3596                     UserWarning,
3597                     stacklevel=3,
3598                 )
3599 
3600         variables, coord_names, dims, indexes = self._rename_all(
3601             name_dict=name_dict, dims_dict=name_dict
3602         )
3603         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3604 
3605     def rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Returns a new object with renamed variables, coordinates and dimensions.
3611 
3612         Parameters
3613         ----------
3614         name_dict : dict-like, optional
3615             Dictionary whose keys are current variable, coordinate or dimension names and
3616             whose values are the desired names.
3617         **names : optional
3618             Keyword form of ``name_dict``.
3619             One of name_dict or names must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed variables, coordinates and dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename_vars
3630         Dataset.rename_dims
3631         DataArray.rename
3632         """
3633         return self._rename(name_dict=name_dict, **names)
3634 
3635     def rename_dims(
3636         self: T_Dataset,
3637         dims_dict: Mapping[Any, Hashable] | None = None,
3638         **dims: Hashable,
3639     ) -> T_Dataset:
3640         """Returns a new object with renamed dimensions only.
3641 
3642         Parameters
3643         ----------
3644         dims_dict : dict-like, optional
3645             Dictionary whose keys are current dimension names and
3646             whose values are the desired names. The desired names must
3647             not be the name of an existing dimension or Variable in the Dataset.
3648         **dims : optional
3649             Keyword form of ``dims_dict``.
3650             One of dims_dict or dims must be provided.
3651 
3652         Returns
3653         -------
3654         renamed : Dataset
3655             Dataset with renamed dimensions.
3656 
3657         See Also
3658         --------
3659         Dataset.swap_dims
3660         Dataset.rename
3661         Dataset.rename_vars
3662         DataArray.rename
3663         """
3664         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3665         for k, v in dims_dict.items():
3666             if k not in self.dims:
3667                 raise ValueError(
3668                     f"cannot rename {k!r} because it is not a "
3669                     "dimension in this dataset"
3670                 )
3671             if v in self.dims or v in self:
3672                 raise ValueError(
3673                     f"Cannot rename {k} to {v} because {v} already exists. "
3674                     "Try using swap_dims instead."
3675                 )
3676 
3677         variables, coord_names, sizes, indexes = self._rename_all(
3678             name_dict={}, dims_dict=dims_dict
3679         )
3680         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3681 
3682     def rename_vars(
3683         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3684     ) -> T_Dataset:
3685         """Returns a new object with renamed variables including coordinates
3686 
3687         Parameters
3688         ----------
3689         name_dict : dict-like, optional
3690             Dictionary whose keys are current variable or coordinate names and
3691             whose values are the desired names.
3692         **names : optional
3693             Keyword form of ``name_dict``.
3694             One of name_dict or names must be provided.
3695 
3696         Returns
3697         -------
3698         renamed : Dataset
3699             Dataset with renamed variables including coordinates
3700 
3701         See Also
3702         --------
3703         Dataset.swap_dims
3704         Dataset.rename
3705         Dataset.rename_dims
3706         DataArray.rename
3707         """
3708         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3709         for k in name_dict:
3710             if k not in self:
3711                 raise ValueError(
3712                     f"cannot rename {k!r} because it is not a "
3713                     "variable or coordinate in this dataset"
3714                 )
3715         variables, coord_names, dims, indexes = self._rename_all(
3716             name_dict=name_dict, dims_dict={}
3717         )
3718         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3719 
3720     def swap_dims(
3721         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3722     ) -> T_Dataset:
3723         """Returns a new object with swapped dimensions.
3724 
3725         Parameters
3726         ----------
3727         dims_dict : dict-like
3728             Dictionary whose keys are current dimension names and whose values
3729             are new names.
3730         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3731             The keyword arguments form of ``dims_dict``.
3732             One of dims_dict or dims_kwargs must be provided.
3733 
3734         Returns
3735         -------
3736         swapped : Dataset
3737             Dataset with swapped dimensions.
3738 
3739         Examples
3740         --------
3741         >>> ds = xr.Dataset(
3742         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3743         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3744         ... )
3745         >>> ds
3746         <xarray.Dataset>
3747         Dimensions:  (x: 2)
3748         Coordinates:
3749           * x        (x) <U1 'a' 'b'
3750             y        (x) int64 0 1
3751         Data variables:
3752             a        (x) int64 5 7
3753             b        (x) float64 0.1 2.4
3754 
3755         >>> ds.swap_dims({"x": "y"})
3756         <xarray.Dataset>
3757         Dimensions:  (y: 2)
3758         Coordinates:
3759             x        (y) <U1 'a' 'b'
3760           * y        (y) int64 0 1
3761         Data variables:
3762             a        (y) int64 5 7
3763             b        (y) float64 0.1 2.4
3764 
3765         >>> ds.swap_dims({"x": "z"})
3766         <xarray.Dataset>
3767         Dimensions:  (z: 2)
3768         Coordinates:
3769             x        (z) <U1 'a' 'b'
3770             y        (z) int64 0 1
3771         Dimensions without coordinates: z
3772         Data variables:
3773             a        (z) int64 5 7
3774             b        (z) float64 0.1 2.4
3775 
3776         See Also
3777         --------
3778         Dataset.rename
3779         DataArray.swap_dims
3780         """
3781         # TODO: deprecate this method in favor of a (less confusing)
3782         # rename_dims() method that only renames dimensions.
3783 
3784         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3785         for k, v in dims_dict.items():
3786             if k not in self.dims:
3787                 raise ValueError(
3788                     f"cannot swap from dimension {k!r} because it is "
3789                     "not an existing dimension"
3790                 )
3791             if v in self.variables and self.variables[v].dims != (k,):
3792                 raise ValueError(
3793                     f"replacement dimension {v!r} is not a 1D "
3794                     f"variable along the old dimension {k!r}"
3795                 )
3796 
3797         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3798 
3799         coord_names = self._coord_names.copy()
3800         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3801 
3802         variables: dict[Hashable, Variable] = {}
3803         indexes: dict[Hashable, Index] = {}
3804         for k, v in self.variables.items():
3805             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3806             var: Variable
3807             if k in result_dims:
3808                 var = v.to_index_variable()
3809                 var.dims = dims
3810                 if k in self._indexes:
3811                     indexes[k] = self._indexes[k]
3812                     variables[k] = var
3813                 else:
3814                     index, index_vars = create_default_index_implicit(var)
3815                     indexes.update({name: index for name in index_vars})
3816                     variables.update(index_vars)
3817                     coord_names.update(index_vars)
3818             else:
3819                 var = v.to_base_variable()
3820                 var.dims = dims
3821                 variables[k] = var
3822 
3823         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3824 
3825     # change type of self and return to T_Dataset once
3826     # https://github.com/python/mypy/issues/12846 is resolved
3827     def expand_dims(
3828         self,
3829         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3830         axis: None | int | Sequence[int] = None,
3831         **dim_kwargs: Any,
3832     ) -> Dataset:
3833         """Return a new object with an additional axis (or axes) inserted at
3834         the corresponding position in the array shape.  The new object is a
3835         view into the underlying array, not a copy.
3836 
3837         If dim is already a scalar coordinate, it will be promoted to a 1D
3838         coordinate consisting of a single value.
3839 
3840         Parameters
3841         ----------
3842         dim : hashable, sequence of hashable, mapping, or None
3843             Dimensions to include on the new variable. If provided as hashable
3844             or sequence of hashable, then dimensions are inserted with length
3845             1. If provided as a mapping, then the keys are the new dimensions
3846             and the values are either integers (giving the length of the new
3847             dimensions) or array-like (giving the coordinates of the new
3848             dimensions).
3849         axis : int, sequence of int, or None, default: None
3850             Axis position(s) where new axis is to be inserted (position(s) on
3851             the result array). If a sequence of integers is passed,
3852             multiple axes are inserted. In this case, dim arguments should be
3853             same length list. If axis=None is passed, all the axes will be
3854             inserted to the start of the result array.
3855         **dim_kwargs : int or sequence or ndarray
3856             The keywords are arbitrary dimensions being inserted and the values
3857             are either the lengths of the new dims (if int is given), or their
3858             coordinates. Note, this is an alternative to passing a dict to the
3859             dim kwarg and will only be used if dim is None.
3860 
3861         Returns
3862         -------
3863         expanded : Dataset
3864             This object, but with additional dimension(s).
3865 
3866         See Also
3867         --------
3868         DataArray.expand_dims
3869         """
3870         if dim is None:
3871             pass
3872         elif isinstance(dim, Mapping):
3873             # We're later going to modify dim in place; don't tamper with
3874             # the input
3875             dim = dict(dim)
3876         elif isinstance(dim, int):
3877             raise TypeError(
3878                 "dim should be hashable or sequence of hashables or mapping"
3879             )
3880         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3881             dim = {dim: 1}
3882         elif isinstance(dim, Sequence):
3883             if len(dim) != len(set(dim)):
3884                 raise ValueError("dims should not contain duplicate values.")
3885             dim = {d: 1 for d in dim}
3886 
3887         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3888         assert isinstance(dim, MutableMapping)
3889 
3890         if axis is None:
3891             axis = list(range(len(dim)))
3892         elif not isinstance(axis, Sequence):
3893             axis = [axis]
3894 
3895         if len(dim) != len(axis):
3896             raise ValueError("lengths of dim and axis should be identical.")
3897         for d in dim:
3898             if d in self.dims:
3899                 raise ValueError(f"Dimension {d} already exists.")
3900             if d in self._variables and not utils.is_scalar(self._variables[d]):
3901                 raise ValueError(
3902                     "{dim} already exists as coordinate or"
3903                     " variable name.".format(dim=d)
3904                 )
3905 
3906         variables: dict[Hashable, Variable] = {}
3907         indexes: dict[Hashable, Index] = dict(self._indexes)
3908         coord_names = self._coord_names.copy()
3909         # If dim is a dict, then ensure that the values are either integers
3910         # or iterables.
3911         for k, v in dim.items():
3912             if hasattr(v, "__iter__"):
3913                 # If the value for the new dimension is an iterable, then
3914                 # save the coordinates to the variables dict, and set the
3915                 # value within the dim dict to the length of the iterable
3916                 # for later use.
3917                 index = PandasIndex(v, k)
3918                 indexes[k] = index
3919                 variables.update(index.create_variables())
3920                 coord_names.add(k)
3921                 dim[k] = variables[k].size
3922             elif isinstance(v, int):
3923                 pass  # Do nothing if the dimensions value is just an int
3924             else:
3925                 raise TypeError(
3926                     "The value of new dimension {k} must be "
3927                     "an iterable or an int".format(k=k)
3928                 )
3929 
3930         for k, v in self._variables.items():
3931             if k not in dim:
3932                 if k in coord_names:  # Do not change coordinates
3933                     variables[k] = v
3934                 else:
3935                     result_ndim = len(v.dims) + len(axis)
3936                     for a in axis:
3937                         if a < -result_ndim or result_ndim - 1 < a:
3938                             raise IndexError(
3939                                 f"Axis {a} of variable {k} is out of bounds of the "
3940                                 f"expanded dimension size {result_ndim}"
3941                             )
3942 
3943                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3944                     if len(axis_pos) != len(set(axis_pos)):
3945                         raise ValueError("axis should not contain duplicate values")
3946                     # We need to sort them to make sure `axis` equals to the
3947                     # axis positions of the result array.
3948                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3949 
3950                     all_dims = list(zip(v.dims, v.shape))
3951                     for d, c in zip_axis_dim:
3952                         all_dims.insert(d, c)
3953                     variables[k] = v.set_dims(dict(all_dims))
3954             else:
3955                 if k not in variables:
3956                     # If dims includes a label of a non-dimension coordinate,
3957                     # it will be promoted to a 1D coordinate with a single value.
3958                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3959                     indexes[k] = index
3960                     variables.update(index_vars)
3961 
3962         return self._replace_with_new_dims(
3963             variables, coord_names=coord_names, indexes=indexes
3964         )
3965 
3966     # change type of self and return to T_Dataset once
3967     # https://github.com/python/mypy/issues/12846 is resolved
3968     def set_index(
3969         self,
3970         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3971         append: bool = False,
3972         **indexes_kwargs: Hashable | Sequence[Hashable],
3973     ) -> Dataset:
3974         """Set Dataset (multi-)indexes using one or more existing coordinates
3975         or variables.
3976 
3977         This legacy method is limited to pandas (multi-)indexes and
3978         1-dimensional "dimension" coordinates. See
3979         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
3980         Xarray-compatible index from one or more arbitrary coordinates.
3981 
3982         Parameters
3983         ----------
3984         indexes : {dim: index, ...}
3985             Mapping from names matching dimensions and values given
3986             by (lists of) the names of existing coordinates or variables to set
3987             as new (multi-)index.
3988         append : bool, default: False
3989             If True, append the supplied index(es) to the existing index(es).
3990             Otherwise replace the existing index(es) (default).
3991         **indexes_kwargs : optional
3992             The keyword arguments form of ``indexes``.
3993             One of indexes or indexes_kwargs must be provided.
3994 
3995         Returns
3996         -------
3997         obj : Dataset
3998             Another dataset, with this dataset's data but replaced coordinates.
3999 
4000         Examples
4001         --------
4002         >>> arr = xr.DataArray(
4003         ...     data=np.ones((2, 3)),
4004         ...     dims=["x", "y"],
4005         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4006         ... )
4007         >>> ds = xr.Dataset({"v": arr})
4008         >>> ds
4009         <xarray.Dataset>
4010         Dimensions:  (x: 2, y: 3)
4011         Coordinates:
4012           * x        (x) int64 0 1
4013           * y        (y) int64 0 1 2
4014             a        (x) int64 3 4
4015         Data variables:
4016             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4017         >>> ds.set_index(x="a")
4018         <xarray.Dataset>
4019         Dimensions:  (x: 2, y: 3)
4020         Coordinates:
4021           * x        (x) int64 3 4
4022           * y        (y) int64 0 1 2
4023         Data variables:
4024             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4025 
4026         See Also
4027         --------
4028         Dataset.reset_index
4029         Dataset.set_xindex
4030         Dataset.swap_dims
4031         """
4032         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4033 
4034         new_indexes: dict[Hashable, Index] = {}
4035         new_variables: dict[Hashable, Variable] = {}
4036         drop_indexes: set[Hashable] = set()
4037         drop_variables: set[Hashable] = set()
4038         replace_dims: dict[Hashable, Hashable] = {}
4039         all_var_names: set[Hashable] = set()
4040 
4041         for dim, _var_names in dim_coords.items():
4042             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4043                 var_names = [_var_names]
4044             else:
4045                 var_names = list(_var_names)
4046 
4047             invalid_vars = set(var_names) - set(self._variables)
4048             if invalid_vars:
4049                 raise ValueError(
4050                     ", ".join([str(v) for v in invalid_vars])
4051                     + " variable(s) do not exist"
4052                 )
4053 
4054             all_var_names.update(var_names)
4055             drop_variables.update(var_names)
4056 
4057             # drop any pre-existing index involved and its corresponding coordinates
4058             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4059             all_index_coord_names = set(index_coord_names)
4060             for k in var_names:
4061                 all_index_coord_names.update(
4062                     self.xindexes.get_all_coords(k, errors="ignore")
4063                 )
4064 
4065             drop_indexes.update(all_index_coord_names)
4066             drop_variables.update(all_index_coord_names)
4067 
4068             if len(var_names) == 1 and (not append or dim not in self._indexes):
4069                 var_name = var_names[0]
4070                 var = self._variables[var_name]
4071                 if var.dims != (dim,):
4072                     raise ValueError(
4073                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4074                         f"variable {var_name!r} that has dimensions {var.dims}"
4075                     )
4076                 idx = PandasIndex.from_variables({dim: var}, options={})
4077                 idx_vars = idx.create_variables({var_name: var})
4078 
4079                 # trick to preserve coordinate order in this case
4080                 if dim in self._coord_names:
4081                     drop_variables.remove(dim)
4082             else:
4083                 if append:
4084                     current_variables = {
4085                         k: self._variables[k] for k in index_coord_names
4086                     }
4087                 else:
4088                     current_variables = {}
4089                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4090                     dim,
4091                     current_variables,
4092                     {k: self._variables[k] for k in var_names},
4093                 )
4094                 for n in idx.index.names:
4095                     replace_dims[n] = dim
4096 
4097             new_indexes.update({k: idx for k in idx_vars})
4098             new_variables.update(idx_vars)
4099 
4100         # re-add deindexed coordinates (convert to base variables)
4101         for k in drop_variables:
4102             if (
4103                 k not in new_variables
4104                 and k not in all_var_names
4105                 and k in self._coord_names
4106             ):
4107                 new_variables[k] = self._variables[k].to_base_variable()
4108 
4109         indexes_: dict[Any, Index] = {
4110             k: v for k, v in self._indexes.items() if k not in drop_indexes
4111         }
4112         indexes_.update(new_indexes)
4113 
4114         variables = {
4115             k: v for k, v in self._variables.items() if k not in drop_variables
4116         }
4117         variables.update(new_variables)
4118 
4119         # update dimensions if necessary, GH: 3512
4120         for k, v in variables.items():
4121             if any(d in replace_dims for d in v.dims):
4122                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4123                 variables[k] = v._replace(dims=new_dims)
4124 
4125         coord_names = self._coord_names - drop_variables | set(new_variables)
4126 
4127         return self._replace_with_new_dims(
4128             variables, coord_names=coord_names, indexes=indexes_
4129         )
4130 
4131     def reset_index(
4132         self: T_Dataset,
4133         dims_or_levels: Hashable | Sequence[Hashable],
4134         drop: bool = False,
4135     ) -> T_Dataset:
4136         """Reset the specified index(es) or multi-index level(s).
4137 
4138         This legacy method is specific to pandas (multi-)indexes and
4139         1-dimensional "dimension" coordinates. See the more generic
4140         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4141         method to respectively drop and set pandas or custom indexes for
4142         arbitrary coordinates.
4143 
4144         Parameters
4145         ----------
4146         dims_or_levels : Hashable or Sequence of Hashable
4147             Name(s) of the dimension(s) and/or multi-index level(s) that will
4148             be reset.
4149         drop : bool, default: False
4150             If True, remove the specified indexes and/or multi-index levels
4151             instead of extracting them as new coordinates (default: False).
4152 
4153         Returns
4154         -------
4155         obj : Dataset
4156             Another dataset, with this dataset's data but replaced coordinates.
4157 
4158         See Also
4159         --------
4160         Dataset.set_index
4161         Dataset.set_xindex
4162         Dataset.drop_indexes
4163         """
4164         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4165             dims_or_levels = [dims_or_levels]
4166 
4167         invalid_coords = set(dims_or_levels) - set(self._indexes)
4168         if invalid_coords:
4169             raise ValueError(
4170                 f"{tuple(invalid_coords)} are not coordinates with an index"
4171             )
4172 
4173         drop_indexes: set[Hashable] = set()
4174         drop_variables: set[Hashable] = set()
4175         seen: set[Index] = set()
4176         new_indexes: dict[Hashable, Index] = {}
4177         new_variables: dict[Hashable, Variable] = {}
4178 
4179         def drop_or_convert(var_names):
4180             if drop:
4181                 drop_variables.update(var_names)
4182             else:
4183                 base_vars = {
4184                     k: self._variables[k].to_base_variable() for k in var_names
4185                 }
4186                 new_variables.update(base_vars)
4187 
4188         for name in dims_or_levels:
4189             index = self._indexes[name]
4190 
4191             if index in seen:
4192                 continue
4193             seen.add(index)
4194 
4195             idx_var_names = set(self.xindexes.get_all_coords(name))
4196             drop_indexes.update(idx_var_names)
4197 
4198             if isinstance(index, PandasMultiIndex):
4199                 # special case for pd.MultiIndex
4200                 level_names = index.index.names
4201                 keep_level_vars = {
4202                     k: self._variables[k]
4203                     for k in level_names
4204                     if k not in dims_or_levels
4205                 }
4206 
4207                 if index.dim not in dims_or_levels and keep_level_vars:
4208                     # do not drop the multi-index completely
4209                     # instead replace it by a new (multi-)index with dropped level(s)
4210                     idx = index.keep_levels(keep_level_vars)
4211                     idx_vars = idx.create_variables(keep_level_vars)
4212                     new_indexes.update({k: idx for k in idx_vars})
4213                     new_variables.update(idx_vars)
4214                     if not isinstance(idx, PandasMultiIndex):
4215                         # multi-index reduced to single index
4216                         # backward compatibility: unique level coordinate renamed to dimension
4217                         drop_variables.update(keep_level_vars)
4218                     drop_or_convert(
4219                         [k for k in level_names if k not in keep_level_vars]
4220                     )
4221                 else:
4222                     # always drop the multi-index dimension variable
4223                     drop_variables.add(index.dim)
4224                     drop_or_convert(level_names)
4225             else:
4226                 drop_or_convert(idx_var_names)
4227 
4228         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4229         indexes.update(new_indexes)
4230 
4231         variables = {
4232             k: v for k, v in self._variables.items() if k not in drop_variables
4233         }
4234         variables.update(new_variables)
4235 
4236         coord_names = self._coord_names - drop_variables
4237 
4238         return self._replace_with_new_dims(
4239             variables, coord_names=coord_names, indexes=indexes
4240         )
4241 
4242     def set_xindex(
4243         self: T_Dataset,
4244         coord_names: str | Sequence[Hashable],
4245         index_cls: type[Index] | None = None,
4246         **options,
4247     ) -> T_Dataset:
4248         """Set a new, Xarray-compatible index from one or more existing
4249         coordinate(s).
4250 
4251         Parameters
4252         ----------
4253         coord_names : str or list
4254             Name(s) of the coordinate(s) used to build the index.
4255             If several names are given, their order matters.
4256         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4257             The type of index to create. By default, try setting
4258             a ``PandasIndex`` if ``len(coord_names) == 1``,
4259             otherwise a ``PandasMultiIndex``.
4260         **options
4261             Options passed to the index constructor.
4262 
4263         Returns
4264         -------
4265         obj : Dataset
4266             Another dataset, with this dataset's data and with a new index.
4267 
4268         """
4269         # the Sequence check is required for mypy
4270         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4271             coord_names = [coord_names]
4272 
4273         if index_cls is None:
4274             if len(coord_names) == 1:
4275                 index_cls = PandasIndex
4276             else:
4277                 index_cls = PandasMultiIndex
4278         else:
4279             if not issubclass(index_cls, Index):
4280                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4281 
4282         invalid_coords = set(coord_names) - self._coord_names
4283 
4284         if invalid_coords:
4285             msg = ["invalid coordinate(s)"]
4286             no_vars = invalid_coords - set(self._variables)
4287             data_vars = invalid_coords - no_vars
4288             if no_vars:
4289                 msg.append(f"those variables don't exist: {no_vars}")
4290             if data_vars:
4291                 msg.append(
4292                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4293                 )
4294             raise ValueError("\n".join(msg))
4295 
4296         # we could be more clever here (e.g., drop-in index replacement if index
4297         # coordinates do not conflict), but let's not allow this for now
4298         indexed_coords = set(coord_names) & set(self._indexes)
4299 
4300         if indexed_coords:
4301             raise ValueError(
4302                 f"those coordinates already have an index: {indexed_coords}"
4303             )
4304 
4305         coord_vars = {name: self._variables[name] for name in coord_names}
4306 
4307         index = index_cls.from_variables(coord_vars, options=options)
4308 
4309         new_coord_vars = index.create_variables(coord_vars)
4310 
4311         # special case for setting a pandas multi-index from level coordinates
4312         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4313         # elements) coordinate
4314         if isinstance(index, PandasMultiIndex):
4315             coord_names = [index.dim] + list(coord_names)
4316 
4317         variables: dict[Hashable, Variable]
4318         indexes: dict[Hashable, Index]
4319 
4320         if len(coord_names) == 1:
4321             variables = self._variables.copy()
4322             indexes = self._indexes.copy()
4323 
4324             name = list(coord_names).pop()
4325             if name in new_coord_vars:
4326                 variables[name] = new_coord_vars[name]
4327             indexes[name] = index
4328         else:
4329             # reorder variables and indexes so that coordinates having the same
4330             # index are next to each other
4331             variables = {}
4332             for name, var in self._variables.items():
4333                 if name not in coord_names:
4334                     variables[name] = var
4335 
4336             indexes = {}
4337             for name, idx in self._indexes.items():
4338                 if name not in coord_names:
4339                     indexes[name] = idx
4340 
4341             for name in coord_names:
4342                 try:
4343                     variables[name] = new_coord_vars[name]
4344                 except KeyError:
4345                     variables[name] = self._variables[name]
4346                 indexes[name] = index
4347 
4348         return self._replace(
4349             variables=variables,
4350             coord_names=self._coord_names | set(coord_names),
4351             indexes=indexes,
4352         )
4353 
4354     def reorder_levels(
4355         self: T_Dataset,
4356         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4357         **dim_order_kwargs: Sequence[int | Hashable],
4358     ) -> T_Dataset:
4359         """Rearrange index levels using input order.
4360 
4361         Parameters
4362         ----------
4363         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4364             Mapping from names matching dimensions and values given
4365             by lists representing new level orders. Every given dimension
4366             must have a multi-index.
4367         **dim_order_kwargs : Sequence of int or Hashable, optional
4368             The keyword arguments form of ``dim_order``.
4369             One of dim_order or dim_order_kwargs must be provided.
4370 
4371         Returns
4372         -------
4373         obj : Dataset
4374             Another dataset, with this dataset's data but replaced
4375             coordinates.
4376         """
4377         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4378         variables = self._variables.copy()
4379         indexes = dict(self._indexes)
4380         new_indexes: dict[Hashable, Index] = {}
4381         new_variables: dict[Hashable, IndexVariable] = {}
4382 
4383         for dim, order in dim_order.items():
4384             index = self._indexes[dim]
4385 
4386             if not isinstance(index, PandasMultiIndex):
4387                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4388 
4389             level_vars = {k: self._variables[k] for k in order}
4390             idx = index.reorder_levels(level_vars)
4391             idx_vars = idx.create_variables(level_vars)
4392             new_indexes.update({k: idx for k in idx_vars})
4393             new_variables.update(idx_vars)
4394 
4395         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4396         indexes.update(new_indexes)
4397 
4398         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4399         variables.update(new_variables)
4400 
4401         return self._replace(variables, indexes=indexes)
4402 
4403     def _get_stack_index(
4404         self,
4405         dim,
4406         multi=False,
4407         create_index=False,
4408     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4409         """Used by stack and unstack to get one pandas (multi-)index among
4410         the indexed coordinates along dimension `dim`.
4411 
4412         If exactly one index is found, return it with its corresponding
4413         coordinate variables(s), otherwise return None and an empty dict.
4414 
4415         If `create_index=True`, create a new index if none is found or raise
4416         an error if multiple indexes are found.
4417 
4418         """
4419         stack_index: Index | None = None
4420         stack_coords: dict[Hashable, Variable] = {}
4421 
4422         for name, index in self._indexes.items():
4423             var = self._variables[name]
4424             if (
4425                 var.ndim == 1
4426                 and var.dims[0] == dim
4427                 and (
4428                     # stack: must be a single coordinate index
4429                     not multi
4430                     and not self.xindexes.is_multi(name)
4431                     # unstack: must be an index that implements .unstack
4432                     or multi
4433                     and type(index).unstack is not Index.unstack
4434                 )
4435             ):
4436                 if stack_index is not None and index is not stack_index:
4437                     # more than one index found, stop
4438                     if create_index:
4439                         raise ValueError(
4440                             f"cannot stack dimension {dim!r} with `create_index=True` "
4441                             "and with more than one index found along that dimension"
4442                         )
4443                     return None, {}
4444                 stack_index = index
4445                 stack_coords[name] = var
4446 
4447         if create_index and stack_index is None:
4448             if dim in self._variables:
4449                 var = self._variables[dim]
4450             else:
4451                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4452             # dummy index (only `stack_coords` will be used to construct the multi-index)
4453             stack_index = PandasIndex([0], dim)
4454             stack_coords = {dim: var}
4455 
4456         return stack_index, stack_coords
4457 
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )
4516 
4517     def stack(
4518         self: T_Dataset,
4519         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4520         create_index: bool | None = True,
4521         index_cls: type[Index] = PandasMultiIndex,
4522         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4523     ) -> T_Dataset:
4524         """
4525         Stack any number of existing dimensions into a single new dimension.
4526 
4527         New dimensions will be added at the end, and by default the corresponding
4528         coordinate variables will be combined into a MultiIndex.
4529 
4530         Parameters
4531         ----------
4532         dimensions : mapping of hashable to sequence of hashable
4533             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4534             dimensions, and the existing dimensions that they replace. An
4535             ellipsis (`...`) will be replaced by all unlisted dimensions.
4536             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4537             all dimensions.
4538         create_index : bool or None, default: True
4539 
4540             - True: create a multi-index for each of the stacked dimensions.
4541             - False: don't create any index.
4542             - None. create a multi-index only if exactly one single (1-d) coordinate
4543               index is found for every dimension to stack.
4544 
4545         index_cls: Index-class, default: PandasMultiIndex
4546             Can be used to pass a custom multi-index type (must be an Xarray index that
4547             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4548         **dimensions_kwargs
4549             The keyword arguments form of ``dimensions``.
4550             One of dimensions or dimensions_kwargs must be provided.
4551 
4552         Returns
4553         -------
4554         stacked : Dataset
4555             Dataset with stacked data.
4556 
4557         See Also
4558         --------
4559         Dataset.unstack
4560         """
4561         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4562         result = self
4563         for new_dim, dims in dimensions.items():
4564             result = result._stack_once(dims, new_dim, index_cls, create_index)
4565         return result
4566 
4567     def to_stacked_array(
4568         self,
4569         new_dim: Hashable,
4570         sample_dims: Collection[Hashable],
4571         variable_dim: Hashable = "variable",
4572         name: Hashable | None = None,
4573     ) -> DataArray:
4574         """Combine variables of differing dimensionality into a DataArray
4575         without broadcasting.
4576 
4577         This method is similar to Dataset.to_array but does not broadcast the
4578         variables.
4579 
4580         Parameters
4581         ----------
4582         new_dim : hashable
4583             Name of the new stacked coordinate
4584         sample_dims : Collection of hashables
4585             List of dimensions that **will not** be stacked. Each array in the
4586             dataset must share these dimensions. For machine learning
4587             applications, these define the dimensions over which samples are
4588             drawn.
4589         variable_dim : hashable, default: "variable"
4590             Name of the level in the stacked coordinate which corresponds to
4591             the variables.
4592         name : hashable, optional
4593             Name of the new data array.
4594 
4595         Returns
4596         -------
4597         stacked : DataArray
4598             DataArray with the specified dimensions and data variables
4599             stacked together. The stacked coordinate is named ``new_dim``
4600             and represented by a MultiIndex object with a level containing the
4601             data variable names. The name of this level is controlled using
4602             the ``variable_dim`` argument.
4603 
4604         See Also
4605         --------
4606         Dataset.to_array
4607         Dataset.stack
4608         DataArray.to_unstacked_dataset
4609 
4610         Examples
4611         --------
4612         >>> data = xr.Dataset(
4613         ...     data_vars={
4614         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4615         ...         "b": ("x", [6, 7]),
4616         ...     },
4617         ...     coords={"y": ["u", "v", "w"]},
4618         ... )
4619 
4620         >>> data
4621         <xarray.Dataset>
4622         Dimensions:  (x: 2, y: 3)
4623         Coordinates:
4624           * y        (y) <U1 'u' 'v' 'w'
4625         Dimensions without coordinates: x
4626         Data variables:
4627             a        (x, y) int64 0 1 2 3 4 5
4628             b        (x) int64 6 7
4629 
4630         >>> data.to_stacked_array("z", sample_dims=["x"])
4631         <xarray.DataArray 'a' (x: 2, z: 4)>
4632         array([[0, 1, 2, 6],
4633                [3, 4, 5, 7]])
4634         Coordinates:
4635           * z         (z) object MultiIndex
4636           * variable  (z) object 'a' 'a' 'a' 'b'
4637           * y         (z) object 'u' 'v' 'w' nan
4638         Dimensions without coordinates: x
4639 
4640         """
4641         from .concat import concat
4642 
4643         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4644 
4645         for variable in self:
4646             dims = self[variable].dims
4647             dims_include_sample_dims = set(sample_dims) <= set(dims)
4648             if not dims_include_sample_dims:
4649                 raise ValueError(
4650                     "All variables in the dataset must contain the "
4651                     "dimensions {}.".format(dims)
4652                 )
4653 
4654         def ensure_stackable(val):
4655             assign_coords = {variable_dim: val.name}
4656             for dim in stacking_dims:
4657                 if dim not in val.dims:
4658                     assign_coords[dim] = None
4659 
4660             expand_dims = set(stacking_dims).difference(set(val.dims))
4661             expand_dims.add(variable_dim)
4662             # must be list for .expand_dims
4663             expand_dims = list(expand_dims)
4664 
4665             return (
4666                 val.assign_coords(**assign_coords)
4667                 .expand_dims(expand_dims)
4668                 .stack({new_dim: (variable_dim,) + stacking_dims})
4669             )
4670 
4671         # concatenate the arrays
4672         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4673         data_array = concat(stackable_vars, dim=new_dim)
4674 
4675         if name is not None:
4676             data_array.name = name
4677 
4678         return data_array
4679 
4680     def _unstack_once(
4681         self: T_Dataset,
4682         dim: Hashable,
4683         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4684         fill_value,
4685         sparse: bool = False,
4686     ) -> T_Dataset:
4687         index, index_vars = index_and_vars
4688         variables: dict[Hashable, Variable] = {}
4689         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4690 
4691         new_indexes, clean_index = index.unstack()
4692         indexes.update(new_indexes)
4693 
4694         for name, idx in new_indexes.items():
4695             variables.update(idx.create_variables(index_vars))
4696 
4697         for name, var in self.variables.items():
4698             if name not in index_vars:
4699                 if dim in var.dims:
4700                     if isinstance(fill_value, Mapping):
4701                         fill_value_ = fill_value[name]
4702                     else:
4703                         fill_value_ = fill_value
4704 
4705                     variables[name] = var._unstack_once(
4706                         index=clean_index,
4707                         dim=dim,
4708                         fill_value=fill_value_,
4709                         sparse=sparse,
4710                     )
4711                 else:
4712                     variables[name] = var
4713 
4714         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4715 
4716         return self._replace_with_new_dims(
4717             variables, coord_names=coord_names, indexes=indexes
4718         )
4719 
4720     def _unstack_full_reindex(
4721         self: T_Dataset,
4722         dim: Hashable,
4723         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4724         fill_value,
4725         sparse: bool,
4726     ) -> T_Dataset:
4727         index, index_vars = index_and_vars
4728         variables: dict[Hashable, Variable] = {}
4729         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4730 
4731         new_indexes, clean_index = index.unstack()
4732         indexes.update(new_indexes)
4733 
4734         new_index_variables = {}
4735         for name, idx in new_indexes.items():
4736             new_index_variables.update(idx.create_variables(index_vars))
4737 
4738         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4739         variables.update(new_index_variables)
4740 
4741         # take a shortcut in case the MultiIndex was not modified.
4742         full_idx = pd.MultiIndex.from_product(
4743             clean_index.levels, names=clean_index.names
4744         )
4745         if clean_index.equals(full_idx):
4746             obj = self
4747         else:
4748             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4749             xr_full_idx = PandasMultiIndex(full_idx, dim)
4750             indexers = Indexes(
4751                 {k: xr_full_idx for k in index_vars},
4752                 xr_full_idx.create_variables(index_vars),
4753             )
4754             obj = self._reindex(
4755                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4756             )
4757 
4758         for name, var in obj.variables.items():
4759             if name not in index_vars:
4760                 if dim in var.dims:
4761                     variables[name] = var.unstack({dim: new_dim_sizes})
4762                 else:
4763                     variables[name] = var
4764 
4765         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4766 
4767         return self._replace_with_new_dims(
4768             variables, coord_names=coord_names, indexes=indexes
4769         )
4770 
4771     def unstack(
4772         self: T_Dataset,
4773         dim: Hashable | Iterable[Hashable] | None = None,
4774         fill_value: Any = xrdtypes.NA,
4775         sparse: bool = False,
4776     ) -> T_Dataset:
4777         """
4778         Unstack existing dimensions corresponding to MultiIndexes into
4779         multiple new dimensions.
4780 
4781         New dimensions will be added at the end.
4782 
4783         Parameters
4784         ----------
4785         dim : hashable or iterable of hashable, optional
4786             Dimension(s) over which to unstack. By default unstacks all
4787             MultiIndexes.
4788         fill_value : scalar or dict-like, default: nan
4789             value to be filled. If a dict-like, maps variable names to
4790             fill values. If not provided or if the dict-like does not
4791             contain all variables, the dtype's NA value will be used.
4792         sparse : bool, default: False
4793             use sparse-array if True
4794 
4795         Returns
4796         -------
4797         unstacked : Dataset
4798             Dataset with unstacked data.
4799 
4800         See Also
4801         --------
4802         Dataset.stack
4803         """
4804 
4805         if dim is None:
4806             dims = list(self.dims)
4807         else:
4808             if isinstance(dim, str) or not isinstance(dim, Iterable):
4809                 dims = [dim]
4810             else:
4811                 dims = list(dim)
4812 
4813             missing_dims = [d for d in dims if d not in self.dims]
4814             if missing_dims:
4815                 raise ValueError(
4816                     f"Dataset does not contain the dimensions: {missing_dims}"
4817                 )
4818 
4819         # each specified dimension must have exactly one multi-index
4820         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4821         for d in dims:
4822             idx, idx_vars = self._get_stack_index(d, multi=True)
4823             if idx is not None:
4824                 stacked_indexes[d] = idx, idx_vars
4825 
4826         if dim is None:
4827             dims = list(stacked_indexes)
4828         else:
4829             non_multi_dims = set(dims) - set(stacked_indexes)
4830             if non_multi_dims:
4831                 raise ValueError(
4832                     "cannot unstack dimensions that do not "
4833                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4834                 )
4835 
4836         result = self.copy(deep=False)
4837 
4838         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4839         # so we can't just access self.variables[v].data for every variable.
4840         # We only check the non-index variables.
4841         # https://github.com/pydata/xarray/issues/5902
4842         nonindexes = [
4843             self.variables[k] for k in set(self.variables) - set(self._indexes)
4844         ]
4845         # Notes for each of these cases:
4846         # 1. Dask arrays don't support assignment by index, which the fast unstack
4847         #    function requires.
4848         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4849         # 2. Sparse doesn't currently support (though we could special-case it)
4850         #    https://github.com/pydata/sparse/issues/422
4851         # 3. pint requires checking if it's a NumPy array until
4852         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4853         #    Once that is resolved, explicitly exclude pint arrays.
4854         #    pint doesn't implement `np.full_like` in a way that's
4855         #    currently compatible.
4856         needs_full_reindex = any(
4857             is_duck_dask_array(v.data)
4858             or isinstance(v.data, sparse_array_type)
4859             or not isinstance(v.data, np.ndarray)
4860             for v in nonindexes
4861         )
4862 
4863         for dim in dims:
4864             if needs_full_reindex:
4865                 result = result._unstack_full_reindex(
4866                     dim, stacked_indexes[dim], fill_value, sparse
4867                 )
4868             else:
4869                 result = result._unstack_once(
4870                     dim, stacked_indexes[dim], fill_value, sparse
4871                 )
4872         return result
4873 
4874     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4875         """Update this dataset's variables with those from another dataset.
4876 
4877         Just like :py:meth:`dict.update` this is a in-place operation.
4878         For a non-inplace version, see :py:meth:`Dataset.merge`.
4879 
4880         Parameters
4881         ----------
4882         other : Dataset or mapping
4883             Variables with which to update this dataset. One of:
4884 
4885             - Dataset
4886             - mapping {var name: DataArray}
4887             - mapping {var name: Variable}
4888             - mapping {var name: (dimension name, array-like)}
4889             - mapping {var name: (tuple of dimension names, array-like)}
4890 
4891         Returns
4892         -------
4893         updated : Dataset
4894             Updated dataset. Note that since the update is in-place this is the input
4895             dataset.
4896 
4897             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4898 
4899         Raises
4900         ------
4901         ValueError
4902             If any dimensions would have inconsistent sizes in the updated
4903             dataset.
4904 
4905         See Also
4906         --------
4907         Dataset.assign
4908         Dataset.merge
4909         """
4910         merge_result = dataset_update_method(self, other)
4911         return self._replace(inplace=True, **merge_result._asdict())
4912 
4913     def merge(
4914         self: T_Dataset,
4915         other: CoercibleMapping | DataArray,
4916         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4917         compat: CompatOptions = "no_conflicts",
4918         join: JoinOptions = "outer",
4919         fill_value: Any = xrdtypes.NA,
4920         combine_attrs: CombineAttrsOptions = "override",
4921     ) -> T_Dataset:
4922         """Merge the arrays of two datasets into a single dataset.
4923 
4924         This method generally does not allow for overriding data, with the
4925         exception of attributes, which are ignored on the second dataset.
4926         Variables with the same name are checked for conflicts via the equals
4927         or identical methods.
4928 
4929         Parameters
4930         ----------
4931         other : Dataset or mapping
4932             Dataset or variables to merge with this dataset.
4933         overwrite_vars : hashable or iterable of hashable, optional
4934             If provided, update variables of these name(s) without checking for
4935             conflicts in this dataset.
4936         compat : {"identical", "equals", "broadcast_equals", \
4937                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4938             String indicating how to compare variables of the same name for
4939             potential conflicts:
4940 
4941             - 'identical': all values, dimensions and attributes must be the
4942               same.
4943             - 'equals': all values and dimensions must be the same.
4944             - 'broadcast_equals': all values must be equal when variables are
4945               broadcast against each other to ensure common dimensions.
4946             - 'no_conflicts': only values which are not null in both datasets
4947               must be equal. The returned dataset then contains the combination
4948               of all non-null values.
4949             - 'override': skip comparing and pick variable from first dataset
4950             - 'minimal': drop conflicting coordinates
4951 
4952         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4953                default: "outer"
4954             Method for joining ``self`` and ``other`` along shared dimensions:
4955 
4956             - 'outer': use the union of the indexes
4957             - 'inner': use the intersection of the indexes
4958             - 'left': use indexes from ``self``
4959             - 'right': use indexes from ``other``
4960             - 'exact': error instead of aligning non-equal indexes
4961             - 'override': use indexes from ``self`` that are the same size
4962               as those of ``other`` in that dimension
4963 
4964         fill_value : scalar or dict-like, optional
4965             Value to use for newly missing values. If a dict-like, maps
4966             variable names (including coordinates) to fill values.
4967         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4968                          "override"} or callable, default: "override"
4969             A callable or a string indicating how to combine attrs of the objects being
4970             merged:
4971 
4972             - "drop": empty attrs on returned Dataset.
4973             - "identical": all attrs must be the same on every object.
4974             - "no_conflicts": attrs from all objects are combined, any that have
4975               the same name must also have the same value.
4976             - "drop_conflicts": attrs from all objects are combined, any that have
4977               the same name but different values are dropped.
4978             - "override": skip comparing and copy attrs from the first dataset to
4979               the result.
4980 
4981             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4982             as its only parameters.
4983 
4984         Returns
4985         -------
4986         merged : Dataset
4987             Merged dataset.
4988 
4989         Raises
4990         ------
4991         MergeError
4992             If any variables conflict (see ``compat``).
4993 
4994         See Also
4995         --------
4996         Dataset.update
4997         """
4998         from .dataarray import DataArray
4999 
5000         other = other.to_dataset() if isinstance(other, DataArray) else other
5001         merge_result = dataset_merge_method(
5002             self,
5003             other,
5004             overwrite_vars=overwrite_vars,
5005             compat=compat,
5006             join=join,
5007             fill_value=fill_value,
5008             combine_attrs=combine_attrs,
5009         )
5010         return self._replace(**merge_result._asdict())
5011 
5012     def _assert_all_in_dataset(
5013         self, names: Iterable[Hashable], virtual_okay: bool = False
5014     ) -> None:
5015         bad_names = set(names) - set(self._variables)
5016         if virtual_okay:
5017             bad_names -= self.virtual_variables
5018         if bad_names:
5019             raise ValueError(
5020                 "One or more of the specified variables "
5021                 "cannot be found in this dataset"
5022             )
5023 
5024     def drop_vars(
5025         self: T_Dataset,
5026         names: Hashable | Iterable[Hashable],
5027         *,
5028         errors: ErrorOptions = "raise",
5029     ) -> T_Dataset:
5030         """Drop variables from this dataset.
5031 
5032         Parameters
5033         ----------
5034         names : hashable or iterable of hashable
5035             Name(s) of variables to drop.
5036         errors : {"raise", "ignore"}, default: "raise"
5037             If 'raise', raises a ValueError error if any of the variable
5038             passed are not in the dataset. If 'ignore', any given names that are in the
5039             dataset are dropped and no error is raised.
5040 
5041         Returns
5042         -------
5043         dropped : Dataset
5044 
5045         """
5046         # the Iterable check is required for mypy
5047         if is_scalar(names) or not isinstance(names, Iterable):
5048             names = {names}
5049         else:
5050             names = set(names)
5051         if errors == "raise":
5052             self._assert_all_in_dataset(names)
5053 
5054         # GH6505
5055         other_names = set()
5056         for var in names:
5057             maybe_midx = self._indexes.get(var, None)
5058             if isinstance(maybe_midx, PandasMultiIndex):
5059                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5060                 idx_other_names = idx_coord_names - set(names)
5061                 other_names.update(idx_other_names)
5062         if other_names:
5063             names |= set(other_names)
5064             warnings.warn(
5065                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5066                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5067                 DeprecationWarning,
5068                 stacklevel=2,
5069             )
5070 
5071         assert_no_index_corrupted(self.xindexes, names)
5072 
5073         variables = {k: v for k, v in self._variables.items() if k not in names}
5074         coord_names = {k for k in self._coord_names if k in variables}
5075         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5076         return self._replace_with_new_dims(
5077             variables, coord_names=coord_names, indexes=indexes
5078         )
5079 
5080     def drop_indexes(
5081         self: T_Dataset,
5082         coord_names: Hashable | Iterable[Hashable],
5083         *,
5084         errors: ErrorOptions = "raise",
5085     ) -> T_Dataset:
5086         """Drop the indexes assigned to the given coordinates.
5087 
5088         Parameters
5089         ----------
5090         coord_names : hashable or iterable of hashable
5091             Name(s) of the coordinate(s) for which to drop the index.
5092         errors : {"raise", "ignore"}, default: "raise"
5093             If 'raise', raises a ValueError error if any of the coordinates
5094             passed have no index or are not in the dataset.
5095             If 'ignore', no error is raised.
5096 
5097         Returns
5098         -------
5099         dropped : Dataset
5100             A new dataset with dropped indexes.
5101 
5102         """
5103         # the Iterable check is required for mypy
5104         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5105             coord_names = {coord_names}
5106         else:
5107             coord_names = set(coord_names)
5108 
5109         if errors == "raise":
5110             invalid_coords = coord_names - self._coord_names
5111             if invalid_coords:
5112                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5113 
5114             unindexed_coords = set(coord_names) - set(self._indexes)
5115             if unindexed_coords:
5116                 raise ValueError(
5117                     f"those coordinates do not have an index: {unindexed_coords}"
5118                 )
5119 
5120         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5121 
5122         variables = {}
5123         for name, var in self._variables.items():
5124             if name in coord_names:
5125                 variables[name] = var.to_base_variable()
5126             else:
5127                 variables[name] = var
5128 
5129         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5130 
5131         return self._replace(variables=variables, indexes=indexes)
5132 
5133     def drop(
5134         self: T_Dataset,
5135         labels=None,
5136         dim=None,
5137         *,
5138         errors: ErrorOptions = "raise",
5139         **labels_kwargs,
5140     ) -> T_Dataset:
5141         """Backward compatible method based on `drop_vars` and `drop_sel`
5142 
5143         Using either `drop_vars` or `drop_sel` is encouraged
5144 
5145         See Also
5146         --------
5147         Dataset.drop_vars
5148         Dataset.drop_sel
5149         """
5150         if errors not in ["raise", "ignore"]:
5151             raise ValueError('errors must be either "raise" or "ignore"')
5152 
5153         if is_dict_like(labels) and not isinstance(labels, dict):
5154             warnings.warn(
5155                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5156                 FutureWarning,
5157                 stacklevel=2,
5158             )
5159             return self.drop_vars(labels, errors=errors)
5160 
5161         if labels_kwargs or isinstance(labels, dict):
5162             if dim is not None:
5163                 raise ValueError("cannot specify dim and dict-like arguments.")
5164             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5165 
5166         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5167             warnings.warn(
5168                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5169                 PendingDeprecationWarning,
5170                 stacklevel=2,
5171             )
5172             return self.drop_vars(labels, errors=errors)
5173         if dim is not None:
5174             warnings.warn(
5175                 "dropping labels using list-like labels is deprecated; using "
5176                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5177                 DeprecationWarning,
5178                 stacklevel=2,
5179             )
5180             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5181 
5182         warnings.warn(
5183             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5184             PendingDeprecationWarning,
5185             stacklevel=2,
5186         )
5187         return self.drop_sel(labels, errors=errors)
5188 
5189     def drop_sel(
5190         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5191     ) -> T_Dataset:
5192         """Drop index labels from this dataset.
5193 
5194         Parameters
5195         ----------
5196         labels : mapping of hashable to Any
5197             Index labels to drop
5198         errors : {"raise", "ignore"}, default: "raise"
5199             If 'raise', raises a ValueError error if
5200             any of the index labels passed are not
5201             in the dataset. If 'ignore', any given labels that are in the
5202             dataset are dropped and no error is raised.
5203         **labels_kwargs : {dim: label, ...}, optional
5204             The keyword arguments form of ``dim`` and ``labels``
5205 
5206         Returns
5207         -------
5208         dropped : Dataset
5209 
5210         Examples
5211         --------
5212         >>> data = np.arange(6).reshape(2, 3)
5213         >>> labels = ["a", "b", "c"]
5214         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5215         >>> ds
5216         <xarray.Dataset>
5217         Dimensions:  (x: 2, y: 3)
5218         Coordinates:
5219           * y        (y) <U1 'a' 'b' 'c'
5220         Dimensions without coordinates: x
5221         Data variables:
5222             A        (x, y) int64 0 1 2 3 4 5
5223         >>> ds.drop_sel(y=["a", "c"])
5224         <xarray.Dataset>
5225         Dimensions:  (x: 2, y: 1)
5226         Coordinates:
5227           * y        (y) <U1 'b'
5228         Dimensions without coordinates: x
5229         Data variables:
5230             A        (x, y) int64 1 4
5231         >>> ds.drop_sel(y="b")
5232         <xarray.Dataset>
5233         Dimensions:  (x: 2, y: 2)
5234         Coordinates:
5235           * y        (y) <U1 'a' 'c'
5236         Dimensions without coordinates: x
5237         Data variables:
5238             A        (x, y) int64 0 2 3 5
5239         """
5240         if errors not in ["raise", "ignore"]:
5241             raise ValueError('errors must be either "raise" or "ignore"')
5242 
5243         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5244 
5245         ds = self
5246         for dim, labels_for_dim in labels.items():
5247             # Don't cast to set, as it would harm performance when labels
5248             # is a large numpy array
5249             if utils.is_scalar(labels_for_dim):
5250                 labels_for_dim = [labels_for_dim]
5251             labels_for_dim = np.asarray(labels_for_dim)
5252             try:
5253                 index = self.get_index(dim)
5254             except KeyError:
5255                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5256             new_index = index.drop(labels_for_dim, errors=errors)
5257             ds = ds.loc[{dim: new_index}]
5258         return ds
5259 
5260     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5261         """Drop index positions from this Dataset.
5262 
5263         Parameters
5264         ----------
5265         indexers : mapping of hashable to Any
5266             Index locations to drop
5267         **indexers_kwargs : {dim: position, ...}, optional
5268             The keyword arguments form of ``dim`` and ``positions``
5269 
5270         Returns
5271         -------
5272         dropped : Dataset
5273 
5274         Raises
5275         ------
5276         IndexError
5277 
5278         Examples
5279         --------
5280         >>> data = np.arange(6).reshape(2, 3)
5281         >>> labels = ["a", "b", "c"]
5282         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5283         >>> ds
5284         <xarray.Dataset>
5285         Dimensions:  (x: 2, y: 3)
5286         Coordinates:
5287           * y        (y) <U1 'a' 'b' 'c'
5288         Dimensions without coordinates: x
5289         Data variables:
5290             A        (x, y) int64 0 1 2 3 4 5
5291         >>> ds.drop_isel(y=[0, 2])
5292         <xarray.Dataset>
5293         Dimensions:  (x: 2, y: 1)
5294         Coordinates:
5295           * y        (y) <U1 'b'
5296         Dimensions without coordinates: x
5297         Data variables:
5298             A        (x, y) int64 1 4
5299         >>> ds.drop_isel(y=1)
5300         <xarray.Dataset>
5301         Dimensions:  (x: 2, y: 2)
5302         Coordinates:
5303           * y        (y) <U1 'a' 'c'
5304         Dimensions without coordinates: x
5305         Data variables:
5306             A        (x, y) int64 0 2 3 5
5307         """
5308 
5309         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5310 
5311         ds = self
5312         dimension_index = {}
5313         for dim, pos_for_dim in indexers.items():
5314             # Don't cast to set, as it would harm performance when labels
5315             # is a large numpy array
5316             if utils.is_scalar(pos_for_dim):
5317                 pos_for_dim = [pos_for_dim]
5318             pos_for_dim = np.asarray(pos_for_dim)
5319             index = self.get_index(dim)
5320             new_index = index.delete(pos_for_dim)
5321             dimension_index[dim] = new_index
5322         ds = ds.loc[dimension_index]
5323         return ds
5324 
5325     def drop_dims(
5326         self: T_Dataset,
5327         drop_dims: Hashable | Iterable[Hashable],
5328         *,
5329         errors: ErrorOptions = "raise",
5330     ) -> T_Dataset:
5331         """Drop dimensions and associated variables from this dataset.
5332 
5333         Parameters
5334         ----------
5335         drop_dims : hashable or iterable of hashable
5336             Dimension or dimensions to drop.
5337         errors : {"raise", "ignore"}, default: "raise"
5338             If 'raise', raises a ValueError error if any of the
5339             dimensions passed are not in the dataset. If 'ignore', any given
5340             dimensions that are in the dataset are dropped and no error is raised.
5341 
5342         Returns
5343         -------
5344         obj : Dataset
5345             The dataset without the given dimensions (or any variables
5346             containing those dimensions).
5347         """
5348         if errors not in ["raise", "ignore"]:
5349             raise ValueError('errors must be either "raise" or "ignore"')
5350 
5351         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5352             drop_dims = {drop_dims}
5353         else:
5354             drop_dims = set(drop_dims)
5355 
5356         if errors == "raise":
5357             missing_dims = drop_dims - set(self.dims)
5358             if missing_dims:
5359                 raise ValueError(
5360                     f"Dataset does not contain the dimensions: {missing_dims}"
5361                 )
5362 
5363         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5364         return self.drop_vars(drop_vars)
5365 
5366     def transpose(
5367         self: T_Dataset,
5368         *dims: Hashable,
5369         missing_dims: ErrorOptionsWithWarn = "raise",
5370     ) -> T_Dataset:
5371         """Return a new Dataset object with all array dimensions transposed.
5372 
5373         Although the order of dimensions on each array will change, the dataset
5374         dimensions themselves will remain in fixed (sorted) order.
5375 
5376         Parameters
5377         ----------
5378         *dims : hashable, optional
5379             By default, reverse the dimensions on each array. Otherwise,
5380             reorder the dimensions to this order.
5381         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5382             What to do if dimensions that should be selected from are not present in the
5383             Dataset:
5384             - "raise": raise an exception
5385             - "warn": raise a warning, and ignore the missing dimensions
5386             - "ignore": ignore the missing dimensions
5387 
5388         Returns
5389         -------
5390         transposed : Dataset
5391             Each array in the dataset (including) coordinates will be
5392             transposed to the given order.
5393 
5394         Notes
5395         -----
5396         This operation returns a view of each array's data. It is
5397         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5398         -- the data will be fully loaded into memory.
5399 
5400         See Also
5401         --------
5402         numpy.transpose
5403         DataArray.transpose
5404         """
5405         # Use infix_dims to check once for missing dimensions
5406         if len(dims) != 0:
5407             _ = list(infix_dims(dims, self.dims, missing_dims))
5408 
5409         ds = self.copy()
5410         for name, var in self._variables.items():
5411             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5412             ds._variables[name] = var.transpose(*var_dims)
5413         return ds
5414 
5415     def dropna(
5416         self: T_Dataset,
5417         dim: Hashable,
5418         how: Literal["any", "all"] = "any",
5419         thresh: int | None = None,
5420         subset: Iterable[Hashable] | None = None,
5421     ) -> T_Dataset:
5422         """Returns a new dataset with dropped labels for missing values along
5423         the provided dimension.
5424 
5425         Parameters
5426         ----------
5427         dim : hashable
5428             Dimension along which to drop missing values. Dropping along
5429             multiple dimensions simultaneously is not yet supported.
5430         how : {"any", "all"}, default: "any"
5431             - any : if any NA values are present, drop that label
5432             - all : if all values are NA, drop that label
5433 
5434         thresh : int or None, optional
5435             If supplied, require this many non-NA values.
5436         subset : iterable of hashable or None, optional
5437             Which variables to check for missing values. By default, all
5438             variables in the dataset are checked.
5439 
5440         Returns
5441         -------
5442         Dataset
5443         """
5444         # TODO: consider supporting multiple dimensions? Or not, given that
5445         # there are some ugly edge cases, e.g., pandas's dropna differs
5446         # depending on the order of the supplied axes.
5447 
5448         if dim not in self.dims:
5449             raise ValueError(f"{dim} must be a single dataset dimension")
5450 
5451         if subset is None:
5452             subset = iter(self.data_vars)
5453 
5454         count = np.zeros(self.dims[dim], dtype=np.int64)
5455         size = np.int_(0)  # for type checking
5456 
5457         for k in subset:
5458             array = self._variables[k]
5459             if dim in array.dims:
5460                 dims = [d for d in array.dims if d != dim]
5461                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5462                 size += math.prod([self.dims[d] for d in dims])
5463 
5464         if thresh is not None:
5465             mask = count >= thresh
5466         elif how == "any":
5467             mask = count == size
5468         elif how == "all":
5469             mask = count > 0
5470         elif how is not None:
5471             raise ValueError(f"invalid how option: {how}")
5472         else:
5473             raise TypeError("must specify how or thresh")
5474 
5475         return self.isel({dim: mask})
5476 
5477     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5478         """Fill missing values in this object.
5479 
5480         This operation follows the normal broadcasting and alignment rules that
5481         xarray uses for binary arithmetic, except the result is aligned to this
5482         object (``join='left'``) instead of aligned to the intersection of
5483         index coordinates (``join='inner'``).
5484 
5485         Parameters
5486         ----------
5487         value : scalar, ndarray, DataArray, dict or Dataset
5488             Used to fill all matching missing values in this dataset's data
5489             variables. Scalars, ndarrays or DataArrays arguments are used to
5490             fill all data with aligned coordinates (for DataArrays).
5491             Dictionaries or datasets match data variables and then align
5492             coordinates if necessary.
5493 
5494         Returns
5495         -------
5496         Dataset
5497 
5498         Examples
5499         --------
5500         >>> ds = xr.Dataset(
5501         ...     {
5502         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5503         ...         "B": ("x", [3, 4, np.nan, 1]),
5504         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5505         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5506         ...     },
5507         ...     coords={"x": [0, 1, 2, 3]},
5508         ... )
5509         >>> ds
5510         <xarray.Dataset>
5511         Dimensions:  (x: 4)
5512         Coordinates:
5513           * x        (x) int64 0 1 2 3
5514         Data variables:
5515             A        (x) float64 nan 2.0 nan 0.0
5516             B        (x) float64 3.0 4.0 nan 1.0
5517             C        (x) float64 nan nan nan 5.0
5518             D        (x) float64 nan 3.0 nan 4.0
5519 
5520         Replace all `NaN` values with 0s.
5521 
5522         >>> ds.fillna(0)
5523         <xarray.Dataset>
5524         Dimensions:  (x: 4)
5525         Coordinates:
5526           * x        (x) int64 0 1 2 3
5527         Data variables:
5528             A        (x) float64 0.0 2.0 0.0 0.0
5529             B        (x) float64 3.0 4.0 0.0 1.0
5530             C        (x) float64 0.0 0.0 0.0 5.0
5531             D        (x) float64 0.0 3.0 0.0 4.0
5532 
5533         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
5534 
5535         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5536         >>> ds.fillna(value=values)
5537         <xarray.Dataset>
5538         Dimensions:  (x: 4)
5539         Coordinates:
5540           * x        (x) int64 0 1 2 3
5541         Data variables:
5542             A        (x) float64 0.0 2.0 0.0 0.0
5543             B        (x) float64 3.0 4.0 1.0 1.0
5544             C        (x) float64 2.0 2.0 2.0 5.0
5545             D        (x) float64 3.0 3.0 3.0 4.0
5546         """
5547         if utils.is_dict_like(value):
5548             value_keys = getattr(value, "data_vars", value).keys()
5549             if not set(value_keys) <= set(self.data_vars.keys()):
5550                 raise ValueError(
5551                     "all variables in the argument to `fillna` "
5552                     "must be contained in the original dataset"
5553                 )
5554         out = ops.fillna(self, value)
5555         return out
5556 
5557     def interpolate_na(
5558         self: T_Dataset,
5559         dim: Hashable | None = None,
5560         method: InterpOptions = "linear",
5561         limit: int = None,
5562         use_coordinate: bool | Hashable = True,
5563         max_gap: (
5564             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5565         ) = None,
5566         **kwargs: Any,
5567     ) -> T_Dataset:
5568         """Fill in NaNs by interpolating according to different methods.
5569 
5570         Parameters
5571         ----------
5572         dim : Hashable or None, optional
5573             Specifies the dimension along which to interpolate.
5574         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5575             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5576             String indicating which method to use for interpolation:
5577 
5578             - 'linear': linear interpolation. Additional keyword
5579               arguments are passed to :py:func:`numpy.interp`
5580             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5581               are passed to :py:func:`scipy.interpolate.interp1d`. If
5582               ``method='polynomial'``, the ``order`` keyword argument must also be
5583               provided.
5584             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5585               respective :py:class:`scipy.interpolate` classes.
5586 
5587         use_coordinate : bool or Hashable, default: True
5588             Specifies which index to use as the x values in the interpolation
5589             formulated as `y = f(x)`. If False, values are treated as if
5590             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5591             used. If ``use_coordinate`` is a string, it specifies the name of a
5592             coordinate variariable to use as the index.
5593         limit : int, default: None
5594             Maximum number of consecutive NaNs to fill. Must be greater than 0
5595             or None for no limit. This filling is done regardless of the size of
5596             the gap in the data. To only interpolate over gaps less than a given length,
5597             see ``max_gap``.
5598         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5599             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5600             Use None for no limit. When interpolating along a datetime64 dimension
5601             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5602 
5603             - a string that is valid input for pandas.to_timedelta
5604             - a :py:class:`numpy.timedelta64` object
5605             - a :py:class:`pandas.Timedelta` object
5606             - a :py:class:`datetime.timedelta` object
5607 
5608             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5609             dimensions has not been implemented yet. Gap length is defined as the difference
5610             between coordinate values at the first data point after a gap and the last value
5611             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5612             between coordinate values at the first (last) valid data point and the first (last) NaN.
5613             For example, consider::
5614 
5615                 <xarray.DataArray (x: 9)>
5616                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5617                 Coordinates:
5618                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5619 
5620             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5621         **kwargs : dict, optional
5622             parameters passed verbatim to the underlying interpolation function
5623 
5624         Returns
5625         -------
5626         interpolated: Dataset
5627             Filled in Dataset.
5628 
5629         See Also
5630         --------
5631         numpy.interp
5632         scipy.interpolate
5633 
5634         Examples
5635         --------
5636         >>> ds = xr.Dataset(
5637         ...     {
5638         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5639         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5640         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5641         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5642         ...     },
5643         ...     coords={"x": [0, 1, 2, 3, 4]},
5644         ... )
5645         >>> ds
5646         <xarray.Dataset>
5647         Dimensions:  (x: 5)
5648         Coordinates:
5649           * x        (x) int64 0 1 2 3 4
5650         Data variables:
5651             A        (x) float64 nan 2.0 3.0 nan 0.0
5652             B        (x) float64 3.0 4.0 nan 1.0 7.0
5653             C        (x) float64 nan nan nan 5.0 0.0
5654             D        (x) float64 nan 3.0 nan -1.0 4.0
5655 
5656         >>> ds.interpolate_na(dim="x", method="linear")
5657         <xarray.Dataset>
5658         Dimensions:  (x: 5)
5659         Coordinates:
5660           * x        (x) int64 0 1 2 3 4
5661         Data variables:
5662             A        (x) float64 nan 2.0 3.0 1.5 0.0
5663             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5664             C        (x) float64 nan nan nan 5.0 0.0
5665             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5666 
5667         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5668         <xarray.Dataset>
5669         Dimensions:  (x: 5)
5670         Coordinates:
5671           * x        (x) int64 0 1 2 3 4
5672         Data variables:
5673             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5674             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5675             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5676             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5677         """
5678         from .missing import _apply_over_vars_with_dim, interp_na
5679 
5680         new = _apply_over_vars_with_dim(
5681             interp_na,
5682             self,
5683             dim=dim,
5684             method=method,
5685             limit=limit,
5686             use_coordinate=use_coordinate,
5687             max_gap=max_gap,
5688             **kwargs,
5689         )
5690         return new
5691 
5692     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5693         """Fill NaN values by propagating values forward
5694 
5695         *Requires bottleneck.*
5696 
5697         Parameters
5698         ----------
5699         dim : Hashable
5700             Specifies the dimension along which to propagate values when
5701             filling.
5702         limit : int or None, optional
5703             The maximum number of consecutive NaN values to forward fill. In
5704             other words, if there is a gap with more than this number of
5705             consecutive NaNs, it will only be partially filled. Must be greater
5706             than 0 or None for no limit. Must be None or greater than or equal
5707             to axis length if filling along chunked axes (dimensions).
5708 
5709         Returns
5710         -------
5711         Dataset
5712         """
5713         from .missing import _apply_over_vars_with_dim, ffill
5714 
5715         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5716         return new
5717 
5718     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5719         """Fill NaN values by propagating values backward
5720 
5721         *Requires bottleneck.*
5722 
5723         Parameters
5724         ----------
5725         dim : Hashable
5726             Specifies the dimension along which to propagate values when
5727             filling.
5728         limit : int or None, optional
5729             The maximum number of consecutive NaN values to backward fill. In
5730             other words, if there is a gap with more than this number of
5731             consecutive NaNs, it will only be partially filled. Must be greater
5732             than 0 or None for no limit. Must be None or greater than or equal
5733             to axis length if filling along chunked axes (dimensions).
5734 
5735         Returns
5736         -------
5737         Dataset
5738         """
5739         from .missing import _apply_over_vars_with_dim, bfill
5740 
5741         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5742         return new
5743 
5744     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5745         """Combine two Datasets, default to data_vars of self.
5746 
5747         The new coordinates follow the normal broadcasting and alignment rules
5748         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5749         filled with np.nan.
5750 
5751         Parameters
5752         ----------
5753         other : Dataset
5754             Used to fill all matching missing values in this array.
5755 
5756         Returns
5757         -------
5758         Dataset
5759         """
5760         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5761         return out
5762 
5763     def reduce(
5764         self: T_Dataset,
5765         func: Callable,
5766         dim: Hashable | Iterable[Hashable] = None,
5767         *,
5768         keep_attrs: bool | None = None,
5769         keepdims: bool = False,
5770         numeric_only: bool = False,
5771         **kwargs: Any,
5772     ) -> T_Dataset:
5773         """Reduce this dataset by applying `func` along some dimension(s).
5774 
5775         Parameters
5776         ----------
5777         func : callable
5778             Function which can be called in the form
5779             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5780             np.ndarray over an integer valued axis.
5781         dim : str or sequence of str, optional
5782             Dimension(s) over which to apply `func`.  By default `func` is
5783             applied over all dimensions.
5784         keep_attrs : bool or None, optional
5785             If True, the dataset's attributes (`attrs`) will be copied from
5786             the original object to the new one.  If False (default), the new
5787             object will be returned without attributes.
5788         keepdims : bool, default: False
5789             If True, the dimensions which are reduced are left in the result
5790             as dimensions of size one. Coordinates that use these dimensions
5791             are removed.
5792         numeric_only : bool, default: False
5793             If True, only apply ``func`` to variables with a numeric dtype.
5794         **kwargs : Any
5795             Additional keyword arguments passed on to ``func``.
5796 
5797         Returns
5798         -------
5799         reduced : Dataset
5800             Dataset with this object's DataArrays replaced with new DataArrays
5801             of summarized data and the indicated dimension(s) removed.
5802         """
5803         if kwargs.get("axis", None) is not None:
5804             raise ValueError(
5805                 "passing 'axis' to Dataset reduce methods is ambiguous."
5806                 " Please use 'dim' instead."
5807             )
5808 
5809         if dim is None or dim is ...:
5810             dims = set(self.dims)
5811         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5812             dims = {dim}
5813         else:
5814             dims = set(dim)
5815 
5816         missing_dimensions = [d for d in dims if d not in self.dims]
5817         if missing_dimensions:
5818             raise ValueError(
5819                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5820             )
5821 
5822         if keep_attrs is None:
5823             keep_attrs = _get_keep_attrs(default=False)
5824 
5825         variables: dict[Hashable, Variable] = {}
5826         for name, var in self._variables.items():
5827             reduce_dims = [d for d in var.dims if d in dims]
5828             if name in self.coords:
5829                 if not reduce_dims:
5830                     variables[name] = var
5831             else:
5832                 if (
5833                     # Some reduction functions (e.g. std, var) need to run on variables
5834                     # that don't have the reduce dims: PR5393
5835                     not reduce_dims
5836                     or not numeric_only
5837                     or np.issubdtype(var.dtype, np.number)
5838                     or (var.dtype == np.bool_)
5839                 ):
5840                     reduce_maybe_single: Hashable | None | list[Hashable]
5841                     if len(reduce_dims) == 1:
5842                         # unpack dimensions for the benefit of functions
5843                         # like np.argmin which can't handle tuple arguments
5844                         (reduce_maybe_single,) = reduce_dims
5845                     elif len(reduce_dims) == var.ndim:
5846                         # prefer to aggregate over axis=None rather than
5847                         # axis=(0, 1) if they will be equivalent, because
5848                         # the former is often more efficient
5849                         reduce_maybe_single = None
5850                     else:
5851                         reduce_maybe_single = reduce_dims
5852                     variables[name] = var.reduce(
5853                         func,
5854                         dim=reduce_maybe_single,
5855                         keep_attrs=keep_attrs,
5856                         keepdims=keepdims,
5857                         **kwargs,
5858                     )
5859 
5860         coord_names = {k for k in self.coords if k in variables}
5861         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5862         attrs = self.attrs if keep_attrs else None
5863         return self._replace_with_new_dims(
5864             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5865         )
5866 
5867     def map(
5868         self: T_Dataset,
5869         func: Callable,
5870         keep_attrs: bool | None = None,
5871         args: Iterable[Any] = (),
5872         **kwargs: Any,
5873     ) -> T_Dataset:
5874         """Apply a function to each data variable in this dataset
5875 
5876         Parameters
5877         ----------
5878         func : callable
5879             Function which can be called in the form `func(x, *args, **kwargs)`
5880             to transform each DataArray `x` in this dataset into another
5881             DataArray.
5882         keep_attrs : bool or None, optional
5883             If True, both the dataset's and variables' attributes (`attrs`) will be
5884             copied from the original objects to the new ones. If False, the new dataset
5885             and variables will be returned without copying the attributes.
5886         args : iterable, optional
5887             Positional arguments passed on to `func`.
5888         **kwargs : Any
5889             Keyword arguments passed on to `func`.
5890 
5891         Returns
5892         -------
5893         applied : Dataset
5894             Resulting dataset from applying ``func`` to each data variable.
5895 
5896         Examples
5897         --------
5898         >>> da = xr.DataArray(np.random.randn(2, 3))
5899         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5900         >>> ds
5901         <xarray.Dataset>
5902         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5903         Dimensions without coordinates: dim_0, dim_1, x
5904         Data variables:
5905             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5906             bar      (x) int64 -1 2
5907         >>> ds.map(np.fabs)
5908         <xarray.Dataset>
5909         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5910         Dimensions without coordinates: dim_0, dim_1, x
5911         Data variables:
5912             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5913             bar      (x) float64 1.0 2.0
5914         """
5915         if keep_attrs is None:
5916             keep_attrs = _get_keep_attrs(default=False)
5917         variables = {
5918             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5919             for k, v in self.data_vars.items()
5920         }
5921         if keep_attrs:
5922             for k, v in variables.items():
5923                 v._copy_attrs_from(self.data_vars[k])
5924         attrs = self.attrs if keep_attrs else None
5925         return type(self)(variables, attrs=attrs)
5926 
5927     def apply(
5928         self: T_Dataset,
5929         func: Callable,
5930         keep_attrs: bool | None = None,
5931         args: Iterable[Any] = (),
5932         **kwargs: Any,
5933     ) -> T_Dataset:
5934         """
5935         Backward compatible implementation of ``map``
5936 
5937         See Also
5938         --------
5939         Dataset.map
5940         """
5941         warnings.warn(
5942             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5943             PendingDeprecationWarning,
5944             stacklevel=2,
5945         )
5946         return self.map(func, keep_attrs, args, **kwargs)
5947 
5948     def assign(
5949         self: T_Dataset,
5950         variables: Mapping[Any, Any] | None = None,
5951         **variables_kwargs: Any,
5952     ) -> T_Dataset:
5953         """Assign new data variables to a Dataset, returning a new object
5954         with all the original variables in addition to the new ones.
5955 
5956         Parameters
5957         ----------
5958         variables : mapping of hashable to Any
5959             Mapping from variables names to the new values. If the new values
5960             are callable, they are computed on the Dataset and assigned to new
5961             data variables. If the values are not callable, (e.g. a DataArray,
5962             scalar, or array), they are simply assigned.
5963         **variables_kwargs
5964             The keyword arguments form of ``variables``.
5965             One of variables or variables_kwargs must be provided.
5966 
5967         Returns
5968         -------
5969         ds : Dataset
5970             A new Dataset with the new variables in addition to all the
5971             existing variables.
5972 
5973         Notes
5974         -----
5975         Since ``kwargs`` is a dictionary, the order of your arguments may not
5976         be preserved, and so the order of the new variables is not well
5977         defined. Assigning multiple variables within the same ``assign`` is
5978         possible, but you cannot reference other variables created within the
5979         same ``assign`` call.
5980 
5981         See Also
5982         --------
5983         pandas.DataFrame.assign
5984 
5985         Examples
5986         --------
5987         >>> x = xr.Dataset(
5988         ...     {
5989         ...         "temperature_c": (
5990         ...             ("lat", "lon"),
5991         ...             20 * np.random.rand(4).reshape(2, 2),
5992         ...         ),
5993         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5994         ...     },
5995         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5996         ... )
5997         >>> x
5998         <xarray.Dataset>
5999         Dimensions:        (lat: 2, lon: 2)
6000         Coordinates:
6001           * lat            (lat) int64 10 20
6002           * lon            (lon) int64 150 160
6003         Data variables:
6004             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6005             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6006 
6007         Where the value is a callable, evaluated on dataset:
6008 
6009         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6010         <xarray.Dataset>
6011         Dimensions:        (lat: 2, lon: 2)
6012         Coordinates:
6013           * lat            (lat) int64 10 20
6014           * lon            (lon) int64 150 160
6015         Data variables:
6016             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6017             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6018             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6019 
6020         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6021 
6022         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6023         <xarray.Dataset>
6024         Dimensions:        (lat: 2, lon: 2)
6025         Coordinates:
6026           * lat            (lat) int64 10 20
6027           * lon            (lon) int64 150 160
6028         Data variables:
6029             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6030             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6031             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6032 
6033         """
6034         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6035         data = self.copy()
6036         # do all calculations first...
6037         results: CoercibleMapping = data._calc_assign_results(variables)
6038         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6039         # ... and then assign
6040         data.update(results)
6041         return data
6042 
6043     def to_array(
6044         self, dim: Hashable = "variable", name: Hashable | None = None
6045     ) -> DataArray:
6046         """Convert this dataset into an xarray.DataArray
6047 
6048         The data variables of this dataset will be broadcast against each other
6049         and stacked along the first axis of the new array. All coordinates of
6050         this dataset will remain coordinates.
6051 
6052         Parameters
6053         ----------
6054         dim : Hashable, default: "variable"
6055             Name of the new dimension.
6056         name : Hashable or None, optional
6057             Name of the new data array.
6058 
6059         Returns
6060         -------
6061         array : xarray.DataArray
6062         """
6063         from .dataarray import DataArray
6064 
6065         data_vars = [self.variables[k] for k in self.data_vars]
6066         broadcast_vars = broadcast_variables(*data_vars)
6067         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6068 
6069         dims = (dim,) + broadcast_vars[0].dims
6070         variable = Variable(dims, data, self.attrs, fastpath=True)
6071 
6072         coords = {k: v.variable for k, v in self.coords.items()}
6073         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6074         new_dim_index = PandasIndex(list(self.data_vars), dim)
6075         indexes[dim] = new_dim_index
6076         coords.update(new_dim_index.create_variables())
6077 
6078         return DataArray._construct_direct(variable, coords, name, indexes)
6079 
6080     def _normalize_dim_order(
6081         self, dim_order: Sequence[Hashable] | None = None
6082     ) -> dict[Hashable, int]:
6083         """
6084         Check the validity of the provided dimensions if any and return the mapping
6085         between dimension name and their size.
6086 
6087         Parameters
6088         ----------
6089         dim_order: Sequence of Hashable or None, optional
6090             Dimension order to validate (default to the alphabetical order if None).
6091 
6092         Returns
6093         -------
6094         result : dict[Hashable, int]
6095             Validated dimensions mapping.
6096 
6097         """
6098         if dim_order is None:
6099             dim_order = list(self.dims)
6100         elif set(dim_order) != set(self.dims):
6101             raise ValueError(
6102                 "dim_order {} does not match the set of dimensions of this "
6103                 "Dataset: {}".format(dim_order, list(self.dims))
6104             )
6105 
6106         ordered_dims = {k: self.dims[k] for k in dim_order}
6107 
6108         return ordered_dims
6109 
6110     def to_pandas(self) -> pd.Series | pd.DataFrame:
6111         """Convert this dataset into a pandas object without changing the number of dimensions.
6112 
6113         The type of the returned object depends on the number of Dataset
6114         dimensions:
6115 
6116         * 0D -> `pandas.Series`
6117         * 1D -> `pandas.DataFrame`
6118 
6119         Only works for Datasets with 1 or fewer dimensions.
6120         """
6121         if len(self.dims) == 0:
6122             return pd.Series({k: v.item() for k, v in self.items()})
6123         if len(self.dims) == 1:
6124             return self.to_dataframe()
6125         raise ValueError(
6126             "cannot convert Datasets with %s dimensions into "
6127             "pandas objects without changing the number of dimensions. "
6128             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6129         )
6130 
6131     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6132         columns = [k for k in self.variables if k not in self.dims]
6133         data = [
6134             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6135             for k in columns
6136         ]
6137         index = self.coords.to_index([*ordered_dims])
6138         return pd.DataFrame(dict(zip(columns, data)), index=index)
6139 
6140     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6141         """Convert this dataset into a pandas.DataFrame.
6142 
6143         Non-index variables in this dataset form the columns of the
6144         DataFrame. The DataFrame is indexed by the Cartesian product of
6145         this dataset's indices.
6146 
6147         Parameters
6148         ----------
6149         dim_order: Sequence of Hashable or None, optional
6150             Hierarchical dimension order for the resulting dataframe. All
6151             arrays are transposed to this order and then written out as flat
6152             vectors in contiguous order, so the last dimension in this list
6153             will be contiguous in the resulting DataFrame. This has a major
6154             influence on which operations are efficient on the resulting
6155             dataframe.
6156 
6157             If provided, must include all dimensions of this dataset. By
6158             default, dimensions are sorted alphabetically.
6159 
6160         Returns
6161         -------
6162         result : DataFrame
6163             Dataset as a pandas DataFrame.
6164 
6165         """
6166 
6167         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6168 
6169         return self._to_dataframe(ordered_dims=ordered_dims)
6170 
6171     def _set_sparse_data_from_dataframe(
6172         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6173     ) -> None:
6174         from sparse import COO
6175 
6176         if isinstance(idx, pd.MultiIndex):
6177             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6178             is_sorted = idx.is_monotonic_increasing
6179             shape = tuple(lev.size for lev in idx.levels)
6180         else:
6181             coords = np.arange(idx.size).reshape(1, -1)
6182             is_sorted = True
6183             shape = (idx.size,)
6184 
6185         for name, values in arrays:
6186             # In virtually all real use cases, the sparse array will now have
6187             # missing values and needs a fill_value. For consistency, don't
6188             # special case the rare exceptions (e.g., dtype=int without a
6189             # MultiIndex).
6190             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6191             values = np.asarray(values, dtype=dtype)
6192 
6193             data = COO(
6194                 coords,
6195                 values,
6196                 shape,
6197                 has_duplicates=False,
6198                 sorted=is_sorted,
6199                 fill_value=fill_value,
6200             )
6201             self[name] = (dims, data)
6202 
6203     def _set_numpy_data_from_dataframe(
6204         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6205     ) -> None:
6206         if not isinstance(idx, pd.MultiIndex):
6207             for name, values in arrays:
6208                 self[name] = (dims, values)
6209             return
6210 
6211         # NB: similar, more general logic, now exists in
6212         # variable.unstack_once; we could consider combining them at some
6213         # point.
6214 
6215         shape = tuple(lev.size for lev in idx.levels)
6216         indexer = tuple(idx.codes)
6217 
6218         # We already verified that the MultiIndex has all unique values, so
6219         # there are missing values if and only if the size of output arrays is
6220         # larger that the index.
6221         missing_values = math.prod(shape) > idx.shape[0]
6222 
6223         for name, values in arrays:
6224             # NumPy indexing is much faster than using DataFrame.reindex() to
6225             # fill in missing values:
6226             # https://stackoverflow.com/a/35049899/809705
6227             if missing_values:
6228                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6229                 data = np.full(shape, fill_value, dtype)
6230             else:
6231                 # If there are no missing values, keep the existing dtype
6232                 # instead of promoting to support NA, e.g., keep integer
6233                 # columns as integers.
6234                 # TODO: consider removing this special case, which doesn't
6235                 # exist for sparse=True.
6236                 data = np.zeros(shape, values.dtype)
6237             data[indexer] = values
6238             self[name] = (dims, data)
6239 
6240     @classmethod
6241     def from_dataframe(
6242         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6243     ) -> T_Dataset:
6244         """Convert a pandas.DataFrame into an xarray.Dataset
6245 
6246         Each column will be converted into an independent variable in the
6247         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6248         into a tensor product of one-dimensional indices (filling in missing
6249         values with NaN). This method will produce a Dataset very similar to
6250         that on which the 'to_dataframe' method was called, except with
6251         possibly redundant dimensions (since all dataset variables will have
6252         the same dimensionality)
6253 
6254         Parameters
6255         ----------
6256         dataframe : DataFrame
6257             DataFrame from which to copy data and indices.
6258         sparse : bool, default: False
6259             If true, create a sparse arrays instead of dense numpy arrays. This
6260             can potentially save a large amount of memory if the DataFrame has
6261             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6262 
6263         Returns
6264         -------
6265         New Dataset.
6266 
6267         See Also
6268         --------
6269         xarray.DataArray.from_series
6270         pandas.DataFrame.to_xarray
6271         """
6272         # TODO: Add an option to remove dimensions along which the variables
6273         # are constant, to enable consistent serialization to/from a dataframe,
6274         # even if some variables have different dimensionality.
6275 
6276         if not dataframe.columns.is_unique:
6277             raise ValueError("cannot convert DataFrame with non-unique columns")
6278 
6279         idx = remove_unused_levels_categories(dataframe.index)
6280 
6281         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6282             raise ValueError(
6283                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6284             )
6285 
6286         # Cast to a NumPy array first, in case the Series is a pandas Extension
6287         # array (which doesn't have a valid NumPy dtype)
6288         # TODO: allow users to control how this casting happens, e.g., by
6289         # forwarding arguments to pandas.Series.to_numpy?
6290         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6291 
6292         indexes: dict[Hashable, Index] = {}
6293         index_vars: dict[Hashable, Variable] = {}
6294 
6295         if isinstance(idx, pd.MultiIndex):
6296             dims = tuple(
6297                 name if name is not None else "level_%i" % n
6298                 for n, name in enumerate(idx.names)
6299             )
6300             for dim, lev in zip(dims, idx.levels):
6301                 xr_idx = PandasIndex(lev, dim)
6302                 indexes[dim] = xr_idx
6303                 index_vars.update(xr_idx.create_variables())
6304         else:
6305             index_name = idx.name if idx.name is not None else "index"
6306             dims = (index_name,)
6307             xr_idx = PandasIndex(idx, index_name)
6308             indexes[index_name] = xr_idx
6309             index_vars.update(xr_idx.create_variables())
6310 
6311         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6312 
6313         if sparse:
6314             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6315         else:
6316             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6317         return obj
6318 
6319     def to_dask_dataframe(
6320         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6321     ) -> DaskDataFrame:
6322         """
6323         Convert this dataset into a dask.dataframe.DataFrame.
6324 
6325         The dimensions, coordinates and data variables in this dataset form
6326         the columns of the DataFrame.
6327 
6328         Parameters
6329         ----------
6330         dim_order : list, optional
6331             Hierarchical dimension order for the resulting dataframe. All
6332             arrays are transposed to this order and then written out as flat
6333             vectors in contiguous order, so the last dimension in this list
6334             will be contiguous in the resulting DataFrame. This has a major
6335             influence on which operations are efficient on the resulting dask
6336             dataframe.
6337 
6338             If provided, must include all dimensions of this dataset. By
6339             default, dimensions are sorted alphabetically.
6340         set_index : bool, default: False
6341             If set_index=True, the dask DataFrame is indexed by this dataset's
6342             coordinate. Since dask DataFrames do not support multi-indexes,
6343             set_index only works if the dataset only contains one dimension.
6344 
6345         Returns
6346         -------
6347         dask.dataframe.DataFrame
6348         """
6349 
6350         import dask.array as da
6351         import dask.dataframe as dd
6352 
6353         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6354 
6355         columns = list(ordered_dims)
6356         columns.extend(k for k in self.coords if k not in self.dims)
6357         columns.extend(self.data_vars)
6358 
6359         series_list = []
6360         for name in columns:
6361             try:
6362                 var = self.variables[name]
6363             except KeyError:
6364                 # dimension without a matching coordinate
6365                 size = self.dims[name]
6366                 data = da.arange(size, chunks=size, dtype=np.int64)
6367                 var = Variable((name,), data)
6368 
6369             # IndexVariable objects have a dummy .chunk() method
6370             if isinstance(var, IndexVariable):
6371                 var = var.to_base_variable()
6372 
6373             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6374             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6375             series_list.append(series)
6376 
6377         df = dd.concat(series_list, axis=1)
6378 
6379         if set_index:
6380             dim_order = [*ordered_dims]
6381 
6382             if len(dim_order) == 1:
6383                 (dim,) = dim_order
6384                 df = df.set_index(dim)
6385             else:
6386                 # triggers an error about multi-indexes, even if only one
6387                 # dimension is passed
6388                 df = df.set_index(dim_order)
6389 
6390         return df
6391 
6392     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6393         """
6394         Convert this dataset to a dictionary following xarray naming
6395         conventions.
6396 
6397         Converts all variables and attributes to native Python objects
6398         Useful for converting to json. To avoid datetime incompatibility
6399         use decode_times=False kwarg in xarrray.open_dataset.
6400 
6401         Parameters
6402         ----------
6403         data : bool, default: True
6404             Whether to include the actual data in the dictionary. When set to
6405             False, returns just the schema.
6406         encoding : bool, default: False
6407             Whether to include the Dataset's encoding in the dictionary.
6408 
6409         Returns
6410         -------
6411         d : dict
6412             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6413             "encoding".
6414 
6415         See Also
6416         --------
6417         Dataset.from_dict
6418         DataArray.to_dict
6419         """
6420         d: dict = {
6421             "coords": {},
6422             "attrs": decode_numpy_dict_values(self.attrs),
6423             "dims": dict(self.dims),
6424             "data_vars": {},
6425         }
6426         for k in self.coords:
6427             d["coords"].update(
6428                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6429             )
6430         for k in self.data_vars:
6431             d["data_vars"].update(
6432                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6433             )
6434         if encoding:
6435             d["encoding"] = dict(self.encoding)
6436         return d
6437 
6438     @classmethod
6439     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6440         """Convert a dictionary into an xarray.Dataset.
6441 
6442         Parameters
6443         ----------
6444         d : dict-like
6445             Mapping with a minimum structure of
6446                 ``{"var_0": {"dims": [..], "data": [..]}, \
6447                             ...}``
6448 
6449         Returns
6450         -------
6451         obj : Dataset
6452 
6453         See also
6454         --------
6455         Dataset.to_dict
6456         DataArray.from_dict
6457 
6458         Examples
6459         --------
6460         >>> d = {
6461         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6462         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6463         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6464         ... }
6465         >>> ds = xr.Dataset.from_dict(d)
6466         >>> ds
6467         <xarray.Dataset>
6468         Dimensions:  (t: 3)
6469         Coordinates:
6470           * t        (t) int64 0 1 2
6471         Data variables:
6472             a        (t) <U1 'a' 'b' 'c'
6473             b        (t) int64 10 20 30
6474 
6475         >>> d = {
6476         ...     "coords": {
6477         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6478         ...     },
6479         ...     "attrs": {"title": "air temperature"},
6480         ...     "dims": "t",
6481         ...     "data_vars": {
6482         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6483         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6484         ...     },
6485         ... }
6486         >>> ds = xr.Dataset.from_dict(d)
6487         >>> ds
6488         <xarray.Dataset>
6489         Dimensions:  (t: 3)
6490         Coordinates:
6491           * t        (t) int64 0 1 2
6492         Data variables:
6493             a        (t) int64 10 20 30
6494             b        (t) <U1 'a' 'b' 'c'
6495         Attributes:
6496             title:    air temperature
6497 
6498         """
6499 
6500         variables: Iterable[tuple[Hashable, Any]]
6501         if not {"coords", "data_vars"}.issubset(set(d)):
6502             variables = d.items()
6503         else:
6504             import itertools
6505 
6506             variables = itertools.chain(
6507                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6508             )
6509         try:
6510             variable_dict = {
6511                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6512             }
6513         except KeyError as e:
6514             raise ValueError(
6515                 "cannot convert dict without the key "
6516                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6517             )
6518         obj = cls(variable_dict)
6519 
6520         # what if coords aren't dims?
6521         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6522         obj = obj.set_coords(coords)
6523 
6524         obj.attrs.update(d.get("attrs", {}))
6525         obj.encoding.update(d.get("encoding", {}))
6526 
6527         return obj
6528 
6529     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6530         variables = {}
6531         keep_attrs = kwargs.pop("keep_attrs", None)
6532         if keep_attrs is None:
6533             keep_attrs = _get_keep_attrs(default=True)
6534         for k, v in self._variables.items():
6535             if k in self._coord_names:
6536                 variables[k] = v
6537             else:
6538                 variables[k] = f(v, *args, **kwargs)
6539                 if keep_attrs:
6540                     variables[k].attrs = v._attrs
6541         attrs = self._attrs if keep_attrs else None
6542         return self._replace_with_new_dims(variables, attrs=attrs)
6543 
6544     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6545         from .dataarray import DataArray
6546         from .groupby import GroupBy
6547 
6548         if isinstance(other, GroupBy):
6549             return NotImplemented
6550         align_type = OPTIONS["arithmetic_join"] if join is None else join
6551         if isinstance(other, (DataArray, Dataset)):
6552             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6553         g = f if not reflexive else lambda x, y: f(y, x)
6554         ds = self._calculate_binary_op(g, other, join=align_type)
6555         return ds
6556 
6557     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6558         from .dataarray import DataArray
6559         from .groupby import GroupBy
6560 
6561         if isinstance(other, GroupBy):
6562             raise TypeError(
6563                 "in-place operations between a Dataset and "
6564                 "a grouped object are not permitted"
6565             )
6566         # we don't actually modify arrays in-place with in-place Dataset
6567         # arithmetic -- this lets us automatically align things
6568         if isinstance(other, (DataArray, Dataset)):
6569             other = other.reindex_like(self, copy=False)
6570         g = ops.inplace_to_noninplace_op(f)
6571         ds = self._calculate_binary_op(g, other, inplace=True)
6572         self._replace_with_new_dims(
6573             ds._variables,
6574             ds._coord_names,
6575             attrs=ds._attrs,
6576             indexes=ds._indexes,
6577             inplace=True,
6578         )
6579         return self
6580 
6581     def _calculate_binary_op(
6582         self, f, other, join="inner", inplace: bool = False
6583     ) -> Dataset:
6584         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6585             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6586                 raise ValueError(
6587                     "datasets must have the same data variables "
6588                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6589                 )
6590 
6591             dest_vars = {}
6592 
6593             for k in lhs_data_vars:
6594                 if k in rhs_data_vars:
6595                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6596                 elif join in ["left", "outer"]:
6597                     dest_vars[k] = f(lhs_vars[k], np.nan)
6598             for k in rhs_data_vars:
6599                 if k not in dest_vars and join in ["right", "outer"]:
6600                     dest_vars[k] = f(rhs_vars[k], np.nan)
6601             return dest_vars
6602 
6603         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6604             # can't use our shortcut of doing the binary operation with
6605             # Variable objects, so apply over our data vars instead.
6606             new_data_vars = apply_over_both(
6607                 self.data_vars, other, self.data_vars, other
6608             )
6609             return type(self)(new_data_vars)
6610 
6611         other_coords: Coordinates | None = getattr(other, "coords", None)
6612         ds = self.coords.merge(other_coords)
6613 
6614         if isinstance(other, Dataset):
6615             new_vars = apply_over_both(
6616                 self.data_vars, other.data_vars, self.variables, other.variables
6617             )
6618         else:
6619             other_variable = getattr(other, "variable", other)
6620             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6621         ds._variables.update(new_vars)
6622         ds._dims = calculate_dimensions(ds._variables)
6623         return ds
6624 
6625     def _copy_attrs_from(self, other):
6626         self.attrs = other.attrs
6627         for v in other.variables:
6628             if v in self.variables:
6629                 self.variables[v].attrs = other.variables[v].attrs
6630 
6631     def diff(
6632         self: T_Dataset,
6633         dim: Hashable,
6634         n: int = 1,
6635         label: Literal["upper", "lower"] = "upper",
6636     ) -> T_Dataset:
6637         """Calculate the n-th order discrete difference along given axis.
6638 
6639         Parameters
6640         ----------
6641         dim : Hashable
6642             Dimension over which to calculate the finite difference.
6643         n : int, default: 1
6644             The number of times values are differenced.
6645         label : {"upper", "lower"}, default: "upper"
6646             The new coordinate in dimension ``dim`` will have the
6647             values of either the minuend's or subtrahend's coordinate
6648             for values 'upper' and 'lower', respectively.
6649 
6650         Returns
6651         -------
6652         difference : Dataset
6653             The n-th order finite difference of this object.
6654 
6655         Notes
6656         -----
6657         `n` matches numpy's behavior and is different from pandas' first argument named
6658         `periods`.
6659 
6660         Examples
6661         --------
6662         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6663         >>> ds.diff("x")
6664         <xarray.Dataset>
6665         Dimensions:  (x: 3)
6666         Dimensions without coordinates: x
6667         Data variables:
6668             foo      (x) int64 0 1 0
6669         >>> ds.diff("x", 2)
6670         <xarray.Dataset>
6671         Dimensions:  (x: 2)
6672         Dimensions without coordinates: x
6673         Data variables:
6674             foo      (x) int64 1 -1
6675 
6676         See Also
6677         --------
6678         Dataset.differentiate
6679         """
6680         if n == 0:
6681             return self
6682         if n < 0:
6683             raise ValueError(f"order `n` must be non-negative but got {n}")
6684 
6685         # prepare slices
6686         slice_start = {dim: slice(None, -1)}
6687         slice_end = {dim: slice(1, None)}
6688 
6689         # prepare new coordinate
6690         if label == "upper":
6691             slice_new = slice_end
6692         elif label == "lower":
6693             slice_new = slice_start
6694         else:
6695             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6696 
6697         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6698         variables = {}
6699 
6700         for name, var in self.variables.items():
6701             if name in index_vars:
6702                 variables[name] = index_vars[name]
6703             elif dim in var.dims:
6704                 if name in self.data_vars:
6705                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6706                 else:
6707                     variables[name] = var.isel(slice_new)
6708             else:
6709                 variables[name] = var
6710 
6711         difference = self._replace_with_new_dims(variables, indexes=indexes)
6712 
6713         if n > 1:
6714             return difference.diff(dim, n - 1)
6715         else:
6716             return difference
6717 
6718     def shift(
6719         self: T_Dataset,
6720         shifts: Mapping[Any, int] | None = None,
6721         fill_value: Any = xrdtypes.NA,
6722         **shifts_kwargs: int,
6723     ) -> T_Dataset:
6724 
6725         """Shift this dataset by an offset along one or more dimensions.
6726 
6727         Only data variables are moved; coordinates stay in place. This is
6728         consistent with the behavior of ``shift`` in pandas.
6729 
6730         Values shifted from beyond array bounds will appear at one end of
6731         each dimension, which are filled according to `fill_value`. For periodic
6732         offsets instead see `roll`.
6733 
6734         Parameters
6735         ----------
6736         shifts : mapping of hashable to int
6737             Integer offset to shift along each of the given dimensions.
6738             Positive offsets shift to the right; negative offsets shift to the
6739             left.
6740         fill_value : scalar or dict-like, optional
6741             Value to use for newly missing values. If a dict-like, maps
6742             variable names (including coordinates) to fill values.
6743         **shifts_kwargs
6744             The keyword arguments form of ``shifts``.
6745             One of shifts or shifts_kwargs must be provided.
6746 
6747         Returns
6748         -------
6749         shifted : Dataset
6750             Dataset with the same coordinates and attributes but shifted data
6751             variables.
6752 
6753         See Also
6754         --------
6755         roll
6756 
6757         Examples
6758         --------
6759         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6760         >>> ds.shift(x=2)
6761         <xarray.Dataset>
6762         Dimensions:  (x: 5)
6763         Dimensions without coordinates: x
6764         Data variables:
6765             foo      (x) object nan nan 'a' 'b' 'c'
6766         """
6767         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6768         invalid = [k for k in shifts if k not in self.dims]
6769         if invalid:
6770             raise ValueError(f"dimensions {invalid!r} do not exist")
6771 
6772         variables = {}
6773         for name, var in self.variables.items():
6774             if name in self.data_vars:
6775                 fill_value_ = (
6776                     fill_value.get(name, xrdtypes.NA)
6777                     if isinstance(fill_value, dict)
6778                     else fill_value
6779                 )
6780 
6781                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6782                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6783             else:
6784                 variables[name] = var
6785 
6786         return self._replace(variables)
6787 
6788     def roll(
6789         self: T_Dataset,
6790         shifts: Mapping[Any, int] | None = None,
6791         roll_coords: bool = False,
6792         **shifts_kwargs: int,
6793     ) -> T_Dataset:
6794         """Roll this dataset by an offset along one or more dimensions.
6795 
6796         Unlike shift, roll treats the given dimensions as periodic, so will not
6797         create any missing values to be filled.
6798 
6799         Also unlike shift, roll may rotate all variables, including coordinates
6800         if specified. The direction of rotation is consistent with
6801         :py:func:`numpy.roll`.
6802 
6803         Parameters
6804         ----------
6805         shifts : mapping of hashable to int, optional
6806             A dict with keys matching dimensions and values given
6807             by integers to rotate each of the given dimensions. Positive
6808             offsets roll to the right; negative offsets roll to the left.
6809         roll_coords : bool, default: False
6810             Indicates whether to roll the coordinates by the offset too.
6811         **shifts_kwargs : {dim: offset, ...}, optional
6812             The keyword arguments form of ``shifts``.
6813             One of shifts or shifts_kwargs must be provided.
6814 
6815         Returns
6816         -------
6817         rolled : Dataset
6818             Dataset with the same attributes but rolled data and coordinates.
6819 
6820         See Also
6821         --------
6822         shift
6823 
6824         Examples
6825         --------
6826         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6827         >>> ds.roll(x=2)
6828         <xarray.Dataset>
6829         Dimensions:  (x: 5)
6830         Coordinates:
6831           * x        (x) int64 0 1 2 3 4
6832         Data variables:
6833             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6834 
6835         >>> ds.roll(x=2, roll_coords=True)
6836         <xarray.Dataset>
6837         Dimensions:  (x: 5)
6838         Coordinates:
6839           * x        (x) int64 3 4 0 1 2
6840         Data variables:
6841             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6842 
6843         """
6844         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6845         invalid = [k for k in shifts if k not in self.dims]
6846         if invalid:
6847             raise ValueError(f"dimensions {invalid!r} do not exist")
6848 
6849         unrolled_vars: tuple[Hashable, ...]
6850 
6851         if roll_coords:
6852             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6853             unrolled_vars = ()
6854         else:
6855             indexes = dict(self._indexes)
6856             index_vars = dict(self.xindexes.variables)
6857             unrolled_vars = tuple(self.coords)
6858 
6859         variables = {}
6860         for k, var in self.variables.items():
6861             if k in index_vars:
6862                 variables[k] = index_vars[k]
6863             elif k not in unrolled_vars:
6864                 variables[k] = var.roll(
6865                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6866                 )
6867             else:
6868                 variables[k] = var
6869 
6870         return self._replace(variables, indexes=indexes)
6871 
6872     def sortby(
6873         self: T_Dataset,
6874         variables: Hashable | DataArray | list[Hashable | DataArray],
6875         ascending: bool = True,
6876     ) -> T_Dataset:
6877         """
6878         Sort object by labels or values (along an axis).
6879 
6880         Sorts the dataset, either along specified dimensions,
6881         or according to values of 1-D dataarrays that share dimension
6882         with calling object.
6883 
6884         If the input variables are dataarrays, then the dataarrays are aligned
6885         (via left-join) to the calling object prior to sorting by cell values.
6886         NaNs are sorted to the end, following Numpy convention.
6887 
6888         If multiple sorts along the same dimension is
6889         given, numpy's lexsort is performed along that dimension:
6890         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6891         and the FIRST key in the sequence is used as the primary sort key,
6892         followed by the 2nd key, etc.
6893 
6894         Parameters
6895         ----------
6896         variables : Hashable, DataArray, or list of hashable or DataArray
6897             1D DataArray objects or name(s) of 1D variable(s) in
6898             coords/data_vars whose values are used to sort the dataset.
6899         ascending : bool, default: True
6900             Whether to sort by ascending or descending order.
6901 
6902         Returns
6903         -------
6904         sorted : Dataset
6905             A new dataset where all the specified dims are sorted by dim
6906             labels.
6907 
6908         See Also
6909         --------
6910         DataArray.sortby
6911         numpy.sort
6912         pandas.sort_values
6913         pandas.sort_index
6914 
6915         Examples
6916         --------
6917         >>> ds = xr.Dataset(
6918         ...     {
6919         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6920         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6921         ...     },
6922         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6923         ... )
6924         >>> ds.sortby("x")
6925         <xarray.Dataset>
6926         Dimensions:  (x: 2, y: 2)
6927         Coordinates:
6928           * x        (x) <U1 'a' 'b'
6929           * y        (y) int64 1 0
6930         Data variables:
6931             A        (x, y) int64 3 4 1 2
6932             B        (x, y) int64 7 8 5 6
6933         """
6934         from .dataarray import DataArray
6935 
6936         if not isinstance(variables, list):
6937             variables = [variables]
6938         else:
6939             variables = variables
6940         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6941         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6942         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6943         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6944         vars_by_dim = defaultdict(list)
6945         for data_array in aligned_other_vars:
6946             if data_array.ndim != 1:
6947                 raise ValueError("Input DataArray is not 1-D.")
6948             (key,) = data_array.dims
6949             vars_by_dim[key].append(data_array)
6950 
6951         indices = {}
6952         for key, arrays in vars_by_dim.items():
6953             order = np.lexsort(tuple(reversed(arrays)))
6954             indices[key] = order if ascending else order[::-1]
6955         return aligned_self.isel(indices)
6956 
6957     def quantile(
6958         self: T_Dataset,
6959         q: ArrayLike,
6960         dim: str | Iterable[Hashable] | None = None,
6961         method: QUANTILE_METHODS = "linear",
6962         numeric_only: bool = False,
6963         keep_attrs: bool = None,
6964         skipna: bool = None,
6965         interpolation: QUANTILE_METHODS = None,
6966     ) -> T_Dataset:
6967         """Compute the qth quantile of the data along the specified dimension.
6968 
6969         Returns the qth quantiles(s) of the array elements for each variable
6970         in the Dataset.
6971 
6972         Parameters
6973         ----------
6974         q : float or array-like of float
6975             Quantile to compute, which must be between 0 and 1 inclusive.
6976         dim : str or Iterable of Hashable, optional
6977             Dimension(s) over which to apply quantile.
6978         method : str, default: "linear"
6979             This optional parameter specifies the interpolation method to use when the
6980             desired quantile lies between two data points. The options sorted by their R
6981             type as summarized in the H&F paper [1]_ are:
6982 
6983                 1. "inverted_cdf" (*)
6984                 2. "averaged_inverted_cdf" (*)
6985                 3. "closest_observation" (*)
6986                 4. "interpolated_inverted_cdf" (*)
6987                 5. "hazen" (*)
6988                 6. "weibull" (*)
6989                 7. "linear"  (default)
6990                 8. "median_unbiased" (*)
6991                 9. "normal_unbiased" (*)
6992 
6993             The first three methods are discontiuous.  The following discontinuous
6994             variations of the default "linear" (7.) option are also available:
6995 
6996                 * "lower"
6997                 * "higher"
6998                 * "midpoint"
6999                 * "nearest"
7000 
7001             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7002             was previously called "interpolation", renamed in accordance with numpy
7003             version 1.22.0.
7004 
7005             (*) These methods require numpy version 1.22 or newer.
7006 
7007         keep_attrs : bool, optional
7008             If True, the dataset's attributes (`attrs`) will be copied from
7009             the original object to the new one.  If False (default), the new
7010             object will be returned without attributes.
7011         numeric_only : bool, optional
7012             If True, only apply ``func`` to variables with a numeric dtype.
7013         skipna : bool, optional
7014             If True, skip missing values (as marked by NaN). By default, only
7015             skips missing values for float dtypes; other dtypes either do not
7016             have a sentinel missing value (int) or skipna=True has not been
7017             implemented (object, datetime64 or timedelta64).
7018 
7019         Returns
7020         -------
7021         quantiles : Dataset
7022             If `q` is a single quantile, then the result is a scalar for each
7023             variable in data_vars. If multiple percentiles are given, first
7024             axis of the result corresponds to the quantile and a quantile
7025             dimension is added to the return Dataset. The other dimensions are
7026             the dimensions that remain after the reduction of the array.
7027 
7028         See Also
7029         --------
7030         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7031 
7032         Examples
7033         --------
7034         >>> ds = xr.Dataset(
7035         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7036         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7037         ... )
7038         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7039         <xarray.Dataset>
7040         Dimensions:   ()
7041         Coordinates:
7042             quantile  float64 0.0
7043         Data variables:
7044             a         float64 0.7
7045         >>> ds.quantile(0, dim="x")
7046         <xarray.Dataset>
7047         Dimensions:   (y: 4)
7048         Coordinates:
7049           * y         (y) float64 1.0 1.5 2.0 2.5
7050             quantile  float64 0.0
7051         Data variables:
7052             a         (y) float64 0.7 4.2 2.6 1.5
7053         >>> ds.quantile([0, 0.5, 1])
7054         <xarray.Dataset>
7055         Dimensions:   (quantile: 3)
7056         Coordinates:
7057           * quantile  (quantile) float64 0.0 0.5 1.0
7058         Data variables:
7059             a         (quantile) float64 0.7 3.4 9.4
7060         >>> ds.quantile([0, 0.5, 1], dim="x")
7061         <xarray.Dataset>
7062         Dimensions:   (quantile: 3, y: 4)
7063         Coordinates:
7064           * y         (y) float64 1.0 1.5 2.0 2.5
7065           * quantile  (quantile) float64 0.0 0.5 1.0
7066         Data variables:
7067             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7068 
7069         References
7070         ----------
7071         .. [1] R. J. Hyndman and Y. Fan,
7072            "Sample quantiles in statistical packages,"
7073            The American Statistician, 50(4), pp. 361-365, 1996
7074         """
7075 
7076         # interpolation renamed to method in version 0.21.0
7077         # check here and in variable to avoid repeated warnings
7078         if interpolation is not None:
7079             warnings.warn(
7080                 "The `interpolation` argument to quantile was renamed to `method`.",
7081                 FutureWarning,
7082             )
7083 
7084             if method != "linear":
7085                 raise TypeError("Cannot pass interpolation and method keywords!")
7086 
7087             method = interpolation
7088 
7089         dims: set[Hashable]
7090         if isinstance(dim, str):
7091             dims = {dim}
7092         elif dim is None or dim is ...:
7093             dims = set(self.dims)
7094         else:
7095             dims = set(dim)
7096 
7097         _assert_empty(
7098             tuple(d for d in dims if d not in self.dims),
7099             "Dataset does not contain the dimensions: %s",
7100         )
7101 
7102         q = np.asarray(q, dtype=np.float64)
7103 
7104         variables = {}
7105         for name, var in self.variables.items():
7106             reduce_dims = [d for d in var.dims if d in dims]
7107             if reduce_dims or not var.dims:
7108                 if name not in self.coords:
7109                     if (
7110                         not numeric_only
7111                         or np.issubdtype(var.dtype, np.number)
7112                         or var.dtype == np.bool_
7113                     ):
7114                         variables[name] = var.quantile(
7115                             q,
7116                             dim=reduce_dims,
7117                             method=method,
7118                             keep_attrs=keep_attrs,
7119                             skipna=skipna,
7120                         )
7121 
7122             else:
7123                 variables[name] = var
7124 
7125         # construct the new dataset
7126         coord_names = {k for k in self.coords if k in variables}
7127         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7128         if keep_attrs is None:
7129             keep_attrs = _get_keep_attrs(default=False)
7130         attrs = self.attrs if keep_attrs else None
7131         new = self._replace_with_new_dims(
7132             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7133         )
7134         return new.assign_coords(quantile=q)
7135 
7136     def rank(
7137         self: T_Dataset,
7138         dim: Hashable,
7139         pct: bool = False,
7140         keep_attrs: bool | None = None,
7141     ) -> T_Dataset:
7142         """Ranks the data.
7143 
7144         Equal values are assigned a rank that is the average of the ranks that
7145         would have been otherwise assigned to all of the values within
7146         that set.
7147         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7148 
7149         NaNs in the input array are returned as NaNs.
7150 
7151         The `bottleneck` library is required.
7152 
7153         Parameters
7154         ----------
7155         dim : Hashable
7156             Dimension over which to compute rank.
7157         pct : bool, default: False
7158             If True, compute percentage ranks, otherwise compute integer ranks.
7159         keep_attrs : bool or None, optional
7160             If True, the dataset's attributes (`attrs`) will be copied from
7161             the original object to the new one.  If False, the new
7162             object will be returned without attributes.
7163 
7164         Returns
7165         -------
7166         ranked : Dataset
7167             Variables that do not depend on `dim` are dropped.
7168         """
7169         if not OPTIONS["use_bottleneck"]:
7170             raise RuntimeError(
7171                 "rank requires bottleneck to be enabled."
7172                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7173             )
7174 
7175         if dim not in self.dims:
7176             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7177 
7178         variables = {}
7179         for name, var in self.variables.items():
7180             if name in self.data_vars:
7181                 if dim in var.dims:
7182                     variables[name] = var.rank(dim, pct=pct)
7183             else:
7184                 variables[name] = var
7185 
7186         coord_names = set(self.coords)
7187         if keep_attrs is None:
7188             keep_attrs = _get_keep_attrs(default=False)
7189         attrs = self.attrs if keep_attrs else None
7190         return self._replace(variables, coord_names, attrs=attrs)
7191 
7192     def differentiate(
7193         self: T_Dataset,
7194         coord: Hashable,
7195         edge_order: Literal[1, 2] = 1,
7196         datetime_unit: DatetimeUnitOptions | None = None,
7197     ) -> T_Dataset:
7198         """ Differentiate with the second order accurate central
7199         differences.
7200 
7201         .. note::
7202             This feature is limited to simple cartesian geometry, i.e. coord
7203             must be one dimensional.
7204 
7205         Parameters
7206         ----------
7207         coord : Hashable
7208             The coordinate to be used to compute the gradient.
7209         edge_order : {1, 2}, default: 1
7210             N-th order accurate differences at the boundaries.
7211         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7212             "us", "ns", "ps", "fs", "as", None}, default: None
7213             Unit to compute gradient. Only valid for datetime coordinate.
7214 
7215         Returns
7216         -------
7217         differentiated: Dataset
7218 
7219         See also
7220         --------
7221         numpy.gradient: corresponding numpy function
7222         """
7223         from .variable import Variable
7224 
7225         if coord not in self.variables and coord not in self.dims:
7226             raise ValueError(f"Coordinate {coord} does not exist.")
7227 
7228         coord_var = self[coord].variable
7229         if coord_var.ndim != 1:
7230             raise ValueError(
7231                 "Coordinate {} must be 1 dimensional but is {}"
7232                 " dimensional".format(coord, coord_var.ndim)
7233             )
7234 
7235         dim = coord_var.dims[0]
7236         if _contains_datetime_like_objects(coord_var):
7237             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7238                 datetime_unit = cast(
7239                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7240                 )
7241             elif datetime_unit is None:
7242                 datetime_unit = "s"  # Default to seconds for cftime objects
7243             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7244 
7245         variables = {}
7246         for k, v in self.variables.items():
7247             if k in self.data_vars and dim in v.dims and k not in self.coords:
7248                 if _contains_datetime_like_objects(v):
7249                     v = v._to_numeric(datetime_unit=datetime_unit)
7250                 grad = duck_array_ops.gradient(
7251                     v.data,
7252                     coord_var.data,
7253                     edge_order=edge_order,
7254                     axis=v.get_axis_num(dim),
7255                 )
7256                 variables[k] = Variable(v.dims, grad)
7257             else:
7258                 variables[k] = v
7259         return self._replace(variables)
7260 
7261     def integrate(
7262         self: T_Dataset,
7263         coord: Hashable | Sequence[Hashable],
7264         datetime_unit: DatetimeUnitOptions = None,
7265     ) -> T_Dataset:
7266         """Integrate along the given coordinate using the trapezoidal rule.
7267 
7268         .. note::
7269             This feature is limited to simple cartesian geometry, i.e. coord
7270             must be one dimensional.
7271 
7272         Parameters
7273         ----------
7274         coord : hashable, or sequence of hashable
7275             Coordinate(s) used for the integration.
7276         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7277                         'ps', 'fs', 'as', None}, optional
7278             Specify the unit if datetime coordinate is used.
7279 
7280         Returns
7281         -------
7282         integrated : Dataset
7283 
7284         See also
7285         --------
7286         DataArray.integrate
7287         numpy.trapz : corresponding numpy function
7288 
7289         Examples
7290         --------
7291         >>> ds = xr.Dataset(
7292         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7293         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7294         ... )
7295         >>> ds
7296         <xarray.Dataset>
7297         Dimensions:  (x: 4)
7298         Coordinates:
7299           * x        (x) int64 0 1 2 3
7300             y        (x) int64 1 7 3 5
7301         Data variables:
7302             a        (x) int64 5 5 6 6
7303             b        (x) int64 1 2 1 0
7304         >>> ds.integrate("x")
7305         <xarray.Dataset>
7306         Dimensions:  ()
7307         Data variables:
7308             a        float64 16.5
7309             b        float64 3.5
7310         >>> ds.integrate("y")
7311         <xarray.Dataset>
7312         Dimensions:  ()
7313         Data variables:
7314             a        float64 20.0
7315             b        float64 4.0
7316         """
7317         if not isinstance(coord, (list, tuple)):
7318             coord = (coord,)
7319         result = self
7320         for c in coord:
7321             result = result._integrate_one(c, datetime_unit=datetime_unit)
7322         return result
7323 
7324     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7325         from .variable import Variable
7326 
7327         if coord not in self.variables and coord not in self.dims:
7328             raise ValueError(f"Coordinate {coord} does not exist.")
7329 
7330         coord_var = self[coord].variable
7331         if coord_var.ndim != 1:
7332             raise ValueError(
7333                 "Coordinate {} must be 1 dimensional but is {}"
7334                 " dimensional".format(coord, coord_var.ndim)
7335             )
7336 
7337         dim = coord_var.dims[0]
7338         if _contains_datetime_like_objects(coord_var):
7339             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7340                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7341             elif datetime_unit is None:
7342                 datetime_unit = "s"  # Default to seconds for cftime objects
7343             coord_var = coord_var._replace(
7344                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7345             )
7346 
7347         variables = {}
7348         coord_names = set()
7349         for k, v in self.variables.items():
7350             if k in self.coords:
7351                 if dim not in v.dims or cumulative:
7352                     variables[k] = v
7353                     coord_names.add(k)
7354             else:
7355                 if k in self.data_vars and dim in v.dims:
7356                     if _contains_datetime_like_objects(v):
7357                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7358                     if cumulative:
7359                         integ = duck_array_ops.cumulative_trapezoid(
7360                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7361                         )
7362                         v_dims = v.dims
7363                     else:
7364                         integ = duck_array_ops.trapz(
7365                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7366                         )
7367                         v_dims = list(v.dims)
7368                         v_dims.remove(dim)
7369                     variables[k] = Variable(v_dims, integ)
7370                 else:
7371                     variables[k] = v
7372         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7373         return self._replace_with_new_dims(
7374             variables, coord_names=coord_names, indexes=indexes
7375         )
7376 
7377     def cumulative_integrate(
7378         self: T_Dataset,
7379         coord: Hashable | Sequence[Hashable],
7380         datetime_unit: DatetimeUnitOptions = None,
7381     ) -> T_Dataset:
7382         """Integrate along the given coordinate using the trapezoidal rule.
7383 
7384         .. note::
7385             This feature is limited to simple cartesian geometry, i.e. coord
7386             must be one dimensional.
7387 
7388             The first entry of the cumulative integral of each variable is always 0, in
7389             order to keep the length of the dimension unchanged between input and
7390             output.
7391 
7392         Parameters
7393         ----------
7394         coord : hashable, or sequence of hashable
7395             Coordinate(s) used for the integration.
7396         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7397                         'ps', 'fs', 'as', None}, optional
7398             Specify the unit if datetime coordinate is used.
7399 
7400         Returns
7401         -------
7402         integrated : Dataset
7403 
7404         See also
7405         --------
7406         DataArray.cumulative_integrate
7407         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7408 
7409         Examples
7410         --------
7411         >>> ds = xr.Dataset(
7412         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7413         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7414         ... )
7415         >>> ds
7416         <xarray.Dataset>
7417         Dimensions:  (x: 4)
7418         Coordinates:
7419           * x        (x) int64 0 1 2 3
7420             y        (x) int64 1 7 3 5
7421         Data variables:
7422             a        (x) int64 5 5 6 6
7423             b        (x) int64 1 2 1 0
7424         >>> ds.cumulative_integrate("x")
7425         <xarray.Dataset>
7426         Dimensions:  (x: 4)
7427         Coordinates:
7428           * x        (x) int64 0 1 2 3
7429             y        (x) int64 1 7 3 5
7430         Data variables:
7431             a        (x) float64 0.0 5.0 10.5 16.5
7432             b        (x) float64 0.0 1.5 3.0 3.5
7433         >>> ds.cumulative_integrate("y")
7434         <xarray.Dataset>
7435         Dimensions:  (x: 4)
7436         Coordinates:
7437           * x        (x) int64 0 1 2 3
7438             y        (x) int64 1 7 3 5
7439         Data variables:
7440             a        (x) float64 0.0 30.0 8.0 20.0
7441             b        (x) float64 0.0 9.0 3.0 4.0
7442         """
7443         if not isinstance(coord, (list, tuple)):
7444             coord = (coord,)
7445         result = self
7446         for c in coord:
7447             result = result._integrate_one(
7448                 c, datetime_unit=datetime_unit, cumulative=True
7449             )
7450         return result
7451 
7452     @property
7453     def real(self: T_Dataset) -> T_Dataset:
7454         """
7455         The real part of each data variable.
7456 
7457         See Also
7458         --------
7459         numpy.ndarray.real
7460         """
7461         return self.map(lambda x: x.real, keep_attrs=True)
7462 
7463     @property
7464     def imag(self: T_Dataset) -> T_Dataset:
7465         """
7466         The imaginary part of each data variable.
7467 
7468         See Also
7469         --------
7470         numpy.ndarray.imag
7471         """
7472         return self.map(lambda x: x.imag, keep_attrs=True)
7473 
7474     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7475 
7476     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7477         """Returns a ``Dataset`` with variables that match specific conditions.
7478 
7479         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7480         containing only the variables for which all the filter tests pass.
7481         These tests are either ``key=value`` for which the attribute ``key``
7482         has the exact value ``value`` or the callable passed into
7483         ``key=callable`` returns True. The callable will be passed a single
7484         value, either the value of the attribute ``key`` or ``None`` if the
7485         DataArray does not have an attribute with the name ``key``.
7486 
7487         Parameters
7488         ----------
7489         **kwargs
7490             key : str
7491                 Attribute name.
7492             value : callable or obj
7493                 If value is a callable, it should return a boolean in the form
7494                 of bool = func(attr) where attr is da.attrs[key].
7495                 Otherwise, value will be compared to the each
7496                 DataArray's attrs[key].
7497 
7498         Returns
7499         -------
7500         new : Dataset
7501             New dataset with variables filtered by attribute.
7502 
7503         Examples
7504         --------
7505         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7506         >>> precip = 10 * np.random.rand(2, 2, 3)
7507         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7508         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7509         >>> dims = ["x", "y", "time"]
7510         >>> temp_attr = dict(standard_name="air_potential_temperature")
7511         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7512 
7513         >>> ds = xr.Dataset(
7514         ...     dict(
7515         ...         temperature=(dims, temp, temp_attr),
7516         ...         precipitation=(dims, precip, precip_attr),
7517         ...     ),
7518         ...     coords=dict(
7519         ...         lon=(["x", "y"], lon),
7520         ...         lat=(["x", "y"], lat),
7521         ...         time=pd.date_range("2014-09-06", periods=3),
7522         ...         reference_time=pd.Timestamp("2014-09-05"),
7523         ...     ),
7524         ... )
7525 
7526         Get variables matching a specific standard_name:
7527 
7528         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7529         <xarray.Dataset>
7530         Dimensions:         (x: 2, y: 2, time: 3)
7531         Coordinates:
7532             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7533             lat             (x, y) float64 42.25 42.21 42.63 42.59
7534           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7535             reference_time  datetime64[ns] 2014-09-05
7536         Dimensions without coordinates: x, y
7537         Data variables:
7538             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7539 
7540         Get all variables that have a standard_name attribute:
7541 
7542         >>> standard_name = lambda v: v is not None
7543         >>> ds.filter_by_attrs(standard_name=standard_name)
7544         <xarray.Dataset>
7545         Dimensions:         (x: 2, y: 2, time: 3)
7546         Coordinates:
7547             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7548             lat             (x, y) float64 42.25 42.21 42.63 42.59
7549           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7550             reference_time  datetime64[ns] 2014-09-05
7551         Dimensions without coordinates: x, y
7552         Data variables:
7553             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7554             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7555 
7556         """
7557         selection = []
7558         for var_name, variable in self.variables.items():
7559             has_value_flag = False
7560             for attr_name, pattern in kwargs.items():
7561                 attr_value = variable.attrs.get(attr_name)
7562                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7563                     has_value_flag = True
7564                 else:
7565                     has_value_flag = False
7566                     break
7567             if has_value_flag is True:
7568                 selection.append(var_name)
7569         return self[selection]
7570 
7571     def unify_chunks(self: T_Dataset) -> T_Dataset:
7572         """Unify chunk size along all chunked dimensions of this Dataset.
7573 
7574         Returns
7575         -------
7576         Dataset with consistent chunk sizes for all dask-array variables
7577 
7578         See Also
7579         --------
7580         dask.array.core.unify_chunks
7581         """
7582 
7583         return unify_chunks(self)[0]
7584 
7585     def map_blocks(
7586         self,
7587         func: Callable[..., T_Xarray],
7588         args: Sequence[Any] = (),
7589         kwargs: Mapping[str, Any] | None = None,
7590         template: DataArray | Dataset | None = None,
7591     ) -> T_Xarray:
7592         """
7593         Apply a function to each block of this Dataset.
7594 
7595         .. warning::
7596             This method is experimental and its signature may change.
7597 
7598         Parameters
7599         ----------
7600         func : callable
7601             User-provided function that accepts a Dataset as its first
7602             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7603             corresponding to one chunk along each chunked dimension. ``func`` will be
7604             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7605 
7606             This function must return either a single DataArray or a single Dataset.
7607 
7608             This function cannot add a new chunked dimension.
7609         args : sequence
7610             Passed to func after unpacking and subsetting any xarray objects by blocks.
7611             xarray objects in args must be aligned with obj, otherwise an error is raised.
7612         kwargs : Mapping or None
7613             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7614             subset to blocks. Passing dask collections in kwargs is not allowed.
7615         template : DataArray, Dataset or None, optional
7616             xarray object representing the final result after compute is called. If not provided,
7617             the function will be first run on mocked-up data, that looks like this object but
7618             has sizes 0, to determine properties of the returned object such as dtype,
7619             variable names, attributes, new dimensions and new indexes (if any).
7620             ``template`` must be provided if the function changes the size of existing dimensions.
7621             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7622             ``attrs`` set by ``func`` will be ignored.
7623 
7624         Returns
7625         -------
7626         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7627         function.
7628 
7629         Notes
7630         -----
7631         This function is designed for when ``func`` needs to manipulate a whole xarray object
7632         subset to each block. Each block is loaded into memory. In the more common case where
7633         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7634 
7635         If none of the variables in this object is backed by dask arrays, calling this function is
7636         equivalent to calling ``func(obj, *args, **kwargs)``.
7637 
7638         See Also
7639         --------
7640         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7641         xarray.DataArray.map_blocks
7642 
7643         Examples
7644         --------
7645         Calculate an anomaly from climatology using ``.groupby()``. Using
7646         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7647         its indices, and its methods like ``.groupby()``.
7648 
7649         >>> def calculate_anomaly(da, groupby_type="time.month"):
7650         ...     gb = da.groupby(groupby_type)
7651         ...     clim = gb.mean(dim="time")
7652         ...     return gb - clim
7653         ...
7654         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7655         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7656         >>> np.random.seed(123)
7657         >>> array = xr.DataArray(
7658         ...     np.random.rand(len(time)),
7659         ...     dims=["time"],
7660         ...     coords={"time": time, "month": month},
7661         ... ).chunk()
7662         >>> ds = xr.Dataset({"a": array})
7663         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7664         <xarray.Dataset>
7665         Dimensions:  (time: 24)
7666         Coordinates:
7667           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7668             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7669         Data variables:
7670             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7671 
7672         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7673         to the function being applied in ``xr.map_blocks()``:
7674 
7675         >>> ds.map_blocks(
7676         ...     calculate_anomaly,
7677         ...     kwargs={"groupby_type": "time.year"},
7678         ...     template=ds,
7679         ... )
7680         <xarray.Dataset>
7681         Dimensions:  (time: 24)
7682         Coordinates:
7683           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7684             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7685         Data variables:
7686             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7687         """
7688         from .parallel import map_blocks
7689 
7690         return map_blocks(func, self, args, kwargs, template)
7691 
7692     def polyfit(
7693         self: T_Dataset,
7694         dim: Hashable,
7695         deg: int,
7696         skipna: bool | None = None,
7697         rcond: float | None = None,
7698         w: Hashable | Any = None,
7699         full: bool = False,
7700         cov: bool | Literal["unscaled"] = False,
7701     ) -> T_Dataset:
7702         """
7703         Least squares polynomial fit.
7704 
7705         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7706         invalid values when `skipna = True`.
7707 
7708         Parameters
7709         ----------
7710         dim : hashable
7711             Coordinate along which to fit the polynomials.
7712         deg : int
7713             Degree of the fitting polynomial.
7714         skipna : bool or None, optional
7715             If True, removes all invalid values before fitting each 1D slices of the array.
7716             Default is True if data is stored in a dask.array or if there is any
7717             invalid values, False otherwise.
7718         rcond : float or None, optional
7719             Relative condition number to the fit.
7720         w : hashable or Any, optional
7721             Weights to apply to the y-coordinate of the sample points.
7722             Can be an array-like object or the name of a coordinate in the dataset.
7723         full : bool, default: False
7724             Whether to return the residuals, matrix rank and singular values in addition
7725             to the coefficients.
7726         cov : bool or "unscaled", default: False
7727             Whether to return to the covariance matrix in addition to the coefficients.
7728             The matrix is not scaled if `cov='unscaled'`.
7729 
7730         Returns
7731         -------
7732         polyfit_results : Dataset
7733             A single dataset which contains (for each "var" in the input dataset):
7734 
7735             [var]_polyfit_coefficients
7736                 The coefficients of the best fit for each variable in this dataset.
7737             [var]_polyfit_residuals
7738                 The residuals of the least-square computation for each variable (only included if `full=True`)
7739                 When the matrix rank is deficient, np.nan is returned.
7740             [dim]_matrix_rank
7741                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7742                 The rank is computed ignoring the NaN values that might be skipped.
7743             [dim]_singular_values
7744                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7745             [var]_polyfit_covariance
7746                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7747 
7748         Warns
7749         -----
7750         RankWarning
7751             The rank of the coefficient matrix in the least-squares fit is deficient.
7752             The warning is not raised with in-memory (not dask) data and `full=True`.
7753 
7754         See Also
7755         --------
7756         numpy.polyfit
7757         numpy.polyval
7758         xarray.polyval
7759         """
7760         from .dataarray import DataArray
7761 
7762         variables = {}
7763         skipna_da = skipna
7764 
7765         x = get_clean_interp_index(self, dim, strict=False)
7766         xname = f"{self[dim].name}_"
7767         order = int(deg) + 1
7768         lhs = np.vander(x, order)
7769 
7770         if rcond is None:
7771             rcond = (
7772                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7773             )
7774 
7775         # Weights:
7776         if w is not None:
7777             if isinstance(w, Hashable):
7778                 w = self.coords[w]
7779             w = np.asarray(w)
7780             if w.ndim != 1:
7781                 raise TypeError("Expected a 1-d array for weights.")
7782             if w.shape[0] != lhs.shape[0]:
7783                 raise TypeError(f"Expected w and {dim} to have the same length")
7784             lhs *= w[:, np.newaxis]
7785 
7786         # Scaling
7787         scale = np.sqrt((lhs * lhs).sum(axis=0))
7788         lhs /= scale
7789 
7790         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7791 
7792         rank = np.linalg.matrix_rank(lhs)
7793 
7794         if full:
7795             rank = DataArray(rank, name=xname + "matrix_rank")
7796             variables[rank.name] = rank
7797             _sing = np.linalg.svd(lhs, compute_uv=False)
7798             sing = DataArray(
7799                 _sing,
7800                 dims=(degree_dim,),
7801                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7802                 name=xname + "singular_values",
7803             )
7804             variables[sing.name] = sing
7805 
7806         for name, da in self.data_vars.items():
7807             if dim not in da.dims:
7808                 continue
7809 
7810             if is_duck_dask_array(da.data) and (
7811                 rank != order or full or skipna is None
7812             ):
7813                 # Current algorithm with dask and skipna=False neither supports
7814                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7815                 skipna_da = True
7816             elif skipna is None:
7817                 skipna_da = bool(np.any(da.isnull()))
7818 
7819             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7820             stacked_coords: dict[Hashable, DataArray] = {}
7821             if dims_to_stack:
7822                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7823                 rhs = da.transpose(dim, *dims_to_stack).stack(
7824                     {stacked_dim: dims_to_stack}
7825                 )
7826                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7827                 scale_da = scale[:, np.newaxis]
7828             else:
7829                 rhs = da
7830                 scale_da = scale
7831 
7832             if w is not None:
7833                 rhs *= w[:, np.newaxis]
7834 
7835             with warnings.catch_warnings():
7836                 if full:  # Copy np.polyfit behavior
7837                     warnings.simplefilter("ignore", np.RankWarning)
7838                 else:  # Raise only once per variable
7839                     warnings.simplefilter("once", np.RankWarning)
7840 
7841                 coeffs, residuals = duck_array_ops.least_squares(
7842                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7843                 )
7844 
7845             if isinstance(name, str):
7846                 name = f"{name}_"
7847             else:
7848                 # Thus a ReprObject => polyfit was called on a DataArray
7849                 name = ""
7850 
7851             coeffs = DataArray(
7852                 coeffs / scale_da,
7853                 dims=[degree_dim] + list(stacked_coords.keys()),
7854                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7855                 name=name + "polyfit_coefficients",
7856             )
7857             if dims_to_stack:
7858                 coeffs = coeffs.unstack(stacked_dim)
7859             variables[coeffs.name] = coeffs
7860 
7861             if full or (cov is True):
7862                 residuals = DataArray(
7863                     residuals if dims_to_stack else residuals.squeeze(),
7864                     dims=list(stacked_coords.keys()),
7865                     coords=stacked_coords,
7866                     name=name + "polyfit_residuals",
7867                 )
7868                 if dims_to_stack:
7869                     residuals = residuals.unstack(stacked_dim)
7870                 variables[residuals.name] = residuals
7871 
7872             if cov:
7873                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7874                 Vbase /= np.outer(scale, scale)
7875                 if cov == "unscaled":
7876                     fac = 1
7877                 else:
7878                     if x.shape[0] <= order:
7879                         raise ValueError(
7880                             "The number of data points must exceed order to scale the covariance matrix."
7881                         )
7882                     fac = residuals / (x.shape[0] - order)
7883                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7884                 variables[name + "polyfit_covariance"] = covariance
7885 
7886         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7887 
7888     def pad(
7889         self: T_Dataset,
7890         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7891         mode: PadModeOptions = "constant",
7892         stat_length: int
7893         | tuple[int, int]
7894         | Mapping[Any, tuple[int, int]]
7895         | None = None,
7896         constant_values: (
7897             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7898         ) = None,
7899         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7900         reflect_type: PadReflectOptions = None,
7901         **pad_width_kwargs: Any,
7902     ) -> T_Dataset:
7903         """Pad this dataset along one or more dimensions.
7904 
7905         .. warning::
7906             This function is experimental and its behaviour is likely to change
7907             especially regarding padding of dimension coordinates (or IndexVariables).
7908 
7909         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7910         coordinates will be padded with the same mode, otherwise coordinates
7911         are padded using the "constant" mode with fill_value dtypes.NA.
7912 
7913         Parameters
7914         ----------
7915         pad_width : mapping of hashable to tuple of int
7916             Mapping with the form of {dim: (pad_before, pad_after)}
7917             describing the number of values padded along each dimension.
7918             {dim: pad} is a shortcut for pad_before = pad_after = pad
7919         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7920             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7921             How to pad the DataArray (taken from numpy docs):
7922 
7923             - "constant": Pads with a constant value.
7924             - "edge": Pads with the edge values of array.
7925             - "linear_ramp": Pads with the linear ramp between end_value and the
7926               array edge value.
7927             - "maximum": Pads with the maximum value of all or part of the
7928               vector along each axis.
7929             - "mean": Pads with the mean value of all or part of the
7930               vector along each axis.
7931             - "median": Pads with the median value of all or part of the
7932               vector along each axis.
7933             - "minimum": Pads with the minimum value of all or part of the
7934               vector along each axis.
7935             - "reflect": Pads with the reflection of the vector mirrored on
7936               the first and last values of the vector along each axis.
7937             - "symmetric": Pads with the reflection of the vector mirrored
7938               along the edge of the array.
7939             - "wrap": Pads with the wrap of the vector along the axis.
7940               The first values are used to pad the end and the
7941               end values are used to pad the beginning.
7942 
7943         stat_length : int, tuple or mapping of hashable to tuple, default: None
7944             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7945             values at edge of each axis used to calculate the statistic value.
7946             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7947             statistic lengths along each dimension.
7948             ((before, after),) yields same before and after statistic lengths
7949             for each dimension.
7950             (stat_length,) or int is a shortcut for before = after = statistic
7951             length for all axes.
7952             Default is ``None``, to use the entire axis.
7953         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7954             Used in 'constant'.  The values to set the padded values for each
7955             axis.
7956             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7957             pad constants along each dimension.
7958             ``((before, after),)`` yields same before and after constants for each
7959             dimension.
7960             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7961             all dimensions.
7962             Default is 0.
7963         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7964             Used in 'linear_ramp'.  The values used for the ending value of the
7965             linear_ramp and that will form the edge of the padded array.
7966             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7967             end values along each dimension.
7968             ``((before, after),)`` yields same before and after end values for each
7969             axis.
7970             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7971             all axes.
7972             Default is 0.
7973         reflect_type : {"even", "odd", None}, optional
7974             Used in "reflect", and "symmetric".  The "even" style is the
7975             default with an unaltered reflection around the edge value.  For
7976             the "odd" style, the extended part of the array is created by
7977             subtracting the reflected values from two times the edge value.
7978         **pad_width_kwargs
7979             The keyword arguments form of ``pad_width``.
7980             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7981 
7982         Returns
7983         -------
7984         padded : Dataset
7985             Dataset with the padded coordinates and data.
7986 
7987         See Also
7988         --------
7989         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7990 
7991         Notes
7992         -----
7993         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7994         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7995         specify ``constant_values=np.nan``
7996 
7997         Padding coordinates will drop their corresponding index (if any) and will reset default
7998         indexes for dimension coordinates.
7999 
8000         Examples
8001         --------
8002         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8003         >>> ds.pad(x=(1, 2))
8004         <xarray.Dataset>
8005         Dimensions:  (x: 8)
8006         Dimensions without coordinates: x
8007         Data variables:
8008             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8009         """
8010         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8011 
8012         if mode in ("edge", "reflect", "symmetric", "wrap"):
8013             coord_pad_mode = mode
8014             coord_pad_options = {
8015                 "stat_length": stat_length,
8016                 "constant_values": constant_values,
8017                 "end_values": end_values,
8018                 "reflect_type": reflect_type,
8019             }
8020         else:
8021             coord_pad_mode = "constant"
8022             coord_pad_options = {}
8023 
8024         variables = {}
8025 
8026         # keep indexes that won't be affected by pad and drop all other indexes
8027         xindexes = self.xindexes
8028         pad_dims = set(pad_width)
8029         indexes = {}
8030         for k, idx in xindexes.items():
8031             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8032                 indexes[k] = idx
8033 
8034         for name, var in self.variables.items():
8035             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8036             if not var_pad_width:
8037                 variables[name] = var
8038             elif name in self.data_vars:
8039                 variables[name] = var.pad(
8040                     pad_width=var_pad_width,
8041                     mode=mode,
8042                     stat_length=stat_length,
8043                     constant_values=constant_values,
8044                     end_values=end_values,
8045                     reflect_type=reflect_type,
8046                 )
8047             else:
8048                 variables[name] = var.pad(
8049                     pad_width=var_pad_width,
8050                     mode=coord_pad_mode,
8051                     **coord_pad_options,  # type: ignore[arg-type]
8052                 )
8053                 # reset default index of dimension coordinates
8054                 if (name,) == var.dims:
8055                     dim_var = {name: variables[name]}
8056                     index = PandasIndex.from_variables(dim_var, options={})
8057                     index_vars = index.create_variables(dim_var)
8058                     indexes[name] = index
8059                     variables[name] = index_vars[name]
8060 
8061         return self._replace_with_new_dims(variables, indexes=indexes)
8062 
8063     def idxmin(
8064         self: T_Dataset,
8065         dim: Hashable | None = None,
8066         skipna: bool | None = None,
8067         fill_value: Any = xrdtypes.NA,
8068         keep_attrs: bool | None = None,
8069     ) -> T_Dataset:
8070         """Return the coordinate label of the minimum value along a dimension.
8071 
8072         Returns a new `Dataset` named after the dimension with the values of
8073         the coordinate labels along that dimension corresponding to minimum
8074         values along that dimension.
8075 
8076         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8077         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8078 
8079         Parameters
8080         ----------
8081         dim : Hashable, optional
8082             Dimension over which to apply `idxmin`.  This is optional for 1D
8083             variables, but required for variables with 2 or more dimensions.
8084         skipna : bool or None, optional
8085             If True, skip missing values (as marked by NaN). By default, only
8086             skips missing values for ``float``, ``complex``, and ``object``
8087             dtypes; other dtypes either do not have a sentinel missing value
8088             (``int``) or ``skipna=True`` has not been implemented
8089             (``datetime64`` or ``timedelta64``).
8090         fill_value : Any, default: NaN
8091             Value to be filled in case all of the values along a dimension are
8092             null.  By default this is NaN.  The fill value and result are
8093             automatically converted to a compatible dtype if possible.
8094             Ignored if ``skipna`` is False.
8095         keep_attrs : bool or None, optional
8096             If True, the attributes (``attrs``) will be copied from the
8097             original object to the new one. If False, the new object
8098             will be returned without attributes.
8099 
8100         Returns
8101         -------
8102         reduced : Dataset
8103             New `Dataset` object with `idxmin` applied to its data and the
8104             indicated dimension removed.
8105 
8106         See Also
8107         --------
8108         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8109 
8110         Examples
8111         --------
8112         >>> array1 = xr.DataArray(
8113         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8114         ... )
8115         >>> array2 = xr.DataArray(
8116         ...     [
8117         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8118         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8119         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8120         ...     ],
8121         ...     dims=["y", "x"],
8122         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8123         ... )
8124         >>> ds = xr.Dataset({"int": array1, "float": array2})
8125         >>> ds.min(dim="x")
8126         <xarray.Dataset>
8127         Dimensions:  (y: 3)
8128         Coordinates:
8129           * y        (y) int64 -1 0 1
8130         Data variables:
8131             int      int64 -2
8132             float    (y) float64 -2.0 -4.0 1.0
8133         >>> ds.argmin(dim="x")
8134         <xarray.Dataset>
8135         Dimensions:  (y: 3)
8136         Coordinates:
8137           * y        (y) int64 -1 0 1
8138         Data variables:
8139             int      int64 4
8140             float    (y) int64 4 0 2
8141         >>> ds.idxmin(dim="x")
8142         <xarray.Dataset>
8143         Dimensions:  (y: 3)
8144         Coordinates:
8145           * y        (y) int64 -1 0 1
8146         Data variables:
8147             int      <U1 'e'
8148             float    (y) object 'e' 'a' 'c'
8149         """
8150         return self.map(
8151             methodcaller(
8152                 "idxmin",
8153                 dim=dim,
8154                 skipna=skipna,
8155                 fill_value=fill_value,
8156                 keep_attrs=keep_attrs,
8157             )
8158         )
8159 
8160     def idxmax(
8161         self: T_Dataset,
8162         dim: Hashable | None = None,
8163         skipna: bool | None = None,
8164         fill_value: Any = xrdtypes.NA,
8165         keep_attrs: bool | None = None,
8166     ) -> T_Dataset:
8167         """Return the coordinate label of the maximum value along a dimension.
8168 
8169         Returns a new `Dataset` named after the dimension with the values of
8170         the coordinate labels along that dimension corresponding to maximum
8171         values along that dimension.
8172 
8173         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8174         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8175 
8176         Parameters
8177         ----------
8178         dim : str, optional
8179             Dimension over which to apply `idxmax`.  This is optional for 1D
8180             variables, but required for variables with 2 or more dimensions.
8181         skipna : bool or None, optional
8182             If True, skip missing values (as marked by NaN). By default, only
8183             skips missing values for ``float``, ``complex``, and ``object``
8184             dtypes; other dtypes either do not have a sentinel missing value
8185             (``int``) or ``skipna=True`` has not been implemented
8186             (``datetime64`` or ``timedelta64``).
8187         fill_value : Any, default: NaN
8188             Value to be filled in case all of the values along a dimension are
8189             null.  By default this is NaN.  The fill value and result are
8190             automatically converted to a compatible dtype if possible.
8191             Ignored if ``skipna`` is False.
8192         keep_attrs : bool or None, optional
8193             If True, the attributes (``attrs``) will be copied from the
8194             original object to the new one. If False, the new object
8195             will be returned without attributes.
8196 
8197         Returns
8198         -------
8199         reduced : Dataset
8200             New `Dataset` object with `idxmax` applied to its data and the
8201             indicated dimension removed.
8202 
8203         See Also
8204         --------
8205         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8206 
8207         Examples
8208         --------
8209         >>> array1 = xr.DataArray(
8210         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8211         ... )
8212         >>> array2 = xr.DataArray(
8213         ...     [
8214         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8215         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8216         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8217         ...     ],
8218         ...     dims=["y", "x"],
8219         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8220         ... )
8221         >>> ds = xr.Dataset({"int": array1, "float": array2})
8222         >>> ds.max(dim="x")
8223         <xarray.Dataset>
8224         Dimensions:  (y: 3)
8225         Coordinates:
8226           * y        (y) int64 -1 0 1
8227         Data variables:
8228             int      int64 2
8229             float    (y) float64 2.0 2.0 1.0
8230         >>> ds.argmax(dim="x")
8231         <xarray.Dataset>
8232         Dimensions:  (y: 3)
8233         Coordinates:
8234           * y        (y) int64 -1 0 1
8235         Data variables:
8236             int      int64 1
8237             float    (y) int64 0 2 2
8238         >>> ds.idxmax(dim="x")
8239         <xarray.Dataset>
8240         Dimensions:  (y: 3)
8241         Coordinates:
8242           * y        (y) int64 -1 0 1
8243         Data variables:
8244             int      <U1 'b'
8245             float    (y) object 'a' 'c' 'c'
8246         """
8247         return self.map(
8248             methodcaller(
8249                 "idxmax",
8250                 dim=dim,
8251                 skipna=skipna,
8252                 fill_value=fill_value,
8253                 keep_attrs=keep_attrs,
8254             )
8255         )
8256 
8257     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8258         """Indices of the minima of the member variables.
8259 
8260         If there are multiple minima, the indices of the first one found will be
8261         returned.
8262 
8263         Parameters
8264         ----------
8265         dim : Hashable, optional
8266             The dimension over which to find the minimum. By default, finds minimum over
8267             all dimensions - for now returning an int for backward compatibility, but
8268             this is deprecated, in future will be an error, since DataArray.argmin will
8269             return a dict with indices for all dimensions, which does not make sense for
8270             a Dataset.
8271         keep_attrs : bool, optional
8272             If True, the attributes (`attrs`) will be copied from the original
8273             object to the new one.  If False (default), the new object will be
8274             returned without attributes.
8275         skipna : bool, optional
8276             If True, skip missing values (as marked by NaN). By default, only
8277             skips missing values for float dtypes; other dtypes either do not
8278             have a sentinel missing value (int) or skipna=True has not been
8279             implemented (object, datetime64 or timedelta64).
8280 
8281         Returns
8282         -------
8283         result : Dataset
8284 
8285         See Also
8286         --------
8287         DataArray.argmin
8288         """
8289         if dim is None:
8290             warnings.warn(
8291                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8292                 "dim changes to return a dict of indices of each dimension, for "
8293                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8294                 "since we don't return a dict of Datasets.",
8295                 DeprecationWarning,
8296                 stacklevel=2,
8297             )
8298         if (
8299             dim is None
8300             or (not isinstance(dim, Sequence) and dim is not ...)
8301             or isinstance(dim, str)
8302         ):
8303             # Return int index if single dimension is passed, and is not part of a
8304             # sequence
8305             argmin_func = getattr(duck_array_ops, "argmin")
8306             return self.reduce(argmin_func, dim=dim, **kwargs)
8307         else:
8308             raise ValueError(
8309                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8310                 "dicts cannot be contained in a Dataset, so cannot call "
8311                 "Dataset.argmin() with a sequence or ... for dim"
8312             )
8313 
8314     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8315         """Indices of the maxima of the member variables.
8316 
8317         If there are multiple maxima, the indices of the first one found will be
8318         returned.
8319 
8320         Parameters
8321         ----------
8322         dim : str, optional
8323             The dimension over which to find the maximum. By default, finds maximum over
8324             all dimensions - for now returning an int for backward compatibility, but
8325             this is deprecated, in future will be an error, since DataArray.argmax will
8326             return a dict with indices for all dimensions, which does not make sense for
8327             a Dataset.
8328         keep_attrs : bool, optional
8329             If True, the attributes (`attrs`) will be copied from the original
8330             object to the new one.  If False (default), the new object will be
8331             returned without attributes.
8332         skipna : bool, optional
8333             If True, skip missing values (as marked by NaN). By default, only
8334             skips missing values for float dtypes; other dtypes either do not
8335             have a sentinel missing value (int) or skipna=True has not been
8336             implemented (object, datetime64 or timedelta64).
8337 
8338         Returns
8339         -------
8340         result : Dataset
8341 
8342         See Also
8343         --------
8344         DataArray.argmax
8345 
8346         """
8347         if dim is None:
8348             warnings.warn(
8349                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8350                 "dim changes to return a dict of indices of each dimension, for "
8351                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8352                 "since we don't return a dict of Datasets.",
8353                 DeprecationWarning,
8354                 stacklevel=2,
8355             )
8356         if (
8357             dim is None
8358             or (not isinstance(dim, Sequence) and dim is not ...)
8359             or isinstance(dim, str)
8360         ):
8361             # Return int index if single dimension is passed, and is not part of a
8362             # sequence
8363             argmax_func = getattr(duck_array_ops, "argmax")
8364             return self.reduce(argmax_func, dim=dim, **kwargs)
8365         else:
8366             raise ValueError(
8367                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8368                 "dicts cannot be contained in a Dataset, so cannot call "
8369                 "Dataset.argmin() with a sequence or ... for dim"
8370             )
8371 
8372     def query(
8373         self: T_Dataset,
8374         queries: Mapping[Any, Any] | None = None,
8375         parser: QueryParserOptions = "pandas",
8376         engine: QueryEngineOptions = None,
8377         missing_dims: ErrorOptionsWithWarn = "raise",
8378         **queries_kwargs: Any,
8379     ) -> T_Dataset:
8380         """Return a new dataset with each array indexed along the specified
8381         dimension(s), where the indexers are given as strings containing
8382         Python expressions to be evaluated against the data variables in the
8383         dataset.
8384 
8385         Parameters
8386         ----------
8387         queries : dict-like, optional
8388             A dict-like with keys matching dimensions and values given by strings
8389             containing Python expressions to be evaluated against the data variables
8390             in the dataset. The expressions will be evaluated using the pandas
8391             eval() function, and can contain any valid Python expressions but cannot
8392             contain any Python statements.
8393         parser : {"pandas", "python"}, default: "pandas"
8394             The parser to use to construct the syntax tree from the expression.
8395             The default of 'pandas' parses code slightly different than standard
8396             Python. Alternatively, you can parse an expression using the 'python'
8397             parser to retain strict Python semantics.
8398         engine : {"python", "numexpr", None}, default: None
8399             The engine used to evaluate the expression. Supported engines are:
8400 
8401             - None: tries to use numexpr, falls back to python
8402             - "numexpr": evaluates expressions using numexpr
8403             - "python": performs operations as if you had evald in top level python
8404 
8405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8406             What to do if dimensions that should be selected from are not present in the
8407             Dataset:
8408 
8409             - "raise": raise an exception
8410             - "warn": raise a warning, and ignore the missing dimensions
8411             - "ignore": ignore the missing dimensions
8412 
8413         **queries_kwargs : {dim: query, ...}, optional
8414             The keyword arguments form of ``queries``.
8415             One of queries or queries_kwargs must be provided.
8416 
8417         Returns
8418         -------
8419         obj : Dataset
8420             A new Dataset with the same contents as this dataset, except each
8421             array and dimension is indexed by the results of the appropriate
8422             queries.
8423 
8424         See Also
8425         --------
8426         Dataset.isel
8427         pandas.eval
8428 
8429         Examples
8430         --------
8431         >>> a = np.arange(0, 5, 1)
8432         >>> b = np.linspace(0, 1, 5)
8433         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8434         >>> ds
8435         <xarray.Dataset>
8436         Dimensions:  (x: 5)
8437         Dimensions without coordinates: x
8438         Data variables:
8439             a        (x) int64 0 1 2 3 4
8440             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8441         >>> ds.query(x="a > 2")
8442         <xarray.Dataset>
8443         Dimensions:  (x: 2)
8444         Dimensions without coordinates: x
8445         Data variables:
8446             a        (x) int64 3 4
8447             b        (x) float64 0.75 1.0
8448         """
8449 
8450         # allow queries to be given either as a dict or as kwargs
8451         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8452 
8453         # check queries
8454         for dim, expr in queries.items():
8455             if not isinstance(expr, str):
8456                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8457                 raise ValueError(msg)
8458 
8459         # evaluate the queries to create the indexers
8460         indexers = {
8461             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8462             for dim, expr in queries.items()
8463         }
8464 
8465         # apply the selection
8466         return self.isel(indexers, missing_dims=missing_dims)
8467 
8468     def curvefit(
8469         self: T_Dataset,
8470         coords: str | DataArray | Iterable[str | DataArray],
8471         func: Callable[..., Any],
8472         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8473         skipna: bool = True,
8474         p0: dict[str, Any] | None = None,
8475         bounds: dict[str, Any] | None = None,
8476         param_names: Sequence[str] | None = None,
8477         kwargs: dict[str, Any] | None = None,
8478     ) -> T_Dataset:
8479         """
8480         Curve fitting optimization for arbitrary functions.
8481 
8482         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8483 
8484         Parameters
8485         ----------
8486         coords : hashable, DataArray, or sequence of hashable or DataArray
8487             Independent coordinate(s) over which to perform the curve fitting. Must share
8488             at least one dimension with the calling object. When fitting multi-dimensional
8489             functions, supply `coords` as a sequence in the same order as arguments in
8490             `func`. To fit along existing dimensions of the calling object, `coords` can
8491             also be specified as a str or sequence of strs.
8492         func : callable
8493             User specified function in the form `f(x, *params)` which returns a numpy
8494             array of length `len(x)`. `params` are the fittable parameters which are optimized
8495             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8496             coordinates, e.g. `f((x0, x1), *params)`.
8497         reduce_dims : hashable or sequence of hashable
8498             Additional dimension(s) over which to aggregate while fitting. For example,
8499             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8500             aggregate all lat and lon points and fit the specified function along the
8501             time dimension.
8502         skipna : bool, default: True
8503             Whether to skip missing values when fitting. Default is True.
8504         p0 : dict-like, optional
8505             Optional dictionary of parameter names to initial guesses passed to the
8506             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8507             be assigned initial values following the default scipy behavior.
8508         bounds : dict-like, optional
8509             Optional dictionary of parameter names to bounding values passed to the
8510             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8511             will be unbounded following the default scipy behavior.
8512         param_names : sequence of hashable, optional
8513             Sequence of names for the fittable parameters of `func`. If not supplied,
8514             this will be automatically determined by arguments of `func`. `param_names`
8515             should be manually supplied when fitting a function that takes a variable
8516             number of parameters.
8517         **kwargs : optional
8518             Additional keyword arguments to passed to scipy curve_fit.
8519 
8520         Returns
8521         -------
8522         curvefit_results : Dataset
8523             A single dataset which contains:
8524 
8525             [var]_curvefit_coefficients
8526                 The coefficients of the best fit.
8527             [var]_curvefit_covariance
8528                 The covariance matrix of the coefficient estimates.
8529 
8530         See Also
8531         --------
8532         Dataset.polyfit
8533         scipy.optimize.curve_fit
8534         """
8535         from scipy.optimize import curve_fit
8536 
8537         from .alignment import broadcast
8538         from .computation import apply_ufunc
8539         from .dataarray import _THIS_ARRAY, DataArray
8540 
8541         if p0 is None:
8542             p0 = {}
8543         if bounds is None:
8544             bounds = {}
8545         if kwargs is None:
8546             kwargs = {}
8547 
8548         if not reduce_dims:
8549             reduce_dims_ = []
8550         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8551             reduce_dims_ = [reduce_dims]
8552         else:
8553             reduce_dims_ = list(reduce_dims)
8554 
8555         if (
8556             isinstance(coords, str)
8557             or isinstance(coords, DataArray)
8558             or not isinstance(coords, Iterable)
8559         ):
8560             coords = [coords]
8561         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8562 
8563         # Determine whether any coords are dims on self
8564         for coord in coords_:
8565             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8566         reduce_dims_ = list(set(reduce_dims_))
8567         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8568         if not reduce_dims_:
8569             raise ValueError(
8570                 "No arguments to `coords` were identified as a dimension on the calling "
8571                 "object, and no dims were supplied to `reduce_dims`. This would result "
8572                 "in fitting on scalar data."
8573             )
8574 
8575         # Broadcast all coords with each other
8576         coords_ = broadcast(*coords_)
8577         coords_ = [
8578             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8579         ]
8580 
8581         params, func_args = _get_func_args(func, param_names)
8582         param_defaults, bounds_defaults = _initialize_curvefit_params(
8583             params, p0, bounds, func_args
8584         )
8585         n_params = len(params)
8586         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8587         kwargs.setdefault(
8588             "bounds",
8589             [
8590                 [bounds_defaults[p][0] for p in params],
8591                 [bounds_defaults[p][1] for p in params],
8592             ],
8593         )
8594 
8595         def _wrapper(Y, *coords_, **kwargs):
8596             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8597             x = np.vstack([c.ravel() for c in coords_])
8598             y = Y.ravel()
8599             if skipna:
8600                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8601                 x = x[:, mask]
8602                 y = y[mask]
8603                 if not len(y):
8604                     popt = np.full([n_params], np.nan)
8605                     pcov = np.full([n_params, n_params], np.nan)
8606                     return popt, pcov
8607             x = np.squeeze(x)
8608             popt, pcov = curve_fit(func, x, y, **kwargs)
8609             return popt, pcov
8610 
8611         result = type(self)()
8612         for name, da in self.data_vars.items():
8613             if name is _THIS_ARRAY:
8614                 name = ""
8615             else:
8616                 name = f"{str(name)}_"
8617 
8618             popt, pcov = apply_ufunc(
8619                 _wrapper,
8620                 da,
8621                 *coords_,
8622                 vectorize=True,
8623                 dask="parallelized",
8624                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8625                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8626                 dask_gufunc_kwargs={
8627                     "output_sizes": {
8628                         "param": n_params,
8629                         "cov_i": n_params,
8630                         "cov_j": n_params,
8631                     },
8632                 },
8633                 output_dtypes=(np.float64, np.float64),
8634                 exclude_dims=set(reduce_dims_),
8635                 kwargs=kwargs,
8636             )
8637             result[name + "curvefit_coefficients"] = popt
8638             result[name + "curvefit_covariance"] = pcov
8639 
8640         result = result.assign_coords(
8641             {"param": params, "cov_i": params, "cov_j": params}
8642         )
8643         result.attrs = self.attrs.copy()
8644 
8645         return result
8646 
8647     def drop_duplicates(
8648         self: T_Dataset,
8649         dim: Hashable | Iterable[Hashable],
8650         keep: Literal["first", "last", False] = "first",
8651     ) -> T_Dataset:
8652         """Returns a new Dataset with duplicate dimension values removed.
8653 
8654         Parameters
8655         ----------
8656         dim : dimension label or labels
8657             Pass `...` to drop duplicates along all dimensions.
8658         keep : {"first", "last", False}, default: "first"
8659             Determines which duplicates (if any) to keep.
8660             - ``"first"`` : Drop duplicates except for the first occurrence.
8661             - ``"last"`` : Drop duplicates except for the last occurrence.
8662             - False : Drop all duplicates.
8663 
8664         Returns
8665         -------
8666         Dataset
8667 
8668         See Also
8669         --------
8670         DataArray.drop_duplicates
8671         """
8672         if isinstance(dim, str):
8673             dims: Iterable = (dim,)
8674         elif dim is ...:
8675             dims = self.dims
8676         elif not isinstance(dim, Iterable):
8677             dims = [dim]
8678         else:
8679             dims = dim
8680 
8681         missing_dims = set(dims) - set(self.dims)
8682         if missing_dims:
8683             raise ValueError(f"'{missing_dims}' not found in dimensions")
8684 
8685         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8686         return self.isel(indexes)
8687 
8688     def convert_calendar(
8689         self: T_Dataset,
8690         calendar: CFCalendar,
8691         dim: Hashable = "time",
8692         align_on: Literal["date", "year", None] = None,
8693         missing: Any | None = None,
8694         use_cftime: bool | None = None,
8695     ) -> T_Dataset:
8696         """Convert the Dataset to another calendar.
8697 
8698         Only converts the individual timestamps, does not modify any data except
8699         in dropping invalid/surplus dates or inserting missing dates.
8700 
8701         If the source and target calendars are either no_leap, all_leap or a
8702         standard type, only the type of the time array is modified.
8703         When converting to a leap year from a non-leap year, the 29th of February
8704         is removed from the array. In the other direction the 29th of February
8705         will be missing in the output, unless `missing` is specified,
8706         in which case that value is inserted.
8707 
8708         For conversions involving `360_day` calendars, see Notes.
8709 
8710         This method is safe to use with sub-daily data as it doesn't touch the
8711         time part of the timestamps.
8712 
8713         Parameters
8714         ---------
8715         calendar : str
8716             The target calendar name.
8717         dim : Hashable, default: "time"
8718             Name of the time coordinate.
8719         align_on : {None, 'date', 'year'}, optional
8720             Must be specified when either source or target is a `360_day` calendar,
8721             ignored otherwise. See Notes.
8722         missing : Any or None, optional
8723             By default, i.e. if the value is None, this method will simply attempt
8724             to convert the dates in the source calendar to the same dates in the
8725             target calendar, and drop any of those that are not possible to
8726             represent.  If a value is provided, a new time coordinate will be
8727             created in the target calendar with the same frequency as the original
8728             time coordinate; for any dates that are not present in the source, the
8729             data will be filled with this value.  Note that using this mode requires
8730             that the source data have an inferable frequency; for more information
8731             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8732             target calendar combinations, this could result in many missing values, see notes.
8733         use_cftime : bool or None, optional
8734             Whether to use cftime objects in the output, only used if `calendar`
8735             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8736             If True, the new time axis uses cftime objects.
8737             If None (default), it uses :py:class:`numpy.datetime64` values if the
8738             date range permits it, and :py:class:`cftime.datetime` objects if not.
8739             If False, it uses :py:class:`numpy.datetime64`  or fails.
8740 
8741         Returns
8742         -------
8743         Dataset
8744             Copy of the dataarray with the time coordinate converted to the
8745             target calendar. If 'missing' was None (default), invalid dates in
8746             the new calendar are dropped, but missing dates are not inserted.
8747             If `missing` was given, the new data is reindexed to have a time axis
8748             with the same frequency as the source, but in the new calendar; any
8749             missing datapoints are filled with `missing`.
8750 
8751         Notes
8752         -----
8753         Passing a value to `missing` is only usable if the source's time coordinate as an
8754         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8755         if the target coordinate, generated from this frequency, has dates equivalent to the
8756         source. It is usually **not** appropriate to use this mode with:
8757 
8758         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8759         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8760             or 'mH' where 24 % m != 0).
8761 
8762         If one of the source or target calendars is `"360_day"`, `align_on` must
8763         be specified and two options are offered.
8764 
8765         - "year"
8766             The dates are translated according to their relative position in the year,
8767             ignoring their original month and day information, meaning that the
8768             missing/surplus days are added/removed at regular intervals.
8769 
8770             From a `360_day` to a standard calendar, the output will be missing the
8771             following dates (day of year in parentheses):
8772 
8773             To a leap year:
8774                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8775                 September 31st (275) and November 30th (335).
8776             To a non-leap year:
8777                 February 6th (36), April 19th (109), July 2nd (183),
8778                 September 12th (255), November 25th (329).
8779 
8780             From a standard calendar to a `"360_day"`, the following dates in the
8781             source array will be dropped:
8782 
8783             From a leap year:
8784                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8785                 September 31st (275), December 1st (336)
8786             From a non-leap year:
8787                 February 6th (37), April 20th (110), July 2nd (183),
8788                 September 13th (256), November 25th (329)
8789 
8790             This option is best used on daily and subdaily data.
8791 
8792         - "date"
8793             The month/day information is conserved and invalid dates are dropped
8794             from the output. This means that when converting from a `"360_day"` to a
8795             standard calendar, all 31st (Jan, March, May, July, August, October and
8796             December) will be missing as there is no equivalent dates in the
8797             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8798             will be dropped as there are no equivalent dates in a standard calendar.
8799 
8800             This option is best used with data on a frequency coarser than daily.
8801         """
8802         return convert_calendar(
8803             self,
8804             calendar,
8805             dim=dim,
8806             align_on=align_on,
8807             missing=missing,
8808             use_cftime=use_cftime,
8809         )
8810 
8811     def interp_calendar(
8812         self: T_Dataset,
8813         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8814         dim: Hashable = "time",
8815     ) -> T_Dataset:
8816         """Interpolates the Dataset to another calendar based on decimal year measure.
8817 
8818         Each timestamp in `source` and `target` are first converted to their decimal
8819         year equivalent then `source` is interpolated on the target coordinate.
8820         The decimal year of a timestamp is its year plus its sub-year component
8821         converted to the fraction of its year. For example "2000-03-01 12:00" is
8822         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8823 
8824         This method should only be used when the time (HH:MM:SS) information of
8825         time coordinate is not important.
8826 
8827         Parameters
8828         ----------
8829         target: DataArray or DatetimeIndex or CFTimeIndex
8830             The target time coordinate of a valid dtype
8831             (np.datetime64 or cftime objects)
8832         dim : Hashable, default: "time"
8833             The time coordinate name.
8834 
8835         Return
8836         ------
8837         DataArray
8838             The source interpolated on the decimal years of target,
8839         """
8840         return interp_calendar(self, target, dim=dim)
8841 
8842     def groupby(
8843         self,
8844         group: Hashable | DataArray | IndexVariable,
8845         squeeze: bool = True,
8846         restore_coord_dims: bool = False,
8847     ) -> DatasetGroupBy:
8848         """Returns a DatasetGroupBy object for performing grouped operations.
8849 
8850         Parameters
8851         ----------
8852         group : Hashable, DataArray or IndexVariable
8853             Array whose unique values should be used to group this array. If a
8854             string, must be the name of a variable contained in this dataset.
8855         squeeze : bool, default: True
8856             If "group" is a dimension of any arrays in this dataset, `squeeze`
8857             controls whether the subarrays have a dimension of length 1 along
8858             that dimension or if the dimension is squeezed out.
8859         restore_coord_dims : bool, default: False
8860             If True, also restore the dimension order of multi-dimensional
8861             coordinates.
8862 
8863         Returns
8864         -------
8865         grouped : DatasetGroupBy
8866             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8867             iterated over in the form of `(unique_value, grouped_array)` pairs.
8868 
8869         See Also
8870         --------
8871         Dataset.groupby_bins
8872         DataArray.groupby
8873         core.groupby.DatasetGroupBy
8874         pandas.DataFrame.groupby
8875         """
8876         from .groupby import DatasetGroupBy
8877 
8878         # While we don't generally check the type of every arg, passing
8879         # multiple dimensions as multiple arguments is common enough, and the
8880         # consequences hidden enough (strings evaluate as true) to warrant
8881         # checking here.
8882         # A future version could make squeeze kwarg only, but would face
8883         # backward-compat issues.
8884         if not isinstance(squeeze, bool):
8885             raise TypeError(
8886                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8887             )
8888 
8889         return DatasetGroupBy(
8890             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8891         )
8892 
8893     def groupby_bins(
8894         self,
8895         group: Hashable | DataArray | IndexVariable,
8896         bins: ArrayLike,
8897         right: bool = True,
8898         labels: ArrayLike | None = None,
8899         precision: int = 3,
8900         include_lowest: bool = False,
8901         squeeze: bool = True,
8902         restore_coord_dims: bool = False,
8903     ) -> DatasetGroupBy:
8904         """Returns a DatasetGroupBy object for performing grouped operations.
8905 
8906         Rather than using all unique values of `group`, the values are discretized
8907         first by applying `pandas.cut` [1]_ to `group`.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose binned values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         bins : int or array-like
8915             If bins is an int, it defines the number of equal-width bins in the
8916             range of x. However, in this case, the range of x is extended by .1%
8917             on each side to include the min or max values of x. If bins is a
8918             sequence it defines the bin edges allowing for non-uniform bin
8919             width. No extension of the range of x is done in this case.
8920         right : bool, default: True
8921             Indicates whether the bins include the rightmost edge or not. If
8922             right == True (the default), then the bins [1,2,3,4] indicate
8923             (1,2], (2,3], (3,4].
8924         labels : array-like or bool, default: None
8925             Used as labels for the resulting bins. Must be of the same length as
8926             the resulting bins. If False, string bin labels are assigned by
8927             `pandas.cut`.
8928         precision : int, default: 3
8929             The precision at which to store and display the bins labels.
8930         include_lowest : bool, default: False
8931             Whether the first interval should be left-inclusive or not.
8932         squeeze : bool, default: True
8933             If "group" is a dimension of any arrays in this dataset, `squeeze`
8934             controls whether the subarrays have a dimension of length 1 along
8935             that dimension or if the dimension is squeezed out.
8936         restore_coord_dims : bool, default: False
8937             If True, also restore the dimension order of multi-dimensional
8938             coordinates.
8939 
8940         Returns
8941         -------
8942         grouped : DatasetGroupBy
8943             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8944             iterated over in the form of `(unique_value, grouped_array)` pairs.
8945             The name of the group has the added suffix `_bins` in order to
8946             distinguish it from the original variable.
8947 
8948         See Also
8949         --------
8950         Dataset.groupby
8951         DataArray.groupby_bins
8952         core.groupby.DatasetGroupBy
8953         pandas.DataFrame.groupby
8954 
8955         References
8956         ----------
8957         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8958         """
8959         from .groupby import DatasetGroupBy
8960 
8961         return DatasetGroupBy(
8962             self,
8963             group,
8964             squeeze=squeeze,
8965             bins=bins,
8966             restore_coord_dims=restore_coord_dims,
8967             cut_kwargs={
8968                 "right": right,
8969                 "labels": labels,
8970                 "precision": precision,
8971                 "include_lowest": include_lowest,
8972             },
8973         )
8974 
8975     def weighted(self, weights: DataArray) -> DatasetWeighted:
8976         """
8977         Weighted Dataset operations.
8978 
8979         Parameters
8980         ----------
8981         weights : DataArray
8982             An array of weights associated with the values in this Dataset.
8983             Each value in the data contributes to the reduction operation
8984             according to its associated weight.
8985 
8986         Notes
8987         -----
8988         ``weights`` must be a DataArray and cannot contain missing values.
8989         Missing values can be replaced by ``weights.fillna(0)``.
8990 
8991         Returns
8992         -------
8993         core.weighted.DatasetWeighted
8994 
8995         See Also
8996         --------
8997         DataArray.weighted
8998         """
8999         from .weighted import DatasetWeighted
9000 
9001         return DatasetWeighted(self, weights)
9002 
9003     def rolling(
9004         self,
9005         dim: Mapping[Any, int] | None = None,
9006         min_periods: int | None = None,
9007         center: bool | Mapping[Any, bool] = False,
9008         **window_kwargs: int,
9009     ) -> DatasetRolling:
9010         """
9011         Rolling window object for Datasets.
9012 
9013         Parameters
9014         ----------
9015         dim : dict, optional
9016             Mapping from the dimension name to create the rolling iterator
9017             along (e.g. `time`) to its moving window size.
9018         min_periods : int or None, default: None
9019             Minimum number of observations in window required to have a value
9020             (otherwise result is NA). The default, None, is equivalent to
9021             setting min_periods equal to the size of the window.
9022         center : bool or Mapping to int, default: False
9023             Set the labels at the center of the window.
9024         **window_kwargs : optional
9025             The keyword arguments form of ``dim``.
9026             One of dim or window_kwargs must be provided.
9027 
9028         Returns
9029         -------
9030         core.rolling.DatasetRolling
9031 
9032         See Also
9033         --------
9034         core.rolling.DatasetRolling
9035         DataArray.rolling
9036         """
9037         from .rolling import DatasetRolling
9038 
9039         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9040         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9041 
9042     def coarsen(
9043         self,
9044         dim: Mapping[Any, int] | None = None,
9045         boundary: CoarsenBoundaryOptions = "exact",
9046         side: SideOptions | Mapping[Any, SideOptions] = "left",
9047         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9048         **window_kwargs: int,
9049     ) -> DatasetCoarsen:
9050         """
9051         Coarsen object for Datasets.
9052 
9053         Parameters
9054         ----------
9055         dim : mapping of hashable to int, optional
9056             Mapping from the dimension name to the window size.
9057         boundary : {"exact", "trim", "pad"}, default: "exact"
9058             If 'exact', a ValueError will be raised if dimension size is not a
9059             multiple of the window size. If 'trim', the excess entries are
9060             dropped. If 'pad', NA will be padded.
9061         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9062         coord_func : str or mapping of hashable to str, default: "mean"
9063             function (name) that is applied to the coordinates,
9064             or a mapping from coordinate name to function (name).
9065 
9066         Returns
9067         -------
9068         core.rolling.DatasetCoarsen
9069 
9070         See Also
9071         --------
9072         core.rolling.DatasetCoarsen
9073         DataArray.coarsen
9074         """
9075         from .rolling import DatasetCoarsen
9076 
9077         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9078         return DatasetCoarsen(
9079             self,
9080             dim,
9081             boundary=boundary,
9082             side=side,
9083             coord_func=coord_func,
9084         )
9085 
9086     def resample(
9087         self,
9088         indexer: Mapping[Any, str] | None = None,
9089         skipna: bool | None = None,
9090         closed: SideOptions | None = None,
9091         label: SideOptions | None = None,
9092         base: int = 0,
9093         keep_attrs: bool | None = None,
9094         loffset: datetime.timedelta | str | None = None,
9095         restore_coord_dims: bool | None = None,
9096         **indexer_kwargs: str,
9097     ) -> DatasetResample:
9098         """Returns a Resample object for performing resampling operations.
9099 
9100         Handles both downsampling and upsampling. The resampled
9101         dimension must be a datetime-like coordinate. If any intervals
9102         contain no values from the original object, they will be given
9103         the value ``NaN``.
9104 
9105         Parameters
9106         ----------
9107         indexer : Mapping of Hashable to str, optional
9108             Mapping from the dimension name to resample frequency [1]_. The
9109             dimension must be datetime-like.
9110         skipna : bool, optional
9111             Whether to skip missing values when aggregating in downsampling.
9112         closed : {"left", "right"}, optional
9113             Side of each interval to treat as closed.
9114         label : {"left", "right"}, optional
9115             Side of each interval to use for labeling.
9116         base : int, default = 0
9117             For frequencies that evenly subdivide 1 day, the "origin" of the
9118             aggregated intervals. For example, for "24H" frequency, base could
9119             range from 0 through 23.
9120         loffset : timedelta or str, optional
9121             Offset used to adjust the resampled time labels. Some pandas date
9122             offset strings are supported.
9123         restore_coord_dims : bool, optional
9124             If True, also restore the dimension order of multi-dimensional
9125             coordinates.
9126         **indexer_kwargs : str
9127             The keyword arguments form of ``indexer``.
9128             One of indexer or indexer_kwargs must be provided.
9129 
9130         Returns
9131         -------
9132         resampled : core.resample.DataArrayResample
9133             This object resampled.
9134 
9135         See Also
9136         --------
9137         DataArray.resample
9138         pandas.Series.resample
9139         pandas.DataFrame.resample
9140 
9141         References
9142         ----------
9143         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9144         """
9145         from .resample import DatasetResample
9146 
9147         return self._resample(
9148             resample_cls=DatasetResample,
9149             indexer=indexer,
9150             skipna=skipna,
9151             closed=closed,
9152             label=label,
9153             base=base,
9154             keep_attrs=keep_attrs,
9155             loffset=loffset,
9156             restore_coord_dims=restore_coord_dims,
9157             **indexer_kwargs,
9158         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the indexs type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the indexs type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def _rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Also used internally by DataArray so that the warning (if any)
3569         is raised at the right stack level.
3570         """
3571         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3572         for k in name_dict.keys():
3573             if k not in self and k not in self.dims:
3574                 raise ValueError(
3575                     f"cannot rename {k!r} because it is not a "
3576                     "variable or dimension in this dataset"
3577                 )
3578 
3579             create_dim_coord = False
3580             new_k = name_dict[k]
3581 
3582             if k in self.dims and new_k in self._coord_names:
3583                 coord_dims = self._variables[name_dict[k]].dims
3584                 if coord_dims == (k,):
3585                     create_dim_coord = True
3586             elif k in self._coord_names and new_k in self.dims:
3587                 coord_dims = self._variables[k].dims
3588                 if coord_dims == (new_k,):
3589                     create_dim_coord = True
3590 
3591             if create_dim_coord:
3592                 warnings.warn(
3593                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3594                     "anymore. Try using swap_dims instead or use set_index "
3595                     "after rename to create an indexed coordinate.",
3596                     UserWarning,
3597                     stacklevel=3,
3598                 )
3599 
3600         variables, coord_names, dims, indexes = self._rename_all(
3601             name_dict=name_dict, dims_dict=name_dict
3602         )
3603         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3604 
3605     def rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Returns a new object with renamed variables, coordinates and dimensions.
3611 
3612         Parameters
3613         ----------
3614         name_dict : dict-like, optional
3615             Dictionary whose keys are current variable, coordinate or dimension names and
3616             whose values are the desired names.
3617         **names : optional
3618             Keyword form of ``name_dict``.
3619             One of name_dict or names must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed variables, coordinates and dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename_vars
3630         Dataset.rename_dims
3631         DataArray.rename
3632         """
3633         return self._rename(name_dict=name_dict, **names)
3634 
3635     def rename_dims(
3636         self: T_Dataset,
3637         dims_dict: Mapping[Any, Hashable] | None = None,
3638         **dims: Hashable,
3639     ) -> T_Dataset:
3640         """Returns a new object with renamed dimensions only.
3641 
3642         Parameters
3643         ----------
3644         dims_dict : dict-like, optional
3645             Dictionary whose keys are current dimension names and
3646             whose values are the desired names. The desired names must
3647             not be the name of an existing dimension or Variable in the Dataset.
3648         **dims : optional
3649             Keyword form of ``dims_dict``.
3650             One of dims_dict or dims must be provided.
3651 
3652         Returns
3653         -------
3654         renamed : Dataset
3655             Dataset with renamed dimensions.
3656 
3657         See Also
3658         --------
3659         Dataset.swap_dims
3660         Dataset.rename
3661         Dataset.rename_vars
3662         DataArray.rename
3663         """
3664         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3665         for k, v in dims_dict.items():
3666             if k not in self.dims:
3667                 raise ValueError(
3668                     f"cannot rename {k!r} because it is not a "
3669                     "dimension in this dataset"
3670                 )
3671             if v in self.dims or v in self:
3672                 raise ValueError(
3673                     f"Cannot rename {k} to {v} because {v} already exists. "
3674                     "Try using swap_dims instead."
3675                 )
3676 
3677         variables, coord_names, sizes, indexes = self._rename_all(
3678             name_dict={}, dims_dict=dims_dict
3679         )
3680         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3681 
3682     def rename_vars(
3683         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3684     ) -> T_Dataset:
3685         """Returns a new object with renamed variables including coordinates
3686 
3687         Parameters
3688         ----------
3689         name_dict : dict-like, optional
3690             Dictionary whose keys are current variable or coordinate names and
3691             whose values are the desired names.
3692         **names : optional
3693             Keyword form of ``name_dict``.
3694             One of name_dict or names must be provided.
3695 
3696         Returns
3697         -------
3698         renamed : Dataset
3699             Dataset with renamed variables including coordinates
3700 
3701         See Also
3702         --------
3703         Dataset.swap_dims
3704         Dataset.rename
3705         Dataset.rename_dims
3706         DataArray.rename
3707         """
3708         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3709         for k in name_dict:
3710             if k not in self:
3711                 raise ValueError(
3712                     f"cannot rename {k!r} because it is not a "
3713                     "variable or coordinate in this dataset"
3714                 )
3715         variables, coord_names, dims, indexes = self._rename_all(
3716             name_dict=name_dict, dims_dict={}
3717         )
3718         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3719 
3720     def swap_dims(
3721         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3722     ) -> T_Dataset:
3723         """Returns a new object with swapped dimensions.
3724 
3725         Parameters
3726         ----------
3727         dims_dict : dict-like
3728             Dictionary whose keys are current dimension names and whose values
3729             are new names.
3730         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3731             The keyword arguments form of ``dims_dict``.
3732             One of dims_dict or dims_kwargs must be provided.
3733 
3734         Returns
3735         -------
3736         swapped : Dataset
3737             Dataset with swapped dimensions.
3738 
3739         Examples
3740         --------
3741         >>> ds = xr.Dataset(
3742         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3743         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3744         ... )
3745         >>> ds
3746         <xarray.Dataset>
3747         Dimensions:  (x: 2)
3748         Coordinates:
3749           * x        (x) <U1 'a' 'b'
3750             y        (x) int64 0 1
3751         Data variables:
3752             a        (x) int64 5 7
3753             b        (x) float64 0.1 2.4
3754 
3755         >>> ds.swap_dims({"x": "y"})
3756         <xarray.Dataset>
3757         Dimensions:  (y: 2)
3758         Coordinates:
3759             x        (y) <U1 'a' 'b'
3760           * y        (y) int64 0 1
3761         Data variables:
3762             a        (y) int64 5 7
3763             b        (y) float64 0.1 2.4
3764 
3765         >>> ds.swap_dims({"x": "z"})
3766         <xarray.Dataset>
3767         Dimensions:  (z: 2)
3768         Coordinates:
3769             x        (z) <U1 'a' 'b'
3770             y        (z) int64 0 1
3771         Dimensions without coordinates: z
3772         Data variables:
3773             a        (z) int64 5 7
3774             b        (z) float64 0.1 2.4
3775 
3776         See Also
3777         --------
3778         Dataset.rename
3779         DataArray.swap_dims
3780         """
3781         # TODO: deprecate this method in favor of a (less confusing)
3782         # rename_dims() method that only renames dimensions.
3783 
3784         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3785         for k, v in dims_dict.items():
3786             if k not in self.dims:
3787                 raise ValueError(
3788                     f"cannot swap from dimension {k!r} because it is "
3789                     "not an existing dimension"
3790                 )
3791             if v in self.variables and self.variables[v].dims != (k,):
3792                 raise ValueError(
3793                     f"replacement dimension {v!r} is not a 1D "
3794                     f"variable along the old dimension {k!r}"
3795                 )
3796 
3797         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3798 
3799         coord_names = self._coord_names.copy()
3800         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3801 
3802         variables: dict[Hashable, Variable] = {}
3803         indexes: dict[Hashable, Index] = {}
3804         for k, v in self.variables.items():
3805             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3806             var: Variable
3807             if k in result_dims:
3808                 var = v.to_index_variable()
3809                 var.dims = dims
3810                 if k in self._indexes:
3811                     indexes[k] = self._indexes[k]
3812                     variables[k] = var
3813                 else:
3814                     index, index_vars = create_default_index_implicit(var)
3815                     indexes.update({name: index for name in index_vars})
3816                     variables.update(index_vars)
3817                     coord_names.update(index_vars)
3818             else:
3819                 var = v.to_base_variable()
3820                 var.dims = dims
3821                 variables[k] = var
3822 
3823         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3824 
3825     # change type of self and return to T_Dataset once
3826     # https://github.com/python/mypy/issues/12846 is resolved
3827     def expand_dims(
3828         self,
3829         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3830         axis: None | int | Sequence[int] = None,
3831         **dim_kwargs: Any,
3832     ) -> Dataset:
3833         """Return a new object with an additional axis (or axes) inserted at
3834         the corresponding position in the array shape.  The new object is a
3835         view into the underlying array, not a copy.
3836 
3837         If dim is already a scalar coordinate, it will be promoted to a 1D
3838         coordinate consisting of a single value.
3839 
3840         Parameters
3841         ----------
3842         dim : hashable, sequence of hashable, mapping, or None
3843             Dimensions to include on the new variable. If provided as hashable
3844             or sequence of hashable, then dimensions are inserted with length
3845             1. If provided as a mapping, then the keys are the new dimensions
3846             and the values are either integers (giving the length of the new
3847             dimensions) or array-like (giving the coordinates of the new
3848             dimensions).
3849         axis : int, sequence of int, or None, default: None
3850             Axis position(s) where new axis is to be inserted (position(s) on
3851             the result array). If a sequence of integers is passed,
3852             multiple axes are inserted. In this case, dim arguments should be
3853             same length list. If axis=None is passed, all the axes will be
3854             inserted to the start of the result array.
3855         **dim_kwargs : int or sequence or ndarray
3856             The keywords are arbitrary dimensions being inserted and the values
3857             are either the lengths of the new dims (if int is given), or their
3858             coordinates. Note, this is an alternative to passing a dict to the
3859             dim kwarg and will only be used if dim is None.
3860 
3861         Returns
3862         -------
3863         expanded : Dataset
3864             This object, but with additional dimension(s).
3865 
3866         See Also
3867         --------
3868         DataArray.expand_dims
3869         """
3870         if dim is None:
3871             pass
3872         elif isinstance(dim, Mapping):
3873             # We're later going to modify dim in place; don't tamper with
3874             # the input
3875             dim = dict(dim)
3876         elif isinstance(dim, int):
3877             raise TypeError(
3878                 "dim should be hashable or sequence of hashables or mapping"
3879             )
3880         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3881             dim = {dim: 1}
3882         elif isinstance(dim, Sequence):
3883             if len(dim) != len(set(dim)):
3884                 raise ValueError("dims should not contain duplicate values.")
3885             dim = {d: 1 for d in dim}
3886 
3887         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3888         assert isinstance(dim, MutableMapping)
3889 
3890         if axis is None:
3891             axis = list(range(len(dim)))
3892         elif not isinstance(axis, Sequence):
3893             axis = [axis]
3894 
3895         if len(dim) != len(axis):
3896             raise ValueError("lengths of dim and axis should be identical.")
3897         for d in dim:
3898             if d in self.dims:
3899                 raise ValueError(f"Dimension {d} already exists.")
3900             if d in self._variables and not utils.is_scalar(self._variables[d]):
3901                 raise ValueError(
3902                     "{dim} already exists as coordinate or"
3903                     " variable name.".format(dim=d)
3904                 )
3905 
3906         variables: dict[Hashable, Variable] = {}
3907         indexes: dict[Hashable, Index] = dict(self._indexes)
3908         coord_names = self._coord_names.copy()
3909         # If dim is a dict, then ensure that the values are either integers
3910         # or iterables.
3911         for k, v in dim.items():
3912             if hasattr(v, "__iter__"):
3913                 # If the value for the new dimension is an iterable, then
3914                 # save the coordinates to the variables dict, and set the
3915                 # value within the dim dict to the length of the iterable
3916                 # for later use.
3917                 index = PandasIndex(v, k)
3918                 indexes[k] = index
3919                 variables.update(index.create_variables())
3920                 coord_names.add(k)
3921                 dim[k] = variables[k].size
3922             elif isinstance(v, int):
3923                 pass  # Do nothing if the dimensions value is just an int
3924             else:
3925                 raise TypeError(
3926                     "The value of new dimension {k} must be "
3927                     "an iterable or an int".format(k=k)
3928                 )
3929 
3930         for k, v in self._variables.items():
3931             if k not in dim:
3932                 if k in coord_names:  # Do not change coordinates
3933                     variables[k] = v
3934                 else:
3935                     result_ndim = len(v.dims) + len(axis)
3936                     for a in axis:
3937                         if a < -result_ndim or result_ndim - 1 < a:
3938                             raise IndexError(
3939                                 f"Axis {a} of variable {k} is out of bounds of the "
3940                                 f"expanded dimension size {result_ndim}"
3941                             )
3942 
3943                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3944                     if len(axis_pos) != len(set(axis_pos)):
3945                         raise ValueError("axis should not contain duplicate values")
3946                     # We need to sort them to make sure `axis` equals to the
3947                     # axis positions of the result array.
3948                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3949 
3950                     all_dims = list(zip(v.dims, v.shape))
3951                     for d, c in zip_axis_dim:
3952                         all_dims.insert(d, c)
3953                     variables[k] = v.set_dims(dict(all_dims))
3954             else:
3955                 if k not in variables:
3956                     # If dims includes a label of a non-dimension coordinate,
3957                     # it will be promoted to a 1D coordinate with a single value.
3958                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3959                     indexes[k] = index
3960                     variables.update(index_vars)
3961 
3962         return self._replace_with_new_dims(
3963             variables, coord_names=coord_names, indexes=indexes
3964         )
3965 
3966     # change type of self and return to T_Dataset once
3967     # https://github.com/python/mypy/issues/12846 is resolved
3968     def set_index(
3969         self,
3970         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3971         append: bool = False,
3972         **indexes_kwargs: Hashable | Sequence[Hashable],
3973     ) -> Dataset:
3974         """Set Dataset (multi-)indexes using one or more existing coordinates
3975         or variables.
3976 
3977         This legacy method is limited to pandas (multi-)indexes and
3978         1-dimensional "dimension" coordinates. See
3979         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
3980         Xarray-compatible index from one or more arbitrary coordinates.
3981 
3982         Parameters
3983         ----------
3984         indexes : {dim: index, ...}
3985             Mapping from names matching dimensions and values given
3986             by (lists of) the names of existing coordinates or variables to set
3987             as new (multi-)index.
3988         append : bool, default: False
3989             If True, append the supplied index(es) to the existing index(es).
3990             Otherwise replace the existing index(es) (default).
3991         **indexes_kwargs : optional
3992             The keyword arguments form of ``indexes``.
3993             One of indexes or indexes_kwargs must be provided.
3994 
3995         Returns
3996         -------
3997         obj : Dataset
3998             Another dataset, with this dataset's data but replaced coordinates.
3999 
4000         Examples
4001         --------
4002         >>> arr = xr.DataArray(
4003         ...     data=np.ones((2, 3)),
4004         ...     dims=["x", "y"],
4005         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4006         ... )
4007         >>> ds = xr.Dataset({"v": arr})
4008         >>> ds
4009         <xarray.Dataset>
4010         Dimensions:  (x: 2, y: 3)
4011         Coordinates:
4012           * x        (x) int64 0 1
4013           * y        (y) int64 0 1 2
4014             a        (x) int64 3 4
4015         Data variables:
4016             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4017         >>> ds.set_index(x="a")
4018         <xarray.Dataset>
4019         Dimensions:  (x: 2, y: 3)
4020         Coordinates:
4021           * x        (x) int64 3 4
4022           * y        (y) int64 0 1 2
4023         Data variables:
4024             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4025 
4026         See Also
4027         --------
4028         Dataset.reset_index
4029         Dataset.set_xindex
4030         Dataset.swap_dims
4031         """
4032         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4033 
4034         new_indexes: dict[Hashable, Index] = {}
4035         new_variables: dict[Hashable, Variable] = {}
4036         drop_indexes: set[Hashable] = set()
4037         drop_variables: set[Hashable] = set()
4038         replace_dims: dict[Hashable, Hashable] = {}
4039         all_var_names: set[Hashable] = set()
4040 
4041         for dim, _var_names in dim_coords.items():
4042             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4043                 var_names = [_var_names]
4044             else:
4045                 var_names = list(_var_names)
4046 
4047             invalid_vars = set(var_names) - set(self._variables)
4048             if invalid_vars:
4049                 raise ValueError(
4050                     ", ".join([str(v) for v in invalid_vars])
4051                     + " variable(s) do not exist"
4052                 )
4053 
4054             all_var_names.update(var_names)
4055             drop_variables.update(var_names)
4056 
4057             # drop any pre-existing index involved and its corresponding coordinates
4058             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4059             all_index_coord_names = set(index_coord_names)
4060             for k in var_names:
4061                 all_index_coord_names.update(
4062                     self.xindexes.get_all_coords(k, errors="ignore")
4063                 )
4064 
4065             drop_indexes.update(all_index_coord_names)
4066             drop_variables.update(all_index_coord_names)
4067 
4068             if len(var_names) == 1 and (not append or dim not in self._indexes):
4069                 var_name = var_names[0]
4070                 var = self._variables[var_name]
4071                 if var.dims != (dim,):
4072                     raise ValueError(
4073                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4074                         f"variable {var_name!r} that has dimensions {var.dims}"
4075                     )
4076                 idx = PandasIndex.from_variables({dim: var}, options={})
4077                 idx_vars = idx.create_variables({var_name: var})
4078 
4079                 # trick to preserve coordinate order in this case
4080                 if dim in self._coord_names:
4081                     drop_variables.remove(dim)
4082             else:
4083                 if append:
4084                     current_variables = {
4085                         k: self._variables[k] for k in index_coord_names
4086                     }
4087                 else:
4088                     current_variables = {}
4089                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4090                     dim,
4091                     current_variables,
4092                     {k: self._variables[k] for k in var_names},
4093                 )
4094                 for n in idx.index.names:
4095                     replace_dims[n] = dim
4096 
4097             new_indexes.update({k: idx for k in idx_vars})
4098             new_variables.update(idx_vars)
4099 
4100         # re-add deindexed coordinates (convert to base variables)
4101         for k in drop_variables:
4102             if (
4103                 k not in new_variables
4104                 and k not in all_var_names
4105                 and k in self._coord_names
4106             ):
4107                 new_variables[k] = self._variables[k].to_base_variable()
4108 
4109         indexes_: dict[Any, Index] = {
4110             k: v for k, v in self._indexes.items() if k not in drop_indexes
4111         }
4112         indexes_.update(new_indexes)
4113 
4114         variables = {
4115             k: v for k, v in self._variables.items() if k not in drop_variables
4116         }
4117         variables.update(new_variables)
4118 
4119         # update dimensions if necessary, GH: 3512
4120         for k, v in variables.items():
4121             if any(d in replace_dims for d in v.dims):
4122                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4123                 variables[k] = v._replace(dims=new_dims)
4124 
4125         coord_names = self._coord_names - drop_variables | set(new_variables)
4126 
4127         return self._replace_with_new_dims(
4128             variables, coord_names=coord_names, indexes=indexes_
4129         )
4130 
4131     def reset_index(
4132         self: T_Dataset,
4133         dims_or_levels: Hashable | Sequence[Hashable],
4134         drop: bool = False,
4135     ) -> T_Dataset:
4136         """Reset the specified index(es) or multi-index level(s).
4137 
4138         This legacy method is specific to pandas (multi-)indexes and
4139         1-dimensional "dimension" coordinates. See the more generic
4140         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4141         method to respectively drop and set pandas or custom indexes for
4142         arbitrary coordinates.
4143 
4144         Parameters
4145         ----------
4146         dims_or_levels : Hashable or Sequence of Hashable
4147             Name(s) of the dimension(s) and/or multi-index level(s) that will
4148             be reset.
4149         drop : bool, default: False
4150             If True, remove the specified indexes and/or multi-index levels
4151             instead of extracting them as new coordinates (default: False).
4152 
4153         Returns
4154         -------
4155         obj : Dataset
4156             Another dataset, with this dataset's data but replaced coordinates.
4157 
4158         See Also
4159         --------
4160         Dataset.set_index
4161         Dataset.set_xindex
4162         Dataset.drop_indexes
4163         """
4164         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4165             dims_or_levels = [dims_or_levels]
4166 
4167         invalid_coords = set(dims_or_levels) - set(self._indexes)
4168         if invalid_coords:
4169             raise ValueError(
4170                 f"{tuple(invalid_coords)} are not coordinates with an index"
4171             )
4172 
4173         drop_indexes: set[Hashable] = set()
4174         drop_variables: set[Hashable] = set()
4175         seen: set[Index] = set()
4176         new_indexes: dict[Hashable, Index] = {}
4177         new_variables: dict[Hashable, Variable] = {}
4178 
4179         def drop_or_convert(var_names):
4180             if drop:
4181                 drop_variables.update(var_names)
4182             else:
4183                 base_vars = {
4184                     k: self._variables[k].to_base_variable() for k in var_names
4185                 }
4186                 new_variables.update(base_vars)
4187 
4188         for name in dims_or_levels:
4189             index = self._indexes[name]
4190 
4191             if index in seen:
4192                 continue
4193             seen.add(index)
4194 
4195             idx_var_names = set(self.xindexes.get_all_coords(name))
4196             drop_indexes.update(idx_var_names)
4197 
4198             if isinstance(index, PandasMultiIndex):
4199                 # special case for pd.MultiIndex
4200                 level_names = index.index.names
4201                 keep_level_vars = {
4202                     k: self._variables[k]
4203                     for k in level_names
4204                     if k not in dims_or_levels
4205                 }
4206 
4207                 if index.dim not in dims_or_levels and keep_level_vars:
4208                     # do not drop the multi-index completely
4209                     # instead replace it by a new (multi-)index with dropped level(s)
4210                     idx = index.keep_levels(keep_level_vars)
4211                     idx_vars = idx.create_variables(keep_level_vars)
4212                     new_indexes.update({k: idx for k in idx_vars})
4213                     new_variables.update(idx_vars)
4214                     if not isinstance(idx, PandasMultiIndex):
4215                         # multi-index reduced to single index
4216                         # backward compatibility: unique level coordinate renamed to dimension
4217                         drop_variables.update(keep_level_vars)
4218                     drop_or_convert(
4219                         [k for k in level_names if k not in keep_level_vars]
4220                     )
4221                 else:
4222                     # always drop the multi-index dimension variable
4223                     drop_variables.add(index.dim)
4224                     drop_or_convert(level_names)
4225             else:
4226                 drop_or_convert(idx_var_names)
4227 
4228         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4229         indexes.update(new_indexes)
4230 
4231         variables = {
4232             k: v for k, v in self._variables.items() if k not in drop_variables
4233         }
4234         variables.update(new_variables)
4235 
4236         coord_names = self._coord_names - drop_variables
4237 
4238         return self._replace_with_new_dims(
4239             variables, coord_names=coord_names, indexes=indexes
4240         )
4241 
4242     def set_xindex(
4243         self: T_Dataset,
4244         coord_names: str | Sequence[Hashable],
4245         index_cls: type[Index] | None = None,
4246         **options,
4247     ) -> T_Dataset:
4248         """Set a new, Xarray-compatible index from one or more existing
4249         coordinate(s).
4250 
4251         Parameters
4252         ----------
4253         coord_names : str or list
4254             Name(s) of the coordinate(s) used to build the index.
4255             If several names are given, their order matters.
4256         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4257             The type of index to create. By default, try setting
4258             a ``PandasIndex`` if ``len(coord_names) == 1``,
4259             otherwise a ``PandasMultiIndex``.
4260         **options
4261             Options passed to the index constructor.
4262 
4263         Returns
4264         -------
4265         obj : Dataset
4266             Another dataset, with this dataset's data and with a new index.
4267 
4268         """
4269         # the Sequence check is required for mypy
4270         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4271             coord_names = [coord_names]
4272 
4273         if index_cls is None:
4274             if len(coord_names) == 1:
4275                 index_cls = PandasIndex
4276             else:
4277                 index_cls = PandasMultiIndex
4278         else:
4279             if not issubclass(index_cls, Index):
4280                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4281 
4282         invalid_coords = set(coord_names) - self._coord_names
4283 
4284         if invalid_coords:
4285             msg = ["invalid coordinate(s)"]
4286             no_vars = invalid_coords - set(self._variables)
4287             data_vars = invalid_coords - no_vars
4288             if no_vars:
4289                 msg.append(f"those variables don't exist: {no_vars}")
4290             if data_vars:
4291                 msg.append(
4292                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4293                 )
4294             raise ValueError("\n".join(msg))
4295 
4296         # we could be more clever here (e.g., drop-in index replacement if index
4297         # coordinates do not conflict), but let's not allow this for now
4298         indexed_coords = set(coord_names) & set(self._indexes)
4299 
4300         if indexed_coords:
4301             raise ValueError(
4302                 f"those coordinates already have an index: {indexed_coords}"
4303             )
4304 
4305         coord_vars = {name: self._variables[name] for name in coord_names}
4306 
4307         index = index_cls.from_variables(coord_vars, options=options)
4308 
4309         new_coord_vars = index.create_variables(coord_vars)
4310 
4311         # special case for setting a pandas multi-index from level coordinates
4312         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4313         # elements) coordinate
4314         if isinstance(index, PandasMultiIndex):
4315             coord_names = [index.dim] + list(coord_names)
4316 
4317         variables: dict[Hashable, Variable]
4318         indexes: dict[Hashable, Index]
4319 
4320         if len(coord_names) == 1:
4321             variables = self._variables.copy()
4322             indexes = self._indexes.copy()
4323 
4324             name = list(coord_names).pop()
4325             if name in new_coord_vars:
4326                 variables[name] = new_coord_vars[name]
4327             indexes[name] = index
4328         else:
4329             # reorder variables and indexes so that coordinates having the same
4330             # index are next to each other
4331             variables = {}
4332             for name, var in self._variables.items():
4333                 if name not in coord_names:
4334                     variables[name] = var
4335 
4336             indexes = {}
4337             for name, idx in self._indexes.items():
4338                 if name not in coord_names:
4339                     indexes[name] = idx
4340 
4341             for name in coord_names:
4342                 try:
4343                     variables[name] = new_coord_vars[name]
4344                 except KeyError:
4345                     variables[name] = self._variables[name]
4346                 indexes[name] = index
4347 
4348         return self._replace(
4349             variables=variables,
4350             coord_names=self._coord_names | set(coord_names),
4351             indexes=indexes,
4352         )
4353 
4354     def reorder_levels(
4355         self: T_Dataset,
4356         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4357         **dim_order_kwargs: Sequence[int | Hashable],
4358     ) -> T_Dataset:
4359         """Rearrange index levels using input order.
4360 
4361         Parameters
4362         ----------
4363         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4364             Mapping from names matching dimensions and values given
4365             by lists representing new level orders. Every given dimension
4366             must have a multi-index.
4367         **dim_order_kwargs : Sequence of int or Hashable, optional
4368             The keyword arguments form of ``dim_order``.
4369             One of dim_order or dim_order_kwargs must be provided.
4370 
4371         Returns
4372         -------
4373         obj : Dataset
4374             Another dataset, with this dataset's data but replaced
4375             coordinates.
4376         """
4377         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4378         variables = self._variables.copy()
4379         indexes = dict(self._indexes)
4380         new_indexes: dict[Hashable, Index] = {}
4381         new_variables: dict[Hashable, IndexVariable] = {}
4382 
4383         for dim, order in dim_order.items():
4384             index = self._indexes[dim]
4385 
4386             if not isinstance(index, PandasMultiIndex):
4387                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4388 
4389             level_vars = {k: self._variables[k] for k in order}
4390             idx = index.reorder_levels(level_vars)
4391             idx_vars = idx.create_variables(level_vars)
4392             new_indexes.update({k: idx for k in idx_vars})
4393             new_variables.update(idx_vars)
4394 
4395         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4396         indexes.update(new_indexes)
4397 
4398         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4399         variables.update(new_variables)
4400 
4401         return self._replace(variables, indexes=indexes)
4402 
4403     def _get_stack_index(
4404         self,
4405         dim,
4406         multi=False,
4407         create_index=False,
4408     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4409         """Used by stack and unstack to get one pandas (multi-)index among
4410         the indexed coordinates along dimension `dim`.
4411 
4412         If exactly one index is found, return it with its corresponding
4413         coordinate variables(s), otherwise return None and an empty dict.
4414 
4415         If `create_index=True`, create a new index if none is found or raise
4416         an error if multiple indexes are found.
4417 
4418         """
4419         stack_index: Index | None = None
4420         stack_coords: dict[Hashable, Variable] = {}
4421 
4422         for name, index in self._indexes.items():
4423             var = self._variables[name]
4424             if (
4425                 var.ndim == 1
4426                 and var.dims[0] == dim
4427                 and (
4428                     # stack: must be a single coordinate index
4429                     not multi
4430                     and not self.xindexes.is_multi(name)
4431                     # unstack: must be an index that implements .unstack
4432                     or multi
4433                     and type(index).unstack is not Index.unstack
4434                 )
4435             ):
4436                 if stack_index is not None and index is not stack_index:
4437                     # more than one index found, stop
4438                     if create_index:
4439                         raise ValueError(
4440                             f"cannot stack dimension {dim!r} with `create_index=True` "
4441                             "and with more than one index found along that dimension"
4442                         )
4443                     return None, {}
4444                 stack_index = index
4445                 stack_coords[name] = var
4446 
4447         if create_index and stack_index is None:
4448             if dim in self._variables:
4449                 var = self._variables[dim]
4450             else:
4451                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4452             # dummy index (only `stack_coords` will be used to construct the multi-index)
4453             stack_index = PandasIndex([0], dim)
4454             stack_coords = {dim: var}
4455 
4456         return stack_index, stack_coords
4457 
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )
4516 
4517     def stack(
4518         self: T_Dataset,
4519         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4520         create_index: bool | None = True,
4521         index_cls: type[Index] = PandasMultiIndex,
4522         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4523     ) -> T_Dataset:
4524         """
4525         Stack any number of existing dimensions into a single new dimension.
4526 
4527         New dimensions will be added at the end, and by default the corresponding
4528         coordinate variables will be combined into a MultiIndex.
4529 
4530         Parameters
4531         ----------
4532         dimensions : mapping of hashable to sequence of hashable
4533             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4534             dimensions, and the existing dimensions that they replace. An
4535             ellipsis (`...`) will be replaced by all unlisted dimensions.
4536             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4537             all dimensions.
4538         create_index : bool or None, default: True
4539 
4540             - True: create a multi-index for each of the stacked dimensions.
4541             - False: don't create any index.
4542             - None. create a multi-index only if exactly one single (1-d) coordinate
4543               index is found for every dimension to stack.
4544 
4545         index_cls: Index-class, default: PandasMultiIndex
4546             Can be used to pass a custom multi-index type (must be an Xarray index that
4547             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4548         **dimensions_kwargs
4549             The keyword arguments form of ``dimensions``.
4550             One of dimensions or dimensions_kwargs must be provided.
4551 
4552         Returns
4553         -------
4554         stacked : Dataset
4555             Dataset with stacked data.
4556 
4557         See Also
4558         --------
4559         Dataset.unstack
4560         """
4561         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4562         result = self
4563         for new_dim, dims in dimensions.items():
4564             result = result._stack_once(dims, new_dim, index_cls, create_index)
4565         return result
4566 
4567     def to_stacked_array(
4568         self,
4569         new_dim: Hashable,
4570         sample_dims: Collection[Hashable],
4571         variable_dim: Hashable = "variable",
4572         name: Hashable | None = None,
4573     ) -> DataArray:
4574         """Combine variables of differing dimensionality into a DataArray
4575         without broadcasting.
4576 
4577         This method is similar to Dataset.to_array but does not broadcast the
4578         variables.
4579 
4580         Parameters
4581         ----------
4582         new_dim : hashable
4583             Name of the new stacked coordinate
4584         sample_dims : Collection of hashables
4585             List of dimensions that **will not** be stacked. Each array in the
4586             dataset must share these dimensions. For machine learning
4587             applications, these define the dimensions over which samples are
4588             drawn.
4589         variable_dim : hashable, default: "variable"
4590             Name of the level in the stacked coordinate which corresponds to
4591             the variables.
4592         name : hashable, optional
4593             Name of the new data array.
4594 
4595         Returns
4596         -------
4597         stacked : DataArray
4598             DataArray with the specified dimensions and data variables
4599             stacked together. The stacked coordinate is named ``new_dim``
4600             and represented by a MultiIndex object with a level containing the
4601             data variable names. The name of this level is controlled using
4602             the ``variable_dim`` argument.
4603 
4604         See Also
4605         --------
4606         Dataset.to_array
4607         Dataset.stack
4608         DataArray.to_unstacked_dataset
4609 
4610         Examples
4611         --------
4612         >>> data = xr.Dataset(
4613         ...     data_vars={
4614         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4615         ...         "b": ("x", [6, 7]),
4616         ...     },
4617         ...     coords={"y": ["u", "v", "w"]},
4618         ... )
4619 
4620         >>> data
4621         <xarray.Dataset>
4622         Dimensions:  (x: 2, y: 3)
4623         Coordinates:
4624           * y        (y) <U1 'u' 'v' 'w'
4625         Dimensions without coordinates: x
4626         Data variables:
4627             a        (x, y) int64 0 1 2 3 4 5
4628             b        (x) int64 6 7
4629 
4630         >>> data.to_stacked_array("z", sample_dims=["x"])
4631         <xarray.DataArray 'a' (x: 2, z: 4)>
4632         array([[0, 1, 2, 6],
4633                [3, 4, 5, 7]])
4634         Coordinates:
4635           * z         (z) object MultiIndex
4636           * variable  (z) object 'a' 'a' 'a' 'b'
4637           * y         (z) object 'u' 'v' 'w' nan
4638         Dimensions without coordinates: x
4639 
4640         """
4641         from .concat import concat
4642 
4643         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4644 
4645         for variable in self:
4646             dims = self[variable].dims
4647             dims_include_sample_dims = set(sample_dims) <= set(dims)
4648             if not dims_include_sample_dims:
4649                 raise ValueError(
4650                     "All variables in the dataset must contain the "
4651                     "dimensions {}.".format(dims)
4652                 )
4653 
4654         def ensure_stackable(val):
4655             assign_coords = {variable_dim: val.name}
4656             for dim in stacking_dims:
4657                 if dim not in val.dims:
4658                     assign_coords[dim] = None
4659 
4660             expand_dims = set(stacking_dims).difference(set(val.dims))
4661             expand_dims.add(variable_dim)
4662             # must be list for .expand_dims
4663             expand_dims = list(expand_dims)
4664 
4665             return (
4666                 val.assign_coords(**assign_coords)
4667                 .expand_dims(expand_dims)
4668                 .stack({new_dim: (variable_dim,) + stacking_dims})
4669             )
4670 
4671         # concatenate the arrays
4672         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4673         data_array = concat(stackable_vars, dim=new_dim)
4674 
4675         if name is not None:
4676             data_array.name = name
4677 
4678         return data_array
4679 
4680     def _unstack_once(
4681         self: T_Dataset,
4682         dim: Hashable,
4683         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4684         fill_value,
4685         sparse: bool = False,
4686     ) -> T_Dataset:
4687         index, index_vars = index_and_vars
4688         variables: dict[Hashable, Variable] = {}
4689         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4690 
4691         new_indexes, clean_index = index.unstack()
4692         indexes.update(new_indexes)
4693 
4694         for name, idx in new_indexes.items():
4695             variables.update(idx.create_variables(index_vars))
4696 
4697         for name, var in self.variables.items():
4698             if name not in index_vars:
4699                 if dim in var.dims:
4700                     if isinstance(fill_value, Mapping):
4701                         fill_value_ = fill_value[name]
4702                     else:
4703                         fill_value_ = fill_value
4704 
4705                     variables[name] = var._unstack_once(
4706                         index=clean_index,
4707                         dim=dim,
4708                         fill_value=fill_value_,
4709                         sparse=sparse,
4710                     )
4711                 else:
4712                     variables[name] = var
4713 
4714         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4715 
4716         return self._replace_with_new_dims(
4717             variables, coord_names=coord_names, indexes=indexes
4718         )
4719 
4720     def _unstack_full_reindex(
4721         self: T_Dataset,
4722         dim: Hashable,
4723         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4724         fill_value,
4725         sparse: bool,
4726     ) -> T_Dataset:
4727         index, index_vars = index_and_vars
4728         variables: dict[Hashable, Variable] = {}
4729         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4730 
4731         new_indexes, clean_index = index.unstack()
4732         indexes.update(new_indexes)
4733 
4734         new_index_variables = {}
4735         for name, idx in new_indexes.items():
4736             new_index_variables.update(idx.create_variables(index_vars))
4737 
4738         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4739         variables.update(new_index_variables)
4740 
4741         # take a shortcut in case the MultiIndex was not modified.
4742         full_idx = pd.MultiIndex.from_product(
4743             clean_index.levels, names=clean_index.names
4744         )
4745         if clean_index.equals(full_idx):
4746             obj = self
4747         else:
4748             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4749             xr_full_idx = PandasMultiIndex(full_idx, dim)
4750             indexers = Indexes(
4751                 {k: xr_full_idx for k in index_vars},
4752                 xr_full_idx.create_variables(index_vars),
4753             )
4754             obj = self._reindex(
4755                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4756             )
4757 
4758         for name, var in obj.variables.items():
4759             if name not in index_vars:
4760                 if dim in var.dims:
4761                     variables[name] = var.unstack({dim: new_dim_sizes})
4762                 else:
4763                     variables[name] = var
4764 
4765         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4766 
4767         return self._replace_with_new_dims(
4768             variables, coord_names=coord_names, indexes=indexes
4769         )
4770 
4771     def unstack(
4772         self: T_Dataset,
4773         dim: Hashable | Iterable[Hashable] | None = None,
4774         fill_value: Any = xrdtypes.NA,
4775         sparse: bool = False,
4776     ) -> T_Dataset:
4777         """
4778         Unstack existing dimensions corresponding to MultiIndexes into
4779         multiple new dimensions.
4780 
4781         New dimensions will be added at the end.
4782 
4783         Parameters
4784         ----------
4785         dim : hashable or iterable of hashable, optional
4786             Dimension(s) over which to unstack. By default unstacks all
4787             MultiIndexes.
4788         fill_value : scalar or dict-like, default: nan
4789             value to be filled. If a dict-like, maps variable names to
4790             fill values. If not provided or if the dict-like does not
4791             contain all variables, the dtype's NA value will be used.
4792         sparse : bool, default: False
4793             use sparse-array if True
4794 
4795         Returns
4796         -------
4797         unstacked : Dataset
4798             Dataset with unstacked data.
4799 
4800         See Also
4801         --------
4802         Dataset.stack
4803         """
4804 
4805         if dim is None:
4806             dims = list(self.dims)
4807         else:
4808             if isinstance(dim, str) or not isinstance(dim, Iterable):
4809                 dims = [dim]
4810             else:
4811                 dims = list(dim)
4812 
4813             missing_dims = [d for d in dims if d not in self.dims]
4814             if missing_dims:
4815                 raise ValueError(
4816                     f"Dataset does not contain the dimensions: {missing_dims}"
4817                 )
4818 
4819         # each specified dimension must have exactly one multi-index
4820         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4821         for d in dims:
4822             idx, idx_vars = self._get_stack_index(d, multi=True)
4823             if idx is not None:
4824                 stacked_indexes[d] = idx, idx_vars
4825 
4826         if dim is None:
4827             dims = list(stacked_indexes)
4828         else:
4829             non_multi_dims = set(dims) - set(stacked_indexes)
4830             if non_multi_dims:
4831                 raise ValueError(
4832                     "cannot unstack dimensions that do not "
4833                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4834                 )
4835 
4836         result = self.copy(deep=False)
4837 
4838         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4839         # so we can't just access self.variables[v].data for every variable.
4840         # We only check the non-index variables.
4841         # https://github.com/pydata/xarray/issues/5902
4842         nonindexes = [
4843             self.variables[k] for k in set(self.variables) - set(self._indexes)
4844         ]
4845         # Notes for each of these cases:
4846         # 1. Dask arrays don't support assignment by index, which the fast unstack
4847         #    function requires.
4848         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4849         # 2. Sparse doesn't currently support (though we could special-case it)
4850         #    https://github.com/pydata/sparse/issues/422
4851         # 3. pint requires checking if it's a NumPy array until
4852         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4853         #    Once that is resolved, explicitly exclude pint arrays.
4854         #    pint doesn't implement `np.full_like` in a way that's
4855         #    currently compatible.
4856         needs_full_reindex = any(
4857             is_duck_dask_array(v.data)
4858             or isinstance(v.data, sparse_array_type)
4859             or not isinstance(v.data, np.ndarray)
4860             for v in nonindexes
4861         )
4862 
4863         for dim in dims:
4864             if needs_full_reindex:
4865                 result = result._unstack_full_reindex(
4866                     dim, stacked_indexes[dim], fill_value, sparse
4867                 )
4868             else:
4869                 result = result._unstack_once(
4870                     dim, stacked_indexes[dim], fill_value, sparse
4871                 )
4872         return result
4873 
4874     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4875         """Update this dataset's variables with those from another dataset.
4876 
4877         Just like :py:meth:`dict.update` this is a in-place operation.
4878         For a non-inplace version, see :py:meth:`Dataset.merge`.
4879 
4880         Parameters
4881         ----------
4882         other : Dataset or mapping
4883             Variables with which to update this dataset. One of:
4884 
4885             - Dataset
4886             - mapping {var name: DataArray}
4887             - mapping {var name: Variable}
4888             - mapping {var name: (dimension name, array-like)}
4889             - mapping {var name: (tuple of dimension names, array-like)}
4890 
4891         Returns
4892         -------
4893         updated : Dataset
4894             Updated dataset. Note that since the update is in-place this is the input
4895             dataset.
4896 
4897             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4898 
4899         Raises
4900         ------
4901         ValueError
4902             If any dimensions would have inconsistent sizes in the updated
4903             dataset.
4904 
4905         See Also
4906         --------
4907         Dataset.assign
4908         Dataset.merge
4909         """
4910         merge_result = dataset_update_method(self, other)
4911         return self._replace(inplace=True, **merge_result._asdict())
4912 
4913     def merge(
4914         self: T_Dataset,
4915         other: CoercibleMapping | DataArray,
4916         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4917         compat: CompatOptions = "no_conflicts",
4918         join: JoinOptions = "outer",
4919         fill_value: Any = xrdtypes.NA,
4920         combine_attrs: CombineAttrsOptions = "override",
4921     ) -> T_Dataset:
4922         """Merge the arrays of two datasets into a single dataset.
4923 
4924         This method generally does not allow for overriding data, with the
4925         exception of attributes, which are ignored on the second dataset.
4926         Variables with the same name are checked for conflicts via the equals
4927         or identical methods.
4928 
4929         Parameters
4930         ----------
4931         other : Dataset or mapping
4932             Dataset or variables to merge with this dataset.
4933         overwrite_vars : hashable or iterable of hashable, optional
4934             If provided, update variables of these name(s) without checking for
4935             conflicts in this dataset.
4936         compat : {"identical", "equals", "broadcast_equals", \
4937                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4938             String indicating how to compare variables of the same name for
4939             potential conflicts:
4940 
4941             - 'identical': all values, dimensions and attributes must be the
4942               same.
4943             - 'equals': all values and dimensions must be the same.
4944             - 'broadcast_equals': all values must be equal when variables are
4945               broadcast against each other to ensure common dimensions.
4946             - 'no_conflicts': only values which are not null in both datasets
4947               must be equal. The returned dataset then contains the combination
4948               of all non-null values.
4949             - 'override': skip comparing and pick variable from first dataset
4950             - 'minimal': drop conflicting coordinates
4951 
4952         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4953                default: "outer"
4954             Method for joining ``self`` and ``other`` along shared dimensions:
4955 
4956             - 'outer': use the union of the indexes
4957             - 'inner': use the intersection of the indexes
4958             - 'left': use indexes from ``self``
4959             - 'right': use indexes from ``other``
4960             - 'exact': error instead of aligning non-equal indexes
4961             - 'override': use indexes from ``self`` that are the same size
4962               as those of ``other`` in that dimension
4963 
4964         fill_value : scalar or dict-like, optional
4965             Value to use for newly missing values. If a dict-like, maps
4966             variable names (including coordinates) to fill values.
4967         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4968                          "override"} or callable, default: "override"
4969             A callable or a string indicating how to combine attrs of the objects being
4970             merged:
4971 
4972             - "drop": empty attrs on returned Dataset.
4973             - "identical": all attrs must be the same on every object.
4974             - "no_conflicts": attrs from all objects are combined, any that have
4975               the same name must also have the same value.
4976             - "drop_conflicts": attrs from all objects are combined, any that have
4977               the same name but different values are dropped.
4978             - "override": skip comparing and copy attrs from the first dataset to
4979               the result.
4980 
4981             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4982             as its only parameters.
4983 
4984         Returns
4985         -------
4986         merged : Dataset
4987             Merged dataset.
4988 
4989         Raises
4990         ------
4991         MergeError
4992             If any variables conflict (see ``compat``).
4993 
4994         See Also
4995         --------
4996         Dataset.update
4997         """
4998         from .dataarray import DataArray
4999 
5000         other = other.to_dataset() if isinstance(other, DataArray) else other
5001         merge_result = dataset_merge_method(
5002             self,
5003             other,
5004             overwrite_vars=overwrite_vars,
5005             compat=compat,
5006             join=join,
5007             fill_value=fill_value,
5008             combine_attrs=combine_attrs,
5009         )
5010         return self._replace(**merge_result._asdict())
5011 
5012     def _assert_all_in_dataset(
5013         self, names: Iterable[Hashable], virtual_okay: bool = False
5014     ) -> None:
5015         bad_names = set(names) - set(self._variables)
5016         if virtual_okay:
5017             bad_names -= self.virtual_variables
5018         if bad_names:
5019             raise ValueError(
5020                 "One or more of the specified variables "
5021                 "cannot be found in this dataset"
5022             )
5023 
5024     def drop_vars(
5025         self: T_Dataset,
5026         names: Hashable | Iterable[Hashable],
5027         *,
5028         errors: ErrorOptions = "raise",
5029     ) -> T_Dataset:
5030         """Drop variables from this dataset.
5031 
5032         Parameters
5033         ----------
5034         names : hashable or iterable of hashable
5035             Name(s) of variables to drop.
5036         errors : {"raise", "ignore"}, default: "raise"
5037             If 'raise', raises a ValueError error if any of the variable
5038             passed are not in the dataset. If 'ignore', any given names that are in the
5039             dataset are dropped and no error is raised.
5040 
5041         Returns
5042         -------
5043         dropped : Dataset
5044 
5045         """
5046         # the Iterable check is required for mypy
5047         if is_scalar(names) or not isinstance(names, Iterable):
5048             names = {names}
5049         else:
5050             names = set(names)
5051         if errors == "raise":
5052             self._assert_all_in_dataset(names)
5053 
5054         # GH6505
5055         other_names = set()
5056         for var in names:
5057             maybe_midx = self._indexes.get(var, None)
5058             if isinstance(maybe_midx, PandasMultiIndex):
5059                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5060                 idx_other_names = idx_coord_names - set(names)
5061                 other_names.update(idx_other_names)
5062         if other_names:
5063             names |= set(other_names)
5064             warnings.warn(
5065                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5066                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5067                 DeprecationWarning,
5068                 stacklevel=2,
5069             )
5070 
5071         assert_no_index_corrupted(self.xindexes, names)
5072 
5073         variables = {k: v for k, v in self._variables.items() if k not in names}
5074         coord_names = {k for k in self._coord_names if k in variables}
5075         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5076         return self._replace_with_new_dims(
5077             variables, coord_names=coord_names, indexes=indexes
5078         )
5079 
5080     def drop_indexes(
5081         self: T_Dataset,
5082         coord_names: Hashable | Iterable[Hashable],
5083         *,
5084         errors: ErrorOptions = "raise",
5085     ) -> T_Dataset:
5086         """Drop the indexes assigned to the given coordinates.
5087 
5088         Parameters
5089         ----------
5090         coord_names : hashable or iterable of hashable
5091             Name(s) of the coordinate(s) for which to drop the index.
5092         errors : {"raise", "ignore"}, default: "raise"
5093             If 'raise', raises a ValueError error if any of the coordinates
5094             passed have no index or are not in the dataset.
5095             If 'ignore', no error is raised.
5096 
5097         Returns
5098         -------
5099         dropped : Dataset
5100             A new dataset with dropped indexes.
5101 
5102         """
5103         # the Iterable check is required for mypy
5104         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5105             coord_names = {coord_names}
5106         else:
5107             coord_names = set(coord_names)
5108 
5109         if errors == "raise":
5110             invalid_coords = coord_names - self._coord_names
5111             if invalid_coords:
5112                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5113 
5114             unindexed_coords = set(coord_names) - set(self._indexes)
5115             if unindexed_coords:
5116                 raise ValueError(
5117                     f"those coordinates do not have an index: {unindexed_coords}"
5118                 )
5119 
5120         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5121 
5122         variables = {}
5123         for name, var in self._variables.items():
5124             if name in coord_names:
5125                 variables[name] = var.to_base_variable()
5126             else:
5127                 variables[name] = var
5128 
5129         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5130 
5131         return self._replace(variables=variables, indexes=indexes)
5132 
5133     def drop(
5134         self: T_Dataset,
5135         labels=None,
5136         dim=None,
5137         *,
5138         errors: ErrorOptions = "raise",
5139         **labels_kwargs,
5140     ) -> T_Dataset:
5141         """Backward compatible method based on `drop_vars` and `drop_sel`
5142 
5143         Using either `drop_vars` or `drop_sel` is encouraged
5144 
5145         See Also
5146         --------
5147         Dataset.drop_vars
5148         Dataset.drop_sel
5149         """
5150         if errors not in ["raise", "ignore"]:
5151             raise ValueError('errors must be either "raise" or "ignore"')
5152 
5153         if is_dict_like(labels) and not isinstance(labels, dict):
5154             warnings.warn(
5155                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5156                 FutureWarning,
5157                 stacklevel=2,
5158             )
5159             return self.drop_vars(labels, errors=errors)
5160 
5161         if labels_kwargs or isinstance(labels, dict):
5162             if dim is not None:
5163                 raise ValueError("cannot specify dim and dict-like arguments.")
5164             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5165 
5166         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5167             warnings.warn(
5168                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5169                 PendingDeprecationWarning,
5170                 stacklevel=2,
5171             )
5172             return self.drop_vars(labels, errors=errors)
5173         if dim is not None:
5174             warnings.warn(
5175                 "dropping labels using list-like labels is deprecated; using "
5176                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5177                 DeprecationWarning,
5178                 stacklevel=2,
5179             )
5180             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5181 
5182         warnings.warn(
5183             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5184             PendingDeprecationWarning,
5185             stacklevel=2,
5186         )
5187         return self.drop_sel(labels, errors=errors)
5188 
5189     def drop_sel(
5190         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5191     ) -> T_Dataset:
5192         """Drop index labels from this dataset.
5193 
5194         Parameters
5195         ----------
5196         labels : mapping of hashable to Any
5197             Index labels to drop
5198         errors : {"raise", "ignore"}, default: "raise"
5199             If 'raise', raises a ValueError error if
5200             any of the index labels passed are not
5201             in the dataset. If 'ignore', any given labels that are in the
5202             dataset are dropped and no error is raised.
5203         **labels_kwargs : {dim: label, ...}, optional
5204             The keyword arguments form of ``dim`` and ``labels``
5205 
5206         Returns
5207         -------
5208         dropped : Dataset
5209 
5210         Examples
5211         --------
5212         >>> data = np.arange(6).reshape(2, 3)
5213         >>> labels = ["a", "b", "c"]
5214         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5215         >>> ds
5216         <xarray.Dataset>
5217         Dimensions:  (x: 2, y: 3)
5218         Coordinates:
5219           * y        (y) <U1 'a' 'b' 'c'
5220         Dimensions without coordinates: x
5221         Data variables:
5222             A        (x, y) int64 0 1 2 3 4 5
5223         >>> ds.drop_sel(y=["a", "c"])
5224         <xarray.Dataset>
5225         Dimensions:  (x: 2, y: 1)
5226         Coordinates:
5227           * y        (y) <U1 'b'
5228         Dimensions without coordinates: x
5229         Data variables:
5230             A        (x, y) int64 1 4
5231         >>> ds.drop_sel(y="b")
5232         <xarray.Dataset>
5233         Dimensions:  (x: 2, y: 2)
5234         Coordinates:
5235           * y        (y) <U1 'a' 'c'
5236         Dimensions without coordinates: x
5237         Data variables:
5238             A        (x, y) int64 0 2 3 5
5239         """
5240         if errors not in ["raise", "ignore"]:
5241             raise ValueError('errors must be either "raise" or "ignore"')
5242 
5243         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5244 
5245         ds = self
5246         for dim, labels_for_dim in labels.items():
5247             # Don't cast to set, as it would harm performance when labels
5248             # is a large numpy array
5249             if utils.is_scalar(labels_for_dim):
5250                 labels_for_dim = [labels_for_dim]
5251             labels_for_dim = np.asarray(labels_for_dim)
5252             try:
5253                 index = self.get_index(dim)
5254             except KeyError:
5255                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5256             new_index = index.drop(labels_for_dim, errors=errors)
5257             ds = ds.loc[{dim: new_index}]
5258         return ds
5259 
5260     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5261         """Drop index positions from this Dataset.
5262 
5263         Parameters
5264         ----------
5265         indexers : mapping of hashable to Any
5266             Index locations to drop
5267         **indexers_kwargs : {dim: position, ...}, optional
5268             The keyword arguments form of ``dim`` and ``positions``
5269 
5270         Returns
5271         -------
5272         dropped : Dataset
5273 
5274         Raises
5275         ------
5276         IndexError
5277 
5278         Examples
5279         --------
5280         >>> data = np.arange(6).reshape(2, 3)
5281         >>> labels = ["a", "b", "c"]
5282         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5283         >>> ds
5284         <xarray.Dataset>
5285         Dimensions:  (x: 2, y: 3)
5286         Coordinates:
5287           * y        (y) <U1 'a' 'b' 'c'
5288         Dimensions without coordinates: x
5289         Data variables:
5290             A        (x, y) int64 0 1 2 3 4 5
5291         >>> ds.drop_isel(y=[0, 2])
5292         <xarray.Dataset>
5293         Dimensions:  (x: 2, y: 1)
5294         Coordinates:
5295           * y        (y) <U1 'b'
5296         Dimensions without coordinates: x
5297         Data variables:
5298             A        (x, y) int64 1 4
5299         >>> ds.drop_isel(y=1)
5300         <xarray.Dataset>
5301         Dimensions:  (x: 2, y: 2)
5302         Coordinates:
5303           * y        (y) <U1 'a' 'c'
5304         Dimensions without coordinates: x
5305         Data variables:
5306             A        (x, y) int64 0 2 3 5
5307         """
5308 
5309         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5310 
5311         ds = self
5312         dimension_index = {}
5313         for dim, pos_for_dim in indexers.items():
5314             # Don't cast to set, as it would harm performance when labels
5315             # is a large numpy array
5316             if utils.is_scalar(pos_for_dim):
5317                 pos_for_dim = [pos_for_dim]
5318             pos_for_dim = np.asarray(pos_for_dim)
5319             index = self.get_index(dim)
5320             new_index = index.delete(pos_for_dim)
5321             dimension_index[dim] = new_index
5322         ds = ds.loc[dimension_index]
5323         return ds
5324 
5325     def drop_dims(
5326         self: T_Dataset,
5327         drop_dims: Hashable | Iterable[Hashable],
5328         *,
5329         errors: ErrorOptions = "raise",
5330     ) -> T_Dataset:
5331         """Drop dimensions and associated variables from this dataset.
5332 
5333         Parameters
5334         ----------
5335         drop_dims : hashable or iterable of hashable
5336             Dimension or dimensions to drop.
5337         errors : {"raise", "ignore"}, default: "raise"
5338             If 'raise', raises a ValueError error if any of the
5339             dimensions passed are not in the dataset. If 'ignore', any given
5340             dimensions that are in the dataset are dropped and no error is raised.
5341 
5342         Returns
5343         -------
5344         obj : Dataset
5345             The dataset without the given dimensions (or any variables
5346             containing those dimensions).
5347         """
5348         if errors not in ["raise", "ignore"]:
5349             raise ValueError('errors must be either "raise" or "ignore"')
5350 
5351         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5352             drop_dims = {drop_dims}
5353         else:
5354             drop_dims = set(drop_dims)
5355 
5356         if errors == "raise":
5357             missing_dims = drop_dims - set(self.dims)
5358             if missing_dims:
5359                 raise ValueError(
5360                     f"Dataset does not contain the dimensions: {missing_dims}"
5361                 )
5362 
5363         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5364         return self.drop_vars(drop_vars)
5365 
5366     def transpose(
5367         self: T_Dataset,
5368         *dims: Hashable,
5369         missing_dims: ErrorOptionsWithWarn = "raise",
5370     ) -> T_Dataset:
5371         """Return a new Dataset object with all array dimensions transposed.
5372 
5373         Although the order of dimensions on each array will change, the dataset
5374         dimensions themselves will remain in fixed (sorted) order.
5375 
5376         Parameters
5377         ----------
5378         *dims : hashable, optional
5379             By default, reverse the dimensions on each array. Otherwise,
5380             reorder the dimensions to this order.
5381         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5382             What to do if dimensions that should be selected from are not present in the
5383             Dataset:
5384             - "raise": raise an exception
5385             - "warn": raise a warning, and ignore the missing dimensions
5386             - "ignore": ignore the missing dimensions
5387 
5388         Returns
5389         -------
5390         transposed : Dataset
5391             Each array in the dataset (including) coordinates will be
5392             transposed to the given order.
5393 
5394         Notes
5395         -----
5396         This operation returns a view of each array's data. It is
5397         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5398         -- the data will be fully loaded into memory.
5399 
5400         See Also
5401         --------
5402         numpy.transpose
5403         DataArray.transpose
5404         """
5405         # Use infix_dims to check once for missing dimensions
5406         if len(dims) != 0:
5407             _ = list(infix_dims(dims, self.dims, missing_dims))
5408 
5409         ds = self.copy()
5410         for name, var in self._variables.items():
5411             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5412             ds._variables[name] = var.transpose(*var_dims)
5413         return ds
5414 
5415     def dropna(
5416         self: T_Dataset,
5417         dim: Hashable,
5418         how: Literal["any", "all"] = "any",
5419         thresh: int | None = None,
5420         subset: Iterable[Hashable] | None = None,
5421     ) -> T_Dataset:
5422         """Returns a new dataset with dropped labels for missing values along
5423         the provided dimension.
5424 
5425         Parameters
5426         ----------
5427         dim : hashable
5428             Dimension along which to drop missing values. Dropping along
5429             multiple dimensions simultaneously is not yet supported.
5430         how : {"any", "all"}, default: "any"
5431             - any : if any NA values are present, drop that label
5432             - all : if all values are NA, drop that label
5433 
5434         thresh : int or None, optional
5435             If supplied, require this many non-NA values.
5436         subset : iterable of hashable or None, optional
5437             Which variables to check for missing values. By default, all
5438             variables in the dataset are checked.
5439 
5440         Returns
5441         -------
5442         Dataset
5443         """
5444         # TODO: consider supporting multiple dimensions? Or not, given that
5445         # there are some ugly edge cases, e.g., pandas's dropna differs
5446         # depending on the order of the supplied axes.
5447 
5448         if dim not in self.dims:
5449             raise ValueError(f"{dim} must be a single dataset dimension")
5450 
5451         if subset is None:
5452             subset = iter(self.data_vars)
5453 
5454         count = np.zeros(self.dims[dim], dtype=np.int64)
5455         size = np.int_(0)  # for type checking
5456 
5457         for k in subset:
5458             array = self._variables[k]
5459             if dim in array.dims:
5460                 dims = [d for d in array.dims if d != dim]
5461                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5462                 size += math.prod([self.dims[d] for d in dims])
5463 
5464         if thresh is not None:
5465             mask = count >= thresh
5466         elif how == "any":
5467             mask = count == size
5468         elif how == "all":
5469             mask = count > 0
5470         elif how is not None:
5471             raise ValueError(f"invalid how option: {how}")
5472         else:
5473             raise TypeError("must specify how or thresh")
5474 
5475         return self.isel({dim: mask})
5476 
5477     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5478         """Fill missing values in this object.
5479 
5480         This operation follows the normal broadcasting and alignment rules that
5481         xarray uses for binary arithmetic, except the result is aligned to this
5482         object (``join='left'``) instead of aligned to the intersection of
5483         index coordinates (``join='inner'``).
5484 
5485         Parameters
5486         ----------
5487         value : scalar, ndarray, DataArray, dict or Dataset
5488             Used to fill all matching missing values in this dataset's data
5489             variables. Scalars, ndarrays or DataArrays arguments are used to
5490             fill all data with aligned coordinates (for DataArrays).
5491             Dictionaries or datasets match data variables and then align
5492             coordinates if necessary.
5493 
5494         Returns
5495         -------
5496         Dataset
5497 
5498         Examples
5499         --------
5500         >>> ds = xr.Dataset(
5501         ...     {
5502         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5503         ...         "B": ("x", [3, 4, np.nan, 1]),
5504         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5505         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5506         ...     },
5507         ...     coords={"x": [0, 1, 2, 3]},
5508         ... )
5509         >>> ds
5510         <xarray.Dataset>
5511         Dimensions:  (x: 4)
5512         Coordinates:
5513           * x        (x) int64 0 1 2 3
5514         Data variables:
5515             A        (x) float64 nan 2.0 nan 0.0
5516             B        (x) float64 3.0 4.0 nan 1.0
5517             C        (x) float64 nan nan nan 5.0
5518             D        (x) float64 nan 3.0 nan 4.0
5519 
5520         Replace all `NaN` values with 0s.
5521 
5522         >>> ds.fillna(0)
5523         <xarray.Dataset>
5524         Dimensions:  (x: 4)
5525         Coordinates:
5526           * x        (x) int64 0 1 2 3
5527         Data variables:
5528             A        (x) float64 0.0 2.0 0.0 0.0
5529             B        (x) float64 3.0 4.0 0.0 1.0
5530             C        (x) float64 0.0 0.0 0.0 5.0
5531             D        (x) float64 0.0 3.0 0.0 4.0
5532 
5533         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
5534 
5535         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5536         >>> ds.fillna(value=values)
5537         <xarray.Dataset>
5538         Dimensions:  (x: 4)
5539         Coordinates:
5540           * x        (x) int64 0 1 2 3
5541         Data variables:
5542             A        (x) float64 0.0 2.0 0.0 0.0
5543             B        (x) float64 3.0 4.0 1.0 1.0
5544             C        (x) float64 2.0 2.0 2.0 5.0
5545             D        (x) float64 3.0 3.0 3.0 4.0
5546         """
5547         if utils.is_dict_like(value):
5548             value_keys = getattr(value, "data_vars", value).keys()
5549             if not set(value_keys) <= set(self.data_vars.keys()):
5550                 raise ValueError(
5551                     "all variables in the argument to `fillna` "
5552                     "must be contained in the original dataset"
5553                 )
5554         out = ops.fillna(self, value)
5555         return out
5556 
5557     def interpolate_na(
5558         self: T_Dataset,
5559         dim: Hashable | None = None,
5560         method: InterpOptions = "linear",
5561         limit: int = None,
5562         use_coordinate: bool | Hashable = True,
5563         max_gap: (
5564             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5565         ) = None,
5566         **kwargs: Any,
5567     ) -> T_Dataset:
5568         """Fill in NaNs by interpolating according to different methods.
5569 
5570         Parameters
5571         ----------
5572         dim : Hashable or None, optional
5573             Specifies the dimension along which to interpolate.
5574         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5575             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5576             String indicating which method to use for interpolation:
5577 
5578             - 'linear': linear interpolation. Additional keyword
5579               arguments are passed to :py:func:`numpy.interp`
5580             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5581               are passed to :py:func:`scipy.interpolate.interp1d`. If
5582               ``method='polynomial'``, the ``order`` keyword argument must also be
5583               provided.
5584             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5585               respective :py:class:`scipy.interpolate` classes.
5586 
5587         use_coordinate : bool or Hashable, default: True
5588             Specifies which index to use as the x values in the interpolation
5589             formulated as `y = f(x)`. If False, values are treated as if
5590             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5591             used. If ``use_coordinate`` is a string, it specifies the name of a
5592             coordinate variariable to use as the index.
5593         limit : int, default: None
5594             Maximum number of consecutive NaNs to fill. Must be greater than 0
5595             or None for no limit. This filling is done regardless of the size of
5596             the gap in the data. To only interpolate over gaps less than a given length,
5597             see ``max_gap``.
5598         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5599             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5600             Use None for no limit. When interpolating along a datetime64 dimension
5601             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5602 
5603             - a string that is valid input for pandas.to_timedelta
5604             - a :py:class:`numpy.timedelta64` object
5605             - a :py:class:`pandas.Timedelta` object
5606             - a :py:class:`datetime.timedelta` object
5607 
5608             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5609             dimensions has not been implemented yet. Gap length is defined as the difference
5610             between coordinate values at the first data point after a gap and the last value
5611             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5612             between coordinate values at the first (last) valid data point and the first (last) NaN.
5613             For example, consider::
5614 
5615                 <xarray.DataArray (x: 9)>
5616                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5617                 Coordinates:
5618                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5619 
5620             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5621         **kwargs : dict, optional
5622             parameters passed verbatim to the underlying interpolation function
5623 
5624         Returns
5625         -------
5626         interpolated: Dataset
5627             Filled in Dataset.
5628 
5629         See Also
5630         --------
5631         numpy.interp
5632         scipy.interpolate
5633 
5634         Examples
5635         --------
5636         >>> ds = xr.Dataset(
5637         ...     {
5638         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5639         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5640         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5641         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5642         ...     },
5643         ...     coords={"x": [0, 1, 2, 3, 4]},
5644         ... )
5645         >>> ds
5646         <xarray.Dataset>
5647         Dimensions:  (x: 5)
5648         Coordinates:
5649           * x        (x) int64 0 1 2 3 4
5650         Data variables:
5651             A        (x) float64 nan 2.0 3.0 nan 0.0
5652             B        (x) float64 3.0 4.0 nan 1.0 7.0
5653             C        (x) float64 nan nan nan 5.0 0.0
5654             D        (x) float64 nan 3.0 nan -1.0 4.0
5655 
5656         >>> ds.interpolate_na(dim="x", method="linear")
5657         <xarray.Dataset>
5658         Dimensions:  (x: 5)
5659         Coordinates:
5660           * x        (x) int64 0 1 2 3 4
5661         Data variables:
5662             A        (x) float64 nan 2.0 3.0 1.5 0.0
5663             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5664             C        (x) float64 nan nan nan 5.0 0.0
5665             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5666 
5667         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5668         <xarray.Dataset>
5669         Dimensions:  (x: 5)
5670         Coordinates:
5671           * x        (x) int64 0 1 2 3 4
5672         Data variables:
5673             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5674             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5675             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5676             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5677         """
5678         from .missing import _apply_over_vars_with_dim, interp_na
5679 
5680         new = _apply_over_vars_with_dim(
5681             interp_na,
5682             self,
5683             dim=dim,
5684             method=method,
5685             limit=limit,
5686             use_coordinate=use_coordinate,
5687             max_gap=max_gap,
5688             **kwargs,
5689         )
5690         return new
5691 
5692     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5693         """Fill NaN values by propagating values forward
5694 
5695         *Requires bottleneck.*
5696 
5697         Parameters
5698         ----------
5699         dim : Hashable
5700             Specifies the dimension along which to propagate values when
5701             filling.
5702         limit : int or None, optional
5703             The maximum number of consecutive NaN values to forward fill. In
5704             other words, if there is a gap with more than this number of
5705             consecutive NaNs, it will only be partially filled. Must be greater
5706             than 0 or None for no limit. Must be None or greater than or equal
5707             to axis length if filling along chunked axes (dimensions).
5708 
5709         Returns
5710         -------
5711         Dataset
5712         """
5713         from .missing import _apply_over_vars_with_dim, ffill
5714 
5715         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5716         return new
5717 
5718     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5719         """Fill NaN values by propagating values backward
5720 
5721         *Requires bottleneck.*
5722 
5723         Parameters
5724         ----------
5725         dim : Hashable
5726             Specifies the dimension along which to propagate values when
5727             filling.
5728         limit : int or None, optional
5729             The maximum number of consecutive NaN values to backward fill. In
5730             other words, if there is a gap with more than this number of
5731             consecutive NaNs, it will only be partially filled. Must be greater
5732             than 0 or None for no limit. Must be None or greater than or equal
5733             to axis length if filling along chunked axes (dimensions).
5734 
5735         Returns
5736         -------
5737         Dataset
5738         """
5739         from .missing import _apply_over_vars_with_dim, bfill
5740 
5741         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5742         return new
5743 
5744     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5745         """Combine two Datasets, default to data_vars of self.
5746 
5747         The new coordinates follow the normal broadcasting and alignment rules
5748         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5749         filled with np.nan.
5750 
5751         Parameters
5752         ----------
5753         other : Dataset
5754             Used to fill all matching missing values in this array.
5755 
5756         Returns
5757         -------
5758         Dataset
5759         """
5760         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5761         return out
5762 
5763     def reduce(
5764         self: T_Dataset,
5765         func: Callable,
5766         dim: Hashable | Iterable[Hashable] = None,
5767         *,
5768         keep_attrs: bool | None = None,
5769         keepdims: bool = False,
5770         numeric_only: bool = False,
5771         **kwargs: Any,
5772     ) -> T_Dataset:
5773         """Reduce this dataset by applying `func` along some dimension(s).
5774 
5775         Parameters
5776         ----------
5777         func : callable
5778             Function which can be called in the form
5779             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5780             np.ndarray over an integer valued axis.
5781         dim : str or sequence of str, optional
5782             Dimension(s) over which to apply `func`.  By default `func` is
5783             applied over all dimensions.
5784         keep_attrs : bool or None, optional
5785             If True, the dataset's attributes (`attrs`) will be copied from
5786             the original object to the new one.  If False (default), the new
5787             object will be returned without attributes.
5788         keepdims : bool, default: False
5789             If True, the dimensions which are reduced are left in the result
5790             as dimensions of size one. Coordinates that use these dimensions
5791             are removed.
5792         numeric_only : bool, default: False
5793             If True, only apply ``func`` to variables with a numeric dtype.
5794         **kwargs : Any
5795             Additional keyword arguments passed on to ``func``.
5796 
5797         Returns
5798         -------
5799         reduced : Dataset
5800             Dataset with this object's DataArrays replaced with new DataArrays
5801             of summarized data and the indicated dimension(s) removed.
5802         """
5803         if kwargs.get("axis", None) is not None:
5804             raise ValueError(
5805                 "passing 'axis' to Dataset reduce methods is ambiguous."
5806                 " Please use 'dim' instead."
5807             )
5808 
5809         if dim is None or dim is ...:
5810             dims = set(self.dims)
5811         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5812             dims = {dim}
5813         else:
5814             dims = set(dim)
5815 
5816         missing_dimensions = [d for d in dims if d not in self.dims]
5817         if missing_dimensions:
5818             raise ValueError(
5819                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5820             )
5821 
5822         if keep_attrs is None:
5823             keep_attrs = _get_keep_attrs(default=False)
5824 
5825         variables: dict[Hashable, Variable] = {}
5826         for name, var in self._variables.items():
5827             reduce_dims = [d for d in var.dims if d in dims]
5828             if name in self.coords:
5829                 if not reduce_dims:
5830                     variables[name] = var
5831             else:
5832                 if (
5833                     # Some reduction functions (e.g. std, var) need to run on variables
5834                     # that don't have the reduce dims: PR5393
5835                     not reduce_dims
5836                     or not numeric_only
5837                     or np.issubdtype(var.dtype, np.number)
5838                     or (var.dtype == np.bool_)
5839                 ):
5840                     reduce_maybe_single: Hashable | None | list[Hashable]
5841                     if len(reduce_dims) == 1:
5842                         # unpack dimensions for the benefit of functions
5843                         # like np.argmin which can't handle tuple arguments
5844                         (reduce_maybe_single,) = reduce_dims
5845                     elif len(reduce_dims) == var.ndim:
5846                         # prefer to aggregate over axis=None rather than
5847                         # axis=(0, 1) if they will be equivalent, because
5848                         # the former is often more efficient
5849                         reduce_maybe_single = None
5850                     else:
5851                         reduce_maybe_single = reduce_dims
5852                     variables[name] = var.reduce(
5853                         func,
5854                         dim=reduce_maybe_single,
5855                         keep_attrs=keep_attrs,
5856                         keepdims=keepdims,
5857                         **kwargs,
5858                     )
5859 
5860         coord_names = {k for k in self.coords if k in variables}
5861         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5862         attrs = self.attrs if keep_attrs else None
5863         return self._replace_with_new_dims(
5864             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5865         )
5866 
5867     def map(
5868         self: T_Dataset,
5869         func: Callable,
5870         keep_attrs: bool | None = None,
5871         args: Iterable[Any] = (),
5872         **kwargs: Any,
5873     ) -> T_Dataset:
5874         """Apply a function to each data variable in this dataset
5875 
5876         Parameters
5877         ----------
5878         func : callable
5879             Function which can be called in the form `func(x, *args, **kwargs)`
5880             to transform each DataArray `x` in this dataset into another
5881             DataArray.
5882         keep_attrs : bool or None, optional
5883             If True, both the dataset's and variables' attributes (`attrs`) will be
5884             copied from the original objects to the new ones. If False, the new dataset
5885             and variables will be returned without copying the attributes.
5886         args : iterable, optional
5887             Positional arguments passed on to `func`.
5888         **kwargs : Any
5889             Keyword arguments passed on to `func`.
5890 
5891         Returns
5892         -------
5893         applied : Dataset
5894             Resulting dataset from applying ``func`` to each data variable.
5895 
5896         Examples
5897         --------
5898         >>> da = xr.DataArray(np.random.randn(2, 3))
5899         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5900         >>> ds
5901         <xarray.Dataset>
5902         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5903         Dimensions without coordinates: dim_0, dim_1, x
5904         Data variables:
5905             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5906             bar      (x) int64 -1 2
5907         >>> ds.map(np.fabs)
5908         <xarray.Dataset>
5909         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5910         Dimensions without coordinates: dim_0, dim_1, x
5911         Data variables:
5912             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5913             bar      (x) float64 1.0 2.0
5914         """
5915         if keep_attrs is None:
5916             keep_attrs = _get_keep_attrs(default=False)
5917         variables = {
5918             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5919             for k, v in self.data_vars.items()
5920         }
5921         if keep_attrs:
5922             for k, v in variables.items():
5923                 v._copy_attrs_from(self.data_vars[k])
5924         attrs = self.attrs if keep_attrs else None
5925         return type(self)(variables, attrs=attrs)
5926 
5927     def apply(
5928         self: T_Dataset,
5929         func: Callable,
5930         keep_attrs: bool | None = None,
5931         args: Iterable[Any] = (),
5932         **kwargs: Any,
5933     ) -> T_Dataset:
5934         """
5935         Backward compatible implementation of ``map``
5936 
5937         See Also
5938         --------
5939         Dataset.map
5940         """
5941         warnings.warn(
5942             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5943             PendingDeprecationWarning,
5944             stacklevel=2,
5945         )
5946         return self.map(func, keep_attrs, args, **kwargs)
5947 
5948     def assign(
5949         self: T_Dataset,
5950         variables: Mapping[Any, Any] | None = None,
5951         **variables_kwargs: Any,
5952     ) -> T_Dataset:
5953         """Assign new data variables to a Dataset, returning a new object
5954         with all the original variables in addition to the new ones.
5955 
5956         Parameters
5957         ----------
5958         variables : mapping of hashable to Any
5959             Mapping from variables names to the new values. If the new values
5960             are callable, they are computed on the Dataset and assigned to new
5961             data variables. If the values are not callable, (e.g. a DataArray,
5962             scalar, or array), they are simply assigned.
5963         **variables_kwargs
5964             The keyword arguments form of ``variables``.
5965             One of variables or variables_kwargs must be provided.
5966 
5967         Returns
5968         -------
5969         ds : Dataset
5970             A new Dataset with the new variables in addition to all the
5971             existing variables.
5972 
5973         Notes
5974         -----
5975         Since ``kwargs`` is a dictionary, the order of your arguments may not
5976         be preserved, and so the order of the new variables is not well
5977         defined. Assigning multiple variables within the same ``assign`` is
5978         possible, but you cannot reference other variables created within the
5979         same ``assign`` call.
5980 
5981         See Also
5982         --------
5983         pandas.DataFrame.assign
5984 
5985         Examples
5986         --------
5987         >>> x = xr.Dataset(
5988         ...     {
5989         ...         "temperature_c": (
5990         ...             ("lat", "lon"),
5991         ...             20 * np.random.rand(4).reshape(2, 2),
5992         ...         ),
5993         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5994         ...     },
5995         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5996         ... )
5997         >>> x
5998         <xarray.Dataset>
5999         Dimensions:        (lat: 2, lon: 2)
6000         Coordinates:
6001           * lat            (lat) int64 10 20
6002           * lon            (lon) int64 150 160
6003         Data variables:
6004             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6005             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6006 
6007         Where the value is a callable, evaluated on dataset:
6008 
6009         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6010         <xarray.Dataset>
6011         Dimensions:        (lat: 2, lon: 2)
6012         Coordinates:
6013           * lat            (lat) int64 10 20
6014           * lon            (lon) int64 150 160
6015         Data variables:
6016             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6017             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6018             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6019 
6020         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6021 
6022         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6023         <xarray.Dataset>
6024         Dimensions:        (lat: 2, lon: 2)
6025         Coordinates:
6026           * lat            (lat) int64 10 20
6027           * lon            (lon) int64 150 160
6028         Data variables:
6029             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6030             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6031             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6032 
6033         """
6034         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6035         data = self.copy()
6036         # do all calculations first...
6037         results: CoercibleMapping = data._calc_assign_results(variables)
6038         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6039         # ... and then assign
6040         data.update(results)
6041         return data
6042 
6043     def to_array(
6044         self, dim: Hashable = "variable", name: Hashable | None = None
6045     ) -> DataArray:
6046         """Convert this dataset into an xarray.DataArray
6047 
6048         The data variables of this dataset will be broadcast against each other
6049         and stacked along the first axis of the new array. All coordinates of
6050         this dataset will remain coordinates.
6051 
6052         Parameters
6053         ----------
6054         dim : Hashable, default: "variable"
6055             Name of the new dimension.
6056         name : Hashable or None, optional
6057             Name of the new data array.
6058 
6059         Returns
6060         -------
6061         array : xarray.DataArray
6062         """
6063         from .dataarray import DataArray
6064 
6065         data_vars = [self.variables[k] for k in self.data_vars]
6066         broadcast_vars = broadcast_variables(*data_vars)
6067         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6068 
6069         dims = (dim,) + broadcast_vars[0].dims
6070         variable = Variable(dims, data, self.attrs, fastpath=True)
6071 
6072         coords = {k: v.variable for k, v in self.coords.items()}
6073         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6074         new_dim_index = PandasIndex(list(self.data_vars), dim)
6075         indexes[dim] = new_dim_index
6076         coords.update(new_dim_index.create_variables())
6077 
6078         return DataArray._construct_direct(variable, coords, name, indexes)
6079 
6080     def _normalize_dim_order(
6081         self, dim_order: Sequence[Hashable] | None = None
6082     ) -> dict[Hashable, int]:
6083         """
6084         Check the validity of the provided dimensions if any and return the mapping
6085         between dimension name and their size.
6086 
6087         Parameters
6088         ----------
6089         dim_order: Sequence of Hashable or None, optional
6090             Dimension order to validate (default to the alphabetical order if None).
6091 
6092         Returns
6093         -------
6094         result : dict[Hashable, int]
6095             Validated dimensions mapping.
6096 
6097         """
6098         if dim_order is None:
6099             dim_order = list(self.dims)
6100         elif set(dim_order) != set(self.dims):
6101             raise ValueError(
6102                 "dim_order {} does not match the set of dimensions of this "
6103                 "Dataset: {}".format(dim_order, list(self.dims))
6104             )
6105 
6106         ordered_dims = {k: self.dims[k] for k in dim_order}
6107 
6108         return ordered_dims
6109 
6110     def to_pandas(self) -> pd.Series | pd.DataFrame:
6111         """Convert this dataset into a pandas object without changing the number of dimensions.
6112 
6113         The type of the returned object depends on the number of Dataset
6114         dimensions:
6115 
6116         * 0D -> `pandas.Series`
6117         * 1D -> `pandas.DataFrame`
6118 
6119         Only works for Datasets with 1 or fewer dimensions.
6120         """
6121         if len(self.dims) == 0:
6122             return pd.Series({k: v.item() for k, v in self.items()})
6123         if len(self.dims) == 1:
6124             return self.to_dataframe()
6125         raise ValueError(
6126             "cannot convert Datasets with %s dimensions into "
6127             "pandas objects without changing the number of dimensions. "
6128             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6129         )
6130 
6131     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6132         columns = [k for k in self.variables if k not in self.dims]
6133         data = [
6134             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6135             for k in columns
6136         ]
6137         index = self.coords.to_index([*ordered_dims])
6138         return pd.DataFrame(dict(zip(columns, data)), index=index)
6139 
6140     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6141         """Convert this dataset into a pandas.DataFrame.
6142 
6143         Non-index variables in this dataset form the columns of the
6144         DataFrame. The DataFrame is indexed by the Cartesian product of
6145         this dataset's indices.
6146 
6147         Parameters
6148         ----------
6149         dim_order: Sequence of Hashable or None, optional
6150             Hierarchical dimension order for the resulting dataframe. All
6151             arrays are transposed to this order and then written out as flat
6152             vectors in contiguous order, so the last dimension in this list
6153             will be contiguous in the resulting DataFrame. This has a major
6154             influence on which operations are efficient on the resulting
6155             dataframe.
6156 
6157             If provided, must include all dimensions of this dataset. By
6158             default, dimensions are sorted alphabetically.
6159 
6160         Returns
6161         -------
6162         result : DataFrame
6163             Dataset as a pandas DataFrame.
6164 
6165         """
6166 
6167         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6168 
6169         return self._to_dataframe(ordered_dims=ordered_dims)
6170 
6171     def _set_sparse_data_from_dataframe(
6172         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6173     ) -> None:
6174         from sparse import COO
6175 
6176         if isinstance(idx, pd.MultiIndex):
6177             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6178             is_sorted = idx.is_monotonic_increasing
6179             shape = tuple(lev.size for lev in idx.levels)
6180         else:
6181             coords = np.arange(idx.size).reshape(1, -1)
6182             is_sorted = True
6183             shape = (idx.size,)
6184 
6185         for name, values in arrays:
6186             # In virtually all real use cases, the sparse array will now have
6187             # missing values and needs a fill_value. For consistency, don't
6188             # special case the rare exceptions (e.g., dtype=int without a
6189             # MultiIndex).
6190             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6191             values = np.asarray(values, dtype=dtype)
6192 
6193             data = COO(
6194                 coords,
6195                 values,
6196                 shape,
6197                 has_duplicates=False,
6198                 sorted=is_sorted,
6199                 fill_value=fill_value,
6200             )
6201             self[name] = (dims, data)
6202 
6203     def _set_numpy_data_from_dataframe(
6204         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6205     ) -> None:
6206         if not isinstance(idx, pd.MultiIndex):
6207             for name, values in arrays:
6208                 self[name] = (dims, values)
6209             return
6210 
6211         # NB: similar, more general logic, now exists in
6212         # variable.unstack_once; we could consider combining them at some
6213         # point.
6214 
6215         shape = tuple(lev.size for lev in idx.levels)
6216         indexer = tuple(idx.codes)
6217 
6218         # We already verified that the MultiIndex has all unique values, so
6219         # there are missing values if and only if the size of output arrays is
6220         # larger that the index.
6221         missing_values = math.prod(shape) > idx.shape[0]
6222 
6223         for name, values in arrays:
6224             # NumPy indexing is much faster than using DataFrame.reindex() to
6225             # fill in missing values:
6226             # https://stackoverflow.com/a/35049899/809705
6227             if missing_values:
6228                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6229                 data = np.full(shape, fill_value, dtype)
6230             else:
6231                 # If there are no missing values, keep the existing dtype
6232                 # instead of promoting to support NA, e.g., keep integer
6233                 # columns as integers.
6234                 # TODO: consider removing this special case, which doesn't
6235                 # exist for sparse=True.
6236                 data = np.zeros(shape, values.dtype)
6237             data[indexer] = values
6238             self[name] = (dims, data)
6239 
6240     @classmethod
6241     def from_dataframe(
6242         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6243     ) -> T_Dataset:
6244         """Convert a pandas.DataFrame into an xarray.Dataset
6245 
6246         Each column will be converted into an independent variable in the
6247         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6248         into a tensor product of one-dimensional indices (filling in missing
6249         values with NaN). This method will produce a Dataset very similar to
6250         that on which the 'to_dataframe' method was called, except with
6251         possibly redundant dimensions (since all dataset variables will have
6252         the same dimensionality)
6253 
6254         Parameters
6255         ----------
6256         dataframe : DataFrame
6257             DataFrame from which to copy data and indices.
6258         sparse : bool, default: False
6259             If true, create a sparse arrays instead of dense numpy arrays. This
6260             can potentially save a large amount of memory if the DataFrame has
6261             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6262 
6263         Returns
6264         -------
6265         New Dataset.
6266 
6267         See Also
6268         --------
6269         xarray.DataArray.from_series
6270         pandas.DataFrame.to_xarray
6271         """
6272         # TODO: Add an option to remove dimensions along which the variables
6273         # are constant, to enable consistent serialization to/from a dataframe,
6274         # even if some variables have different dimensionality.
6275 
6276         if not dataframe.columns.is_unique:
6277             raise ValueError("cannot convert DataFrame with non-unique columns")
6278 
6279         idx = remove_unused_levels_categories(dataframe.index)
6280 
6281         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6282             raise ValueError(
6283                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6284             )
6285 
6286         # Cast to a NumPy array first, in case the Series is a pandas Extension
6287         # array (which doesn't have a valid NumPy dtype)
6288         # TODO: allow users to control how this casting happens, e.g., by
6289         # forwarding arguments to pandas.Series.to_numpy?
6290         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6291 
6292         indexes: dict[Hashable, Index] = {}
6293         index_vars: dict[Hashable, Variable] = {}
6294 
6295         if isinstance(idx, pd.MultiIndex):
6296             dims = tuple(
6297                 name if name is not None else "level_%i" % n
6298                 for n, name in enumerate(idx.names)
6299             )
6300             for dim, lev in zip(dims, idx.levels):
6301                 xr_idx = PandasIndex(lev, dim)
6302                 indexes[dim] = xr_idx
6303                 index_vars.update(xr_idx.create_variables())
6304         else:
6305             index_name = idx.name if idx.name is not None else "index"
6306             dims = (index_name,)
6307             xr_idx = PandasIndex(idx, index_name)
6308             indexes[index_name] = xr_idx
6309             index_vars.update(xr_idx.create_variables())
6310 
6311         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6312 
6313         if sparse:
6314             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6315         else:
6316             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6317         return obj
6318 
6319     def to_dask_dataframe(
6320         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6321     ) -> DaskDataFrame:
6322         """
6323         Convert this dataset into a dask.dataframe.DataFrame.
6324 
6325         The dimensions, coordinates and data variables in this dataset form
6326         the columns of the DataFrame.
6327 
6328         Parameters
6329         ----------
6330         dim_order : list, optional
6331             Hierarchical dimension order for the resulting dataframe. All
6332             arrays are transposed to this order and then written out as flat
6333             vectors in contiguous order, so the last dimension in this list
6334             will be contiguous in the resulting DataFrame. This has a major
6335             influence on which operations are efficient on the resulting dask
6336             dataframe.
6337 
6338             If provided, must include all dimensions of this dataset. By
6339             default, dimensions are sorted alphabetically.
6340         set_index : bool, default: False
6341             If set_index=True, the dask DataFrame is indexed by this dataset's
6342             coordinate. Since dask DataFrames do not support multi-indexes,
6343             set_index only works if the dataset only contains one dimension.
6344 
6345         Returns
6346         -------
6347         dask.dataframe.DataFrame
6348         """
6349 
6350         import dask.array as da
6351         import dask.dataframe as dd
6352 
6353         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6354 
6355         columns = list(ordered_dims)
6356         columns.extend(k for k in self.coords if k not in self.dims)
6357         columns.extend(self.data_vars)
6358 
6359         series_list = []
6360         for name in columns:
6361             try:
6362                 var = self.variables[name]
6363             except KeyError:
6364                 # dimension without a matching coordinate
6365                 size = self.dims[name]
6366                 data = da.arange(size, chunks=size, dtype=np.int64)
6367                 var = Variable((name,), data)
6368 
6369             # IndexVariable objects have a dummy .chunk() method
6370             if isinstance(var, IndexVariable):
6371                 var = var.to_base_variable()
6372 
6373             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6374             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6375             series_list.append(series)
6376 
6377         df = dd.concat(series_list, axis=1)
6378 
6379         if set_index:
6380             dim_order = [*ordered_dims]
6381 
6382             if len(dim_order) == 1:
6383                 (dim,) = dim_order
6384                 df = df.set_index(dim)
6385             else:
6386                 # triggers an error about multi-indexes, even if only one
6387                 # dimension is passed
6388                 df = df.set_index(dim_order)
6389 
6390         return df
6391 
6392     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6393         """
6394         Convert this dataset to a dictionary following xarray naming
6395         conventions.
6396 
6397         Converts all variables and attributes to native Python objects
6398         Useful for converting to json. To avoid datetime incompatibility
6399         use decode_times=False kwarg in xarrray.open_dataset.
6400 
6401         Parameters
6402         ----------
6403         data : bool, default: True
6404             Whether to include the actual data in the dictionary. When set to
6405             False, returns just the schema.
6406         encoding : bool, default: False
6407             Whether to include the Dataset's encoding in the dictionary.
6408 
6409         Returns
6410         -------
6411         d : dict
6412             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6413             "encoding".
6414 
6415         See Also
6416         --------
6417         Dataset.from_dict
6418         DataArray.to_dict
6419         """
6420         d: dict = {
6421             "coords": {},
6422             "attrs": decode_numpy_dict_values(self.attrs),
6423             "dims": dict(self.dims),
6424             "data_vars": {},
6425         }
6426         for k in self.coords:
6427             d["coords"].update(
6428                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6429             )
6430         for k in self.data_vars:
6431             d["data_vars"].update(
6432                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6433             )
6434         if encoding:
6435             d["encoding"] = dict(self.encoding)
6436         return d
6437 
6438     @classmethod
6439     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6440         """Convert a dictionary into an xarray.Dataset.
6441 
6442         Parameters
6443         ----------
6444         d : dict-like
6445             Mapping with a minimum structure of
6446                 ``{"var_0": {"dims": [..], "data": [..]}, \
6447                             ...}``
6448 
6449         Returns
6450         -------
6451         obj : Dataset
6452 
6453         See also
6454         --------
6455         Dataset.to_dict
6456         DataArray.from_dict
6457 
6458         Examples
6459         --------
6460         >>> d = {
6461         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6462         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6463         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6464         ... }
6465         >>> ds = xr.Dataset.from_dict(d)
6466         >>> ds
6467         <xarray.Dataset>
6468         Dimensions:  (t: 3)
6469         Coordinates:
6470           * t        (t) int64 0 1 2
6471         Data variables:
6472             a        (t) <U1 'a' 'b' 'c'
6473             b        (t) int64 10 20 30
6474 
6475         >>> d = {
6476         ...     "coords": {
6477         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6478         ...     },
6479         ...     "attrs": {"title": "air temperature"},
6480         ...     "dims": "t",
6481         ...     "data_vars": {
6482         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6483         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6484         ...     },
6485         ... }
6486         >>> ds = xr.Dataset.from_dict(d)
6487         >>> ds
6488         <xarray.Dataset>
6489         Dimensions:  (t: 3)
6490         Coordinates:
6491           * t        (t) int64 0 1 2
6492         Data variables:
6493             a        (t) int64 10 20 30
6494             b        (t) <U1 'a' 'b' 'c'
6495         Attributes:
6496             title:    air temperature
6497 
6498         """
6499 
6500         variables: Iterable[tuple[Hashable, Any]]
6501         if not {"coords", "data_vars"}.issubset(set(d)):
6502             variables = d.items()
6503         else:
6504             import itertools
6505 
6506             variables = itertools.chain(
6507                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6508             )
6509         try:
6510             variable_dict = {
6511                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6512             }
6513         except KeyError as e:
6514             raise ValueError(
6515                 "cannot convert dict without the key "
6516                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6517             )
6518         obj = cls(variable_dict)
6519 
6520         # what if coords aren't dims?
6521         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6522         obj = obj.set_coords(coords)
6523 
6524         obj.attrs.update(d.get("attrs", {}))
6525         obj.encoding.update(d.get("encoding", {}))
6526 
6527         return obj
6528 
6529     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6530         variables = {}
6531         keep_attrs = kwargs.pop("keep_attrs", None)
6532         if keep_attrs is None:
6533             keep_attrs = _get_keep_attrs(default=True)
6534         for k, v in self._variables.items():
6535             if k in self._coord_names:
6536                 variables[k] = v
6537             else:
6538                 variables[k] = f(v, *args, **kwargs)
6539                 if keep_attrs:
6540                     variables[k].attrs = v._attrs
6541         attrs = self._attrs if keep_attrs else None
6542         return self._replace_with_new_dims(variables, attrs=attrs)
6543 
6544     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6545         from .dataarray import DataArray
6546         from .groupby import GroupBy
6547 
6548         if isinstance(other, GroupBy):
6549             return NotImplemented
6550         align_type = OPTIONS["arithmetic_join"] if join is None else join
6551         if isinstance(other, (DataArray, Dataset)):
6552             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6553         g = f if not reflexive else lambda x, y: f(y, x)
6554         ds = self._calculate_binary_op(g, other, join=align_type)
6555         return ds
6556 
6557     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6558         from .dataarray import DataArray
6559         from .groupby import GroupBy
6560 
6561         if isinstance(other, GroupBy):
6562             raise TypeError(
6563                 "in-place operations between a Dataset and "
6564                 "a grouped object are not permitted"
6565             )
6566         # we don't actually modify arrays in-place with in-place Dataset
6567         # arithmetic -- this lets us automatically align things
6568         if isinstance(other, (DataArray, Dataset)):
6569             other = other.reindex_like(self, copy=False)
6570         g = ops.inplace_to_noninplace_op(f)
6571         ds = self._calculate_binary_op(g, other, inplace=True)
6572         self._replace_with_new_dims(
6573             ds._variables,
6574             ds._coord_names,
6575             attrs=ds._attrs,
6576             indexes=ds._indexes,
6577             inplace=True,
6578         )
6579         return self
6580 
6581     def _calculate_binary_op(
6582         self, f, other, join="inner", inplace: bool = False
6583     ) -> Dataset:
6584         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6585             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6586                 raise ValueError(
6587                     "datasets must have the same data variables "
6588                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6589                 )
6590 
6591             dest_vars = {}
6592 
6593             for k in lhs_data_vars:
6594                 if k in rhs_data_vars:
6595                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6596                 elif join in ["left", "outer"]:
6597                     dest_vars[k] = f(lhs_vars[k], np.nan)
6598             for k in rhs_data_vars:
6599                 if k not in dest_vars and join in ["right", "outer"]:
6600                     dest_vars[k] = f(rhs_vars[k], np.nan)
6601             return dest_vars
6602 
6603         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6604             # can't use our shortcut of doing the binary operation with
6605             # Variable objects, so apply over our data vars instead.
6606             new_data_vars = apply_over_both(
6607                 self.data_vars, other, self.data_vars, other
6608             )
6609             return type(self)(new_data_vars)
6610 
6611         other_coords: Coordinates | None = getattr(other, "coords", None)
6612         ds = self.coords.merge(other_coords)
6613 
6614         if isinstance(other, Dataset):
6615             new_vars = apply_over_both(
6616                 self.data_vars, other.data_vars, self.variables, other.variables
6617             )
6618         else:
6619             other_variable = getattr(other, "variable", other)
6620             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6621         ds._variables.update(new_vars)
6622         ds._dims = calculate_dimensions(ds._variables)
6623         return ds
6624 
6625     def _copy_attrs_from(self, other):
6626         self.attrs = other.attrs
6627         for v in other.variables:
6628             if v in self.variables:
6629                 self.variables[v].attrs = other.variables[v].attrs
6630 
6631     def diff(
6632         self: T_Dataset,
6633         dim: Hashable,
6634         n: int = 1,
6635         label: Literal["upper", "lower"] = "upper",
6636     ) -> T_Dataset:
6637         """Calculate the n-th order discrete difference along given axis.
6638 
6639         Parameters
6640         ----------
6641         dim : Hashable
6642             Dimension over which to calculate the finite difference.
6643         n : int, default: 1
6644             The number of times values are differenced.
6645         label : {"upper", "lower"}, default: "upper"
6646             The new coordinate in dimension ``dim`` will have the
6647             values of either the minuend's or subtrahend's coordinate
6648             for values 'upper' and 'lower', respectively.
6649 
6650         Returns
6651         -------
6652         difference : Dataset
6653             The n-th order finite difference of this object.
6654 
6655         Notes
6656         -----
6657         `n` matches numpy's behavior and is different from pandas' first argument named
6658         `periods`.
6659 
6660         Examples
6661         --------
6662         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6663         >>> ds.diff("x")
6664         <xarray.Dataset>
6665         Dimensions:  (x: 3)
6666         Dimensions without coordinates: x
6667         Data variables:
6668             foo      (x) int64 0 1 0
6669         >>> ds.diff("x", 2)
6670         <xarray.Dataset>
6671         Dimensions:  (x: 2)
6672         Dimensions without coordinates: x
6673         Data variables:
6674             foo      (x) int64 1 -1
6675 
6676         See Also
6677         --------
6678         Dataset.differentiate
6679         """
6680         if n == 0:
6681             return self
6682         if n < 0:
6683             raise ValueError(f"order `n` must be non-negative but got {n}")
6684 
6685         # prepare slices
6686         slice_start = {dim: slice(None, -1)}
6687         slice_end = {dim: slice(1, None)}
6688 
6689         # prepare new coordinate
6690         if label == "upper":
6691             slice_new = slice_end
6692         elif label == "lower":
6693             slice_new = slice_start
6694         else:
6695             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6696 
6697         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6698         variables = {}
6699 
6700         for name, var in self.variables.items():
6701             if name in index_vars:
6702                 variables[name] = index_vars[name]
6703             elif dim in var.dims:
6704                 if name in self.data_vars:
6705                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6706                 else:
6707                     variables[name] = var.isel(slice_new)
6708             else:
6709                 variables[name] = var
6710 
6711         difference = self._replace_with_new_dims(variables, indexes=indexes)
6712 
6713         if n > 1:
6714             return difference.diff(dim, n - 1)
6715         else:
6716             return difference
6717 
6718     def shift(
6719         self: T_Dataset,
6720         shifts: Mapping[Any, int] | None = None,
6721         fill_value: Any = xrdtypes.NA,
6722         **shifts_kwargs: int,
6723     ) -> T_Dataset:
6724 
6725         """Shift this dataset by an offset along one or more dimensions.
6726 
6727         Only data variables are moved; coordinates stay in place. This is
6728         consistent with the behavior of ``shift`` in pandas.
6729 
6730         Values shifted from beyond array bounds will appear at one end of
6731         each dimension, which are filled according to `fill_value`. For periodic
6732         offsets instead see `roll`.
6733 
6734         Parameters
6735         ----------
6736         shifts : mapping of hashable to int
6737             Integer offset to shift along each of the given dimensions.
6738             Positive offsets shift to the right; negative offsets shift to the
6739             left.
6740         fill_value : scalar or dict-like, optional
6741             Value to use for newly missing values. If a dict-like, maps
6742             variable names (including coordinates) to fill values.
6743         **shifts_kwargs
6744             The keyword arguments form of ``shifts``.
6745             One of shifts or shifts_kwargs must be provided.
6746 
6747         Returns
6748         -------
6749         shifted : Dataset
6750             Dataset with the same coordinates and attributes but shifted data
6751             variables.
6752 
6753         See Also
6754         --------
6755         roll
6756 
6757         Examples
6758         --------
6759         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6760         >>> ds.shift(x=2)
6761         <xarray.Dataset>
6762         Dimensions:  (x: 5)
6763         Dimensions without coordinates: x
6764         Data variables:
6765             foo      (x) object nan nan 'a' 'b' 'c'
6766         """
6767         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6768         invalid = [k for k in shifts if k not in self.dims]
6769         if invalid:
6770             raise ValueError(f"dimensions {invalid!r} do not exist")
6771 
6772         variables = {}
6773         for name, var in self.variables.items():
6774             if name in self.data_vars:
6775                 fill_value_ = (
6776                     fill_value.get(name, xrdtypes.NA)
6777                     if isinstance(fill_value, dict)
6778                     else fill_value
6779                 )
6780 
6781                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6782                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6783             else:
6784                 variables[name] = var
6785 
6786         return self._replace(variables)
6787 
6788     def roll(
6789         self: T_Dataset,
6790         shifts: Mapping[Any, int] | None = None,
6791         roll_coords: bool = False,
6792         **shifts_kwargs: int,
6793     ) -> T_Dataset:
6794         """Roll this dataset by an offset along one or more dimensions.
6795 
6796         Unlike shift, roll treats the given dimensions as periodic, so will not
6797         create any missing values to be filled.
6798 
6799         Also unlike shift, roll may rotate all variables, including coordinates
6800         if specified. The direction of rotation is consistent with
6801         :py:func:`numpy.roll`.
6802 
6803         Parameters
6804         ----------
6805         shifts : mapping of hashable to int, optional
6806             A dict with keys matching dimensions and values given
6807             by integers to rotate each of the given dimensions. Positive
6808             offsets roll to the right; negative offsets roll to the left.
6809         roll_coords : bool, default: False
6810             Indicates whether to roll the coordinates by the offset too.
6811         **shifts_kwargs : {dim: offset, ...}, optional
6812             The keyword arguments form of ``shifts``.
6813             One of shifts or shifts_kwargs must be provided.
6814 
6815         Returns
6816         -------
6817         rolled : Dataset
6818             Dataset with the same attributes but rolled data and coordinates.
6819 
6820         See Also
6821         --------
6822         shift
6823 
6824         Examples
6825         --------
6826         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6827         >>> ds.roll(x=2)
6828         <xarray.Dataset>
6829         Dimensions:  (x: 5)
6830         Coordinates:
6831           * x        (x) int64 0 1 2 3 4
6832         Data variables:
6833             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6834 
6835         >>> ds.roll(x=2, roll_coords=True)
6836         <xarray.Dataset>
6837         Dimensions:  (x: 5)
6838         Coordinates:
6839           * x        (x) int64 3 4 0 1 2
6840         Data variables:
6841             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6842 
6843         """
6844         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6845         invalid = [k for k in shifts if k not in self.dims]
6846         if invalid:
6847             raise ValueError(f"dimensions {invalid!r} do not exist")
6848 
6849         unrolled_vars: tuple[Hashable, ...]
6850 
6851         if roll_coords:
6852             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6853             unrolled_vars = ()
6854         else:
6855             indexes = dict(self._indexes)
6856             index_vars = dict(self.xindexes.variables)
6857             unrolled_vars = tuple(self.coords)
6858 
6859         variables = {}
6860         for k, var in self.variables.items():
6861             if k in index_vars:
6862                 variables[k] = index_vars[k]
6863             elif k not in unrolled_vars:
6864                 variables[k] = var.roll(
6865                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6866                 )
6867             else:
6868                 variables[k] = var
6869 
6870         return self._replace(variables, indexes=indexes)
6871 
6872     def sortby(
6873         self: T_Dataset,
6874         variables: Hashable | DataArray | list[Hashable | DataArray],
6875         ascending: bool = True,
6876     ) -> T_Dataset:
6877         """
6878         Sort object by labels or values (along an axis).
6879 
6880         Sorts the dataset, either along specified dimensions,
6881         or according to values of 1-D dataarrays that share dimension
6882         with calling object.
6883 
6884         If the input variables are dataarrays, then the dataarrays are aligned
6885         (via left-join) to the calling object prior to sorting by cell values.
6886         NaNs are sorted to the end, following Numpy convention.
6887 
6888         If multiple sorts along the same dimension is
6889         given, numpy's lexsort is performed along that dimension:
6890         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6891         and the FIRST key in the sequence is used as the primary sort key,
6892         followed by the 2nd key, etc.
6893 
6894         Parameters
6895         ----------
6896         variables : Hashable, DataArray, or list of hashable or DataArray
6897             1D DataArray objects or name(s) of 1D variable(s) in
6898             coords/data_vars whose values are used to sort the dataset.
6899         ascending : bool, default: True
6900             Whether to sort by ascending or descending order.
6901 
6902         Returns
6903         -------
6904         sorted : Dataset
6905             A new dataset where all the specified dims are sorted by dim
6906             labels.
6907 
6908         See Also
6909         --------
6910         DataArray.sortby
6911         numpy.sort
6912         pandas.sort_values
6913         pandas.sort_index
6914 
6915         Examples
6916         --------
6917         >>> ds = xr.Dataset(
6918         ...     {
6919         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6920         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6921         ...     },
6922         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6923         ... )
6924         >>> ds.sortby("x")
6925         <xarray.Dataset>
6926         Dimensions:  (x: 2, y: 2)
6927         Coordinates:
6928           * x        (x) <U1 'a' 'b'
6929           * y        (y) int64 1 0
6930         Data variables:
6931             A        (x, y) int64 3 4 1 2
6932             B        (x, y) int64 7 8 5 6
6933         """
6934         from .dataarray import DataArray
6935 
6936         if not isinstance(variables, list):
6937             variables = [variables]
6938         else:
6939             variables = variables
6940         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6941         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6942         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6943         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6944         vars_by_dim = defaultdict(list)
6945         for data_array in aligned_other_vars:
6946             if data_array.ndim != 1:
6947                 raise ValueError("Input DataArray is not 1-D.")
6948             (key,) = data_array.dims
6949             vars_by_dim[key].append(data_array)
6950 
6951         indices = {}
6952         for key, arrays in vars_by_dim.items():
6953             order = np.lexsort(tuple(reversed(arrays)))
6954             indices[key] = order if ascending else order[::-1]
6955         return aligned_self.isel(indices)
6956 
6957     def quantile(
6958         self: T_Dataset,
6959         q: ArrayLike,
6960         dim: str | Iterable[Hashable] | None = None,
6961         method: QUANTILE_METHODS = "linear",
6962         numeric_only: bool = False,
6963         keep_attrs: bool = None,
6964         skipna: bool = None,
6965         interpolation: QUANTILE_METHODS = None,
6966     ) -> T_Dataset:
6967         """Compute the qth quantile of the data along the specified dimension.
6968 
6969         Returns the qth quantiles(s) of the array elements for each variable
6970         in the Dataset.
6971 
6972         Parameters
6973         ----------
6974         q : float or array-like of float
6975             Quantile to compute, which must be between 0 and 1 inclusive.
6976         dim : str or Iterable of Hashable, optional
6977             Dimension(s) over which to apply quantile.
6978         method : str, default: "linear"
6979             This optional parameter specifies the interpolation method to use when the
6980             desired quantile lies between two data points. The options sorted by their R
6981             type as summarized in the H&F paper [1]_ are:
6982 
6983                 1. "inverted_cdf" (*)
6984                 2. "averaged_inverted_cdf" (*)
6985                 3. "closest_observation" (*)
6986                 4. "interpolated_inverted_cdf" (*)
6987                 5. "hazen" (*)
6988                 6. "weibull" (*)
6989                 7. "linear"  (default)
6990                 8. "median_unbiased" (*)
6991                 9. "normal_unbiased" (*)
6992 
6993             The first three methods are discontiuous.  The following discontinuous
6994             variations of the default "linear" (7.) option are also available:
6995 
6996                 * "lower"
6997                 * "higher"
6998                 * "midpoint"
6999                 * "nearest"
7000 
7001             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7002             was previously called "interpolation", renamed in accordance with numpy
7003             version 1.22.0.
7004 
7005             (*) These methods require numpy version 1.22 or newer.
7006 
7007         keep_attrs : bool, optional
7008             If True, the dataset's attributes (`attrs`) will be copied from
7009             the original object to the new one.  If False (default), the new
7010             object will be returned without attributes.
7011         numeric_only : bool, optional
7012             If True, only apply ``func`` to variables with a numeric dtype.
7013         skipna : bool, optional
7014             If True, skip missing values (as marked by NaN). By default, only
7015             skips missing values for float dtypes; other dtypes either do not
7016             have a sentinel missing value (int) or skipna=True has not been
7017             implemented (object, datetime64 or timedelta64).
7018 
7019         Returns
7020         -------
7021         quantiles : Dataset
7022             If `q` is a single quantile, then the result is a scalar for each
7023             variable in data_vars. If multiple percentiles are given, first
7024             axis of the result corresponds to the quantile and a quantile
7025             dimension is added to the return Dataset. The other dimensions are
7026             the dimensions that remain after the reduction of the array.
7027 
7028         See Also
7029         --------
7030         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7031 
7032         Examples
7033         --------
7034         >>> ds = xr.Dataset(
7035         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7036         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7037         ... )
7038         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7039         <xarray.Dataset>
7040         Dimensions:   ()
7041         Coordinates:
7042             quantile  float64 0.0
7043         Data variables:
7044             a         float64 0.7
7045         >>> ds.quantile(0, dim="x")
7046         <xarray.Dataset>
7047         Dimensions:   (y: 4)
7048         Coordinates:
7049           * y         (y) float64 1.0 1.5 2.0 2.5
7050             quantile  float64 0.0
7051         Data variables:
7052             a         (y) float64 0.7 4.2 2.6 1.5
7053         >>> ds.quantile([0, 0.5, 1])
7054         <xarray.Dataset>
7055         Dimensions:   (quantile: 3)
7056         Coordinates:
7057           * quantile  (quantile) float64 0.0 0.5 1.0
7058         Data variables:
7059             a         (quantile) float64 0.7 3.4 9.4
7060         >>> ds.quantile([0, 0.5, 1], dim="x")
7061         <xarray.Dataset>
7062         Dimensions:   (quantile: 3, y: 4)
7063         Coordinates:
7064           * y         (y) float64 1.0 1.5 2.0 2.5
7065           * quantile  (quantile) float64 0.0 0.5 1.0
7066         Data variables:
7067             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7068 
7069         References
7070         ----------
7071         .. [1] R. J. Hyndman and Y. Fan,
7072            "Sample quantiles in statistical packages,"
7073            The American Statistician, 50(4), pp. 361-365, 1996
7074         """
7075 
7076         # interpolation renamed to method in version 0.21.0
7077         # check here and in variable to avoid repeated warnings
7078         if interpolation is not None:
7079             warnings.warn(
7080                 "The `interpolation` argument to quantile was renamed to `method`.",
7081                 FutureWarning,
7082             )
7083 
7084             if method != "linear":
7085                 raise TypeError("Cannot pass interpolation and method keywords!")
7086 
7087             method = interpolation
7088 
7089         dims: set[Hashable]
7090         if isinstance(dim, str):
7091             dims = {dim}
7092         elif dim is None or dim is ...:
7093             dims = set(self.dims)
7094         else:
7095             dims = set(dim)
7096 
7097         _assert_empty(
7098             tuple(d for d in dims if d not in self.dims),
7099             "Dataset does not contain the dimensions: %s",
7100         )
7101 
7102         q = np.asarray(q, dtype=np.float64)
7103 
7104         variables = {}
7105         for name, var in self.variables.items():
7106             reduce_dims = [d for d in var.dims if d in dims]
7107             if reduce_dims or not var.dims:
7108                 if name not in self.coords:
7109                     if (
7110                         not numeric_only
7111                         or np.issubdtype(var.dtype, np.number)
7112                         or var.dtype == np.bool_
7113                     ):
7114                         variables[name] = var.quantile(
7115                             q,
7116                             dim=reduce_dims,
7117                             method=method,
7118                             keep_attrs=keep_attrs,
7119                             skipna=skipna,
7120                         )
7121 
7122             else:
7123                 variables[name] = var
7124 
7125         # construct the new dataset
7126         coord_names = {k for k in self.coords if k in variables}
7127         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7128         if keep_attrs is None:
7129             keep_attrs = _get_keep_attrs(default=False)
7130         attrs = self.attrs if keep_attrs else None
7131         new = self._replace_with_new_dims(
7132             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7133         )
7134         return new.assign_coords(quantile=q)
7135 
7136     def rank(
7137         self: T_Dataset,
7138         dim: Hashable,
7139         pct: bool = False,
7140         keep_attrs: bool | None = None,
7141     ) -> T_Dataset:
7142         """Ranks the data.
7143 
7144         Equal values are assigned a rank that is the average of the ranks that
7145         would have been otherwise assigned to all of the values within
7146         that set.
7147         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7148 
7149         NaNs in the input array are returned as NaNs.
7150 
7151         The `bottleneck` library is required.
7152 
7153         Parameters
7154         ----------
7155         dim : Hashable
7156             Dimension over which to compute rank.
7157         pct : bool, default: False
7158             If True, compute percentage ranks, otherwise compute integer ranks.
7159         keep_attrs : bool or None, optional
7160             If True, the dataset's attributes (`attrs`) will be copied from
7161             the original object to the new one.  If False, the new
7162             object will be returned without attributes.
7163 
7164         Returns
7165         -------
7166         ranked : Dataset
7167             Variables that do not depend on `dim` are dropped.
7168         """
7169         if not OPTIONS["use_bottleneck"]:
7170             raise RuntimeError(
7171                 "rank requires bottleneck to be enabled."
7172                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7173             )
7174 
7175         if dim not in self.dims:
7176             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7177 
7178         variables = {}
7179         for name, var in self.variables.items():
7180             if name in self.data_vars:
7181                 if dim in var.dims:
7182                     variables[name] = var.rank(dim, pct=pct)
7183             else:
7184                 variables[name] = var
7185 
7186         coord_names = set(self.coords)
7187         if keep_attrs is None:
7188             keep_attrs = _get_keep_attrs(default=False)
7189         attrs = self.attrs if keep_attrs else None
7190         return self._replace(variables, coord_names, attrs=attrs)
7191 
7192     def differentiate(
7193         self: T_Dataset,
7194         coord: Hashable,
7195         edge_order: Literal[1, 2] = 1,
7196         datetime_unit: DatetimeUnitOptions | None = None,
7197     ) -> T_Dataset:
7198         """ Differentiate with the second order accurate central
7199         differences.
7200 
7201         .. note::
7202             This feature is limited to simple cartesian geometry, i.e. coord
7203             must be one dimensional.
7204 
7205         Parameters
7206         ----------
7207         coord : Hashable
7208             The coordinate to be used to compute the gradient.
7209         edge_order : {1, 2}, default: 1
7210             N-th order accurate differences at the boundaries.
7211         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7212             "us", "ns", "ps", "fs", "as", None}, default: None
7213             Unit to compute gradient. Only valid for datetime coordinate.
7214 
7215         Returns
7216         -------
7217         differentiated: Dataset
7218 
7219         See also
7220         --------
7221         numpy.gradient: corresponding numpy function
7222         """
7223         from .variable import Variable
7224 
7225         if coord not in self.variables and coord not in self.dims:
7226             raise ValueError(f"Coordinate {coord} does not exist.")
7227 
7228         coord_var = self[coord].variable
7229         if coord_var.ndim != 1:
7230             raise ValueError(
7231                 "Coordinate {} must be 1 dimensional but is {}"
7232                 " dimensional".format(coord, coord_var.ndim)
7233             )
7234 
7235         dim = coord_var.dims[0]
7236         if _contains_datetime_like_objects(coord_var):
7237             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7238                 datetime_unit = cast(
7239                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7240                 )
7241             elif datetime_unit is None:
7242                 datetime_unit = "s"  # Default to seconds for cftime objects
7243             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7244 
7245         variables = {}
7246         for k, v in self.variables.items():
7247             if k in self.data_vars and dim in v.dims and k not in self.coords:
7248                 if _contains_datetime_like_objects(v):
7249                     v = v._to_numeric(datetime_unit=datetime_unit)
7250                 grad = duck_array_ops.gradient(
7251                     v.data,
7252                     coord_var.data,
7253                     edge_order=edge_order,
7254                     axis=v.get_axis_num(dim),
7255                 )
7256                 variables[k] = Variable(v.dims, grad)
7257             else:
7258                 variables[k] = v
7259         return self._replace(variables)
7260 
7261     def integrate(
7262         self: T_Dataset,
7263         coord: Hashable | Sequence[Hashable],
7264         datetime_unit: DatetimeUnitOptions = None,
7265     ) -> T_Dataset:
7266         """Integrate along the given coordinate using the trapezoidal rule.
7267 
7268         .. note::
7269             This feature is limited to simple cartesian geometry, i.e. coord
7270             must be one dimensional.
7271 
7272         Parameters
7273         ----------
7274         coord : hashable, or sequence of hashable
7275             Coordinate(s) used for the integration.
7276         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7277                         'ps', 'fs', 'as', None}, optional
7278             Specify the unit if datetime coordinate is used.
7279 
7280         Returns
7281         -------
7282         integrated : Dataset
7283 
7284         See also
7285         --------
7286         DataArray.integrate
7287         numpy.trapz : corresponding numpy function
7288 
7289         Examples
7290         --------
7291         >>> ds = xr.Dataset(
7292         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7293         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7294         ... )
7295         >>> ds
7296         <xarray.Dataset>
7297         Dimensions:  (x: 4)
7298         Coordinates:
7299           * x        (x) int64 0 1 2 3
7300             y        (x) int64 1 7 3 5
7301         Data variables:
7302             a        (x) int64 5 5 6 6
7303             b        (x) int64 1 2 1 0
7304         >>> ds.integrate("x")
7305         <xarray.Dataset>
7306         Dimensions:  ()
7307         Data variables:
7308             a        float64 16.5
7309             b        float64 3.5
7310         >>> ds.integrate("y")
7311         <xarray.Dataset>
7312         Dimensions:  ()
7313         Data variables:
7314             a        float64 20.0
7315             b        float64 4.0
7316         """
7317         if not isinstance(coord, (list, tuple)):
7318             coord = (coord,)
7319         result = self
7320         for c in coord:
7321             result = result._integrate_one(c, datetime_unit=datetime_unit)
7322         return result
7323 
7324     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7325         from .variable import Variable
7326 
7327         if coord not in self.variables and coord not in self.dims:
7328             raise ValueError(f"Coordinate {coord} does not exist.")
7329 
7330         coord_var = self[coord].variable
7331         if coord_var.ndim != 1:
7332             raise ValueError(
7333                 "Coordinate {} must be 1 dimensional but is {}"
7334                 " dimensional".format(coord, coord_var.ndim)
7335             )
7336 
7337         dim = coord_var.dims[0]
7338         if _contains_datetime_like_objects(coord_var):
7339             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7340                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7341             elif datetime_unit is None:
7342                 datetime_unit = "s"  # Default to seconds for cftime objects
7343             coord_var = coord_var._replace(
7344                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7345             )
7346 
7347         variables = {}
7348         coord_names = set()
7349         for k, v in self.variables.items():
7350             if k in self.coords:
7351                 if dim not in v.dims or cumulative:
7352                     variables[k] = v
7353                     coord_names.add(k)
7354             else:
7355                 if k in self.data_vars and dim in v.dims:
7356                     if _contains_datetime_like_objects(v):
7357                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7358                     if cumulative:
7359                         integ = duck_array_ops.cumulative_trapezoid(
7360                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7361                         )
7362                         v_dims = v.dims
7363                     else:
7364                         integ = duck_array_ops.trapz(
7365                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7366                         )
7367                         v_dims = list(v.dims)
7368                         v_dims.remove(dim)
7369                     variables[k] = Variable(v_dims, integ)
7370                 else:
7371                     variables[k] = v
7372         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7373         return self._replace_with_new_dims(
7374             variables, coord_names=coord_names, indexes=indexes
7375         )
7376 
7377     def cumulative_integrate(
7378         self: T_Dataset,
7379         coord: Hashable | Sequence[Hashable],
7380         datetime_unit: DatetimeUnitOptions = None,
7381     ) -> T_Dataset:
7382         """Integrate along the given coordinate using the trapezoidal rule.
7383 
7384         .. note::
7385             This feature is limited to simple cartesian geometry, i.e. coord
7386             must be one dimensional.
7387 
7388             The first entry of the cumulative integral of each variable is always 0, in
7389             order to keep the length of the dimension unchanged between input and
7390             output.
7391 
7392         Parameters
7393         ----------
7394         coord : hashable, or sequence of hashable
7395             Coordinate(s) used for the integration.
7396         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7397                         'ps', 'fs', 'as', None}, optional
7398             Specify the unit if datetime coordinate is used.
7399 
7400         Returns
7401         -------
7402         integrated : Dataset
7403 
7404         See also
7405         --------
7406         DataArray.cumulative_integrate
7407         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7408 
7409         Examples
7410         --------
7411         >>> ds = xr.Dataset(
7412         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7413         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7414         ... )
7415         >>> ds
7416         <xarray.Dataset>
7417         Dimensions:  (x: 4)
7418         Coordinates:
7419           * x        (x) int64 0 1 2 3
7420             y        (x) int64 1 7 3 5
7421         Data variables:
7422             a        (x) int64 5 5 6 6
7423             b        (x) int64 1 2 1 0
7424         >>> ds.cumulative_integrate("x")
7425         <xarray.Dataset>
7426         Dimensions:  (x: 4)
7427         Coordinates:
7428           * x        (x) int64 0 1 2 3
7429             y        (x) int64 1 7 3 5
7430         Data variables:
7431             a        (x) float64 0.0 5.0 10.5 16.5
7432             b        (x) float64 0.0 1.5 3.0 3.5
7433         >>> ds.cumulative_integrate("y")
7434         <xarray.Dataset>
7435         Dimensions:  (x: 4)
7436         Coordinates:
7437           * x        (x) int64 0 1 2 3
7438             y        (x) int64 1 7 3 5
7439         Data variables:
7440             a        (x) float64 0.0 30.0 8.0 20.0
7441             b        (x) float64 0.0 9.0 3.0 4.0
7442         """
7443         if not isinstance(coord, (list, tuple)):
7444             coord = (coord,)
7445         result = self
7446         for c in coord:
7447             result = result._integrate_one(
7448                 c, datetime_unit=datetime_unit, cumulative=True
7449             )
7450         return result
7451 
7452     @property
7453     def real(self: T_Dataset) -> T_Dataset:
7454         """
7455         The real part of each data variable.
7456 
7457         See Also
7458         --------
7459         numpy.ndarray.real
7460         """
7461         return self.map(lambda x: x.real, keep_attrs=True)
7462 
7463     @property
7464     def imag(self: T_Dataset) -> T_Dataset:
7465         """
7466         The imaginary part of each data variable.
7467 
7468         See Also
7469         --------
7470         numpy.ndarray.imag
7471         """
7472         return self.map(lambda x: x.imag, keep_attrs=True)
7473 
7474     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7475 
7476     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7477         """Returns a ``Dataset`` with variables that match specific conditions.
7478 
7479         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7480         containing only the variables for which all the filter tests pass.
7481         These tests are either ``key=value`` for which the attribute ``key``
7482         has the exact value ``value`` or the callable passed into
7483         ``key=callable`` returns True. The callable will be passed a single
7484         value, either the value of the attribute ``key`` or ``None`` if the
7485         DataArray does not have an attribute with the name ``key``.
7486 
7487         Parameters
7488         ----------
7489         **kwargs
7490             key : str
7491                 Attribute name.
7492             value : callable or obj
7493                 If value is a callable, it should return a boolean in the form
7494                 of bool = func(attr) where attr is da.attrs[key].
7495                 Otherwise, value will be compared to the each
7496                 DataArray's attrs[key].
7497 
7498         Returns
7499         -------
7500         new : Dataset
7501             New dataset with variables filtered by attribute.
7502 
7503         Examples
7504         --------
7505         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7506         >>> precip = 10 * np.random.rand(2, 2, 3)
7507         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7508         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7509         >>> dims = ["x", "y", "time"]
7510         >>> temp_attr = dict(standard_name="air_potential_temperature")
7511         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7512 
7513         >>> ds = xr.Dataset(
7514         ...     dict(
7515         ...         temperature=(dims, temp, temp_attr),
7516         ...         precipitation=(dims, precip, precip_attr),
7517         ...     ),
7518         ...     coords=dict(
7519         ...         lon=(["x", "y"], lon),
7520         ...         lat=(["x", "y"], lat),
7521         ...         time=pd.date_range("2014-09-06", periods=3),
7522         ...         reference_time=pd.Timestamp("2014-09-05"),
7523         ...     ),
7524         ... )
7525 
7526         Get variables matching a specific standard_name:
7527 
7528         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7529         <xarray.Dataset>
7530         Dimensions:         (x: 2, y: 2, time: 3)
7531         Coordinates:
7532             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7533             lat             (x, y) float64 42.25 42.21 42.63 42.59
7534           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7535             reference_time  datetime64[ns] 2014-09-05
7536         Dimensions without coordinates: x, y
7537         Data variables:
7538             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7539 
7540         Get all variables that have a standard_name attribute:
7541 
7542         >>> standard_name = lambda v: v is not None
7543         >>> ds.filter_by_attrs(standard_name=standard_name)
7544         <xarray.Dataset>
7545         Dimensions:         (x: 2, y: 2, time: 3)
7546         Coordinates:
7547             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7548             lat             (x, y) float64 42.25 42.21 42.63 42.59
7549           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7550             reference_time  datetime64[ns] 2014-09-05
7551         Dimensions without coordinates: x, y
7552         Data variables:
7553             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7554             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7555 
7556         """
7557         selection = []
7558         for var_name, variable in self.variables.items():
7559             has_value_flag = False
7560             for attr_name, pattern in kwargs.items():
7561                 attr_value = variable.attrs.get(attr_name)
7562                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7563                     has_value_flag = True
7564                 else:
7565                     has_value_flag = False
7566                     break
7567             if has_value_flag is True:
7568                 selection.append(var_name)
7569         return self[selection]
7570 
7571     def unify_chunks(self: T_Dataset) -> T_Dataset:
7572         """Unify chunk size along all chunked dimensions of this Dataset.
7573 
7574         Returns
7575         -------
7576         Dataset with consistent chunk sizes for all dask-array variables
7577 
7578         See Also
7579         --------
7580         dask.array.core.unify_chunks
7581         """
7582 
7583         return unify_chunks(self)[0]
7584 
7585     def map_blocks(
7586         self,
7587         func: Callable[..., T_Xarray],
7588         args: Sequence[Any] = (),
7589         kwargs: Mapping[str, Any] | None = None,
7590         template: DataArray | Dataset | None = None,
7591     ) -> T_Xarray:
7592         """
7593         Apply a function to each block of this Dataset.
7594 
7595         .. warning::
7596             This method is experimental and its signature may change.
7597 
7598         Parameters
7599         ----------
7600         func : callable
7601             User-provided function that accepts a Dataset as its first
7602             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7603             corresponding to one chunk along each chunked dimension. ``func`` will be
7604             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7605 
7606             This function must return either a single DataArray or a single Dataset.
7607 
7608             This function cannot add a new chunked dimension.
7609         args : sequence
7610             Passed to func after unpacking and subsetting any xarray objects by blocks.
7611             xarray objects in args must be aligned with obj, otherwise an error is raised.
7612         kwargs : Mapping or None
7613             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7614             subset to blocks. Passing dask collections in kwargs is not allowed.
7615         template : DataArray, Dataset or None, optional
7616             xarray object representing the final result after compute is called. If not provided,
7617             the function will be first run on mocked-up data, that looks like this object but
7618             has sizes 0, to determine properties of the returned object such as dtype,
7619             variable names, attributes, new dimensions and new indexes (if any).
7620             ``template`` must be provided if the function changes the size of existing dimensions.
7621             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7622             ``attrs`` set by ``func`` will be ignored.
7623 
7624         Returns
7625         -------
7626         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7627         function.
7628 
7629         Notes
7630         -----
7631         This function is designed for when ``func`` needs to manipulate a whole xarray object
7632         subset to each block. Each block is loaded into memory. In the more common case where
7633         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7634 
7635         If none of the variables in this object is backed by dask arrays, calling this function is
7636         equivalent to calling ``func(obj, *args, **kwargs)``.
7637 
7638         See Also
7639         --------
7640         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7641         xarray.DataArray.map_blocks
7642 
7643         Examples
7644         --------
7645         Calculate an anomaly from climatology using ``.groupby()``. Using
7646         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7647         its indices, and its methods like ``.groupby()``.
7648 
7649         >>> def calculate_anomaly(da, groupby_type="time.month"):
7650         ...     gb = da.groupby(groupby_type)
7651         ...     clim = gb.mean(dim="time")
7652         ...     return gb - clim
7653         ...
7654         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7655         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7656         >>> np.random.seed(123)
7657         >>> array = xr.DataArray(
7658         ...     np.random.rand(len(time)),
7659         ...     dims=["time"],
7660         ...     coords={"time": time, "month": month},
7661         ... ).chunk()
7662         >>> ds = xr.Dataset({"a": array})
7663         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7664         <xarray.Dataset>
7665         Dimensions:  (time: 24)
7666         Coordinates:
7667           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7668             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7669         Data variables:
7670             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7671 
7672         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7673         to the function being applied in ``xr.map_blocks()``:
7674 
7675         >>> ds.map_blocks(
7676         ...     calculate_anomaly,
7677         ...     kwargs={"groupby_type": "time.year"},
7678         ...     template=ds,
7679         ... )
7680         <xarray.Dataset>
7681         Dimensions:  (time: 24)
7682         Coordinates:
7683           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7684             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7685         Data variables:
7686             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7687         """
7688         from .parallel import map_blocks
7689 
7690         return map_blocks(func, self, args, kwargs, template)
7691 
7692     def polyfit(
7693         self: T_Dataset,
7694         dim: Hashable,
7695         deg: int,
7696         skipna: bool | None = None,
7697         rcond: float | None = None,
7698         w: Hashable | Any = None,
7699         full: bool = False,
7700         cov: bool | Literal["unscaled"] = False,
7701     ) -> T_Dataset:
7702         """
7703         Least squares polynomial fit.
7704 
7705         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7706         invalid values when `skipna = True`.
7707 
7708         Parameters
7709         ----------
7710         dim : hashable
7711             Coordinate along which to fit the polynomials.
7712         deg : int
7713             Degree of the fitting polynomial.
7714         skipna : bool or None, optional
7715             If True, removes all invalid values before fitting each 1D slices of the array.
7716             Default is True if data is stored in a dask.array or if there is any
7717             invalid values, False otherwise.
7718         rcond : float or None, optional
7719             Relative condition number to the fit.
7720         w : hashable or Any, optional
7721             Weights to apply to the y-coordinate of the sample points.
7722             Can be an array-like object or the name of a coordinate in the dataset.
7723         full : bool, default: False
7724             Whether to return the residuals, matrix rank and singular values in addition
7725             to the coefficients.
7726         cov : bool or "unscaled", default: False
7727             Whether to return to the covariance matrix in addition to the coefficients.
7728             The matrix is not scaled if `cov='unscaled'`.
7729 
7730         Returns
7731         -------
7732         polyfit_results : Dataset
7733             A single dataset which contains (for each "var" in the input dataset):
7734 
7735             [var]_polyfit_coefficients
7736                 The coefficients of the best fit for each variable in this dataset.
7737             [var]_polyfit_residuals
7738                 The residuals of the least-square computation for each variable (only included if `full=True`)
7739                 When the matrix rank is deficient, np.nan is returned.
7740             [dim]_matrix_rank
7741                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7742                 The rank is computed ignoring the NaN values that might be skipped.
7743             [dim]_singular_values
7744                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7745             [var]_polyfit_covariance
7746                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7747 
7748         Warns
7749         -----
7750         RankWarning
7751             The rank of the coefficient matrix in the least-squares fit is deficient.
7752             The warning is not raised with in-memory (not dask) data and `full=True`.
7753 
7754         See Also
7755         --------
7756         numpy.polyfit
7757         numpy.polyval
7758         xarray.polyval
7759         """
7760         from .dataarray import DataArray
7761 
7762         variables = {}
7763         skipna_da = skipna
7764 
7765         x = get_clean_interp_index(self, dim, strict=False)
7766         xname = f"{self[dim].name}_"
7767         order = int(deg) + 1
7768         lhs = np.vander(x, order)
7769 
7770         if rcond is None:
7771             rcond = (
7772                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7773             )
7774 
7775         # Weights:
7776         if w is not None:
7777             if isinstance(w, Hashable):
7778                 w = self.coords[w]
7779             w = np.asarray(w)
7780             if w.ndim != 1:
7781                 raise TypeError("Expected a 1-d array for weights.")
7782             if w.shape[0] != lhs.shape[0]:
7783                 raise TypeError(f"Expected w and {dim} to have the same length")
7784             lhs *= w[:, np.newaxis]
7785 
7786         # Scaling
7787         scale = np.sqrt((lhs * lhs).sum(axis=0))
7788         lhs /= scale
7789 
7790         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7791 
7792         rank = np.linalg.matrix_rank(lhs)
7793 
7794         if full:
7795             rank = DataArray(rank, name=xname + "matrix_rank")
7796             variables[rank.name] = rank
7797             _sing = np.linalg.svd(lhs, compute_uv=False)
7798             sing = DataArray(
7799                 _sing,
7800                 dims=(degree_dim,),
7801                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7802                 name=xname + "singular_values",
7803             )
7804             variables[sing.name] = sing
7805 
7806         for name, da in self.data_vars.items():
7807             if dim not in da.dims:
7808                 continue
7809 
7810             if is_duck_dask_array(da.data) and (
7811                 rank != order or full or skipna is None
7812             ):
7813                 # Current algorithm with dask and skipna=False neither supports
7814                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7815                 skipna_da = True
7816             elif skipna is None:
7817                 skipna_da = bool(np.any(da.isnull()))
7818 
7819             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7820             stacked_coords: dict[Hashable, DataArray] = {}
7821             if dims_to_stack:
7822                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7823                 rhs = da.transpose(dim, *dims_to_stack).stack(
7824                     {stacked_dim: dims_to_stack}
7825                 )
7826                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7827                 scale_da = scale[:, np.newaxis]
7828             else:
7829                 rhs = da
7830                 scale_da = scale
7831 
7832             if w is not None:
7833                 rhs *= w[:, np.newaxis]
7834 
7835             with warnings.catch_warnings():
7836                 if full:  # Copy np.polyfit behavior
7837                     warnings.simplefilter("ignore", np.RankWarning)
7838                 else:  # Raise only once per variable
7839                     warnings.simplefilter("once", np.RankWarning)
7840 
7841                 coeffs, residuals = duck_array_ops.least_squares(
7842                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7843                 )
7844 
7845             if isinstance(name, str):
7846                 name = f"{name}_"
7847             else:
7848                 # Thus a ReprObject => polyfit was called on a DataArray
7849                 name = ""
7850 
7851             coeffs = DataArray(
7852                 coeffs / scale_da,
7853                 dims=[degree_dim] + list(stacked_coords.keys()),
7854                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7855                 name=name + "polyfit_coefficients",
7856             )
7857             if dims_to_stack:
7858                 coeffs = coeffs.unstack(stacked_dim)
7859             variables[coeffs.name] = coeffs
7860 
7861             if full or (cov is True):
7862                 residuals = DataArray(
7863                     residuals if dims_to_stack else residuals.squeeze(),
7864                     dims=list(stacked_coords.keys()),
7865                     coords=stacked_coords,
7866                     name=name + "polyfit_residuals",
7867                 )
7868                 if dims_to_stack:
7869                     residuals = residuals.unstack(stacked_dim)
7870                 variables[residuals.name] = residuals
7871 
7872             if cov:
7873                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7874                 Vbase /= np.outer(scale, scale)
7875                 if cov == "unscaled":
7876                     fac = 1
7877                 else:
7878                     if x.shape[0] <= order:
7879                         raise ValueError(
7880                             "The number of data points must exceed order to scale the covariance matrix."
7881                         )
7882                     fac = residuals / (x.shape[0] - order)
7883                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7884                 variables[name + "polyfit_covariance"] = covariance
7885 
7886         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7887 
7888     def pad(
7889         self: T_Dataset,
7890         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7891         mode: PadModeOptions = "constant",
7892         stat_length: int
7893         | tuple[int, int]
7894         | Mapping[Any, tuple[int, int]]
7895         | None = None,
7896         constant_values: (
7897             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7898         ) = None,
7899         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7900         reflect_type: PadReflectOptions = None,
7901         **pad_width_kwargs: Any,
7902     ) -> T_Dataset:
7903         """Pad this dataset along one or more dimensions.
7904 
7905         .. warning::
7906             This function is experimental and its behaviour is likely to change
7907             especially regarding padding of dimension coordinates (or IndexVariables).
7908 
7909         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7910         coordinates will be padded with the same mode, otherwise coordinates
7911         are padded using the "constant" mode with fill_value dtypes.NA.
7912 
7913         Parameters
7914         ----------
7915         pad_width : mapping of hashable to tuple of int
7916             Mapping with the form of {dim: (pad_before, pad_after)}
7917             describing the number of values padded along each dimension.
7918             {dim: pad} is a shortcut for pad_before = pad_after = pad
7919         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7920             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7921             How to pad the DataArray (taken from numpy docs):
7922 
7923             - "constant": Pads with a constant value.
7924             - "edge": Pads with the edge values of array.
7925             - "linear_ramp": Pads with the linear ramp between end_value and the
7926               array edge value.
7927             - "maximum": Pads with the maximum value of all or part of the
7928               vector along each axis.
7929             - "mean": Pads with the mean value of all or part of the
7930               vector along each axis.
7931             - "median": Pads with the median value of all or part of the
7932               vector along each axis.
7933             - "minimum": Pads with the minimum value of all or part of the
7934               vector along each axis.
7935             - "reflect": Pads with the reflection of the vector mirrored on
7936               the first and last values of the vector along each axis.
7937             - "symmetric": Pads with the reflection of the vector mirrored
7938               along the edge of the array.
7939             - "wrap": Pads with the wrap of the vector along the axis.
7940               The first values are used to pad the end and the
7941               end values are used to pad the beginning.
7942 
7943         stat_length : int, tuple or mapping of hashable to tuple, default: None
7944             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7945             values at edge of each axis used to calculate the statistic value.
7946             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7947             statistic lengths along each dimension.
7948             ((before, after),) yields same before and after statistic lengths
7949             for each dimension.
7950             (stat_length,) or int is a shortcut for before = after = statistic
7951             length for all axes.
7952             Default is ``None``, to use the entire axis.
7953         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7954             Used in 'constant'.  The values to set the padded values for each
7955             axis.
7956             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7957             pad constants along each dimension.
7958             ``((before, after),)`` yields same before and after constants for each
7959             dimension.
7960             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7961             all dimensions.
7962             Default is 0.
7963         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7964             Used in 'linear_ramp'.  The values used for the ending value of the
7965             linear_ramp and that will form the edge of the padded array.
7966             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7967             end values along each dimension.
7968             ``((before, after),)`` yields same before and after end values for each
7969             axis.
7970             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7971             all axes.
7972             Default is 0.
7973         reflect_type : {"even", "odd", None}, optional
7974             Used in "reflect", and "symmetric".  The "even" style is the
7975             default with an unaltered reflection around the edge value.  For
7976             the "odd" style, the extended part of the array is created by
7977             subtracting the reflected values from two times the edge value.
7978         **pad_width_kwargs
7979             The keyword arguments form of ``pad_width``.
7980             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7981 
7982         Returns
7983         -------
7984         padded : Dataset
7985             Dataset with the padded coordinates and data.
7986 
7987         See Also
7988         --------
7989         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7990 
7991         Notes
7992         -----
7993         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7994         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7995         specify ``constant_values=np.nan``
7996 
7997         Padding coordinates will drop their corresponding index (if any) and will reset default
7998         indexes for dimension coordinates.
7999 
8000         Examples
8001         --------
8002         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8003         >>> ds.pad(x=(1, 2))
8004         <xarray.Dataset>
8005         Dimensions:  (x: 8)
8006         Dimensions without coordinates: x
8007         Data variables:
8008             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8009         """
8010         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8011 
8012         if mode in ("edge", "reflect", "symmetric", "wrap"):
8013             coord_pad_mode = mode
8014             coord_pad_options = {
8015                 "stat_length": stat_length,
8016                 "constant_values": constant_values,
8017                 "end_values": end_values,
8018                 "reflect_type": reflect_type,
8019             }
8020         else:
8021             coord_pad_mode = "constant"
8022             coord_pad_options = {}
8023 
8024         variables = {}
8025 
8026         # keep indexes that won't be affected by pad and drop all other indexes
8027         xindexes = self.xindexes
8028         pad_dims = set(pad_width)
8029         indexes = {}
8030         for k, idx in xindexes.items():
8031             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8032                 indexes[k] = idx
8033 
8034         for name, var in self.variables.items():
8035             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8036             if not var_pad_width:
8037                 variables[name] = var
8038             elif name in self.data_vars:
8039                 variables[name] = var.pad(
8040                     pad_width=var_pad_width,
8041                     mode=mode,
8042                     stat_length=stat_length,
8043                     constant_values=constant_values,
8044                     end_values=end_values,
8045                     reflect_type=reflect_type,
8046                 )
8047             else:
8048                 variables[name] = var.pad(
8049                     pad_width=var_pad_width,
8050                     mode=coord_pad_mode,
8051                     **coord_pad_options,  # type: ignore[arg-type]
8052                 )
8053                 # reset default index of dimension coordinates
8054                 if (name,) == var.dims:
8055                     dim_var = {name: variables[name]}
8056                     index = PandasIndex.from_variables(dim_var, options={})
8057                     index_vars = index.create_variables(dim_var)
8058                     indexes[name] = index
8059                     variables[name] = index_vars[name]
8060 
8061         return self._replace_with_new_dims(variables, indexes=indexes)
8062 
8063     def idxmin(
8064         self: T_Dataset,
8065         dim: Hashable | None = None,
8066         skipna: bool | None = None,
8067         fill_value: Any = xrdtypes.NA,
8068         keep_attrs: bool | None = None,
8069     ) -> T_Dataset:
8070         """Return the coordinate label of the minimum value along a dimension.
8071 
8072         Returns a new `Dataset` named after the dimension with the values of
8073         the coordinate labels along that dimension corresponding to minimum
8074         values along that dimension.
8075 
8076         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8077         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8078 
8079         Parameters
8080         ----------
8081         dim : Hashable, optional
8082             Dimension over which to apply `idxmin`.  This is optional for 1D
8083             variables, but required for variables with 2 or more dimensions.
8084         skipna : bool or None, optional
8085             If True, skip missing values (as marked by NaN). By default, only
8086             skips missing values for ``float``, ``complex``, and ``object``
8087             dtypes; other dtypes either do not have a sentinel missing value
8088             (``int``) or ``skipna=True`` has not been implemented
8089             (``datetime64`` or ``timedelta64``).
8090         fill_value : Any, default: NaN
8091             Value to be filled in case all of the values along a dimension are
8092             null.  By default this is NaN.  The fill value and result are
8093             automatically converted to a compatible dtype if possible.
8094             Ignored if ``skipna`` is False.
8095         keep_attrs : bool or None, optional
8096             If True, the attributes (``attrs``) will be copied from the
8097             original object to the new one. If False, the new object
8098             will be returned without attributes.
8099 
8100         Returns
8101         -------
8102         reduced : Dataset
8103             New `Dataset` object with `idxmin` applied to its data and the
8104             indicated dimension removed.
8105 
8106         See Also
8107         --------
8108         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8109 
8110         Examples
8111         --------
8112         >>> array1 = xr.DataArray(
8113         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8114         ... )
8115         >>> array2 = xr.DataArray(
8116         ...     [
8117         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8118         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8119         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8120         ...     ],
8121         ...     dims=["y", "x"],
8122         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8123         ... )
8124         >>> ds = xr.Dataset({"int": array1, "float": array2})
8125         >>> ds.min(dim="x")
8126         <xarray.Dataset>
8127         Dimensions:  (y: 3)
8128         Coordinates:
8129           * y        (y) int64 -1 0 1
8130         Data variables:
8131             int      int64 -2
8132             float    (y) float64 -2.0 -4.0 1.0
8133         >>> ds.argmin(dim="x")
8134         <xarray.Dataset>
8135         Dimensions:  (y: 3)
8136         Coordinates:
8137           * y        (y) int64 -1 0 1
8138         Data variables:
8139             int      int64 4
8140             float    (y) int64 4 0 2
8141         >>> ds.idxmin(dim="x")
8142         <xarray.Dataset>
8143         Dimensions:  (y: 3)
8144         Coordinates:
8145           * y        (y) int64 -1 0 1
8146         Data variables:
8147             int      <U1 'e'
8148             float    (y) object 'e' 'a' 'c'
8149         """
8150         return self.map(
8151             methodcaller(
8152                 "idxmin",
8153                 dim=dim,
8154                 skipna=skipna,
8155                 fill_value=fill_value,
8156                 keep_attrs=keep_attrs,
8157             )
8158         )
8159 
8160     def idxmax(
8161         self: T_Dataset,
8162         dim: Hashable | None = None,
8163         skipna: bool | None = None,
8164         fill_value: Any = xrdtypes.NA,
8165         keep_attrs: bool | None = None,
8166     ) -> T_Dataset:
8167         """Return the coordinate label of the maximum value along a dimension.
8168 
8169         Returns a new `Dataset` named after the dimension with the values of
8170         the coordinate labels along that dimension corresponding to maximum
8171         values along that dimension.
8172 
8173         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8174         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8175 
8176         Parameters
8177         ----------
8178         dim : str, optional
8179             Dimension over which to apply `idxmax`.  This is optional for 1D
8180             variables, but required for variables with 2 or more dimensions.
8181         skipna : bool or None, optional
8182             If True, skip missing values (as marked by NaN). By default, only
8183             skips missing values for ``float``, ``complex``, and ``object``
8184             dtypes; other dtypes either do not have a sentinel missing value
8185             (``int``) or ``skipna=True`` has not been implemented
8186             (``datetime64`` or ``timedelta64``).
8187         fill_value : Any, default: NaN
8188             Value to be filled in case all of the values along a dimension are
8189             null.  By default this is NaN.  The fill value and result are
8190             automatically converted to a compatible dtype if possible.
8191             Ignored if ``skipna`` is False.
8192         keep_attrs : bool or None, optional
8193             If True, the attributes (``attrs``) will be copied from the
8194             original object to the new one. If False, the new object
8195             will be returned without attributes.
8196 
8197         Returns
8198         -------
8199         reduced : Dataset
8200             New `Dataset` object with `idxmax` applied to its data and the
8201             indicated dimension removed.
8202 
8203         See Also
8204         --------
8205         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8206 
8207         Examples
8208         --------
8209         >>> array1 = xr.DataArray(
8210         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8211         ... )
8212         >>> array2 = xr.DataArray(
8213         ...     [
8214         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8215         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8216         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8217         ...     ],
8218         ...     dims=["y", "x"],
8219         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8220         ... )
8221         >>> ds = xr.Dataset({"int": array1, "float": array2})
8222         >>> ds.max(dim="x")
8223         <xarray.Dataset>
8224         Dimensions:  (y: 3)
8225         Coordinates:
8226           * y        (y) int64 -1 0 1
8227         Data variables:
8228             int      int64 2
8229             float    (y) float64 2.0 2.0 1.0
8230         >>> ds.argmax(dim="x")
8231         <xarray.Dataset>
8232         Dimensions:  (y: 3)
8233         Coordinates:
8234           * y        (y) int64 -1 0 1
8235         Data variables:
8236             int      int64 1
8237             float    (y) int64 0 2 2
8238         >>> ds.idxmax(dim="x")
8239         <xarray.Dataset>
8240         Dimensions:  (y: 3)
8241         Coordinates:
8242           * y        (y) int64 -1 0 1
8243         Data variables:
8244             int      <U1 'b'
8245             float    (y) object 'a' 'c' 'c'
8246         """
8247         return self.map(
8248             methodcaller(
8249                 "idxmax",
8250                 dim=dim,
8251                 skipna=skipna,
8252                 fill_value=fill_value,
8253                 keep_attrs=keep_attrs,
8254             )
8255         )
8256 
8257     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8258         """Indices of the minima of the member variables.
8259 
8260         If there are multiple minima, the indices of the first one found will be
8261         returned.
8262 
8263         Parameters
8264         ----------
8265         dim : Hashable, optional
8266             The dimension over which to find the minimum. By default, finds minimum over
8267             all dimensions - for now returning an int for backward compatibility, but
8268             this is deprecated, in future will be an error, since DataArray.argmin will
8269             return a dict with indices for all dimensions, which does not make sense for
8270             a Dataset.
8271         keep_attrs : bool, optional
8272             If True, the attributes (`attrs`) will be copied from the original
8273             object to the new one.  If False (default), the new object will be
8274             returned without attributes.
8275         skipna : bool, optional
8276             If True, skip missing values (as marked by NaN). By default, only
8277             skips missing values for float dtypes; other dtypes either do not
8278             have a sentinel missing value (int) or skipna=True has not been
8279             implemented (object, datetime64 or timedelta64).
8280 
8281         Returns
8282         -------
8283         result : Dataset
8284 
8285         See Also
8286         --------
8287         DataArray.argmin
8288         """
8289         if dim is None:
8290             warnings.warn(
8291                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8292                 "dim changes to return a dict of indices of each dimension, for "
8293                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8294                 "since we don't return a dict of Datasets.",
8295                 DeprecationWarning,
8296                 stacklevel=2,
8297             )
8298         if (
8299             dim is None
8300             or (not isinstance(dim, Sequence) and dim is not ...)
8301             or isinstance(dim, str)
8302         ):
8303             # Return int index if single dimension is passed, and is not part of a
8304             # sequence
8305             argmin_func = getattr(duck_array_ops, "argmin")
8306             return self.reduce(argmin_func, dim=dim, **kwargs)
8307         else:
8308             raise ValueError(
8309                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8310                 "dicts cannot be contained in a Dataset, so cannot call "
8311                 "Dataset.argmin() with a sequence or ... for dim"
8312             )
8313 
8314     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8315         """Indices of the maxima of the member variables.
8316 
8317         If there are multiple maxima, the indices of the first one found will be
8318         returned.
8319 
8320         Parameters
8321         ----------
8322         dim : str, optional
8323             The dimension over which to find the maximum. By default, finds maximum over
8324             all dimensions - for now returning an int for backward compatibility, but
8325             this is deprecated, in future will be an error, since DataArray.argmax will
8326             return a dict with indices for all dimensions, which does not make sense for
8327             a Dataset.
8328         keep_attrs : bool, optional
8329             If True, the attributes (`attrs`) will be copied from the original
8330             object to the new one.  If False (default), the new object will be
8331             returned without attributes.
8332         skipna : bool, optional
8333             If True, skip missing values (as marked by NaN). By default, only
8334             skips missing values for float dtypes; other dtypes either do not
8335             have a sentinel missing value (int) or skipna=True has not been
8336             implemented (object, datetime64 or timedelta64).
8337 
8338         Returns
8339         -------
8340         result : Dataset
8341 
8342         See Also
8343         --------
8344         DataArray.argmax
8345 
8346         """
8347         if dim is None:
8348             warnings.warn(
8349                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8350                 "dim changes to return a dict of indices of each dimension, for "
8351                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8352                 "since we don't return a dict of Datasets.",
8353                 DeprecationWarning,
8354                 stacklevel=2,
8355             )
8356         if (
8357             dim is None
8358             or (not isinstance(dim, Sequence) and dim is not ...)
8359             or isinstance(dim, str)
8360         ):
8361             # Return int index if single dimension is passed, and is not part of a
8362             # sequence
8363             argmax_func = getattr(duck_array_ops, "argmax")
8364             return self.reduce(argmax_func, dim=dim, **kwargs)
8365         else:
8366             raise ValueError(
8367                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8368                 "dicts cannot be contained in a Dataset, so cannot call "
8369                 "Dataset.argmin() with a sequence or ... for dim"
8370             )
8371 
8372     def query(
8373         self: T_Dataset,
8374         queries: Mapping[Any, Any] | None = None,
8375         parser: QueryParserOptions = "pandas",
8376         engine: QueryEngineOptions = None,
8377         missing_dims: ErrorOptionsWithWarn = "raise",
8378         **queries_kwargs: Any,
8379     ) -> T_Dataset:
8380         """Return a new dataset with each array indexed along the specified
8381         dimension(s), where the indexers are given as strings containing
8382         Python expressions to be evaluated against the data variables in the
8383         dataset.
8384 
8385         Parameters
8386         ----------
8387         queries : dict-like, optional
8388             A dict-like with keys matching dimensions and values given by strings
8389             containing Python expressions to be evaluated against the data variables
8390             in the dataset. The expressions will be evaluated using the pandas
8391             eval() function, and can contain any valid Python expressions but cannot
8392             contain any Python statements.
8393         parser : {"pandas", "python"}, default: "pandas"
8394             The parser to use to construct the syntax tree from the expression.
8395             The default of 'pandas' parses code slightly different than standard
8396             Python. Alternatively, you can parse an expression using the 'python'
8397             parser to retain strict Python semantics.
8398         engine : {"python", "numexpr", None}, default: None
8399             The engine used to evaluate the expression. Supported engines are:
8400 
8401             - None: tries to use numexpr, falls back to python
8402             - "numexpr": evaluates expressions using numexpr
8403             - "python": performs operations as if you had evald in top level python
8404 
8405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8406             What to do if dimensions that should be selected from are not present in the
8407             Dataset:
8408 
8409             - "raise": raise an exception
8410             - "warn": raise a warning, and ignore the missing dimensions
8411             - "ignore": ignore the missing dimensions
8412 
8413         **queries_kwargs : {dim: query, ...}, optional
8414             The keyword arguments form of ``queries``.
8415             One of queries or queries_kwargs must be provided.
8416 
8417         Returns
8418         -------
8419         obj : Dataset
8420             A new Dataset with the same contents as this dataset, except each
8421             array and dimension is indexed by the results of the appropriate
8422             queries.
8423 
8424         See Also
8425         --------
8426         Dataset.isel
8427         pandas.eval
8428 
8429         Examples
8430         --------
8431         >>> a = np.arange(0, 5, 1)
8432         >>> b = np.linspace(0, 1, 5)
8433         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8434         >>> ds
8435         <xarray.Dataset>
8436         Dimensions:  (x: 5)
8437         Dimensions without coordinates: x
8438         Data variables:
8439             a        (x) int64 0 1 2 3 4
8440             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8441         >>> ds.query(x="a > 2")
8442         <xarray.Dataset>
8443         Dimensions:  (x: 2)
8444         Dimensions without coordinates: x
8445         Data variables:
8446             a        (x) int64 3 4
8447             b        (x) float64 0.75 1.0
8448         """
8449 
8450         # allow queries to be given either as a dict or as kwargs
8451         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8452 
8453         # check queries
8454         for dim, expr in queries.items():
8455             if not isinstance(expr, str):
8456                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8457                 raise ValueError(msg)
8458 
8459         # evaluate the queries to create the indexers
8460         indexers = {
8461             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8462             for dim, expr in queries.items()
8463         }
8464 
8465         # apply the selection
8466         return self.isel(indexers, missing_dims=missing_dims)
8467 
8468     def curvefit(
8469         self: T_Dataset,
8470         coords: str | DataArray | Iterable[str | DataArray],
8471         func: Callable[..., Any],
8472         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8473         skipna: bool = True,
8474         p0: dict[str, Any] | None = None,
8475         bounds: dict[str, Any] | None = None,
8476         param_names: Sequence[str] | None = None,
8477         kwargs: dict[str, Any] | None = None,
8478     ) -> T_Dataset:
8479         """
8480         Curve fitting optimization for arbitrary functions.
8481 
8482         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8483 
8484         Parameters
8485         ----------
8486         coords : hashable, DataArray, or sequence of hashable or DataArray
8487             Independent coordinate(s) over which to perform the curve fitting. Must share
8488             at least one dimension with the calling object. When fitting multi-dimensional
8489             functions, supply `coords` as a sequence in the same order as arguments in
8490             `func`. To fit along existing dimensions of the calling object, `coords` can
8491             also be specified as a str or sequence of strs.
8492         func : callable
8493             User specified function in the form `f(x, *params)` which returns a numpy
8494             array of length `len(x)`. `params` are the fittable parameters which are optimized
8495             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8496             coordinates, e.g. `f((x0, x1), *params)`.
8497         reduce_dims : hashable or sequence of hashable
8498             Additional dimension(s) over which to aggregate while fitting. For example,
8499             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8500             aggregate all lat and lon points and fit the specified function along the
8501             time dimension.
8502         skipna : bool, default: True
8503             Whether to skip missing values when fitting. Default is True.
8504         p0 : dict-like, optional
8505             Optional dictionary of parameter names to initial guesses passed to the
8506             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8507             be assigned initial values following the default scipy behavior.
8508         bounds : dict-like, optional
8509             Optional dictionary of parameter names to bounding values passed to the
8510             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8511             will be unbounded following the default scipy behavior.
8512         param_names : sequence of hashable, optional
8513             Sequence of names for the fittable parameters of `func`. If not supplied,
8514             this will be automatically determined by arguments of `func`. `param_names`
8515             should be manually supplied when fitting a function that takes a variable
8516             number of parameters.
8517         **kwargs : optional
8518             Additional keyword arguments to passed to scipy curve_fit.
8519 
8520         Returns
8521         -------
8522         curvefit_results : Dataset
8523             A single dataset which contains:
8524 
8525             [var]_curvefit_coefficients
8526                 The coefficients of the best fit.
8527             [var]_curvefit_covariance
8528                 The covariance matrix of the coefficient estimates.
8529 
8530         See Also
8531         --------
8532         Dataset.polyfit
8533         scipy.optimize.curve_fit
8534         """
8535         from scipy.optimize import curve_fit
8536 
8537         from .alignment import broadcast
8538         from .computation import apply_ufunc
8539         from .dataarray import _THIS_ARRAY, DataArray
8540 
8541         if p0 is None:
8542             p0 = {}
8543         if bounds is None:
8544             bounds = {}
8545         if kwargs is None:
8546             kwargs = {}
8547 
8548         if not reduce_dims:
8549             reduce_dims_ = []
8550         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8551             reduce_dims_ = [reduce_dims]
8552         else:
8553             reduce_dims_ = list(reduce_dims)
8554 
8555         if (
8556             isinstance(coords, str)
8557             or isinstance(coords, DataArray)
8558             or not isinstance(coords, Iterable)
8559         ):
8560             coords = [coords]
8561         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8562 
8563         # Determine whether any coords are dims on self
8564         for coord in coords_:
8565             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8566         reduce_dims_ = list(set(reduce_dims_))
8567         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8568         if not reduce_dims_:
8569             raise ValueError(
8570                 "No arguments to `coords` were identified as a dimension on the calling "
8571                 "object, and no dims were supplied to `reduce_dims`. This would result "
8572                 "in fitting on scalar data."
8573             )
8574 
8575         # Broadcast all coords with each other
8576         coords_ = broadcast(*coords_)
8577         coords_ = [
8578             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8579         ]
8580 
8581         params, func_args = _get_func_args(func, param_names)
8582         param_defaults, bounds_defaults = _initialize_curvefit_params(
8583             params, p0, bounds, func_args
8584         )
8585         n_params = len(params)
8586         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8587         kwargs.setdefault(
8588             "bounds",
8589             [
8590                 [bounds_defaults[p][0] for p in params],
8591                 [bounds_defaults[p][1] for p in params],
8592             ],
8593         )
8594 
8595         def _wrapper(Y, *coords_, **kwargs):
8596             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8597             x = np.vstack([c.ravel() for c in coords_])
8598             y = Y.ravel()
8599             if skipna:
8600                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8601                 x = x[:, mask]
8602                 y = y[mask]
8603                 if not len(y):
8604                     popt = np.full([n_params], np.nan)
8605                     pcov = np.full([n_params, n_params], np.nan)
8606                     return popt, pcov
8607             x = np.squeeze(x)
8608             popt, pcov = curve_fit(func, x, y, **kwargs)
8609             return popt, pcov
8610 
8611         result = type(self)()
8612         for name, da in self.data_vars.items():
8613             if name is _THIS_ARRAY:
8614                 name = ""
8615             else:
8616                 name = f"{str(name)}_"
8617 
8618             popt, pcov = apply_ufunc(
8619                 _wrapper,
8620                 da,
8621                 *coords_,
8622                 vectorize=True,
8623                 dask="parallelized",
8624                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8625                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8626                 dask_gufunc_kwargs={
8627                     "output_sizes": {
8628                         "param": n_params,
8629                         "cov_i": n_params,
8630                         "cov_j": n_params,
8631                     },
8632                 },
8633                 output_dtypes=(np.float64, np.float64),
8634                 exclude_dims=set(reduce_dims_),
8635                 kwargs=kwargs,
8636             )
8637             result[name + "curvefit_coefficients"] = popt
8638             result[name + "curvefit_covariance"] = pcov
8639 
8640         result = result.assign_coords(
8641             {"param": params, "cov_i": params, "cov_j": params}
8642         )
8643         result.attrs = self.attrs.copy()
8644 
8645         return result
8646 
8647     def drop_duplicates(
8648         self: T_Dataset,
8649         dim: Hashable | Iterable[Hashable],
8650         keep: Literal["first", "last", False] = "first",
8651     ) -> T_Dataset:
8652         """Returns a new Dataset with duplicate dimension values removed.
8653 
8654         Parameters
8655         ----------
8656         dim : dimension label or labels
8657             Pass `...` to drop duplicates along all dimensions.
8658         keep : {"first", "last", False}, default: "first"
8659             Determines which duplicates (if any) to keep.
8660             - ``"first"`` : Drop duplicates except for the first occurrence.
8661             - ``"last"`` : Drop duplicates except for the last occurrence.
8662             - False : Drop all duplicates.
8663 
8664         Returns
8665         -------
8666         Dataset
8667 
8668         See Also
8669         --------
8670         DataArray.drop_duplicates
8671         """
8672         if isinstance(dim, str):
8673             dims: Iterable = (dim,)
8674         elif dim is ...:
8675             dims = self.dims
8676         elif not isinstance(dim, Iterable):
8677             dims = [dim]
8678         else:
8679             dims = dim
8680 
8681         missing_dims = set(dims) - set(self.dims)
8682         if missing_dims:
8683             raise ValueError(f"'{missing_dims}' not found in dimensions")
8684 
8685         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8686         return self.isel(indexes)
8687 
8688     def convert_calendar(
8689         self: T_Dataset,
8690         calendar: CFCalendar,
8691         dim: Hashable = "time",
8692         align_on: Literal["date", "year", None] = None,
8693         missing: Any | None = None,
8694         use_cftime: bool | None = None,
8695     ) -> T_Dataset:
8696         """Convert the Dataset to another calendar.
8697 
8698         Only converts the individual timestamps, does not modify any data except
8699         in dropping invalid/surplus dates or inserting missing dates.
8700 
8701         If the source and target calendars are either no_leap, all_leap or a
8702         standard type, only the type of the time array is modified.
8703         When converting to a leap year from a non-leap year, the 29th of February
8704         is removed from the array. In the other direction the 29th of February
8705         will be missing in the output, unless `missing` is specified,
8706         in which case that value is inserted.
8707 
8708         For conversions involving `360_day` calendars, see Notes.
8709 
8710         This method is safe to use with sub-daily data as it doesn't touch the
8711         time part of the timestamps.
8712 
8713         Parameters
8714         ---------
8715         calendar : str
8716             The target calendar name.
8717         dim : Hashable, default: "time"
8718             Name of the time coordinate.
8719         align_on : {None, 'date', 'year'}, optional
8720             Must be specified when either source or target is a `360_day` calendar,
8721             ignored otherwise. See Notes.
8722         missing : Any or None, optional
8723             By default, i.e. if the value is None, this method will simply attempt
8724             to convert the dates in the source calendar to the same dates in the
8725             target calendar, and drop any of those that are not possible to
8726             represent.  If a value is provided, a new time coordinate will be
8727             created in the target calendar with the same frequency as the original
8728             time coordinate; for any dates that are not present in the source, the
8729             data will be filled with this value.  Note that using this mode requires
8730             that the source data have an inferable frequency; for more information
8731             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8732             target calendar combinations, this could result in many missing values, see notes.
8733         use_cftime : bool or None, optional
8734             Whether to use cftime objects in the output, only used if `calendar`
8735             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8736             If True, the new time axis uses cftime objects.
8737             If None (default), it uses :py:class:`numpy.datetime64` values if the
8738             date range permits it, and :py:class:`cftime.datetime` objects if not.
8739             If False, it uses :py:class:`numpy.datetime64`  or fails.
8740 
8741         Returns
8742         -------
8743         Dataset
8744             Copy of the dataarray with the time coordinate converted to the
8745             target calendar. If 'missing' was None (default), invalid dates in
8746             the new calendar are dropped, but missing dates are not inserted.
8747             If `missing` was given, the new data is reindexed to have a time axis
8748             with the same frequency as the source, but in the new calendar; any
8749             missing datapoints are filled with `missing`.
8750 
8751         Notes
8752         -----
8753         Passing a value to `missing` is only usable if the source's time coordinate as an
8754         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8755         if the target coordinate, generated from this frequency, has dates equivalent to the
8756         source. It is usually **not** appropriate to use this mode with:
8757 
8758         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8759         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8760             or 'mH' where 24 % m != 0).
8761 
8762         If one of the source or target calendars is `"360_day"`, `align_on` must
8763         be specified and two options are offered.
8764 
8765         - "year"
8766             The dates are translated according to their relative position in the year,
8767             ignoring their original month and day information, meaning that the
8768             missing/surplus days are added/removed at regular intervals.
8769 
8770             From a `360_day` to a standard calendar, the output will be missing the
8771             following dates (day of year in parentheses):
8772 
8773             To a leap year:
8774                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8775                 September 31st (275) and November 30th (335).
8776             To a non-leap year:
8777                 February 6th (36), April 19th (109), July 2nd (183),
8778                 September 12th (255), November 25th (329).
8779 
8780             From a standard calendar to a `"360_day"`, the following dates in the
8781             source array will be dropped:
8782 
8783             From a leap year:
8784                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8785                 September 31st (275), December 1st (336)
8786             From a non-leap year:
8787                 February 6th (37), April 20th (110), July 2nd (183),
8788                 September 13th (256), November 25th (329)
8789 
8790             This option is best used on daily and subdaily data.
8791 
8792         - "date"
8793             The month/day information is conserved and invalid dates are dropped
8794             from the output. This means that when converting from a `"360_day"` to a
8795             standard calendar, all 31st (Jan, March, May, July, August, October and
8796             December) will be missing as there is no equivalent dates in the
8797             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8798             will be dropped as there are no equivalent dates in a standard calendar.
8799 
8800             This option is best used with data on a frequency coarser than daily.
8801         """
8802         return convert_calendar(
8803             self,
8804             calendar,
8805             dim=dim,
8806             align_on=align_on,
8807             missing=missing,
8808             use_cftime=use_cftime,
8809         )
8810 
8811     def interp_calendar(
8812         self: T_Dataset,
8813         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8814         dim: Hashable = "time",
8815     ) -> T_Dataset:
8816         """Interpolates the Dataset to another calendar based on decimal year measure.
8817 
8818         Each timestamp in `source` and `target` are first converted to their decimal
8819         year equivalent then `source` is interpolated on the target coordinate.
8820         The decimal year of a timestamp is its year plus its sub-year component
8821         converted to the fraction of its year. For example "2000-03-01 12:00" is
8822         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8823 
8824         This method should only be used when the time (HH:MM:SS) information of
8825         time coordinate is not important.
8826 
8827         Parameters
8828         ----------
8829         target: DataArray or DatetimeIndex or CFTimeIndex
8830             The target time coordinate of a valid dtype
8831             (np.datetime64 or cftime objects)
8832         dim : Hashable, default: "time"
8833             The time coordinate name.
8834 
8835         Return
8836         ------
8837         DataArray
8838             The source interpolated on the decimal years of target,
8839         """
8840         return interp_calendar(self, target, dim=dim)
8841 
8842     def groupby(
8843         self,
8844         group: Hashable | DataArray | IndexVariable,
8845         squeeze: bool = True,
8846         restore_coord_dims: bool = False,
8847     ) -> DatasetGroupBy:
8848         """Returns a DatasetGroupBy object for performing grouped operations.
8849 
8850         Parameters
8851         ----------
8852         group : Hashable, DataArray or IndexVariable
8853             Array whose unique values should be used to group this array. If a
8854             string, must be the name of a variable contained in this dataset.
8855         squeeze : bool, default: True
8856             If "group" is a dimension of any arrays in this dataset, `squeeze`
8857             controls whether the subarrays have a dimension of length 1 along
8858             that dimension or if the dimension is squeezed out.
8859         restore_coord_dims : bool, default: False
8860             If True, also restore the dimension order of multi-dimensional
8861             coordinates.
8862 
8863         Returns
8864         -------
8865         grouped : DatasetGroupBy
8866             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8867             iterated over in the form of `(unique_value, grouped_array)` pairs.
8868 
8869         See Also
8870         --------
8871         Dataset.groupby_bins
8872         DataArray.groupby
8873         core.groupby.DatasetGroupBy
8874         pandas.DataFrame.groupby
8875         """
8876         from .groupby import DatasetGroupBy
8877 
8878         # While we don't generally check the type of every arg, passing
8879         # multiple dimensions as multiple arguments is common enough, and the
8880         # consequences hidden enough (strings evaluate as true) to warrant
8881         # checking here.
8882         # A future version could make squeeze kwarg only, but would face
8883         # backward-compat issues.
8884         if not isinstance(squeeze, bool):
8885             raise TypeError(
8886                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8887             )
8888 
8889         return DatasetGroupBy(
8890             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8891         )
8892 
8893     def groupby_bins(
8894         self,
8895         group: Hashable | DataArray | IndexVariable,
8896         bins: ArrayLike,
8897         right: bool = True,
8898         labels: ArrayLike | None = None,
8899         precision: int = 3,
8900         include_lowest: bool = False,
8901         squeeze: bool = True,
8902         restore_coord_dims: bool = False,
8903     ) -> DatasetGroupBy:
8904         """Returns a DatasetGroupBy object for performing grouped operations.
8905 
8906         Rather than using all unique values of `group`, the values are discretized
8907         first by applying `pandas.cut` [1]_ to `group`.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose binned values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         bins : int or array-like
8915             If bins is an int, it defines the number of equal-width bins in the
8916             range of x. However, in this case, the range of x is extended by .1%
8917             on each side to include the min or max values of x. If bins is a
8918             sequence it defines the bin edges allowing for non-uniform bin
8919             width. No extension of the range of x is done in this case.
8920         right : bool, default: True
8921             Indicates whether the bins include the rightmost edge or not. If
8922             right == True (the default), then the bins [1,2,3,4] indicate
8923             (1,2], (2,3], (3,4].
8924         labels : array-like or bool, default: None
8925             Used as labels for the resulting bins. Must be of the same length as
8926             the resulting bins. If False, string bin labels are assigned by
8927             `pandas.cut`.
8928         precision : int, default: 3
8929             The precision at which to store and display the bins labels.
8930         include_lowest : bool, default: False
8931             Whether the first interval should be left-inclusive or not.
8932         squeeze : bool, default: True
8933             If "group" is a dimension of any arrays in this dataset, `squeeze`
8934             controls whether the subarrays have a dimension of length 1 along
8935             that dimension or if the dimension is squeezed out.
8936         restore_coord_dims : bool, default: False
8937             If True, also restore the dimension order of multi-dimensional
8938             coordinates.
8939 
8940         Returns
8941         -------
8942         grouped : DatasetGroupBy
8943             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8944             iterated over in the form of `(unique_value, grouped_array)` pairs.
8945             The name of the group has the added suffix `_bins` in order to
8946             distinguish it from the original variable.
8947 
8948         See Also
8949         --------
8950         Dataset.groupby
8951         DataArray.groupby_bins
8952         core.groupby.DatasetGroupBy
8953         pandas.DataFrame.groupby
8954 
8955         References
8956         ----------
8957         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8958         """
8959         from .groupby import DatasetGroupBy
8960 
8961         return DatasetGroupBy(
8962             self,
8963             group,
8964             squeeze=squeeze,
8965             bins=bins,
8966             restore_coord_dims=restore_coord_dims,
8967             cut_kwargs={
8968                 "right": right,
8969                 "labels": labels,
8970                 "precision": precision,
8971                 "include_lowest": include_lowest,
8972             },
8973         )
8974 
8975     def weighted(self, weights: DataArray) -> DatasetWeighted:
8976         """
8977         Weighted Dataset operations.
8978 
8979         Parameters
8980         ----------
8981         weights : DataArray
8982             An array of weights associated with the values in this Dataset.
8983             Each value in the data contributes to the reduction operation
8984             according to its associated weight.
8985 
8986         Notes
8987         -----
8988         ``weights`` must be a DataArray and cannot contain missing values.
8989         Missing values can be replaced by ``weights.fillna(0)``.
8990 
8991         Returns
8992         -------
8993         core.weighted.DatasetWeighted
8994 
8995         See Also
8996         --------
8997         DataArray.weighted
8998         """
8999         from .weighted import DatasetWeighted
9000 
9001         return DatasetWeighted(self, weights)
9002 
9003     def rolling(
9004         self,
9005         dim: Mapping[Any, int] | None = None,
9006         min_periods: int | None = None,
9007         center: bool | Mapping[Any, bool] = False,
9008         **window_kwargs: int,
9009     ) -> DatasetRolling:
9010         """
9011         Rolling window object for Datasets.
9012 
9013         Parameters
9014         ----------
9015         dim : dict, optional
9016             Mapping from the dimension name to create the rolling iterator
9017             along (e.g. `time`) to its moving window size.
9018         min_periods : int or None, default: None
9019             Minimum number of observations in window required to have a value
9020             (otherwise result is NA). The default, None, is equivalent to
9021             setting min_periods equal to the size of the window.
9022         center : bool or Mapping to int, default: False
9023             Set the labels at the center of the window.
9024         **window_kwargs : optional
9025             The keyword arguments form of ``dim``.
9026             One of dim or window_kwargs must be provided.
9027 
9028         Returns
9029         -------
9030         core.rolling.DatasetRolling
9031 
9032         See Also
9033         --------
9034         core.rolling.DatasetRolling
9035         DataArray.rolling
9036         """
9037         from .rolling import DatasetRolling
9038 
9039         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9040         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9041 
9042     def coarsen(
9043         self,
9044         dim: Mapping[Any, int] | None = None,
9045         boundary: CoarsenBoundaryOptions = "exact",
9046         side: SideOptions | Mapping[Any, SideOptions] = "left",
9047         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9048         **window_kwargs: int,
9049     ) -> DatasetCoarsen:
9050         """
9051         Coarsen object for Datasets.
9052 
9053         Parameters
9054         ----------
9055         dim : mapping of hashable to int, optional
9056             Mapping from the dimension name to the window size.
9057         boundary : {"exact", "trim", "pad"}, default: "exact"
9058             If 'exact', a ValueError will be raised if dimension size is not a
9059             multiple of the window size. If 'trim', the excess entries are
9060             dropped. If 'pad', NA will be padded.
9061         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9062         coord_func : str or mapping of hashable to str, default: "mean"
9063             function (name) that is applied to the coordinates,
9064             or a mapping from coordinate name to function (name).
9065 
9066         Returns
9067         -------
9068         core.rolling.DatasetCoarsen
9069 
9070         See Also
9071         --------
9072         core.rolling.DatasetCoarsen
9073         DataArray.coarsen
9074         """
9075         from .rolling import DatasetCoarsen
9076 
9077         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9078         return DatasetCoarsen(
9079             self,
9080             dim,
9081             boundary=boundary,
9082             side=side,
9083             coord_func=coord_func,
9084         )
9085 
9086     def resample(
9087         self,
9088         indexer: Mapping[Any, str] | None = None,
9089         skipna: bool | None = None,
9090         closed: SideOptions | None = None,
9091         label: SideOptions | None = None,
9092         base: int = 0,
9093         keep_attrs: bool | None = None,
9094         loffset: datetime.timedelta | str | None = None,
9095         restore_coord_dims: bool | None = None,
9096         **indexer_kwargs: str,
9097     ) -> DatasetResample:
9098         """Returns a Resample object for performing resampling operations.
9099 
9100         Handles both downsampling and upsampling. The resampled
9101         dimension must be a datetime-like coordinate. If any intervals
9102         contain no values from the original object, they will be given
9103         the value ``NaN``.
9104 
9105         Parameters
9106         ----------
9107         indexer : Mapping of Hashable to str, optional
9108             Mapping from the dimension name to resample frequency [1]_. The
9109             dimension must be datetime-like.
9110         skipna : bool, optional
9111             Whether to skip missing values when aggregating in downsampling.
9112         closed : {"left", "right"}, optional
9113             Side of each interval to treat as closed.
9114         label : {"left", "right"}, optional
9115             Side of each interval to use for labeling.
9116         base : int, default = 0
9117             For frequencies that evenly subdivide 1 day, the "origin" of the
9118             aggregated intervals. For example, for "24H" frequency, base could
9119             range from 0 through 23.
9120         loffset : timedelta or str, optional
9121             Offset used to adjust the resampled time labels. Some pandas date
9122             offset strings are supported.
9123         restore_coord_dims : bool, optional
9124             If True, also restore the dimension order of multi-dimensional
9125             coordinates.
9126         **indexer_kwargs : str
9127             The keyword arguments form of ``indexer``.
9128             One of indexer or indexer_kwargs must be provided.
9129 
9130         Returns
9131         -------
9132         resampled : core.resample.DataArrayResample
9133             This object resampled.
9134 
9135         See Also
9136         --------
9137         DataArray.resample
9138         pandas.Series.resample
9139         pandas.DataFrame.resample
9140 
9141         References
9142         ----------
9143         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9144         """
9145         from .resample import DatasetResample
9146 
9147         return self._resample(
9148             resample_cls=DatasetResample,
9149             indexer=indexer,
9150             skipna=skipna,
9151             closed=closed,
9152             label=label,
9153             base=base,
9154             keep_attrs=keep_attrs,
9155             loffset=loffset,
9156             restore_coord_dims=restore_coord_dims,
9157             **indexer_kwargs,
9158         )

</code><intended_behavior>For both 'set_index' and 'reset_index', there should be clear, additional logic to verify and update '_coord_names' appropriately post-operation. It involves adding or removing coordinate names based on how indices are restructured. If a coordinate is promoted to an index or vice versa, '_coord_names' should be updated accordingly to maintain accuracy.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>_stack_once</method>
<code>
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )

</code><intended_behavior>Given that stacking can significantly alter the dataset's structure by combining multiple coordinates into a single new one, this method should include a final step where '_coord_names' is revisited. Any coordinates that have been combined into a new multiindex should have their names removed from '_coord_names', and the new coordinate name should be added. This would prevent the inconsistency between the physical structure of the dataset and its coordinate metadata, ensuring operations relying on '_coord_names' (like calculating dataset length) remain accurate.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the indexs type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the indexs type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def _rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Also used internally by DataArray so that the warning (if any)
3569         is raised at the right stack level.
3570         """
3571         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3572         for k in name_dict.keys():
3573             if k not in self and k not in self.dims:
3574                 raise ValueError(
3575                     f"cannot rename {k!r} because it is not a "
3576                     "variable or dimension in this dataset"
3577                 )
3578 
3579             create_dim_coord = False
3580             new_k = name_dict[k]
3581 
3582             if k in self.dims and new_k in self._coord_names:
3583                 coord_dims = self._variables[name_dict[k]].dims
3584                 if coord_dims == (k,):
3585                     create_dim_coord = True
3586             elif k in self._coord_names and new_k in self.dims:
3587                 coord_dims = self._variables[k].dims
3588                 if coord_dims == (new_k,):
3589                     create_dim_coord = True
3590 
3591             if create_dim_coord:
3592                 warnings.warn(
3593                     f"rename {k!r} to {name_dict[k]!r} does not create an index "
3594                     "anymore. Try using swap_dims instead or use set_index "
3595                     "after rename to create an indexed coordinate.",
3596                     UserWarning,
3597                     stacklevel=3,
3598                 )
3599 
3600         variables, coord_names, dims, indexes = self._rename_all(
3601             name_dict=name_dict, dims_dict=name_dict
3602         )
3603         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3604 
3605     def rename(
3606         self: T_Dataset,
3607         name_dict: Mapping[Any, Hashable] | None = None,
3608         **names: Hashable,
3609     ) -> T_Dataset:
3610         """Returns a new object with renamed variables, coordinates and dimensions.
3611 
3612         Parameters
3613         ----------
3614         name_dict : dict-like, optional
3615             Dictionary whose keys are current variable, coordinate or dimension names and
3616             whose values are the desired names.
3617         **names : optional
3618             Keyword form of ``name_dict``.
3619             One of name_dict or names must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed variables, coordinates and dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename_vars
3630         Dataset.rename_dims
3631         DataArray.rename
3632         """
3633         return self._rename(name_dict=name_dict, **names)
3634 
3635     def rename_dims(
3636         self: T_Dataset,
3637         dims_dict: Mapping[Any, Hashable] | None = None,
3638         **dims: Hashable,
3639     ) -> T_Dataset:
3640         """Returns a new object with renamed dimensions only.
3641 
3642         Parameters
3643         ----------
3644         dims_dict : dict-like, optional
3645             Dictionary whose keys are current dimension names and
3646             whose values are the desired names. The desired names must
3647             not be the name of an existing dimension or Variable in the Dataset.
3648         **dims : optional
3649             Keyword form of ``dims_dict``.
3650             One of dims_dict or dims must be provided.
3651 
3652         Returns
3653         -------
3654         renamed : Dataset
3655             Dataset with renamed dimensions.
3656 
3657         See Also
3658         --------
3659         Dataset.swap_dims
3660         Dataset.rename
3661         Dataset.rename_vars
3662         DataArray.rename
3663         """
3664         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3665         for k, v in dims_dict.items():
3666             if k not in self.dims:
3667                 raise ValueError(
3668                     f"cannot rename {k!r} because it is not a "
3669                     "dimension in this dataset"
3670                 )
3671             if v in self.dims or v in self:
3672                 raise ValueError(
3673                     f"Cannot rename {k} to {v} because {v} already exists. "
3674                     "Try using swap_dims instead."
3675                 )
3676 
3677         variables, coord_names, sizes, indexes = self._rename_all(
3678             name_dict={}, dims_dict=dims_dict
3679         )
3680         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3681 
3682     def rename_vars(
3683         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3684     ) -> T_Dataset:
3685         """Returns a new object with renamed variables including coordinates
3686 
3687         Parameters
3688         ----------
3689         name_dict : dict-like, optional
3690             Dictionary whose keys are current variable or coordinate names and
3691             whose values are the desired names.
3692         **names : optional
3693             Keyword form of ``name_dict``.
3694             One of name_dict or names must be provided.
3695 
3696         Returns
3697         -------
3698         renamed : Dataset
3699             Dataset with renamed variables including coordinates
3700 
3701         See Also
3702         --------
3703         Dataset.swap_dims
3704         Dataset.rename
3705         Dataset.rename_dims
3706         DataArray.rename
3707         """
3708         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3709         for k in name_dict:
3710             if k not in self:
3711                 raise ValueError(
3712                     f"cannot rename {k!r} because it is not a "
3713                     "variable or coordinate in this dataset"
3714                 )
3715         variables, coord_names, dims, indexes = self._rename_all(
3716             name_dict=name_dict, dims_dict={}
3717         )
3718         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3719 
3720     def swap_dims(
3721         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3722     ) -> T_Dataset:
3723         """Returns a new object with swapped dimensions.
3724 
3725         Parameters
3726         ----------
3727         dims_dict : dict-like
3728             Dictionary whose keys are current dimension names and whose values
3729             are new names.
3730         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3731             The keyword arguments form of ``dims_dict``.
3732             One of dims_dict or dims_kwargs must be provided.
3733 
3734         Returns
3735         -------
3736         swapped : Dataset
3737             Dataset with swapped dimensions.
3738 
3739         Examples
3740         --------
3741         >>> ds = xr.Dataset(
3742         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3743         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3744         ... )
3745         >>> ds
3746         <xarray.Dataset>
3747         Dimensions:  (x: 2)
3748         Coordinates:
3749           * x        (x) <U1 'a' 'b'
3750             y        (x) int64 0 1
3751         Data variables:
3752             a        (x) int64 5 7
3753             b        (x) float64 0.1 2.4
3754 
3755         >>> ds.swap_dims({"x": "y"})
3756         <xarray.Dataset>
3757         Dimensions:  (y: 2)
3758         Coordinates:
3759             x        (y) <U1 'a' 'b'
3760           * y        (y) int64 0 1
3761         Data variables:
3762             a        (y) int64 5 7
3763             b        (y) float64 0.1 2.4
3764 
3765         >>> ds.swap_dims({"x": "z"})
3766         <xarray.Dataset>
3767         Dimensions:  (z: 2)
3768         Coordinates:
3769             x        (z) <U1 'a' 'b'
3770             y        (z) int64 0 1
3771         Dimensions without coordinates: z
3772         Data variables:
3773             a        (z) int64 5 7
3774             b        (z) float64 0.1 2.4
3775 
3776         See Also
3777         --------
3778         Dataset.rename
3779         DataArray.swap_dims
3780         """
3781         # TODO: deprecate this method in favor of a (less confusing)
3782         # rename_dims() method that only renames dimensions.
3783 
3784         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3785         for k, v in dims_dict.items():
3786             if k not in self.dims:
3787                 raise ValueError(
3788                     f"cannot swap from dimension {k!r} because it is "
3789                     "not an existing dimension"
3790                 )
3791             if v in self.variables and self.variables[v].dims != (k,):
3792                 raise ValueError(
3793                     f"replacement dimension {v!r} is not a 1D "
3794                     f"variable along the old dimension {k!r}"
3795                 )
3796 
3797         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3798 
3799         coord_names = self._coord_names.copy()
3800         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3801 
3802         variables: dict[Hashable, Variable] = {}
3803         indexes: dict[Hashable, Index] = {}
3804         for k, v in self.variables.items():
3805             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3806             var: Variable
3807             if k in result_dims:
3808                 var = v.to_index_variable()
3809                 var.dims = dims
3810                 if k in self._indexes:
3811                     indexes[k] = self._indexes[k]
3812                     variables[k] = var
3813                 else:
3814                     index, index_vars = create_default_index_implicit(var)
3815                     indexes.update({name: index for name in index_vars})
3816                     variables.update(index_vars)
3817                     coord_names.update(index_vars)
3818             else:
3819                 var = v.to_base_variable()
3820                 var.dims = dims
3821                 variables[k] = var
3822 
3823         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3824 
3825     # change type of self and return to T_Dataset once
3826     # https://github.com/python/mypy/issues/12846 is resolved
3827     def expand_dims(
3828         self,
3829         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3830         axis: None | int | Sequence[int] = None,
3831         **dim_kwargs: Any,
3832     ) -> Dataset:
3833         """Return a new object with an additional axis (or axes) inserted at
3834         the corresponding position in the array shape.  The new object is a
3835         view into the underlying array, not a copy.
3836 
3837         If dim is already a scalar coordinate, it will be promoted to a 1D
3838         coordinate consisting of a single value.
3839 
3840         Parameters
3841         ----------
3842         dim : hashable, sequence of hashable, mapping, or None
3843             Dimensions to include on the new variable. If provided as hashable
3844             or sequence of hashable, then dimensions are inserted with length
3845             1. If provided as a mapping, then the keys are the new dimensions
3846             and the values are either integers (giving the length of the new
3847             dimensions) or array-like (giving the coordinates of the new
3848             dimensions).
3849         axis : int, sequence of int, or None, default: None
3850             Axis position(s) where new axis is to be inserted (position(s) on
3851             the result array). If a sequence of integers is passed,
3852             multiple axes are inserted. In this case, dim arguments should be
3853             same length list. If axis=None is passed, all the axes will be
3854             inserted to the start of the result array.
3855         **dim_kwargs : int or sequence or ndarray
3856             The keywords are arbitrary dimensions being inserted and the values
3857             are either the lengths of the new dims (if int is given), or their
3858             coordinates. Note, this is an alternative to passing a dict to the
3859             dim kwarg and will only be used if dim is None.
3860 
3861         Returns
3862         -------
3863         expanded : Dataset
3864             This object, but with additional dimension(s).
3865 
3866         See Also
3867         --------
3868         DataArray.expand_dims
3869         """
3870         if dim is None:
3871             pass
3872         elif isinstance(dim, Mapping):
3873             # We're later going to modify dim in place; don't tamper with
3874             # the input
3875             dim = dict(dim)
3876         elif isinstance(dim, int):
3877             raise TypeError(
3878                 "dim should be hashable or sequence of hashables or mapping"
3879             )
3880         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3881             dim = {dim: 1}
3882         elif isinstance(dim, Sequence):
3883             if len(dim) != len(set(dim)):
3884                 raise ValueError("dims should not contain duplicate values.")
3885             dim = {d: 1 for d in dim}
3886 
3887         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3888         assert isinstance(dim, MutableMapping)
3889 
3890         if axis is None:
3891             axis = list(range(len(dim)))
3892         elif not isinstance(axis, Sequence):
3893             axis = [axis]
3894 
3895         if len(dim) != len(axis):
3896             raise ValueError("lengths of dim and axis should be identical.")
3897         for d in dim:
3898             if d in self.dims:
3899                 raise ValueError(f"Dimension {d} already exists.")
3900             if d in self._variables and not utils.is_scalar(self._variables[d]):
3901                 raise ValueError(
3902                     "{dim} already exists as coordinate or"
3903                     " variable name.".format(dim=d)
3904                 )
3905 
3906         variables: dict[Hashable, Variable] = {}
3907         indexes: dict[Hashable, Index] = dict(self._indexes)
3908         coord_names = self._coord_names.copy()
3909         # If dim is a dict, then ensure that the values are either integers
3910         # or iterables.
3911         for k, v in dim.items():
3912             if hasattr(v, "__iter__"):
3913                 # If the value for the new dimension is an iterable, then
3914                 # save the coordinates to the variables dict, and set the
3915                 # value within the dim dict to the length of the iterable
3916                 # for later use.
3917                 index = PandasIndex(v, k)
3918                 indexes[k] = index
3919                 variables.update(index.create_variables())
3920                 coord_names.add(k)
3921                 dim[k] = variables[k].size
3922             elif isinstance(v, int):
3923                 pass  # Do nothing if the dimensions value is just an int
3924             else:
3925                 raise TypeError(
3926                     "The value of new dimension {k} must be "
3927                     "an iterable or an int".format(k=k)
3928                 )
3929 
3930         for k, v in self._variables.items():
3931             if k not in dim:
3932                 if k in coord_names:  # Do not change coordinates
3933                     variables[k] = v
3934                 else:
3935                     result_ndim = len(v.dims) + len(axis)
3936                     for a in axis:
3937                         if a < -result_ndim or result_ndim - 1 < a:
3938                             raise IndexError(
3939                                 f"Axis {a} of variable {k} is out of bounds of the "
3940                                 f"expanded dimension size {result_ndim}"
3941                             )
3942 
3943                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3944                     if len(axis_pos) != len(set(axis_pos)):
3945                         raise ValueError("axis should not contain duplicate values")
3946                     # We need to sort them to make sure `axis` equals to the
3947                     # axis positions of the result array.
3948                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3949 
3950                     all_dims = list(zip(v.dims, v.shape))
3951                     for d, c in zip_axis_dim:
3952                         all_dims.insert(d, c)
3953                     variables[k] = v.set_dims(dict(all_dims))
3954             else:
3955                 if k not in variables:
3956                     # If dims includes a label of a non-dimension coordinate,
3957                     # it will be promoted to a 1D coordinate with a single value.
3958                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3959                     indexes[k] = index
3960                     variables.update(index_vars)
3961 
3962         return self._replace_with_new_dims(
3963             variables, coord_names=coord_names, indexes=indexes
3964         )
3965 
3966     # change type of self and return to T_Dataset once
3967     # https://github.com/python/mypy/issues/12846 is resolved
3968     def set_index(
3969         self,
3970         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3971         append: bool = False,
3972         **indexes_kwargs: Hashable | Sequence[Hashable],
3973     ) -> Dataset:
3974         """Set Dataset (multi-)indexes using one or more existing coordinates
3975         or variables.
3976 
3977         This legacy method is limited to pandas (multi-)indexes and
3978         1-dimensional "dimension" coordinates. See
3979         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom
3980         Xarray-compatible index from one or more arbitrary coordinates.
3981 
3982         Parameters
3983         ----------
3984         indexes : {dim: index, ...}
3985             Mapping from names matching dimensions and values given
3986             by (lists of) the names of existing coordinates or variables to set
3987             as new (multi-)index.
3988         append : bool, default: False
3989             If True, append the supplied index(es) to the existing index(es).
3990             Otherwise replace the existing index(es) (default).
3991         **indexes_kwargs : optional
3992             The keyword arguments form of ``indexes``.
3993             One of indexes or indexes_kwargs must be provided.
3994 
3995         Returns
3996         -------
3997         obj : Dataset
3998             Another dataset, with this dataset's data but replaced coordinates.
3999 
4000         Examples
4001         --------
4002         >>> arr = xr.DataArray(
4003         ...     data=np.ones((2, 3)),
4004         ...     dims=["x", "y"],
4005         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
4006         ... )
4007         >>> ds = xr.Dataset({"v": arr})
4008         >>> ds
4009         <xarray.Dataset>
4010         Dimensions:  (x: 2, y: 3)
4011         Coordinates:
4012           * x        (x) int64 0 1
4013           * y        (y) int64 0 1 2
4014             a        (x) int64 3 4
4015         Data variables:
4016             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4017         >>> ds.set_index(x="a")
4018         <xarray.Dataset>
4019         Dimensions:  (x: 2, y: 3)
4020         Coordinates:
4021           * x        (x) int64 3 4
4022           * y        (y) int64 0 1 2
4023         Data variables:
4024             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
4025 
4026         See Also
4027         --------
4028         Dataset.reset_index
4029         Dataset.set_xindex
4030         Dataset.swap_dims
4031         """
4032         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
4033 
4034         new_indexes: dict[Hashable, Index] = {}
4035         new_variables: dict[Hashable, Variable] = {}
4036         drop_indexes: set[Hashable] = set()
4037         drop_variables: set[Hashable] = set()
4038         replace_dims: dict[Hashable, Hashable] = {}
4039         all_var_names: set[Hashable] = set()
4040 
4041         for dim, _var_names in dim_coords.items():
4042             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4043                 var_names = [_var_names]
4044             else:
4045                 var_names = list(_var_names)
4046 
4047             invalid_vars = set(var_names) - set(self._variables)
4048             if invalid_vars:
4049                 raise ValueError(
4050                     ", ".join([str(v) for v in invalid_vars])
4051                     + " variable(s) do not exist"
4052                 )
4053 
4054             all_var_names.update(var_names)
4055             drop_variables.update(var_names)
4056 
4057             # drop any pre-existing index involved and its corresponding coordinates
4058             index_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4059             all_index_coord_names = set(index_coord_names)
4060             for k in var_names:
4061                 all_index_coord_names.update(
4062                     self.xindexes.get_all_coords(k, errors="ignore")
4063                 )
4064 
4065             drop_indexes.update(all_index_coord_names)
4066             drop_variables.update(all_index_coord_names)
4067 
4068             if len(var_names) == 1 and (not append or dim not in self._indexes):
4069                 var_name = var_names[0]
4070                 var = self._variables[var_name]
4071                 if var.dims != (dim,):
4072                     raise ValueError(
4073                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4074                         f"variable {var_name!r} that has dimensions {var.dims}"
4075                     )
4076                 idx = PandasIndex.from_variables({dim: var}, options={})
4077                 idx_vars = idx.create_variables({var_name: var})
4078 
4079                 # trick to preserve coordinate order in this case
4080                 if dim in self._coord_names:
4081                     drop_variables.remove(dim)
4082             else:
4083                 if append:
4084                     current_variables = {
4085                         k: self._variables[k] for k in index_coord_names
4086                     }
4087                 else:
4088                     current_variables = {}
4089                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4090                     dim,
4091                     current_variables,
4092                     {k: self._variables[k] for k in var_names},
4093                 )
4094                 for n in idx.index.names:
4095                     replace_dims[n] = dim
4096 
4097             new_indexes.update({k: idx for k in idx_vars})
4098             new_variables.update(idx_vars)
4099 
4100         # re-add deindexed coordinates (convert to base variables)
4101         for k in drop_variables:
4102             if (
4103                 k not in new_variables
4104                 and k not in all_var_names
4105                 and k in self._coord_names
4106             ):
4107                 new_variables[k] = self._variables[k].to_base_variable()
4108 
4109         indexes_: dict[Any, Index] = {
4110             k: v for k, v in self._indexes.items() if k not in drop_indexes
4111         }
4112         indexes_.update(new_indexes)
4113 
4114         variables = {
4115             k: v for k, v in self._variables.items() if k not in drop_variables
4116         }
4117         variables.update(new_variables)
4118 
4119         # update dimensions if necessary, GH: 3512
4120         for k, v in variables.items():
4121             if any(d in replace_dims for d in v.dims):
4122                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4123                 variables[k] = v._replace(dims=new_dims)
4124 
4125         coord_names = self._coord_names - drop_variables | set(new_variables)
4126 
4127         return self._replace_with_new_dims(
4128             variables, coord_names=coord_names, indexes=indexes_
4129         )
4130 
4131     def reset_index(
4132         self: T_Dataset,
4133         dims_or_levels: Hashable | Sequence[Hashable],
4134         drop: bool = False,
4135     ) -> T_Dataset:
4136         """Reset the specified index(es) or multi-index level(s).
4137 
4138         This legacy method is specific to pandas (multi-)indexes and
4139         1-dimensional "dimension" coordinates. See the more generic
4140         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`
4141         method to respectively drop and set pandas or custom indexes for
4142         arbitrary coordinates.
4143 
4144         Parameters
4145         ----------
4146         dims_or_levels : Hashable or Sequence of Hashable
4147             Name(s) of the dimension(s) and/or multi-index level(s) that will
4148             be reset.
4149         drop : bool, default: False
4150             If True, remove the specified indexes and/or multi-index levels
4151             instead of extracting them as new coordinates (default: False).
4152 
4153         Returns
4154         -------
4155         obj : Dataset
4156             Another dataset, with this dataset's data but replaced coordinates.
4157 
4158         See Also
4159         --------
4160         Dataset.set_index
4161         Dataset.set_xindex
4162         Dataset.drop_indexes
4163         """
4164         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4165             dims_or_levels = [dims_or_levels]
4166 
4167         invalid_coords = set(dims_or_levels) - set(self._indexes)
4168         if invalid_coords:
4169             raise ValueError(
4170                 f"{tuple(invalid_coords)} are not coordinates with an index"
4171             )
4172 
4173         drop_indexes: set[Hashable] = set()
4174         drop_variables: set[Hashable] = set()
4175         seen: set[Index] = set()
4176         new_indexes: dict[Hashable, Index] = {}
4177         new_variables: dict[Hashable, Variable] = {}
4178 
4179         def drop_or_convert(var_names):
4180             if drop:
4181                 drop_variables.update(var_names)
4182             else:
4183                 base_vars = {
4184                     k: self._variables[k].to_base_variable() for k in var_names
4185                 }
4186                 new_variables.update(base_vars)
4187 
4188         for name in dims_or_levels:
4189             index = self._indexes[name]
4190 
4191             if index in seen:
4192                 continue
4193             seen.add(index)
4194 
4195             idx_var_names = set(self.xindexes.get_all_coords(name))
4196             drop_indexes.update(idx_var_names)
4197 
4198             if isinstance(index, PandasMultiIndex):
4199                 # special case for pd.MultiIndex
4200                 level_names = index.index.names
4201                 keep_level_vars = {
4202                     k: self._variables[k]
4203                     for k in level_names
4204                     if k not in dims_or_levels
4205                 }
4206 
4207                 if index.dim not in dims_or_levels and keep_level_vars:
4208                     # do not drop the multi-index completely
4209                     # instead replace it by a new (multi-)index with dropped level(s)
4210                     idx = index.keep_levels(keep_level_vars)
4211                     idx_vars = idx.create_variables(keep_level_vars)
4212                     new_indexes.update({k: idx for k in idx_vars})
4213                     new_variables.update(idx_vars)
4214                     if not isinstance(idx, PandasMultiIndex):
4215                         # multi-index reduced to single index
4216                         # backward compatibility: unique level coordinate renamed to dimension
4217                         drop_variables.update(keep_level_vars)
4218                     drop_or_convert(
4219                         [k for k in level_names if k not in keep_level_vars]
4220                     )
4221                 else:
4222                     # always drop the multi-index dimension variable
4223                     drop_variables.add(index.dim)
4224                     drop_or_convert(level_names)
4225             else:
4226                 drop_or_convert(idx_var_names)
4227 
4228         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4229         indexes.update(new_indexes)
4230 
4231         variables = {
4232             k: v for k, v in self._variables.items() if k not in drop_variables
4233         }
4234         variables.update(new_variables)
4235 
4236         coord_names = self._coord_names - drop_variables
4237 
4238         return self._replace_with_new_dims(
4239             variables, coord_names=coord_names, indexes=indexes
4240         )
4241 
4242     def set_xindex(
4243         self: T_Dataset,
4244         coord_names: str | Sequence[Hashable],
4245         index_cls: type[Index] | None = None,
4246         **options,
4247     ) -> T_Dataset:
4248         """Set a new, Xarray-compatible index from one or more existing
4249         coordinate(s).
4250 
4251         Parameters
4252         ----------
4253         coord_names : str or list
4254             Name(s) of the coordinate(s) used to build the index.
4255             If several names are given, their order matters.
4256         index_cls : subclass of :class:`~xarray.indexes.Index`, optional
4257             The type of index to create. By default, try setting
4258             a ``PandasIndex`` if ``len(coord_names) == 1``,
4259             otherwise a ``PandasMultiIndex``.
4260         **options
4261             Options passed to the index constructor.
4262 
4263         Returns
4264         -------
4265         obj : Dataset
4266             Another dataset, with this dataset's data and with a new index.
4267 
4268         """
4269         # the Sequence check is required for mypy
4270         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):
4271             coord_names = [coord_names]
4272 
4273         if index_cls is None:
4274             if len(coord_names) == 1:
4275                 index_cls = PandasIndex
4276             else:
4277                 index_cls = PandasMultiIndex
4278         else:
4279             if not issubclass(index_cls, Index):
4280                 raise TypeError(f"{index_cls} is not a subclass of xarray.Index")
4281 
4282         invalid_coords = set(coord_names) - self._coord_names
4283 
4284         if invalid_coords:
4285             msg = ["invalid coordinate(s)"]
4286             no_vars = invalid_coords - set(self._variables)
4287             data_vars = invalid_coords - no_vars
4288             if no_vars:
4289                 msg.append(f"those variables don't exist: {no_vars}")
4290             if data_vars:
4291                 msg.append(
4292                     f"those variables are data variables: {data_vars}, use `set_coords` first"
4293                 )
4294             raise ValueError("\n".join(msg))
4295 
4296         # we could be more clever here (e.g., drop-in index replacement if index
4297         # coordinates do not conflict), but let's not allow this for now
4298         indexed_coords = set(coord_names) & set(self._indexes)
4299 
4300         if indexed_coords:
4301             raise ValueError(
4302                 f"those coordinates already have an index: {indexed_coords}"
4303             )
4304 
4305         coord_vars = {name: self._variables[name] for name in coord_names}
4306 
4307         index = index_cls.from_variables(coord_vars, options=options)
4308 
4309         new_coord_vars = index.create_variables(coord_vars)
4310 
4311         # special case for setting a pandas multi-index from level coordinates
4312         # TODO: remove it once we depreciate pandas multi-index dimension (tuple
4313         # elements) coordinate
4314         if isinstance(index, PandasMultiIndex):
4315             coord_names = [index.dim] + list(coord_names)
4316 
4317         variables: dict[Hashable, Variable]
4318         indexes: dict[Hashable, Index]
4319 
4320         if len(coord_names) == 1:
4321             variables = self._variables.copy()
4322             indexes = self._indexes.copy()
4323 
4324             name = list(coord_names).pop()
4325             if name in new_coord_vars:
4326                 variables[name] = new_coord_vars[name]
4327             indexes[name] = index
4328         else:
4329             # reorder variables and indexes so that coordinates having the same
4330             # index are next to each other
4331             variables = {}
4332             for name, var in self._variables.items():
4333                 if name not in coord_names:
4334                     variables[name] = var
4335 
4336             indexes = {}
4337             for name, idx in self._indexes.items():
4338                 if name not in coord_names:
4339                     indexes[name] = idx
4340 
4341             for name in coord_names:
4342                 try:
4343                     variables[name] = new_coord_vars[name]
4344                 except KeyError:
4345                     variables[name] = self._variables[name]
4346                 indexes[name] = index
4347 
4348         return self._replace(
4349             variables=variables,
4350             coord_names=self._coord_names | set(coord_names),
4351             indexes=indexes,
4352         )
4353 
4354     def reorder_levels(
4355         self: T_Dataset,
4356         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4357         **dim_order_kwargs: Sequence[int | Hashable],
4358     ) -> T_Dataset:
4359         """Rearrange index levels using input order.
4360 
4361         Parameters
4362         ----------
4363         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4364             Mapping from names matching dimensions and values given
4365             by lists representing new level orders. Every given dimension
4366             must have a multi-index.
4367         **dim_order_kwargs : Sequence of int or Hashable, optional
4368             The keyword arguments form of ``dim_order``.
4369             One of dim_order or dim_order_kwargs must be provided.
4370 
4371         Returns
4372         -------
4373         obj : Dataset
4374             Another dataset, with this dataset's data but replaced
4375             coordinates.
4376         """
4377         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4378         variables = self._variables.copy()
4379         indexes = dict(self._indexes)
4380         new_indexes: dict[Hashable, Index] = {}
4381         new_variables: dict[Hashable, IndexVariable] = {}
4382 
4383         for dim, order in dim_order.items():
4384             index = self._indexes[dim]
4385 
4386             if not isinstance(index, PandasMultiIndex):
4387                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4388 
4389             level_vars = {k: self._variables[k] for k in order}
4390             idx = index.reorder_levels(level_vars)
4391             idx_vars = idx.create_variables(level_vars)
4392             new_indexes.update({k: idx for k in idx_vars})
4393             new_variables.update(idx_vars)
4394 
4395         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4396         indexes.update(new_indexes)
4397 
4398         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4399         variables.update(new_variables)
4400 
4401         return self._replace(variables, indexes=indexes)
4402 
4403     def _get_stack_index(
4404         self,
4405         dim,
4406         multi=False,
4407         create_index=False,
4408     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4409         """Used by stack and unstack to get one pandas (multi-)index among
4410         the indexed coordinates along dimension `dim`.
4411 
4412         If exactly one index is found, return it with its corresponding
4413         coordinate variables(s), otherwise return None and an empty dict.
4414 
4415         If `create_index=True`, create a new index if none is found or raise
4416         an error if multiple indexes are found.
4417 
4418         """
4419         stack_index: Index | None = None
4420         stack_coords: dict[Hashable, Variable] = {}
4421 
4422         for name, index in self._indexes.items():
4423             var = self._variables[name]
4424             if (
4425                 var.ndim == 1
4426                 and var.dims[0] == dim
4427                 and (
4428                     # stack: must be a single coordinate index
4429                     not multi
4430                     and not self.xindexes.is_multi(name)
4431                     # unstack: must be an index that implements .unstack
4432                     or multi
4433                     and type(index).unstack is not Index.unstack
4434                 )
4435             ):
4436                 if stack_index is not None and index is not stack_index:
4437                     # more than one index found, stop
4438                     if create_index:
4439                         raise ValueError(
4440                             f"cannot stack dimension {dim!r} with `create_index=True` "
4441                             "and with more than one index found along that dimension"
4442                         )
4443                     return None, {}
4444                 stack_index = index
4445                 stack_coords[name] = var
4446 
4447         if create_index and stack_index is None:
4448             if dim in self._variables:
4449                 var = self._variables[dim]
4450             else:
4451                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4452             # dummy index (only `stack_coords` will be used to construct the multi-index)
4453             stack_index = PandasIndex([0], dim)
4454             stack_coords = {dim: var}
4455 
4456         return stack_index, stack_coords
4457 
4458     def _stack_once(
4459         self: T_Dataset,
4460         dims: Sequence[Hashable | Ellipsis],
4461         new_dim: Hashable,
4462         index_cls: type[Index],
4463         create_index: bool | None = True,
4464     ) -> T_Dataset:
4465         if dims == ...:
4466             raise ValueError("Please use [...] for dims, rather than just ...")
4467         if ... in dims:
4468             dims = list(infix_dims(dims, self.dims))
4469 
4470         new_variables: dict[Hashable, Variable] = {}
4471         stacked_var_names: list[Hashable] = []
4472         drop_indexes: list[Hashable] = []
4473 
4474         for name, var in self.variables.items():
4475             if any(d in var.dims for d in dims):
4476                 add_dims = [d for d in dims if d not in var.dims]
4477                 vdims = list(var.dims) + add_dims
4478                 shape = [self.dims[d] for d in vdims]
4479                 exp_var = var.set_dims(vdims, shape)
4480                 stacked_var = exp_var.stack(**{new_dim: dims})
4481                 new_variables[name] = stacked_var
4482                 stacked_var_names.append(name)
4483             else:
4484                 new_variables[name] = var.copy(deep=False)
4485 
4486         # drop indexes of stacked coordinates (if any)
4487         for name in stacked_var_names:
4488             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4489 
4490         new_indexes = {}
4491         new_coord_names = set(self._coord_names)
4492         if create_index or create_index is None:
4493             product_vars: dict[Any, Variable] = {}
4494             for dim in dims:
4495                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4496                 if idx is not None:
4497                     product_vars.update(idx_vars)
4498 
4499             if len(product_vars) == len(dims):
4500                 idx = index_cls.stack(product_vars, new_dim)
4501                 new_indexes[new_dim] = idx
4502                 new_indexes.update({k: idx for k in product_vars})
4503                 idx_vars = idx.create_variables(product_vars)
4504                 # keep consistent multi-index coordinate order
4505                 for k in idx_vars:
4506                     new_variables.pop(k, None)
4507                 new_variables.update(idx_vars)
4508                 new_coord_names.update(idx_vars)
4509 
4510         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4511         indexes.update(new_indexes)
4512 
4513         return self._replace_with_new_dims(
4514             new_variables, coord_names=new_coord_names, indexes=indexes
4515         )
4516 
4517     def stack(
4518         self: T_Dataset,
4519         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4520         create_index: bool | None = True,
4521         index_cls: type[Index] = PandasMultiIndex,
4522         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4523     ) -> T_Dataset:
4524         """
4525         Stack any number of existing dimensions into a single new dimension.
4526 
4527         New dimensions will be added at the end, and by default the corresponding
4528         coordinate variables will be combined into a MultiIndex.
4529 
4530         Parameters
4531         ----------
4532         dimensions : mapping of hashable to sequence of hashable
4533             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4534             dimensions, and the existing dimensions that they replace. An
4535             ellipsis (`...`) will be replaced by all unlisted dimensions.
4536             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4537             all dimensions.
4538         create_index : bool or None, default: True
4539 
4540             - True: create a multi-index for each of the stacked dimensions.
4541             - False: don't create any index.
4542             - None. create a multi-index only if exactly one single (1-d) coordinate
4543               index is found for every dimension to stack.
4544 
4545         index_cls: Index-class, default: PandasMultiIndex
4546             Can be used to pass a custom multi-index type (must be an Xarray index that
4547             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4548         **dimensions_kwargs
4549             The keyword arguments form of ``dimensions``.
4550             One of dimensions or dimensions_kwargs must be provided.
4551 
4552         Returns
4553         -------
4554         stacked : Dataset
4555             Dataset with stacked data.
4556 
4557         See Also
4558         --------
4559         Dataset.unstack
4560         """
4561         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4562         result = self
4563         for new_dim, dims in dimensions.items():
4564             result = result._stack_once(dims, new_dim, index_cls, create_index)
4565         return result
4566 
4567     def to_stacked_array(
4568         self,
4569         new_dim: Hashable,
4570         sample_dims: Collection[Hashable],
4571         variable_dim: Hashable = "variable",
4572         name: Hashable | None = None,
4573     ) -> DataArray:
4574         """Combine variables of differing dimensionality into a DataArray
4575         without broadcasting.
4576 
4577         This method is similar to Dataset.to_array but does not broadcast the
4578         variables.
4579 
4580         Parameters
4581         ----------
4582         new_dim : hashable
4583             Name of the new stacked coordinate
4584         sample_dims : Collection of hashables
4585             List of dimensions that **will not** be stacked. Each array in the
4586             dataset must share these dimensions. For machine learning
4587             applications, these define the dimensions over which samples are
4588             drawn.
4589         variable_dim : hashable, default: "variable"
4590             Name of the level in the stacked coordinate which corresponds to
4591             the variables.
4592         name : hashable, optional
4593             Name of the new data array.
4594 
4595         Returns
4596         -------
4597         stacked : DataArray
4598             DataArray with the specified dimensions and data variables
4599             stacked together. The stacked coordinate is named ``new_dim``
4600             and represented by a MultiIndex object with a level containing the
4601             data variable names. The name of this level is controlled using
4602             the ``variable_dim`` argument.
4603 
4604         See Also
4605         --------
4606         Dataset.to_array
4607         Dataset.stack
4608         DataArray.to_unstacked_dataset
4609 
4610         Examples
4611         --------
4612         >>> data = xr.Dataset(
4613         ...     data_vars={
4614         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4615         ...         "b": ("x", [6, 7]),
4616         ...     },
4617         ...     coords={"y": ["u", "v", "w"]},
4618         ... )
4619 
4620         >>> data
4621         <xarray.Dataset>
4622         Dimensions:  (x: 2, y: 3)
4623         Coordinates:
4624           * y        (y) <U1 'u' 'v' 'w'
4625         Dimensions without coordinates: x
4626         Data variables:
4627             a        (x, y) int64 0 1 2 3 4 5
4628             b        (x) int64 6 7
4629 
4630         >>> data.to_stacked_array("z", sample_dims=["x"])
4631         <xarray.DataArray 'a' (x: 2, z: 4)>
4632         array([[0, 1, 2, 6],
4633                [3, 4, 5, 7]])
4634         Coordinates:
4635           * z         (z) object MultiIndex
4636           * variable  (z) object 'a' 'a' 'a' 'b'
4637           * y         (z) object 'u' 'v' 'w' nan
4638         Dimensions without coordinates: x
4639 
4640         """
4641         from .concat import concat
4642 
4643         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4644 
4645         for variable in self:
4646             dims = self[variable].dims
4647             dims_include_sample_dims = set(sample_dims) <= set(dims)
4648             if not dims_include_sample_dims:
4649                 raise ValueError(
4650                     "All variables in the dataset must contain the "
4651                     "dimensions {}.".format(dims)
4652                 )
4653 
4654         def ensure_stackable(val):
4655             assign_coords = {variable_dim: val.name}
4656             for dim in stacking_dims:
4657                 if dim not in val.dims:
4658                     assign_coords[dim] = None
4659 
4660             expand_dims = set(stacking_dims).difference(set(val.dims))
4661             expand_dims.add(variable_dim)
4662             # must be list for .expand_dims
4663             expand_dims = list(expand_dims)
4664 
4665             return (
4666                 val.assign_coords(**assign_coords)
4667                 .expand_dims(expand_dims)
4668                 .stack({new_dim: (variable_dim,) + stacking_dims})
4669             )
4670 
4671         # concatenate the arrays
4672         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4673         data_array = concat(stackable_vars, dim=new_dim)
4674 
4675         if name is not None:
4676             data_array.name = name
4677 
4678         return data_array
4679 
4680     def _unstack_once(
4681         self: T_Dataset,
4682         dim: Hashable,
4683         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4684         fill_value,
4685         sparse: bool = False,
4686     ) -> T_Dataset:
4687         index, index_vars = index_and_vars
4688         variables: dict[Hashable, Variable] = {}
4689         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4690 
4691         new_indexes, clean_index = index.unstack()
4692         indexes.update(new_indexes)
4693 
4694         for name, idx in new_indexes.items():
4695             variables.update(idx.create_variables(index_vars))
4696 
4697         for name, var in self.variables.items():
4698             if name not in index_vars:
4699                 if dim in var.dims:
4700                     if isinstance(fill_value, Mapping):
4701                         fill_value_ = fill_value[name]
4702                     else:
4703                         fill_value_ = fill_value
4704 
4705                     variables[name] = var._unstack_once(
4706                         index=clean_index,
4707                         dim=dim,
4708                         fill_value=fill_value_,
4709                         sparse=sparse,
4710                     )
4711                 else:
4712                     variables[name] = var
4713 
4714         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4715 
4716         return self._replace_with_new_dims(
4717             variables, coord_names=coord_names, indexes=indexes
4718         )
4719 
4720     def _unstack_full_reindex(
4721         self: T_Dataset,
4722         dim: Hashable,
4723         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4724         fill_value,
4725         sparse: bool,
4726     ) -> T_Dataset:
4727         index, index_vars = index_and_vars
4728         variables: dict[Hashable, Variable] = {}
4729         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4730 
4731         new_indexes, clean_index = index.unstack()
4732         indexes.update(new_indexes)
4733 
4734         new_index_variables = {}
4735         for name, idx in new_indexes.items():
4736             new_index_variables.update(idx.create_variables(index_vars))
4737 
4738         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4739         variables.update(new_index_variables)
4740 
4741         # take a shortcut in case the MultiIndex was not modified.
4742         full_idx = pd.MultiIndex.from_product(
4743             clean_index.levels, names=clean_index.names
4744         )
4745         if clean_index.equals(full_idx):
4746             obj = self
4747         else:
4748             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4749             xr_full_idx = PandasMultiIndex(full_idx, dim)
4750             indexers = Indexes(
4751                 {k: xr_full_idx for k in index_vars},
4752                 xr_full_idx.create_variables(index_vars),
4753             )
4754             obj = self._reindex(
4755                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4756             )
4757 
4758         for name, var in obj.variables.items():
4759             if name not in index_vars:
4760                 if dim in var.dims:
4761                     variables[name] = var.unstack({dim: new_dim_sizes})
4762                 else:
4763                     variables[name] = var
4764 
4765         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4766 
4767         return self._replace_with_new_dims(
4768             variables, coord_names=coord_names, indexes=indexes
4769         )
4770 
4771     def unstack(
4772         self: T_Dataset,
4773         dim: Hashable | Iterable[Hashable] | None = None,
4774         fill_value: Any = xrdtypes.NA,
4775         sparse: bool = False,
4776     ) -> T_Dataset:
4777         """
4778         Unstack existing dimensions corresponding to MultiIndexes into
4779         multiple new dimensions.
4780 
4781         New dimensions will be added at the end.
4782 
4783         Parameters
4784         ----------
4785         dim : hashable or iterable of hashable, optional
4786             Dimension(s) over which to unstack. By default unstacks all
4787             MultiIndexes.
4788         fill_value : scalar or dict-like, default: nan
4789             value to be filled. If a dict-like, maps variable names to
4790             fill values. If not provided or if the dict-like does not
4791             contain all variables, the dtype's NA value will be used.
4792         sparse : bool, default: False
4793             use sparse-array if True
4794 
4795         Returns
4796         -------
4797         unstacked : Dataset
4798             Dataset with unstacked data.
4799 
4800         See Also
4801         --------
4802         Dataset.stack
4803         """
4804 
4805         if dim is None:
4806             dims = list(self.dims)
4807         else:
4808             if isinstance(dim, str) or not isinstance(dim, Iterable):
4809                 dims = [dim]
4810             else:
4811                 dims = list(dim)
4812 
4813             missing_dims = [d for d in dims if d not in self.dims]
4814             if missing_dims:
4815                 raise ValueError(
4816                     f"Dataset does not contain the dimensions: {missing_dims}"
4817                 )
4818 
4819         # each specified dimension must have exactly one multi-index
4820         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4821         for d in dims:
4822             idx, idx_vars = self._get_stack_index(d, multi=True)
4823             if idx is not None:
4824                 stacked_indexes[d] = idx, idx_vars
4825 
4826         if dim is None:
4827             dims = list(stacked_indexes)
4828         else:
4829             non_multi_dims = set(dims) - set(stacked_indexes)
4830             if non_multi_dims:
4831                 raise ValueError(
4832                     "cannot unstack dimensions that do not "
4833                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4834                 )
4835 
4836         result = self.copy(deep=False)
4837 
4838         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4839         # so we can't just access self.variables[v].data for every variable.
4840         # We only check the non-index variables.
4841         # https://github.com/pydata/xarray/issues/5902
4842         nonindexes = [
4843             self.variables[k] for k in set(self.variables) - set(self._indexes)
4844         ]
4845         # Notes for each of these cases:
4846         # 1. Dask arrays don't support assignment by index, which the fast unstack
4847         #    function requires.
4848         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4849         # 2. Sparse doesn't currently support (though we could special-case it)
4850         #    https://github.com/pydata/sparse/issues/422
4851         # 3. pint requires checking if it's a NumPy array until
4852         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4853         #    Once that is resolved, explicitly exclude pint arrays.
4854         #    pint doesn't implement `np.full_like` in a way that's
4855         #    currently compatible.
4856         needs_full_reindex = any(
4857             is_duck_dask_array(v.data)
4858             or isinstance(v.data, sparse_array_type)
4859             or not isinstance(v.data, np.ndarray)
4860             for v in nonindexes
4861         )
4862 
4863         for dim in dims:
4864             if needs_full_reindex:
4865                 result = result._unstack_full_reindex(
4866                     dim, stacked_indexes[dim], fill_value, sparse
4867                 )
4868             else:
4869                 result = result._unstack_once(
4870                     dim, stacked_indexes[dim], fill_value, sparse
4871                 )
4872         return result
4873 
4874     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4875         """Update this dataset's variables with those from another dataset.
4876 
4877         Just like :py:meth:`dict.update` this is a in-place operation.
4878         For a non-inplace version, see :py:meth:`Dataset.merge`.
4879 
4880         Parameters
4881         ----------
4882         other : Dataset or mapping
4883             Variables with which to update this dataset. One of:
4884 
4885             - Dataset
4886             - mapping {var name: DataArray}
4887             - mapping {var name: Variable}
4888             - mapping {var name: (dimension name, array-like)}
4889             - mapping {var name: (tuple of dimension names, array-like)}
4890 
4891         Returns
4892         -------
4893         updated : Dataset
4894             Updated dataset. Note that since the update is in-place this is the input
4895             dataset.
4896 
4897             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4898 
4899         Raises
4900         ------
4901         ValueError
4902             If any dimensions would have inconsistent sizes in the updated
4903             dataset.
4904 
4905         See Also
4906         --------
4907         Dataset.assign
4908         Dataset.merge
4909         """
4910         merge_result = dataset_update_method(self, other)
4911         return self._replace(inplace=True, **merge_result._asdict())
4912 
4913     def merge(
4914         self: T_Dataset,
4915         other: CoercibleMapping | DataArray,
4916         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4917         compat: CompatOptions = "no_conflicts",
4918         join: JoinOptions = "outer",
4919         fill_value: Any = xrdtypes.NA,
4920         combine_attrs: CombineAttrsOptions = "override",
4921     ) -> T_Dataset:
4922         """Merge the arrays of two datasets into a single dataset.
4923 
4924         This method generally does not allow for overriding data, with the
4925         exception of attributes, which are ignored on the second dataset.
4926         Variables with the same name are checked for conflicts via the equals
4927         or identical methods.
4928 
4929         Parameters
4930         ----------
4931         other : Dataset or mapping
4932             Dataset or variables to merge with this dataset.
4933         overwrite_vars : hashable or iterable of hashable, optional
4934             If provided, update variables of these name(s) without checking for
4935             conflicts in this dataset.
4936         compat : {"identical", "equals", "broadcast_equals", \
4937                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4938             String indicating how to compare variables of the same name for
4939             potential conflicts:
4940 
4941             - 'identical': all values, dimensions and attributes must be the
4942               same.
4943             - 'equals': all values and dimensions must be the same.
4944             - 'broadcast_equals': all values must be equal when variables are
4945               broadcast against each other to ensure common dimensions.
4946             - 'no_conflicts': only values which are not null in both datasets
4947               must be equal. The returned dataset then contains the combination
4948               of all non-null values.
4949             - 'override': skip comparing and pick variable from first dataset
4950             - 'minimal': drop conflicting coordinates
4951 
4952         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4953                default: "outer"
4954             Method for joining ``self`` and ``other`` along shared dimensions:
4955 
4956             - 'outer': use the union of the indexes
4957             - 'inner': use the intersection of the indexes
4958             - 'left': use indexes from ``self``
4959             - 'right': use indexes from ``other``
4960             - 'exact': error instead of aligning non-equal indexes
4961             - 'override': use indexes from ``self`` that are the same size
4962               as those of ``other`` in that dimension
4963 
4964         fill_value : scalar or dict-like, optional
4965             Value to use for newly missing values. If a dict-like, maps
4966             variable names (including coordinates) to fill values.
4967         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4968                          "override"} or callable, default: "override"
4969             A callable or a string indicating how to combine attrs of the objects being
4970             merged:
4971 
4972             - "drop": empty attrs on returned Dataset.
4973             - "identical": all attrs must be the same on every object.
4974             - "no_conflicts": attrs from all objects are combined, any that have
4975               the same name must also have the same value.
4976             - "drop_conflicts": attrs from all objects are combined, any that have
4977               the same name but different values are dropped.
4978             - "override": skip comparing and copy attrs from the first dataset to
4979               the result.
4980 
4981             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4982             as its only parameters.
4983 
4984         Returns
4985         -------
4986         merged : Dataset
4987             Merged dataset.
4988 
4989         Raises
4990         ------
4991         MergeError
4992             If any variables conflict (see ``compat``).
4993 
4994         See Also
4995         --------
4996         Dataset.update
4997         """
4998         from .dataarray import DataArray
4999 
5000         other = other.to_dataset() if isinstance(other, DataArray) else other
5001         merge_result = dataset_merge_method(
5002             self,
5003             other,
5004             overwrite_vars=overwrite_vars,
5005             compat=compat,
5006             join=join,
5007             fill_value=fill_value,
5008             combine_attrs=combine_attrs,
5009         )
5010         return self._replace(**merge_result._asdict())
5011 
5012     def _assert_all_in_dataset(
5013         self, names: Iterable[Hashable], virtual_okay: bool = False
5014     ) -> None:
5015         bad_names = set(names) - set(self._variables)
5016         if virtual_okay:
5017             bad_names -= self.virtual_variables
5018         if bad_names:
5019             raise ValueError(
5020                 "One or more of the specified variables "
5021                 "cannot be found in this dataset"
5022             )
5023 
5024     def drop_vars(
5025         self: T_Dataset,
5026         names: Hashable | Iterable[Hashable],
5027         *,
5028         errors: ErrorOptions = "raise",
5029     ) -> T_Dataset:
5030         """Drop variables from this dataset.
5031 
5032         Parameters
5033         ----------
5034         names : hashable or iterable of hashable
5035             Name(s) of variables to drop.
5036         errors : {"raise", "ignore"}, default: "raise"
5037             If 'raise', raises a ValueError error if any of the variable
5038             passed are not in the dataset. If 'ignore', any given names that are in the
5039             dataset are dropped and no error is raised.
5040 
5041         Returns
5042         -------
5043         dropped : Dataset
5044 
5045         """
5046         # the Iterable check is required for mypy
5047         if is_scalar(names) or not isinstance(names, Iterable):
5048             names = {names}
5049         else:
5050             names = set(names)
5051         if errors == "raise":
5052             self._assert_all_in_dataset(names)
5053 
5054         # GH6505
5055         other_names = set()
5056         for var in names:
5057             maybe_midx = self._indexes.get(var, None)
5058             if isinstance(maybe_midx, PandasMultiIndex):
5059                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
5060                 idx_other_names = idx_coord_names - set(names)
5061                 other_names.update(idx_other_names)
5062         if other_names:
5063             names |= set(other_names)
5064             warnings.warn(
5065                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
5066                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
5067                 DeprecationWarning,
5068                 stacklevel=2,
5069             )
5070 
5071         assert_no_index_corrupted(self.xindexes, names)
5072 
5073         variables = {k: v for k, v in self._variables.items() if k not in names}
5074         coord_names = {k for k in self._coord_names if k in variables}
5075         indexes = {k: v for k, v in self._indexes.items() if k not in names}
5076         return self._replace_with_new_dims(
5077             variables, coord_names=coord_names, indexes=indexes
5078         )
5079 
5080     def drop_indexes(
5081         self: T_Dataset,
5082         coord_names: Hashable | Iterable[Hashable],
5083         *,
5084         errors: ErrorOptions = "raise",
5085     ) -> T_Dataset:
5086         """Drop the indexes assigned to the given coordinates.
5087 
5088         Parameters
5089         ----------
5090         coord_names : hashable or iterable of hashable
5091             Name(s) of the coordinate(s) for which to drop the index.
5092         errors : {"raise", "ignore"}, default: "raise"
5093             If 'raise', raises a ValueError error if any of the coordinates
5094             passed have no index or are not in the dataset.
5095             If 'ignore', no error is raised.
5096 
5097         Returns
5098         -------
5099         dropped : Dataset
5100             A new dataset with dropped indexes.
5101 
5102         """
5103         # the Iterable check is required for mypy
5104         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):
5105             coord_names = {coord_names}
5106         else:
5107             coord_names = set(coord_names)
5108 
5109         if errors == "raise":
5110             invalid_coords = coord_names - self._coord_names
5111             if invalid_coords:
5112                 raise ValueError(f"those coordinates don't exist: {invalid_coords}")
5113 
5114             unindexed_coords = set(coord_names) - set(self._indexes)
5115             if unindexed_coords:
5116                 raise ValueError(
5117                     f"those coordinates do not have an index: {unindexed_coords}"
5118                 )
5119 
5120         assert_no_index_corrupted(self.xindexes, coord_names, action="remove index(es)")
5121 
5122         variables = {}
5123         for name, var in self._variables.items():
5124             if name in coord_names:
5125                 variables[name] = var.to_base_variable()
5126             else:
5127                 variables[name] = var
5128 
5129         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}
5130 
5131         return self._replace(variables=variables, indexes=indexes)
5132 
5133     def drop(
5134         self: T_Dataset,
5135         labels=None,
5136         dim=None,
5137         *,
5138         errors: ErrorOptions = "raise",
5139         **labels_kwargs,
5140     ) -> T_Dataset:
5141         """Backward compatible method based on `drop_vars` and `drop_sel`
5142 
5143         Using either `drop_vars` or `drop_sel` is encouraged
5144 
5145         See Also
5146         --------
5147         Dataset.drop_vars
5148         Dataset.drop_sel
5149         """
5150         if errors not in ["raise", "ignore"]:
5151             raise ValueError('errors must be either "raise" or "ignore"')
5152 
5153         if is_dict_like(labels) and not isinstance(labels, dict):
5154             warnings.warn(
5155                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
5156                 FutureWarning,
5157                 stacklevel=2,
5158             )
5159             return self.drop_vars(labels, errors=errors)
5160 
5161         if labels_kwargs or isinstance(labels, dict):
5162             if dim is not None:
5163                 raise ValueError("cannot specify dim and dict-like arguments.")
5164             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
5165 
5166         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
5167             warnings.warn(
5168                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
5169                 PendingDeprecationWarning,
5170                 stacklevel=2,
5171             )
5172             return self.drop_vars(labels, errors=errors)
5173         if dim is not None:
5174             warnings.warn(
5175                 "dropping labels using list-like labels is deprecated; using "
5176                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
5177                 DeprecationWarning,
5178                 stacklevel=2,
5179             )
5180             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
5181 
5182         warnings.warn(
5183             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
5184             PendingDeprecationWarning,
5185             stacklevel=2,
5186         )
5187         return self.drop_sel(labels, errors=errors)
5188 
5189     def drop_sel(
5190         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
5191     ) -> T_Dataset:
5192         """Drop index labels from this dataset.
5193 
5194         Parameters
5195         ----------
5196         labels : mapping of hashable to Any
5197             Index labels to drop
5198         errors : {"raise", "ignore"}, default: "raise"
5199             If 'raise', raises a ValueError error if
5200             any of the index labels passed are not
5201             in the dataset. If 'ignore', any given labels that are in the
5202             dataset are dropped and no error is raised.
5203         **labels_kwargs : {dim: label, ...}, optional
5204             The keyword arguments form of ``dim`` and ``labels``
5205 
5206         Returns
5207         -------
5208         dropped : Dataset
5209 
5210         Examples
5211         --------
5212         >>> data = np.arange(6).reshape(2, 3)
5213         >>> labels = ["a", "b", "c"]
5214         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5215         >>> ds
5216         <xarray.Dataset>
5217         Dimensions:  (x: 2, y: 3)
5218         Coordinates:
5219           * y        (y) <U1 'a' 'b' 'c'
5220         Dimensions without coordinates: x
5221         Data variables:
5222             A        (x, y) int64 0 1 2 3 4 5
5223         >>> ds.drop_sel(y=["a", "c"])
5224         <xarray.Dataset>
5225         Dimensions:  (x: 2, y: 1)
5226         Coordinates:
5227           * y        (y) <U1 'b'
5228         Dimensions without coordinates: x
5229         Data variables:
5230             A        (x, y) int64 1 4
5231         >>> ds.drop_sel(y="b")
5232         <xarray.Dataset>
5233         Dimensions:  (x: 2, y: 2)
5234         Coordinates:
5235           * y        (y) <U1 'a' 'c'
5236         Dimensions without coordinates: x
5237         Data variables:
5238             A        (x, y) int64 0 2 3 5
5239         """
5240         if errors not in ["raise", "ignore"]:
5241             raise ValueError('errors must be either "raise" or "ignore"')
5242 
5243         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
5244 
5245         ds = self
5246         for dim, labels_for_dim in labels.items():
5247             # Don't cast to set, as it would harm performance when labels
5248             # is a large numpy array
5249             if utils.is_scalar(labels_for_dim):
5250                 labels_for_dim = [labels_for_dim]
5251             labels_for_dim = np.asarray(labels_for_dim)
5252             try:
5253                 index = self.get_index(dim)
5254             except KeyError:
5255                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5256             new_index = index.drop(labels_for_dim, errors=errors)
5257             ds = ds.loc[{dim: new_index}]
5258         return ds
5259 
5260     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5261         """Drop index positions from this Dataset.
5262 
5263         Parameters
5264         ----------
5265         indexers : mapping of hashable to Any
5266             Index locations to drop
5267         **indexers_kwargs : {dim: position, ...}, optional
5268             The keyword arguments form of ``dim`` and ``positions``
5269 
5270         Returns
5271         -------
5272         dropped : Dataset
5273 
5274         Raises
5275         ------
5276         IndexError
5277 
5278         Examples
5279         --------
5280         >>> data = np.arange(6).reshape(2, 3)
5281         >>> labels = ["a", "b", "c"]
5282         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5283         >>> ds
5284         <xarray.Dataset>
5285         Dimensions:  (x: 2, y: 3)
5286         Coordinates:
5287           * y        (y) <U1 'a' 'b' 'c'
5288         Dimensions without coordinates: x
5289         Data variables:
5290             A        (x, y) int64 0 1 2 3 4 5
5291         >>> ds.drop_isel(y=[0, 2])
5292         <xarray.Dataset>
5293         Dimensions:  (x: 2, y: 1)
5294         Coordinates:
5295           * y        (y) <U1 'b'
5296         Dimensions without coordinates: x
5297         Data variables:
5298             A        (x, y) int64 1 4
5299         >>> ds.drop_isel(y=1)
5300         <xarray.Dataset>
5301         Dimensions:  (x: 2, y: 2)
5302         Coordinates:
5303           * y        (y) <U1 'a' 'c'
5304         Dimensions without coordinates: x
5305         Data variables:
5306             A        (x, y) int64 0 2 3 5
5307         """
5308 
5309         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5310 
5311         ds = self
5312         dimension_index = {}
5313         for dim, pos_for_dim in indexers.items():
5314             # Don't cast to set, as it would harm performance when labels
5315             # is a large numpy array
5316             if utils.is_scalar(pos_for_dim):
5317                 pos_for_dim = [pos_for_dim]
5318             pos_for_dim = np.asarray(pos_for_dim)
5319             index = self.get_index(dim)
5320             new_index = index.delete(pos_for_dim)
5321             dimension_index[dim] = new_index
5322         ds = ds.loc[dimension_index]
5323         return ds
5324 
5325     def drop_dims(
5326         self: T_Dataset,
5327         drop_dims: Hashable | Iterable[Hashable],
5328         *,
5329         errors: ErrorOptions = "raise",
5330     ) -> T_Dataset:
5331         """Drop dimensions and associated variables from this dataset.
5332 
5333         Parameters
5334         ----------
5335         drop_dims : hashable or iterable of hashable
5336             Dimension or dimensions to drop.
5337         errors : {"raise", "ignore"}, default: "raise"
5338             If 'raise', raises a ValueError error if any of the
5339             dimensions passed are not in the dataset. If 'ignore', any given
5340             dimensions that are in the dataset are dropped and no error is raised.
5341 
5342         Returns
5343         -------
5344         obj : Dataset
5345             The dataset without the given dimensions (or any variables
5346             containing those dimensions).
5347         """
5348         if errors not in ["raise", "ignore"]:
5349             raise ValueError('errors must be either "raise" or "ignore"')
5350 
5351         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5352             drop_dims = {drop_dims}
5353         else:
5354             drop_dims = set(drop_dims)
5355 
5356         if errors == "raise":
5357             missing_dims = drop_dims - set(self.dims)
5358             if missing_dims:
5359                 raise ValueError(
5360                     f"Dataset does not contain the dimensions: {missing_dims}"
5361                 )
5362 
5363         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5364         return self.drop_vars(drop_vars)
5365 
5366     def transpose(
5367         self: T_Dataset,
5368         *dims: Hashable,
5369         missing_dims: ErrorOptionsWithWarn = "raise",
5370     ) -> T_Dataset:
5371         """Return a new Dataset object with all array dimensions transposed.
5372 
5373         Although the order of dimensions on each array will change, the dataset
5374         dimensions themselves will remain in fixed (sorted) order.
5375 
5376         Parameters
5377         ----------
5378         *dims : hashable, optional
5379             By default, reverse the dimensions on each array. Otherwise,
5380             reorder the dimensions to this order.
5381         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5382             What to do if dimensions that should be selected from are not present in the
5383             Dataset:
5384             - "raise": raise an exception
5385             - "warn": raise a warning, and ignore the missing dimensions
5386             - "ignore": ignore the missing dimensions
5387 
5388         Returns
5389         -------
5390         transposed : Dataset
5391             Each array in the dataset (including) coordinates will be
5392             transposed to the given order.
5393 
5394         Notes
5395         -----
5396         This operation returns a view of each array's data. It is
5397         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5398         -- the data will be fully loaded into memory.
5399 
5400         See Also
5401         --------
5402         numpy.transpose
5403         DataArray.transpose
5404         """
5405         # Use infix_dims to check once for missing dimensions
5406         if len(dims) != 0:
5407             _ = list(infix_dims(dims, self.dims, missing_dims))
5408 
5409         ds = self.copy()
5410         for name, var in self._variables.items():
5411             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5412             ds._variables[name] = var.transpose(*var_dims)
5413         return ds
5414 
5415     def dropna(
5416         self: T_Dataset,
5417         dim: Hashable,
5418         how: Literal["any", "all"] = "any",
5419         thresh: int | None = None,
5420         subset: Iterable[Hashable] | None = None,
5421     ) -> T_Dataset:
5422         """Returns a new dataset with dropped labels for missing values along
5423         the provided dimension.
5424 
5425         Parameters
5426         ----------
5427         dim : hashable
5428             Dimension along which to drop missing values. Dropping along
5429             multiple dimensions simultaneously is not yet supported.
5430         how : {"any", "all"}, default: "any"
5431             - any : if any NA values are present, drop that label
5432             - all : if all values are NA, drop that label
5433 
5434         thresh : int or None, optional
5435             If supplied, require this many non-NA values.
5436         subset : iterable of hashable or None, optional
5437             Which variables to check for missing values. By default, all
5438             variables in the dataset are checked.
5439 
5440         Returns
5441         -------
5442         Dataset
5443         """
5444         # TODO: consider supporting multiple dimensions? Or not, given that
5445         # there are some ugly edge cases, e.g., pandas's dropna differs
5446         # depending on the order of the supplied axes.
5447 
5448         if dim not in self.dims:
5449             raise ValueError(f"{dim} must be a single dataset dimension")
5450 
5451         if subset is None:
5452             subset = iter(self.data_vars)
5453 
5454         count = np.zeros(self.dims[dim], dtype=np.int64)
5455         size = np.int_(0)  # for type checking
5456 
5457         for k in subset:
5458             array = self._variables[k]
5459             if dim in array.dims:
5460                 dims = [d for d in array.dims if d != dim]
5461                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5462                 size += math.prod([self.dims[d] for d in dims])
5463 
5464         if thresh is not None:
5465             mask = count >= thresh
5466         elif how == "any":
5467             mask = count == size
5468         elif how == "all":
5469             mask = count > 0
5470         elif how is not None:
5471             raise ValueError(f"invalid how option: {how}")
5472         else:
5473             raise TypeError("must specify how or thresh")
5474 
5475         return self.isel({dim: mask})
5476 
5477     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5478         """Fill missing values in this object.
5479 
5480         This operation follows the normal broadcasting and alignment rules that
5481         xarray uses for binary arithmetic, except the result is aligned to this
5482         object (``join='left'``) instead of aligned to the intersection of
5483         index coordinates (``join='inner'``).
5484 
5485         Parameters
5486         ----------
5487         value : scalar, ndarray, DataArray, dict or Dataset
5488             Used to fill all matching missing values in this dataset's data
5489             variables. Scalars, ndarrays or DataArrays arguments are used to
5490             fill all data with aligned coordinates (for DataArrays).
5491             Dictionaries or datasets match data variables and then align
5492             coordinates if necessary.
5493 
5494         Returns
5495         -------
5496         Dataset
5497 
5498         Examples
5499         --------
5500         >>> ds = xr.Dataset(
5501         ...     {
5502         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5503         ...         "B": ("x", [3, 4, np.nan, 1]),
5504         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5505         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5506         ...     },
5507         ...     coords={"x": [0, 1, 2, 3]},
5508         ... )
5509         >>> ds
5510         <xarray.Dataset>
5511         Dimensions:  (x: 4)
5512         Coordinates:
5513           * x        (x) int64 0 1 2 3
5514         Data variables:
5515             A        (x) float64 nan 2.0 nan 0.0
5516             B        (x) float64 3.0 4.0 nan 1.0
5517             C        (x) float64 nan nan nan 5.0
5518             D        (x) float64 nan 3.0 nan 4.0
5519 
5520         Replace all `NaN` values with 0s.
5521 
5522         >>> ds.fillna(0)
5523         <xarray.Dataset>
5524         Dimensions:  (x: 4)
5525         Coordinates:
5526           * x        (x) int64 0 1 2 3
5527         Data variables:
5528             A        (x) float64 0.0 2.0 0.0 0.0
5529             B        (x) float64 3.0 4.0 0.0 1.0
5530             C        (x) float64 0.0 0.0 0.0 5.0
5531             D        (x) float64 0.0 3.0 0.0 4.0
5532 
5533         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
5534 
5535         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5536         >>> ds.fillna(value=values)
5537         <xarray.Dataset>
5538         Dimensions:  (x: 4)
5539         Coordinates:
5540           * x        (x) int64 0 1 2 3
5541         Data variables:
5542             A        (x) float64 0.0 2.0 0.0 0.0
5543             B        (x) float64 3.0 4.0 1.0 1.0
5544             C        (x) float64 2.0 2.0 2.0 5.0
5545             D        (x) float64 3.0 3.0 3.0 4.0
5546         """
5547         if utils.is_dict_like(value):
5548             value_keys = getattr(value, "data_vars", value).keys()
5549             if not set(value_keys) <= set(self.data_vars.keys()):
5550                 raise ValueError(
5551                     "all variables in the argument to `fillna` "
5552                     "must be contained in the original dataset"
5553                 )
5554         out = ops.fillna(self, value)
5555         return out
5556 
5557     def interpolate_na(
5558         self: T_Dataset,
5559         dim: Hashable | None = None,
5560         method: InterpOptions = "linear",
5561         limit: int = None,
5562         use_coordinate: bool | Hashable = True,
5563         max_gap: (
5564             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5565         ) = None,
5566         **kwargs: Any,
5567     ) -> T_Dataset:
5568         """Fill in NaNs by interpolating according to different methods.
5569 
5570         Parameters
5571         ----------
5572         dim : Hashable or None, optional
5573             Specifies the dimension along which to interpolate.
5574         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5575             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5576             String indicating which method to use for interpolation:
5577 
5578             - 'linear': linear interpolation. Additional keyword
5579               arguments are passed to :py:func:`numpy.interp`
5580             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5581               are passed to :py:func:`scipy.interpolate.interp1d`. If
5582               ``method='polynomial'``, the ``order`` keyword argument must also be
5583               provided.
5584             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5585               respective :py:class:`scipy.interpolate` classes.
5586 
5587         use_coordinate : bool or Hashable, default: True
5588             Specifies which index to use as the x values in the interpolation
5589             formulated as `y = f(x)`. If False, values are treated as if
5590             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5591             used. If ``use_coordinate`` is a string, it specifies the name of a
5592             coordinate variariable to use as the index.
5593         limit : int, default: None
5594             Maximum number of consecutive NaNs to fill. Must be greater than 0
5595             or None for no limit. This filling is done regardless of the size of
5596             the gap in the data. To only interpolate over gaps less than a given length,
5597             see ``max_gap``.
5598         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5599             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5600             Use None for no limit. When interpolating along a datetime64 dimension
5601             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5602 
5603             - a string that is valid input for pandas.to_timedelta
5604             - a :py:class:`numpy.timedelta64` object
5605             - a :py:class:`pandas.Timedelta` object
5606             - a :py:class:`datetime.timedelta` object
5607 
5608             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5609             dimensions has not been implemented yet. Gap length is defined as the difference
5610             between coordinate values at the first data point after a gap and the last value
5611             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5612             between coordinate values at the first (last) valid data point and the first (last) NaN.
5613             For example, consider::
5614 
5615                 <xarray.DataArray (x: 9)>
5616                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5617                 Coordinates:
5618                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5619 
5620             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5621         **kwargs : dict, optional
5622             parameters passed verbatim to the underlying interpolation function
5623 
5624         Returns
5625         -------
5626         interpolated: Dataset
5627             Filled in Dataset.
5628 
5629         See Also
5630         --------
5631         numpy.interp
5632         scipy.interpolate
5633 
5634         Examples
5635         --------
5636         >>> ds = xr.Dataset(
5637         ...     {
5638         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5639         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5640         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5641         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5642         ...     },
5643         ...     coords={"x": [0, 1, 2, 3, 4]},
5644         ... )
5645         >>> ds
5646         <xarray.Dataset>
5647         Dimensions:  (x: 5)
5648         Coordinates:
5649           * x        (x) int64 0 1 2 3 4
5650         Data variables:
5651             A        (x) float64 nan 2.0 3.0 nan 0.0
5652             B        (x) float64 3.0 4.0 nan 1.0 7.0
5653             C        (x) float64 nan nan nan 5.0 0.0
5654             D        (x) float64 nan 3.0 nan -1.0 4.0
5655 
5656         >>> ds.interpolate_na(dim="x", method="linear")
5657         <xarray.Dataset>
5658         Dimensions:  (x: 5)
5659         Coordinates:
5660           * x        (x) int64 0 1 2 3 4
5661         Data variables:
5662             A        (x) float64 nan 2.0 3.0 1.5 0.0
5663             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5664             C        (x) float64 nan nan nan 5.0 0.0
5665             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5666 
5667         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5668         <xarray.Dataset>
5669         Dimensions:  (x: 5)
5670         Coordinates:
5671           * x        (x) int64 0 1 2 3 4
5672         Data variables:
5673             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5674             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5675             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5676             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5677         """
5678         from .missing import _apply_over_vars_with_dim, interp_na
5679 
5680         new = _apply_over_vars_with_dim(
5681             interp_na,
5682             self,
5683             dim=dim,
5684             method=method,
5685             limit=limit,
5686             use_coordinate=use_coordinate,
5687             max_gap=max_gap,
5688             **kwargs,
5689         )
5690         return new
5691 
5692     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5693         """Fill NaN values by propagating values forward
5694 
5695         *Requires bottleneck.*
5696 
5697         Parameters
5698         ----------
5699         dim : Hashable
5700             Specifies the dimension along which to propagate values when
5701             filling.
5702         limit : int or None, optional
5703             The maximum number of consecutive NaN values to forward fill. In
5704             other words, if there is a gap with more than this number of
5705             consecutive NaNs, it will only be partially filled. Must be greater
5706             than 0 or None for no limit. Must be None or greater than or equal
5707             to axis length if filling along chunked axes (dimensions).
5708 
5709         Returns
5710         -------
5711         Dataset
5712         """
5713         from .missing import _apply_over_vars_with_dim, ffill
5714 
5715         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5716         return new
5717 
5718     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5719         """Fill NaN values by propagating values backward
5720 
5721         *Requires bottleneck.*
5722 
5723         Parameters
5724         ----------
5725         dim : Hashable
5726             Specifies the dimension along which to propagate values when
5727             filling.
5728         limit : int or None, optional
5729             The maximum number of consecutive NaN values to backward fill. In
5730             other words, if there is a gap with more than this number of
5731             consecutive NaNs, it will only be partially filled. Must be greater
5732             than 0 or None for no limit. Must be None or greater than or equal
5733             to axis length if filling along chunked axes (dimensions).
5734 
5735         Returns
5736         -------
5737         Dataset
5738         """
5739         from .missing import _apply_over_vars_with_dim, bfill
5740 
5741         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5742         return new
5743 
5744     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5745         """Combine two Datasets, default to data_vars of self.
5746 
5747         The new coordinates follow the normal broadcasting and alignment rules
5748         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5749         filled with np.nan.
5750 
5751         Parameters
5752         ----------
5753         other : Dataset
5754             Used to fill all matching missing values in this array.
5755 
5756         Returns
5757         -------
5758         Dataset
5759         """
5760         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5761         return out
5762 
5763     def reduce(
5764         self: T_Dataset,
5765         func: Callable,
5766         dim: Hashable | Iterable[Hashable] = None,
5767         *,
5768         keep_attrs: bool | None = None,
5769         keepdims: bool = False,
5770         numeric_only: bool = False,
5771         **kwargs: Any,
5772     ) -> T_Dataset:
5773         """Reduce this dataset by applying `func` along some dimension(s).
5774 
5775         Parameters
5776         ----------
5777         func : callable
5778             Function which can be called in the form
5779             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5780             np.ndarray over an integer valued axis.
5781         dim : str or sequence of str, optional
5782             Dimension(s) over which to apply `func`.  By default `func` is
5783             applied over all dimensions.
5784         keep_attrs : bool or None, optional
5785             If True, the dataset's attributes (`attrs`) will be copied from
5786             the original object to the new one.  If False (default), the new
5787             object will be returned without attributes.
5788         keepdims : bool, default: False
5789             If True, the dimensions which are reduced are left in the result
5790             as dimensions of size one. Coordinates that use these dimensions
5791             are removed.
5792         numeric_only : bool, default: False
5793             If True, only apply ``func`` to variables with a numeric dtype.
5794         **kwargs : Any
5795             Additional keyword arguments passed on to ``func``.
5796 
5797         Returns
5798         -------
5799         reduced : Dataset
5800             Dataset with this object's DataArrays replaced with new DataArrays
5801             of summarized data and the indicated dimension(s) removed.
5802         """
5803         if kwargs.get("axis", None) is not None:
5804             raise ValueError(
5805                 "passing 'axis' to Dataset reduce methods is ambiguous."
5806                 " Please use 'dim' instead."
5807             )
5808 
5809         if dim is None or dim is ...:
5810             dims = set(self.dims)
5811         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5812             dims = {dim}
5813         else:
5814             dims = set(dim)
5815 
5816         missing_dimensions = [d for d in dims if d not in self.dims]
5817         if missing_dimensions:
5818             raise ValueError(
5819                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5820             )
5821 
5822         if keep_attrs is None:
5823             keep_attrs = _get_keep_attrs(default=False)
5824 
5825         variables: dict[Hashable, Variable] = {}
5826         for name, var in self._variables.items():
5827             reduce_dims = [d for d in var.dims if d in dims]
5828             if name in self.coords:
5829                 if not reduce_dims:
5830                     variables[name] = var
5831             else:
5832                 if (
5833                     # Some reduction functions (e.g. std, var) need to run on variables
5834                     # that don't have the reduce dims: PR5393
5835                     not reduce_dims
5836                     or not numeric_only
5837                     or np.issubdtype(var.dtype, np.number)
5838                     or (var.dtype == np.bool_)
5839                 ):
5840                     reduce_maybe_single: Hashable | None | list[Hashable]
5841                     if len(reduce_dims) == 1:
5842                         # unpack dimensions for the benefit of functions
5843                         # like np.argmin which can't handle tuple arguments
5844                         (reduce_maybe_single,) = reduce_dims
5845                     elif len(reduce_dims) == var.ndim:
5846                         # prefer to aggregate over axis=None rather than
5847                         # axis=(0, 1) if they will be equivalent, because
5848                         # the former is often more efficient
5849                         reduce_maybe_single = None
5850                     else:
5851                         reduce_maybe_single = reduce_dims
5852                     variables[name] = var.reduce(
5853                         func,
5854                         dim=reduce_maybe_single,
5855                         keep_attrs=keep_attrs,
5856                         keepdims=keepdims,
5857                         **kwargs,
5858                     )
5859 
5860         coord_names = {k for k in self.coords if k in variables}
5861         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5862         attrs = self.attrs if keep_attrs else None
5863         return self._replace_with_new_dims(
5864             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5865         )
5866 
5867     def map(
5868         self: T_Dataset,
5869         func: Callable,
5870         keep_attrs: bool | None = None,
5871         args: Iterable[Any] = (),
5872         **kwargs: Any,
5873     ) -> T_Dataset:
5874         """Apply a function to each data variable in this dataset
5875 
5876         Parameters
5877         ----------
5878         func : callable
5879             Function which can be called in the form `func(x, *args, **kwargs)`
5880             to transform each DataArray `x` in this dataset into another
5881             DataArray.
5882         keep_attrs : bool or None, optional
5883             If True, both the dataset's and variables' attributes (`attrs`) will be
5884             copied from the original objects to the new ones. If False, the new dataset
5885             and variables will be returned without copying the attributes.
5886         args : iterable, optional
5887             Positional arguments passed on to `func`.
5888         **kwargs : Any
5889             Keyword arguments passed on to `func`.
5890 
5891         Returns
5892         -------
5893         applied : Dataset
5894             Resulting dataset from applying ``func`` to each data variable.
5895 
5896         Examples
5897         --------
5898         >>> da = xr.DataArray(np.random.randn(2, 3))
5899         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5900         >>> ds
5901         <xarray.Dataset>
5902         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5903         Dimensions without coordinates: dim_0, dim_1, x
5904         Data variables:
5905             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5906             bar      (x) int64 -1 2
5907         >>> ds.map(np.fabs)
5908         <xarray.Dataset>
5909         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5910         Dimensions without coordinates: dim_0, dim_1, x
5911         Data variables:
5912             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5913             bar      (x) float64 1.0 2.0
5914         """
5915         if keep_attrs is None:
5916             keep_attrs = _get_keep_attrs(default=False)
5917         variables = {
5918             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5919             for k, v in self.data_vars.items()
5920         }
5921         if keep_attrs:
5922             for k, v in variables.items():
5923                 v._copy_attrs_from(self.data_vars[k])
5924         attrs = self.attrs if keep_attrs else None
5925         return type(self)(variables, attrs=attrs)
5926 
5927     def apply(
5928         self: T_Dataset,
5929         func: Callable,
5930         keep_attrs: bool | None = None,
5931         args: Iterable[Any] = (),
5932         **kwargs: Any,
5933     ) -> T_Dataset:
5934         """
5935         Backward compatible implementation of ``map``
5936 
5937         See Also
5938         --------
5939         Dataset.map
5940         """
5941         warnings.warn(
5942             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5943             PendingDeprecationWarning,
5944             stacklevel=2,
5945         )
5946         return self.map(func, keep_attrs, args, **kwargs)
5947 
5948     def assign(
5949         self: T_Dataset,
5950         variables: Mapping[Any, Any] | None = None,
5951         **variables_kwargs: Any,
5952     ) -> T_Dataset:
5953         """Assign new data variables to a Dataset, returning a new object
5954         with all the original variables in addition to the new ones.
5955 
5956         Parameters
5957         ----------
5958         variables : mapping of hashable to Any
5959             Mapping from variables names to the new values. If the new values
5960             are callable, they are computed on the Dataset and assigned to new
5961             data variables. If the values are not callable, (e.g. a DataArray,
5962             scalar, or array), they are simply assigned.
5963         **variables_kwargs
5964             The keyword arguments form of ``variables``.
5965             One of variables or variables_kwargs must be provided.
5966 
5967         Returns
5968         -------
5969         ds : Dataset
5970             A new Dataset with the new variables in addition to all the
5971             existing variables.
5972 
5973         Notes
5974         -----
5975         Since ``kwargs`` is a dictionary, the order of your arguments may not
5976         be preserved, and so the order of the new variables is not well
5977         defined. Assigning multiple variables within the same ``assign`` is
5978         possible, but you cannot reference other variables created within the
5979         same ``assign`` call.
5980 
5981         See Also
5982         --------
5983         pandas.DataFrame.assign
5984 
5985         Examples
5986         --------
5987         >>> x = xr.Dataset(
5988         ...     {
5989         ...         "temperature_c": (
5990         ...             ("lat", "lon"),
5991         ...             20 * np.random.rand(4).reshape(2, 2),
5992         ...         ),
5993         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5994         ...     },
5995         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5996         ... )
5997         >>> x
5998         <xarray.Dataset>
5999         Dimensions:        (lat: 2, lon: 2)
6000         Coordinates:
6001           * lat            (lat) int64 10 20
6002           * lon            (lon) int64 150 160
6003         Data variables:
6004             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6005             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6006 
6007         Where the value is a callable, evaluated on dataset:
6008 
6009         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
6010         <xarray.Dataset>
6011         Dimensions:        (lat: 2, lon: 2)
6012         Coordinates:
6013           * lat            (lat) int64 10 20
6014           * lon            (lon) int64 150 160
6015         Data variables:
6016             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6017             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6018             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6019 
6020         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
6021 
6022         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
6023         <xarray.Dataset>
6024         Dimensions:        (lat: 2, lon: 2)
6025         Coordinates:
6026           * lat            (lat) int64 10 20
6027           * lon            (lon) int64 150 160
6028         Data variables:
6029             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
6030             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
6031             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
6032 
6033         """
6034         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
6035         data = self.copy()
6036         # do all calculations first...
6037         results: CoercibleMapping = data._calc_assign_results(variables)
6038         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
6039         # ... and then assign
6040         data.update(results)
6041         return data
6042 
6043     def to_array(
6044         self, dim: Hashable = "variable", name: Hashable | None = None
6045     ) -> DataArray:
6046         """Convert this dataset into an xarray.DataArray
6047 
6048         The data variables of this dataset will be broadcast against each other
6049         and stacked along the first axis of the new array. All coordinates of
6050         this dataset will remain coordinates.
6051 
6052         Parameters
6053         ----------
6054         dim : Hashable, default: "variable"
6055             Name of the new dimension.
6056         name : Hashable or None, optional
6057             Name of the new data array.
6058 
6059         Returns
6060         -------
6061         array : xarray.DataArray
6062         """
6063         from .dataarray import DataArray
6064 
6065         data_vars = [self.variables[k] for k in self.data_vars]
6066         broadcast_vars = broadcast_variables(*data_vars)
6067         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
6068 
6069         dims = (dim,) + broadcast_vars[0].dims
6070         variable = Variable(dims, data, self.attrs, fastpath=True)
6071 
6072         coords = {k: v.variable for k, v in self.coords.items()}
6073         indexes = filter_indexes_from_coords(self._indexes, set(coords))
6074         new_dim_index = PandasIndex(list(self.data_vars), dim)
6075         indexes[dim] = new_dim_index
6076         coords.update(new_dim_index.create_variables())
6077 
6078         return DataArray._construct_direct(variable, coords, name, indexes)
6079 
6080     def _normalize_dim_order(
6081         self, dim_order: Sequence[Hashable] | None = None
6082     ) -> dict[Hashable, int]:
6083         """
6084         Check the validity of the provided dimensions if any and return the mapping
6085         between dimension name and their size.
6086 
6087         Parameters
6088         ----------
6089         dim_order: Sequence of Hashable or None, optional
6090             Dimension order to validate (default to the alphabetical order if None).
6091 
6092         Returns
6093         -------
6094         result : dict[Hashable, int]
6095             Validated dimensions mapping.
6096 
6097         """
6098         if dim_order is None:
6099             dim_order = list(self.dims)
6100         elif set(dim_order) != set(self.dims):
6101             raise ValueError(
6102                 "dim_order {} does not match the set of dimensions of this "
6103                 "Dataset: {}".format(dim_order, list(self.dims))
6104             )
6105 
6106         ordered_dims = {k: self.dims[k] for k in dim_order}
6107 
6108         return ordered_dims
6109 
6110     def to_pandas(self) -> pd.Series | pd.DataFrame:
6111         """Convert this dataset into a pandas object without changing the number of dimensions.
6112 
6113         The type of the returned object depends on the number of Dataset
6114         dimensions:
6115 
6116         * 0D -> `pandas.Series`
6117         * 1D -> `pandas.DataFrame`
6118 
6119         Only works for Datasets with 1 or fewer dimensions.
6120         """
6121         if len(self.dims) == 0:
6122             return pd.Series({k: v.item() for k, v in self.items()})
6123         if len(self.dims) == 1:
6124             return self.to_dataframe()
6125         raise ValueError(
6126             "cannot convert Datasets with %s dimensions into "
6127             "pandas objects without changing the number of dimensions. "
6128             "Please use Dataset.to_dataframe() instead." % len(self.dims)
6129         )
6130 
6131     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
6132         columns = [k for k in self.variables if k not in self.dims]
6133         data = [
6134             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
6135             for k in columns
6136         ]
6137         index = self.coords.to_index([*ordered_dims])
6138         return pd.DataFrame(dict(zip(columns, data)), index=index)
6139 
6140     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
6141         """Convert this dataset into a pandas.DataFrame.
6142 
6143         Non-index variables in this dataset form the columns of the
6144         DataFrame. The DataFrame is indexed by the Cartesian product of
6145         this dataset's indices.
6146 
6147         Parameters
6148         ----------
6149         dim_order: Sequence of Hashable or None, optional
6150             Hierarchical dimension order for the resulting dataframe. All
6151             arrays are transposed to this order and then written out as flat
6152             vectors in contiguous order, so the last dimension in this list
6153             will be contiguous in the resulting DataFrame. This has a major
6154             influence on which operations are efficient on the resulting
6155             dataframe.
6156 
6157             If provided, must include all dimensions of this dataset. By
6158             default, dimensions are sorted alphabetically.
6159 
6160         Returns
6161         -------
6162         result : DataFrame
6163             Dataset as a pandas DataFrame.
6164 
6165         """
6166 
6167         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6168 
6169         return self._to_dataframe(ordered_dims=ordered_dims)
6170 
6171     def _set_sparse_data_from_dataframe(
6172         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6173     ) -> None:
6174         from sparse import COO
6175 
6176         if isinstance(idx, pd.MultiIndex):
6177             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
6178             is_sorted = idx.is_monotonic_increasing
6179             shape = tuple(lev.size for lev in idx.levels)
6180         else:
6181             coords = np.arange(idx.size).reshape(1, -1)
6182             is_sorted = True
6183             shape = (idx.size,)
6184 
6185         for name, values in arrays:
6186             # In virtually all real use cases, the sparse array will now have
6187             # missing values and needs a fill_value. For consistency, don't
6188             # special case the rare exceptions (e.g., dtype=int without a
6189             # MultiIndex).
6190             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6191             values = np.asarray(values, dtype=dtype)
6192 
6193             data = COO(
6194                 coords,
6195                 values,
6196                 shape,
6197                 has_duplicates=False,
6198                 sorted=is_sorted,
6199                 fill_value=fill_value,
6200             )
6201             self[name] = (dims, data)
6202 
6203     def _set_numpy_data_from_dataframe(
6204         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
6205     ) -> None:
6206         if not isinstance(idx, pd.MultiIndex):
6207             for name, values in arrays:
6208                 self[name] = (dims, values)
6209             return
6210 
6211         # NB: similar, more general logic, now exists in
6212         # variable.unstack_once; we could consider combining them at some
6213         # point.
6214 
6215         shape = tuple(lev.size for lev in idx.levels)
6216         indexer = tuple(idx.codes)
6217 
6218         # We already verified that the MultiIndex has all unique values, so
6219         # there are missing values if and only if the size of output arrays is
6220         # larger that the index.
6221         missing_values = math.prod(shape) > idx.shape[0]
6222 
6223         for name, values in arrays:
6224             # NumPy indexing is much faster than using DataFrame.reindex() to
6225             # fill in missing values:
6226             # https://stackoverflow.com/a/35049899/809705
6227             if missing_values:
6228                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
6229                 data = np.full(shape, fill_value, dtype)
6230             else:
6231                 # If there are no missing values, keep the existing dtype
6232                 # instead of promoting to support NA, e.g., keep integer
6233                 # columns as integers.
6234                 # TODO: consider removing this special case, which doesn't
6235                 # exist for sparse=True.
6236                 data = np.zeros(shape, values.dtype)
6237             data[indexer] = values
6238             self[name] = (dims, data)
6239 
6240     @classmethod
6241     def from_dataframe(
6242         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
6243     ) -> T_Dataset:
6244         """Convert a pandas.DataFrame into an xarray.Dataset
6245 
6246         Each column will be converted into an independent variable in the
6247         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
6248         into a tensor product of one-dimensional indices (filling in missing
6249         values with NaN). This method will produce a Dataset very similar to
6250         that on which the 'to_dataframe' method was called, except with
6251         possibly redundant dimensions (since all dataset variables will have
6252         the same dimensionality)
6253 
6254         Parameters
6255         ----------
6256         dataframe : DataFrame
6257             DataFrame from which to copy data and indices.
6258         sparse : bool, default: False
6259             If true, create a sparse arrays instead of dense numpy arrays. This
6260             can potentially save a large amount of memory if the DataFrame has
6261             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6262 
6263         Returns
6264         -------
6265         New Dataset.
6266 
6267         See Also
6268         --------
6269         xarray.DataArray.from_series
6270         pandas.DataFrame.to_xarray
6271         """
6272         # TODO: Add an option to remove dimensions along which the variables
6273         # are constant, to enable consistent serialization to/from a dataframe,
6274         # even if some variables have different dimensionality.
6275 
6276         if not dataframe.columns.is_unique:
6277             raise ValueError("cannot convert DataFrame with non-unique columns")
6278 
6279         idx = remove_unused_levels_categories(dataframe.index)
6280 
6281         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6282             raise ValueError(
6283                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6284             )
6285 
6286         # Cast to a NumPy array first, in case the Series is a pandas Extension
6287         # array (which doesn't have a valid NumPy dtype)
6288         # TODO: allow users to control how this casting happens, e.g., by
6289         # forwarding arguments to pandas.Series.to_numpy?
6290         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6291 
6292         indexes: dict[Hashable, Index] = {}
6293         index_vars: dict[Hashable, Variable] = {}
6294 
6295         if isinstance(idx, pd.MultiIndex):
6296             dims = tuple(
6297                 name if name is not None else "level_%i" % n
6298                 for n, name in enumerate(idx.names)
6299             )
6300             for dim, lev in zip(dims, idx.levels):
6301                 xr_idx = PandasIndex(lev, dim)
6302                 indexes[dim] = xr_idx
6303                 index_vars.update(xr_idx.create_variables())
6304         else:
6305             index_name = idx.name if idx.name is not None else "index"
6306             dims = (index_name,)
6307             xr_idx = PandasIndex(idx, index_name)
6308             indexes[index_name] = xr_idx
6309             index_vars.update(xr_idx.create_variables())
6310 
6311         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6312 
6313         if sparse:
6314             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6315         else:
6316             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6317         return obj
6318 
6319     def to_dask_dataframe(
6320         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6321     ) -> DaskDataFrame:
6322         """
6323         Convert this dataset into a dask.dataframe.DataFrame.
6324 
6325         The dimensions, coordinates and data variables in this dataset form
6326         the columns of the DataFrame.
6327 
6328         Parameters
6329         ----------
6330         dim_order : list, optional
6331             Hierarchical dimension order for the resulting dataframe. All
6332             arrays are transposed to this order and then written out as flat
6333             vectors in contiguous order, so the last dimension in this list
6334             will be contiguous in the resulting DataFrame. This has a major
6335             influence on which operations are efficient on the resulting dask
6336             dataframe.
6337 
6338             If provided, must include all dimensions of this dataset. By
6339             default, dimensions are sorted alphabetically.
6340         set_index : bool, default: False
6341             If set_index=True, the dask DataFrame is indexed by this dataset's
6342             coordinate. Since dask DataFrames do not support multi-indexes,
6343             set_index only works if the dataset only contains one dimension.
6344 
6345         Returns
6346         -------
6347         dask.dataframe.DataFrame
6348         """
6349 
6350         import dask.array as da
6351         import dask.dataframe as dd
6352 
6353         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6354 
6355         columns = list(ordered_dims)
6356         columns.extend(k for k in self.coords if k not in self.dims)
6357         columns.extend(self.data_vars)
6358 
6359         series_list = []
6360         for name in columns:
6361             try:
6362                 var = self.variables[name]
6363             except KeyError:
6364                 # dimension without a matching coordinate
6365                 size = self.dims[name]
6366                 data = da.arange(size, chunks=size, dtype=np.int64)
6367                 var = Variable((name,), data)
6368 
6369             # IndexVariable objects have a dummy .chunk() method
6370             if isinstance(var, IndexVariable):
6371                 var = var.to_base_variable()
6372 
6373             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6374             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6375             series_list.append(series)
6376 
6377         df = dd.concat(series_list, axis=1)
6378 
6379         if set_index:
6380             dim_order = [*ordered_dims]
6381 
6382             if len(dim_order) == 1:
6383                 (dim,) = dim_order
6384                 df = df.set_index(dim)
6385             else:
6386                 # triggers an error about multi-indexes, even if only one
6387                 # dimension is passed
6388                 df = df.set_index(dim_order)
6389 
6390         return df
6391 
6392     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6393         """
6394         Convert this dataset to a dictionary following xarray naming
6395         conventions.
6396 
6397         Converts all variables and attributes to native Python objects
6398         Useful for converting to json. To avoid datetime incompatibility
6399         use decode_times=False kwarg in xarrray.open_dataset.
6400 
6401         Parameters
6402         ----------
6403         data : bool, default: True
6404             Whether to include the actual data in the dictionary. When set to
6405             False, returns just the schema.
6406         encoding : bool, default: False
6407             Whether to include the Dataset's encoding in the dictionary.
6408 
6409         Returns
6410         -------
6411         d : dict
6412             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6413             "encoding".
6414 
6415         See Also
6416         --------
6417         Dataset.from_dict
6418         DataArray.to_dict
6419         """
6420         d: dict = {
6421             "coords": {},
6422             "attrs": decode_numpy_dict_values(self.attrs),
6423             "dims": dict(self.dims),
6424             "data_vars": {},
6425         }
6426         for k in self.coords:
6427             d["coords"].update(
6428                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6429             )
6430         for k in self.data_vars:
6431             d["data_vars"].update(
6432                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6433             )
6434         if encoding:
6435             d["encoding"] = dict(self.encoding)
6436         return d
6437 
6438     @classmethod
6439     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6440         """Convert a dictionary into an xarray.Dataset.
6441 
6442         Parameters
6443         ----------
6444         d : dict-like
6445             Mapping with a minimum structure of
6446                 ``{"var_0": {"dims": [..], "data": [..]}, \
6447                             ...}``
6448 
6449         Returns
6450         -------
6451         obj : Dataset
6452 
6453         See also
6454         --------
6455         Dataset.to_dict
6456         DataArray.from_dict
6457 
6458         Examples
6459         --------
6460         >>> d = {
6461         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6462         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6463         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6464         ... }
6465         >>> ds = xr.Dataset.from_dict(d)
6466         >>> ds
6467         <xarray.Dataset>
6468         Dimensions:  (t: 3)
6469         Coordinates:
6470           * t        (t) int64 0 1 2
6471         Data variables:
6472             a        (t) <U1 'a' 'b' 'c'
6473             b        (t) int64 10 20 30
6474 
6475         >>> d = {
6476         ...     "coords": {
6477         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6478         ...     },
6479         ...     "attrs": {"title": "air temperature"},
6480         ...     "dims": "t",
6481         ...     "data_vars": {
6482         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6483         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6484         ...     },
6485         ... }
6486         >>> ds = xr.Dataset.from_dict(d)
6487         >>> ds
6488         <xarray.Dataset>
6489         Dimensions:  (t: 3)
6490         Coordinates:
6491           * t        (t) int64 0 1 2
6492         Data variables:
6493             a        (t) int64 10 20 30
6494             b        (t) <U1 'a' 'b' 'c'
6495         Attributes:
6496             title:    air temperature
6497 
6498         """
6499 
6500         variables: Iterable[tuple[Hashable, Any]]
6501         if not {"coords", "data_vars"}.issubset(set(d)):
6502             variables = d.items()
6503         else:
6504             import itertools
6505 
6506             variables = itertools.chain(
6507                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6508             )
6509         try:
6510             variable_dict = {
6511                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6512             }
6513         except KeyError as e:
6514             raise ValueError(
6515                 "cannot convert dict without the key "
6516                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6517             )
6518         obj = cls(variable_dict)
6519 
6520         # what if coords aren't dims?
6521         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6522         obj = obj.set_coords(coords)
6523 
6524         obj.attrs.update(d.get("attrs", {}))
6525         obj.encoding.update(d.get("encoding", {}))
6526 
6527         return obj
6528 
6529     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6530         variables = {}
6531         keep_attrs = kwargs.pop("keep_attrs", None)
6532         if keep_attrs is None:
6533             keep_attrs = _get_keep_attrs(default=True)
6534         for k, v in self._variables.items():
6535             if k in self._coord_names:
6536                 variables[k] = v
6537             else:
6538                 variables[k] = f(v, *args, **kwargs)
6539                 if keep_attrs:
6540                     variables[k].attrs = v._attrs
6541         attrs = self._attrs if keep_attrs else None
6542         return self._replace_with_new_dims(variables, attrs=attrs)
6543 
6544     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6545         from .dataarray import DataArray
6546         from .groupby import GroupBy
6547 
6548         if isinstance(other, GroupBy):
6549             return NotImplemented
6550         align_type = OPTIONS["arithmetic_join"] if join is None else join
6551         if isinstance(other, (DataArray, Dataset)):
6552             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6553         g = f if not reflexive else lambda x, y: f(y, x)
6554         ds = self._calculate_binary_op(g, other, join=align_type)
6555         return ds
6556 
6557     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6558         from .dataarray import DataArray
6559         from .groupby import GroupBy
6560 
6561         if isinstance(other, GroupBy):
6562             raise TypeError(
6563                 "in-place operations between a Dataset and "
6564                 "a grouped object are not permitted"
6565             )
6566         # we don't actually modify arrays in-place with in-place Dataset
6567         # arithmetic -- this lets us automatically align things
6568         if isinstance(other, (DataArray, Dataset)):
6569             other = other.reindex_like(self, copy=False)
6570         g = ops.inplace_to_noninplace_op(f)
6571         ds = self._calculate_binary_op(g, other, inplace=True)
6572         self._replace_with_new_dims(
6573             ds._variables,
6574             ds._coord_names,
6575             attrs=ds._attrs,
6576             indexes=ds._indexes,
6577             inplace=True,
6578         )
6579         return self
6580 
6581     def _calculate_binary_op(
6582         self, f, other, join="inner", inplace: bool = False
6583     ) -> Dataset:
6584         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6585             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6586                 raise ValueError(
6587                     "datasets must have the same data variables "
6588                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6589                 )
6590 
6591             dest_vars = {}
6592 
6593             for k in lhs_data_vars:
6594                 if k in rhs_data_vars:
6595                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6596                 elif join in ["left", "outer"]:
6597                     dest_vars[k] = f(lhs_vars[k], np.nan)
6598             for k in rhs_data_vars:
6599                 if k not in dest_vars and join in ["right", "outer"]:
6600                     dest_vars[k] = f(rhs_vars[k], np.nan)
6601             return dest_vars
6602 
6603         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6604             # can't use our shortcut of doing the binary operation with
6605             # Variable objects, so apply over our data vars instead.
6606             new_data_vars = apply_over_both(
6607                 self.data_vars, other, self.data_vars, other
6608             )
6609             return type(self)(new_data_vars)
6610 
6611         other_coords: Coordinates | None = getattr(other, "coords", None)
6612         ds = self.coords.merge(other_coords)
6613 
6614         if isinstance(other, Dataset):
6615             new_vars = apply_over_both(
6616                 self.data_vars, other.data_vars, self.variables, other.variables
6617             )
6618         else:
6619             other_variable = getattr(other, "variable", other)
6620             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6621         ds._variables.update(new_vars)
6622         ds._dims = calculate_dimensions(ds._variables)
6623         return ds
6624 
6625     def _copy_attrs_from(self, other):
6626         self.attrs = other.attrs
6627         for v in other.variables:
6628             if v in self.variables:
6629                 self.variables[v].attrs = other.variables[v].attrs
6630 
6631     def diff(
6632         self: T_Dataset,
6633         dim: Hashable,
6634         n: int = 1,
6635         label: Literal["upper", "lower"] = "upper",
6636     ) -> T_Dataset:
6637         """Calculate the n-th order discrete difference along given axis.
6638 
6639         Parameters
6640         ----------
6641         dim : Hashable
6642             Dimension over which to calculate the finite difference.
6643         n : int, default: 1
6644             The number of times values are differenced.
6645         label : {"upper", "lower"}, default: "upper"
6646             The new coordinate in dimension ``dim`` will have the
6647             values of either the minuend's or subtrahend's coordinate
6648             for values 'upper' and 'lower', respectively.
6649 
6650         Returns
6651         -------
6652         difference : Dataset
6653             The n-th order finite difference of this object.
6654 
6655         Notes
6656         -----
6657         `n` matches numpy's behavior and is different from pandas' first argument named
6658         `periods`.
6659 
6660         Examples
6661         --------
6662         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6663         >>> ds.diff("x")
6664         <xarray.Dataset>
6665         Dimensions:  (x: 3)
6666         Dimensions without coordinates: x
6667         Data variables:
6668             foo      (x) int64 0 1 0
6669         >>> ds.diff("x", 2)
6670         <xarray.Dataset>
6671         Dimensions:  (x: 2)
6672         Dimensions without coordinates: x
6673         Data variables:
6674             foo      (x) int64 1 -1
6675 
6676         See Also
6677         --------
6678         Dataset.differentiate
6679         """
6680         if n == 0:
6681             return self
6682         if n < 0:
6683             raise ValueError(f"order `n` must be non-negative but got {n}")
6684 
6685         # prepare slices
6686         slice_start = {dim: slice(None, -1)}
6687         slice_end = {dim: slice(1, None)}
6688 
6689         # prepare new coordinate
6690         if label == "upper":
6691             slice_new = slice_end
6692         elif label == "lower":
6693             slice_new = slice_start
6694         else:
6695             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6696 
6697         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6698         variables = {}
6699 
6700         for name, var in self.variables.items():
6701             if name in index_vars:
6702                 variables[name] = index_vars[name]
6703             elif dim in var.dims:
6704                 if name in self.data_vars:
6705                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6706                 else:
6707                     variables[name] = var.isel(slice_new)
6708             else:
6709                 variables[name] = var
6710 
6711         difference = self._replace_with_new_dims(variables, indexes=indexes)
6712 
6713         if n > 1:
6714             return difference.diff(dim, n - 1)
6715         else:
6716             return difference
6717 
6718     def shift(
6719         self: T_Dataset,
6720         shifts: Mapping[Any, int] | None = None,
6721         fill_value: Any = xrdtypes.NA,
6722         **shifts_kwargs: int,
6723     ) -> T_Dataset:
6724 
6725         """Shift this dataset by an offset along one or more dimensions.
6726 
6727         Only data variables are moved; coordinates stay in place. This is
6728         consistent with the behavior of ``shift`` in pandas.
6729 
6730         Values shifted from beyond array bounds will appear at one end of
6731         each dimension, which are filled according to `fill_value`. For periodic
6732         offsets instead see `roll`.
6733 
6734         Parameters
6735         ----------
6736         shifts : mapping of hashable to int
6737             Integer offset to shift along each of the given dimensions.
6738             Positive offsets shift to the right; negative offsets shift to the
6739             left.
6740         fill_value : scalar or dict-like, optional
6741             Value to use for newly missing values. If a dict-like, maps
6742             variable names (including coordinates) to fill values.
6743         **shifts_kwargs
6744             The keyword arguments form of ``shifts``.
6745             One of shifts or shifts_kwargs must be provided.
6746 
6747         Returns
6748         -------
6749         shifted : Dataset
6750             Dataset with the same coordinates and attributes but shifted data
6751             variables.
6752 
6753         See Also
6754         --------
6755         roll
6756 
6757         Examples
6758         --------
6759         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6760         >>> ds.shift(x=2)
6761         <xarray.Dataset>
6762         Dimensions:  (x: 5)
6763         Dimensions without coordinates: x
6764         Data variables:
6765             foo      (x) object nan nan 'a' 'b' 'c'
6766         """
6767         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6768         invalid = [k for k in shifts if k not in self.dims]
6769         if invalid:
6770             raise ValueError(f"dimensions {invalid!r} do not exist")
6771 
6772         variables = {}
6773         for name, var in self.variables.items():
6774             if name in self.data_vars:
6775                 fill_value_ = (
6776                     fill_value.get(name, xrdtypes.NA)
6777                     if isinstance(fill_value, dict)
6778                     else fill_value
6779                 )
6780 
6781                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6782                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6783             else:
6784                 variables[name] = var
6785 
6786         return self._replace(variables)
6787 
6788     def roll(
6789         self: T_Dataset,
6790         shifts: Mapping[Any, int] | None = None,
6791         roll_coords: bool = False,
6792         **shifts_kwargs: int,
6793     ) -> T_Dataset:
6794         """Roll this dataset by an offset along one or more dimensions.
6795 
6796         Unlike shift, roll treats the given dimensions as periodic, so will not
6797         create any missing values to be filled.
6798 
6799         Also unlike shift, roll may rotate all variables, including coordinates
6800         if specified. The direction of rotation is consistent with
6801         :py:func:`numpy.roll`.
6802 
6803         Parameters
6804         ----------
6805         shifts : mapping of hashable to int, optional
6806             A dict with keys matching dimensions and values given
6807             by integers to rotate each of the given dimensions. Positive
6808             offsets roll to the right; negative offsets roll to the left.
6809         roll_coords : bool, default: False
6810             Indicates whether to roll the coordinates by the offset too.
6811         **shifts_kwargs : {dim: offset, ...}, optional
6812             The keyword arguments form of ``shifts``.
6813             One of shifts or shifts_kwargs must be provided.
6814 
6815         Returns
6816         -------
6817         rolled : Dataset
6818             Dataset with the same attributes but rolled data and coordinates.
6819 
6820         See Also
6821         --------
6822         shift
6823 
6824         Examples
6825         --------
6826         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6827         >>> ds.roll(x=2)
6828         <xarray.Dataset>
6829         Dimensions:  (x: 5)
6830         Coordinates:
6831           * x        (x) int64 0 1 2 3 4
6832         Data variables:
6833             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6834 
6835         >>> ds.roll(x=2, roll_coords=True)
6836         <xarray.Dataset>
6837         Dimensions:  (x: 5)
6838         Coordinates:
6839           * x        (x) int64 3 4 0 1 2
6840         Data variables:
6841             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6842 
6843         """
6844         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6845         invalid = [k for k in shifts if k not in self.dims]
6846         if invalid:
6847             raise ValueError(f"dimensions {invalid!r} do not exist")
6848 
6849         unrolled_vars: tuple[Hashable, ...]
6850 
6851         if roll_coords:
6852             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6853             unrolled_vars = ()
6854         else:
6855             indexes = dict(self._indexes)
6856             index_vars = dict(self.xindexes.variables)
6857             unrolled_vars = tuple(self.coords)
6858 
6859         variables = {}
6860         for k, var in self.variables.items():
6861             if k in index_vars:
6862                 variables[k] = index_vars[k]
6863             elif k not in unrolled_vars:
6864                 variables[k] = var.roll(
6865                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6866                 )
6867             else:
6868                 variables[k] = var
6869 
6870         return self._replace(variables, indexes=indexes)
6871 
6872     def sortby(
6873         self: T_Dataset,
6874         variables: Hashable | DataArray | list[Hashable | DataArray],
6875         ascending: bool = True,
6876     ) -> T_Dataset:
6877         """
6878         Sort object by labels or values (along an axis).
6879 
6880         Sorts the dataset, either along specified dimensions,
6881         or according to values of 1-D dataarrays that share dimension
6882         with calling object.
6883 
6884         If the input variables are dataarrays, then the dataarrays are aligned
6885         (via left-join) to the calling object prior to sorting by cell values.
6886         NaNs are sorted to the end, following Numpy convention.
6887 
6888         If multiple sorts along the same dimension is
6889         given, numpy's lexsort is performed along that dimension:
6890         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6891         and the FIRST key in the sequence is used as the primary sort key,
6892         followed by the 2nd key, etc.
6893 
6894         Parameters
6895         ----------
6896         variables : Hashable, DataArray, or list of hashable or DataArray
6897             1D DataArray objects or name(s) of 1D variable(s) in
6898             coords/data_vars whose values are used to sort the dataset.
6899         ascending : bool, default: True
6900             Whether to sort by ascending or descending order.
6901 
6902         Returns
6903         -------
6904         sorted : Dataset
6905             A new dataset where all the specified dims are sorted by dim
6906             labels.
6907 
6908         See Also
6909         --------
6910         DataArray.sortby
6911         numpy.sort
6912         pandas.sort_values
6913         pandas.sort_index
6914 
6915         Examples
6916         --------
6917         >>> ds = xr.Dataset(
6918         ...     {
6919         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6920         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6921         ...     },
6922         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6923         ... )
6924         >>> ds.sortby("x")
6925         <xarray.Dataset>
6926         Dimensions:  (x: 2, y: 2)
6927         Coordinates:
6928           * x        (x) <U1 'a' 'b'
6929           * y        (y) int64 1 0
6930         Data variables:
6931             A        (x, y) int64 3 4 1 2
6932             B        (x, y) int64 7 8 5 6
6933         """
6934         from .dataarray import DataArray
6935 
6936         if not isinstance(variables, list):
6937             variables = [variables]
6938         else:
6939             variables = variables
6940         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6941         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6942         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6943         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6944         vars_by_dim = defaultdict(list)
6945         for data_array in aligned_other_vars:
6946             if data_array.ndim != 1:
6947                 raise ValueError("Input DataArray is not 1-D.")
6948             (key,) = data_array.dims
6949             vars_by_dim[key].append(data_array)
6950 
6951         indices = {}
6952         for key, arrays in vars_by_dim.items():
6953             order = np.lexsort(tuple(reversed(arrays)))
6954             indices[key] = order if ascending else order[::-1]
6955         return aligned_self.isel(indices)
6956 
6957     def quantile(
6958         self: T_Dataset,
6959         q: ArrayLike,
6960         dim: str | Iterable[Hashable] | None = None,
6961         method: QUANTILE_METHODS = "linear",
6962         numeric_only: bool = False,
6963         keep_attrs: bool = None,
6964         skipna: bool = None,
6965         interpolation: QUANTILE_METHODS = None,
6966     ) -> T_Dataset:
6967         """Compute the qth quantile of the data along the specified dimension.
6968 
6969         Returns the qth quantiles(s) of the array elements for each variable
6970         in the Dataset.
6971 
6972         Parameters
6973         ----------
6974         q : float or array-like of float
6975             Quantile to compute, which must be between 0 and 1 inclusive.
6976         dim : str or Iterable of Hashable, optional
6977             Dimension(s) over which to apply quantile.
6978         method : str, default: "linear"
6979             This optional parameter specifies the interpolation method to use when the
6980             desired quantile lies between two data points. The options sorted by their R
6981             type as summarized in the H&F paper [1]_ are:
6982 
6983                 1. "inverted_cdf" (*)
6984                 2. "averaged_inverted_cdf" (*)
6985                 3. "closest_observation" (*)
6986                 4. "interpolated_inverted_cdf" (*)
6987                 5. "hazen" (*)
6988                 6. "weibull" (*)
6989                 7. "linear"  (default)
6990                 8. "median_unbiased" (*)
6991                 9. "normal_unbiased" (*)
6992 
6993             The first three methods are discontiuous.  The following discontinuous
6994             variations of the default "linear" (7.) option are also available:
6995 
6996                 * "lower"
6997                 * "higher"
6998                 * "midpoint"
6999                 * "nearest"
7000 
7001             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
7002             was previously called "interpolation", renamed in accordance with numpy
7003             version 1.22.0.
7004 
7005             (*) These methods require numpy version 1.22 or newer.
7006 
7007         keep_attrs : bool, optional
7008             If True, the dataset's attributes (`attrs`) will be copied from
7009             the original object to the new one.  If False (default), the new
7010             object will be returned without attributes.
7011         numeric_only : bool, optional
7012             If True, only apply ``func`` to variables with a numeric dtype.
7013         skipna : bool, optional
7014             If True, skip missing values (as marked by NaN). By default, only
7015             skips missing values for float dtypes; other dtypes either do not
7016             have a sentinel missing value (int) or skipna=True has not been
7017             implemented (object, datetime64 or timedelta64).
7018 
7019         Returns
7020         -------
7021         quantiles : Dataset
7022             If `q` is a single quantile, then the result is a scalar for each
7023             variable in data_vars. If multiple percentiles are given, first
7024             axis of the result corresponds to the quantile and a quantile
7025             dimension is added to the return Dataset. The other dimensions are
7026             the dimensions that remain after the reduction of the array.
7027 
7028         See Also
7029         --------
7030         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
7031 
7032         Examples
7033         --------
7034         >>> ds = xr.Dataset(
7035         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
7036         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
7037         ... )
7038         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
7039         <xarray.Dataset>
7040         Dimensions:   ()
7041         Coordinates:
7042             quantile  float64 0.0
7043         Data variables:
7044             a         float64 0.7
7045         >>> ds.quantile(0, dim="x")
7046         <xarray.Dataset>
7047         Dimensions:   (y: 4)
7048         Coordinates:
7049           * y         (y) float64 1.0 1.5 2.0 2.5
7050             quantile  float64 0.0
7051         Data variables:
7052             a         (y) float64 0.7 4.2 2.6 1.5
7053         >>> ds.quantile([0, 0.5, 1])
7054         <xarray.Dataset>
7055         Dimensions:   (quantile: 3)
7056         Coordinates:
7057           * quantile  (quantile) float64 0.0 0.5 1.0
7058         Data variables:
7059             a         (quantile) float64 0.7 3.4 9.4
7060         >>> ds.quantile([0, 0.5, 1], dim="x")
7061         <xarray.Dataset>
7062         Dimensions:   (quantile: 3, y: 4)
7063         Coordinates:
7064           * y         (y) float64 1.0 1.5 2.0 2.5
7065           * quantile  (quantile) float64 0.0 0.5 1.0
7066         Data variables:
7067             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
7068 
7069         References
7070         ----------
7071         .. [1] R. J. Hyndman and Y. Fan,
7072            "Sample quantiles in statistical packages,"
7073            The American Statistician, 50(4), pp. 361-365, 1996
7074         """
7075 
7076         # interpolation renamed to method in version 0.21.0
7077         # check here and in variable to avoid repeated warnings
7078         if interpolation is not None:
7079             warnings.warn(
7080                 "The `interpolation` argument to quantile was renamed to `method`.",
7081                 FutureWarning,
7082             )
7083 
7084             if method != "linear":
7085                 raise TypeError("Cannot pass interpolation and method keywords!")
7086 
7087             method = interpolation
7088 
7089         dims: set[Hashable]
7090         if isinstance(dim, str):
7091             dims = {dim}
7092         elif dim is None or dim is ...:
7093             dims = set(self.dims)
7094         else:
7095             dims = set(dim)
7096 
7097         _assert_empty(
7098             tuple(d for d in dims if d not in self.dims),
7099             "Dataset does not contain the dimensions: %s",
7100         )
7101 
7102         q = np.asarray(q, dtype=np.float64)
7103 
7104         variables = {}
7105         for name, var in self.variables.items():
7106             reduce_dims = [d for d in var.dims if d in dims]
7107             if reduce_dims or not var.dims:
7108                 if name not in self.coords:
7109                     if (
7110                         not numeric_only
7111                         or np.issubdtype(var.dtype, np.number)
7112                         or var.dtype == np.bool_
7113                     ):
7114                         variables[name] = var.quantile(
7115                             q,
7116                             dim=reduce_dims,
7117                             method=method,
7118                             keep_attrs=keep_attrs,
7119                             skipna=skipna,
7120                         )
7121 
7122             else:
7123                 variables[name] = var
7124 
7125         # construct the new dataset
7126         coord_names = {k for k in self.coords if k in variables}
7127         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7128         if keep_attrs is None:
7129             keep_attrs = _get_keep_attrs(default=False)
7130         attrs = self.attrs if keep_attrs else None
7131         new = self._replace_with_new_dims(
7132             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
7133         )
7134         return new.assign_coords(quantile=q)
7135 
7136     def rank(
7137         self: T_Dataset,
7138         dim: Hashable,
7139         pct: bool = False,
7140         keep_attrs: bool | None = None,
7141     ) -> T_Dataset:
7142         """Ranks the data.
7143 
7144         Equal values are assigned a rank that is the average of the ranks that
7145         would have been otherwise assigned to all of the values within
7146         that set.
7147         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
7148 
7149         NaNs in the input array are returned as NaNs.
7150 
7151         The `bottleneck` library is required.
7152 
7153         Parameters
7154         ----------
7155         dim : Hashable
7156             Dimension over which to compute rank.
7157         pct : bool, default: False
7158             If True, compute percentage ranks, otherwise compute integer ranks.
7159         keep_attrs : bool or None, optional
7160             If True, the dataset's attributes (`attrs`) will be copied from
7161             the original object to the new one.  If False, the new
7162             object will be returned without attributes.
7163 
7164         Returns
7165         -------
7166         ranked : Dataset
7167             Variables that do not depend on `dim` are dropped.
7168         """
7169         if not OPTIONS["use_bottleneck"]:
7170             raise RuntimeError(
7171                 "rank requires bottleneck to be enabled."
7172                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
7173             )
7174 
7175         if dim not in self.dims:
7176             raise ValueError(f"Dataset does not contain the dimension: {dim}")
7177 
7178         variables = {}
7179         for name, var in self.variables.items():
7180             if name in self.data_vars:
7181                 if dim in var.dims:
7182                     variables[name] = var.rank(dim, pct=pct)
7183             else:
7184                 variables[name] = var
7185 
7186         coord_names = set(self.coords)
7187         if keep_attrs is None:
7188             keep_attrs = _get_keep_attrs(default=False)
7189         attrs = self.attrs if keep_attrs else None
7190         return self._replace(variables, coord_names, attrs=attrs)
7191 
7192     def differentiate(
7193         self: T_Dataset,
7194         coord: Hashable,
7195         edge_order: Literal[1, 2] = 1,
7196         datetime_unit: DatetimeUnitOptions | None = None,
7197     ) -> T_Dataset:
7198         """ Differentiate with the second order accurate central
7199         differences.
7200 
7201         .. note::
7202             This feature is limited to simple cartesian geometry, i.e. coord
7203             must be one dimensional.
7204 
7205         Parameters
7206         ----------
7207         coord : Hashable
7208             The coordinate to be used to compute the gradient.
7209         edge_order : {1, 2}, default: 1
7210             N-th order accurate differences at the boundaries.
7211         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
7212             "us", "ns", "ps", "fs", "as", None}, default: None
7213             Unit to compute gradient. Only valid for datetime coordinate.
7214 
7215         Returns
7216         -------
7217         differentiated: Dataset
7218 
7219         See also
7220         --------
7221         numpy.gradient: corresponding numpy function
7222         """
7223         from .variable import Variable
7224 
7225         if coord not in self.variables and coord not in self.dims:
7226             raise ValueError(f"Coordinate {coord} does not exist.")
7227 
7228         coord_var = self[coord].variable
7229         if coord_var.ndim != 1:
7230             raise ValueError(
7231                 "Coordinate {} must be 1 dimensional but is {}"
7232                 " dimensional".format(coord, coord_var.ndim)
7233             )
7234 
7235         dim = coord_var.dims[0]
7236         if _contains_datetime_like_objects(coord_var):
7237             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7238                 datetime_unit = cast(
7239                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
7240                 )
7241             elif datetime_unit is None:
7242                 datetime_unit = "s"  # Default to seconds for cftime objects
7243             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
7244 
7245         variables = {}
7246         for k, v in self.variables.items():
7247             if k in self.data_vars and dim in v.dims and k not in self.coords:
7248                 if _contains_datetime_like_objects(v):
7249                     v = v._to_numeric(datetime_unit=datetime_unit)
7250                 grad = duck_array_ops.gradient(
7251                     v.data,
7252                     coord_var.data,
7253                     edge_order=edge_order,
7254                     axis=v.get_axis_num(dim),
7255                 )
7256                 variables[k] = Variable(v.dims, grad)
7257             else:
7258                 variables[k] = v
7259         return self._replace(variables)
7260 
7261     def integrate(
7262         self: T_Dataset,
7263         coord: Hashable | Sequence[Hashable],
7264         datetime_unit: DatetimeUnitOptions = None,
7265     ) -> T_Dataset:
7266         """Integrate along the given coordinate using the trapezoidal rule.
7267 
7268         .. note::
7269             This feature is limited to simple cartesian geometry, i.e. coord
7270             must be one dimensional.
7271 
7272         Parameters
7273         ----------
7274         coord : hashable, or sequence of hashable
7275             Coordinate(s) used for the integration.
7276         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7277                         'ps', 'fs', 'as', None}, optional
7278             Specify the unit if datetime coordinate is used.
7279 
7280         Returns
7281         -------
7282         integrated : Dataset
7283 
7284         See also
7285         --------
7286         DataArray.integrate
7287         numpy.trapz : corresponding numpy function
7288 
7289         Examples
7290         --------
7291         >>> ds = xr.Dataset(
7292         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7293         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7294         ... )
7295         >>> ds
7296         <xarray.Dataset>
7297         Dimensions:  (x: 4)
7298         Coordinates:
7299           * x        (x) int64 0 1 2 3
7300             y        (x) int64 1 7 3 5
7301         Data variables:
7302             a        (x) int64 5 5 6 6
7303             b        (x) int64 1 2 1 0
7304         >>> ds.integrate("x")
7305         <xarray.Dataset>
7306         Dimensions:  ()
7307         Data variables:
7308             a        float64 16.5
7309             b        float64 3.5
7310         >>> ds.integrate("y")
7311         <xarray.Dataset>
7312         Dimensions:  ()
7313         Data variables:
7314             a        float64 20.0
7315             b        float64 4.0
7316         """
7317         if not isinstance(coord, (list, tuple)):
7318             coord = (coord,)
7319         result = self
7320         for c in coord:
7321             result = result._integrate_one(c, datetime_unit=datetime_unit)
7322         return result
7323 
7324     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7325         from .variable import Variable
7326 
7327         if coord not in self.variables and coord not in self.dims:
7328             raise ValueError(f"Coordinate {coord} does not exist.")
7329 
7330         coord_var = self[coord].variable
7331         if coord_var.ndim != 1:
7332             raise ValueError(
7333                 "Coordinate {} must be 1 dimensional but is {}"
7334                 " dimensional".format(coord, coord_var.ndim)
7335             )
7336 
7337         dim = coord_var.dims[0]
7338         if _contains_datetime_like_objects(coord_var):
7339             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7340                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7341             elif datetime_unit is None:
7342                 datetime_unit = "s"  # Default to seconds for cftime objects
7343             coord_var = coord_var._replace(
7344                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7345             )
7346 
7347         variables = {}
7348         coord_names = set()
7349         for k, v in self.variables.items():
7350             if k in self.coords:
7351                 if dim not in v.dims or cumulative:
7352                     variables[k] = v
7353                     coord_names.add(k)
7354             else:
7355                 if k in self.data_vars and dim in v.dims:
7356                     if _contains_datetime_like_objects(v):
7357                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7358                     if cumulative:
7359                         integ = duck_array_ops.cumulative_trapezoid(
7360                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7361                         )
7362                         v_dims = v.dims
7363                     else:
7364                         integ = duck_array_ops.trapz(
7365                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7366                         )
7367                         v_dims = list(v.dims)
7368                         v_dims.remove(dim)
7369                     variables[k] = Variable(v_dims, integ)
7370                 else:
7371                     variables[k] = v
7372         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7373         return self._replace_with_new_dims(
7374             variables, coord_names=coord_names, indexes=indexes
7375         )
7376 
7377     def cumulative_integrate(
7378         self: T_Dataset,
7379         coord: Hashable | Sequence[Hashable],
7380         datetime_unit: DatetimeUnitOptions = None,
7381     ) -> T_Dataset:
7382         """Integrate along the given coordinate using the trapezoidal rule.
7383 
7384         .. note::
7385             This feature is limited to simple cartesian geometry, i.e. coord
7386             must be one dimensional.
7387 
7388             The first entry of the cumulative integral of each variable is always 0, in
7389             order to keep the length of the dimension unchanged between input and
7390             output.
7391 
7392         Parameters
7393         ----------
7394         coord : hashable, or sequence of hashable
7395             Coordinate(s) used for the integration.
7396         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7397                         'ps', 'fs', 'as', None}, optional
7398             Specify the unit if datetime coordinate is used.
7399 
7400         Returns
7401         -------
7402         integrated : Dataset
7403 
7404         See also
7405         --------
7406         DataArray.cumulative_integrate
7407         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7408 
7409         Examples
7410         --------
7411         >>> ds = xr.Dataset(
7412         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7413         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7414         ... )
7415         >>> ds
7416         <xarray.Dataset>
7417         Dimensions:  (x: 4)
7418         Coordinates:
7419           * x        (x) int64 0 1 2 3
7420             y        (x) int64 1 7 3 5
7421         Data variables:
7422             a        (x) int64 5 5 6 6
7423             b        (x) int64 1 2 1 0
7424         >>> ds.cumulative_integrate("x")
7425         <xarray.Dataset>
7426         Dimensions:  (x: 4)
7427         Coordinates:
7428           * x        (x) int64 0 1 2 3
7429             y        (x) int64 1 7 3 5
7430         Data variables:
7431             a        (x) float64 0.0 5.0 10.5 16.5
7432             b        (x) float64 0.0 1.5 3.0 3.5
7433         >>> ds.cumulative_integrate("y")
7434         <xarray.Dataset>
7435         Dimensions:  (x: 4)
7436         Coordinates:
7437           * x        (x) int64 0 1 2 3
7438             y        (x) int64 1 7 3 5
7439         Data variables:
7440             a        (x) float64 0.0 30.0 8.0 20.0
7441             b        (x) float64 0.0 9.0 3.0 4.0
7442         """
7443         if not isinstance(coord, (list, tuple)):
7444             coord = (coord,)
7445         result = self
7446         for c in coord:
7447             result = result._integrate_one(
7448                 c, datetime_unit=datetime_unit, cumulative=True
7449             )
7450         return result
7451 
7452     @property
7453     def real(self: T_Dataset) -> T_Dataset:
7454         """
7455         The real part of each data variable.
7456 
7457         See Also
7458         --------
7459         numpy.ndarray.real
7460         """
7461         return self.map(lambda x: x.real, keep_attrs=True)
7462 
7463     @property
7464     def imag(self: T_Dataset) -> T_Dataset:
7465         """
7466         The imaginary part of each data variable.
7467 
7468         See Also
7469         --------
7470         numpy.ndarray.imag
7471         """
7472         return self.map(lambda x: x.imag, keep_attrs=True)
7473 
7474     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7475 
7476     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7477         """Returns a ``Dataset`` with variables that match specific conditions.
7478 
7479         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7480         containing only the variables for which all the filter tests pass.
7481         These tests are either ``key=value`` for which the attribute ``key``
7482         has the exact value ``value`` or the callable passed into
7483         ``key=callable`` returns True. The callable will be passed a single
7484         value, either the value of the attribute ``key`` or ``None`` if the
7485         DataArray does not have an attribute with the name ``key``.
7486 
7487         Parameters
7488         ----------
7489         **kwargs
7490             key : str
7491                 Attribute name.
7492             value : callable or obj
7493                 If value is a callable, it should return a boolean in the form
7494                 of bool = func(attr) where attr is da.attrs[key].
7495                 Otherwise, value will be compared to the each
7496                 DataArray's attrs[key].
7497 
7498         Returns
7499         -------
7500         new : Dataset
7501             New dataset with variables filtered by attribute.
7502 
7503         Examples
7504         --------
7505         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7506         >>> precip = 10 * np.random.rand(2, 2, 3)
7507         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7508         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7509         >>> dims = ["x", "y", "time"]
7510         >>> temp_attr = dict(standard_name="air_potential_temperature")
7511         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7512 
7513         >>> ds = xr.Dataset(
7514         ...     dict(
7515         ...         temperature=(dims, temp, temp_attr),
7516         ...         precipitation=(dims, precip, precip_attr),
7517         ...     ),
7518         ...     coords=dict(
7519         ...         lon=(["x", "y"], lon),
7520         ...         lat=(["x", "y"], lat),
7521         ...         time=pd.date_range("2014-09-06", periods=3),
7522         ...         reference_time=pd.Timestamp("2014-09-05"),
7523         ...     ),
7524         ... )
7525 
7526         Get variables matching a specific standard_name:
7527 
7528         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7529         <xarray.Dataset>
7530         Dimensions:         (x: 2, y: 2, time: 3)
7531         Coordinates:
7532             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7533             lat             (x, y) float64 42.25 42.21 42.63 42.59
7534           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7535             reference_time  datetime64[ns] 2014-09-05
7536         Dimensions without coordinates: x, y
7537         Data variables:
7538             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7539 
7540         Get all variables that have a standard_name attribute:
7541 
7542         >>> standard_name = lambda v: v is not None
7543         >>> ds.filter_by_attrs(standard_name=standard_name)
7544         <xarray.Dataset>
7545         Dimensions:         (x: 2, y: 2, time: 3)
7546         Coordinates:
7547             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7548             lat             (x, y) float64 42.25 42.21 42.63 42.59
7549           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7550             reference_time  datetime64[ns] 2014-09-05
7551         Dimensions without coordinates: x, y
7552         Data variables:
7553             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7554             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7555 
7556         """
7557         selection = []
7558         for var_name, variable in self.variables.items():
7559             has_value_flag = False
7560             for attr_name, pattern in kwargs.items():
7561                 attr_value = variable.attrs.get(attr_name)
7562                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7563                     has_value_flag = True
7564                 else:
7565                     has_value_flag = False
7566                     break
7567             if has_value_flag is True:
7568                 selection.append(var_name)
7569         return self[selection]
7570 
7571     def unify_chunks(self: T_Dataset) -> T_Dataset:
7572         """Unify chunk size along all chunked dimensions of this Dataset.
7573 
7574         Returns
7575         -------
7576         Dataset with consistent chunk sizes for all dask-array variables
7577 
7578         See Also
7579         --------
7580         dask.array.core.unify_chunks
7581         """
7582 
7583         return unify_chunks(self)[0]
7584 
7585     def map_blocks(
7586         self,
7587         func: Callable[..., T_Xarray],
7588         args: Sequence[Any] = (),
7589         kwargs: Mapping[str, Any] | None = None,
7590         template: DataArray | Dataset | None = None,
7591     ) -> T_Xarray:
7592         """
7593         Apply a function to each block of this Dataset.
7594 
7595         .. warning::
7596             This method is experimental and its signature may change.
7597 
7598         Parameters
7599         ----------
7600         func : callable
7601             User-provided function that accepts a Dataset as its first
7602             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7603             corresponding to one chunk along each chunked dimension. ``func`` will be
7604             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7605 
7606             This function must return either a single DataArray or a single Dataset.
7607 
7608             This function cannot add a new chunked dimension.
7609         args : sequence
7610             Passed to func after unpacking and subsetting any xarray objects by blocks.
7611             xarray objects in args must be aligned with obj, otherwise an error is raised.
7612         kwargs : Mapping or None
7613             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7614             subset to blocks. Passing dask collections in kwargs is not allowed.
7615         template : DataArray, Dataset or None, optional
7616             xarray object representing the final result after compute is called. If not provided,
7617             the function will be first run on mocked-up data, that looks like this object but
7618             has sizes 0, to determine properties of the returned object such as dtype,
7619             variable names, attributes, new dimensions and new indexes (if any).
7620             ``template`` must be provided if the function changes the size of existing dimensions.
7621             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7622             ``attrs`` set by ``func`` will be ignored.
7623 
7624         Returns
7625         -------
7626         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7627         function.
7628 
7629         Notes
7630         -----
7631         This function is designed for when ``func`` needs to manipulate a whole xarray object
7632         subset to each block. Each block is loaded into memory. In the more common case where
7633         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7634 
7635         If none of the variables in this object is backed by dask arrays, calling this function is
7636         equivalent to calling ``func(obj, *args, **kwargs)``.
7637 
7638         See Also
7639         --------
7640         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7641         xarray.DataArray.map_blocks
7642 
7643         Examples
7644         --------
7645         Calculate an anomaly from climatology using ``.groupby()``. Using
7646         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7647         its indices, and its methods like ``.groupby()``.
7648 
7649         >>> def calculate_anomaly(da, groupby_type="time.month"):
7650         ...     gb = da.groupby(groupby_type)
7651         ...     clim = gb.mean(dim="time")
7652         ...     return gb - clim
7653         ...
7654         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7655         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7656         >>> np.random.seed(123)
7657         >>> array = xr.DataArray(
7658         ...     np.random.rand(len(time)),
7659         ...     dims=["time"],
7660         ...     coords={"time": time, "month": month},
7661         ... ).chunk()
7662         >>> ds = xr.Dataset({"a": array})
7663         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7664         <xarray.Dataset>
7665         Dimensions:  (time: 24)
7666         Coordinates:
7667           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7668             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7669         Data variables:
7670             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7671 
7672         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7673         to the function being applied in ``xr.map_blocks()``:
7674 
7675         >>> ds.map_blocks(
7676         ...     calculate_anomaly,
7677         ...     kwargs={"groupby_type": "time.year"},
7678         ...     template=ds,
7679         ... )
7680         <xarray.Dataset>
7681         Dimensions:  (time: 24)
7682         Coordinates:
7683           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7684             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7685         Data variables:
7686             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7687         """
7688         from .parallel import map_blocks
7689 
7690         return map_blocks(func, self, args, kwargs, template)
7691 
7692     def polyfit(
7693         self: T_Dataset,
7694         dim: Hashable,
7695         deg: int,
7696         skipna: bool | None = None,
7697         rcond: float | None = None,
7698         w: Hashable | Any = None,
7699         full: bool = False,
7700         cov: bool | Literal["unscaled"] = False,
7701     ) -> T_Dataset:
7702         """
7703         Least squares polynomial fit.
7704 
7705         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7706         invalid values when `skipna = True`.
7707 
7708         Parameters
7709         ----------
7710         dim : hashable
7711             Coordinate along which to fit the polynomials.
7712         deg : int
7713             Degree of the fitting polynomial.
7714         skipna : bool or None, optional
7715             If True, removes all invalid values before fitting each 1D slices of the array.
7716             Default is True if data is stored in a dask.array or if there is any
7717             invalid values, False otherwise.
7718         rcond : float or None, optional
7719             Relative condition number to the fit.
7720         w : hashable or Any, optional
7721             Weights to apply to the y-coordinate of the sample points.
7722             Can be an array-like object or the name of a coordinate in the dataset.
7723         full : bool, default: False
7724             Whether to return the residuals, matrix rank and singular values in addition
7725             to the coefficients.
7726         cov : bool or "unscaled", default: False
7727             Whether to return to the covariance matrix in addition to the coefficients.
7728             The matrix is not scaled if `cov='unscaled'`.
7729 
7730         Returns
7731         -------
7732         polyfit_results : Dataset
7733             A single dataset which contains (for each "var" in the input dataset):
7734 
7735             [var]_polyfit_coefficients
7736                 The coefficients of the best fit for each variable in this dataset.
7737             [var]_polyfit_residuals
7738                 The residuals of the least-square computation for each variable (only included if `full=True`)
7739                 When the matrix rank is deficient, np.nan is returned.
7740             [dim]_matrix_rank
7741                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7742                 The rank is computed ignoring the NaN values that might be skipped.
7743             [dim]_singular_values
7744                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7745             [var]_polyfit_covariance
7746                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7747 
7748         Warns
7749         -----
7750         RankWarning
7751             The rank of the coefficient matrix in the least-squares fit is deficient.
7752             The warning is not raised with in-memory (not dask) data and `full=True`.
7753 
7754         See Also
7755         --------
7756         numpy.polyfit
7757         numpy.polyval
7758         xarray.polyval
7759         """
7760         from .dataarray import DataArray
7761 
7762         variables = {}
7763         skipna_da = skipna
7764 
7765         x = get_clean_interp_index(self, dim, strict=False)
7766         xname = f"{self[dim].name}_"
7767         order = int(deg) + 1
7768         lhs = np.vander(x, order)
7769 
7770         if rcond is None:
7771             rcond = (
7772                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7773             )
7774 
7775         # Weights:
7776         if w is not None:
7777             if isinstance(w, Hashable):
7778                 w = self.coords[w]
7779             w = np.asarray(w)
7780             if w.ndim != 1:
7781                 raise TypeError("Expected a 1-d array for weights.")
7782             if w.shape[0] != lhs.shape[0]:
7783                 raise TypeError(f"Expected w and {dim} to have the same length")
7784             lhs *= w[:, np.newaxis]
7785 
7786         # Scaling
7787         scale = np.sqrt((lhs * lhs).sum(axis=0))
7788         lhs /= scale
7789 
7790         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7791 
7792         rank = np.linalg.matrix_rank(lhs)
7793 
7794         if full:
7795             rank = DataArray(rank, name=xname + "matrix_rank")
7796             variables[rank.name] = rank
7797             _sing = np.linalg.svd(lhs, compute_uv=False)
7798             sing = DataArray(
7799                 _sing,
7800                 dims=(degree_dim,),
7801                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7802                 name=xname + "singular_values",
7803             )
7804             variables[sing.name] = sing
7805 
7806         for name, da in self.data_vars.items():
7807             if dim not in da.dims:
7808                 continue
7809 
7810             if is_duck_dask_array(da.data) and (
7811                 rank != order or full or skipna is None
7812             ):
7813                 # Current algorithm with dask and skipna=False neither supports
7814                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7815                 skipna_da = True
7816             elif skipna is None:
7817                 skipna_da = bool(np.any(da.isnull()))
7818 
7819             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7820             stacked_coords: dict[Hashable, DataArray] = {}
7821             if dims_to_stack:
7822                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7823                 rhs = da.transpose(dim, *dims_to_stack).stack(
7824                     {stacked_dim: dims_to_stack}
7825                 )
7826                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7827                 scale_da = scale[:, np.newaxis]
7828             else:
7829                 rhs = da
7830                 scale_da = scale
7831 
7832             if w is not None:
7833                 rhs *= w[:, np.newaxis]
7834 
7835             with warnings.catch_warnings():
7836                 if full:  # Copy np.polyfit behavior
7837                     warnings.simplefilter("ignore", np.RankWarning)
7838                 else:  # Raise only once per variable
7839                     warnings.simplefilter("once", np.RankWarning)
7840 
7841                 coeffs, residuals = duck_array_ops.least_squares(
7842                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7843                 )
7844 
7845             if isinstance(name, str):
7846                 name = f"{name}_"
7847             else:
7848                 # Thus a ReprObject => polyfit was called on a DataArray
7849                 name = ""
7850 
7851             coeffs = DataArray(
7852                 coeffs / scale_da,
7853                 dims=[degree_dim] + list(stacked_coords.keys()),
7854                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7855                 name=name + "polyfit_coefficients",
7856             )
7857             if dims_to_stack:
7858                 coeffs = coeffs.unstack(stacked_dim)
7859             variables[coeffs.name] = coeffs
7860 
7861             if full or (cov is True):
7862                 residuals = DataArray(
7863                     residuals if dims_to_stack else residuals.squeeze(),
7864                     dims=list(stacked_coords.keys()),
7865                     coords=stacked_coords,
7866                     name=name + "polyfit_residuals",
7867                 )
7868                 if dims_to_stack:
7869                     residuals = residuals.unstack(stacked_dim)
7870                 variables[residuals.name] = residuals
7871 
7872             if cov:
7873                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7874                 Vbase /= np.outer(scale, scale)
7875                 if cov == "unscaled":
7876                     fac = 1
7877                 else:
7878                     if x.shape[0] <= order:
7879                         raise ValueError(
7880                             "The number of data points must exceed order to scale the covariance matrix."
7881                         )
7882                     fac = residuals / (x.shape[0] - order)
7883                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7884                 variables[name + "polyfit_covariance"] = covariance
7885 
7886         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7887 
7888     def pad(
7889         self: T_Dataset,
7890         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7891         mode: PadModeOptions = "constant",
7892         stat_length: int
7893         | tuple[int, int]
7894         | Mapping[Any, tuple[int, int]]
7895         | None = None,
7896         constant_values: (
7897             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7898         ) = None,
7899         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7900         reflect_type: PadReflectOptions = None,
7901         **pad_width_kwargs: Any,
7902     ) -> T_Dataset:
7903         """Pad this dataset along one or more dimensions.
7904 
7905         .. warning::
7906             This function is experimental and its behaviour is likely to change
7907             especially regarding padding of dimension coordinates (or IndexVariables).
7908 
7909         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7910         coordinates will be padded with the same mode, otherwise coordinates
7911         are padded using the "constant" mode with fill_value dtypes.NA.
7912 
7913         Parameters
7914         ----------
7915         pad_width : mapping of hashable to tuple of int
7916             Mapping with the form of {dim: (pad_before, pad_after)}
7917             describing the number of values padded along each dimension.
7918             {dim: pad} is a shortcut for pad_before = pad_after = pad
7919         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7920             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7921             How to pad the DataArray (taken from numpy docs):
7922 
7923             - "constant": Pads with a constant value.
7924             - "edge": Pads with the edge values of array.
7925             - "linear_ramp": Pads with the linear ramp between end_value and the
7926               array edge value.
7927             - "maximum": Pads with the maximum value of all or part of the
7928               vector along each axis.
7929             - "mean": Pads with the mean value of all or part of the
7930               vector along each axis.
7931             - "median": Pads with the median value of all or part of the
7932               vector along each axis.
7933             - "minimum": Pads with the minimum value of all or part of the
7934               vector along each axis.
7935             - "reflect": Pads with the reflection of the vector mirrored on
7936               the first and last values of the vector along each axis.
7937             - "symmetric": Pads with the reflection of the vector mirrored
7938               along the edge of the array.
7939             - "wrap": Pads with the wrap of the vector along the axis.
7940               The first values are used to pad the end and the
7941               end values are used to pad the beginning.
7942 
7943         stat_length : int, tuple or mapping of hashable to tuple, default: None
7944             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7945             values at edge of each axis used to calculate the statistic value.
7946             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7947             statistic lengths along each dimension.
7948             ((before, after),) yields same before and after statistic lengths
7949             for each dimension.
7950             (stat_length,) or int is a shortcut for before = after = statistic
7951             length for all axes.
7952             Default is ``None``, to use the entire axis.
7953         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7954             Used in 'constant'.  The values to set the padded values for each
7955             axis.
7956             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7957             pad constants along each dimension.
7958             ``((before, after),)`` yields same before and after constants for each
7959             dimension.
7960             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7961             all dimensions.
7962             Default is 0.
7963         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7964             Used in 'linear_ramp'.  The values used for the ending value of the
7965             linear_ramp and that will form the edge of the padded array.
7966             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7967             end values along each dimension.
7968             ``((before, after),)`` yields same before and after end values for each
7969             axis.
7970             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7971             all axes.
7972             Default is 0.
7973         reflect_type : {"even", "odd", None}, optional
7974             Used in "reflect", and "symmetric".  The "even" style is the
7975             default with an unaltered reflection around the edge value.  For
7976             the "odd" style, the extended part of the array is created by
7977             subtracting the reflected values from two times the edge value.
7978         **pad_width_kwargs
7979             The keyword arguments form of ``pad_width``.
7980             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7981 
7982         Returns
7983         -------
7984         padded : Dataset
7985             Dataset with the padded coordinates and data.
7986 
7987         See Also
7988         --------
7989         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7990 
7991         Notes
7992         -----
7993         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7994         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7995         specify ``constant_values=np.nan``
7996 
7997         Padding coordinates will drop their corresponding index (if any) and will reset default
7998         indexes for dimension coordinates.
7999 
8000         Examples
8001         --------
8002         >>> ds = xr.Dataset({"foo": ("x", range(5))})
8003         >>> ds.pad(x=(1, 2))
8004         <xarray.Dataset>
8005         Dimensions:  (x: 8)
8006         Dimensions without coordinates: x
8007         Data variables:
8008             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
8009         """
8010         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
8011 
8012         if mode in ("edge", "reflect", "symmetric", "wrap"):
8013             coord_pad_mode = mode
8014             coord_pad_options = {
8015                 "stat_length": stat_length,
8016                 "constant_values": constant_values,
8017                 "end_values": end_values,
8018                 "reflect_type": reflect_type,
8019             }
8020         else:
8021             coord_pad_mode = "constant"
8022             coord_pad_options = {}
8023 
8024         variables = {}
8025 
8026         # keep indexes that won't be affected by pad and drop all other indexes
8027         xindexes = self.xindexes
8028         pad_dims = set(pad_width)
8029         indexes = {}
8030         for k, idx in xindexes.items():
8031             if not pad_dims.intersection(xindexes.get_all_dims(k)):
8032                 indexes[k] = idx
8033 
8034         for name, var in self.variables.items():
8035             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
8036             if not var_pad_width:
8037                 variables[name] = var
8038             elif name in self.data_vars:
8039                 variables[name] = var.pad(
8040                     pad_width=var_pad_width,
8041                     mode=mode,
8042                     stat_length=stat_length,
8043                     constant_values=constant_values,
8044                     end_values=end_values,
8045                     reflect_type=reflect_type,
8046                 )
8047             else:
8048                 variables[name] = var.pad(
8049                     pad_width=var_pad_width,
8050                     mode=coord_pad_mode,
8051                     **coord_pad_options,  # type: ignore[arg-type]
8052                 )
8053                 # reset default index of dimension coordinates
8054                 if (name,) == var.dims:
8055                     dim_var = {name: variables[name]}
8056                     index = PandasIndex.from_variables(dim_var, options={})
8057                     index_vars = index.create_variables(dim_var)
8058                     indexes[name] = index
8059                     variables[name] = index_vars[name]
8060 
8061         return self._replace_with_new_dims(variables, indexes=indexes)
8062 
8063     def idxmin(
8064         self: T_Dataset,
8065         dim: Hashable | None = None,
8066         skipna: bool | None = None,
8067         fill_value: Any = xrdtypes.NA,
8068         keep_attrs: bool | None = None,
8069     ) -> T_Dataset:
8070         """Return the coordinate label of the minimum value along a dimension.
8071 
8072         Returns a new `Dataset` named after the dimension with the values of
8073         the coordinate labels along that dimension corresponding to minimum
8074         values along that dimension.
8075 
8076         In comparison to :py:meth:`~Dataset.argmin`, this returns the
8077         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
8078 
8079         Parameters
8080         ----------
8081         dim : Hashable, optional
8082             Dimension over which to apply `idxmin`.  This is optional for 1D
8083             variables, but required for variables with 2 or more dimensions.
8084         skipna : bool or None, optional
8085             If True, skip missing values (as marked by NaN). By default, only
8086             skips missing values for ``float``, ``complex``, and ``object``
8087             dtypes; other dtypes either do not have a sentinel missing value
8088             (``int``) or ``skipna=True`` has not been implemented
8089             (``datetime64`` or ``timedelta64``).
8090         fill_value : Any, default: NaN
8091             Value to be filled in case all of the values along a dimension are
8092             null.  By default this is NaN.  The fill value and result are
8093             automatically converted to a compatible dtype if possible.
8094             Ignored if ``skipna`` is False.
8095         keep_attrs : bool or None, optional
8096             If True, the attributes (``attrs``) will be copied from the
8097             original object to the new one. If False, the new object
8098             will be returned without attributes.
8099 
8100         Returns
8101         -------
8102         reduced : Dataset
8103             New `Dataset` object with `idxmin` applied to its data and the
8104             indicated dimension removed.
8105 
8106         See Also
8107         --------
8108         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
8109 
8110         Examples
8111         --------
8112         >>> array1 = xr.DataArray(
8113         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8114         ... )
8115         >>> array2 = xr.DataArray(
8116         ...     [
8117         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8118         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8119         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8120         ...     ],
8121         ...     dims=["y", "x"],
8122         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8123         ... )
8124         >>> ds = xr.Dataset({"int": array1, "float": array2})
8125         >>> ds.min(dim="x")
8126         <xarray.Dataset>
8127         Dimensions:  (y: 3)
8128         Coordinates:
8129           * y        (y) int64 -1 0 1
8130         Data variables:
8131             int      int64 -2
8132             float    (y) float64 -2.0 -4.0 1.0
8133         >>> ds.argmin(dim="x")
8134         <xarray.Dataset>
8135         Dimensions:  (y: 3)
8136         Coordinates:
8137           * y        (y) int64 -1 0 1
8138         Data variables:
8139             int      int64 4
8140             float    (y) int64 4 0 2
8141         >>> ds.idxmin(dim="x")
8142         <xarray.Dataset>
8143         Dimensions:  (y: 3)
8144         Coordinates:
8145           * y        (y) int64 -1 0 1
8146         Data variables:
8147             int      <U1 'e'
8148             float    (y) object 'e' 'a' 'c'
8149         """
8150         return self.map(
8151             methodcaller(
8152                 "idxmin",
8153                 dim=dim,
8154                 skipna=skipna,
8155                 fill_value=fill_value,
8156                 keep_attrs=keep_attrs,
8157             )
8158         )
8159 
8160     def idxmax(
8161         self: T_Dataset,
8162         dim: Hashable | None = None,
8163         skipna: bool | None = None,
8164         fill_value: Any = xrdtypes.NA,
8165         keep_attrs: bool | None = None,
8166     ) -> T_Dataset:
8167         """Return the coordinate label of the maximum value along a dimension.
8168 
8169         Returns a new `Dataset` named after the dimension with the values of
8170         the coordinate labels along that dimension corresponding to maximum
8171         values along that dimension.
8172 
8173         In comparison to :py:meth:`~Dataset.argmax`, this returns the
8174         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
8175 
8176         Parameters
8177         ----------
8178         dim : str, optional
8179             Dimension over which to apply `idxmax`.  This is optional for 1D
8180             variables, but required for variables with 2 or more dimensions.
8181         skipna : bool or None, optional
8182             If True, skip missing values (as marked by NaN). By default, only
8183             skips missing values for ``float``, ``complex``, and ``object``
8184             dtypes; other dtypes either do not have a sentinel missing value
8185             (``int``) or ``skipna=True`` has not been implemented
8186             (``datetime64`` or ``timedelta64``).
8187         fill_value : Any, default: NaN
8188             Value to be filled in case all of the values along a dimension are
8189             null.  By default this is NaN.  The fill value and result are
8190             automatically converted to a compatible dtype if possible.
8191             Ignored if ``skipna`` is False.
8192         keep_attrs : bool or None, optional
8193             If True, the attributes (``attrs``) will be copied from the
8194             original object to the new one. If False, the new object
8195             will be returned without attributes.
8196 
8197         Returns
8198         -------
8199         reduced : Dataset
8200             New `Dataset` object with `idxmax` applied to its data and the
8201             indicated dimension removed.
8202 
8203         See Also
8204         --------
8205         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
8206 
8207         Examples
8208         --------
8209         >>> array1 = xr.DataArray(
8210         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
8211         ... )
8212         >>> array2 = xr.DataArray(
8213         ...     [
8214         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
8215         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
8216         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
8217         ...     ],
8218         ...     dims=["y", "x"],
8219         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
8220         ... )
8221         >>> ds = xr.Dataset({"int": array1, "float": array2})
8222         >>> ds.max(dim="x")
8223         <xarray.Dataset>
8224         Dimensions:  (y: 3)
8225         Coordinates:
8226           * y        (y) int64 -1 0 1
8227         Data variables:
8228             int      int64 2
8229             float    (y) float64 2.0 2.0 1.0
8230         >>> ds.argmax(dim="x")
8231         <xarray.Dataset>
8232         Dimensions:  (y: 3)
8233         Coordinates:
8234           * y        (y) int64 -1 0 1
8235         Data variables:
8236             int      int64 1
8237             float    (y) int64 0 2 2
8238         >>> ds.idxmax(dim="x")
8239         <xarray.Dataset>
8240         Dimensions:  (y: 3)
8241         Coordinates:
8242           * y        (y) int64 -1 0 1
8243         Data variables:
8244             int      <U1 'b'
8245             float    (y) object 'a' 'c' 'c'
8246         """
8247         return self.map(
8248             methodcaller(
8249                 "idxmax",
8250                 dim=dim,
8251                 skipna=skipna,
8252                 fill_value=fill_value,
8253                 keep_attrs=keep_attrs,
8254             )
8255         )
8256 
8257     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8258         """Indices of the minima of the member variables.
8259 
8260         If there are multiple minima, the indices of the first one found will be
8261         returned.
8262 
8263         Parameters
8264         ----------
8265         dim : Hashable, optional
8266             The dimension over which to find the minimum. By default, finds minimum over
8267             all dimensions - for now returning an int for backward compatibility, but
8268             this is deprecated, in future will be an error, since DataArray.argmin will
8269             return a dict with indices for all dimensions, which does not make sense for
8270             a Dataset.
8271         keep_attrs : bool, optional
8272             If True, the attributes (`attrs`) will be copied from the original
8273             object to the new one.  If False (default), the new object will be
8274             returned without attributes.
8275         skipna : bool, optional
8276             If True, skip missing values (as marked by NaN). By default, only
8277             skips missing values for float dtypes; other dtypes either do not
8278             have a sentinel missing value (int) or skipna=True has not been
8279             implemented (object, datetime64 or timedelta64).
8280 
8281         Returns
8282         -------
8283         result : Dataset
8284 
8285         See Also
8286         --------
8287         DataArray.argmin
8288         """
8289         if dim is None:
8290             warnings.warn(
8291                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8292                 "dim changes to return a dict of indices of each dimension, for "
8293                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8294                 "since we don't return a dict of Datasets.",
8295                 DeprecationWarning,
8296                 stacklevel=2,
8297             )
8298         if (
8299             dim is None
8300             or (not isinstance(dim, Sequence) and dim is not ...)
8301             or isinstance(dim, str)
8302         ):
8303             # Return int index if single dimension is passed, and is not part of a
8304             # sequence
8305             argmin_func = getattr(duck_array_ops, "argmin")
8306             return self.reduce(argmin_func, dim=dim, **kwargs)
8307         else:
8308             raise ValueError(
8309                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8310                 "dicts cannot be contained in a Dataset, so cannot call "
8311                 "Dataset.argmin() with a sequence or ... for dim"
8312             )
8313 
8314     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8315         """Indices of the maxima of the member variables.
8316 
8317         If there are multiple maxima, the indices of the first one found will be
8318         returned.
8319 
8320         Parameters
8321         ----------
8322         dim : str, optional
8323             The dimension over which to find the maximum. By default, finds maximum over
8324             all dimensions - for now returning an int for backward compatibility, but
8325             this is deprecated, in future will be an error, since DataArray.argmax will
8326             return a dict with indices for all dimensions, which does not make sense for
8327             a Dataset.
8328         keep_attrs : bool, optional
8329             If True, the attributes (`attrs`) will be copied from the original
8330             object to the new one.  If False (default), the new object will be
8331             returned without attributes.
8332         skipna : bool, optional
8333             If True, skip missing values (as marked by NaN). By default, only
8334             skips missing values for float dtypes; other dtypes either do not
8335             have a sentinel missing value (int) or skipna=True has not been
8336             implemented (object, datetime64 or timedelta64).
8337 
8338         Returns
8339         -------
8340         result : Dataset
8341 
8342         See Also
8343         --------
8344         DataArray.argmax
8345 
8346         """
8347         if dim is None:
8348             warnings.warn(
8349                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8350                 "dim changes to return a dict of indices of each dimension, for "
8351                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8352                 "since we don't return a dict of Datasets.",
8353                 DeprecationWarning,
8354                 stacklevel=2,
8355             )
8356         if (
8357             dim is None
8358             or (not isinstance(dim, Sequence) and dim is not ...)
8359             or isinstance(dim, str)
8360         ):
8361             # Return int index if single dimension is passed, and is not part of a
8362             # sequence
8363             argmax_func = getattr(duck_array_ops, "argmax")
8364             return self.reduce(argmax_func, dim=dim, **kwargs)
8365         else:
8366             raise ValueError(
8367                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8368                 "dicts cannot be contained in a Dataset, so cannot call "
8369                 "Dataset.argmin() with a sequence or ... for dim"
8370             )
8371 
8372     def query(
8373         self: T_Dataset,
8374         queries: Mapping[Any, Any] | None = None,
8375         parser: QueryParserOptions = "pandas",
8376         engine: QueryEngineOptions = None,
8377         missing_dims: ErrorOptionsWithWarn = "raise",
8378         **queries_kwargs: Any,
8379     ) -> T_Dataset:
8380         """Return a new dataset with each array indexed along the specified
8381         dimension(s), where the indexers are given as strings containing
8382         Python expressions to be evaluated against the data variables in the
8383         dataset.
8384 
8385         Parameters
8386         ----------
8387         queries : dict-like, optional
8388             A dict-like with keys matching dimensions and values given by strings
8389             containing Python expressions to be evaluated against the data variables
8390             in the dataset. The expressions will be evaluated using the pandas
8391             eval() function, and can contain any valid Python expressions but cannot
8392             contain any Python statements.
8393         parser : {"pandas", "python"}, default: "pandas"
8394             The parser to use to construct the syntax tree from the expression.
8395             The default of 'pandas' parses code slightly different than standard
8396             Python. Alternatively, you can parse an expression using the 'python'
8397             parser to retain strict Python semantics.
8398         engine : {"python", "numexpr", None}, default: None
8399             The engine used to evaluate the expression. Supported engines are:
8400 
8401             - None: tries to use numexpr, falls back to python
8402             - "numexpr": evaluates expressions using numexpr
8403             - "python": performs operations as if you had evald in top level python
8404 
8405         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8406             What to do if dimensions that should be selected from are not present in the
8407             Dataset:
8408 
8409             - "raise": raise an exception
8410             - "warn": raise a warning, and ignore the missing dimensions
8411             - "ignore": ignore the missing dimensions
8412 
8413         **queries_kwargs : {dim: query, ...}, optional
8414             The keyword arguments form of ``queries``.
8415             One of queries or queries_kwargs must be provided.
8416 
8417         Returns
8418         -------
8419         obj : Dataset
8420             A new Dataset with the same contents as this dataset, except each
8421             array and dimension is indexed by the results of the appropriate
8422             queries.
8423 
8424         See Also
8425         --------
8426         Dataset.isel
8427         pandas.eval
8428 
8429         Examples
8430         --------
8431         >>> a = np.arange(0, 5, 1)
8432         >>> b = np.linspace(0, 1, 5)
8433         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8434         >>> ds
8435         <xarray.Dataset>
8436         Dimensions:  (x: 5)
8437         Dimensions without coordinates: x
8438         Data variables:
8439             a        (x) int64 0 1 2 3 4
8440             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8441         >>> ds.query(x="a > 2")
8442         <xarray.Dataset>
8443         Dimensions:  (x: 2)
8444         Dimensions without coordinates: x
8445         Data variables:
8446             a        (x) int64 3 4
8447             b        (x) float64 0.75 1.0
8448         """
8449 
8450         # allow queries to be given either as a dict or as kwargs
8451         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8452 
8453         # check queries
8454         for dim, expr in queries.items():
8455             if not isinstance(expr, str):
8456                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8457                 raise ValueError(msg)
8458 
8459         # evaluate the queries to create the indexers
8460         indexers = {
8461             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8462             for dim, expr in queries.items()
8463         }
8464 
8465         # apply the selection
8466         return self.isel(indexers, missing_dims=missing_dims)
8467 
8468     def curvefit(
8469         self: T_Dataset,
8470         coords: str | DataArray | Iterable[str | DataArray],
8471         func: Callable[..., Any],
8472         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8473         skipna: bool = True,
8474         p0: dict[str, Any] | None = None,
8475         bounds: dict[str, Any] | None = None,
8476         param_names: Sequence[str] | None = None,
8477         kwargs: dict[str, Any] | None = None,
8478     ) -> T_Dataset:
8479         """
8480         Curve fitting optimization for arbitrary functions.
8481 
8482         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8483 
8484         Parameters
8485         ----------
8486         coords : hashable, DataArray, or sequence of hashable or DataArray
8487             Independent coordinate(s) over which to perform the curve fitting. Must share
8488             at least one dimension with the calling object. When fitting multi-dimensional
8489             functions, supply `coords` as a sequence in the same order as arguments in
8490             `func`. To fit along existing dimensions of the calling object, `coords` can
8491             also be specified as a str or sequence of strs.
8492         func : callable
8493             User specified function in the form `f(x, *params)` which returns a numpy
8494             array of length `len(x)`. `params` are the fittable parameters which are optimized
8495             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8496             coordinates, e.g. `f((x0, x1), *params)`.
8497         reduce_dims : hashable or sequence of hashable
8498             Additional dimension(s) over which to aggregate while fitting. For example,
8499             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8500             aggregate all lat and lon points and fit the specified function along the
8501             time dimension.
8502         skipna : bool, default: True
8503             Whether to skip missing values when fitting. Default is True.
8504         p0 : dict-like, optional
8505             Optional dictionary of parameter names to initial guesses passed to the
8506             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8507             be assigned initial values following the default scipy behavior.
8508         bounds : dict-like, optional
8509             Optional dictionary of parameter names to bounding values passed to the
8510             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8511             will be unbounded following the default scipy behavior.
8512         param_names : sequence of hashable, optional
8513             Sequence of names for the fittable parameters of `func`. If not supplied,
8514             this will be automatically determined by arguments of `func`. `param_names`
8515             should be manually supplied when fitting a function that takes a variable
8516             number of parameters.
8517         **kwargs : optional
8518             Additional keyword arguments to passed to scipy curve_fit.
8519 
8520         Returns
8521         -------
8522         curvefit_results : Dataset
8523             A single dataset which contains:
8524 
8525             [var]_curvefit_coefficients
8526                 The coefficients of the best fit.
8527             [var]_curvefit_covariance
8528                 The covariance matrix of the coefficient estimates.
8529 
8530         See Also
8531         --------
8532         Dataset.polyfit
8533         scipy.optimize.curve_fit
8534         """
8535         from scipy.optimize import curve_fit
8536 
8537         from .alignment import broadcast
8538         from .computation import apply_ufunc
8539         from .dataarray import _THIS_ARRAY, DataArray
8540 
8541         if p0 is None:
8542             p0 = {}
8543         if bounds is None:
8544             bounds = {}
8545         if kwargs is None:
8546             kwargs = {}
8547 
8548         if not reduce_dims:
8549             reduce_dims_ = []
8550         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8551             reduce_dims_ = [reduce_dims]
8552         else:
8553             reduce_dims_ = list(reduce_dims)
8554 
8555         if (
8556             isinstance(coords, str)
8557             or isinstance(coords, DataArray)
8558             or not isinstance(coords, Iterable)
8559         ):
8560             coords = [coords]
8561         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8562 
8563         # Determine whether any coords are dims on self
8564         for coord in coords_:
8565             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8566         reduce_dims_ = list(set(reduce_dims_))
8567         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8568         if not reduce_dims_:
8569             raise ValueError(
8570                 "No arguments to `coords` were identified as a dimension on the calling "
8571                 "object, and no dims were supplied to `reduce_dims`. This would result "
8572                 "in fitting on scalar data."
8573             )
8574 
8575         # Broadcast all coords with each other
8576         coords_ = broadcast(*coords_)
8577         coords_ = [
8578             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8579         ]
8580 
8581         params, func_args = _get_func_args(func, param_names)
8582         param_defaults, bounds_defaults = _initialize_curvefit_params(
8583             params, p0, bounds, func_args
8584         )
8585         n_params = len(params)
8586         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8587         kwargs.setdefault(
8588             "bounds",
8589             [
8590                 [bounds_defaults[p][0] for p in params],
8591                 [bounds_defaults[p][1] for p in params],
8592             ],
8593         )
8594 
8595         def _wrapper(Y, *coords_, **kwargs):
8596             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8597             x = np.vstack([c.ravel() for c in coords_])
8598             y = Y.ravel()
8599             if skipna:
8600                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8601                 x = x[:, mask]
8602                 y = y[mask]
8603                 if not len(y):
8604                     popt = np.full([n_params], np.nan)
8605                     pcov = np.full([n_params, n_params], np.nan)
8606                     return popt, pcov
8607             x = np.squeeze(x)
8608             popt, pcov = curve_fit(func, x, y, **kwargs)
8609             return popt, pcov
8610 
8611         result = type(self)()
8612         for name, da in self.data_vars.items():
8613             if name is _THIS_ARRAY:
8614                 name = ""
8615             else:
8616                 name = f"{str(name)}_"
8617 
8618             popt, pcov = apply_ufunc(
8619                 _wrapper,
8620                 da,
8621                 *coords_,
8622                 vectorize=True,
8623                 dask="parallelized",
8624                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8625                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8626                 dask_gufunc_kwargs={
8627                     "output_sizes": {
8628                         "param": n_params,
8629                         "cov_i": n_params,
8630                         "cov_j": n_params,
8631                     },
8632                 },
8633                 output_dtypes=(np.float64, np.float64),
8634                 exclude_dims=set(reduce_dims_),
8635                 kwargs=kwargs,
8636             )
8637             result[name + "curvefit_coefficients"] = popt
8638             result[name + "curvefit_covariance"] = pcov
8639 
8640         result = result.assign_coords(
8641             {"param": params, "cov_i": params, "cov_j": params}
8642         )
8643         result.attrs = self.attrs.copy()
8644 
8645         return result
8646 
8647     def drop_duplicates(
8648         self: T_Dataset,
8649         dim: Hashable | Iterable[Hashable],
8650         keep: Literal["first", "last", False] = "first",
8651     ) -> T_Dataset:
8652         """Returns a new Dataset with duplicate dimension values removed.
8653 
8654         Parameters
8655         ----------
8656         dim : dimension label or labels
8657             Pass `...` to drop duplicates along all dimensions.
8658         keep : {"first", "last", False}, default: "first"
8659             Determines which duplicates (if any) to keep.
8660             - ``"first"`` : Drop duplicates except for the first occurrence.
8661             - ``"last"`` : Drop duplicates except for the last occurrence.
8662             - False : Drop all duplicates.
8663 
8664         Returns
8665         -------
8666         Dataset
8667 
8668         See Also
8669         --------
8670         DataArray.drop_duplicates
8671         """
8672         if isinstance(dim, str):
8673             dims: Iterable = (dim,)
8674         elif dim is ...:
8675             dims = self.dims
8676         elif not isinstance(dim, Iterable):
8677             dims = [dim]
8678         else:
8679             dims = dim
8680 
8681         missing_dims = set(dims) - set(self.dims)
8682         if missing_dims:
8683             raise ValueError(f"'{missing_dims}' not found in dimensions")
8684 
8685         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8686         return self.isel(indexes)
8687 
8688     def convert_calendar(
8689         self: T_Dataset,
8690         calendar: CFCalendar,
8691         dim: Hashable = "time",
8692         align_on: Literal["date", "year", None] = None,
8693         missing: Any | None = None,
8694         use_cftime: bool | None = None,
8695     ) -> T_Dataset:
8696         """Convert the Dataset to another calendar.
8697 
8698         Only converts the individual timestamps, does not modify any data except
8699         in dropping invalid/surplus dates or inserting missing dates.
8700 
8701         If the source and target calendars are either no_leap, all_leap or a
8702         standard type, only the type of the time array is modified.
8703         When converting to a leap year from a non-leap year, the 29th of February
8704         is removed from the array. In the other direction the 29th of February
8705         will be missing in the output, unless `missing` is specified,
8706         in which case that value is inserted.
8707 
8708         For conversions involving `360_day` calendars, see Notes.
8709 
8710         This method is safe to use with sub-daily data as it doesn't touch the
8711         time part of the timestamps.
8712 
8713         Parameters
8714         ---------
8715         calendar : str
8716             The target calendar name.
8717         dim : Hashable, default: "time"
8718             Name of the time coordinate.
8719         align_on : {None, 'date', 'year'}, optional
8720             Must be specified when either source or target is a `360_day` calendar,
8721             ignored otherwise. See Notes.
8722         missing : Any or None, optional
8723             By default, i.e. if the value is None, this method will simply attempt
8724             to convert the dates in the source calendar to the same dates in the
8725             target calendar, and drop any of those that are not possible to
8726             represent.  If a value is provided, a new time coordinate will be
8727             created in the target calendar with the same frequency as the original
8728             time coordinate; for any dates that are not present in the source, the
8729             data will be filled with this value.  Note that using this mode requires
8730             that the source data have an inferable frequency; for more information
8731             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8732             target calendar combinations, this could result in many missing values, see notes.
8733         use_cftime : bool or None, optional
8734             Whether to use cftime objects in the output, only used if `calendar`
8735             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8736             If True, the new time axis uses cftime objects.
8737             If None (default), it uses :py:class:`numpy.datetime64` values if the
8738             date range permits it, and :py:class:`cftime.datetime` objects if not.
8739             If False, it uses :py:class:`numpy.datetime64`  or fails.
8740 
8741         Returns
8742         -------
8743         Dataset
8744             Copy of the dataarray with the time coordinate converted to the
8745             target calendar. If 'missing' was None (default), invalid dates in
8746             the new calendar are dropped, but missing dates are not inserted.
8747             If `missing` was given, the new data is reindexed to have a time axis
8748             with the same frequency as the source, but in the new calendar; any
8749             missing datapoints are filled with `missing`.
8750 
8751         Notes
8752         -----
8753         Passing a value to `missing` is only usable if the source's time coordinate as an
8754         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8755         if the target coordinate, generated from this frequency, has dates equivalent to the
8756         source. It is usually **not** appropriate to use this mode with:
8757 
8758         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8759         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8760             or 'mH' where 24 % m != 0).
8761 
8762         If one of the source or target calendars is `"360_day"`, `align_on` must
8763         be specified and two options are offered.
8764 
8765         - "year"
8766             The dates are translated according to their relative position in the year,
8767             ignoring their original month and day information, meaning that the
8768             missing/surplus days are added/removed at regular intervals.
8769 
8770             From a `360_day` to a standard calendar, the output will be missing the
8771             following dates (day of year in parentheses):
8772 
8773             To a leap year:
8774                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8775                 September 31st (275) and November 30th (335).
8776             To a non-leap year:
8777                 February 6th (36), April 19th (109), July 2nd (183),
8778                 September 12th (255), November 25th (329).
8779 
8780             From a standard calendar to a `"360_day"`, the following dates in the
8781             source array will be dropped:
8782 
8783             From a leap year:
8784                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8785                 September 31st (275), December 1st (336)
8786             From a non-leap year:
8787                 February 6th (37), April 20th (110), July 2nd (183),
8788                 September 13th (256), November 25th (329)
8789 
8790             This option is best used on daily and subdaily data.
8791 
8792         - "date"
8793             The month/day information is conserved and invalid dates are dropped
8794             from the output. This means that when converting from a `"360_day"` to a
8795             standard calendar, all 31st (Jan, March, May, July, August, October and
8796             December) will be missing as there is no equivalent dates in the
8797             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8798             will be dropped as there are no equivalent dates in a standard calendar.
8799 
8800             This option is best used with data on a frequency coarser than daily.
8801         """
8802         return convert_calendar(
8803             self,
8804             calendar,
8805             dim=dim,
8806             align_on=align_on,
8807             missing=missing,
8808             use_cftime=use_cftime,
8809         )
8810 
8811     def interp_calendar(
8812         self: T_Dataset,
8813         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8814         dim: Hashable = "time",
8815     ) -> T_Dataset:
8816         """Interpolates the Dataset to another calendar based on decimal year measure.
8817 
8818         Each timestamp in `source` and `target` are first converted to their decimal
8819         year equivalent then `source` is interpolated on the target coordinate.
8820         The decimal year of a timestamp is its year plus its sub-year component
8821         converted to the fraction of its year. For example "2000-03-01 12:00" is
8822         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8823 
8824         This method should only be used when the time (HH:MM:SS) information of
8825         time coordinate is not important.
8826 
8827         Parameters
8828         ----------
8829         target: DataArray or DatetimeIndex or CFTimeIndex
8830             The target time coordinate of a valid dtype
8831             (np.datetime64 or cftime objects)
8832         dim : Hashable, default: "time"
8833             The time coordinate name.
8834 
8835         Return
8836         ------
8837         DataArray
8838             The source interpolated on the decimal years of target,
8839         """
8840         return interp_calendar(self, target, dim=dim)
8841 
8842     def groupby(
8843         self,
8844         group: Hashable | DataArray | IndexVariable,
8845         squeeze: bool = True,
8846         restore_coord_dims: bool = False,
8847     ) -> DatasetGroupBy:
8848         """Returns a DatasetGroupBy object for performing grouped operations.
8849 
8850         Parameters
8851         ----------
8852         group : Hashable, DataArray or IndexVariable
8853             Array whose unique values should be used to group this array. If a
8854             string, must be the name of a variable contained in this dataset.
8855         squeeze : bool, default: True
8856             If "group" is a dimension of any arrays in this dataset, `squeeze`
8857             controls whether the subarrays have a dimension of length 1 along
8858             that dimension or if the dimension is squeezed out.
8859         restore_coord_dims : bool, default: False
8860             If True, also restore the dimension order of multi-dimensional
8861             coordinates.
8862 
8863         Returns
8864         -------
8865         grouped : DatasetGroupBy
8866             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8867             iterated over in the form of `(unique_value, grouped_array)` pairs.
8868 
8869         See Also
8870         --------
8871         Dataset.groupby_bins
8872         DataArray.groupby
8873         core.groupby.DatasetGroupBy
8874         pandas.DataFrame.groupby
8875         """
8876         from .groupby import DatasetGroupBy
8877 
8878         # While we don't generally check the type of every arg, passing
8879         # multiple dimensions as multiple arguments is common enough, and the
8880         # consequences hidden enough (strings evaluate as true) to warrant
8881         # checking here.
8882         # A future version could make squeeze kwarg only, but would face
8883         # backward-compat issues.
8884         if not isinstance(squeeze, bool):
8885             raise TypeError(
8886                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8887             )
8888 
8889         return DatasetGroupBy(
8890             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8891         )
8892 
8893     def groupby_bins(
8894         self,
8895         group: Hashable | DataArray | IndexVariable,
8896         bins: ArrayLike,
8897         right: bool = True,
8898         labels: ArrayLike | None = None,
8899         precision: int = 3,
8900         include_lowest: bool = False,
8901         squeeze: bool = True,
8902         restore_coord_dims: bool = False,
8903     ) -> DatasetGroupBy:
8904         """Returns a DatasetGroupBy object for performing grouped operations.
8905 
8906         Rather than using all unique values of `group`, the values are discretized
8907         first by applying `pandas.cut` [1]_ to `group`.
8908 
8909         Parameters
8910         ----------
8911         group : Hashable, DataArray or IndexVariable
8912             Array whose binned values should be used to group this array. If a
8913             string, must be the name of a variable contained in this dataset.
8914         bins : int or array-like
8915             If bins is an int, it defines the number of equal-width bins in the
8916             range of x. However, in this case, the range of x is extended by .1%
8917             on each side to include the min or max values of x. If bins is a
8918             sequence it defines the bin edges allowing for non-uniform bin
8919             width. No extension of the range of x is done in this case.
8920         right : bool, default: True
8921             Indicates whether the bins include the rightmost edge or not. If
8922             right == True (the default), then the bins [1,2,3,4] indicate
8923             (1,2], (2,3], (3,4].
8924         labels : array-like or bool, default: None
8925             Used as labels for the resulting bins. Must be of the same length as
8926             the resulting bins. If False, string bin labels are assigned by
8927             `pandas.cut`.
8928         precision : int, default: 3
8929             The precision at which to store and display the bins labels.
8930         include_lowest : bool, default: False
8931             Whether the first interval should be left-inclusive or not.
8932         squeeze : bool, default: True
8933             If "group" is a dimension of any arrays in this dataset, `squeeze`
8934             controls whether the subarrays have a dimension of length 1 along
8935             that dimension or if the dimension is squeezed out.
8936         restore_coord_dims : bool, default: False
8937             If True, also restore the dimension order of multi-dimensional
8938             coordinates.
8939 
8940         Returns
8941         -------
8942         grouped : DatasetGroupBy
8943             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8944             iterated over in the form of `(unique_value, grouped_array)` pairs.
8945             The name of the group has the added suffix `_bins` in order to
8946             distinguish it from the original variable.
8947 
8948         See Also
8949         --------
8950         Dataset.groupby
8951         DataArray.groupby_bins
8952         core.groupby.DatasetGroupBy
8953         pandas.DataFrame.groupby
8954 
8955         References
8956         ----------
8957         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8958         """
8959         from .groupby import DatasetGroupBy
8960 
8961         return DatasetGroupBy(
8962             self,
8963             group,
8964             squeeze=squeeze,
8965             bins=bins,
8966             restore_coord_dims=restore_coord_dims,
8967             cut_kwargs={
8968                 "right": right,
8969                 "labels": labels,
8970                 "precision": precision,
8971                 "include_lowest": include_lowest,
8972             },
8973         )
8974 
8975     def weighted(self, weights: DataArray) -> DatasetWeighted:
8976         """
8977         Weighted Dataset operations.
8978 
8979         Parameters
8980         ----------
8981         weights : DataArray
8982             An array of weights associated with the values in this Dataset.
8983             Each value in the data contributes to the reduction operation
8984             according to its associated weight.
8985 
8986         Notes
8987         -----
8988         ``weights`` must be a DataArray and cannot contain missing values.
8989         Missing values can be replaced by ``weights.fillna(0)``.
8990 
8991         Returns
8992         -------
8993         core.weighted.DatasetWeighted
8994 
8995         See Also
8996         --------
8997         DataArray.weighted
8998         """
8999         from .weighted import DatasetWeighted
9000 
9001         return DatasetWeighted(self, weights)
9002 
9003     def rolling(
9004         self,
9005         dim: Mapping[Any, int] | None = None,
9006         min_periods: int | None = None,
9007         center: bool | Mapping[Any, bool] = False,
9008         **window_kwargs: int,
9009     ) -> DatasetRolling:
9010         """
9011         Rolling window object for Datasets.
9012 
9013         Parameters
9014         ----------
9015         dim : dict, optional
9016             Mapping from the dimension name to create the rolling iterator
9017             along (e.g. `time`) to its moving window size.
9018         min_periods : int or None, default: None
9019             Minimum number of observations in window required to have a value
9020             (otherwise result is NA). The default, None, is equivalent to
9021             setting min_periods equal to the size of the window.
9022         center : bool or Mapping to int, default: False
9023             Set the labels at the center of the window.
9024         **window_kwargs : optional
9025             The keyword arguments form of ``dim``.
9026             One of dim or window_kwargs must be provided.
9027 
9028         Returns
9029         -------
9030         core.rolling.DatasetRolling
9031 
9032         See Also
9033         --------
9034         core.rolling.DatasetRolling
9035         DataArray.rolling
9036         """
9037         from .rolling import DatasetRolling
9038 
9039         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
9040         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
9041 
9042     def coarsen(
9043         self,
9044         dim: Mapping[Any, int] | None = None,
9045         boundary: CoarsenBoundaryOptions = "exact",
9046         side: SideOptions | Mapping[Any, SideOptions] = "left",
9047         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
9048         **window_kwargs: int,
9049     ) -> DatasetCoarsen:
9050         """
9051         Coarsen object for Datasets.
9052 
9053         Parameters
9054         ----------
9055         dim : mapping of hashable to int, optional
9056             Mapping from the dimension name to the window size.
9057         boundary : {"exact", "trim", "pad"}, default: "exact"
9058             If 'exact', a ValueError will be raised if dimension size is not a
9059             multiple of the window size. If 'trim', the excess entries are
9060             dropped. If 'pad', NA will be padded.
9061         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
9062         coord_func : str or mapping of hashable to str, default: "mean"
9063             function (name) that is applied to the coordinates,
9064             or a mapping from coordinate name to function (name).
9065 
9066         Returns
9067         -------
9068         core.rolling.DatasetCoarsen
9069 
9070         See Also
9071         --------
9072         core.rolling.DatasetCoarsen
9073         DataArray.coarsen
9074         """
9075         from .rolling import DatasetCoarsen
9076 
9077         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
9078         return DatasetCoarsen(
9079             self,
9080             dim,
9081             boundary=boundary,
9082             side=side,
9083             coord_func=coord_func,
9084         )
9085 
9086     def resample(
9087         self,
9088         indexer: Mapping[Any, str] | None = None,
9089         skipna: bool | None = None,
9090         closed: SideOptions | None = None,
9091         label: SideOptions | None = None,
9092         base: int = 0,
9093         keep_attrs: bool | None = None,
9094         loffset: datetime.timedelta | str | None = None,
9095         restore_coord_dims: bool | None = None,
9096         **indexer_kwargs: str,
9097     ) -> DatasetResample:
9098         """Returns a Resample object for performing resampling operations.
9099 
9100         Handles both downsampling and upsampling. The resampled
9101         dimension must be a datetime-like coordinate. If any intervals
9102         contain no values from the original object, they will be given
9103         the value ``NaN``.
9104 
9105         Parameters
9106         ----------
9107         indexer : Mapping of Hashable to str, optional
9108             Mapping from the dimension name to resample frequency [1]_. The
9109             dimension must be datetime-like.
9110         skipna : bool, optional
9111             Whether to skip missing values when aggregating in downsampling.
9112         closed : {"left", "right"}, optional
9113             Side of each interval to treat as closed.
9114         label : {"left", "right"}, optional
9115             Side of each interval to use for labeling.
9116         base : int, default = 0
9117             For frequencies that evenly subdivide 1 day, the "origin" of the
9118             aggregated intervals. For example, for "24H" frequency, base could
9119             range from 0 through 23.
9120         loffset : timedelta or str, optional
9121             Offset used to adjust the resampled time labels. Some pandas date
9122             offset strings are supported.
9123         restore_coord_dims : bool, optional
9124             If True, also restore the dimension order of multi-dimensional
9125             coordinates.
9126         **indexer_kwargs : str
9127             The keyword arguments form of ``indexer``.
9128             One of indexer or indexer_kwargs must be provided.
9129 
9130         Returns
9131         -------
9132         resampled : core.resample.DataArrayResample
9133             This object resampled.
9134 
9135         See Also
9136         --------
9137         DataArray.resample
9138         pandas.Series.resample
9139         pandas.DataFrame.resample
9140 
9141         References
9142         ----------
9143         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
9144         """
9145         from .resample import DatasetResample
9146 
9147         return self._resample(
9148             resample_cls=DatasetResample,
9149             indexer=indexer,
9150             skipna=skipna,
9151             closed=closed,
9152             label=label,
9153             base=base,
9154             keep_attrs=keep_attrs,
9155             loffset=loffset,
9156             restore_coord_dims=restore_coord_dims,
9157             **indexer_kwargs,
9158         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 22:06:47.301 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 22:06:47.301 | INFO     | Search-only: skipping patch selection
2025-10-15 22:06:47.391 | INFO     | Task pydata__xarray-7101 completed successfully.
2025-10-15 22:06:47.392 | INFO     | No patch generated. You can try running ACR again.
