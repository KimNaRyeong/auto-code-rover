[
    {
        "rel_file_path": "sklearn/linear_model/coordinate_descent.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.20/sklearn/linear_model/coordinate_descent.py",
        "start": 803,
        "end": 932,
        "class_name": "Lasso",
        "method_name": null,
        "code": "803 class Lasso(ElasticNet):\n804     \"\"\"Linear Model trained with L1 prior as regularizer (aka the Lasso)\n805 \n806     The optimization objective for Lasso is::\n807 \n808         (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n809 \n810     Technically the Lasso model is optimizing the same objective function as\n811     the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n812 \n813     Read more in the :ref:`User Guide <lasso>`.\n814 \n815     Parameters\n816     ----------\n817     alpha : float, optional\n818         Constant that multiplies the L1 term. Defaults to 1.0.\n819         ``alpha = 0`` is equivalent to an ordinary least square, solved\n820         by the :class:`LinearRegression` object. For numerical\n821         reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n822         Given this, you should use the :class:`LinearRegression` object.\n823 \n824     fit_intercept : boolean\n825         whether to calculate the intercept for this model. If set\n826         to false, no intercept will be used in calculations\n827         (e.g. data is expected to be already centered).\n828 \n829     normalize : boolean, optional, default False\n830         This parameter is ignored when ``fit_intercept`` is set to False.\n831         If True, the regressors X will be normalized before regression by\n832         subtracting the mean and dividing by the l2-norm.\n833         If you wish to standardize, please use\n834         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n835         on an estimator with ``normalize=False``.\n836 \n837     precompute : True | False | array-like, default=False\n838         Whether to use a precomputed Gram matrix to speed up\n839         calculations. If set to ``'auto'`` let us decide. The Gram\n840         matrix can also be passed as argument. For sparse input\n841         this option is always ``True`` to preserve sparsity.\n842 \n843     copy_X : boolean, optional, default True\n844         If ``True``, X will be copied; else, it may be overwritten.\n845 \n846     max_iter : int, optional\n847         The maximum number of iterations\n848 \n849     tol : float, optional\n850         The tolerance for the optimization: if the updates are\n851         smaller than ``tol``, the optimization code checks the\n852         dual gap for optimality and continues until it is smaller\n853         than ``tol``.\n854 \n855     warm_start : bool, optional\n856         When set to True, reuse the solution of the previous call to fit as\n857         initialization, otherwise, just erase the previous solution.\n858 \n859     positive : bool, optional\n860         When set to ``True``, forces the coefficients to be positive.\n861 \n862     random_state : int, RandomState instance or None, optional, default None\n863         The seed of the pseudo random number generator that selects a random\n864         feature to update.  If int, random_state is the seed used by the random\n865         number generator; If RandomState instance, random_state is the random\n866         number generator; If None, the random number generator is the\n867         RandomState instance used by `np.random`. Used when ``selection`` ==\n868         'random'.\n869 \n870     selection : str, default 'cyclic'\n871         If set to 'random', a random coefficient is updated every iteration\n872         rather than looping over features sequentially by default. This\n873         (setting to 'random') often leads to significantly faster convergence\n874         especially when tol is higher than 1e-4.\n875 \n876     Attributes\n877     ----------\n878     coef_ : array, shape (n_features,) | (n_targets, n_features)\n879         parameter vector (w in the cost function formula)\n880 \n881     sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) | \\\n882             (n_targets, n_features)\n883         ``sparse_coef_`` is a readonly property derived from ``coef_``\n884 \n885     intercept_ : float | array, shape (n_targets,)\n886         independent term in decision function.\n887 \n888     n_iter_ : int | array-like, shape (n_targets,)\n889         number of iterations run by the coordinate descent solver to reach\n890         the specified tolerance.\n891 \n892     Examples\n893     --------\n894     >>> from sklearn import linear_model\n895     >>> clf = linear_model.Lasso(alpha=0.1)\n896     >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n897     Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n898        normalize=False, positive=False, precompute=False, random_state=None,\n899        selection='cyclic', tol=0.0001, warm_start=False)\n900     >>> print(clf.coef_)\n901     [ 0.85  0.  ]\n902     >>> print(clf.intercept_)\n903     0.15\n904 \n905     See also\n906     --------\n907     lars_path\n908     lasso_path\n909     LassoLars\n910     LassoCV\n911     LassoLarsCV\n912     sklearn.decomposition.sparse_encode\n913 \n914     Notes\n915     -----\n916     The algorithm used to fit the model is coordinate descent.\n917 \n918     To avoid unnecessary memory duplication the X argument of the fit method\n919     should be directly passed as a Fortran-contiguous numpy array.\n920     \"\"\"\n921     path = staticmethod(enet_path)\n922 \n923     def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n924                  precompute=False, copy_X=True, max_iter=1000,\n925                  tol=1e-4, warm_start=False, positive=False,\n926                  random_state=None, selection='cyclic'):\n927         super(Lasso, self).__init__(\n928             alpha=alpha, l1_ratio=1.0, fit_intercept=fit_intercept,\n929             normalize=normalize, precompute=precompute, copy_X=copy_X,\n930             max_iter=max_iter, tol=tol, warm_start=warm_start,\n931             positive=positive, random_state=random_state,\n932             selection=selection)\n",
        "intended_behavior": "The `fit` method (whether directly within `Lasso` or inherited from `ElasticNet`) should ensure that when `fit_intercept=False`, the shape of `coef_` is correctly managed as a 1-D numpy array even if the input data is a single feature in shape. This may involve adding conditional checks around the reshaping or setting of `coef_` depending on the `fit_intercept` flag. Any logic that directly influences the shape of `coef_` based on the `fit_intercept` parameter should explicitly ensure that the resulting shape of `coef_` matches the expectation for both cases where `fit_intercept` is `True` or `False`. This adjustment preserves the functionality of allowing users to specify whether an intercept should be fitted while maintaining the integrity and expected behavior of the `coef_` attribute post-fitting."
    }
]