2025-10-15 23:55:11.048 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 23:55:15.112 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=339, output_tokens=12, cost=0.003750
2025-10-15 23:55:15.112 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 23:55:15.116 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 23:55:22.093 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=647, output_tokens=313, cost=0.015860
2025-10-15 23:55:22.102 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 23:55:26.625 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=667, output_tokens=145, cost=0.011020
2025-10-15 23:55:26.626 | DEBUG    | Extracted a valid json.
2025-10-15 23:55:26.627 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"CalibratedClassifierCV\")",
        "search_class(\"DBSCAN\")",
        "search_class(\"DummyClassifier\")",
        "search_class(\"DummyRegressor\")",
        "search_class(\"BaseBagging\")",
        "search_class(\"BaseForest\")",
        "search_class(\"BaseGradientBoosting\")",
        "search_class(\"IsotonicRegression\")",
        "search_class(\"KernelRidge\")",
        "search_class(\"GaussianNB\")",
        "search_class(\"BaseDiscreteNB\")",
        "search_class(\"KernelDensity\")",
        "search_class(\"BaseDecisionTree\")"
    ],
    "bug_locations": []
}
2025-10-15 23:55:26.720 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 23:55:26.721 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 23:55:41.382 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3836, output_tokens=613, cost=0.056750
2025-10-15 23:55:41.395 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 23:56:00.317 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4665, output_tokens=838, cost=0.071790
2025-10-15 23:56:00.329 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 23:56:21.953 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1192, output_tokens=956, cost=0.040600
2025-10-15 23:56:21.954 | DEBUG    | Extracted a valid json.
2025-10-15 23:56:21.954 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/calibration.py",
            "class": "CalibratedClassifierCV",
            "method": "fit",
            "intended_behavior": "Within the `fit` method, replace or supplement any custom validation of `sample_weight` with a standardized call to `_check_sample_weight`, ensuring that `sample_weight` is properly validated without altering the method's core functionality."
        },
        {
            "file": "sklearn/cluster/_dbscan.py",
            "class": "DBSCAN",
            "method": "fit",
            "intended_behavior": "The `fit` method should use `_check_sample_weight` to validate `sample_weight`, replacing any bespoke validation logic to ensure consistency in handling sample weights across the project."
        },
        {
            "file": "sklearn/dummy.py",
            "class": "DummyClassifier",
            "method": "fit",
            "intended_behavior": "Use `_check_sample_weight` in the `fit` method for validating `sample_weight`, aiming to standardize the way sample weights are checked across different estimators."
        },
        {
            "file": "sklearn/dummy.py",
            "class": "DummyRegressor",
            "method": "fit",
            "intended_behavior": "The `fit` method should incorporate `_check_sample_weight` for `sample_weight` validation to align with the standardized approach of handling sample weights in the project."
        },
        {
            "file": "sklearn/ensemble/_bagging.py",
            "class": "BaseBagging",
            "method": "fit",
            "intended_behavior": "Employ `_check_sample_weight` in the `fit` method for consistent `sample_weight` validation, ensuring that it follows the newly established project-wide standard."
        },
        {
            "file": "sklearn/ensemble/_forest.py",
            "class": "BaseForest",
            "method": "fit",
            "intended_behavior": "Adapt the `fit` method to utilize `_check_sample_weight` for validating `sample_weight`, ensuring the method's adherence to the unified validation process."
        },
        {
            "file": "sklearn/ensemble/_gb.py",
            "class": "BaseGradientBoosting",
            "method": "fit",
            "intended_behavior": "The `fit` method should make use of `_check_sample_weight` to validate `sample_weight`, thereby standardizing sample weight validation across the framework."
        },
        {
            "file": "sklearn/isotonic.py",
            "class": "IsotonicRegression",
            "method": "fit",
            "intended_behavior": "Adapt the `fit` method to include `_check_sample_weight` for `sample_weight` validation, aligning with the consistent approach of handling sample weights."
        },
        {
            "file": "sklearn/kernel_ridge.py",
            "class": "KernelRidge",
            "method": "fit",
            "intended_behavior": "Within the `fit` method, replace any existing sample weight validation logic with `_check_sample_weight`, ensuring consistent validation across estimators."
        },
        {
            "file": "sklearn/naive_bayes.py",
            "class": "GaussianNB",
            "method": "fit",
            "intended_behavior": "Utilize `_check_sample_weight` for validating `sample_weight` in the `fit` method, aligning GaussianNB's handling of sample weights with the rest of the project."
        },
        {
            "file": "sklearn/naive_bayes.py",
            "class": "BaseDiscreteNB",
            "method": "fit",
            "intended_behavior": "The `fit` method should incorporate `_check_sample_weight` for `sample_weight` validation, ensuring consistency in sample weight handling."
        },
        {
            "file": "sklearn/neighbors/_kde.py",
            "class": "KernelDensity",
            "method": "fit",
            "intended_behavior": "Leverage `_check_sample_weight` in the `fit` method for validating `sample_weight`, aligning the method with the standardized validation approach."
        },
        {
            "file": "sklearn/tree/_classes.py",
            "class": "BaseDecisionTree",
            "method": "fit",
            "intended_behavior": "Update the `fit` method to use `_check_sample_weight` for `sample_weight` validation, standardizing the way sample weights are validated across different parts of the project."
        }
    ]
}
2025-10-15 23:56:21.988 | DEBUG    | Bug location extracted successfully: [<file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>fit</method>
<code>
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self

</code><intended_behavior>Within the `fit` method, replace or supplement any custom validation of `sample_weight` with a standardized call to `_check_sample_weight`, ensuring that `sample_weight` is properly validated without altering the method's core functionality.</intended_behavior>, <file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>None</method>
<code>
31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,
32                              MetaEstimatorMixin):
33     """Probability calibration with isotonic regression or sigmoid.
34 
35     See glossary entry for :term:`cross-validation estimator`.
36 
37     With this class, the base_estimator is fit on the train set of the
38     cross-validation generator and the test set is used for calibration.
39     The probabilities for each of the folds are then averaged
40     for prediction. In case that cv="prefit" is passed to __init__,
41     it is assumed that base_estimator has been fitted already and all
42     data is used for calibration. Note that data for fitting the
43     classifier and for calibrating it must be disjoint.
44 
45     Read more in the :ref:`User Guide <calibration>`.
46 
47     Parameters
48     ----------
49     base_estimator : instance BaseEstimator
50         The classifier whose output decision function needs to be calibrated
51         to offer more accurate predict_proba outputs. If cv=prefit, the
52         classifier must have been fit already on data.
53 
54     method : 'sigmoid' or 'isotonic'
55         The method to use for calibration. Can be 'sigmoid' which
56         corresponds to Platt's method or 'isotonic' which is a
57         non-parametric approach. It is not advised to use isotonic calibration
58         with too few calibration samples ``(<<1000)`` since it tends to
59         overfit.
60         Use sigmoids (Platt's calibration) in this case.
61 
62     cv : integer, cross-validation generator, iterable or "prefit", optional
63         Determines the cross-validation splitting strategy.
64         Possible inputs for cv are:
65 
66         - None, to use the default 5-fold cross-validation,
67         - integer, to specify the number of folds.
68         - :term:`CV splitter`,
69         - An iterable yielding (train, test) splits as arrays of indices.
70 
71         For integer/None inputs, if ``y`` is binary or multiclass,
72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
74         is used.
75 
76         Refer :ref:`User Guide <cross_validation>` for the various
77         cross-validation strategies that can be used here.
78 
79         If "prefit" is passed, it is assumed that base_estimator has been
80         fitted already and all data is used for calibration.
81 
82         .. versionchanged:: 0.22
83             ``cv`` default value if None changed from 3-fold to 5-fold.
84 
85     Attributes
86     ----------
87     classes_ : array, shape (n_classes)
88         The class labels.
89 
90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
91         The list of calibrated classifiers, one for each crossvalidation fold,
92         which has been fitted on all but the validation fold and calibrated
93         on the validation fold.
94 
95     References
96     ----------
97     .. [1] Obtaining calibrated probability estimates from decision trees
98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
99 
100     .. [2] Transforming Classifier Scores into Accurate Multiclass
101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
102 
103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
104            Regularized Likelihood Methods, J. Platt, (1999)
105 
106     .. [4] Predicting Good Probabilities with Supervised Learning,
107            A. Niculescu-Mizil & R. Caruana, ICML 2005
108     """
109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):
110         self.base_estimator = base_estimator
111         self.method = method
112         self.cv = cv
113 
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self
202 
203     def predict_proba(self, X):
204         """Posterior probabilities of classification
205 
206         This function returns posterior probabilities of classification
207         according to each class on an array of test vectors X.
208 
209         Parameters
210         ----------
211         X : array-like, shape (n_samples, n_features)
212             The samples.
213 
214         Returns
215         -------
216         C : array, shape (n_samples, n_classes)
217             The predicted probas.
218         """
219         check_is_fitted(self)
220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
221                         force_all_finite=False)
222         # Compute the arithmetic mean of the predictions of the calibrated
223         # classifiers
224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))
225         for calibrated_classifier in self.calibrated_classifiers_:
226             proba = calibrated_classifier.predict_proba(X)
227             mean_proba += proba
228 
229         mean_proba /= len(self.calibrated_classifiers_)
230 
231         return mean_proba
232 
233     def predict(self, X):
234         """Predict the target of new samples. Can be different from the
235         prediction of the uncalibrated classifier.
236 
237         Parameters
238         ----------
239         X : array-like, shape (n_samples, n_features)
240             The samples.
241 
242         Returns
243         -------
244         C : array, shape (n_samples,)
245             The predicted class.
246         """
247         check_is_fitted(self)
248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>The `fit` method should use `_check_sample_weight` to validate `sample_weight`, replacing any bespoke validation logic to ensure consistency in handling sample weights across the project.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>Use `_check_sample_weight` in the `fit` method for validating `sample_weight`, aiming to standardize the way sample weights are checked across different estimators.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>fit</method>
<code>
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self

</code><intended_behavior>The `fit` method should incorporate `_check_sample_weight` for `sample_weight` validation to align with the standardized approach of handling sample weights in the project.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>None</method>
<code>
382 class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):
383     """
384     DummyRegressor is a regressor that makes predictions using
385     simple rules.
386 
387     This regressor is useful as a simple baseline to compare with other
388     (real) regressors. Do not use it for real problems.
389 
390     Read more in the :ref:`User Guide <dummy_estimators>`.
391 
392     Parameters
393     ----------
394     strategy : str
395         Strategy to use to generate predictions.
396 
397         * "mean": always predicts the mean of the training set
398         * "median": always predicts the median of the training set
399         * "quantile": always predicts a specified quantile of the training set,
400           provided with the quantile parameter.
401         * "constant": always predicts a constant value that is provided by
402           the user.
403 
404     constant : int or float or array-like of shape (n_outputs,)
405         The explicit constant as predicted by the "constant" strategy. This
406         parameter is useful only for the "constant" strategy.
407 
408     quantile : float in [0.0, 1.0]
409         The quantile to predict using the "quantile" strategy. A quantile of
410         0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the
411         maximum.
412 
413     Attributes
414     ----------
415     constant_ : array, shape (1, n_outputs)
416         Mean or median or quantile of the training targets or constant value
417         given by the user.
418 
419     n_outputs_ : int,
420         Number of outputs.
421 
422     Examples
423     --------
424     >>> import numpy as np
425     >>> from sklearn.dummy import DummyRegressor
426     >>> X = np.array([1.0, 2.0, 3.0, 4.0])
427     >>> y = np.array([2.0, 3.0, 5.0, 10.0])
428     >>> dummy_regr = DummyRegressor(strategy="mean")
429     >>> dummy_regr.fit(X, y)
430     DummyRegressor()
431     >>> dummy_regr.predict(X)
432     array([5., 5., 5., 5.])
433     >>> dummy_regr.score(X, y)
434     0.0
435     """
436 
437     def __init__(self, strategy="mean", constant=None, quantile=None):
438         self.strategy = strategy
439         self.constant = constant
440         self.quantile = quantile
441 
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self
519 
520     def predict(self, X, return_std=False):
521         """
522         Perform classification on test vectors X.
523 
524         Parameters
525         ----------
526         X : {array-like, object with finite length or shape}
527             Training data, requires length = n_samples
528 
529         return_std : boolean, optional
530             Whether to return the standard deviation of posterior prediction.
531             All zeros in this case.
532 
533         Returns
534         -------
535         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
536             Predicted target values for X.
537 
538         y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)
539             Standard deviation of predictive distribution of query points.
540         """
541         check_is_fitted(self)
542         n_samples = _num_samples(X)
543 
544         y = np.full((n_samples, self.n_outputs_), self.constant_,
545                     dtype=np.array(self.constant_).dtype)
546         y_std = np.zeros((n_samples, self.n_outputs_))
547 
548         if self.n_outputs_ == 1:
549             y = np.ravel(y)
550             y_std = np.ravel(y_std)
551 
552         return (y, y_std) if return_std else y
553 
554     def _more_tags(self):
555         return {'poor_score': True, 'no_validation': True}
556 
557     def score(self, X, y, sample_weight=None):
558         """Returns the coefficient of determination R^2 of the prediction.
559 
560         The coefficient R^2 is defined as (1 - u/v), where u is the residual
561         sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
562         sum of squares ((y_true - y_true.mean()) ** 2).sum().
563         The best possible score is 1.0 and it can be negative (because the
564         model can be arbitrarily worse). A constant model that always
565         predicts the expected value of y, disregarding the input features,
566         would get a R^2 score of 0.0.
567 
568         Parameters
569         ----------
570         X : {array-like, None}
571             Test samples with shape = (n_samples, n_features) or None.
572             For some estimators this may be a
573             precomputed kernel matrix instead, shape = (n_samples,
574             n_samples_fitted], where n_samples_fitted is the number of
575             samples used in the fitting for the estimator.
576             Passing None as test samples gives the same result
577             as passing real test samples, since DummyRegressor
578             operates independently of the sampled observations.
579 
580         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
581             True values for X.
582 
583         sample_weight : array-like of shape (n_samples,), default=None
584             Sample weights.
585 
586         Returns
587         -------
588         score : float
589             R^2 of self.predict(X) wrt. y.
590         """
591         if X is None:
592             X = np.zeros(shape=(len(y), 1))
593         return super().score(X, y, sample_weight)
594 
595     @deprecated(
596         "The outputs_2d_ attribute is deprecated in version 0.22 "
597         "and will be removed in version 0.24. It is equivalent to "
598         "n_outputs_ > 1."
599     )
600     @property
601     def outputs_2d_(self):
602         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>fit</method>
<code>
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)

</code><intended_behavior>Employ `_check_sample_weight` in the `fit` method for consistent `sample_weight` validation, ensuring that it follows the newly established project-wide standard.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>None</method>
<code>
185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):
186     """Base class for Bagging meta-estimator.
187 
188     Warning: This class should not be used directly. Use derived classes
189     instead.
190     """
191 
192     @abstractmethod
193     def __init__(self,
194                  base_estimator=None,
195                  n_estimators=10,
196                  max_samples=1.0,
197                  max_features=1.0,
198                  bootstrap=True,
199                  bootstrap_features=False,
200                  oob_score=False,
201                  warm_start=False,
202                  n_jobs=None,
203                  random_state=None,
204                  verbose=0):
205         super().__init__(
206             base_estimator=base_estimator,
207             n_estimators=n_estimators)
208 
209         self.max_samples = max_samples
210         self.max_features = max_features
211         self.bootstrap = bootstrap
212         self.bootstrap_features = bootstrap_features
213         self.oob_score = oob_score
214         self.warm_start = warm_start
215         self.n_jobs = n_jobs
216         self.random_state = random_state
217         self.verbose = verbose
218 
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
243 
244     def _parallel_args(self):
245         return {}
246 
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self
392 
393     @abstractmethod
394     def _set_oob_score(self, X, y):
395         """Calculate out of bag predictions and score."""
396 
397     def _validate_y(self, y):
398         if len(y.shape) == 1 or y.shape[1] == 1:
399             return column_or_1d(y, warn=True)
400         else:
401             return y
402 
403     def _get_estimators_indices(self):
404         # Get drawn indices along both sample and feature axes
405         for seed in self._seeds:
406             # Operations accessing random_state must be performed identically
407             # to those in `_parallel_build_estimators()`
408             random_state = np.random.RandomState(seed)
409             feature_indices, sample_indices = _generate_bagging_indices(
410                 random_state, self.bootstrap_features, self.bootstrap,
411                 self.n_features_, self._n_samples, self._max_features,
412                 self._max_samples)
413 
414             yield feature_indices, sample_indices
415 
416     @property
417     def estimators_samples_(self):
418         """
419         The subset of drawn samples for each base estimator.
420 
421         Returns a dynamically generated list of indices identifying
422         the samples used for fitting each member of the ensemble, i.e.,
423         the in-bag samples.
424 
425         Note: the list is re-created at each call to the property in order
426         to reduce the object memory footprint by not storing the sampling
427         data. Thus fetching the property may be slower than expected.
428         """
429         return [sample_indices
430                 for _, sample_indices in self._get_estimators_indices()]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>fit</method>
<code>
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self

</code><intended_behavior>Adapt the `fit` method to utilize `_check_sample_weight` for validating `sample_weight`, ensuring the method's adherence to the unified validation process.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>None</method>
<code>
172 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):
173     """
174     Base class for forests of trees.
175 
176     Warning: This class should not be used directly. Use derived classes
177     instead.
178     """
179 
180     @abstractmethod
181     def __init__(self,
182                  base_estimator,
183                  n_estimators=100,
184                  estimator_params=tuple(),
185                  bootstrap=False,
186                  oob_score=False,
187                  n_jobs=None,
188                  random_state=None,
189                  verbose=0,
190                  warm_start=False,
191                  class_weight=None,
192                  max_samples=None):
193         super().__init__(
194             base_estimator=base_estimator,
195             n_estimators=n_estimators,
196             estimator_params=estimator_params)
197 
198         self.bootstrap = bootstrap
199         self.oob_score = oob_score
200         self.n_jobs = n_jobs
201         self.random_state = random_state
202         self.verbose = verbose
203         self.warm_start = warm_start
204         self.class_weight = class_weight
205         self.max_samples = max_samples
206 
207     def apply(self, X):
208         """
209         Apply trees in the forest to X, return leaf indices.
210 
211         Parameters
212         ----------
213         X : {array-like or sparse matrix} of shape (n_samples, n_features)
214             The input samples. Internally, its dtype will be converted to
215             ``dtype=np.float32``. If a sparse matrix is provided, it will be
216             converted into a sparse ``csr_matrix``.
217 
218         Returns
219         -------
220         X_leaves : array_like, shape = [n_samples, n_estimators]
221             For each datapoint x in X and for each tree in the forest,
222             return the index of the leaf x ends up in.
223         """
224         X = self._validate_X_predict(X)
225         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
226                            **_joblib_parallel_args(prefer="threads"))(
227             delayed(tree.apply)(X, check_input=False)
228             for tree in self.estimators_)
229 
230         return np.array(results).T
231 
232     def decision_path(self, X):
233         """
234         Return the decision path in the forest.
235 
236         .. versionadded:: 0.18
237 
238         Parameters
239         ----------
240         X : {array-like or sparse matrix} of shape (n_samples, n_features)
241             The input samples. Internally, its dtype will be converted to
242             ``dtype=np.float32``. If a sparse matrix is provided, it will be
243             converted into a sparse ``csr_matrix``.
244 
245         Returns
246         -------
247         indicator : sparse csr array, shape = [n_samples, n_nodes]
248             Return a node indicator matrix where non zero elements
249             indicates that the samples goes through the nodes.
250 
251         n_nodes_ptr : array of size (n_estimators + 1, )
252             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
253             gives the indicator value for the i-th estimator.
254 
255         """
256         X = self._validate_X_predict(X)
257         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
258                               **_joblib_parallel_args(prefer='threads'))(
259             delayed(tree.decision_path)(X,
260                                      check_input=False)
261             for tree in self.estimators_)
262 
263         n_nodes = [0]
264         n_nodes.extend([i.shape[1] for i in indicators])
265         n_nodes_ptr = np.array(n_nodes).cumsum()
266 
267         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
268 
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self
398 
399     @abstractmethod
400     def _set_oob_score(self, X, y):
401         """
402         Calculate out of bag predictions and score."""
403 
404     def _validate_y_class_weight(self, y):
405         # Default implementation
406         return y, None
407 
408     def _validate_X_predict(self, X):
409         """
410         Validate X whenever one tries to predict, apply, predict_proba."""
411         check_is_fitted(self)
412 
413         return self.estimators_[0]._validate_X_predict(X, check_input=True)
414 
415     @property
416     def feature_importances_(self):
417         """
418         Return the feature importances (the higher, the more important the
419            feature).
420 
421         Returns
422         -------
423         feature_importances_ : array, shape = [n_features]
424             The values of this array sum to 1, unless all trees are single node
425             trees consisting of only the root node, in which case it will be an
426             array of zeros.
427         """
428         check_is_fitted(self)
429 
430         all_importances = Parallel(n_jobs=self.n_jobs,
431                                    **_joblib_parallel_args(prefer='threads'))(
432             delayed(getattr)(tree, 'feature_importances_')
433             for tree in self.estimators_ if tree.tree_.node_count > 1)
434 
435         if not all_importances:
436             return np.zeros(self.n_features_, dtype=np.float64)
437 
438         all_importances = np.mean(all_importances,
439                                   axis=0, dtype=np.float64)
440         return all_importances / np.sum(all_importances)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self

</code><intended_behavior>The `fit` method should make use of `_check_sample_weight` to validate `sample_weight`, thereby standardizing sample weight validation across the framework.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features, ccp_alpha,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='deprecated',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.ccp_alpha = ccp_alpha
1192         self.init = init
1193         self.random_state = random_state
1194         self.alpha = alpha
1195         self.verbose = verbose
1196         self.max_leaf_nodes = max_leaf_nodes
1197         self.warm_start = warm_start
1198         self.presort = presort
1199         self.validation_fraction = validation_fraction
1200         self.n_iter_no_change = n_iter_no_change
1201         self.tol = tol
1202 
1203     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1204                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1205         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1206 
1207         assert sample_mask.dtype == np.bool
1208         loss = self.loss_
1209         original_y = y
1210 
1211         # Need to pass a copy of raw_predictions to negative_gradient()
1212         # because raw_predictions is partially updated at the end of the loop
1213         # in update_terminal_regions(), and gradients need to be evaluated at
1214         # iteration i - 1.
1215         raw_predictions_copy = raw_predictions.copy()
1216 
1217         for k in range(loss.K):
1218             if loss.is_multi_class:
1219                 y = np.array(original_y == k, dtype=np.float64)
1220 
1221             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1222                                               sample_weight=sample_weight)
1223 
1224             # induce regression tree on residuals
1225             tree = DecisionTreeRegressor(
1226                 criterion=self.criterion,
1227                 splitter='best',
1228                 max_depth=self.max_depth,
1229                 min_samples_split=self.min_samples_split,
1230                 min_samples_leaf=self.min_samples_leaf,
1231                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1232                 min_impurity_decrease=self.min_impurity_decrease,
1233                 min_impurity_split=self.min_impurity_split,
1234                 max_features=self.max_features,
1235                 max_leaf_nodes=self.max_leaf_nodes,
1236                 random_state=random_state,
1237                 ccp_alpha=self.ccp_alpha)
1238 
1239             if self.subsample < 1.0:
1240                 # no inplace multiplication!
1241                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1242 
1243             X = X_csr if X_csr is not None else X
1244             tree.fit(X, residual, sample_weight=sample_weight,
1245                      check_input=False, X_idx_sorted=X_idx_sorted)
1246 
1247             # update tree leaves
1248             loss.update_terminal_regions(
1249                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1250                 sample_mask, learning_rate=self.learning_rate, k=k)
1251 
1252             # add tree to ensemble
1253             self.estimators_[i, k] = tree
1254 
1255         return raw_predictions
1256 
1257     def _check_params(self):
1258         """Check validity of parameters and raise ValueError if not valid. """
1259         if self.n_estimators <= 0:
1260             raise ValueError("n_estimators must be greater than 0 but "
1261                              "was %r" % self.n_estimators)
1262 
1263         if self.learning_rate <= 0.0:
1264             raise ValueError("learning_rate must be greater than 0 but "
1265                              "was %r" % self.learning_rate)
1266 
1267         if (self.loss not in self._SUPPORTED_LOSS
1268                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1269             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1270 
1271         if self.loss == 'deviance':
1272             loss_class = (_gb_losses.MultinomialDeviance
1273                           if len(self.classes_) > 2
1274                           else _gb_losses.BinomialDeviance)
1275         else:
1276             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1277 
1278         if self.loss in ('huber', 'quantile'):
1279             self.loss_ = loss_class(self.n_classes_, self.alpha)
1280         else:
1281             self.loss_ = loss_class(self.n_classes_)
1282 
1283         if not (0.0 < self.subsample <= 1.0):
1284             raise ValueError("subsample must be in (0,1] but "
1285                              "was %r" % self.subsample)
1286 
1287         if self.init is not None:
1288             # init must be an estimator or 'zero'
1289             if isinstance(self.init, BaseEstimator):
1290                 self.loss_.check_init_estimator(self.init)
1291             elif not (isinstance(self.init, str) and self.init == 'zero'):
1292                 raise ValueError(
1293                     "The init parameter must be an estimator or 'zero'. "
1294                     "Got init={}".format(self.init)
1295                 )
1296 
1297         if not (0.0 < self.alpha < 1.0):
1298             raise ValueError("alpha must be in (0.0, 1.0) but "
1299                              "was %r" % self.alpha)
1300 
1301         if isinstance(self.max_features, str):
1302             if self.max_features == "auto":
1303                 # if is_classification
1304                 if self.n_classes_ > 1:
1305                     max_features = max(1, int(np.sqrt(self.n_features_)))
1306                 else:
1307                     # is regression
1308                     max_features = self.n_features_
1309             elif self.max_features == "sqrt":
1310                 max_features = max(1, int(np.sqrt(self.n_features_)))
1311             elif self.max_features == "log2":
1312                 max_features = max(1, int(np.log2(self.n_features_)))
1313             else:
1314                 raise ValueError("Invalid value for max_features: %r. "
1315                                  "Allowed string values are 'auto', 'sqrt' "
1316                                  "or 'log2'." % self.max_features)
1317         elif self.max_features is None:
1318             max_features = self.n_features_
1319         elif isinstance(self.max_features, numbers.Integral):
1320             max_features = self.max_features
1321         else:  # float
1322             if 0. < self.max_features <= 1.:
1323                 max_features = max(int(self.max_features *
1324                                        self.n_features_), 1)
1325             else:
1326                 raise ValueError("max_features must be in (0, n_features]")
1327 
1328         self.max_features_ = max_features
1329 
1330         if not isinstance(self.n_iter_no_change,
1331                           (numbers.Integral, type(None))):
1332             raise ValueError("n_iter_no_change should either be None or an "
1333                              "integer. %r was passed"
1334                              % self.n_iter_no_change)
1335 
1336         if self.presort != 'deprecated':
1337             warnings.warn("The parameter 'presort' is deprecated and has no "
1338                           "effect. It will be removed in v0.24. You can "
1339                           "suppress this warning by not passing any value "
1340                           "to the 'presort' parameter. We also recommend "
1341                           "using HistGradientBoosting models instead.",
1342                           FutureWarning)
1343 
1344     def _init_state(self):
1345         """Initialize model state and allocate model state data structures. """
1346 
1347         self.init_ = self.init
1348         if self.init_ is None:
1349             self.init_ = self.loss_.init_estimator()
1350 
1351         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1352                                     dtype=np.object)
1353         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1354         # do oob?
1355         if self.subsample < 1.0:
1356             self.oob_improvement_ = np.zeros((self.n_estimators),
1357                                              dtype=np.float64)
1358 
1359     def _clear_state(self):
1360         """Clear the state of the gradient boosting model. """
1361         if hasattr(self, 'estimators_'):
1362             self.estimators_ = np.empty((0, 0), dtype=np.object)
1363         if hasattr(self, 'train_score_'):
1364             del self.train_score_
1365         if hasattr(self, 'oob_improvement_'):
1366             del self.oob_improvement_
1367         if hasattr(self, 'init_'):
1368             del self.init_
1369         if hasattr(self, '_rng'):
1370             del self._rng
1371 
1372     def _resize_state(self):
1373         """Add additional ``n_estimators`` entries to all attributes. """
1374         # self.n_estimators is the number of additional est to fit
1375         total_n_estimators = self.n_estimators
1376         if total_n_estimators < self.estimators_.shape[0]:
1377             raise ValueError('resize with smaller n_estimators %d < %d' %
1378                              (total_n_estimators, self.estimators_[0]))
1379 
1380         self.estimators_ = np.resize(self.estimators_,
1381                                      (total_n_estimators, self.loss_.K))
1382         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1383         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1384             # if do oob resize arrays or create new if not available
1385             if hasattr(self, 'oob_improvement_'):
1386                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1387                                                   total_n_estimators)
1388             else:
1389                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1390                                                  dtype=np.float64)
1391 
1392     def _is_initialized(self):
1393         return len(getattr(self, 'estimators_', [])) > 0
1394 
1395     def _check_initialized(self):
1396         """Check that the estimator is initialized, raising an error if not."""
1397         check_is_fitted(self)
1398 
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self
1548 
1549     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1550                     X_val, y_val, sample_weight_val,
1551                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1552         """Iteratively fits the stages.
1553 
1554         For each stage it computes the progress (OOB, train score)
1555         and delegates to ``_fit_stage``.
1556         Returns the number of stages fit; might differ from ``n_estimators``
1557         due to early stopping.
1558         """
1559         n_samples = X.shape[0]
1560         do_oob = self.subsample < 1.0
1561         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1562         n_inbag = max(1, int(self.subsample * n_samples))
1563         loss_ = self.loss_
1564 
1565         if self.verbose:
1566             verbose_reporter = VerboseReporter(self.verbose)
1567             verbose_reporter.init(self, begin_at_stage)
1568 
1569         X_csc = csc_matrix(X) if issparse(X) else None
1570         X_csr = csr_matrix(X) if issparse(X) else None
1571 
1572         if self.n_iter_no_change is not None:
1573             loss_history = np.full(self.n_iter_no_change, np.inf)
1574             # We create a generator to get the predictions for X_val after
1575             # the addition of each successive stage
1576             y_val_pred_iter = self._staged_raw_predict(X_val)
1577 
1578         # perform boosting iterations
1579         i = begin_at_stage
1580         for i in range(begin_at_stage, self.n_estimators):
1581 
1582             # subsampling
1583             if do_oob:
1584                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1585                                                   random_state)
1586                 # OOB score before adding this stage
1587                 old_oob_score = loss_(y[~sample_mask],
1588                                       raw_predictions[~sample_mask],
1589                                       sample_weight[~sample_mask])
1590 
1591             # fit next stage of trees
1592             raw_predictions = self._fit_stage(
1593                 i, X, y, raw_predictions, sample_weight, sample_mask,
1594                 random_state, X_idx_sorted, X_csc, X_csr)
1595 
1596             # track deviance (= loss)
1597             if do_oob:
1598                 self.train_score_[i] = loss_(y[sample_mask],
1599                                              raw_predictions[sample_mask],
1600                                              sample_weight[sample_mask])
1601                 self.oob_improvement_[i] = (
1602                     old_oob_score - loss_(y[~sample_mask],
1603                                           raw_predictions[~sample_mask],
1604                                           sample_weight[~sample_mask]))
1605             else:
1606                 # no need to fancy index w/ no subsampling
1607                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1608 
1609             if self.verbose > 0:
1610                 verbose_reporter.update(i, self)
1611 
1612             if monitor is not None:
1613                 early_stopping = monitor(i, self, locals())
1614                 if early_stopping:
1615                     break
1616 
1617             # We also provide an early stopping based on the score from
1618             # validation set (X_val, y_val), if n_iter_no_change is set
1619             if self.n_iter_no_change is not None:
1620                 # By calling next(y_val_pred_iter), we get the predictions
1621                 # for X_val after the addition of the current stage
1622                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1623                                         sample_weight_val)
1624 
1625                 # Require validation_score to be better (less) than at least
1626                 # one of the last n_iter_no_change evaluations
1627                 if np.any(validation_loss + self.tol < loss_history):
1628                     loss_history[i % len(loss_history)] = validation_loss
1629                 else:
1630                     break
1631 
1632         return i + 1
1633 
1634     def _make_estimator(self, append=True):
1635         # we don't need _make_estimator
1636         raise NotImplementedError()
1637 
1638     def _raw_predict_init(self, X):
1639         """Check input and compute raw predictions of the init estimtor."""
1640         self._check_initialized()
1641         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1642         if X.shape[1] != self.n_features_:
1643             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1644                 self.n_features_, X.shape[1]))
1645         if self.init_ == 'zero':
1646             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1647                                        dtype=np.float64)
1648         else:
1649             raw_predictions = self.loss_.get_init_raw_predictions(
1650                 X, self.init_).astype(np.float64)
1651         return raw_predictions
1652 
1653     def _raw_predict(self, X):
1654         """Return the sum of the trees raw predictions (+ init estimator)."""
1655         raw_predictions = self._raw_predict_init(X)
1656         predict_stages(self.estimators_, X, self.learning_rate,
1657                        raw_predictions)
1658         return raw_predictions
1659 
1660     def _staged_raw_predict(self, X):
1661         """Compute raw predictions of ``X`` for each iteration.
1662 
1663         This method allows monitoring (i.e. determine error on testing set)
1664         after each stage.
1665 
1666         Parameters
1667         ----------
1668         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1669             The input samples. Internally, it will be converted to
1670             ``dtype=np.float32`` and if a sparse matrix is provided
1671             to a sparse ``csr_matrix``.
1672 
1673         Returns
1674         -------
1675         raw_predictions : generator of array, shape (n_samples, k)
1676             The raw predictions of the input samples. The order of the
1677             classes corresponds to that in the attribute :term:`classes_`.
1678             Regression and binary classification are special cases with
1679             ``k == 1``, otherwise ``k==n_classes``.
1680         """
1681         X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1682         raw_predictions = self._raw_predict_init(X)
1683         for i in range(self.estimators_.shape[0]):
1684             predict_stage(self.estimators_, i, X, self.learning_rate,
1685                           raw_predictions)
1686             yield raw_predictions.copy()
1687 
1688     @property
1689     def feature_importances_(self):
1690         """Return the feature importances (the higher, the more important the
1691            feature).
1692 
1693         Returns
1694         -------
1695         feature_importances_ : array, shape (n_features,)
1696             The values of this array sum to 1, unless all trees are single node
1697             trees consisting of only the root node, in which case it will be an
1698             array of zeros.
1699         """
1700         self._check_initialized()
1701 
1702         relevant_trees = [tree
1703                           for stage in self.estimators_ for tree in stage
1704                           if tree.tree_.node_count > 1]
1705         if not relevant_trees:
1706             # degenerate case where all trees have only one node
1707             return np.zeros(shape=self.n_features_, dtype=np.float64)
1708 
1709         relevant_feature_importances = [
1710             tree.tree_.compute_feature_importances(normalize=False)
1711             for tree in relevant_trees
1712         ]
1713         avg_feature_importances = np.mean(relevant_feature_importances,
1714                                           axis=0, dtype=np.float64)
1715         return avg_feature_importances / np.sum(avg_feature_importances)
1716 
1717     def _compute_partial_dependence_recursion(self, grid, target_features):
1718         """Fast partial dependence computation.
1719 
1720         Parameters
1721         ----------
1722         grid : ndarray, shape (n_samples, n_target_features)
1723             The grid points on which the partial dependence should be
1724             evaluated.
1725         target_features : ndarray, shape (n_target_features)
1726             The set of target features for which the partial dependence
1727             should be evaluated.
1728 
1729         Returns
1730         -------
1731         averaged_predictions : ndarray, shape \
1732                 (n_trees_per_iteration, n_samples)
1733             The value of the partial dependence function on each grid point.
1734         """
1735         check_is_fitted(self,
1736                         msg="'estimator' parameter must be a fitted estimator")
1737         if self.init is not None:
1738             warnings.warn(
1739                 'Using recursion method with a non-constant init predictor '
1740                 'will lead to incorrect partial dependence values. '
1741                 'Got init=%s.' % self.init,
1742                 UserWarning
1743             )
1744         grid = np.asarray(grid, dtype=DTYPE, order='C')
1745         n_estimators, n_trees_per_stage = self.estimators_.shape
1746         averaged_predictions = np.zeros((n_trees_per_stage, grid.shape[0]),
1747                                         dtype=np.float64, order='C')
1748         for stage in range(n_estimators):
1749             for k in range(n_trees_per_stage):
1750                 tree = self.estimators_[stage, k].tree_
1751                 tree.compute_partial_dependence(grid, target_features,
1752                                                 averaged_predictions[k])
1753         averaged_predictions *= self.learning_rate
1754 
1755         return averaged_predictions
1756 
1757     def _validate_y(self, y, sample_weight):
1758         # 'sample_weight' is not utilised but is used for
1759         # consistency with similar method _validate_y of GBC
1760         self.n_classes_ = 1
1761         if y.dtype.kind == 'O':
1762             y = y.astype(DOUBLE)
1763         # Default implementation
1764         return y
1765 
1766     def apply(self, X):
1767         """Apply trees in the ensemble to X, return leaf indices.
1768 
1769         .. versionadded:: 0.17
1770 
1771         Parameters
1772         ----------
1773         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1774             The input samples. Internally, its dtype will be converted to
1775             ``dtype=np.float32``. If a sparse matrix is provided, it will
1776             be converted to a sparse ``csr_matrix``.
1777 
1778         Returns
1779         -------
1780         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1781             For each datapoint x in X and for each tree in the ensemble,
1782             return the index of the leaf x ends up in each estimator.
1783             In the case of binary classification n_classes is 1.
1784         """
1785 
1786         self._check_initialized()
1787         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1788 
1789         # n_classes will be equal to 1 in the binary classification or the
1790         # regression case.
1791         n_estimators, n_classes = self.estimators_.shape
1792         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1793 
1794         for i in range(n_estimators):
1795             for j in range(n_classes):
1796                 estimator = self.estimators_[i, j]
1797                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1798 
1799         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/isotonic.py</file>
<class>IsotonicRegression</class>
<method>fit</method>
<code>
302     def fit(self, X, y, sample_weight=None):
303         """Fit the model using X, y as training data.
304 
305         Parameters
306         ----------
307         X : array-like of shape (n_samples,)
308             Training data.
309 
310         y : array-like of shape (n_samples,)
311             Training target.
312 
313         sample_weight : array-like of shape (n_samples,), default=None
314             Weights. If set to None, all weights will be set to 1 (equal
315             weights).
316 
317         Returns
318         -------
319         self : object
320             Returns an instance of self.
321 
322         Notes
323         -----
324         X is stored for future use, as :meth:`transform` needs X to interpolate
325         new input data.
326         """
327         check_params = dict(accept_sparse=False, ensure_2d=False)
328         X = check_array(X, dtype=[np.float64, np.float32], **check_params)
329         y = check_array(y, dtype=X.dtype, **check_params)
330         check_consistent_length(X, y, sample_weight)
331 
332         # Transform y by running the isotonic regression algorithm and
333         # transform X accordingly.
334         X, y = self._build_y(X, y, sample_weight)
335 
336         # It is necessary to store the non-redundant part of the training set
337         # on the model to make it possible to support model persistence via
338         # the pickle module as the object built by scipy.interp1d is not
339         # picklable directly.
340         self._necessary_X_, self._necessary_y_ = X, y
341 
342         # Build the interpolation function
343         self._build_f(X, y)
344         return self

</code><intended_behavior>Adapt the `fit` method to include `_check_sample_weight` for `sample_weight` validation, aligning with the consistent approach of handling sample weights.</intended_behavior>, <file>sklearn/isotonic.py</file>
<class>IsotonicRegression</class>
<method>None</method>
<code>
140 class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
141     """Isotonic regression model.
142 
143     The isotonic regression optimization problem is defined by::
144 
145         min sum w_i (y[i] - y_[i]) ** 2
146 
147         subject to y_[i] <= y_[j] whenever X[i] <= X[j]
148         and min(y_) = y_min, max(y_) = y_max
149 
150     where:
151         - ``y[i]`` are inputs (real numbers)
152         - ``y_[i]`` are fitted
153         - ``X`` specifies the order.
154           If ``X`` is non-decreasing then ``y_`` is non-decreasing.
155         - ``w[i]`` are optional strictly positive weights (default to 1.0)
156 
157     Read more in the :ref:`User Guide <isotonic>`.
158 
159     Parameters
160     ----------
161     y_min : optional, default: None
162         If not None, set the lowest value of the fit to y_min.
163 
164     y_max : optional, default: None
165         If not None, set the highest value of the fit to y_max.
166 
167     increasing : boolean or string, optional, default: True
168         If boolean, whether or not to fit the isotonic regression with y
169         increasing or decreasing.
170 
171         The string value "auto" determines whether y should
172         increase or decrease based on the Spearman correlation estimate's
173         sign.
174 
175     out_of_bounds : string, optional, default: "nan"
176         The ``out_of_bounds`` parameter handles how x-values outside of the
177         training domain are handled.  When set to "nan", predicted y-values
178         will be NaN.  When set to "clip", predicted y-values will be
179         set to the value corresponding to the nearest train interval endpoint.
180         When set to "raise", allow ``interp1d`` to throw ValueError.
181 
182 
183     Attributes
184     ----------
185     X_min_ : float
186         Minimum value of input array `X_` for left bound.
187 
188     X_max_ : float
189         Maximum value of input array `X_` for right bound.
190 
191     f_ : function
192         The stepwise interpolating function that covers the input domain ``X``.
193 
194     Notes
195     -----
196     Ties are broken using the secondary method from Leeuw, 1977.
197 
198     References
199     ----------
200     Isotonic Median Regression: A Linear Programming Approach
201     Nilotpal Chakravarti
202     Mathematics of Operations Research
203     Vol. 14, No. 2 (May, 1989), pp. 303-308
204 
205     Isotone Optimization in R : Pool-Adjacent-Violators
206     Algorithm (PAVA) and Active Set Methods
207     Leeuw, Hornik, Mair
208     Journal of Statistical Software 2009
209 
210     Correctness of Kruskal's algorithms for monotone regression with ties
211     Leeuw, Psychometrica, 1977
212 
213     Examples
214     --------
215     >>> from sklearn.datasets import make_regression
216     >>> from sklearn.isotonic import IsotonicRegression
217     >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)
218     >>> iso_reg = IsotonicRegression().fit(X.flatten(), y)
219     >>> iso_reg.predict([.1, .2])
220     array([1.8628..., 3.7256...])
221     """
222     def __init__(self, y_min=None, y_max=None, increasing=True,
223                  out_of_bounds='nan'):
224         self.y_min = y_min
225         self.y_max = y_max
226         self.increasing = increasing
227         self.out_of_bounds = out_of_bounds
228 
229     def _check_fit_data(self, X, y, sample_weight=None):
230         if len(X.shape) != 1:
231             raise ValueError("X should be a 1d array")
232 
233     def _build_f(self, X, y):
234         """Build the f_ interp1d function."""
235 
236         # Handle the out_of_bounds argument by setting bounds_error
237         if self.out_of_bounds not in ["raise", "nan", "clip"]:
238             raise ValueError("The argument ``out_of_bounds`` must be in "
239                              "'nan', 'clip', 'raise'; got {0}"
240                              .format(self.out_of_bounds))
241 
242         bounds_error = self.out_of_bounds == "raise"
243         if len(y) == 1:
244             # single y, constant prediction
245             self.f_ = lambda x: y.repeat(x.shape)
246         else:
247             self.f_ = interpolate.interp1d(X, y, kind='linear',
248                                            bounds_error=bounds_error)
249 
250     def _build_y(self, X, y, sample_weight, trim_duplicates=True):
251         """Build the y_ IsotonicRegression."""
252         self._check_fit_data(X, y, sample_weight)
253 
254         # Determine increasing if auto-determination requested
255         if self.increasing == 'auto':
256             self.increasing_ = check_increasing(X, y)
257         else:
258             self.increasing_ = self.increasing
259 
260         # If sample_weights is passed, removed zero-weight values and clean
261         # order
262         if sample_weight is not None:
263             sample_weight = check_array(sample_weight, ensure_2d=False,
264                                         dtype=X.dtype)
265             mask = sample_weight > 0
266             X, y, sample_weight = X[mask], y[mask], sample_weight[mask]
267         else:
268             sample_weight = np.ones(len(y), dtype=X.dtype)
269 
270         order = np.lexsort((y, X))
271         X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]
272         unique_X, unique_y, unique_sample_weight = _make_unique(
273             X, y, sample_weight)
274 
275         # Store _X_ and _y_ to maintain backward compat during the deprecation
276         # period of X_ and y_
277         self._X_ = X = unique_X
278         self._y_ = y = isotonic_regression(unique_y, unique_sample_weight,
279                                            self.y_min, self.y_max,
280                                            increasing=self.increasing_)
281 
282         # Handle the left and right bounds on X
283         self.X_min_, self.X_max_ = np.min(X), np.max(X)
284 
285         if trim_duplicates:
286             # Remove unnecessary points for faster prediction
287             keep_data = np.ones((len(y),), dtype=bool)
288             # Aside from the 1st and last point, remove points whose y values
289             # are equal to both the point before and the point after it.
290             keep_data[1:-1] = np.logical_or(
291                 np.not_equal(y[1:-1], y[:-2]),
292                 np.not_equal(y[1:-1], y[2:])
293             )
294             return X[keep_data], y[keep_data]
295         else:
296             # The ability to turn off trim_duplicates is only used to it make
297             # easier to unit test that removing duplicates in y does not have
298             # any impact the resulting interpolation function (besides
299             # prediction speed).
300             return X, y
301 
302     def fit(self, X, y, sample_weight=None):
303         """Fit the model using X, y as training data.
304 
305         Parameters
306         ----------
307         X : array-like of shape (n_samples,)
308             Training data.
309 
310         y : array-like of shape (n_samples,)
311             Training target.
312 
313         sample_weight : array-like of shape (n_samples,), default=None
314             Weights. If set to None, all weights will be set to 1 (equal
315             weights).
316 
317         Returns
318         -------
319         self : object
320             Returns an instance of self.
321 
322         Notes
323         -----
324         X is stored for future use, as :meth:`transform` needs X to interpolate
325         new input data.
326         """
327         check_params = dict(accept_sparse=False, ensure_2d=False)
328         X = check_array(X, dtype=[np.float64, np.float32], **check_params)
329         y = check_array(y, dtype=X.dtype, **check_params)
330         check_consistent_length(X, y, sample_weight)
331 
332         # Transform y by running the isotonic regression algorithm and
333         # transform X accordingly.
334         X, y = self._build_y(X, y, sample_weight)
335 
336         # It is necessary to store the non-redundant part of the training set
337         # on the model to make it possible to support model persistence via
338         # the pickle module as the object built by scipy.interp1d is not
339         # picklable directly.
340         self._necessary_X_, self._necessary_y_ = X, y
341 
342         # Build the interpolation function
343         self._build_f(X, y)
344         return self
345 
346     def transform(self, T):
347         """Transform new data by linear interpolation
348 
349         Parameters
350         ----------
351         T : array-like of shape (n_samples,)
352             Data to transform.
353 
354         Returns
355         -------
356         T_ : array, shape=(n_samples,)
357             The transformed data
358         """
359 
360         if hasattr(self, '_necessary_X_'):
361             dtype = self._necessary_X_.dtype
362         else:
363             dtype = np.float64
364 
365         T = check_array(T, dtype=dtype, ensure_2d=False)
366 
367         if len(T.shape) != 1:
368             raise ValueError("Isotonic regression input should be a 1d array")
369 
370         # Handle the out_of_bounds argument by clipping if needed
371         if self.out_of_bounds not in ["raise", "nan", "clip"]:
372             raise ValueError("The argument ``out_of_bounds`` must be in "
373                              "'nan', 'clip', 'raise'; got {0}"
374                              .format(self.out_of_bounds))
375 
376         if self.out_of_bounds == "clip":
377             T = np.clip(T, self.X_min_, self.X_max_)
378 
379         res = self.f_(T)
380 
381         # on scipy 0.17, interp1d up-casts to float64, so we cast back
382         res = res.astype(T.dtype)
383 
384         return res
385 
386     def predict(self, T):
387         """Predict new data by linear interpolation.
388 
389         Parameters
390         ----------
391         T : array-like of shape (n_samples,)
392             Data to transform.
393 
394         Returns
395         -------
396         T_ : array, shape=(n_samples,)
397             Transformed data.
398         """
399         return self.transform(T)
400 
401     def __getstate__(self):
402         """Pickle-protocol - return state of the estimator. """
403         state = super().__getstate__()
404         # remove interpolation method
405         state.pop('f_', None)
406         return state
407 
408     def __setstate__(self, state):
409         """Pickle-protocol - set state of the estimator.
410 
411         We need to rebuild the interpolation function.
412         """
413         super().__setstate__(state)
414         if hasattr(self, '_necessary_X_') and hasattr(self, '_necessary_y_'):
415             self._build_f(self._necessary_X_, self._necessary_y_)
416 
417     def _more_tags(self):
418         return {'X_types': ['1darray']}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/kernel_ridge.py</file>
<class>KernelRidge</class>
<method>fit</method>
<code>
130     def fit(self, X, y=None, sample_weight=None):
131         """Fit Kernel Ridge regression model
132 
133         Parameters
134         ----------
135         X : {array-like, sparse matrix} of shape (n_samples, n_features)
136             Training data. If kernel == "precomputed" this is instead
137             a precomputed kernel matrix, shape = [n_samples,
138             n_samples].
139 
140         y : array-like of shape (n_samples,) or (n_samples, n_targets)
141             Target values
142 
143         sample_weight : float or array-like of shape [n_samples]
144             Individual weights for each sample, ignored if None is passed.
145 
146         Returns
147         -------
148         self : returns an instance of self.
149         """
150         # Convert data
151         X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True,
152                          y_numeric=True)
153         if sample_weight is not None and not isinstance(sample_weight, float):
154             sample_weight = check_array(sample_weight, ensure_2d=False)
155 
156         K = self._get_kernel(X)
157         alpha = np.atleast_1d(self.alpha)
158 
159         ravel = False
160         if len(y.shape) == 1:
161             y = y.reshape(-1, 1)
162             ravel = True
163 
164         copy = self.kernel == "precomputed"
165         self.dual_coef_ = _solve_cholesky_kernel(K, y, alpha,
166                                                  sample_weight,
167                                                  copy)
168         if ravel:
169             self.dual_coef_ = self.dual_coef_.ravel()
170 
171         self.X_fit_ = X
172 
173         return self

</code><intended_behavior>Within the `fit` method, replace any existing sample weight validation logic with `_check_sample_weight`, ensuring consistent validation across estimators.</intended_behavior>, <file>sklearn/kernel_ridge.py</file>
<class>KernelRidge</class>
<method>None</method>
<code>
16 class KernelRidge(MultiOutputMixin, RegressorMixin, BaseEstimator):
17     """Kernel ridge regression.
18 
19     Kernel ridge regression (KRR) combines ridge regression (linear least
20     squares with l2-norm regularization) with the kernel trick. It thus
21     learns a linear function in the space induced by the respective kernel and
22     the data. For non-linear kernels, this corresponds to a non-linear
23     function in the original space.
24 
25     The form of the model learned by KRR is identical to support vector
26     regression (SVR). However, different loss functions are used: KRR uses
27     squared error loss while support vector regression uses epsilon-insensitive
28     loss, both combined with l2 regularization. In contrast to SVR, fitting a
29     KRR model can be done in closed-form and is typically faster for
30     medium-sized datasets. On the other hand, the learned model is non-sparse
31     and thus slower than SVR, which learns a sparse model for epsilon > 0, at
32     prediction-time.
33 
34     This estimator has built-in support for multi-variate regression
35     (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
36 
37     Read more in the :ref:`User Guide <kernel_ridge>`.
38 
39     Parameters
40     ----------
41     alpha : {float, array-like}, shape = [n_targets]
42         Small positive values of alpha improve the conditioning of the problem
43         and reduce the variance of the estimates.  Alpha corresponds to
44         ``(2*C)^-1`` in other linear models such as LogisticRegression or
45         LinearSVC. If an array is passed, penalties are assumed to be specific
46         to the targets. Hence they must correspond in number.
47 
48     kernel : string or callable, default="linear"
49         Kernel mapping used internally. A callable should accept two arguments
50         and the keyword arguments passed to this object as kernel_params, and
51         should return a floating point number. Set to "precomputed" in
52         order to pass a precomputed kernel matrix to the estimator
53         methods instead of samples.
54 
55     gamma : float, default=None
56         Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
57         and sigmoid kernels. Interpretation of the default value is left to
58         the kernel; see the documentation for sklearn.metrics.pairwise.
59         Ignored by other kernels.
60 
61     degree : float, default=3
62         Degree of the polynomial kernel. Ignored by other kernels.
63 
64     coef0 : float, default=1
65         Zero coefficient for polynomial and sigmoid kernels.
66         Ignored by other kernels.
67 
68     kernel_params : mapping of string to any, optional
69         Additional parameters (keyword arguments) for kernel function passed
70         as callable object.
71 
72     Attributes
73     ----------
74     dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
75         Representation of weight vector(s) in kernel space
76 
77     X_fit_ : {array-like, sparse matrix} of shape (n_samples, n_features)
78         Training data, which is also required for prediction. If
79         kernel == "precomputed" this is instead the precomputed
80         training matrix, shape = [n_samples, n_samples].
81 
82     References
83     ----------
84     * Kevin P. Murphy
85       "Machine Learning: A Probabilistic Perspective", The MIT Press
86       chapter 14.4.3, pp. 492-493
87 
88     See also
89     --------
90     sklearn.linear_model.Ridge:
91         Linear ridge regression.
92     sklearn.svm.SVR:
93         Support Vector Regression implemented using libsvm.
94 
95     Examples
96     --------
97     >>> from sklearn.kernel_ridge import KernelRidge
98     >>> import numpy as np
99     >>> n_samples, n_features = 10, 5
100     >>> rng = np.random.RandomState(0)
101     >>> y = rng.randn(n_samples)
102     >>> X = rng.randn(n_samples, n_features)
103     >>> clf = KernelRidge(alpha=1.0)
104     >>> clf.fit(X, y)
105     KernelRidge(alpha=1.0)
106     """
107     def __init__(self, alpha=1, kernel="linear", gamma=None, degree=3, coef0=1,
108                  kernel_params=None):
109         self.alpha = alpha
110         self.kernel = kernel
111         self.gamma = gamma
112         self.degree = degree
113         self.coef0 = coef0
114         self.kernel_params = kernel_params
115 
116     def _get_kernel(self, X, Y=None):
117         if callable(self.kernel):
118             params = self.kernel_params or {}
119         else:
120             params = {"gamma": self.gamma,
121                       "degree": self.degree,
122                       "coef0": self.coef0}
123         return pairwise_kernels(X, Y, metric=self.kernel,
124                                 filter_params=True, **params)
125 
126     @property
127     def _pairwise(self):
128         return self.kernel == "precomputed"
129 
130     def fit(self, X, y=None, sample_weight=None):
131         """Fit Kernel Ridge regression model
132 
133         Parameters
134         ----------
135         X : {array-like, sparse matrix} of shape (n_samples, n_features)
136             Training data. If kernel == "precomputed" this is instead
137             a precomputed kernel matrix, shape = [n_samples,
138             n_samples].
139 
140         y : array-like of shape (n_samples,) or (n_samples, n_targets)
141             Target values
142 
143         sample_weight : float or array-like of shape [n_samples]
144             Individual weights for each sample, ignored if None is passed.
145 
146         Returns
147         -------
148         self : returns an instance of self.
149         """
150         # Convert data
151         X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True,
152                          y_numeric=True)
153         if sample_weight is not None and not isinstance(sample_weight, float):
154             sample_weight = check_array(sample_weight, ensure_2d=False)
155 
156         K = self._get_kernel(X)
157         alpha = np.atleast_1d(self.alpha)
158 
159         ravel = False
160         if len(y.shape) == 1:
161             y = y.reshape(-1, 1)
162             ravel = True
163 
164         copy = self.kernel == "precomputed"
165         self.dual_coef_ = _solve_cholesky_kernel(K, y, alpha,
166                                                  sample_weight,
167                                                  copy)
168         if ravel:
169             self.dual_coef_ = self.dual_coef_.ravel()
170 
171         self.X_fit_ = X
172 
173         return self
174 
175     def predict(self, X):
176         """Predict using the kernel ridge model
177 
178         Parameters
179         ----------
180         X : {array-like, sparse matrix} of shape (n_samples, n_features)
181             Samples. If kernel == "precomputed" this is instead a
182             precomputed kernel matrix, shape = [n_samples,
183             n_samples_fitted], where n_samples_fitted is the number of
184             samples used in the fitting for this estimator.
185 
186         Returns
187         -------
188         C : ndarray of shape (n_samples,) or (n_samples, n_targets)
189             Returns predicted values.
190         """
191         check_is_fitted(self)
192         K = self._get_kernel(X, self.X_fit_)
193         return np.dot(K, self.dual_coef_)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>GaussianNB</class>
<method>fit</method>
<code>
185     def fit(self, X, y, sample_weight=None):
186         """Fit Gaussian Naive Bayes according to X, y
187 
188         Parameters
189         ----------
190         X : array-like, shape (n_samples, n_features)
191             Training vectors, where n_samples is the number of samples
192             and n_features is the number of features.
193 
194         y : array-like, shape (n_samples,)
195             Target values.
196 
197         sample_weight : array-like, shape (n_samples,), optional (default=None)
198             Weights applied to individual samples (1. for unweighted).
199 
200             .. versionadded:: 0.17
201                Gaussian Naive Bayes supports fitting with *sample_weight*.
202 
203         Returns
204         -------
205         self : object
206         """
207         y = column_or_1d(y, warn=True)
208         return self._partial_fit(X, y, np.unique(y), _refit=True,
209                                  sample_weight=sample_weight)

</code><intended_behavior>Utilize `_check_sample_weight` for validating `sample_weight` in the `fit` method, aligning GaussianNB's handling of sample weights with the rest of the project.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>GaussianNB</class>
<method>None</method>
<code>
118 class GaussianNB(BaseNB):
119     """
120     Gaussian Naive Bayes (GaussianNB)
121 
122     Can perform online updates to model parameters via :meth:`partial_fit`.
123     For details on algorithm used to update feature means and variance online,
124     see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:
125 
126         http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf
127 
128     Read more in the :ref:`User Guide <gaussian_naive_bayes>`.
129 
130     Parameters
131     ----------
132     priors : array-like, shape (n_classes,)
133         Prior probabilities of the classes. If specified the priors are not
134         adjusted according to the data.
135 
136     var_smoothing : float, optional (default=1e-9)
137         Portion of the largest variance of all features that is added to
138         variances for calculation stability.
139 
140     Attributes
141     ----------
142     class_prior_ : array, shape (n_classes,)
143         probability of each class.
144 
145     class_count_ : array, shape (n_classes,)
146         number of training samples observed in each class.
147 
148     classes_ : array, shape (n_classes,)
149         class labels known to the classifier
150 
151     theta_ : array, shape (n_classes, n_features)
152         mean of each feature per class
153 
154     sigma_ : array, shape (n_classes, n_features)
155         variance of each feature per class
156 
157     epsilon_ : float
158         absolute additive value to variances
159 
160     classes_ : array-like, shape (n_classes,)
161         Unique class labels.
162 
163     Examples
164     --------
165     >>> import numpy as np
166     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
167     >>> Y = np.array([1, 1, 1, 2, 2, 2])
168     >>> from sklearn.naive_bayes import GaussianNB
169     >>> clf = GaussianNB()
170     >>> clf.fit(X, Y)
171     GaussianNB()
172     >>> print(clf.predict([[-0.8, -1]]))
173     [1]
174     >>> clf_pf = GaussianNB()
175     >>> clf_pf.partial_fit(X, Y, np.unique(Y))
176     GaussianNB()
177     >>> print(clf_pf.predict([[-0.8, -1]]))
178     [1]
179     """
180 
181     def __init__(self, priors=None, var_smoothing=1e-9):
182         self.priors = priors
183         self.var_smoothing = var_smoothing
184 
185     def fit(self, X, y, sample_weight=None):
186         """Fit Gaussian Naive Bayes according to X, y
187 
188         Parameters
189         ----------
190         X : array-like, shape (n_samples, n_features)
191             Training vectors, where n_samples is the number of samples
192             and n_features is the number of features.
193 
194         y : array-like, shape (n_samples,)
195             Target values.
196 
197         sample_weight : array-like, shape (n_samples,), optional (default=None)
198             Weights applied to individual samples (1. for unweighted).
199 
200             .. versionadded:: 0.17
201                Gaussian Naive Bayes supports fitting with *sample_weight*.
202 
203         Returns
204         -------
205         self : object
206         """
207         y = column_or_1d(y, warn=True)
208         return self._partial_fit(X, y, np.unique(y), _refit=True,
209                                  sample_weight=sample_weight)
210 
211     def _check_X(self, X):
212         return check_array(X)
213 
214     @staticmethod
215     def _update_mean_variance(n_past, mu, var, X, sample_weight=None):
216         """Compute online update of Gaussian mean and variance.
217 
218         Given starting sample count, mean, and variance, a new set of
219         points X, and optionally sample weights, return the updated mean and
220         variance. (NB - each dimension (column) in X is treated as independent
221         -- you get variance, not covariance).
222 
223         Can take scalar mean and variance, or vector mean and variance to
224         simultaneously update a number of independent Gaussians.
225 
226         See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:
227 
228         http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf
229 
230         Parameters
231         ----------
232         n_past : int
233             Number of samples represented in old mean and variance. If sample
234             weights were given, this should contain the sum of sample
235             weights represented in old mean and variance.
236 
237         mu : array-like, shape (number of Gaussians,)
238             Means for Gaussians in original set.
239 
240         var : array-like, shape (number of Gaussians,)
241             Variances for Gaussians in original set.
242 
243         sample_weight : array-like, shape (n_samples,), optional (default=None)
244             Weights applied to individual samples (1. for unweighted).
245 
246         Returns
247         -------
248         total_mu : array-like, shape (number of Gaussians,)
249             Updated mean for each Gaussian over the combined set.
250 
251         total_var : array-like, shape (number of Gaussians,)
252             Updated variance for each Gaussian over the combined set.
253         """
254         if X.shape[0] == 0:
255             return mu, var
256 
257         # Compute (potentially weighted) mean and variance of new datapoints
258         if sample_weight is not None:
259             n_new = float(sample_weight.sum())
260             new_mu = np.average(X, axis=0, weights=sample_weight)
261             new_var = np.average((X - new_mu) ** 2, axis=0,
262                                  weights=sample_weight)
263         else:
264             n_new = X.shape[0]
265             new_var = np.var(X, axis=0)
266             new_mu = np.mean(X, axis=0)
267 
268         if n_past == 0:
269             return new_mu, new_var
270 
271         n_total = float(n_past + n_new)
272 
273         # Combine mean of old and new data, taking into consideration
274         # (weighted) number of observations
275         total_mu = (n_new * new_mu + n_past * mu) / n_total
276 
277         # Combine variance of old and new data, taking into consideration
278         # (weighted) number of observations. This is achieved by combining
279         # the sum-of-squared-differences (ssd)
280         old_ssd = n_past * var
281         new_ssd = n_new * new_var
282         total_ssd = (old_ssd + new_ssd +
283                      (n_new * n_past / n_total) * (mu - new_mu) ** 2)
284         total_var = total_ssd / n_total
285 
286         return total_mu, total_var
287 
288     def partial_fit(self, X, y, classes=None, sample_weight=None):
289         """Incremental fit on a batch of samples.
290 
291         This method is expected to be called several times consecutively
292         on different chunks of a dataset so as to implement out-of-core
293         or online learning.
294 
295         This is especially useful when the whole dataset is too big to fit in
296         memory at once.
297 
298         This method has some performance and numerical stability overhead,
299         hence it is better to call partial_fit on chunks of data that are
300         as large as possible (as long as fitting in the memory budget) to
301         hide the overhead.
302 
303         Parameters
304         ----------
305         X : array-like, shape (n_samples, n_features)
306             Training vectors, where n_samples is the number of samples and
307             n_features is the number of features.
308 
309         y : array-like, shape (n_samples,)
310             Target values.
311 
312         classes : array-like, shape (n_classes,), optional (default=None)
313             List of all the classes that can possibly appear in the y vector.
314 
315             Must be provided at the first call to partial_fit, can be omitted
316             in subsequent calls.
317 
318         sample_weight : array-like, shape (n_samples,), optional (default=None)
319             Weights applied to individual samples (1. for unweighted).
320 
321             .. versionadded:: 0.17
322 
323         Returns
324         -------
325         self : object
326         """
327         return self._partial_fit(X, y, classes, _refit=False,
328                                  sample_weight=sample_weight)
329 
330     def _partial_fit(self, X, y, classes=None, _refit=False,
331                      sample_weight=None):
332         """Actual implementation of Gaussian NB fitting.
333 
334         Parameters
335         ----------
336         X : array-like, shape (n_samples, n_features)
337             Training vectors, where n_samples is the number of samples and
338             n_features is the number of features.
339 
340         y : array-like, shape (n_samples,)
341             Target values.
342 
343         classes : array-like, shape (n_classes,), optional (default=None)
344             List of all the classes that can possibly appear in the y vector.
345 
346             Must be provided at the first call to partial_fit, can be omitted
347             in subsequent calls.
348 
349         _refit : bool, optional (default=False)
350             If true, act as though this were the first time we called
351             _partial_fit (ie, throw away any past fitting and start over).
352 
353         sample_weight : array-like, shape (n_samples,), optional (default=None)
354             Weights applied to individual samples (1. for unweighted).
355 
356         Returns
357         -------
358         self : object
359         """
360         X, y = check_X_y(X, y)
361         if sample_weight is not None:
362             sample_weight = check_array(sample_weight, ensure_2d=False)
363             check_consistent_length(y, sample_weight)
364 
365         # If the ratio of data variance between dimensions is too small, it
366         # will cause numerical errors. To address this, we artificially
367         # boost the variance by epsilon, a small fraction of the standard
368         # deviation of the largest dimension.
369         self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()
370 
371         if _refit:
372             self.classes_ = None
373 
374         if _check_partial_fit_first_call(self, classes):
375             # This is the first call to partial_fit:
376             # initialize various cumulative counters
377             n_features = X.shape[1]
378             n_classes = len(self.classes_)
379             self.theta_ = np.zeros((n_classes, n_features))
380             self.sigma_ = np.zeros((n_classes, n_features))
381 
382             self.class_count_ = np.zeros(n_classes, dtype=np.float64)
383 
384             # Initialise the class prior
385             # Take into account the priors
386             if self.priors is not None:
387                 priors = np.asarray(self.priors)
388                 # Check that the provide prior match the number of classes
389                 if len(priors) != n_classes:
390                     raise ValueError('Number of priors must match number of'
391                                      ' classes.')
392                 # Check that the sum is 1
393                 if not np.isclose(priors.sum(), 1.0):
394                     raise ValueError('The sum of the priors should be 1.')
395                 # Check that the prior are non-negative
396                 if (priors < 0).any():
397                     raise ValueError('Priors must be non-negative.')
398                 self.class_prior_ = priors
399             else:
400                 # Initialize the priors to zeros for each class
401                 self.class_prior_ = np.zeros(len(self.classes_),
402                                              dtype=np.float64)
403         else:
404             if X.shape[1] != self.theta_.shape[1]:
405                 msg = "Number of features %d does not match previous data %d."
406                 raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))
407             # Put epsilon back in each time
408             self.sigma_[:, :] -= self.epsilon_
409 
410         classes = self.classes_
411 
412         unique_y = np.unique(y)
413         unique_y_in_classes = np.in1d(unique_y, classes)
414 
415         if not np.all(unique_y_in_classes):
416             raise ValueError("The target label(s) %s in y do not exist in the "
417                              "initial classes %s" %
418                              (unique_y[~unique_y_in_classes], classes))
419 
420         for y_i in unique_y:
421             i = classes.searchsorted(y_i)
422             X_i = X[y == y_i, :]
423 
424             if sample_weight is not None:
425                 sw_i = sample_weight[y == y_i]
426                 N_i = sw_i.sum()
427             else:
428                 sw_i = None
429                 N_i = X_i.shape[0]
430 
431             new_theta, new_sigma = self._update_mean_variance(
432                 self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],
433                 X_i, sw_i)
434 
435             self.theta_[i, :] = new_theta
436             self.sigma_[i, :] = new_sigma
437             self.class_count_[i] += N_i
438 
439         self.sigma_[:, :] += self.epsilon_
440 
441         # Update if only no priors is provided
442         if self.priors is None:
443             # Empirical prior, with sample_weight taken into account
444             self.class_prior_ = self.class_count_ / self.class_count_.sum()
445 
446         return self
447 
448     def _joint_log_likelihood(self, X):
449         joint_log_likelihood = []
450         for i in range(np.size(self.classes_)):
451             jointi = np.log(self.class_prior_[i])
452             n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))
453             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /
454                                  (self.sigma_[i, :]), 1)
455             joint_log_likelihood.append(jointi + n_ij)
456 
457         joint_log_likelihood = np.array(joint_log_likelihood).T
458         return joint_log_likelihood

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>BaseDiscreteNB</class>
<method>fit</method>
<code>
592     def fit(self, X, y, sample_weight=None):
593         """Fit Naive Bayes classifier according to X, y
594 
595         Parameters
596         ----------
597         X : {array-like, sparse matrix} of shape (n_samples, n_features)
598             Training vectors, where n_samples is the number of samples and
599             n_features is the number of features.
600 
601         y : array-like of shape (n_samples,)
602             Target values.
603 
604         sample_weight : array-like of shape (n_samples,), default=None
605             Weights applied to individual samples (1. for unweighted).
606 
607         Returns
608         -------
609         self : object
610         """
611         X, y = self._check_X_y(X, y)
612         _, n_features = X.shape
613         self.n_features_ = n_features
614 
615         labelbin = LabelBinarizer()
616         Y = labelbin.fit_transform(y)
617         self.classes_ = labelbin.classes_
618         if Y.shape[1] == 1:
619             Y = np.concatenate((1 - Y, Y), axis=1)
620 
621         # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.
622         # We convert it to np.float64 to support sample_weight consistently;
623         # this means we also don't have to cast X to floating point
624         if sample_weight is not None:
625             Y = Y.astype(np.float64, copy=False)
626             sample_weight = np.asarray(sample_weight)
627             sample_weight = np.atleast_2d(sample_weight)
628             Y *= check_array(sample_weight).T
629 
630         class_prior = self.class_prior
631 
632         # Count raw events from data before updating the class log prior
633         # and feature log probas
634         n_effective_classes = Y.shape[1]
635 
636         self._init_counters(n_effective_classes, n_features)
637         self._count(X, Y)
638         alpha = self._check_alpha()
639         self._update_feature_log_prob(alpha)
640         self._update_class_log_prior(class_prior=class_prior)
641         return self

</code><intended_behavior>The `fit` method should incorporate `_check_sample_weight` for `sample_weight` validation, ensuring consistency in sample weight handling.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>BaseDiscreteNB</class>
<method>None</method>
<code>
464 class BaseDiscreteNB(BaseNB):
465     """Abstract base class for naive Bayes on discrete/categorical data
466 
467     Any estimator based on this class should provide:
468 
469     __init__
470     _joint_log_likelihood(X) as per BaseNB
471     """
472 
473     def _check_X(self, X):
474         return check_array(X, accept_sparse='csr')
475 
476     def _check_X_y(self, X, y):
477         return check_X_y(X, y, accept_sparse='csr')
478 
479     def _update_class_log_prior(self, class_prior=None):
480         n_classes = len(self.classes_)
481         if class_prior is not None:
482             if len(class_prior) != n_classes:
483                 raise ValueError("Number of priors must match number of"
484                                  " classes.")
485             self.class_log_prior_ = np.log(class_prior)
486         elif self.fit_prior:
487             with warnings.catch_warnings():
488                 # silence the warning when count is 0 because class was not yet
489                 # observed
490                 warnings.simplefilter("ignore", RuntimeWarning)
491                 log_class_count = np.log(self.class_count_)
492 
493             # empirical prior, with sample_weight taken into account
494             self.class_log_prior_ = (log_class_count -
495                                      np.log(self.class_count_.sum()))
496         else:
497             self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))
498 
499     def _check_alpha(self):
500         if np.min(self.alpha) < 0:
501             raise ValueError('Smoothing parameter alpha = %.1e. '
502                              'alpha should be > 0.' % np.min(self.alpha))
503         if isinstance(self.alpha, np.ndarray):
504             if not self.alpha.shape[0] == self.n_features_:
505                 raise ValueError("alpha should be a scalar or a numpy array "
506                                  "with shape [n_features]")
507         if np.min(self.alpha) < _ALPHA_MIN:
508             warnings.warn('alpha too small will result in numeric errors, '
509                           'setting alpha = %.1e' % _ALPHA_MIN)
510             return np.maximum(self.alpha, _ALPHA_MIN)
511         return self.alpha
512 
513     def partial_fit(self, X, y, classes=None, sample_weight=None):
514         """Incremental fit on a batch of samples.
515 
516         This method is expected to be called several times consecutively
517         on different chunks of a dataset so as to implement out-of-core
518         or online learning.
519 
520         This is especially useful when the whole dataset is too big to fit in
521         memory at once.
522 
523         This method has some performance overhead hence it is better to call
524         partial_fit on chunks of data that are as large as possible
525         (as long as fitting in the memory budget) to hide the overhead.
526 
527         Parameters
528         ----------
529         X : {array-like, sparse matrix} of shape (n_samples, n_features)
530             Training vectors, where n_samples is the number of samples and
531             n_features is the number of features.
532 
533         y : array-like of shape (n_samples,)
534             Target values.
535 
536         classes : array-like of shape (n_classes) (default=None)
537             List of all the classes that can possibly appear in the y vector.
538 
539             Must be provided at the first call to partial_fit, can be omitted
540             in subsequent calls.
541 
542         sample_weight : array-like of shape (n_samples,), default=None
543             Weights applied to individual samples (1. for unweighted).
544 
545         Returns
546         -------
547         self : object
548         """
549         X, y = self._check_X_y(X, y)
550         _, n_features = X.shape
551 
552         if _check_partial_fit_first_call(self, classes):
553             # This is the first call to partial_fit:
554             # initialize various cumulative counters
555             n_effective_classes = len(classes) if len(classes) > 1 else 2
556             self._init_counters(n_effective_classes, n_features)
557             self.n_features_ = n_features
558         elif n_features != self.n_features_:
559             msg = "Number of features %d does not match previous data %d."
560             raise ValueError(msg % (n_features, self.n_features_))
561 
562         Y = label_binarize(y, classes=self.classes_)
563         if Y.shape[1] == 1:
564             Y = np.concatenate((1 - Y, Y), axis=1)
565 
566         if X.shape[0] != Y.shape[0]:
567             msg = "X.shape[0]=%d and y.shape[0]=%d are incompatible."
568             raise ValueError(msg % (X.shape[0], y.shape[0]))
569 
570         # label_binarize() returns arrays with dtype=np.int64.
571         # We convert it to np.float64 to support sample_weight consistently
572         Y = Y.astype(np.float64, copy=False)
573         if sample_weight is not None:
574             sample_weight = np.atleast_2d(sample_weight)
575             Y *= check_array(sample_weight).T
576 
577         class_prior = self.class_prior
578 
579         # Count raw events from data before updating the class log prior
580         # and feature log probas
581         self._count(X, Y)
582 
583         # XXX: OPTIM: we could introduce a public finalization method to
584         # be called by the user explicitly just once after several consecutive
585         # calls to partial_fit and prior any call to predict[_[log_]proba]
586         # to avoid computing the smooth log probas at each call to partial fit
587         alpha = self._check_alpha()
588         self._update_feature_log_prob(alpha)
589         self._update_class_log_prior(class_prior=class_prior)
590         return self
591 
592     def fit(self, X, y, sample_weight=None):
593         """Fit Naive Bayes classifier according to X, y
594 
595         Parameters
596         ----------
597         X : {array-like, sparse matrix} of shape (n_samples, n_features)
598             Training vectors, where n_samples is the number of samples and
599             n_features is the number of features.
600 
601         y : array-like of shape (n_samples,)
602             Target values.
603 
604         sample_weight : array-like of shape (n_samples,), default=None
605             Weights applied to individual samples (1. for unweighted).
606 
607         Returns
608         -------
609         self : object
610         """
611         X, y = self._check_X_y(X, y)
612         _, n_features = X.shape
613         self.n_features_ = n_features
614 
615         labelbin = LabelBinarizer()
616         Y = labelbin.fit_transform(y)
617         self.classes_ = labelbin.classes_
618         if Y.shape[1] == 1:
619             Y = np.concatenate((1 - Y, Y), axis=1)
620 
621         # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.
622         # We convert it to np.float64 to support sample_weight consistently;
623         # this means we also don't have to cast X to floating point
624         if sample_weight is not None:
625             Y = Y.astype(np.float64, copy=False)
626             sample_weight = np.asarray(sample_weight)
627             sample_weight = np.atleast_2d(sample_weight)
628             Y *= check_array(sample_weight).T
629 
630         class_prior = self.class_prior
631 
632         # Count raw events from data before updating the class log prior
633         # and feature log probas
634         n_effective_classes = Y.shape[1]
635 
636         self._init_counters(n_effective_classes, n_features)
637         self._count(X, Y)
638         alpha = self._check_alpha()
639         self._update_feature_log_prob(alpha)
640         self._update_class_log_prior(class_prior=class_prior)
641         return self
642 
643     def _init_counters(self, n_effective_classes, n_features):
644         self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)
645         self.feature_count_ = np.zeros((n_effective_classes, n_features),
646                                        dtype=np.float64)
647 
648     # XXX The following is a stopgap measure; we need to set the dimensions
649     # of class_log_prior_ and feature_log_prob_ correctly.
650     def _get_coef(self):
651         return (self.feature_log_prob_[1:]
652                 if len(self.classes_) == 2 else self.feature_log_prob_)
653 
654     def _get_intercept(self):
655         return (self.class_log_prior_[1:]
656                 if len(self.classes_) == 2 else self.class_log_prior_)
657 
658     coef_ = property(_get_coef)
659     intercept_ = property(_get_intercept)
660 
661     def _more_tags(self):
662         return {'poor_score': True}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/neighbors/_kde.py</file>
<class>KernelDensity</class>
<method>fit</method>
<code>
134     def fit(self, X, y=None, sample_weight=None):
135         """Fit the Kernel Density model on the data.
136 
137         Parameters
138         ----------
139         X : array_like, shape (n_samples, n_features)
140             List of n_features-dimensional data points.  Each row
141             corresponds to a single data point.
142         y : None
143             Ignored. This parameter exists only for compatibility with
144             :class:`sklearn.pipeline.Pipeline`.
145         sample_weight : array_like, shape (n_samples,), optional
146             List of sample weights attached to the data X.
147 
148         Returns
149         -------
150         self : object
151             Returns instance of object.
152         """
153         algorithm = self._choose_algorithm(self.algorithm, self.metric)
154         X = check_array(X, order='C', dtype=DTYPE)
155 
156         if sample_weight is not None:
157             sample_weight = check_array(sample_weight, order='C', dtype=DTYPE,
158                                         ensure_2d=False)
159             if sample_weight.ndim != 1:
160                 raise ValueError("the shape of sample_weight must be ({0},),"
161                                  " but was {1}".format(X.shape[0],
162                                                        sample_weight.shape))
163             check_consistent_length(X, sample_weight)
164             if sample_weight.min() <= 0:
165                 raise ValueError("sample_weight must have positive values")
166 
167         kwargs = self.metric_params
168         if kwargs is None:
169             kwargs = {}
170         self.tree_ = TREE_DICT[algorithm](X, metric=self.metric,
171                                           leaf_size=self.leaf_size,
172                                           sample_weight=sample_weight,
173                                           **kwargs)
174         return self

</code><intended_behavior>Leverage `_check_sample_weight` in the `fit` method for validating `sample_weight`, aligning the method with the standardized validation approach.</intended_behavior>, <file>sklearn/neighbors/_kde.py</file>
<class>KernelDensity</class>
<method>None</method>
<code>
25 class KernelDensity(BaseEstimator):
26     """Kernel Density Estimation.
27 
28     Read more in the :ref:`User Guide <kernel_density>`.
29 
30     Parameters
31     ----------
32     bandwidth : float
33         The bandwidth of the kernel.
34 
35     algorithm : str
36         The tree algorithm to use.  Valid options are
37         ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'.
38 
39     kernel : str
40         The kernel to use.  Valid kernels are
41         ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']
42         Default is 'gaussian'.
43 
44     metric : str
45         The distance metric to use.  Note that not all metrics are
46         valid with all algorithms.  Refer to the documentation of
47         :class:`BallTree` and :class:`KDTree` for a description of
48         available algorithms.  Note that the normalization of the density
49         output is correct only for the Euclidean distance metric. Default
50         is 'euclidean'.
51 
52     atol : float
53         The desired absolute tolerance of the result.  A larger tolerance will
54         generally lead to faster execution. Default is 0.
55 
56     rtol : float
57         The desired relative tolerance of the result.  A larger tolerance will
58         generally lead to faster execution.  Default is 1E-8.
59 
60     breadth_first : bool
61         If true (default), use a breadth-first approach to the problem.
62         Otherwise use a depth-first approach.
63 
64     leaf_size : int
65         Specify the leaf size of the underlying tree.  See :class:`BallTree`
66         or :class:`KDTree` for details.  Default is 40.
67 
68     metric_params : dict
69         Additional parameters to be passed to the tree for use with the
70         metric.  For more information, see the documentation of
71         :class:`BallTree` or :class:`KDTree`.
72 
73     See Also
74     --------
75     sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point
76         problems.
77     sklearn.neighbors.BallTree : Ball tree for fast generalized N-point
78         problems.
79 
80     Examples
81     --------
82     Compute a gaussian kernel density estimate with a fixed bandwidth.
83     >>> import numpy as np
84     >>> rng = np.random.RandomState(42)
85     >>> X = rng.random_sample((100, 3))
86     >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)
87     >>> log_density = kde.score_samples(X[:3])
88     >>> log_density
89     array([-1.52955942, -1.51462041, -1.60244657])
90     """
91     def __init__(self, bandwidth=1.0, algorithm='auto',
92                  kernel='gaussian', metric="euclidean", atol=0, rtol=0,
93                  breadth_first=True, leaf_size=40, metric_params=None):
94         self.algorithm = algorithm
95         self.bandwidth = bandwidth
96         self.kernel = kernel
97         self.metric = metric
98         self.atol = atol
99         self.rtol = rtol
100         self.breadth_first = breadth_first
101         self.leaf_size = leaf_size
102         self.metric_params = metric_params
103 
104         # run the choose algorithm code so that exceptions will happen here
105         # we're using clone() in the GenerativeBayes classifier,
106         # so we can't do this kind of logic in __init__
107         self._choose_algorithm(self.algorithm, self.metric)
108 
109         if bandwidth <= 0:
110             raise ValueError("bandwidth must be positive")
111         if kernel not in VALID_KERNELS:
112             raise ValueError("invalid kernel: '{0}'".format(kernel))
113 
114     def _choose_algorithm(self, algorithm, metric):
115         # given the algorithm string + metric string, choose the optimal
116         # algorithm to compute the result.
117         if algorithm == 'auto':
118             # use KD Tree if possible
119             if metric in KDTree.valid_metrics:
120                 return 'kd_tree'
121             elif metric in BallTree.valid_metrics:
122                 return 'ball_tree'
123             else:
124                 raise ValueError("invalid metric: '{0}'".format(metric))
125         elif algorithm in TREE_DICT:
126             if metric not in TREE_DICT[algorithm].valid_metrics:
127                 raise ValueError("invalid metric for {0}: "
128                                  "'{1}'".format(TREE_DICT[algorithm],
129                                                 metric))
130             return algorithm
131         else:
132             raise ValueError("invalid algorithm: '{0}'".format(algorithm))
133 
134     def fit(self, X, y=None, sample_weight=None):
135         """Fit the Kernel Density model on the data.
136 
137         Parameters
138         ----------
139         X : array_like, shape (n_samples, n_features)
140             List of n_features-dimensional data points.  Each row
141             corresponds to a single data point.
142         y : None
143             Ignored. This parameter exists only for compatibility with
144             :class:`sklearn.pipeline.Pipeline`.
145         sample_weight : array_like, shape (n_samples,), optional
146             List of sample weights attached to the data X.
147 
148         Returns
149         -------
150         self : object
151             Returns instance of object.
152         """
153         algorithm = self._choose_algorithm(self.algorithm, self.metric)
154         X = check_array(X, order='C', dtype=DTYPE)
155 
156         if sample_weight is not None:
157             sample_weight = check_array(sample_weight, order='C', dtype=DTYPE,
158                                         ensure_2d=False)
159             if sample_weight.ndim != 1:
160                 raise ValueError("the shape of sample_weight must be ({0},),"
161                                  " but was {1}".format(X.shape[0],
162                                                        sample_weight.shape))
163             check_consistent_length(X, sample_weight)
164             if sample_weight.min() <= 0:
165                 raise ValueError("sample_weight must have positive values")
166 
167         kwargs = self.metric_params
168         if kwargs is None:
169             kwargs = {}
170         self.tree_ = TREE_DICT[algorithm](X, metric=self.metric,
171                                           leaf_size=self.leaf_size,
172                                           sample_weight=sample_weight,
173                                           **kwargs)
174         return self
175 
176     def score_samples(self, X):
177         """Evaluate the log density model on the data.
178 
179         Parameters
180         ----------
181         X : array_like, shape (n_samples, n_features)
182             An array of points to query.  Last dimension should match dimension
183             of training data (n_features).
184 
185         Returns
186         -------
187         density : ndarray, shape (n_samples,)
188             The array of log(density) evaluations. These are normalized to be
189             probability densities, so values will be low for high-dimensional
190             data.
191         """
192         # The returned density is normalized to the number of points.
193         # For it to be a probability, we must scale it.  For this reason
194         # we'll also scale atol.
195         X = check_array(X, order='C', dtype=DTYPE)
196         if self.tree_.sample_weight is None:
197             N = self.tree_.data.shape[0]
198         else:
199             N = self.tree_.sum_weight
200         atol_N = self.atol * N
201         log_density = self.tree_.kernel_density(
202             X, h=self.bandwidth, kernel=self.kernel, atol=atol_N,
203             rtol=self.rtol, breadth_first=self.breadth_first, return_log=True)
204         log_density -= np.log(N)
205         return log_density
206 
207     def score(self, X, y=None):
208         """Compute the total log probability density under the model.
209 
210         Parameters
211         ----------
212         X : array_like, shape (n_samples, n_features)
213             List of n_features-dimensional data points.  Each row
214             corresponds to a single data point.
215         y : None
216             Ignored. This parameter exists only for compatibility with
217             :class:`sklearn.pipeline.Pipeline`.
218 
219         Returns
220         -------
221         logprob : float
222             Total log-likelihood of the data in X. This is normalized to be a
223             probability density, so the value will be low for high-dimensional
224             data.
225         """
226         return np.sum(self.score_samples(X))
227 
228     def sample(self, n_samples=1, random_state=None):
229         """Generate random samples from the model.
230 
231         Currently, this is implemented only for gaussian and tophat kernels.
232 
233         Parameters
234         ----------
235         n_samples : int, optional
236             Number of samples to generate. Defaults to 1.
237 
238         random_state : int, RandomState instance or None. default to None
239             If int, random_state is the seed used by the random number
240             generator; If RandomState instance, random_state is the random
241             number generator; If None, the random number generator is the
242             RandomState instance used by `np.random`.
243 
244         Returns
245         -------
246         X : array_like, shape (n_samples, n_features)
247             List of samples.
248         """
249         # TODO: implement sampling for other valid kernel shapes
250         if self.kernel not in ['gaussian', 'tophat']:
251             raise NotImplementedError()
252 
253         data = np.asarray(self.tree_.data)
254 
255         rng = check_random_state(random_state)
256         u = rng.uniform(0, 1, size=n_samples)
257         if self.tree_.sample_weight is None:
258             i = (u * data.shape[0]).astype(np.int64)
259         else:
260             cumsum_weight = np.cumsum(np.asarray(self.tree_.sample_weight))
261             sum_weight = cumsum_weight[-1]
262             i = np.searchsorted(cumsum_weight, u * sum_weight)
263         if self.kernel == 'gaussian':
264             return np.atleast_2d(rng.normal(data[i], self.bandwidth))
265 
266         elif self.kernel == 'tophat':
267             # we first draw points from a d-dimensional normal distribution,
268             # then use an incomplete gamma function to map them to a uniform
269             # d-dimensional tophat distribution.
270             dim = data.shape[1]
271             X = rng.normal(size=(n_samples, dim))
272             s_sq = row_norms(X, squared=True)
273             correction = (gammainc(0.5 * dim, 0.5 * s_sq) ** (1. / dim)
274                           * self.bandwidth / np.sqrt(s_sq))
275             return data[i] + X * correction[:, np.newaxis]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/tree/_classes.py</file>
<class>BaseDecisionTree</class>
<method>fit</method>
<code>
129     def fit(self, X, y, sample_weight=None, check_input=True,
130             X_idx_sorted=None):
131 
132         random_state = check_random_state(self.random_state)
133 
134         if self.ccp_alpha < 0.0:
135             raise ValueError("ccp_alpha must be greater than or equal to 0")
136 
137         if check_input:
138             X = check_array(X, dtype=DTYPE, accept_sparse="csc")
139             y = check_array(y, ensure_2d=False, dtype=None)
140             if issparse(X):
141                 X.sort_indices()
142 
143                 if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
144                     raise ValueError("No support for np.int64 index based "
145                                      "sparse matrices")
146 
147         # Determine output settings
148         n_samples, self.n_features_ = X.shape
149         is_classification = is_classifier(self)
150 
151         y = np.atleast_1d(y)
152         expanded_class_weight = None
153 
154         if y.ndim == 1:
155             # reshape is necessary to preserve the data contiguity against vs
156             # [:, np.newaxis] that does not.
157             y = np.reshape(y, (-1, 1))
158 
159         self.n_outputs_ = y.shape[1]
160 
161         if is_classification:
162             check_classification_targets(y)
163             y = np.copy(y)
164 
165             self.classes_ = []
166             self.n_classes_ = []
167 
168             if self.class_weight is not None:
169                 y_original = np.copy(y)
170 
171             y_encoded = np.zeros(y.shape, dtype=np.int)
172             for k in range(self.n_outputs_):
173                 classes_k, y_encoded[:, k] = np.unique(y[:, k],
174                                                        return_inverse=True)
175                 self.classes_.append(classes_k)
176                 self.n_classes_.append(classes_k.shape[0])
177             y = y_encoded
178 
179             if self.class_weight is not None:
180                 expanded_class_weight = compute_sample_weight(
181                     self.class_weight, y_original)
182 
183             self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)
184 
185         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
186             y = np.ascontiguousarray(y, dtype=DOUBLE)
187 
188         # Check parameters
189         max_depth = ((2 ** 31) - 1 if self.max_depth is None
190                      else self.max_depth)
191         max_leaf_nodes = (-1 if self.max_leaf_nodes is None
192                           else self.max_leaf_nodes)
193 
194         if isinstance(self.min_samples_leaf, numbers.Integral):
195             if not 1 <= self.min_samples_leaf:
196                 raise ValueError("min_samples_leaf must be at least 1 "
197                                  "or in (0, 0.5], got %s"
198                                  % self.min_samples_leaf)
199             min_samples_leaf = self.min_samples_leaf
200         else:  # float
201             if not 0. < self.min_samples_leaf <= 0.5:
202                 raise ValueError("min_samples_leaf must be at least 1 "
203                                  "or in (0, 0.5], got %s"
204                                  % self.min_samples_leaf)
205             min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
206 
207         if isinstance(self.min_samples_split, numbers.Integral):
208             if not 2 <= self.min_samples_split:
209                 raise ValueError("min_samples_split must be an integer "
210                                  "greater than 1 or a float in (0.0, 1.0]; "
211                                  "got the integer %s"
212                                  % self.min_samples_split)
213             min_samples_split = self.min_samples_split
214         else:  # float
215             if not 0. < self.min_samples_split <= 1.:
216                 raise ValueError("min_samples_split must be an integer "
217                                  "greater than 1 or a float in (0.0, 1.0]; "
218                                  "got the float %s"
219                                  % self.min_samples_split)
220             min_samples_split = int(ceil(self.min_samples_split * n_samples))
221             min_samples_split = max(2, min_samples_split)
222 
223         min_samples_split = max(min_samples_split, 2 * min_samples_leaf)
224 
225         if isinstance(self.max_features, str):
226             if self.max_features == "auto":
227                 if is_classification:
228                     max_features = max(1, int(np.sqrt(self.n_features_)))
229                 else:
230                     max_features = self.n_features_
231             elif self.max_features == "sqrt":
232                 max_features = max(1, int(np.sqrt(self.n_features_)))
233             elif self.max_features == "log2":
234                 max_features = max(1, int(np.log2(self.n_features_)))
235             else:
236                 raise ValueError(
237                     'Invalid value for max_features. Allowed string '
238                     'values are "auto", "sqrt" or "log2".')
239         elif self.max_features is None:
240             max_features = self.n_features_
241         elif isinstance(self.max_features, numbers.Integral):
242             max_features = self.max_features
243         else:  # float
244             if self.max_features > 0.0:
245                 max_features = max(1,
246                                    int(self.max_features * self.n_features_))
247             else:
248                 max_features = 0
249 
250         self.max_features_ = max_features
251 
252         if len(y) != n_samples:
253             raise ValueError("Number of labels=%d does not match "
254                              "number of samples=%d" % (len(y), n_samples))
255         if not 0 <= self.min_weight_fraction_leaf <= 0.5:
256             raise ValueError("min_weight_fraction_leaf must in [0, 0.5]")
257         if max_depth <= 0:
258             raise ValueError("max_depth must be greater than zero. ")
259         if not (0 < max_features <= self.n_features_):
260             raise ValueError("max_features must be in (0, n_features]")
261         if not isinstance(max_leaf_nodes, numbers.Integral):
262             raise ValueError("max_leaf_nodes must be integral number but was "
263                              "%r" % max_leaf_nodes)
264         if -1 < max_leaf_nodes < 2:
265             raise ValueError(("max_leaf_nodes {0} must be either None "
266                               "or larger than 1").format(max_leaf_nodes))
267 
268         if sample_weight is not None:
269             if (getattr(sample_weight, "dtype", None) != DOUBLE or
270                     not sample_weight.flags.contiguous):
271                 sample_weight = np.ascontiguousarray(
272                     sample_weight, dtype=DOUBLE)
273             if len(sample_weight.shape) > 1:
274                 raise ValueError("Sample weights array has more "
275                                  "than one dimension: %d" %
276                                  len(sample_weight.shape))
277             if len(sample_weight) != n_samples:
278                 raise ValueError("Number of weights=%d does not match "
279                                  "number of samples=%d" %
280                                  (len(sample_weight), n_samples))
281 
282         if expanded_class_weight is not None:
283             if sample_weight is not None:
284                 sample_weight = sample_weight * expanded_class_weight
285             else:
286                 sample_weight = expanded_class_weight
287 
288         # Set min_weight_leaf from min_weight_fraction_leaf
289         if sample_weight is None:
290             min_weight_leaf = (self.min_weight_fraction_leaf *
291                                n_samples)
292         else:
293             min_weight_leaf = (self.min_weight_fraction_leaf *
294                                np.sum(sample_weight))
295 
296         if self.min_impurity_split is not None:
297             warnings.warn("The min_impurity_split parameter is deprecated. "
298                           "Its default value will change from 1e-7 to 0 in "
299                           "version 0.23, and it will be removed in 0.25. "
300                           "Use the min_impurity_decrease parameter instead.",
301                           FutureWarning)
302             min_impurity_split = self.min_impurity_split
303         else:
304             min_impurity_split = 1e-7
305 
306         if min_impurity_split < 0.:
307             raise ValueError("min_impurity_split must be greater than "
308                              "or equal to 0")
309 
310         if self.min_impurity_decrease < 0.:
311             raise ValueError("min_impurity_decrease must be greater than "
312                              "or equal to 0")
313 
314         if self.presort != 'deprecated':
315             warnings.warn("The parameter 'presort' is deprecated and has no "
316                           "effect. It will be removed in v0.24. You can "
317                           "suppress this warning by not passing any value "
318                           "to the 'presort' parameter.",
319                           FutureWarning)
320 
321         # Build tree
322         criterion = self.criterion
323         if not isinstance(criterion, Criterion):
324             if is_classification:
325                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,
326                                                          self.n_classes_)
327             else:
328                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_,
329                                                          n_samples)
330 
331         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS
332 
333         splitter = self.splitter
334         if not isinstance(self.splitter, Splitter):
335             splitter = SPLITTERS[self.splitter](criterion,
336                                                 self.max_features_,
337                                                 min_samples_leaf,
338                                                 min_weight_leaf,
339                                                 random_state)
340 
341         if is_classifier(self):
342             self.tree_ = Tree(self.n_features_,
343                               self.n_classes_, self.n_outputs_)
344         else:
345             self.tree_ = Tree(self.n_features_,
346                               # TODO: tree should't need this in this case
347                               np.array([1] * self.n_outputs_, dtype=np.intp),
348                               self.n_outputs_)
349 
350         # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise
351         if max_leaf_nodes < 0:
352             builder = DepthFirstTreeBuilder(splitter, min_samples_split,
353                                             min_samples_leaf,
354                                             min_weight_leaf,
355                                             max_depth,
356                                             self.min_impurity_decrease,
357                                             min_impurity_split)
358         else:
359             builder = BestFirstTreeBuilder(splitter, min_samples_split,
360                                            min_samples_leaf,
361                                            min_weight_leaf,
362                                            max_depth,
363                                            max_leaf_nodes,
364                                            self.min_impurity_decrease,
365                                            min_impurity_split)
366 
367         builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
368 
369         if self.n_outputs_ == 1 and is_classifier(self):
370             self.n_classes_ = self.n_classes_[0]
371             self.classes_ = self.classes_[0]
372 
373         self._prune_tree()
374 
375         return self

</code><intended_behavior>Update the `fit` method to use `_check_sample_weight` for `sample_weight` validation, standardizing the way sample weights are validated across different parts of the project.</intended_behavior>, <file>sklearn/tree/_classes.py</file>
<class>BaseDecisionTree</class>
<method>None</method>
<code>
76 class BaseDecisionTree(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):
77     """Base class for decision trees.
78 
79     Warning: This class should not be used directly.
80     Use derived classes instead.
81     """
82 
83     @abstractmethod
84     def __init__(self,
85                  criterion,
86                  splitter,
87                  max_depth,
88                  min_samples_split,
89                  min_samples_leaf,
90                  min_weight_fraction_leaf,
91                  max_features,
92                  max_leaf_nodes,
93                  random_state,
94                  min_impurity_decrease,
95                  min_impurity_split,
96                  class_weight=None,
97                  presort='deprecated',
98                  ccp_alpha=0.0):
99         self.criterion = criterion
100         self.splitter = splitter
101         self.max_depth = max_depth
102         self.min_samples_split = min_samples_split
103         self.min_samples_leaf = min_samples_leaf
104         self.min_weight_fraction_leaf = min_weight_fraction_leaf
105         self.max_features = max_features
106         self.random_state = random_state
107         self.max_leaf_nodes = max_leaf_nodes
108         self.min_impurity_decrease = min_impurity_decrease
109         self.min_impurity_split = min_impurity_split
110         self.class_weight = class_weight
111         self.presort = presort
112         self.ccp_alpha = ccp_alpha
113 
114     def get_depth(self):
115         """Returns the depth of the decision tree.
116 
117         The depth of a tree is the maximum distance between the root
118         and any leaf.
119         """
120         check_is_fitted(self)
121         return self.tree_.max_depth
122 
123     def get_n_leaves(self):
124         """Returns the number of leaves of the decision tree.
125         """
126         check_is_fitted(self)
127         return self.tree_.n_leaves
128 
129     def fit(self, X, y, sample_weight=None, check_input=True,
130             X_idx_sorted=None):
131 
132         random_state = check_random_state(self.random_state)
133 
134         if self.ccp_alpha < 0.0:
135             raise ValueError("ccp_alpha must be greater than or equal to 0")
136 
137         if check_input:
138             X = check_array(X, dtype=DTYPE, accept_sparse="csc")
139             y = check_array(y, ensure_2d=False, dtype=None)
140             if issparse(X):
141                 X.sort_indices()
142 
143                 if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
144                     raise ValueError("No support for np.int64 index based "
145                                      "sparse matrices")
146 
147         # Determine output settings
148         n_samples, self.n_features_ = X.shape
149         is_classification = is_classifier(self)
150 
151         y = np.atleast_1d(y)
152         expanded_class_weight = None
153 
154         if y.ndim == 1:
155             # reshape is necessary to preserve the data contiguity against vs
156             # [:, np.newaxis] that does not.
157             y = np.reshape(y, (-1, 1))
158 
159         self.n_outputs_ = y.shape[1]
160 
161         if is_classification:
162             check_classification_targets(y)
163             y = np.copy(y)
164 
165             self.classes_ = []
166             self.n_classes_ = []
167 
168             if self.class_weight is not None:
169                 y_original = np.copy(y)
170 
171             y_encoded = np.zeros(y.shape, dtype=np.int)
172             for k in range(self.n_outputs_):
173                 classes_k, y_encoded[:, k] = np.unique(y[:, k],
174                                                        return_inverse=True)
175                 self.classes_.append(classes_k)
176                 self.n_classes_.append(classes_k.shape[0])
177             y = y_encoded
178 
179             if self.class_weight is not None:
180                 expanded_class_weight = compute_sample_weight(
181                     self.class_weight, y_original)
182 
183             self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)
184 
185         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
186             y = np.ascontiguousarray(y, dtype=DOUBLE)
187 
188         # Check parameters
189         max_depth = ((2 ** 31) - 1 if self.max_depth is None
190                      else self.max_depth)
191         max_leaf_nodes = (-1 if self.max_leaf_nodes is None
192                           else self.max_leaf_nodes)
193 
194         if isinstance(self.min_samples_leaf, numbers.Integral):
195             if not 1 <= self.min_samples_leaf:
196                 raise ValueError("min_samples_leaf must be at least 1 "
197                                  "or in (0, 0.5], got %s"
198                                  % self.min_samples_leaf)
199             min_samples_leaf = self.min_samples_leaf
200         else:  # float
201             if not 0. < self.min_samples_leaf <= 0.5:
202                 raise ValueError("min_samples_leaf must be at least 1 "
203                                  "or in (0, 0.5], got %s"
204                                  % self.min_samples_leaf)
205             min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
206 
207         if isinstance(self.min_samples_split, numbers.Integral):
208             if not 2 <= self.min_samples_split:
209                 raise ValueError("min_samples_split must be an integer "
210                                  "greater than 1 or a float in (0.0, 1.0]; "
211                                  "got the integer %s"
212                                  % self.min_samples_split)
213             min_samples_split = self.min_samples_split
214         else:  # float
215             if not 0. < self.min_samples_split <= 1.:
216                 raise ValueError("min_samples_split must be an integer "
217                                  "greater than 1 or a float in (0.0, 1.0]; "
218                                  "got the float %s"
219                                  % self.min_samples_split)
220             min_samples_split = int(ceil(self.min_samples_split * n_samples))
221             min_samples_split = max(2, min_samples_split)
222 
223         min_samples_split = max(min_samples_split, 2 * min_samples_leaf)
224 
225         if isinstance(self.max_features, str):
226             if self.max_features == "auto":
227                 if is_classification:
228                     max_features = max(1, int(np.sqrt(self.n_features_)))
229                 else:
230                     max_features = self.n_features_
231             elif self.max_features == "sqrt":
232                 max_features = max(1, int(np.sqrt(self.n_features_)))
233             elif self.max_features == "log2":
234                 max_features = max(1, int(np.log2(self.n_features_)))
235             else:
236                 raise ValueError(
237                     'Invalid value for max_features. Allowed string '
238                     'values are "auto", "sqrt" or "log2".')
239         elif self.max_features is None:
240             max_features = self.n_features_
241         elif isinstance(self.max_features, numbers.Integral):
242             max_features = self.max_features
243         else:  # float
244             if self.max_features > 0.0:
245                 max_features = max(1,
246                                    int(self.max_features * self.n_features_))
247             else:
248                 max_features = 0
249 
250         self.max_features_ = max_features
251 
252         if len(y) != n_samples:
253             raise ValueError("Number of labels=%d does not match "
254                              "number of samples=%d" % (len(y), n_samples))
255         if not 0 <= self.min_weight_fraction_leaf <= 0.5:
256             raise ValueError("min_weight_fraction_leaf must in [0, 0.5]")
257         if max_depth <= 0:
258             raise ValueError("max_depth must be greater than zero. ")
259         if not (0 < max_features <= self.n_features_):
260             raise ValueError("max_features must be in (0, n_features]")
261         if not isinstance(max_leaf_nodes, numbers.Integral):
262             raise ValueError("max_leaf_nodes must be integral number but was "
263                              "%r" % max_leaf_nodes)
264         if -1 < max_leaf_nodes < 2:
265             raise ValueError(("max_leaf_nodes {0} must be either None "
266                               "or larger than 1").format(max_leaf_nodes))
267 
268         if sample_weight is not None:
269             if (getattr(sample_weight, "dtype", None) != DOUBLE or
270                     not sample_weight.flags.contiguous):
271                 sample_weight = np.ascontiguousarray(
272                     sample_weight, dtype=DOUBLE)
273             if len(sample_weight.shape) > 1:
274                 raise ValueError("Sample weights array has more "
275                                  "than one dimension: %d" %
276                                  len(sample_weight.shape))
277             if len(sample_weight) != n_samples:
278                 raise ValueError("Number of weights=%d does not match "
279                                  "number of samples=%d" %
280                                  (len(sample_weight), n_samples))
281 
282         if expanded_class_weight is not None:
283             if sample_weight is not None:
284                 sample_weight = sample_weight * expanded_class_weight
285             else:
286                 sample_weight = expanded_class_weight
287 
288         # Set min_weight_leaf from min_weight_fraction_leaf
289         if sample_weight is None:
290             min_weight_leaf = (self.min_weight_fraction_leaf *
291                                n_samples)
292         else:
293             min_weight_leaf = (self.min_weight_fraction_leaf *
294                                np.sum(sample_weight))
295 
296         if self.min_impurity_split is not None:
297             warnings.warn("The min_impurity_split parameter is deprecated. "
298                           "Its default value will change from 1e-7 to 0 in "
299                           "version 0.23, and it will be removed in 0.25. "
300                           "Use the min_impurity_decrease parameter instead.",
301                           FutureWarning)
302             min_impurity_split = self.min_impurity_split
303         else:
304             min_impurity_split = 1e-7
305 
306         if min_impurity_split < 0.:
307             raise ValueError("min_impurity_split must be greater than "
308                              "or equal to 0")
309 
310         if self.min_impurity_decrease < 0.:
311             raise ValueError("min_impurity_decrease must be greater than "
312                              "or equal to 0")
313 
314         if self.presort != 'deprecated':
315             warnings.warn("The parameter 'presort' is deprecated and has no "
316                           "effect. It will be removed in v0.24. You can "
317                           "suppress this warning by not passing any value "
318                           "to the 'presort' parameter.",
319                           FutureWarning)
320 
321         # Build tree
322         criterion = self.criterion
323         if not isinstance(criterion, Criterion):
324             if is_classification:
325                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,
326                                                          self.n_classes_)
327             else:
328                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_,
329                                                          n_samples)
330 
331         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS
332 
333         splitter = self.splitter
334         if not isinstance(self.splitter, Splitter):
335             splitter = SPLITTERS[self.splitter](criterion,
336                                                 self.max_features_,
337                                                 min_samples_leaf,
338                                                 min_weight_leaf,
339                                                 random_state)
340 
341         if is_classifier(self):
342             self.tree_ = Tree(self.n_features_,
343                               self.n_classes_, self.n_outputs_)
344         else:
345             self.tree_ = Tree(self.n_features_,
346                               # TODO: tree should't need this in this case
347                               np.array([1] * self.n_outputs_, dtype=np.intp),
348                               self.n_outputs_)
349 
350         # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise
351         if max_leaf_nodes < 0:
352             builder = DepthFirstTreeBuilder(splitter, min_samples_split,
353                                             min_samples_leaf,
354                                             min_weight_leaf,
355                                             max_depth,
356                                             self.min_impurity_decrease,
357                                             min_impurity_split)
358         else:
359             builder = BestFirstTreeBuilder(splitter, min_samples_split,
360                                            min_samples_leaf,
361                                            min_weight_leaf,
362                                            max_depth,
363                                            max_leaf_nodes,
364                                            self.min_impurity_decrease,
365                                            min_impurity_split)
366 
367         builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
368 
369         if self.n_outputs_ == 1 and is_classifier(self):
370             self.n_classes_ = self.n_classes_[0]
371             self.classes_ = self.classes_[0]
372 
373         self._prune_tree()
374 
375         return self
376 
377     def _validate_X_predict(self, X, check_input):
378         """Validate X whenever one tries to predict, apply, predict_proba"""
379         if check_input:
380             X = check_array(X, dtype=DTYPE, accept_sparse="csr")
381             if issparse(X) and (X.indices.dtype != np.intc or
382                                 X.indptr.dtype != np.intc):
383                 raise ValueError("No support for np.int64 index based "
384                                  "sparse matrices")
385 
386         n_features = X.shape[1]
387         if self.n_features_ != n_features:
388             raise ValueError("Number of features of the model must "
389                              "match the input. Model n_features is %s and "
390                              "input n_features is %s "
391                              % (self.n_features_, n_features))
392 
393         return X
394 
395     def predict(self, X, check_input=True):
396         """Predict class or regression value for X.
397 
398         For a classification model, the predicted class for each sample in X is
399         returned. For a regression model, the predicted value based on X is
400         returned.
401 
402         Parameters
403         ----------
404         X : array-like or sparse matrix of shape (n_samples, n_features)
405             The input samples. Internally, it will be converted to
406             ``dtype=np.float32`` and if a sparse matrix is provided
407             to a sparse ``csr_matrix``.
408 
409         check_input : boolean, (default=True)
410             Allow to bypass several input checking.
411             Don't use this parameter unless you know what you do.
412 
413         Returns
414         -------
415         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
416             The predicted classes, or the predict values.
417         """
418         check_is_fitted(self)
419         X = self._validate_X_predict(X, check_input)
420         proba = self.tree_.predict(X)
421         n_samples = X.shape[0]
422 
423         # Classification
424         if is_classifier(self):
425             if self.n_outputs_ == 1:
426                 return self.classes_.take(np.argmax(proba, axis=1), axis=0)
427 
428             else:
429                 class_type = self.classes_[0].dtype
430                 predictions = np.zeros((n_samples, self.n_outputs_),
431                                        dtype=class_type)
432                 for k in range(self.n_outputs_):
433                     predictions[:, k] = self.classes_[k].take(
434                         np.argmax(proba[:, k], axis=1),
435                         axis=0)
436 
437                 return predictions
438 
439         # Regression
440         else:
441             if self.n_outputs_ == 1:
442                 return proba[:, 0]
443 
444             else:
445                 return proba[:, :, 0]
446 
447     def apply(self, X, check_input=True):
448         """
449         Returns the index of the leaf that each sample is predicted as.
450 
451         .. versionadded:: 0.17
452 
453         Parameters
454         ----------
455         X : {array-like, sparse matrix} of shape (n_samples, n_features)
456             The input samples. Internally, it will be converted to
457             ``dtype=np.float32`` and if a sparse matrix is provided
458             to a sparse ``csr_matrix``.
459 
460         check_input : boolean, (default=True)
461             Allow to bypass several input checking.
462             Don't use this parameter unless you know what you do.
463 
464         Returns
465         -------
466         X_leaves : array_like, shape = [n_samples,]
467             For each datapoint x in X, return the index of the leaf x
468             ends up in. Leaves are numbered within
469             ``[0; self.tree_.node_count)``, possibly with gaps in the
470             numbering.
471         """
472         check_is_fitted(self)
473         X = self._validate_X_predict(X, check_input)
474         return self.tree_.apply(X)
475 
476     def decision_path(self, X, check_input=True):
477         """Return the decision path in the tree
478 
479         .. versionadded:: 0.18
480 
481         Parameters
482         ----------
483         X : {array-like, sparse matrix} of shape (n_samples, n_features)
484             The input samples. Internally, it will be converted to
485             ``dtype=np.float32`` and if a sparse matrix is provided
486             to a sparse ``csr_matrix``.
487 
488         check_input : boolean, (default=True)
489             Allow to bypass several input checking.
490             Don't use this parameter unless you know what you do.
491 
492         Returns
493         -------
494         indicator : sparse csr array, shape = [n_samples, n_nodes]
495             Return a node indicator matrix where non zero elements
496             indicates that the samples goes through the nodes.
497 
498         """
499         X = self._validate_X_predict(X, check_input)
500         return self.tree_.decision_path(X)
501 
502     def _prune_tree(self):
503         """Prune tree using Minimal Cost-Complexity Pruning."""
504         check_is_fitted(self)
505 
506         if self.ccp_alpha < 0.0:
507             raise ValueError("ccp_alpha must be greater than or equal to 0")
508 
509         if self.ccp_alpha == 0.0:
510             return
511 
512         # build pruned tree
513         if is_classifier(self):
514             n_classes = np.atleast_1d(self.n_classes_)
515             pruned_tree = Tree(self.n_features_, n_classes, self.n_outputs_)
516         else:
517             pruned_tree = Tree(self.n_features_,
518                                # TODO: the tree shouldn't need this param
519                                np.array([1] * self.n_outputs_, dtype=np.intp),
520                                self.n_outputs_)
521         _build_pruned_tree_ccp(pruned_tree, self.tree_, self.ccp_alpha)
522 
523         self.tree_ = pruned_tree
524 
525     def cost_complexity_pruning_path(self, X, y, sample_weight=None):
526         """Compute the pruning path during Minimal Cost-Complexity Pruning.
527 
528         See :ref:`minimal_cost_complexity_pruning` for details on the pruning
529         process.
530 
531         Parameters
532         ----------
533         X : {array-like, sparse matrix} of shape (n_samples, n_features)
534             The training input samples. Internally, it will be converted to
535             ``dtype=np.float32`` and if a sparse matrix is provided
536             to a sparse ``csc_matrix``.
537 
538         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
539             The target values (class labels) as integers or strings.
540 
541         sample_weight : array-like of shape (n_samples,), default=None
542             Sample weights. If None, then samples are equally weighted. Splits
543             that would create child nodes with net zero or negative weight are
544             ignored while searching for a split in each node. Splits are also
545             ignored if they would result in any single class carrying a
546             negative weight in either child node.
547 
548         Returns
549         -------
550         ccp_path : Bunch
551             Dictionary-like object, with attributes:
552 
553             ccp_alphas : ndarray
554                 Effective alphas of subtree during pruning.
555 
556             impurities : ndarray
557                 Sum of the impurities of the subtree leaves for the
558                 corresponding alpha value in ``ccp_alphas``.
559         """
560         est = clone(self).set_params(ccp_alpha=0.0)
561         est.fit(X, y, sample_weight=sample_weight)
562         return Bunch(**ccp_pruning_path(est.tree_))
563 
564     @property
565     def feature_importances_(self):
566         """Return the feature importances.
567 
568         The importance of a feature is computed as the (normalized) total
569         reduction of the criterion brought by that feature.
570         It is also known as the Gini importance.
571 
572         Returns
573         -------
574         feature_importances_ : array, shape = [n_features]
575         """
576         check_is_fitted(self)
577 
578         return self.tree_.compute_feature_importances()

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 23:56:21.988 | INFO     | Search completed. Bug locations: [<file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>fit</method>
<code>
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self

</code><intended_behavior>Within the `fit` method, replace or supplement any custom validation of `sample_weight` with a standardized call to `_check_sample_weight`, ensuring that `sample_weight` is properly validated without altering the method's core functionality.</intended_behavior>, <file>sklearn/calibration.py</file>
<class>CalibratedClassifierCV</class>
<method>None</method>
<code>
31 class CalibratedClassifierCV(BaseEstimator, ClassifierMixin,
32                              MetaEstimatorMixin):
33     """Probability calibration with isotonic regression or sigmoid.
34 
35     See glossary entry for :term:`cross-validation estimator`.
36 
37     With this class, the base_estimator is fit on the train set of the
38     cross-validation generator and the test set is used for calibration.
39     The probabilities for each of the folds are then averaged
40     for prediction. In case that cv="prefit" is passed to __init__,
41     it is assumed that base_estimator has been fitted already and all
42     data is used for calibration. Note that data for fitting the
43     classifier and for calibrating it must be disjoint.
44 
45     Read more in the :ref:`User Guide <calibration>`.
46 
47     Parameters
48     ----------
49     base_estimator : instance BaseEstimator
50         The classifier whose output decision function needs to be calibrated
51         to offer more accurate predict_proba outputs. If cv=prefit, the
52         classifier must have been fit already on data.
53 
54     method : 'sigmoid' or 'isotonic'
55         The method to use for calibration. Can be 'sigmoid' which
56         corresponds to Platt's method or 'isotonic' which is a
57         non-parametric approach. It is not advised to use isotonic calibration
58         with too few calibration samples ``(<<1000)`` since it tends to
59         overfit.
60         Use sigmoids (Platt's calibration) in this case.
61 
62     cv : integer, cross-validation generator, iterable or "prefit", optional
63         Determines the cross-validation splitting strategy.
64         Possible inputs for cv are:
65 
66         - None, to use the default 5-fold cross-validation,
67         - integer, to specify the number of folds.
68         - :term:`CV splitter`,
69         - An iterable yielding (train, test) splits as arrays of indices.
70 
71         For integer/None inputs, if ``y`` is binary or multiclass,
72         :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
73         neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
74         is used.
75 
76         Refer :ref:`User Guide <cross_validation>` for the various
77         cross-validation strategies that can be used here.
78 
79         If "prefit" is passed, it is assumed that base_estimator has been
80         fitted already and all data is used for calibration.
81 
82         .. versionchanged:: 0.22
83             ``cv`` default value if None changed from 3-fold to 5-fold.
84 
85     Attributes
86     ----------
87     classes_ : array, shape (n_classes)
88         The class labels.
89 
90     calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
91         The list of calibrated classifiers, one for each crossvalidation fold,
92         which has been fitted on all but the validation fold and calibrated
93         on the validation fold.
94 
95     References
96     ----------
97     .. [1] Obtaining calibrated probability estimates from decision trees
98            and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
99 
100     .. [2] Transforming Classifier Scores into Accurate Multiclass
101            Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
102 
103     .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
104            Regularized Likelihood Methods, J. Platt, (1999)
105 
106     .. [4] Predicting Good Probabilities with Supervised Learning,
107            A. Niculescu-Mizil & R. Caruana, ICML 2005
108     """
109     def __init__(self, base_estimator=None, method='sigmoid', cv=None):
110         self.base_estimator = base_estimator
111         self.method = method
112         self.cv = cv
113 
114     def fit(self, X, y, sample_weight=None):
115         """Fit the calibrated model
116 
117         Parameters
118         ----------
119         X : array-like, shape (n_samples, n_features)
120             Training data.
121 
122         y : array-like, shape (n_samples,)
123             Target values.
124 
125         sample_weight : array-like of shape (n_samples,), default=None
126             Sample weights. If None, then samples are equally weighted.
127 
128         Returns
129         -------
130         self : object
131             Returns an instance of self.
132         """
133         X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
134                          force_all_finite=False, allow_nd=True)
135         X, y = indexable(X, y)
136         le = LabelBinarizer().fit(y)
137         self.classes_ = le.classes_
138 
139         # Check that each cross-validation fold can have at least one
140         # example per class
141         n_folds = self.cv if isinstance(self.cv, int) \
142             else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
143         if n_folds and \
144                 np.any([np.sum(y == class_) < n_folds for class_ in
145                         self.classes_]):
146             raise ValueError("Requesting %d-fold cross-validation but provided"
147                              " less than %d examples for at least one class."
148                              % (n_folds, n_folds))
149 
150         self.calibrated_classifiers_ = []
151         if self.base_estimator is None:
152             # we want all classifiers that don't expose a random_state
153             # to be deterministic (and we don't want to expose this one).
154             base_estimator = LinearSVC(random_state=0)
155         else:
156             base_estimator = self.base_estimator
157 
158         if self.cv == "prefit":
159             calibrated_classifier = _CalibratedClassifier(
160                 base_estimator, method=self.method)
161             if sample_weight is not None:
162                 calibrated_classifier.fit(X, y, sample_weight)
163             else:
164                 calibrated_classifier.fit(X, y)
165             self.calibrated_classifiers_.append(calibrated_classifier)
166         else:
167             cv = check_cv(self.cv, y, classifier=True)
168             fit_parameters = signature(base_estimator.fit).parameters
169             estimator_name = type(base_estimator).__name__
170             if (sample_weight is not None
171                     and "sample_weight" not in fit_parameters):
172                 warnings.warn("%s does not support sample_weight. Samples"
173                               " weights are only used for the calibration"
174                               " itself." % estimator_name)
175                 sample_weight = check_array(sample_weight, ensure_2d=False)
176                 base_estimator_sample_weight = None
177             else:
178                 if sample_weight is not None:
179                     sample_weight = check_array(sample_weight, ensure_2d=False)
180                     check_consistent_length(y, sample_weight)
181                 base_estimator_sample_weight = sample_weight
182             for train, test in cv.split(X, y):
183                 this_estimator = clone(base_estimator)
184                 if base_estimator_sample_weight is not None:
185                     this_estimator.fit(
186                         X[train], y[train],
187                         sample_weight=base_estimator_sample_weight[train])
188                 else:
189                     this_estimator.fit(X[train], y[train])
190 
191                 calibrated_classifier = _CalibratedClassifier(
192                     this_estimator, method=self.method,
193                     classes=self.classes_)
194                 if sample_weight is not None:
195                     calibrated_classifier.fit(X[test], y[test],
196                                               sample_weight[test])
197                 else:
198                     calibrated_classifier.fit(X[test], y[test])
199                 self.calibrated_classifiers_.append(calibrated_classifier)
200 
201         return self
202 
203     def predict_proba(self, X):
204         """Posterior probabilities of classification
205 
206         This function returns posterior probabilities of classification
207         according to each class on an array of test vectors X.
208 
209         Parameters
210         ----------
211         X : array-like, shape (n_samples, n_features)
212             The samples.
213 
214         Returns
215         -------
216         C : array, shape (n_samples, n_classes)
217             The predicted probas.
218         """
219         check_is_fitted(self)
220         X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
221                         force_all_finite=False)
222         # Compute the arithmetic mean of the predictions of the calibrated
223         # classifiers
224         mean_proba = np.zeros((X.shape[0], len(self.classes_)))
225         for calibrated_classifier in self.calibrated_classifiers_:
226             proba = calibrated_classifier.predict_proba(X)
227             mean_proba += proba
228 
229         mean_proba /= len(self.calibrated_classifiers_)
230 
231         return mean_proba
232 
233     def predict(self, X):
234         """Predict the target of new samples. Can be different from the
235         prediction of the uncalibrated classifier.
236 
237         Parameters
238         ----------
239         X : array-like, shape (n_samples, n_features)
240             The samples.
241 
242         Returns
243         -------
244         C : array, shape (n_samples,)
245             The predicted class.
246         """
247         check_is_fitted(self)
248         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>The `fit` method should use `_check_sample_weight` to validate `sample_weight`, replacing any bespoke validation logic to ensure consistency in handling sample weights across the project.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>fit</method>
<code>
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self

</code><intended_behavior>Use `_check_sample_weight` in the `fit` method for validating `sample_weight`, aiming to standardize the way sample weights are checked across different estimators.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyClassifier</class>
<method>None</method>
<code>
23 class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
24     """
25     DummyClassifier is a classifier that makes predictions using simple rules.
26 
27     This classifier is useful as a simple baseline to compare with other
28     (real) classifiers. Do not use it for real problems.
29 
30     Read more in the :ref:`User Guide <dummy_estimators>`.
31 
32     Parameters
33     ----------
34     strategy : str, default="stratified"
35         Strategy to use to generate predictions.
36 
37         * "stratified": generates predictions by respecting the training
38           set's class distribution.
39         * "most_frequent": always predicts the most frequent label in the
40           training set.
41         * "prior": always predicts the class that maximizes the class prior
42           (like "most_frequent") and ``predict_proba`` returns the class prior.
43         * "uniform": generates predictions uniformly at random.
44         * "constant": always predicts a constant label that is provided by
45           the user. This is useful for metrics that evaluate a non-majority
46           class
47 
48           .. versionadded:: 0.17
49              Dummy Classifier now supports prior fitting strategy using
50              parameter *prior*.
51 
52     random_state : int, RandomState instance or None, optional, default=None
53         If int, random_state is the seed used by the random number generator;
54         If RandomState instance, random_state is the random number generator;
55         If None, the random number generator is the RandomState instance used
56         by `np.random`.
57 
58     constant : int or str or array-like of shape (n_outputs,)
59         The explicit constant as predicted by the "constant" strategy. This
60         parameter is useful only for the "constant" strategy.
61 
62     Attributes
63     ----------
64     classes_ : array or list of array of shape (n_classes,)
65         Class labels for each output.
66 
67     n_classes_ : array or list of array of shape (n_classes,)
68         Number of label for each output.
69 
70     class_prior_ : array or list of array of shape (n_classes,)
71         Probability of each class for each output.
72 
73     n_outputs_ : int,
74         Number of outputs.
75 
76     sparse_output_ : bool,
77         True if the array returned from predict is to be in sparse CSC format.
78         Is automatically set to True if the input y is passed in sparse format.
79 
80     Examples
81     --------
82     >>> import numpy as np
83     >>> from sklearn.dummy import DummyClassifier
84     >>> X = np.array([-1, 1, 1, 1])
85     >>> y = np.array([0, 1, 1, 1])
86     >>> dummy_clf = DummyClassifier(strategy="most_frequent")
87     >>> dummy_clf.fit(X, y)
88     DummyClassifier(strategy='most_frequent')
89     >>> dummy_clf.predict(X)
90     array([1, 1, 1, 1])
91     >>> dummy_clf.score(X, y)
92     0.75
93     """
94 
95     def __init__(self, strategy="stratified", random_state=None,
96                  constant=None):
97         self.strategy = strategy
98         self.random_state = random_state
99         self.constant = constant
100 
101     def fit(self, X, y, sample_weight=None):
102         """Fit the random classifier.
103 
104         Parameters
105         ----------
106         X : {array-like, object with finite length or shape}
107             Training data, requires length = n_samples
108 
109         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
110             Target values.
111 
112         sample_weight : array-like of shape (n_samples,), default=None
113             Sample weights.
114 
115         Returns
116         -------
117         self : object
118         """
119         allowed_strategies = ("most_frequent", "stratified", "uniform",
120                               "constant", "prior")
121         if self.strategy not in allowed_strategies:
122             raise ValueError("Unknown strategy type: %s, expected one of %s."
123                              % (self.strategy, allowed_strategies))
124 
125         if self.strategy == "uniform" and sp.issparse(y):
126             y = y.toarray()
127             warnings.warn('A local copy of the target data has been converted '
128                           'to a numpy array. Predicting on sparse target data '
129                           'with the uniform strategy would not save memory '
130                           'and would be slower.',
131                           UserWarning)
132 
133         self.sparse_output_ = sp.issparse(y)
134 
135         if not self.sparse_output_:
136             y = np.asarray(y)
137             y = np.atleast_1d(y)
138 
139         if y.ndim == 1:
140             y = np.reshape(y, (-1, 1))
141 
142         self.n_outputs_ = y.shape[1]
143 
144         check_consistent_length(X, y, sample_weight)
145 
146         if self.strategy == "constant":
147             if self.constant is None:
148                 raise ValueError("Constant target value has to be specified "
149                                  "when the constant strategy is used.")
150             else:
151                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
152                 if constant.shape[0] != self.n_outputs_:
153                     raise ValueError("Constant target value should have "
154                                      "shape (%d, 1)." % self.n_outputs_)
155 
156         (self.classes_,
157          self.n_classes_,
158          self.class_prior_) = class_distribution(y, sample_weight)
159 
160         if self.strategy == "constant":
161             for k in range(self.n_outputs_):
162                 if not any(constant[k][0] == c for c in self.classes_[k]):
163                     # Checking in case of constant strategy if the constant
164                     # provided by the user is in y.
165                     err_msg = ("The constant target value must be present in "
166                                "the training data. You provided constant={}. "
167                                "Possible values are: {}."
168                                .format(self.constant, list(self.classes_[k])))
169                     raise ValueError(err_msg)
170 
171         if self.n_outputs_ == 1:
172             self.n_classes_ = self.n_classes_[0]
173             self.classes_ = self.classes_[0]
174             self.class_prior_ = self.class_prior_[0]
175 
176         return self
177 
178     def predict(self, X):
179         """Perform classification on test vectors X.
180 
181         Parameters
182         ----------
183         X : {array-like, object with finite length or shape}
184             Training data, requires length = n_samples
185 
186         Returns
187         -------
188         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
189             Predicted target values for X.
190         """
191         check_is_fitted(self)
192 
193         # numpy random_state expects Python int and not long as size argument
194         # under Windows
195         n_samples = _num_samples(X)
196         rs = check_random_state(self.random_state)
197 
198         n_classes_ = self.n_classes_
199         classes_ = self.classes_
200         class_prior_ = self.class_prior_
201         constant = self.constant
202         if self.n_outputs_ == 1:
203             # Get same type even for self.n_outputs_ == 1
204             n_classes_ = [n_classes_]
205             classes_ = [classes_]
206             class_prior_ = [class_prior_]
207             constant = [constant]
208         # Compute probability only once
209         if self.strategy == "stratified":
210             proba = self.predict_proba(X)
211             if self.n_outputs_ == 1:
212                 proba = [proba]
213 
214         if self.sparse_output_:
215             class_prob = None
216             if self.strategy in ("most_frequent", "prior"):
217                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]
218 
219             elif self.strategy == "stratified":
220                 class_prob = class_prior_
221 
222             elif self.strategy == "uniform":
223                 raise ValueError("Sparse target prediction is not "
224                                  "supported with the uniform strategy")
225 
226             elif self.strategy == "constant":
227                 classes_ = [np.array([c]) for c in constant]
228 
229             y = _random_choice_csc(n_samples, classes_, class_prob,
230                                   self.random_state)
231         else:
232             if self.strategy in ("most_frequent", "prior"):
233                 y = np.tile([classes_[k][class_prior_[k].argmax()] for
234                              k in range(self.n_outputs_)], [n_samples, 1])
235 
236             elif self.strategy == "stratified":
237                 y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
238                                k in range(self.n_outputs_)]).T
239 
240             elif self.strategy == "uniform":
241                 ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
242                        for k in range(self.n_outputs_)]
243                 y = np.vstack(ret).T
244 
245             elif self.strategy == "constant":
246                 y = np.tile(self.constant, (n_samples, 1))
247 
248             if self.n_outputs_ == 1:
249                 y = np.ravel(y)
250 
251         return y
252 
253     def predict_proba(self, X):
254         """
255         Return probability estimates for the test vectors X.
256 
257         Parameters
258         ----------
259         X : {array-like, object with finite length or shape}
260             Training data, requires length = n_samples
261 
262         Returns
263         -------
264         P : array-like or list of array-lke of shape (n_samples, n_classes)
265             Returns the probability of the sample for each class in
266             the model, where classes are ordered arithmetically, for each
267             output.
268         """
269         check_is_fitted(self)
270 
271         # numpy random_state expects Python int and not long as size argument
272         # under Windows
273         n_samples = _num_samples(X)
274         rs = check_random_state(self.random_state)
275 
276         n_classes_ = self.n_classes_
277         classes_ = self.classes_
278         class_prior_ = self.class_prior_
279         constant = self.constant
280         if self.n_outputs_ == 1:
281             # Get same type even for self.n_outputs_ == 1
282             n_classes_ = [n_classes_]
283             classes_ = [classes_]
284             class_prior_ = [class_prior_]
285             constant = [constant]
286 
287         P = []
288         for k in range(self.n_outputs_):
289             if self.strategy == "most_frequent":
290                 ind = class_prior_[k].argmax()
291                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
292                 out[:, ind] = 1.0
293             elif self.strategy == "prior":
294                 out = np.ones((n_samples, 1)) * class_prior_[k]
295 
296             elif self.strategy == "stratified":
297                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
298                 out = out.astype(np.float64)
299 
300             elif self.strategy == "uniform":
301                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
302                 out /= n_classes_[k]
303 
304             elif self.strategy == "constant":
305                 ind = np.where(classes_[k] == constant[k])
306                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
307                 out[:, ind] = 1.0
308 
309             P.append(out)
310 
311         if self.n_outputs_ == 1:
312             P = P[0]
313 
314         return P
315 
316     def predict_log_proba(self, X):
317         """
318         Return log probability estimates for the test vectors X.
319 
320         Parameters
321         ----------
322         X : {array-like, object with finite length or shape}
323             Training data, requires length = n_samples
324 
325         Returns
326         -------
327         P : array-like or list of array-like of shape (n_samples, n_classes)
328             Returns the log probability of the sample for each class in
329             the model, where classes are ordered arithmetically for each
330             output.
331         """
332         proba = self.predict_proba(X)
333         if self.n_outputs_ == 1:
334             return np.log(proba)
335         else:
336             return [np.log(p) for p in proba]
337 
338     def _more_tags(self):
339         return {'poor_score': True, 'no_validation': True}
340 
341     def score(self, X, y, sample_weight=None):
342         """Returns the mean accuracy on the given test data and labels.
343 
344         In multi-label classification, this is the subset accuracy
345         which is a harsh metric since you require for each sample that
346         each label set be correctly predicted.
347 
348         Parameters
349         ----------
350         X : {array-like, None}
351             Test samples with shape = (n_samples, n_features) or
352             None. Passing None as test samples gives the same result
353             as passing real test samples, since DummyClassifier
354             operates independently of the sampled observations.
355 
356         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
357             True labels for X.
358 
359         sample_weight : array-like of shape (n_samples,), default=None
360             Sample weights.
361 
362         Returns
363         -------
364         score : float
365             Mean accuracy of self.predict(X) wrt. y.
366 
367         """
368         if X is None:
369             X = np.zeros(shape=(len(y), 1))
370         return super().score(X, y, sample_weight)
371 
372     @deprecated(
373         "The outputs_2d_ attribute is deprecated in version 0.22 "
374         "and will be removed in version 0.24. It is equivalent to "
375         "n_outputs_ > 1."
376     )
377     @property
378     def outputs_2d_(self):
379         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>fit</method>
<code>
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self

</code><intended_behavior>The `fit` method should incorporate `_check_sample_weight` for `sample_weight` validation to align with the standardized approach of handling sample weights in the project.</intended_behavior>, <file>sklearn/dummy.py</file>
<class>DummyRegressor</class>
<method>None</method>
<code>
382 class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):
383     """
384     DummyRegressor is a regressor that makes predictions using
385     simple rules.
386 
387     This regressor is useful as a simple baseline to compare with other
388     (real) regressors. Do not use it for real problems.
389 
390     Read more in the :ref:`User Guide <dummy_estimators>`.
391 
392     Parameters
393     ----------
394     strategy : str
395         Strategy to use to generate predictions.
396 
397         * "mean": always predicts the mean of the training set
398         * "median": always predicts the median of the training set
399         * "quantile": always predicts a specified quantile of the training set,
400           provided with the quantile parameter.
401         * "constant": always predicts a constant value that is provided by
402           the user.
403 
404     constant : int or float or array-like of shape (n_outputs,)
405         The explicit constant as predicted by the "constant" strategy. This
406         parameter is useful only for the "constant" strategy.
407 
408     quantile : float in [0.0, 1.0]
409         The quantile to predict using the "quantile" strategy. A quantile of
410         0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the
411         maximum.
412 
413     Attributes
414     ----------
415     constant_ : array, shape (1, n_outputs)
416         Mean or median or quantile of the training targets or constant value
417         given by the user.
418 
419     n_outputs_ : int,
420         Number of outputs.
421 
422     Examples
423     --------
424     >>> import numpy as np
425     >>> from sklearn.dummy import DummyRegressor
426     >>> X = np.array([1.0, 2.0, 3.0, 4.0])
427     >>> y = np.array([2.0, 3.0, 5.0, 10.0])
428     >>> dummy_regr = DummyRegressor(strategy="mean")
429     >>> dummy_regr.fit(X, y)
430     DummyRegressor()
431     >>> dummy_regr.predict(X)
432     array([5., 5., 5., 5.])
433     >>> dummy_regr.score(X, y)
434     0.0
435     """
436 
437     def __init__(self, strategy="mean", constant=None, quantile=None):
438         self.strategy = strategy
439         self.constant = constant
440         self.quantile = quantile
441 
442     def fit(self, X, y, sample_weight=None):
443         """Fit the random regressor.
444 
445         Parameters
446         ----------
447         X : {array-like, object with finite length or shape}
448             Training data, requires length = n_samples
449 
450         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
451             Target values.
452 
453         sample_weight : array-like of shape (n_samples,), default=None
454             Sample weights.
455 
456         Returns
457         -------
458         self : object
459         """
460         allowed_strategies = ("mean", "median", "quantile", "constant")
461         if self.strategy not in allowed_strategies:
462             raise ValueError("Unknown strategy type: %s, expected one of %s."
463                              % (self.strategy, allowed_strategies))
464 
465         y = check_array(y, ensure_2d=False)
466         if len(y) == 0:
467             raise ValueError("y must not be empty.")
468 
469         if y.ndim == 1:
470             y = np.reshape(y, (-1, 1))
471         self.n_outputs_ = y.shape[1]
472 
473         check_consistent_length(X, y, sample_weight)
474         if sample_weight is not None:
475             sample_weight = np.asarray(sample_weight)
476 
477         if self.strategy == "mean":
478             self.constant_ = np.average(y, axis=0, weights=sample_weight)
479 
480         elif self.strategy == "median":
481             if sample_weight is None:
482                 self.constant_ = np.median(y, axis=0)
483             else:
484                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
485                                                        percentile=50.)
486                                   for k in range(self.n_outputs_)]
487 
488         elif self.strategy == "quantile":
489             if self.quantile is None or not np.isscalar(self.quantile):
490                 raise ValueError("Quantile must be a scalar in the range "
491                                  "[0.0, 1.0], but got %s." % self.quantile)
492 
493             percentile = self.quantile * 100.0
494             if sample_weight is None:
495                 self.constant_ = np.percentile(y, axis=0, q=percentile)
496             else:
497                 self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
498                                                        percentile=percentile)
499                                   for k in range(self.n_outputs_)]
500 
501         elif self.strategy == "constant":
502             if self.constant is None:
503                 raise TypeError("Constant target value has to be specified "
504                                 "when the constant strategy is used.")
505 
506             self.constant = check_array(self.constant,
507                                         accept_sparse=['csr', 'csc', 'coo'],
508                                         ensure_2d=False, ensure_min_samples=0)
509 
510             if self.n_outputs_ != 1 and self.constant.shape[0] != y.shape[1]:
511                 raise ValueError(
512                     "Constant target value should have "
513                     "shape (%d, 1)." % y.shape[1])
514 
515             self.constant_ = self.constant
516 
517         self.constant_ = np.reshape(self.constant_, (1, -1))
518         return self
519 
520     def predict(self, X, return_std=False):
521         """
522         Perform classification on test vectors X.
523 
524         Parameters
525         ----------
526         X : {array-like, object with finite length or shape}
527             Training data, requires length = n_samples
528 
529         return_std : boolean, optional
530             Whether to return the standard deviation of posterior prediction.
531             All zeros in this case.
532 
533         Returns
534         -------
535         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
536             Predicted target values for X.
537 
538         y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)
539             Standard deviation of predictive distribution of query points.
540         """
541         check_is_fitted(self)
542         n_samples = _num_samples(X)
543 
544         y = np.full((n_samples, self.n_outputs_), self.constant_,
545                     dtype=np.array(self.constant_).dtype)
546         y_std = np.zeros((n_samples, self.n_outputs_))
547 
548         if self.n_outputs_ == 1:
549             y = np.ravel(y)
550             y_std = np.ravel(y_std)
551 
552         return (y, y_std) if return_std else y
553 
554     def _more_tags(self):
555         return {'poor_score': True, 'no_validation': True}
556 
557     def score(self, X, y, sample_weight=None):
558         """Returns the coefficient of determination R^2 of the prediction.
559 
560         The coefficient R^2 is defined as (1 - u/v), where u is the residual
561         sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
562         sum of squares ((y_true - y_true.mean()) ** 2).sum().
563         The best possible score is 1.0 and it can be negative (because the
564         model can be arbitrarily worse). A constant model that always
565         predicts the expected value of y, disregarding the input features,
566         would get a R^2 score of 0.0.
567 
568         Parameters
569         ----------
570         X : {array-like, None}
571             Test samples with shape = (n_samples, n_features) or None.
572             For some estimators this may be a
573             precomputed kernel matrix instead, shape = (n_samples,
574             n_samples_fitted], where n_samples_fitted is the number of
575             samples used in the fitting for the estimator.
576             Passing None as test samples gives the same result
577             as passing real test samples, since DummyRegressor
578             operates independently of the sampled observations.
579 
580         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
581             True values for X.
582 
583         sample_weight : array-like of shape (n_samples,), default=None
584             Sample weights.
585 
586         Returns
587         -------
588         score : float
589             R^2 of self.predict(X) wrt. y.
590         """
591         if X is None:
592             X = np.zeros(shape=(len(y), 1))
593         return super().score(X, y, sample_weight)
594 
595     @deprecated(
596         "The outputs_2d_ attribute is deprecated in version 0.22 "
597         "and will be removed in version 0.24. It is equivalent to "
598         "n_outputs_ > 1."
599     )
600     @property
601     def outputs_2d_(self):
602         return self.n_outputs_ != 1

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>fit</method>
<code>
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)

</code><intended_behavior>Employ `_check_sample_weight` in the `fit` method for consistent `sample_weight` validation, ensuring that it follows the newly established project-wide standard.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>None</method>
<code>
185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):
186     """Base class for Bagging meta-estimator.
187 
188     Warning: This class should not be used directly. Use derived classes
189     instead.
190     """
191 
192     @abstractmethod
193     def __init__(self,
194                  base_estimator=None,
195                  n_estimators=10,
196                  max_samples=1.0,
197                  max_features=1.0,
198                  bootstrap=True,
199                  bootstrap_features=False,
200                  oob_score=False,
201                  warm_start=False,
202                  n_jobs=None,
203                  random_state=None,
204                  verbose=0):
205         super().__init__(
206             base_estimator=base_estimator,
207             n_estimators=n_estimators)
208 
209         self.max_samples = max_samples
210         self.max_features = max_features
211         self.bootstrap = bootstrap
212         self.bootstrap_features = bootstrap_features
213         self.oob_score = oob_score
214         self.warm_start = warm_start
215         self.n_jobs = n_jobs
216         self.random_state = random_state
217         self.verbose = verbose
218 
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
243 
244     def _parallel_args(self):
245         return {}
246 
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self
392 
393     @abstractmethod
394     def _set_oob_score(self, X, y):
395         """Calculate out of bag predictions and score."""
396 
397     def _validate_y(self, y):
398         if len(y.shape) == 1 or y.shape[1] == 1:
399             return column_or_1d(y, warn=True)
400         else:
401             return y
402 
403     def _get_estimators_indices(self):
404         # Get drawn indices along both sample and feature axes
405         for seed in self._seeds:
406             # Operations accessing random_state must be performed identically
407             # to those in `_parallel_build_estimators()`
408             random_state = np.random.RandomState(seed)
409             feature_indices, sample_indices = _generate_bagging_indices(
410                 random_state, self.bootstrap_features, self.bootstrap,
411                 self.n_features_, self._n_samples, self._max_features,
412                 self._max_samples)
413 
414             yield feature_indices, sample_indices
415 
416     @property
417     def estimators_samples_(self):
418         """
419         The subset of drawn samples for each base estimator.
420 
421         Returns a dynamically generated list of indices identifying
422         the samples used for fitting each member of the ensemble, i.e.,
423         the in-bag samples.
424 
425         Note: the list is re-created at each call to the property in order
426         to reduce the object memory footprint by not storing the sampling
427         data. Thus fetching the property may be slower than expected.
428         """
429         return [sample_indices
430                 for _, sample_indices in self._get_estimators_indices()]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>fit</method>
<code>
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self

</code><intended_behavior>Adapt the `fit` method to utilize `_check_sample_weight` for validating `sample_weight`, ensuring the method's adherence to the unified validation process.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>BaseForest</class>
<method>None</method>
<code>
172 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):
173     """
174     Base class for forests of trees.
175 
176     Warning: This class should not be used directly. Use derived classes
177     instead.
178     """
179 
180     @abstractmethod
181     def __init__(self,
182                  base_estimator,
183                  n_estimators=100,
184                  estimator_params=tuple(),
185                  bootstrap=False,
186                  oob_score=False,
187                  n_jobs=None,
188                  random_state=None,
189                  verbose=0,
190                  warm_start=False,
191                  class_weight=None,
192                  max_samples=None):
193         super().__init__(
194             base_estimator=base_estimator,
195             n_estimators=n_estimators,
196             estimator_params=estimator_params)
197 
198         self.bootstrap = bootstrap
199         self.oob_score = oob_score
200         self.n_jobs = n_jobs
201         self.random_state = random_state
202         self.verbose = verbose
203         self.warm_start = warm_start
204         self.class_weight = class_weight
205         self.max_samples = max_samples
206 
207     def apply(self, X):
208         """
209         Apply trees in the forest to X, return leaf indices.
210 
211         Parameters
212         ----------
213         X : {array-like or sparse matrix} of shape (n_samples, n_features)
214             The input samples. Internally, its dtype will be converted to
215             ``dtype=np.float32``. If a sparse matrix is provided, it will be
216             converted into a sparse ``csr_matrix``.
217 
218         Returns
219         -------
220         X_leaves : array_like, shape = [n_samples, n_estimators]
221             For each datapoint x in X and for each tree in the forest,
222             return the index of the leaf x ends up in.
223         """
224         X = self._validate_X_predict(X)
225         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
226                            **_joblib_parallel_args(prefer="threads"))(
227             delayed(tree.apply)(X, check_input=False)
228             for tree in self.estimators_)
229 
230         return np.array(results).T
231 
232     def decision_path(self, X):
233         """
234         Return the decision path in the forest.
235 
236         .. versionadded:: 0.18
237 
238         Parameters
239         ----------
240         X : {array-like or sparse matrix} of shape (n_samples, n_features)
241             The input samples. Internally, its dtype will be converted to
242             ``dtype=np.float32``. If a sparse matrix is provided, it will be
243             converted into a sparse ``csr_matrix``.
244 
245         Returns
246         -------
247         indicator : sparse csr array, shape = [n_samples, n_nodes]
248             Return a node indicator matrix where non zero elements
249             indicates that the samples goes through the nodes.
250 
251         n_nodes_ptr : array of size (n_estimators + 1, )
252             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
253             gives the indicator value for the i-th estimator.
254 
255         """
256         X = self._validate_X_predict(X)
257         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
258                               **_joblib_parallel_args(prefer='threads'))(
259             delayed(tree.decision_path)(X,
260                                      check_input=False)
261             for tree in self.estimators_)
262 
263         n_nodes = [0]
264         n_nodes.extend([i.shape[1] for i in indicators])
265         n_nodes_ptr = np.array(n_nodes).cumsum()
266 
267         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
268 
269     def fit(self, X, y, sample_weight=None):
270         """
271         Build a forest of trees from the training set (X, y).
272 
273         Parameters
274         ----------
275         X : array-like or sparse matrix of shape (n_samples, n_features)
276             The training input samples. Internally, its dtype will be converted
277             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
278             converted into a sparse ``csc_matrix``.
279 
280         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
281             The target values (class labels in classification, real numbers in
282             regression).
283 
284         sample_weight : array-like of shape (n_samples,), default=None
285             Sample weights. If None, then samples are equally weighted. Splits
286             that would create child nodes with net zero or negative weight are
287             ignored while searching for a split in each node. In the case of
288             classification, splits are also ignored if they would result in any
289             single class carrying a negative weight in either child node.
290 
291         Returns
292         -------
293         self : object
294         """
295         # Validate or convert input data
296         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
297         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
298         if sample_weight is not None:
299             sample_weight = check_array(sample_weight, ensure_2d=False)
300         if issparse(X):
301             # Pre-sort indices to avoid that each individual tree of the
302             # ensemble sorts the indices.
303             X.sort_indices()
304 
305         # Remap output
306         self.n_features_ = X.shape[1]
307 
308         y = np.atleast_1d(y)
309         if y.ndim == 2 and y.shape[1] == 1:
310             warn("A column-vector y was passed when a 1d array was"
311                  " expected. Please change the shape of y to "
312                  "(n_samples,), for example using ravel().",
313                  DataConversionWarning, stacklevel=2)
314 
315         if y.ndim == 1:
316             # reshape is necessary to preserve the data contiguity against vs
317             # [:, np.newaxis] that does not.
318             y = np.reshape(y, (-1, 1))
319 
320         self.n_outputs_ = y.shape[1]
321 
322         y, expanded_class_weight = self._validate_y_class_weight(y)
323 
324         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
325             y = np.ascontiguousarray(y, dtype=DOUBLE)
326 
327         if expanded_class_weight is not None:
328             if sample_weight is not None:
329                 sample_weight = sample_weight * expanded_class_weight
330             else:
331                 sample_weight = expanded_class_weight
332 
333         # Get bootstrap sample size
334         n_samples_bootstrap = _get_n_samples_bootstrap(
335             n_samples=X.shape[0],
336             max_samples=self.max_samples
337         )
338 
339         # Check parameters
340         self._validate_estimator()
341 
342         if not self.bootstrap and self.oob_score:
343             raise ValueError("Out of bag estimation only available"
344                              " if bootstrap=True")
345 
346         random_state = check_random_state(self.random_state)
347 
348         if not self.warm_start or not hasattr(self, "estimators_"):
349             # Free allocated memory, if any
350             self.estimators_ = []
351 
352         n_more_estimators = self.n_estimators - len(self.estimators_)
353 
354         if n_more_estimators < 0:
355             raise ValueError('n_estimators=%d must be larger or equal to '
356                              'len(estimators_)=%d when warm_start==True'
357                              % (self.n_estimators, len(self.estimators_)))
358 
359         elif n_more_estimators == 0:
360             warn("Warm-start fitting without increasing n_estimators does not "
361                  "fit new trees.")
362         else:
363             if self.warm_start and len(self.estimators_) > 0:
364                 # We draw from the random state to get the random state we
365                 # would have got if we hadn't used a warm_start.
366                 random_state.randint(MAX_INT, size=len(self.estimators_))
367 
368             trees = [self._make_estimator(append=False,
369                                           random_state=random_state)
370                      for i in range(n_more_estimators)]
371 
372             # Parallel loop: we prefer the threading backend as the Cython code
373             # for fitting the trees is internally releasing the Python GIL
374             # making threading more efficient than multiprocessing in
375             # that case. However, for joblib 0.12+ we respect any
376             # parallel_backend contexts set at a higher level,
377             # since correctness does not rely on using threads.
378             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
379                              **_joblib_parallel_args(prefer='threads'))(
380                 delayed(_parallel_build_trees)(
381                     t, self, X, y, sample_weight, i, len(trees),
382                     verbose=self.verbose, class_weight=self.class_weight,
383                     n_samples_bootstrap=n_samples_bootstrap)
384                 for i, t in enumerate(trees))
385 
386             # Collect newly grown trees
387             self.estimators_.extend(trees)
388 
389         if self.oob_score:
390             self._set_oob_score(X, y)
391 
392         # Decapsulate classes_ attributes
393         if hasattr(self, "classes_") and self.n_outputs_ == 1:
394             self.n_classes_ = self.n_classes_[0]
395             self.classes_ = self.classes_[0]
396 
397         return self
398 
399     @abstractmethod
400     def _set_oob_score(self, X, y):
401         """
402         Calculate out of bag predictions and score."""
403 
404     def _validate_y_class_weight(self, y):
405         # Default implementation
406         return y, None
407 
408     def _validate_X_predict(self, X):
409         """
410         Validate X whenever one tries to predict, apply, predict_proba."""
411         check_is_fitted(self)
412 
413         return self.estimators_[0]._validate_X_predict(X, check_input=True)
414 
415     @property
416     def feature_importances_(self):
417         """
418         Return the feature importances (the higher, the more important the
419            feature).
420 
421         Returns
422         -------
423         feature_importances_ : array, shape = [n_features]
424             The values of this array sum to 1, unless all trees are single node
425             trees consisting of only the root node, in which case it will be an
426             array of zeros.
427         """
428         check_is_fitted(self)
429 
430         all_importances = Parallel(n_jobs=self.n_jobs,
431                                    **_joblib_parallel_args(prefer='threads'))(
432             delayed(getattr)(tree, 'feature_importances_')
433             for tree in self.estimators_ if tree.tree_.node_count > 1)
434 
435         if not all_importances:
436             return np.zeros(self.n_features_, dtype=np.float64)
437 
438         all_importances = np.mean(all_importances,
439                                   axis=0, dtype=np.float64)
440         return all_importances / np.sum(all_importances)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self

</code><intended_behavior>The `fit` method should make use of `_check_sample_weight` to validate `sample_weight`, thereby standardizing sample weight validation across the framework.</intended_behavior>, <file>sklearn/ensemble/_gb.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features, ccp_alpha,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='deprecated',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.ccp_alpha = ccp_alpha
1192         self.init = init
1193         self.random_state = random_state
1194         self.alpha = alpha
1195         self.verbose = verbose
1196         self.max_leaf_nodes = max_leaf_nodes
1197         self.warm_start = warm_start
1198         self.presort = presort
1199         self.validation_fraction = validation_fraction
1200         self.n_iter_no_change = n_iter_no_change
1201         self.tol = tol
1202 
1203     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1204                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1205         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1206 
1207         assert sample_mask.dtype == np.bool
1208         loss = self.loss_
1209         original_y = y
1210 
1211         # Need to pass a copy of raw_predictions to negative_gradient()
1212         # because raw_predictions is partially updated at the end of the loop
1213         # in update_terminal_regions(), and gradients need to be evaluated at
1214         # iteration i - 1.
1215         raw_predictions_copy = raw_predictions.copy()
1216 
1217         for k in range(loss.K):
1218             if loss.is_multi_class:
1219                 y = np.array(original_y == k, dtype=np.float64)
1220 
1221             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1222                                               sample_weight=sample_weight)
1223 
1224             # induce regression tree on residuals
1225             tree = DecisionTreeRegressor(
1226                 criterion=self.criterion,
1227                 splitter='best',
1228                 max_depth=self.max_depth,
1229                 min_samples_split=self.min_samples_split,
1230                 min_samples_leaf=self.min_samples_leaf,
1231                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1232                 min_impurity_decrease=self.min_impurity_decrease,
1233                 min_impurity_split=self.min_impurity_split,
1234                 max_features=self.max_features,
1235                 max_leaf_nodes=self.max_leaf_nodes,
1236                 random_state=random_state,
1237                 ccp_alpha=self.ccp_alpha)
1238 
1239             if self.subsample < 1.0:
1240                 # no inplace multiplication!
1241                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1242 
1243             X = X_csr if X_csr is not None else X
1244             tree.fit(X, residual, sample_weight=sample_weight,
1245                      check_input=False, X_idx_sorted=X_idx_sorted)
1246 
1247             # update tree leaves
1248             loss.update_terminal_regions(
1249                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1250                 sample_mask, learning_rate=self.learning_rate, k=k)
1251 
1252             # add tree to ensemble
1253             self.estimators_[i, k] = tree
1254 
1255         return raw_predictions
1256 
1257     def _check_params(self):
1258         """Check validity of parameters and raise ValueError if not valid. """
1259         if self.n_estimators <= 0:
1260             raise ValueError("n_estimators must be greater than 0 but "
1261                              "was %r" % self.n_estimators)
1262 
1263         if self.learning_rate <= 0.0:
1264             raise ValueError("learning_rate must be greater than 0 but "
1265                              "was %r" % self.learning_rate)
1266 
1267         if (self.loss not in self._SUPPORTED_LOSS
1268                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1269             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1270 
1271         if self.loss == 'deviance':
1272             loss_class = (_gb_losses.MultinomialDeviance
1273                           if len(self.classes_) > 2
1274                           else _gb_losses.BinomialDeviance)
1275         else:
1276             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1277 
1278         if self.loss in ('huber', 'quantile'):
1279             self.loss_ = loss_class(self.n_classes_, self.alpha)
1280         else:
1281             self.loss_ = loss_class(self.n_classes_)
1282 
1283         if not (0.0 < self.subsample <= 1.0):
1284             raise ValueError("subsample must be in (0,1] but "
1285                              "was %r" % self.subsample)
1286 
1287         if self.init is not None:
1288             # init must be an estimator or 'zero'
1289             if isinstance(self.init, BaseEstimator):
1290                 self.loss_.check_init_estimator(self.init)
1291             elif not (isinstance(self.init, str) and self.init == 'zero'):
1292                 raise ValueError(
1293                     "The init parameter must be an estimator or 'zero'. "
1294                     "Got init={}".format(self.init)
1295                 )
1296 
1297         if not (0.0 < self.alpha < 1.0):
1298             raise ValueError("alpha must be in (0.0, 1.0) but "
1299                              "was %r" % self.alpha)
1300 
1301         if isinstance(self.max_features, str):
1302             if self.max_features == "auto":
1303                 # if is_classification
1304                 if self.n_classes_ > 1:
1305                     max_features = max(1, int(np.sqrt(self.n_features_)))
1306                 else:
1307                     # is regression
1308                     max_features = self.n_features_
1309             elif self.max_features == "sqrt":
1310                 max_features = max(1, int(np.sqrt(self.n_features_)))
1311             elif self.max_features == "log2":
1312                 max_features = max(1, int(np.log2(self.n_features_)))
1313             else:
1314                 raise ValueError("Invalid value for max_features: %r. "
1315                                  "Allowed string values are 'auto', 'sqrt' "
1316                                  "or 'log2'." % self.max_features)
1317         elif self.max_features is None:
1318             max_features = self.n_features_
1319         elif isinstance(self.max_features, numbers.Integral):
1320             max_features = self.max_features
1321         else:  # float
1322             if 0. < self.max_features <= 1.:
1323                 max_features = max(int(self.max_features *
1324                                        self.n_features_), 1)
1325             else:
1326                 raise ValueError("max_features must be in (0, n_features]")
1327 
1328         self.max_features_ = max_features
1329 
1330         if not isinstance(self.n_iter_no_change,
1331                           (numbers.Integral, type(None))):
1332             raise ValueError("n_iter_no_change should either be None or an "
1333                              "integer. %r was passed"
1334                              % self.n_iter_no_change)
1335 
1336         if self.presort != 'deprecated':
1337             warnings.warn("The parameter 'presort' is deprecated and has no "
1338                           "effect. It will be removed in v0.24. You can "
1339                           "suppress this warning by not passing any value "
1340                           "to the 'presort' parameter. We also recommend "
1341                           "using HistGradientBoosting models instead.",
1342                           FutureWarning)
1343 
1344     def _init_state(self):
1345         """Initialize model state and allocate model state data structures. """
1346 
1347         self.init_ = self.init
1348         if self.init_ is None:
1349             self.init_ = self.loss_.init_estimator()
1350 
1351         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1352                                     dtype=np.object)
1353         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1354         # do oob?
1355         if self.subsample < 1.0:
1356             self.oob_improvement_ = np.zeros((self.n_estimators),
1357                                              dtype=np.float64)
1358 
1359     def _clear_state(self):
1360         """Clear the state of the gradient boosting model. """
1361         if hasattr(self, 'estimators_'):
1362             self.estimators_ = np.empty((0, 0), dtype=np.object)
1363         if hasattr(self, 'train_score_'):
1364             del self.train_score_
1365         if hasattr(self, 'oob_improvement_'):
1366             del self.oob_improvement_
1367         if hasattr(self, 'init_'):
1368             del self.init_
1369         if hasattr(self, '_rng'):
1370             del self._rng
1371 
1372     def _resize_state(self):
1373         """Add additional ``n_estimators`` entries to all attributes. """
1374         # self.n_estimators is the number of additional est to fit
1375         total_n_estimators = self.n_estimators
1376         if total_n_estimators < self.estimators_.shape[0]:
1377             raise ValueError('resize with smaller n_estimators %d < %d' %
1378                              (total_n_estimators, self.estimators_[0]))
1379 
1380         self.estimators_ = np.resize(self.estimators_,
1381                                      (total_n_estimators, self.loss_.K))
1382         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1383         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1384             # if do oob resize arrays or create new if not available
1385             if hasattr(self, 'oob_improvement_'):
1386                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1387                                                   total_n_estimators)
1388             else:
1389                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1390                                                  dtype=np.float64)
1391 
1392     def _is_initialized(self):
1393         return len(getattr(self, 'estimators_', [])) > 0
1394 
1395     def _check_initialized(self):
1396         """Check that the estimator is initialized, raising an error if not."""
1397         check_is_fitted(self)
1398 
1399     def fit(self, X, y, sample_weight=None, monitor=None):
1400         """Fit the gradient boosting model.
1401 
1402         Parameters
1403         ----------
1404         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1405             The input samples. Internally, it will be converted to
1406             ``dtype=np.float32`` and if a sparse matrix is provided
1407             to a sparse ``csr_matrix``.
1408 
1409         y : array-like, shape (n_samples,)
1410             Target values (strings or integers in classification, real numbers
1411             in regression)
1412             For classification, labels must correspond to classes.
1413 
1414         sample_weight : array-like, shape (n_samples,) or None
1415             Sample weights. If None, then samples are equally weighted. Splits
1416             that would create child nodes with net zero or negative weight are
1417             ignored while searching for a split in each node. In the case of
1418             classification, splits are also ignored if they would result in any
1419             single class carrying a negative weight in either child node.
1420 
1421         monitor : callable, optional
1422             The monitor is called after each iteration with the current
1423             iteration, a reference to the estimator and the local variables of
1424             ``_fit_stages`` as keyword arguments ``callable(i, self,
1425             locals())``. If the callable returns ``True`` the fitting procedure
1426             is stopped. The monitor can be used for various things such as
1427             computing held-out estimates, early stopping, model introspect, and
1428             snapshoting.
1429 
1430         Returns
1431         -------
1432         self : object
1433         """
1434         # if not warmstart - clear the estimator state
1435         if not self.warm_start:
1436             self._clear_state()
1437 
1438         # Check input
1439         # Since check_array converts both X and y to the same dtype, but the
1440         # trees use different types for X and y, checking them separately.
1441         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1442         n_samples, self.n_features_ = X.shape
1443 
1444         sample_weight_is_none = sample_weight is None
1445         if sample_weight_is_none:
1446             sample_weight = np.ones(n_samples, dtype=np.float32)
1447         else:
1448             sample_weight = column_or_1d(sample_weight, warn=True)
1449             sample_weight_is_none = False
1450 
1451         check_consistent_length(X, y, sample_weight)
1452 
1453         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
1454         y = column_or_1d(y, warn=True)
1455         y = self._validate_y(y, sample_weight)
1456 
1457         if self.n_iter_no_change is not None:
1458             stratify = y if is_classifier(self) else None
1459             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1460                 train_test_split(X, y, sample_weight,
1461                                  random_state=self.random_state,
1462                                  test_size=self.validation_fraction,
1463                                  stratify=stratify))
1464             if is_classifier(self):
1465                 if self.n_classes_ != np.unique(y).shape[0]:
1466                     # We choose to error here. The problem is that the init
1467                     # estimator would be trained on y, which has some missing
1468                     # classes now, so its predictions would not have the
1469                     # correct shape.
1470                     raise ValueError(
1471                         'The training data after the early stopping split '
1472                         'is missing some classes. Try using another random '
1473                         'seed.'
1474                     )
1475         else:
1476             X_val = y_val = sample_weight_val = None
1477 
1478         self._check_params()
1479 
1480         if not self._is_initialized():
1481             # init state
1482             self._init_state()
1483 
1484             # fit initial model and initialize raw predictions
1485             if self.init_ == 'zero':
1486                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1487                                            dtype=np.float64)
1488             else:
1489                 # XXX clean this once we have a support_sample_weight tag
1490                 if sample_weight_is_none:
1491                     self.init_.fit(X, y)
1492                 else:
1493                     msg = ("The initial estimator {} does not support sample "
1494                            "weights.".format(self.init_.__class__.__name__))
1495                     try:
1496                         self.init_.fit(X, y, sample_weight=sample_weight)
1497                     except TypeError:  # regular estimator without SW support
1498                         raise ValueError(msg)
1499                     except ValueError as e:
1500                         if "pass parameters to specific steps of "\
1501                            "your pipeline using the "\
1502                            "stepname__parameter" in str(e):  # pipeline
1503                             raise ValueError(msg) from e
1504                         else:  # regular estimator whose input checking failed
1505                             raise
1506 
1507                 raw_predictions = \
1508                     self.loss_.get_init_raw_predictions(X, self.init_)
1509 
1510             begin_at_stage = 0
1511 
1512             # The rng state must be preserved if warm_start is True
1513             self._rng = check_random_state(self.random_state)
1514 
1515         else:
1516             # add more estimators to fitted model
1517             # invariant: warm_start = True
1518             if self.n_estimators < self.estimators_.shape[0]:
1519                 raise ValueError('n_estimators=%d must be larger or equal to '
1520                                  'estimators_.shape[0]=%d when '
1521                                  'warm_start==True'
1522                                  % (self.n_estimators,
1523                                     self.estimators_.shape[0]))
1524             begin_at_stage = self.estimators_.shape[0]
1525             # The requirements of _decision_function (called in two lines
1526             # below) are more constrained than fit. It accepts only CSR
1527             # matrices.
1528             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1529             raw_predictions = self._raw_predict(X)
1530             self._resize_state()
1531 
1532         X_idx_sorted = None
1533 
1534         # fit the boosting stages
1535         n_stages = self._fit_stages(
1536             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1537             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1538 
1539         # change shape of arrays after fit (early-stopping or additional ests)
1540         if n_stages != self.estimators_.shape[0]:
1541             self.estimators_ = self.estimators_[:n_stages]
1542             self.train_score_ = self.train_score_[:n_stages]
1543             if hasattr(self, 'oob_improvement_'):
1544                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1545 
1546         self.n_estimators_ = n_stages
1547         return self
1548 
1549     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1550                     X_val, y_val, sample_weight_val,
1551                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1552         """Iteratively fits the stages.
1553 
1554         For each stage it computes the progress (OOB, train score)
1555         and delegates to ``_fit_stage``.
1556         Returns the number of stages fit; might differ from ``n_estimators``
1557         due to early stopping.
1558         """
1559         n_samples = X.shape[0]
1560         do_oob = self.subsample < 1.0
1561         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1562         n_inbag = max(1, int(self.subsample * n_samples))
1563         loss_ = self.loss_
1564 
1565         if self.verbose:
1566             verbose_reporter = VerboseReporter(self.verbose)
1567             verbose_reporter.init(self, begin_at_stage)
1568 
1569         X_csc = csc_matrix(X) if issparse(X) else None
1570         X_csr = csr_matrix(X) if issparse(X) else None
1571 
1572         if self.n_iter_no_change is not None:
1573             loss_history = np.full(self.n_iter_no_change, np.inf)
1574             # We create a generator to get the predictions for X_val after
1575             # the addition of each successive stage
1576             y_val_pred_iter = self._staged_raw_predict(X_val)
1577 
1578         # perform boosting iterations
1579         i = begin_at_stage
1580         for i in range(begin_at_stage, self.n_estimators):
1581 
1582             # subsampling
1583             if do_oob:
1584                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1585                                                   random_state)
1586                 # OOB score before adding this stage
1587                 old_oob_score = loss_(y[~sample_mask],
1588                                       raw_predictions[~sample_mask],
1589                                       sample_weight[~sample_mask])
1590 
1591             # fit next stage of trees
1592             raw_predictions = self._fit_stage(
1593                 i, X, y, raw_predictions, sample_weight, sample_mask,
1594                 random_state, X_idx_sorted, X_csc, X_csr)
1595 
1596             # track deviance (= loss)
1597             if do_oob:
1598                 self.train_score_[i] = loss_(y[sample_mask],
1599                                              raw_predictions[sample_mask],
1600                                              sample_weight[sample_mask])
1601                 self.oob_improvement_[i] = (
1602                     old_oob_score - loss_(y[~sample_mask],
1603                                           raw_predictions[~sample_mask],
1604                                           sample_weight[~sample_mask]))
1605             else:
1606                 # no need to fancy index w/ no subsampling
1607                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1608 
1609             if self.verbose > 0:
1610                 verbose_reporter.update(i, self)
1611 
1612             if monitor is not None:
1613                 early_stopping = monitor(i, self, locals())
1614                 if early_stopping:
1615                     break
1616 
1617             # We also provide an early stopping based on the score from
1618             # validation set (X_val, y_val), if n_iter_no_change is set
1619             if self.n_iter_no_change is not None:
1620                 # By calling next(y_val_pred_iter), we get the predictions
1621                 # for X_val after the addition of the current stage
1622                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1623                                         sample_weight_val)
1624 
1625                 # Require validation_score to be better (less) than at least
1626                 # one of the last n_iter_no_change evaluations
1627                 if np.any(validation_loss + self.tol < loss_history):
1628                     loss_history[i % len(loss_history)] = validation_loss
1629                 else:
1630                     break
1631 
1632         return i + 1
1633 
1634     def _make_estimator(self, append=True):
1635         # we don't need _make_estimator
1636         raise NotImplementedError()
1637 
1638     def _raw_predict_init(self, X):
1639         """Check input and compute raw predictions of the init estimtor."""
1640         self._check_initialized()
1641         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1642         if X.shape[1] != self.n_features_:
1643             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1644                 self.n_features_, X.shape[1]))
1645         if self.init_ == 'zero':
1646             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1647                                        dtype=np.float64)
1648         else:
1649             raw_predictions = self.loss_.get_init_raw_predictions(
1650                 X, self.init_).astype(np.float64)
1651         return raw_predictions
1652 
1653     def _raw_predict(self, X):
1654         """Return the sum of the trees raw predictions (+ init estimator)."""
1655         raw_predictions = self._raw_predict_init(X)
1656         predict_stages(self.estimators_, X, self.learning_rate,
1657                        raw_predictions)
1658         return raw_predictions
1659 
1660     def _staged_raw_predict(self, X):
1661         """Compute raw predictions of ``X`` for each iteration.
1662 
1663         This method allows monitoring (i.e. determine error on testing set)
1664         after each stage.
1665 
1666         Parameters
1667         ----------
1668         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1669             The input samples. Internally, it will be converted to
1670             ``dtype=np.float32`` and if a sparse matrix is provided
1671             to a sparse ``csr_matrix``.
1672 
1673         Returns
1674         -------
1675         raw_predictions : generator of array, shape (n_samples, k)
1676             The raw predictions of the input samples. The order of the
1677             classes corresponds to that in the attribute :term:`classes_`.
1678             Regression and binary classification are special cases with
1679             ``k == 1``, otherwise ``k==n_classes``.
1680         """
1681         X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1682         raw_predictions = self._raw_predict_init(X)
1683         for i in range(self.estimators_.shape[0]):
1684             predict_stage(self.estimators_, i, X, self.learning_rate,
1685                           raw_predictions)
1686             yield raw_predictions.copy()
1687 
1688     @property
1689     def feature_importances_(self):
1690         """Return the feature importances (the higher, the more important the
1691            feature).
1692 
1693         Returns
1694         -------
1695         feature_importances_ : array, shape (n_features,)
1696             The values of this array sum to 1, unless all trees are single node
1697             trees consisting of only the root node, in which case it will be an
1698             array of zeros.
1699         """
1700         self._check_initialized()
1701 
1702         relevant_trees = [tree
1703                           for stage in self.estimators_ for tree in stage
1704                           if tree.tree_.node_count > 1]
1705         if not relevant_trees:
1706             # degenerate case where all trees have only one node
1707             return np.zeros(shape=self.n_features_, dtype=np.float64)
1708 
1709         relevant_feature_importances = [
1710             tree.tree_.compute_feature_importances(normalize=False)
1711             for tree in relevant_trees
1712         ]
1713         avg_feature_importances = np.mean(relevant_feature_importances,
1714                                           axis=0, dtype=np.float64)
1715         return avg_feature_importances / np.sum(avg_feature_importances)
1716 
1717     def _compute_partial_dependence_recursion(self, grid, target_features):
1718         """Fast partial dependence computation.
1719 
1720         Parameters
1721         ----------
1722         grid : ndarray, shape (n_samples, n_target_features)
1723             The grid points on which the partial dependence should be
1724             evaluated.
1725         target_features : ndarray, shape (n_target_features)
1726             The set of target features for which the partial dependence
1727             should be evaluated.
1728 
1729         Returns
1730         -------
1731         averaged_predictions : ndarray, shape \
1732                 (n_trees_per_iteration, n_samples)
1733             The value of the partial dependence function on each grid point.
1734         """
1735         check_is_fitted(self,
1736                         msg="'estimator' parameter must be a fitted estimator")
1737         if self.init is not None:
1738             warnings.warn(
1739                 'Using recursion method with a non-constant init predictor '
1740                 'will lead to incorrect partial dependence values. '
1741                 'Got init=%s.' % self.init,
1742                 UserWarning
1743             )
1744         grid = np.asarray(grid, dtype=DTYPE, order='C')
1745         n_estimators, n_trees_per_stage = self.estimators_.shape
1746         averaged_predictions = np.zeros((n_trees_per_stage, grid.shape[0]),
1747                                         dtype=np.float64, order='C')
1748         for stage in range(n_estimators):
1749             for k in range(n_trees_per_stage):
1750                 tree = self.estimators_[stage, k].tree_
1751                 tree.compute_partial_dependence(grid, target_features,
1752                                                 averaged_predictions[k])
1753         averaged_predictions *= self.learning_rate
1754 
1755         return averaged_predictions
1756 
1757     def _validate_y(self, y, sample_weight):
1758         # 'sample_weight' is not utilised but is used for
1759         # consistency with similar method _validate_y of GBC
1760         self.n_classes_ = 1
1761         if y.dtype.kind == 'O':
1762             y = y.astype(DOUBLE)
1763         # Default implementation
1764         return y
1765 
1766     def apply(self, X):
1767         """Apply trees in the ensemble to X, return leaf indices.
1768 
1769         .. versionadded:: 0.17
1770 
1771         Parameters
1772         ----------
1773         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1774             The input samples. Internally, its dtype will be converted to
1775             ``dtype=np.float32``. If a sparse matrix is provided, it will
1776             be converted to a sparse ``csr_matrix``.
1777 
1778         Returns
1779         -------
1780         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1781             For each datapoint x in X and for each tree in the ensemble,
1782             return the index of the leaf x ends up in each estimator.
1783             In the case of binary classification n_classes is 1.
1784         """
1785 
1786         self._check_initialized()
1787         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1788 
1789         # n_classes will be equal to 1 in the binary classification or the
1790         # regression case.
1791         n_estimators, n_classes = self.estimators_.shape
1792         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1793 
1794         for i in range(n_estimators):
1795             for j in range(n_classes):
1796                 estimator = self.estimators_[i, j]
1797                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1798 
1799         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/isotonic.py</file>
<class>IsotonicRegression</class>
<method>fit</method>
<code>
302     def fit(self, X, y, sample_weight=None):
303         """Fit the model using X, y as training data.
304 
305         Parameters
306         ----------
307         X : array-like of shape (n_samples,)
308             Training data.
309 
310         y : array-like of shape (n_samples,)
311             Training target.
312 
313         sample_weight : array-like of shape (n_samples,), default=None
314             Weights. If set to None, all weights will be set to 1 (equal
315             weights).
316 
317         Returns
318         -------
319         self : object
320             Returns an instance of self.
321 
322         Notes
323         -----
324         X is stored for future use, as :meth:`transform` needs X to interpolate
325         new input data.
326         """
327         check_params = dict(accept_sparse=False, ensure_2d=False)
328         X = check_array(X, dtype=[np.float64, np.float32], **check_params)
329         y = check_array(y, dtype=X.dtype, **check_params)
330         check_consistent_length(X, y, sample_weight)
331 
332         # Transform y by running the isotonic regression algorithm and
333         # transform X accordingly.
334         X, y = self._build_y(X, y, sample_weight)
335 
336         # It is necessary to store the non-redundant part of the training set
337         # on the model to make it possible to support model persistence via
338         # the pickle module as the object built by scipy.interp1d is not
339         # picklable directly.
340         self._necessary_X_, self._necessary_y_ = X, y
341 
342         # Build the interpolation function
343         self._build_f(X, y)
344         return self

</code><intended_behavior>Adapt the `fit` method to include `_check_sample_weight` for `sample_weight` validation, aligning with the consistent approach of handling sample weights.</intended_behavior>, <file>sklearn/isotonic.py</file>
<class>IsotonicRegression</class>
<method>None</method>
<code>
140 class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
141     """Isotonic regression model.
142 
143     The isotonic regression optimization problem is defined by::
144 
145         min sum w_i (y[i] - y_[i]) ** 2
146 
147         subject to y_[i] <= y_[j] whenever X[i] <= X[j]
148         and min(y_) = y_min, max(y_) = y_max
149 
150     where:
151         - ``y[i]`` are inputs (real numbers)
152         - ``y_[i]`` are fitted
153         - ``X`` specifies the order.
154           If ``X`` is non-decreasing then ``y_`` is non-decreasing.
155         - ``w[i]`` are optional strictly positive weights (default to 1.0)
156 
157     Read more in the :ref:`User Guide <isotonic>`.
158 
159     Parameters
160     ----------
161     y_min : optional, default: None
162         If not None, set the lowest value of the fit to y_min.
163 
164     y_max : optional, default: None
165         If not None, set the highest value of the fit to y_max.
166 
167     increasing : boolean or string, optional, default: True
168         If boolean, whether or not to fit the isotonic regression with y
169         increasing or decreasing.
170 
171         The string value "auto" determines whether y should
172         increase or decrease based on the Spearman correlation estimate's
173         sign.
174 
175     out_of_bounds : string, optional, default: "nan"
176         The ``out_of_bounds`` parameter handles how x-values outside of the
177         training domain are handled.  When set to "nan", predicted y-values
178         will be NaN.  When set to "clip", predicted y-values will be
179         set to the value corresponding to the nearest train interval endpoint.
180         When set to "raise", allow ``interp1d`` to throw ValueError.
181 
182 
183     Attributes
184     ----------
185     X_min_ : float
186         Minimum value of input array `X_` for left bound.
187 
188     X_max_ : float
189         Maximum value of input array `X_` for right bound.
190 
191     f_ : function
192         The stepwise interpolating function that covers the input domain ``X``.
193 
194     Notes
195     -----
196     Ties are broken using the secondary method from Leeuw, 1977.
197 
198     References
199     ----------
200     Isotonic Median Regression: A Linear Programming Approach
201     Nilotpal Chakravarti
202     Mathematics of Operations Research
203     Vol. 14, No. 2 (May, 1989), pp. 303-308
204 
205     Isotone Optimization in R : Pool-Adjacent-Violators
206     Algorithm (PAVA) and Active Set Methods
207     Leeuw, Hornik, Mair
208     Journal of Statistical Software 2009
209 
210     Correctness of Kruskal's algorithms for monotone regression with ties
211     Leeuw, Psychometrica, 1977
212 
213     Examples
214     --------
215     >>> from sklearn.datasets import make_regression
216     >>> from sklearn.isotonic import IsotonicRegression
217     >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)
218     >>> iso_reg = IsotonicRegression().fit(X.flatten(), y)
219     >>> iso_reg.predict([.1, .2])
220     array([1.8628..., 3.7256...])
221     """
222     def __init__(self, y_min=None, y_max=None, increasing=True,
223                  out_of_bounds='nan'):
224         self.y_min = y_min
225         self.y_max = y_max
226         self.increasing = increasing
227         self.out_of_bounds = out_of_bounds
228 
229     def _check_fit_data(self, X, y, sample_weight=None):
230         if len(X.shape) != 1:
231             raise ValueError("X should be a 1d array")
232 
233     def _build_f(self, X, y):
234         """Build the f_ interp1d function."""
235 
236         # Handle the out_of_bounds argument by setting bounds_error
237         if self.out_of_bounds not in ["raise", "nan", "clip"]:
238             raise ValueError("The argument ``out_of_bounds`` must be in "
239                              "'nan', 'clip', 'raise'; got {0}"
240                              .format(self.out_of_bounds))
241 
242         bounds_error = self.out_of_bounds == "raise"
243         if len(y) == 1:
244             # single y, constant prediction
245             self.f_ = lambda x: y.repeat(x.shape)
246         else:
247             self.f_ = interpolate.interp1d(X, y, kind='linear',
248                                            bounds_error=bounds_error)
249 
250     def _build_y(self, X, y, sample_weight, trim_duplicates=True):
251         """Build the y_ IsotonicRegression."""
252         self._check_fit_data(X, y, sample_weight)
253 
254         # Determine increasing if auto-determination requested
255         if self.increasing == 'auto':
256             self.increasing_ = check_increasing(X, y)
257         else:
258             self.increasing_ = self.increasing
259 
260         # If sample_weights is passed, removed zero-weight values and clean
261         # order
262         if sample_weight is not None:
263             sample_weight = check_array(sample_weight, ensure_2d=False,
264                                         dtype=X.dtype)
265             mask = sample_weight > 0
266             X, y, sample_weight = X[mask], y[mask], sample_weight[mask]
267         else:
268             sample_weight = np.ones(len(y), dtype=X.dtype)
269 
270         order = np.lexsort((y, X))
271         X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]
272         unique_X, unique_y, unique_sample_weight = _make_unique(
273             X, y, sample_weight)
274 
275         # Store _X_ and _y_ to maintain backward compat during the deprecation
276         # period of X_ and y_
277         self._X_ = X = unique_X
278         self._y_ = y = isotonic_regression(unique_y, unique_sample_weight,
279                                            self.y_min, self.y_max,
280                                            increasing=self.increasing_)
281 
282         # Handle the left and right bounds on X
283         self.X_min_, self.X_max_ = np.min(X), np.max(X)
284 
285         if trim_duplicates:
286             # Remove unnecessary points for faster prediction
287             keep_data = np.ones((len(y),), dtype=bool)
288             # Aside from the 1st and last point, remove points whose y values
289             # are equal to both the point before and the point after it.
290             keep_data[1:-1] = np.logical_or(
291                 np.not_equal(y[1:-1], y[:-2]),
292                 np.not_equal(y[1:-1], y[2:])
293             )
294             return X[keep_data], y[keep_data]
295         else:
296             # The ability to turn off trim_duplicates is only used to it make
297             # easier to unit test that removing duplicates in y does not have
298             # any impact the resulting interpolation function (besides
299             # prediction speed).
300             return X, y
301 
302     def fit(self, X, y, sample_weight=None):
303         """Fit the model using X, y as training data.
304 
305         Parameters
306         ----------
307         X : array-like of shape (n_samples,)
308             Training data.
309 
310         y : array-like of shape (n_samples,)
311             Training target.
312 
313         sample_weight : array-like of shape (n_samples,), default=None
314             Weights. If set to None, all weights will be set to 1 (equal
315             weights).
316 
317         Returns
318         -------
319         self : object
320             Returns an instance of self.
321 
322         Notes
323         -----
324         X is stored for future use, as :meth:`transform` needs X to interpolate
325         new input data.
326         """
327         check_params = dict(accept_sparse=False, ensure_2d=False)
328         X = check_array(X, dtype=[np.float64, np.float32], **check_params)
329         y = check_array(y, dtype=X.dtype, **check_params)
330         check_consistent_length(X, y, sample_weight)
331 
332         # Transform y by running the isotonic regression algorithm and
333         # transform X accordingly.
334         X, y = self._build_y(X, y, sample_weight)
335 
336         # It is necessary to store the non-redundant part of the training set
337         # on the model to make it possible to support model persistence via
338         # the pickle module as the object built by scipy.interp1d is not
339         # picklable directly.
340         self._necessary_X_, self._necessary_y_ = X, y
341 
342         # Build the interpolation function
343         self._build_f(X, y)
344         return self
345 
346     def transform(self, T):
347         """Transform new data by linear interpolation
348 
349         Parameters
350         ----------
351         T : array-like of shape (n_samples,)
352             Data to transform.
353 
354         Returns
355         -------
356         T_ : array, shape=(n_samples,)
357             The transformed data
358         """
359 
360         if hasattr(self, '_necessary_X_'):
361             dtype = self._necessary_X_.dtype
362         else:
363             dtype = np.float64
364 
365         T = check_array(T, dtype=dtype, ensure_2d=False)
366 
367         if len(T.shape) != 1:
368             raise ValueError("Isotonic regression input should be a 1d array")
369 
370         # Handle the out_of_bounds argument by clipping if needed
371         if self.out_of_bounds not in ["raise", "nan", "clip"]:
372             raise ValueError("The argument ``out_of_bounds`` must be in "
373                              "'nan', 'clip', 'raise'; got {0}"
374                              .format(self.out_of_bounds))
375 
376         if self.out_of_bounds == "clip":
377             T = np.clip(T, self.X_min_, self.X_max_)
378 
379         res = self.f_(T)
380 
381         # on scipy 0.17, interp1d up-casts to float64, so we cast back
382         res = res.astype(T.dtype)
383 
384         return res
385 
386     def predict(self, T):
387         """Predict new data by linear interpolation.
388 
389         Parameters
390         ----------
391         T : array-like of shape (n_samples,)
392             Data to transform.
393 
394         Returns
395         -------
396         T_ : array, shape=(n_samples,)
397             Transformed data.
398         """
399         return self.transform(T)
400 
401     def __getstate__(self):
402         """Pickle-protocol - return state of the estimator. """
403         state = super().__getstate__()
404         # remove interpolation method
405         state.pop('f_', None)
406         return state
407 
408     def __setstate__(self, state):
409         """Pickle-protocol - set state of the estimator.
410 
411         We need to rebuild the interpolation function.
412         """
413         super().__setstate__(state)
414         if hasattr(self, '_necessary_X_') and hasattr(self, '_necessary_y_'):
415             self._build_f(self._necessary_X_, self._necessary_y_)
416 
417     def _more_tags(self):
418         return {'X_types': ['1darray']}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/kernel_ridge.py</file>
<class>KernelRidge</class>
<method>fit</method>
<code>
130     def fit(self, X, y=None, sample_weight=None):
131         """Fit Kernel Ridge regression model
132 
133         Parameters
134         ----------
135         X : {array-like, sparse matrix} of shape (n_samples, n_features)
136             Training data. If kernel == "precomputed" this is instead
137             a precomputed kernel matrix, shape = [n_samples,
138             n_samples].
139 
140         y : array-like of shape (n_samples,) or (n_samples, n_targets)
141             Target values
142 
143         sample_weight : float or array-like of shape [n_samples]
144             Individual weights for each sample, ignored if None is passed.
145 
146         Returns
147         -------
148         self : returns an instance of self.
149         """
150         # Convert data
151         X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True,
152                          y_numeric=True)
153         if sample_weight is not None and not isinstance(sample_weight, float):
154             sample_weight = check_array(sample_weight, ensure_2d=False)
155 
156         K = self._get_kernel(X)
157         alpha = np.atleast_1d(self.alpha)
158 
159         ravel = False
160         if len(y.shape) == 1:
161             y = y.reshape(-1, 1)
162             ravel = True
163 
164         copy = self.kernel == "precomputed"
165         self.dual_coef_ = _solve_cholesky_kernel(K, y, alpha,
166                                                  sample_weight,
167                                                  copy)
168         if ravel:
169             self.dual_coef_ = self.dual_coef_.ravel()
170 
171         self.X_fit_ = X
172 
173         return self

</code><intended_behavior>Within the `fit` method, replace any existing sample weight validation logic with `_check_sample_weight`, ensuring consistent validation across estimators.</intended_behavior>, <file>sklearn/kernel_ridge.py</file>
<class>KernelRidge</class>
<method>None</method>
<code>
16 class KernelRidge(MultiOutputMixin, RegressorMixin, BaseEstimator):
17     """Kernel ridge regression.
18 
19     Kernel ridge regression (KRR) combines ridge regression (linear least
20     squares with l2-norm regularization) with the kernel trick. It thus
21     learns a linear function in the space induced by the respective kernel and
22     the data. For non-linear kernels, this corresponds to a non-linear
23     function in the original space.
24 
25     The form of the model learned by KRR is identical to support vector
26     regression (SVR). However, different loss functions are used: KRR uses
27     squared error loss while support vector regression uses epsilon-insensitive
28     loss, both combined with l2 regularization. In contrast to SVR, fitting a
29     KRR model can be done in closed-form and is typically faster for
30     medium-sized datasets. On the other hand, the learned model is non-sparse
31     and thus slower than SVR, which learns a sparse model for epsilon > 0, at
32     prediction-time.
33 
34     This estimator has built-in support for multi-variate regression
35     (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
36 
37     Read more in the :ref:`User Guide <kernel_ridge>`.
38 
39     Parameters
40     ----------
41     alpha : {float, array-like}, shape = [n_targets]
42         Small positive values of alpha improve the conditioning of the problem
43         and reduce the variance of the estimates.  Alpha corresponds to
44         ``(2*C)^-1`` in other linear models such as LogisticRegression or
45         LinearSVC. If an array is passed, penalties are assumed to be specific
46         to the targets. Hence they must correspond in number.
47 
48     kernel : string or callable, default="linear"
49         Kernel mapping used internally. A callable should accept two arguments
50         and the keyword arguments passed to this object as kernel_params, and
51         should return a floating point number. Set to "precomputed" in
52         order to pass a precomputed kernel matrix to the estimator
53         methods instead of samples.
54 
55     gamma : float, default=None
56         Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
57         and sigmoid kernels. Interpretation of the default value is left to
58         the kernel; see the documentation for sklearn.metrics.pairwise.
59         Ignored by other kernels.
60 
61     degree : float, default=3
62         Degree of the polynomial kernel. Ignored by other kernels.
63 
64     coef0 : float, default=1
65         Zero coefficient for polynomial and sigmoid kernels.
66         Ignored by other kernels.
67 
68     kernel_params : mapping of string to any, optional
69         Additional parameters (keyword arguments) for kernel function passed
70         as callable object.
71 
72     Attributes
73     ----------
74     dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
75         Representation of weight vector(s) in kernel space
76 
77     X_fit_ : {array-like, sparse matrix} of shape (n_samples, n_features)
78         Training data, which is also required for prediction. If
79         kernel == "precomputed" this is instead the precomputed
80         training matrix, shape = [n_samples, n_samples].
81 
82     References
83     ----------
84     * Kevin P. Murphy
85       "Machine Learning: A Probabilistic Perspective", The MIT Press
86       chapter 14.4.3, pp. 492-493
87 
88     See also
89     --------
90     sklearn.linear_model.Ridge:
91         Linear ridge regression.
92     sklearn.svm.SVR:
93         Support Vector Regression implemented using libsvm.
94 
95     Examples
96     --------
97     >>> from sklearn.kernel_ridge import KernelRidge
98     >>> import numpy as np
99     >>> n_samples, n_features = 10, 5
100     >>> rng = np.random.RandomState(0)
101     >>> y = rng.randn(n_samples)
102     >>> X = rng.randn(n_samples, n_features)
103     >>> clf = KernelRidge(alpha=1.0)
104     >>> clf.fit(X, y)
105     KernelRidge(alpha=1.0)
106     """
107     def __init__(self, alpha=1, kernel="linear", gamma=None, degree=3, coef0=1,
108                  kernel_params=None):
109         self.alpha = alpha
110         self.kernel = kernel
111         self.gamma = gamma
112         self.degree = degree
113         self.coef0 = coef0
114         self.kernel_params = kernel_params
115 
116     def _get_kernel(self, X, Y=None):
117         if callable(self.kernel):
118             params = self.kernel_params or {}
119         else:
120             params = {"gamma": self.gamma,
121                       "degree": self.degree,
122                       "coef0": self.coef0}
123         return pairwise_kernels(X, Y, metric=self.kernel,
124                                 filter_params=True, **params)
125 
126     @property
127     def _pairwise(self):
128         return self.kernel == "precomputed"
129 
130     def fit(self, X, y=None, sample_weight=None):
131         """Fit Kernel Ridge regression model
132 
133         Parameters
134         ----------
135         X : {array-like, sparse matrix} of shape (n_samples, n_features)
136             Training data. If kernel == "precomputed" this is instead
137             a precomputed kernel matrix, shape = [n_samples,
138             n_samples].
139 
140         y : array-like of shape (n_samples,) or (n_samples, n_targets)
141             Target values
142 
143         sample_weight : float or array-like of shape [n_samples]
144             Individual weights for each sample, ignored if None is passed.
145 
146         Returns
147         -------
148         self : returns an instance of self.
149         """
150         # Convert data
151         X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True,
152                          y_numeric=True)
153         if sample_weight is not None and not isinstance(sample_weight, float):
154             sample_weight = check_array(sample_weight, ensure_2d=False)
155 
156         K = self._get_kernel(X)
157         alpha = np.atleast_1d(self.alpha)
158 
159         ravel = False
160         if len(y.shape) == 1:
161             y = y.reshape(-1, 1)
162             ravel = True
163 
164         copy = self.kernel == "precomputed"
165         self.dual_coef_ = _solve_cholesky_kernel(K, y, alpha,
166                                                  sample_weight,
167                                                  copy)
168         if ravel:
169             self.dual_coef_ = self.dual_coef_.ravel()
170 
171         self.X_fit_ = X
172 
173         return self
174 
175     def predict(self, X):
176         """Predict using the kernel ridge model
177 
178         Parameters
179         ----------
180         X : {array-like, sparse matrix} of shape (n_samples, n_features)
181             Samples. If kernel == "precomputed" this is instead a
182             precomputed kernel matrix, shape = [n_samples,
183             n_samples_fitted], where n_samples_fitted is the number of
184             samples used in the fitting for this estimator.
185 
186         Returns
187         -------
188         C : ndarray of shape (n_samples,) or (n_samples, n_targets)
189             Returns predicted values.
190         """
191         check_is_fitted(self)
192         K = self._get_kernel(X, self.X_fit_)
193         return np.dot(K, self.dual_coef_)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>GaussianNB</class>
<method>fit</method>
<code>
185     def fit(self, X, y, sample_weight=None):
186         """Fit Gaussian Naive Bayes according to X, y
187 
188         Parameters
189         ----------
190         X : array-like, shape (n_samples, n_features)
191             Training vectors, where n_samples is the number of samples
192             and n_features is the number of features.
193 
194         y : array-like, shape (n_samples,)
195             Target values.
196 
197         sample_weight : array-like, shape (n_samples,), optional (default=None)
198             Weights applied to individual samples (1. for unweighted).
199 
200             .. versionadded:: 0.17
201                Gaussian Naive Bayes supports fitting with *sample_weight*.
202 
203         Returns
204         -------
205         self : object
206         """
207         y = column_or_1d(y, warn=True)
208         return self._partial_fit(X, y, np.unique(y), _refit=True,
209                                  sample_weight=sample_weight)

</code><intended_behavior>Utilize `_check_sample_weight` for validating `sample_weight` in the `fit` method, aligning GaussianNB's handling of sample weights with the rest of the project.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>GaussianNB</class>
<method>None</method>
<code>
118 class GaussianNB(BaseNB):
119     """
120     Gaussian Naive Bayes (GaussianNB)
121 
122     Can perform online updates to model parameters via :meth:`partial_fit`.
123     For details on algorithm used to update feature means and variance online,
124     see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:
125 
126         http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf
127 
128     Read more in the :ref:`User Guide <gaussian_naive_bayes>`.
129 
130     Parameters
131     ----------
132     priors : array-like, shape (n_classes,)
133         Prior probabilities of the classes. If specified the priors are not
134         adjusted according to the data.
135 
136     var_smoothing : float, optional (default=1e-9)
137         Portion of the largest variance of all features that is added to
138         variances for calculation stability.
139 
140     Attributes
141     ----------
142     class_prior_ : array, shape (n_classes,)
143         probability of each class.
144 
145     class_count_ : array, shape (n_classes,)
146         number of training samples observed in each class.
147 
148     classes_ : array, shape (n_classes,)
149         class labels known to the classifier
150 
151     theta_ : array, shape (n_classes, n_features)
152         mean of each feature per class
153 
154     sigma_ : array, shape (n_classes, n_features)
155         variance of each feature per class
156 
157     epsilon_ : float
158         absolute additive value to variances
159 
160     classes_ : array-like, shape (n_classes,)
161         Unique class labels.
162 
163     Examples
164     --------
165     >>> import numpy as np
166     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
167     >>> Y = np.array([1, 1, 1, 2, 2, 2])
168     >>> from sklearn.naive_bayes import GaussianNB
169     >>> clf = GaussianNB()
170     >>> clf.fit(X, Y)
171     GaussianNB()
172     >>> print(clf.predict([[-0.8, -1]]))
173     [1]
174     >>> clf_pf = GaussianNB()
175     >>> clf_pf.partial_fit(X, Y, np.unique(Y))
176     GaussianNB()
177     >>> print(clf_pf.predict([[-0.8, -1]]))
178     [1]
179     """
180 
181     def __init__(self, priors=None, var_smoothing=1e-9):
182         self.priors = priors
183         self.var_smoothing = var_smoothing
184 
185     def fit(self, X, y, sample_weight=None):
186         """Fit Gaussian Naive Bayes according to X, y
187 
188         Parameters
189         ----------
190         X : array-like, shape (n_samples, n_features)
191             Training vectors, where n_samples is the number of samples
192             and n_features is the number of features.
193 
194         y : array-like, shape (n_samples,)
195             Target values.
196 
197         sample_weight : array-like, shape (n_samples,), optional (default=None)
198             Weights applied to individual samples (1. for unweighted).
199 
200             .. versionadded:: 0.17
201                Gaussian Naive Bayes supports fitting with *sample_weight*.
202 
203         Returns
204         -------
205         self : object
206         """
207         y = column_or_1d(y, warn=True)
208         return self._partial_fit(X, y, np.unique(y), _refit=True,
209                                  sample_weight=sample_weight)
210 
211     def _check_X(self, X):
212         return check_array(X)
213 
214     @staticmethod
215     def _update_mean_variance(n_past, mu, var, X, sample_weight=None):
216         """Compute online update of Gaussian mean and variance.
217 
218         Given starting sample count, mean, and variance, a new set of
219         points X, and optionally sample weights, return the updated mean and
220         variance. (NB - each dimension (column) in X is treated as independent
221         -- you get variance, not covariance).
222 
223         Can take scalar mean and variance, or vector mean and variance to
224         simultaneously update a number of independent Gaussians.
225 
226         See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:
227 
228         http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf
229 
230         Parameters
231         ----------
232         n_past : int
233             Number of samples represented in old mean and variance. If sample
234             weights were given, this should contain the sum of sample
235             weights represented in old mean and variance.
236 
237         mu : array-like, shape (number of Gaussians,)
238             Means for Gaussians in original set.
239 
240         var : array-like, shape (number of Gaussians,)
241             Variances for Gaussians in original set.
242 
243         sample_weight : array-like, shape (n_samples,), optional (default=None)
244             Weights applied to individual samples (1. for unweighted).
245 
246         Returns
247         -------
248         total_mu : array-like, shape (number of Gaussians,)
249             Updated mean for each Gaussian over the combined set.
250 
251         total_var : array-like, shape (number of Gaussians,)
252             Updated variance for each Gaussian over the combined set.
253         """
254         if X.shape[0] == 0:
255             return mu, var
256 
257         # Compute (potentially weighted) mean and variance of new datapoints
258         if sample_weight is not None:
259             n_new = float(sample_weight.sum())
260             new_mu = np.average(X, axis=0, weights=sample_weight)
261             new_var = np.average((X - new_mu) ** 2, axis=0,
262                                  weights=sample_weight)
263         else:
264             n_new = X.shape[0]
265             new_var = np.var(X, axis=0)
266             new_mu = np.mean(X, axis=0)
267 
268         if n_past == 0:
269             return new_mu, new_var
270 
271         n_total = float(n_past + n_new)
272 
273         # Combine mean of old and new data, taking into consideration
274         # (weighted) number of observations
275         total_mu = (n_new * new_mu + n_past * mu) / n_total
276 
277         # Combine variance of old and new data, taking into consideration
278         # (weighted) number of observations. This is achieved by combining
279         # the sum-of-squared-differences (ssd)
280         old_ssd = n_past * var
281         new_ssd = n_new * new_var
282         total_ssd = (old_ssd + new_ssd +
283                      (n_new * n_past / n_total) * (mu - new_mu) ** 2)
284         total_var = total_ssd / n_total
285 
286         return total_mu, total_var
287 
288     def partial_fit(self, X, y, classes=None, sample_weight=None):
289         """Incremental fit on a batch of samples.
290 
291         This method is expected to be called several times consecutively
292         on different chunks of a dataset so as to implement out-of-core
293         or online learning.
294 
295         This is especially useful when the whole dataset is too big to fit in
296         memory at once.
297 
298         This method has some performance and numerical stability overhead,
299         hence it is better to call partial_fit on chunks of data that are
300         as large as possible (as long as fitting in the memory budget) to
301         hide the overhead.
302 
303         Parameters
304         ----------
305         X : array-like, shape (n_samples, n_features)
306             Training vectors, where n_samples is the number of samples and
307             n_features is the number of features.
308 
309         y : array-like, shape (n_samples,)
310             Target values.
311 
312         classes : array-like, shape (n_classes,), optional (default=None)
313             List of all the classes that can possibly appear in the y vector.
314 
315             Must be provided at the first call to partial_fit, can be omitted
316             in subsequent calls.
317 
318         sample_weight : array-like, shape (n_samples,), optional (default=None)
319             Weights applied to individual samples (1. for unweighted).
320 
321             .. versionadded:: 0.17
322 
323         Returns
324         -------
325         self : object
326         """
327         return self._partial_fit(X, y, classes, _refit=False,
328                                  sample_weight=sample_weight)
329 
330     def _partial_fit(self, X, y, classes=None, _refit=False,
331                      sample_weight=None):
332         """Actual implementation of Gaussian NB fitting.
333 
334         Parameters
335         ----------
336         X : array-like, shape (n_samples, n_features)
337             Training vectors, where n_samples is the number of samples and
338             n_features is the number of features.
339 
340         y : array-like, shape (n_samples,)
341             Target values.
342 
343         classes : array-like, shape (n_classes,), optional (default=None)
344             List of all the classes that can possibly appear in the y vector.
345 
346             Must be provided at the first call to partial_fit, can be omitted
347             in subsequent calls.
348 
349         _refit : bool, optional (default=False)
350             If true, act as though this were the first time we called
351             _partial_fit (ie, throw away any past fitting and start over).
352 
353         sample_weight : array-like, shape (n_samples,), optional (default=None)
354             Weights applied to individual samples (1. for unweighted).
355 
356         Returns
357         -------
358         self : object
359         """
360         X, y = check_X_y(X, y)
361         if sample_weight is not None:
362             sample_weight = check_array(sample_weight, ensure_2d=False)
363             check_consistent_length(y, sample_weight)
364 
365         # If the ratio of data variance between dimensions is too small, it
366         # will cause numerical errors. To address this, we artificially
367         # boost the variance by epsilon, a small fraction of the standard
368         # deviation of the largest dimension.
369         self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()
370 
371         if _refit:
372             self.classes_ = None
373 
374         if _check_partial_fit_first_call(self, classes):
375             # This is the first call to partial_fit:
376             # initialize various cumulative counters
377             n_features = X.shape[1]
378             n_classes = len(self.classes_)
379             self.theta_ = np.zeros((n_classes, n_features))
380             self.sigma_ = np.zeros((n_classes, n_features))
381 
382             self.class_count_ = np.zeros(n_classes, dtype=np.float64)
383 
384             # Initialise the class prior
385             # Take into account the priors
386             if self.priors is not None:
387                 priors = np.asarray(self.priors)
388                 # Check that the provide prior match the number of classes
389                 if len(priors) != n_classes:
390                     raise ValueError('Number of priors must match number of'
391                                      ' classes.')
392                 # Check that the sum is 1
393                 if not np.isclose(priors.sum(), 1.0):
394                     raise ValueError('The sum of the priors should be 1.')
395                 # Check that the prior are non-negative
396                 if (priors < 0).any():
397                     raise ValueError('Priors must be non-negative.')
398                 self.class_prior_ = priors
399             else:
400                 # Initialize the priors to zeros for each class
401                 self.class_prior_ = np.zeros(len(self.classes_),
402                                              dtype=np.float64)
403         else:
404             if X.shape[1] != self.theta_.shape[1]:
405                 msg = "Number of features %d does not match previous data %d."
406                 raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))
407             # Put epsilon back in each time
408             self.sigma_[:, :] -= self.epsilon_
409 
410         classes = self.classes_
411 
412         unique_y = np.unique(y)
413         unique_y_in_classes = np.in1d(unique_y, classes)
414 
415         if not np.all(unique_y_in_classes):
416             raise ValueError("The target label(s) %s in y do not exist in the "
417                              "initial classes %s" %
418                              (unique_y[~unique_y_in_classes], classes))
419 
420         for y_i in unique_y:
421             i = classes.searchsorted(y_i)
422             X_i = X[y == y_i, :]
423 
424             if sample_weight is not None:
425                 sw_i = sample_weight[y == y_i]
426                 N_i = sw_i.sum()
427             else:
428                 sw_i = None
429                 N_i = X_i.shape[0]
430 
431             new_theta, new_sigma = self._update_mean_variance(
432                 self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],
433                 X_i, sw_i)
434 
435             self.theta_[i, :] = new_theta
436             self.sigma_[i, :] = new_sigma
437             self.class_count_[i] += N_i
438 
439         self.sigma_[:, :] += self.epsilon_
440 
441         # Update if only no priors is provided
442         if self.priors is None:
443             # Empirical prior, with sample_weight taken into account
444             self.class_prior_ = self.class_count_ / self.class_count_.sum()
445 
446         return self
447 
448     def _joint_log_likelihood(self, X):
449         joint_log_likelihood = []
450         for i in range(np.size(self.classes_)):
451             jointi = np.log(self.class_prior_[i])
452             n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))
453             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /
454                                  (self.sigma_[i, :]), 1)
455             joint_log_likelihood.append(jointi + n_ij)
456 
457         joint_log_likelihood = np.array(joint_log_likelihood).T
458         return joint_log_likelihood

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>BaseDiscreteNB</class>
<method>fit</method>
<code>
592     def fit(self, X, y, sample_weight=None):
593         """Fit Naive Bayes classifier according to X, y
594 
595         Parameters
596         ----------
597         X : {array-like, sparse matrix} of shape (n_samples, n_features)
598             Training vectors, where n_samples is the number of samples and
599             n_features is the number of features.
600 
601         y : array-like of shape (n_samples,)
602             Target values.
603 
604         sample_weight : array-like of shape (n_samples,), default=None
605             Weights applied to individual samples (1. for unweighted).
606 
607         Returns
608         -------
609         self : object
610         """
611         X, y = self._check_X_y(X, y)
612         _, n_features = X.shape
613         self.n_features_ = n_features
614 
615         labelbin = LabelBinarizer()
616         Y = labelbin.fit_transform(y)
617         self.classes_ = labelbin.classes_
618         if Y.shape[1] == 1:
619             Y = np.concatenate((1 - Y, Y), axis=1)
620 
621         # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.
622         # We convert it to np.float64 to support sample_weight consistently;
623         # this means we also don't have to cast X to floating point
624         if sample_weight is not None:
625             Y = Y.astype(np.float64, copy=False)
626             sample_weight = np.asarray(sample_weight)
627             sample_weight = np.atleast_2d(sample_weight)
628             Y *= check_array(sample_weight).T
629 
630         class_prior = self.class_prior
631 
632         # Count raw events from data before updating the class log prior
633         # and feature log probas
634         n_effective_classes = Y.shape[1]
635 
636         self._init_counters(n_effective_classes, n_features)
637         self._count(X, Y)
638         alpha = self._check_alpha()
639         self._update_feature_log_prob(alpha)
640         self._update_class_log_prior(class_prior=class_prior)
641         return self

</code><intended_behavior>The `fit` method should incorporate `_check_sample_weight` for `sample_weight` validation, ensuring consistency in sample weight handling.</intended_behavior>, <file>sklearn/naive_bayes.py</file>
<class>BaseDiscreteNB</class>
<method>None</method>
<code>
464 class BaseDiscreteNB(BaseNB):
465     """Abstract base class for naive Bayes on discrete/categorical data
466 
467     Any estimator based on this class should provide:
468 
469     __init__
470     _joint_log_likelihood(X) as per BaseNB
471     """
472 
473     def _check_X(self, X):
474         return check_array(X, accept_sparse='csr')
475 
476     def _check_X_y(self, X, y):
477         return check_X_y(X, y, accept_sparse='csr')
478 
479     def _update_class_log_prior(self, class_prior=None):
480         n_classes = len(self.classes_)
481         if class_prior is not None:
482             if len(class_prior) != n_classes:
483                 raise ValueError("Number of priors must match number of"
484                                  " classes.")
485             self.class_log_prior_ = np.log(class_prior)
486         elif self.fit_prior:
487             with warnings.catch_warnings():
488                 # silence the warning when count is 0 because class was not yet
489                 # observed
490                 warnings.simplefilter("ignore", RuntimeWarning)
491                 log_class_count = np.log(self.class_count_)
492 
493             # empirical prior, with sample_weight taken into account
494             self.class_log_prior_ = (log_class_count -
495                                      np.log(self.class_count_.sum()))
496         else:
497             self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))
498 
499     def _check_alpha(self):
500         if np.min(self.alpha) < 0:
501             raise ValueError('Smoothing parameter alpha = %.1e. '
502                              'alpha should be > 0.' % np.min(self.alpha))
503         if isinstance(self.alpha, np.ndarray):
504             if not self.alpha.shape[0] == self.n_features_:
505                 raise ValueError("alpha should be a scalar or a numpy array "
506                                  "with shape [n_features]")
507         if np.min(self.alpha) < _ALPHA_MIN:
508             warnings.warn('alpha too small will result in numeric errors, '
509                           'setting alpha = %.1e' % _ALPHA_MIN)
510             return np.maximum(self.alpha, _ALPHA_MIN)
511         return self.alpha
512 
513     def partial_fit(self, X, y, classes=None, sample_weight=None):
514         """Incremental fit on a batch of samples.
515 
516         This method is expected to be called several times consecutively
517         on different chunks of a dataset so as to implement out-of-core
518         or online learning.
519 
520         This is especially useful when the whole dataset is too big to fit in
521         memory at once.
522 
523         This method has some performance overhead hence it is better to call
524         partial_fit on chunks of data that are as large as possible
525         (as long as fitting in the memory budget) to hide the overhead.
526 
527         Parameters
528         ----------
529         X : {array-like, sparse matrix} of shape (n_samples, n_features)
530             Training vectors, where n_samples is the number of samples and
531             n_features is the number of features.
532 
533         y : array-like of shape (n_samples,)
534             Target values.
535 
536         classes : array-like of shape (n_classes) (default=None)
537             List of all the classes that can possibly appear in the y vector.
538 
539             Must be provided at the first call to partial_fit, can be omitted
540             in subsequent calls.
541 
542         sample_weight : array-like of shape (n_samples,), default=None
543             Weights applied to individual samples (1. for unweighted).
544 
545         Returns
546         -------
547         self : object
548         """
549         X, y = self._check_X_y(X, y)
550         _, n_features = X.shape
551 
552         if _check_partial_fit_first_call(self, classes):
553             # This is the first call to partial_fit:
554             # initialize various cumulative counters
555             n_effective_classes = len(classes) if len(classes) > 1 else 2
556             self._init_counters(n_effective_classes, n_features)
557             self.n_features_ = n_features
558         elif n_features != self.n_features_:
559             msg = "Number of features %d does not match previous data %d."
560             raise ValueError(msg % (n_features, self.n_features_))
561 
562         Y = label_binarize(y, classes=self.classes_)
563         if Y.shape[1] == 1:
564             Y = np.concatenate((1 - Y, Y), axis=1)
565 
566         if X.shape[0] != Y.shape[0]:
567             msg = "X.shape[0]=%d and y.shape[0]=%d are incompatible."
568             raise ValueError(msg % (X.shape[0], y.shape[0]))
569 
570         # label_binarize() returns arrays with dtype=np.int64.
571         # We convert it to np.float64 to support sample_weight consistently
572         Y = Y.astype(np.float64, copy=False)
573         if sample_weight is not None:
574             sample_weight = np.atleast_2d(sample_weight)
575             Y *= check_array(sample_weight).T
576 
577         class_prior = self.class_prior
578 
579         # Count raw events from data before updating the class log prior
580         # and feature log probas
581         self._count(X, Y)
582 
583         # XXX: OPTIM: we could introduce a public finalization method to
584         # be called by the user explicitly just once after several consecutive
585         # calls to partial_fit and prior any call to predict[_[log_]proba]
586         # to avoid computing the smooth log probas at each call to partial fit
587         alpha = self._check_alpha()
588         self._update_feature_log_prob(alpha)
589         self._update_class_log_prior(class_prior=class_prior)
590         return self
591 
592     def fit(self, X, y, sample_weight=None):
593         """Fit Naive Bayes classifier according to X, y
594 
595         Parameters
596         ----------
597         X : {array-like, sparse matrix} of shape (n_samples, n_features)
598             Training vectors, where n_samples is the number of samples and
599             n_features is the number of features.
600 
601         y : array-like of shape (n_samples,)
602             Target values.
603 
604         sample_weight : array-like of shape (n_samples,), default=None
605             Weights applied to individual samples (1. for unweighted).
606 
607         Returns
608         -------
609         self : object
610         """
611         X, y = self._check_X_y(X, y)
612         _, n_features = X.shape
613         self.n_features_ = n_features
614 
615         labelbin = LabelBinarizer()
616         Y = labelbin.fit_transform(y)
617         self.classes_ = labelbin.classes_
618         if Y.shape[1] == 1:
619             Y = np.concatenate((1 - Y, Y), axis=1)
620 
621         # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.
622         # We convert it to np.float64 to support sample_weight consistently;
623         # this means we also don't have to cast X to floating point
624         if sample_weight is not None:
625             Y = Y.astype(np.float64, copy=False)
626             sample_weight = np.asarray(sample_weight)
627             sample_weight = np.atleast_2d(sample_weight)
628             Y *= check_array(sample_weight).T
629 
630         class_prior = self.class_prior
631 
632         # Count raw events from data before updating the class log prior
633         # and feature log probas
634         n_effective_classes = Y.shape[1]
635 
636         self._init_counters(n_effective_classes, n_features)
637         self._count(X, Y)
638         alpha = self._check_alpha()
639         self._update_feature_log_prob(alpha)
640         self._update_class_log_prior(class_prior=class_prior)
641         return self
642 
643     def _init_counters(self, n_effective_classes, n_features):
644         self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)
645         self.feature_count_ = np.zeros((n_effective_classes, n_features),
646                                        dtype=np.float64)
647 
648     # XXX The following is a stopgap measure; we need to set the dimensions
649     # of class_log_prior_ and feature_log_prob_ correctly.
650     def _get_coef(self):
651         return (self.feature_log_prob_[1:]
652                 if len(self.classes_) == 2 else self.feature_log_prob_)
653 
654     def _get_intercept(self):
655         return (self.class_log_prior_[1:]
656                 if len(self.classes_) == 2 else self.class_log_prior_)
657 
658     coef_ = property(_get_coef)
659     intercept_ = property(_get_intercept)
660 
661     def _more_tags(self):
662         return {'poor_score': True}

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/neighbors/_kde.py</file>
<class>KernelDensity</class>
<method>fit</method>
<code>
134     def fit(self, X, y=None, sample_weight=None):
135         """Fit the Kernel Density model on the data.
136 
137         Parameters
138         ----------
139         X : array_like, shape (n_samples, n_features)
140             List of n_features-dimensional data points.  Each row
141             corresponds to a single data point.
142         y : None
143             Ignored. This parameter exists only for compatibility with
144             :class:`sklearn.pipeline.Pipeline`.
145         sample_weight : array_like, shape (n_samples,), optional
146             List of sample weights attached to the data X.
147 
148         Returns
149         -------
150         self : object
151             Returns instance of object.
152         """
153         algorithm = self._choose_algorithm(self.algorithm, self.metric)
154         X = check_array(X, order='C', dtype=DTYPE)
155 
156         if sample_weight is not None:
157             sample_weight = check_array(sample_weight, order='C', dtype=DTYPE,
158                                         ensure_2d=False)
159             if sample_weight.ndim != 1:
160                 raise ValueError("the shape of sample_weight must be ({0},),"
161                                  " but was {1}".format(X.shape[0],
162                                                        sample_weight.shape))
163             check_consistent_length(X, sample_weight)
164             if sample_weight.min() <= 0:
165                 raise ValueError("sample_weight must have positive values")
166 
167         kwargs = self.metric_params
168         if kwargs is None:
169             kwargs = {}
170         self.tree_ = TREE_DICT[algorithm](X, metric=self.metric,
171                                           leaf_size=self.leaf_size,
172                                           sample_weight=sample_weight,
173                                           **kwargs)
174         return self

</code><intended_behavior>Leverage `_check_sample_weight` in the `fit` method for validating `sample_weight`, aligning the method with the standardized validation approach.</intended_behavior>, <file>sklearn/neighbors/_kde.py</file>
<class>KernelDensity</class>
<method>None</method>
<code>
25 class KernelDensity(BaseEstimator):
26     """Kernel Density Estimation.
27 
28     Read more in the :ref:`User Guide <kernel_density>`.
29 
30     Parameters
31     ----------
32     bandwidth : float
33         The bandwidth of the kernel.
34 
35     algorithm : str
36         The tree algorithm to use.  Valid options are
37         ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'.
38 
39     kernel : str
40         The kernel to use.  Valid kernels are
41         ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine']
42         Default is 'gaussian'.
43 
44     metric : str
45         The distance metric to use.  Note that not all metrics are
46         valid with all algorithms.  Refer to the documentation of
47         :class:`BallTree` and :class:`KDTree` for a description of
48         available algorithms.  Note that the normalization of the density
49         output is correct only for the Euclidean distance metric. Default
50         is 'euclidean'.
51 
52     atol : float
53         The desired absolute tolerance of the result.  A larger tolerance will
54         generally lead to faster execution. Default is 0.
55 
56     rtol : float
57         The desired relative tolerance of the result.  A larger tolerance will
58         generally lead to faster execution.  Default is 1E-8.
59 
60     breadth_first : bool
61         If true (default), use a breadth-first approach to the problem.
62         Otherwise use a depth-first approach.
63 
64     leaf_size : int
65         Specify the leaf size of the underlying tree.  See :class:`BallTree`
66         or :class:`KDTree` for details.  Default is 40.
67 
68     metric_params : dict
69         Additional parameters to be passed to the tree for use with the
70         metric.  For more information, see the documentation of
71         :class:`BallTree` or :class:`KDTree`.
72 
73     See Also
74     --------
75     sklearn.neighbors.KDTree : K-dimensional tree for fast generalized N-point
76         problems.
77     sklearn.neighbors.BallTree : Ball tree for fast generalized N-point
78         problems.
79 
80     Examples
81     --------
82     Compute a gaussian kernel density estimate with a fixed bandwidth.
83     >>> import numpy as np
84     >>> rng = np.random.RandomState(42)
85     >>> X = rng.random_sample((100, 3))
86     >>> kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)
87     >>> log_density = kde.score_samples(X[:3])
88     >>> log_density
89     array([-1.52955942, -1.51462041, -1.60244657])
90     """
91     def __init__(self, bandwidth=1.0, algorithm='auto',
92                  kernel='gaussian', metric="euclidean", atol=0, rtol=0,
93                  breadth_first=True, leaf_size=40, metric_params=None):
94         self.algorithm = algorithm
95         self.bandwidth = bandwidth
96         self.kernel = kernel
97         self.metric = metric
98         self.atol = atol
99         self.rtol = rtol
100         self.breadth_first = breadth_first
101         self.leaf_size = leaf_size
102         self.metric_params = metric_params
103 
104         # run the choose algorithm code so that exceptions will happen here
105         # we're using clone() in the GenerativeBayes classifier,
106         # so we can't do this kind of logic in __init__
107         self._choose_algorithm(self.algorithm, self.metric)
108 
109         if bandwidth <= 0:
110             raise ValueError("bandwidth must be positive")
111         if kernel not in VALID_KERNELS:
112             raise ValueError("invalid kernel: '{0}'".format(kernel))
113 
114     def _choose_algorithm(self, algorithm, metric):
115         # given the algorithm string + metric string, choose the optimal
116         # algorithm to compute the result.
117         if algorithm == 'auto':
118             # use KD Tree if possible
119             if metric in KDTree.valid_metrics:
120                 return 'kd_tree'
121             elif metric in BallTree.valid_metrics:
122                 return 'ball_tree'
123             else:
124                 raise ValueError("invalid metric: '{0}'".format(metric))
125         elif algorithm in TREE_DICT:
126             if metric not in TREE_DICT[algorithm].valid_metrics:
127                 raise ValueError("invalid metric for {0}: "
128                                  "'{1}'".format(TREE_DICT[algorithm],
129                                                 metric))
130             return algorithm
131         else:
132             raise ValueError("invalid algorithm: '{0}'".format(algorithm))
133 
134     def fit(self, X, y=None, sample_weight=None):
135         """Fit the Kernel Density model on the data.
136 
137         Parameters
138         ----------
139         X : array_like, shape (n_samples, n_features)
140             List of n_features-dimensional data points.  Each row
141             corresponds to a single data point.
142         y : None
143             Ignored. This parameter exists only for compatibility with
144             :class:`sklearn.pipeline.Pipeline`.
145         sample_weight : array_like, shape (n_samples,), optional
146             List of sample weights attached to the data X.
147 
148         Returns
149         -------
150         self : object
151             Returns instance of object.
152         """
153         algorithm = self._choose_algorithm(self.algorithm, self.metric)
154         X = check_array(X, order='C', dtype=DTYPE)
155 
156         if sample_weight is not None:
157             sample_weight = check_array(sample_weight, order='C', dtype=DTYPE,
158                                         ensure_2d=False)
159             if sample_weight.ndim != 1:
160                 raise ValueError("the shape of sample_weight must be ({0},),"
161                                  " but was {1}".format(X.shape[0],
162                                                        sample_weight.shape))
163             check_consistent_length(X, sample_weight)
164             if sample_weight.min() <= 0:
165                 raise ValueError("sample_weight must have positive values")
166 
167         kwargs = self.metric_params
168         if kwargs is None:
169             kwargs = {}
170         self.tree_ = TREE_DICT[algorithm](X, metric=self.metric,
171                                           leaf_size=self.leaf_size,
172                                           sample_weight=sample_weight,
173                                           **kwargs)
174         return self
175 
176     def score_samples(self, X):
177         """Evaluate the log density model on the data.
178 
179         Parameters
180         ----------
181         X : array_like, shape (n_samples, n_features)
182             An array of points to query.  Last dimension should match dimension
183             of training data (n_features).
184 
185         Returns
186         -------
187         density : ndarray, shape (n_samples,)
188             The array of log(density) evaluations. These are normalized to be
189             probability densities, so values will be low for high-dimensional
190             data.
191         """
192         # The returned density is normalized to the number of points.
193         # For it to be a probability, we must scale it.  For this reason
194         # we'll also scale atol.
195         X = check_array(X, order='C', dtype=DTYPE)
196         if self.tree_.sample_weight is None:
197             N = self.tree_.data.shape[0]
198         else:
199             N = self.tree_.sum_weight
200         atol_N = self.atol * N
201         log_density = self.tree_.kernel_density(
202             X, h=self.bandwidth, kernel=self.kernel, atol=atol_N,
203             rtol=self.rtol, breadth_first=self.breadth_first, return_log=True)
204         log_density -= np.log(N)
205         return log_density
206 
207     def score(self, X, y=None):
208         """Compute the total log probability density under the model.
209 
210         Parameters
211         ----------
212         X : array_like, shape (n_samples, n_features)
213             List of n_features-dimensional data points.  Each row
214             corresponds to a single data point.
215         y : None
216             Ignored. This parameter exists only for compatibility with
217             :class:`sklearn.pipeline.Pipeline`.
218 
219         Returns
220         -------
221         logprob : float
222             Total log-likelihood of the data in X. This is normalized to be a
223             probability density, so the value will be low for high-dimensional
224             data.
225         """
226         return np.sum(self.score_samples(X))
227 
228     def sample(self, n_samples=1, random_state=None):
229         """Generate random samples from the model.
230 
231         Currently, this is implemented only for gaussian and tophat kernels.
232 
233         Parameters
234         ----------
235         n_samples : int, optional
236             Number of samples to generate. Defaults to 1.
237 
238         random_state : int, RandomState instance or None. default to None
239             If int, random_state is the seed used by the random number
240             generator; If RandomState instance, random_state is the random
241             number generator; If None, the random number generator is the
242             RandomState instance used by `np.random`.
243 
244         Returns
245         -------
246         X : array_like, shape (n_samples, n_features)
247             List of samples.
248         """
249         # TODO: implement sampling for other valid kernel shapes
250         if self.kernel not in ['gaussian', 'tophat']:
251             raise NotImplementedError()
252 
253         data = np.asarray(self.tree_.data)
254 
255         rng = check_random_state(random_state)
256         u = rng.uniform(0, 1, size=n_samples)
257         if self.tree_.sample_weight is None:
258             i = (u * data.shape[0]).astype(np.int64)
259         else:
260             cumsum_weight = np.cumsum(np.asarray(self.tree_.sample_weight))
261             sum_weight = cumsum_weight[-1]
262             i = np.searchsorted(cumsum_weight, u * sum_weight)
263         if self.kernel == 'gaussian':
264             return np.atleast_2d(rng.normal(data[i], self.bandwidth))
265 
266         elif self.kernel == 'tophat':
267             # we first draw points from a d-dimensional normal distribution,
268             # then use an incomplete gamma function to map them to a uniform
269             # d-dimensional tophat distribution.
270             dim = data.shape[1]
271             X = rng.normal(size=(n_samples, dim))
272             s_sq = row_norms(X, squared=True)
273             correction = (gammainc(0.5 * dim, 0.5 * s_sq) ** (1. / dim)
274                           * self.bandwidth / np.sqrt(s_sq))
275             return data[i] + X * correction[:, np.newaxis]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/tree/_classes.py</file>
<class>BaseDecisionTree</class>
<method>fit</method>
<code>
129     def fit(self, X, y, sample_weight=None, check_input=True,
130             X_idx_sorted=None):
131 
132         random_state = check_random_state(self.random_state)
133 
134         if self.ccp_alpha < 0.0:
135             raise ValueError("ccp_alpha must be greater than or equal to 0")
136 
137         if check_input:
138             X = check_array(X, dtype=DTYPE, accept_sparse="csc")
139             y = check_array(y, ensure_2d=False, dtype=None)
140             if issparse(X):
141                 X.sort_indices()
142 
143                 if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
144                     raise ValueError("No support for np.int64 index based "
145                                      "sparse matrices")
146 
147         # Determine output settings
148         n_samples, self.n_features_ = X.shape
149         is_classification = is_classifier(self)
150 
151         y = np.atleast_1d(y)
152         expanded_class_weight = None
153 
154         if y.ndim == 1:
155             # reshape is necessary to preserve the data contiguity against vs
156             # [:, np.newaxis] that does not.
157             y = np.reshape(y, (-1, 1))
158 
159         self.n_outputs_ = y.shape[1]
160 
161         if is_classification:
162             check_classification_targets(y)
163             y = np.copy(y)
164 
165             self.classes_ = []
166             self.n_classes_ = []
167 
168             if self.class_weight is not None:
169                 y_original = np.copy(y)
170 
171             y_encoded = np.zeros(y.shape, dtype=np.int)
172             for k in range(self.n_outputs_):
173                 classes_k, y_encoded[:, k] = np.unique(y[:, k],
174                                                        return_inverse=True)
175                 self.classes_.append(classes_k)
176                 self.n_classes_.append(classes_k.shape[0])
177             y = y_encoded
178 
179             if self.class_weight is not None:
180                 expanded_class_weight = compute_sample_weight(
181                     self.class_weight, y_original)
182 
183             self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)
184 
185         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
186             y = np.ascontiguousarray(y, dtype=DOUBLE)
187 
188         # Check parameters
189         max_depth = ((2 ** 31) - 1 if self.max_depth is None
190                      else self.max_depth)
191         max_leaf_nodes = (-1 if self.max_leaf_nodes is None
192                           else self.max_leaf_nodes)
193 
194         if isinstance(self.min_samples_leaf, numbers.Integral):
195             if not 1 <= self.min_samples_leaf:
196                 raise ValueError("min_samples_leaf must be at least 1 "
197                                  "or in (0, 0.5], got %s"
198                                  % self.min_samples_leaf)
199             min_samples_leaf = self.min_samples_leaf
200         else:  # float
201             if not 0. < self.min_samples_leaf <= 0.5:
202                 raise ValueError("min_samples_leaf must be at least 1 "
203                                  "or in (0, 0.5], got %s"
204                                  % self.min_samples_leaf)
205             min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
206 
207         if isinstance(self.min_samples_split, numbers.Integral):
208             if not 2 <= self.min_samples_split:
209                 raise ValueError("min_samples_split must be an integer "
210                                  "greater than 1 or a float in (0.0, 1.0]; "
211                                  "got the integer %s"
212                                  % self.min_samples_split)
213             min_samples_split = self.min_samples_split
214         else:  # float
215             if not 0. < self.min_samples_split <= 1.:
216                 raise ValueError("min_samples_split must be an integer "
217                                  "greater than 1 or a float in (0.0, 1.0]; "
218                                  "got the float %s"
219                                  % self.min_samples_split)
220             min_samples_split = int(ceil(self.min_samples_split * n_samples))
221             min_samples_split = max(2, min_samples_split)
222 
223         min_samples_split = max(min_samples_split, 2 * min_samples_leaf)
224 
225         if isinstance(self.max_features, str):
226             if self.max_features == "auto":
227                 if is_classification:
228                     max_features = max(1, int(np.sqrt(self.n_features_)))
229                 else:
230                     max_features = self.n_features_
231             elif self.max_features == "sqrt":
232                 max_features = max(1, int(np.sqrt(self.n_features_)))
233             elif self.max_features == "log2":
234                 max_features = max(1, int(np.log2(self.n_features_)))
235             else:
236                 raise ValueError(
237                     'Invalid value for max_features. Allowed string '
238                     'values are "auto", "sqrt" or "log2".')
239         elif self.max_features is None:
240             max_features = self.n_features_
241         elif isinstance(self.max_features, numbers.Integral):
242             max_features = self.max_features
243         else:  # float
244             if self.max_features > 0.0:
245                 max_features = max(1,
246                                    int(self.max_features * self.n_features_))
247             else:
248                 max_features = 0
249 
250         self.max_features_ = max_features
251 
252         if len(y) != n_samples:
253             raise ValueError("Number of labels=%d does not match "
254                              "number of samples=%d" % (len(y), n_samples))
255         if not 0 <= self.min_weight_fraction_leaf <= 0.5:
256             raise ValueError("min_weight_fraction_leaf must in [0, 0.5]")
257         if max_depth <= 0:
258             raise ValueError("max_depth must be greater than zero. ")
259         if not (0 < max_features <= self.n_features_):
260             raise ValueError("max_features must be in (0, n_features]")
261         if not isinstance(max_leaf_nodes, numbers.Integral):
262             raise ValueError("max_leaf_nodes must be integral number but was "
263                              "%r" % max_leaf_nodes)
264         if -1 < max_leaf_nodes < 2:
265             raise ValueError(("max_leaf_nodes {0} must be either None "
266                               "or larger than 1").format(max_leaf_nodes))
267 
268         if sample_weight is not None:
269             if (getattr(sample_weight, "dtype", None) != DOUBLE or
270                     not sample_weight.flags.contiguous):
271                 sample_weight = np.ascontiguousarray(
272                     sample_weight, dtype=DOUBLE)
273             if len(sample_weight.shape) > 1:
274                 raise ValueError("Sample weights array has more "
275                                  "than one dimension: %d" %
276                                  len(sample_weight.shape))
277             if len(sample_weight) != n_samples:
278                 raise ValueError("Number of weights=%d does not match "
279                                  "number of samples=%d" %
280                                  (len(sample_weight), n_samples))
281 
282         if expanded_class_weight is not None:
283             if sample_weight is not None:
284                 sample_weight = sample_weight * expanded_class_weight
285             else:
286                 sample_weight = expanded_class_weight
287 
288         # Set min_weight_leaf from min_weight_fraction_leaf
289         if sample_weight is None:
290             min_weight_leaf = (self.min_weight_fraction_leaf *
291                                n_samples)
292         else:
293             min_weight_leaf = (self.min_weight_fraction_leaf *
294                                np.sum(sample_weight))
295 
296         if self.min_impurity_split is not None:
297             warnings.warn("The min_impurity_split parameter is deprecated. "
298                           "Its default value will change from 1e-7 to 0 in "
299                           "version 0.23, and it will be removed in 0.25. "
300                           "Use the min_impurity_decrease parameter instead.",
301                           FutureWarning)
302             min_impurity_split = self.min_impurity_split
303         else:
304             min_impurity_split = 1e-7
305 
306         if min_impurity_split < 0.:
307             raise ValueError("min_impurity_split must be greater than "
308                              "or equal to 0")
309 
310         if self.min_impurity_decrease < 0.:
311             raise ValueError("min_impurity_decrease must be greater than "
312                              "or equal to 0")
313 
314         if self.presort != 'deprecated':
315             warnings.warn("The parameter 'presort' is deprecated and has no "
316                           "effect. It will be removed in v0.24. You can "
317                           "suppress this warning by not passing any value "
318                           "to the 'presort' parameter.",
319                           FutureWarning)
320 
321         # Build tree
322         criterion = self.criterion
323         if not isinstance(criterion, Criterion):
324             if is_classification:
325                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,
326                                                          self.n_classes_)
327             else:
328                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_,
329                                                          n_samples)
330 
331         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS
332 
333         splitter = self.splitter
334         if not isinstance(self.splitter, Splitter):
335             splitter = SPLITTERS[self.splitter](criterion,
336                                                 self.max_features_,
337                                                 min_samples_leaf,
338                                                 min_weight_leaf,
339                                                 random_state)
340 
341         if is_classifier(self):
342             self.tree_ = Tree(self.n_features_,
343                               self.n_classes_, self.n_outputs_)
344         else:
345             self.tree_ = Tree(self.n_features_,
346                               # TODO: tree should't need this in this case
347                               np.array([1] * self.n_outputs_, dtype=np.intp),
348                               self.n_outputs_)
349 
350         # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise
351         if max_leaf_nodes < 0:
352             builder = DepthFirstTreeBuilder(splitter, min_samples_split,
353                                             min_samples_leaf,
354                                             min_weight_leaf,
355                                             max_depth,
356                                             self.min_impurity_decrease,
357                                             min_impurity_split)
358         else:
359             builder = BestFirstTreeBuilder(splitter, min_samples_split,
360                                            min_samples_leaf,
361                                            min_weight_leaf,
362                                            max_depth,
363                                            max_leaf_nodes,
364                                            self.min_impurity_decrease,
365                                            min_impurity_split)
366 
367         builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
368 
369         if self.n_outputs_ == 1 and is_classifier(self):
370             self.n_classes_ = self.n_classes_[0]
371             self.classes_ = self.classes_[0]
372 
373         self._prune_tree()
374 
375         return self

</code><intended_behavior>Update the `fit` method to use `_check_sample_weight` for `sample_weight` validation, standardizing the way sample weights are validated across different parts of the project.</intended_behavior>, <file>sklearn/tree/_classes.py</file>
<class>BaseDecisionTree</class>
<method>None</method>
<code>
76 class BaseDecisionTree(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):
77     """Base class for decision trees.
78 
79     Warning: This class should not be used directly.
80     Use derived classes instead.
81     """
82 
83     @abstractmethod
84     def __init__(self,
85                  criterion,
86                  splitter,
87                  max_depth,
88                  min_samples_split,
89                  min_samples_leaf,
90                  min_weight_fraction_leaf,
91                  max_features,
92                  max_leaf_nodes,
93                  random_state,
94                  min_impurity_decrease,
95                  min_impurity_split,
96                  class_weight=None,
97                  presort='deprecated',
98                  ccp_alpha=0.0):
99         self.criterion = criterion
100         self.splitter = splitter
101         self.max_depth = max_depth
102         self.min_samples_split = min_samples_split
103         self.min_samples_leaf = min_samples_leaf
104         self.min_weight_fraction_leaf = min_weight_fraction_leaf
105         self.max_features = max_features
106         self.random_state = random_state
107         self.max_leaf_nodes = max_leaf_nodes
108         self.min_impurity_decrease = min_impurity_decrease
109         self.min_impurity_split = min_impurity_split
110         self.class_weight = class_weight
111         self.presort = presort
112         self.ccp_alpha = ccp_alpha
113 
114     def get_depth(self):
115         """Returns the depth of the decision tree.
116 
117         The depth of a tree is the maximum distance between the root
118         and any leaf.
119         """
120         check_is_fitted(self)
121         return self.tree_.max_depth
122 
123     def get_n_leaves(self):
124         """Returns the number of leaves of the decision tree.
125         """
126         check_is_fitted(self)
127         return self.tree_.n_leaves
128 
129     def fit(self, X, y, sample_weight=None, check_input=True,
130             X_idx_sorted=None):
131 
132         random_state = check_random_state(self.random_state)
133 
134         if self.ccp_alpha < 0.0:
135             raise ValueError("ccp_alpha must be greater than or equal to 0")
136 
137         if check_input:
138             X = check_array(X, dtype=DTYPE, accept_sparse="csc")
139             y = check_array(y, ensure_2d=False, dtype=None)
140             if issparse(X):
141                 X.sort_indices()
142 
143                 if X.indices.dtype != np.intc or X.indptr.dtype != np.intc:
144                     raise ValueError("No support for np.int64 index based "
145                                      "sparse matrices")
146 
147         # Determine output settings
148         n_samples, self.n_features_ = X.shape
149         is_classification = is_classifier(self)
150 
151         y = np.atleast_1d(y)
152         expanded_class_weight = None
153 
154         if y.ndim == 1:
155             # reshape is necessary to preserve the data contiguity against vs
156             # [:, np.newaxis] that does not.
157             y = np.reshape(y, (-1, 1))
158 
159         self.n_outputs_ = y.shape[1]
160 
161         if is_classification:
162             check_classification_targets(y)
163             y = np.copy(y)
164 
165             self.classes_ = []
166             self.n_classes_ = []
167 
168             if self.class_weight is not None:
169                 y_original = np.copy(y)
170 
171             y_encoded = np.zeros(y.shape, dtype=np.int)
172             for k in range(self.n_outputs_):
173                 classes_k, y_encoded[:, k] = np.unique(y[:, k],
174                                                        return_inverse=True)
175                 self.classes_.append(classes_k)
176                 self.n_classes_.append(classes_k.shape[0])
177             y = y_encoded
178 
179             if self.class_weight is not None:
180                 expanded_class_weight = compute_sample_weight(
181                     self.class_weight, y_original)
182 
183             self.n_classes_ = np.array(self.n_classes_, dtype=np.intp)
184 
185         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
186             y = np.ascontiguousarray(y, dtype=DOUBLE)
187 
188         # Check parameters
189         max_depth = ((2 ** 31) - 1 if self.max_depth is None
190                      else self.max_depth)
191         max_leaf_nodes = (-1 if self.max_leaf_nodes is None
192                           else self.max_leaf_nodes)
193 
194         if isinstance(self.min_samples_leaf, numbers.Integral):
195             if not 1 <= self.min_samples_leaf:
196                 raise ValueError("min_samples_leaf must be at least 1 "
197                                  "or in (0, 0.5], got %s"
198                                  % self.min_samples_leaf)
199             min_samples_leaf = self.min_samples_leaf
200         else:  # float
201             if not 0. < self.min_samples_leaf <= 0.5:
202                 raise ValueError("min_samples_leaf must be at least 1 "
203                                  "or in (0, 0.5], got %s"
204                                  % self.min_samples_leaf)
205             min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))
206 
207         if isinstance(self.min_samples_split, numbers.Integral):
208             if not 2 <= self.min_samples_split:
209                 raise ValueError("min_samples_split must be an integer "
210                                  "greater than 1 or a float in (0.0, 1.0]; "
211                                  "got the integer %s"
212                                  % self.min_samples_split)
213             min_samples_split = self.min_samples_split
214         else:  # float
215             if not 0. < self.min_samples_split <= 1.:
216                 raise ValueError("min_samples_split must be an integer "
217                                  "greater than 1 or a float in (0.0, 1.0]; "
218                                  "got the float %s"
219                                  % self.min_samples_split)
220             min_samples_split = int(ceil(self.min_samples_split * n_samples))
221             min_samples_split = max(2, min_samples_split)
222 
223         min_samples_split = max(min_samples_split, 2 * min_samples_leaf)
224 
225         if isinstance(self.max_features, str):
226             if self.max_features == "auto":
227                 if is_classification:
228                     max_features = max(1, int(np.sqrt(self.n_features_)))
229                 else:
230                     max_features = self.n_features_
231             elif self.max_features == "sqrt":
232                 max_features = max(1, int(np.sqrt(self.n_features_)))
233             elif self.max_features == "log2":
234                 max_features = max(1, int(np.log2(self.n_features_)))
235             else:
236                 raise ValueError(
237                     'Invalid value for max_features. Allowed string '
238                     'values are "auto", "sqrt" or "log2".')
239         elif self.max_features is None:
240             max_features = self.n_features_
241         elif isinstance(self.max_features, numbers.Integral):
242             max_features = self.max_features
243         else:  # float
244             if self.max_features > 0.0:
245                 max_features = max(1,
246                                    int(self.max_features * self.n_features_))
247             else:
248                 max_features = 0
249 
250         self.max_features_ = max_features
251 
252         if len(y) != n_samples:
253             raise ValueError("Number of labels=%d does not match "
254                              "number of samples=%d" % (len(y), n_samples))
255         if not 0 <= self.min_weight_fraction_leaf <= 0.5:
256             raise ValueError("min_weight_fraction_leaf must in [0, 0.5]")
257         if max_depth <= 0:
258             raise ValueError("max_depth must be greater than zero. ")
259         if not (0 < max_features <= self.n_features_):
260             raise ValueError("max_features must be in (0, n_features]")
261         if not isinstance(max_leaf_nodes, numbers.Integral):
262             raise ValueError("max_leaf_nodes must be integral number but was "
263                              "%r" % max_leaf_nodes)
264         if -1 < max_leaf_nodes < 2:
265             raise ValueError(("max_leaf_nodes {0} must be either None "
266                               "or larger than 1").format(max_leaf_nodes))
267 
268         if sample_weight is not None:
269             if (getattr(sample_weight, "dtype", None) != DOUBLE or
270                     not sample_weight.flags.contiguous):
271                 sample_weight = np.ascontiguousarray(
272                     sample_weight, dtype=DOUBLE)
273             if len(sample_weight.shape) > 1:
274                 raise ValueError("Sample weights array has more "
275                                  "than one dimension: %d" %
276                                  len(sample_weight.shape))
277             if len(sample_weight) != n_samples:
278                 raise ValueError("Number of weights=%d does not match "
279                                  "number of samples=%d" %
280                                  (len(sample_weight), n_samples))
281 
282         if expanded_class_weight is not None:
283             if sample_weight is not None:
284                 sample_weight = sample_weight * expanded_class_weight
285             else:
286                 sample_weight = expanded_class_weight
287 
288         # Set min_weight_leaf from min_weight_fraction_leaf
289         if sample_weight is None:
290             min_weight_leaf = (self.min_weight_fraction_leaf *
291                                n_samples)
292         else:
293             min_weight_leaf = (self.min_weight_fraction_leaf *
294                                np.sum(sample_weight))
295 
296         if self.min_impurity_split is not None:
297             warnings.warn("The min_impurity_split parameter is deprecated. "
298                           "Its default value will change from 1e-7 to 0 in "
299                           "version 0.23, and it will be removed in 0.25. "
300                           "Use the min_impurity_decrease parameter instead.",
301                           FutureWarning)
302             min_impurity_split = self.min_impurity_split
303         else:
304             min_impurity_split = 1e-7
305 
306         if min_impurity_split < 0.:
307             raise ValueError("min_impurity_split must be greater than "
308                              "or equal to 0")
309 
310         if self.min_impurity_decrease < 0.:
311             raise ValueError("min_impurity_decrease must be greater than "
312                              "or equal to 0")
313 
314         if self.presort != 'deprecated':
315             warnings.warn("The parameter 'presort' is deprecated and has no "
316                           "effect. It will be removed in v0.24. You can "
317                           "suppress this warning by not passing any value "
318                           "to the 'presort' parameter.",
319                           FutureWarning)
320 
321         # Build tree
322         criterion = self.criterion
323         if not isinstance(criterion, Criterion):
324             if is_classification:
325                 criterion = CRITERIA_CLF[self.criterion](self.n_outputs_,
326                                                          self.n_classes_)
327             else:
328                 criterion = CRITERIA_REG[self.criterion](self.n_outputs_,
329                                                          n_samples)
330 
331         SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS
332 
333         splitter = self.splitter
334         if not isinstance(self.splitter, Splitter):
335             splitter = SPLITTERS[self.splitter](criterion,
336                                                 self.max_features_,
337                                                 min_samples_leaf,
338                                                 min_weight_leaf,
339                                                 random_state)
340 
341         if is_classifier(self):
342             self.tree_ = Tree(self.n_features_,
343                               self.n_classes_, self.n_outputs_)
344         else:
345             self.tree_ = Tree(self.n_features_,
346                               # TODO: tree should't need this in this case
347                               np.array([1] * self.n_outputs_, dtype=np.intp),
348                               self.n_outputs_)
349 
350         # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise
351         if max_leaf_nodes < 0:
352             builder = DepthFirstTreeBuilder(splitter, min_samples_split,
353                                             min_samples_leaf,
354                                             min_weight_leaf,
355                                             max_depth,
356                                             self.min_impurity_decrease,
357                                             min_impurity_split)
358         else:
359             builder = BestFirstTreeBuilder(splitter, min_samples_split,
360                                            min_samples_leaf,
361                                            min_weight_leaf,
362                                            max_depth,
363                                            max_leaf_nodes,
364                                            self.min_impurity_decrease,
365                                            min_impurity_split)
366 
367         builder.build(self.tree_, X, y, sample_weight, X_idx_sorted)
368 
369         if self.n_outputs_ == 1 and is_classifier(self):
370             self.n_classes_ = self.n_classes_[0]
371             self.classes_ = self.classes_[0]
372 
373         self._prune_tree()
374 
375         return self
376 
377     def _validate_X_predict(self, X, check_input):
378         """Validate X whenever one tries to predict, apply, predict_proba"""
379         if check_input:
380             X = check_array(X, dtype=DTYPE, accept_sparse="csr")
381             if issparse(X) and (X.indices.dtype != np.intc or
382                                 X.indptr.dtype != np.intc):
383                 raise ValueError("No support for np.int64 index based "
384                                  "sparse matrices")
385 
386         n_features = X.shape[1]
387         if self.n_features_ != n_features:
388             raise ValueError("Number of features of the model must "
389                              "match the input. Model n_features is %s and "
390                              "input n_features is %s "
391                              % (self.n_features_, n_features))
392 
393         return X
394 
395     def predict(self, X, check_input=True):
396         """Predict class or regression value for X.
397 
398         For a classification model, the predicted class for each sample in X is
399         returned. For a regression model, the predicted value based on X is
400         returned.
401 
402         Parameters
403         ----------
404         X : array-like or sparse matrix of shape (n_samples, n_features)
405             The input samples. Internally, it will be converted to
406             ``dtype=np.float32`` and if a sparse matrix is provided
407             to a sparse ``csr_matrix``.
408 
409         check_input : boolean, (default=True)
410             Allow to bypass several input checking.
411             Don't use this parameter unless you know what you do.
412 
413         Returns
414         -------
415         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
416             The predicted classes, or the predict values.
417         """
418         check_is_fitted(self)
419         X = self._validate_X_predict(X, check_input)
420         proba = self.tree_.predict(X)
421         n_samples = X.shape[0]
422 
423         # Classification
424         if is_classifier(self):
425             if self.n_outputs_ == 1:
426                 return self.classes_.take(np.argmax(proba, axis=1), axis=0)
427 
428             else:
429                 class_type = self.classes_[0].dtype
430                 predictions = np.zeros((n_samples, self.n_outputs_),
431                                        dtype=class_type)
432                 for k in range(self.n_outputs_):
433                     predictions[:, k] = self.classes_[k].take(
434                         np.argmax(proba[:, k], axis=1),
435                         axis=0)
436 
437                 return predictions
438 
439         # Regression
440         else:
441             if self.n_outputs_ == 1:
442                 return proba[:, 0]
443 
444             else:
445                 return proba[:, :, 0]
446 
447     def apply(self, X, check_input=True):
448         """
449         Returns the index of the leaf that each sample is predicted as.
450 
451         .. versionadded:: 0.17
452 
453         Parameters
454         ----------
455         X : {array-like, sparse matrix} of shape (n_samples, n_features)
456             The input samples. Internally, it will be converted to
457             ``dtype=np.float32`` and if a sparse matrix is provided
458             to a sparse ``csr_matrix``.
459 
460         check_input : boolean, (default=True)
461             Allow to bypass several input checking.
462             Don't use this parameter unless you know what you do.
463 
464         Returns
465         -------
466         X_leaves : array_like, shape = [n_samples,]
467             For each datapoint x in X, return the index of the leaf x
468             ends up in. Leaves are numbered within
469             ``[0; self.tree_.node_count)``, possibly with gaps in the
470             numbering.
471         """
472         check_is_fitted(self)
473         X = self._validate_X_predict(X, check_input)
474         return self.tree_.apply(X)
475 
476     def decision_path(self, X, check_input=True):
477         """Return the decision path in the tree
478 
479         .. versionadded:: 0.18
480 
481         Parameters
482         ----------
483         X : {array-like, sparse matrix} of shape (n_samples, n_features)
484             The input samples. Internally, it will be converted to
485             ``dtype=np.float32`` and if a sparse matrix is provided
486             to a sparse ``csr_matrix``.
487 
488         check_input : boolean, (default=True)
489             Allow to bypass several input checking.
490             Don't use this parameter unless you know what you do.
491 
492         Returns
493         -------
494         indicator : sparse csr array, shape = [n_samples, n_nodes]
495             Return a node indicator matrix where non zero elements
496             indicates that the samples goes through the nodes.
497 
498         """
499         X = self._validate_X_predict(X, check_input)
500         return self.tree_.decision_path(X)
501 
502     def _prune_tree(self):
503         """Prune tree using Minimal Cost-Complexity Pruning."""
504         check_is_fitted(self)
505 
506         if self.ccp_alpha < 0.0:
507             raise ValueError("ccp_alpha must be greater than or equal to 0")
508 
509         if self.ccp_alpha == 0.0:
510             return
511 
512         # build pruned tree
513         if is_classifier(self):
514             n_classes = np.atleast_1d(self.n_classes_)
515             pruned_tree = Tree(self.n_features_, n_classes, self.n_outputs_)
516         else:
517             pruned_tree = Tree(self.n_features_,
518                                # TODO: the tree shouldn't need this param
519                                np.array([1] * self.n_outputs_, dtype=np.intp),
520                                self.n_outputs_)
521         _build_pruned_tree_ccp(pruned_tree, self.tree_, self.ccp_alpha)
522 
523         self.tree_ = pruned_tree
524 
525     def cost_complexity_pruning_path(self, X, y, sample_weight=None):
526         """Compute the pruning path during Minimal Cost-Complexity Pruning.
527 
528         See :ref:`minimal_cost_complexity_pruning` for details on the pruning
529         process.
530 
531         Parameters
532         ----------
533         X : {array-like, sparse matrix} of shape (n_samples, n_features)
534             The training input samples. Internally, it will be converted to
535             ``dtype=np.float32`` and if a sparse matrix is provided
536             to a sparse ``csc_matrix``.
537 
538         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
539             The target values (class labels) as integers or strings.
540 
541         sample_weight : array-like of shape (n_samples,), default=None
542             Sample weights. If None, then samples are equally weighted. Splits
543             that would create child nodes with net zero or negative weight are
544             ignored while searching for a split in each node. Splits are also
545             ignored if they would result in any single class carrying a
546             negative weight in either child node.
547 
548         Returns
549         -------
550         ccp_path : Bunch
551             Dictionary-like object, with attributes:
552 
553             ccp_alphas : ndarray
554                 Effective alphas of subtree during pruning.
555 
556             impurities : ndarray
557                 Sum of the impurities of the subtree leaves for the
558                 corresponding alpha value in ``ccp_alphas``.
559         """
560         est = clone(self).set_params(ccp_alpha=0.0)
561         est.fit(X, y, sample_weight=sample_weight)
562         return Bunch(**ccp_pruning_path(est.tree_))
563 
564     @property
565     def feature_importances_(self):
566         """Return the feature importances.
567 
568         The importance of a feature is computed as the (normalized) total
569         reduction of the criterion brought by that feature.
570         It is also known as the Gini importance.
571 
572         Returns
573         -------
574         feature_importances_ : array, shape = [n_features]
575         """
576         check_is_fitted(self)
577 
578         return self.tree_.compute_feature_importances()

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 23:56:21.989 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 23:56:21.989 | INFO     | Search-only: skipping patch selection
2025-10-15 23:56:22.064 | INFO     | Task scikit-learn__scikit-learn-15495 completed successfully.
2025-10-15 23:56:22.065 | INFO     | No patch generated. You can try running ACR again.
