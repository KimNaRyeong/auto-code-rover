[
    {
        "rel_file_path": "sklearn/linear_model/huber.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/linear_model/huber.py",
        "start": 234,
        "end": 306,
        "class_name": "HuberRegressor",
        "method_name": "fit",
        "code": "234     def fit(self, X, y, sample_weight=None):\n235         \"\"\"Fit the model according to the given training data.\n236 \n237         Parameters\n238         ----------\n239         X : array-like, shape (n_samples, n_features)\n240             Training vector, where n_samples in the number of samples and\n241             n_features is the number of features.\n242 \n243         y : array-like, shape (n_samples,)\n244             Target vector relative to X.\n245 \n246         sample_weight : array-like, shape (n_samples,)\n247             Weight given to each sample.\n248 \n249         Returns\n250         -------\n251         self : object\n252         \"\"\"\n253         X, y = check_X_y(\n254             X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n255         if sample_weight is not None:\n256             sample_weight = np.array(sample_weight)\n257             check_consistent_length(y, sample_weight)\n258         else:\n259             sample_weight = np.ones_like(y)\n260 \n261         if self.epsilon < 1.0:\n262             raise ValueError(\n263                 \"epsilon should be greater than or equal to 1.0, got %f\"\n264                 % self.epsilon)\n265 \n266         if self.warm_start and hasattr(self, 'coef_'):\n267             parameters = np.concatenate(\n268                 (self.coef_, [self.intercept_, self.scale_]))\n269         else:\n270             if self.fit_intercept:\n271                 parameters = np.zeros(X.shape[1] + 2)\n272             else:\n273                 parameters = np.zeros(X.shape[1] + 1)\n274             # Make sure to initialize the scale parameter to a strictly\n275             # positive value:\n276             parameters[-1] = 1\n277 \n278         # Sigma or the scale factor should be non-negative.\n279         # Setting it to be zero might cause undefined bounds hence we set it\n280         # to a value close to zero.\n281         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))\n282         bounds[-1][0] = np.finfo(np.float64).eps * 10\n283 \n284         parameters, f, dict_ = optimize.fmin_l_bfgs_b(\n285             _huber_loss_and_gradient, parameters,\n286             args=(X, y, self.epsilon, self.alpha, sample_weight),\n287             maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,\n288             iprint=0)\n289         if dict_['warnflag'] == 2:\n290             raise ValueError(\"HuberRegressor convergence failed:\"\n291                              \" l-BFGS-b solver terminated with %s\"\n292                              % dict_['task'].decode('ascii'))\n293         # In scipy <= 1.0.0, nit may exceed maxiter.\n294         # See https://github.com/scipy/scipy/issues/7854.\n295         self.n_iter_ = min(dict_['nit'], self.max_iter)\n296         self.scale_ = parameters[-1]\n297         if self.fit_intercept:\n298             self.intercept_ = parameters[-2]\n299         else:\n300             self.intercept_ = 0.0\n301         self.coef_ = parameters[:X.shape[1]]\n302 \n303         residual = np.abs(\n304             y - safe_sparse_dot(X, self.coef_) - self.intercept_)\n305         self.outliers_ = residual > self.scale_ * self.epsilon\n306         return self\n",
        "intended_behavior": "Before the method processes the input X for fitting, there should be an additional check or conversion for boolean input arrays. If X is found to be of boolean dtype, it should be automatically converted to float to preserve compatibility with the optimization routines that fit eventually delegates to. This conversion ensures that the method can handle boolean predictors by transforming them into a numeric format that is expected by the subsequent processing steps, thus preventing the TypeError. This conversion step should be placed after the input validation checks (check_X_y) to ensure that all inputs are validated before any transformation. This behavior should also be documented in the method's docstring to inform users of the automatic type conversion."
    },
    {
        "rel_file_path": "sklearn/linear_model/huber.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/linear_model/huber.py",
        "start": 124,
        "end": 306,
        "class_name": "HuberRegressor",
        "method_name": null,
        "code": "124 class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):\n125     \"\"\"Linear regression model that is robust to outliers.\n126 \n127     The Huber Regressor optimizes the squared loss for the samples where\n128     ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n129     where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n130     to be optimized. The parameter sigma makes sure that if y is scaled up\n131     or down by a certain factor, one does not need to rescale epsilon to\n132     achieve the same robustness. Note that this does not take into account\n133     the fact that the different features of X may be of different scales.\n134 \n135     This makes sure that the loss function is not heavily influenced by the\n136     outliers while not completely ignoring their effect.\n137 \n138     Read more in the :ref:`User Guide <huber_regression>`\n139 \n140     .. versionadded:: 0.18\n141 \n142     Parameters\n143     ----------\n144     epsilon : float, greater than 1.0, default 1.35\n145         The parameter epsilon controls the number of samples that should be\n146         classified as outliers. The smaller the epsilon, the more robust it is\n147         to outliers.\n148 \n149     max_iter : int, default 100\n150         Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b\n151         should run for.\n152 \n153     alpha : float, default 0.0001\n154         Regularization parameter.\n155 \n156     warm_start : bool, default False\n157         This is useful if the stored attributes of a previously used model\n158         has to be reused. If set to False, then the coefficients will\n159         be rewritten for every call to fit.\n160         See :term:`the Glossary <warm_start>`.\n161 \n162     fit_intercept : bool, default True\n163         Whether or not to fit the intercept. This can be set to False\n164         if the data is already centered around the origin.\n165 \n166     tol : float, default 1e-5\n167         The iteration will stop when\n168         ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n169         where pg_i is the i-th component of the projected gradient.\n170 \n171     Attributes\n172     ----------\n173     coef_ : array, shape (n_features,)\n174         Features got by optimizing the Huber loss.\n175 \n176     intercept_ : float\n177         Bias.\n178 \n179     scale_ : float\n180         The value by which ``|y - X'w - c|`` is scaled down.\n181 \n182     n_iter_ : int\n183         Number of iterations that fmin_l_bfgs_b has run for.\n184 \n185         .. versionchanged:: 0.20\n186 \n187             In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n188             ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n189 \n190     outliers_ : array, shape (n_samples,)\n191         A boolean mask which is set to True where the samples are identified\n192         as outliers.\n193 \n194     Examples\n195     --------\n196     >>> import numpy as np\n197     >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n198     >>> from sklearn.datasets import make_regression\n199     >>> rng = np.random.RandomState(0)\n200     >>> X, y, coef = make_regression(\n201     ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n202     >>> X[:4] = rng.uniform(10, 20, (4, 2))\n203     >>> y[:4] = rng.uniform(10, 20, 4)\n204     >>> huber = HuberRegressor().fit(X, y)\n205     >>> huber.score(X, y) # doctest: +ELLIPSIS\n206     -7.284608623514573\n207     >>> huber.predict(X[:1,])\n208     array([806.7200...])\n209     >>> linear = LinearRegression().fit(X, y)\n210     >>> print(\"True coefficients:\", coef)\n211     True coefficients: [20.4923...  34.1698...]\n212     >>> print(\"Huber coefficients:\", huber.coef_)\n213     Huber coefficients: [17.7906... 31.0106...]\n214     >>> print(\"Linear Regression coefficients:\", linear.coef_)\n215     Linear Regression coefficients: [-1.9221...  7.0226...]\n216 \n217     References\n218     ----------\n219     .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n220            Concomitant scale estimates, pg 172\n221     .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n222            https://statweb.stanford.edu/~owen/reports/hhu.pdf\n223     \"\"\"\n224 \n225     def __init__(self, epsilon=1.35, max_iter=100, alpha=0.0001,\n226                  warm_start=False, fit_intercept=True, tol=1e-05):\n227         self.epsilon = epsilon\n228         self.max_iter = max_iter\n229         self.alpha = alpha\n230         self.warm_start = warm_start\n231         self.fit_intercept = fit_intercept\n232         self.tol = tol\n233 \n234     def fit(self, X, y, sample_weight=None):\n235         \"\"\"Fit the model according to the given training data.\n236 \n237         Parameters\n238         ----------\n239         X : array-like, shape (n_samples, n_features)\n240             Training vector, where n_samples in the number of samples and\n241             n_features is the number of features.\n242 \n243         y : array-like, shape (n_samples,)\n244             Target vector relative to X.\n245 \n246         sample_weight : array-like, shape (n_samples,)\n247             Weight given to each sample.\n248 \n249         Returns\n250         -------\n251         self : object\n252         \"\"\"\n253         X, y = check_X_y(\n254             X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n255         if sample_weight is not None:\n256             sample_weight = np.array(sample_weight)\n257             check_consistent_length(y, sample_weight)\n258         else:\n259             sample_weight = np.ones_like(y)\n260 \n261         if self.epsilon < 1.0:\n262             raise ValueError(\n263                 \"epsilon should be greater than or equal to 1.0, got %f\"\n264                 % self.epsilon)\n265 \n266         if self.warm_start and hasattr(self, 'coef_'):\n267             parameters = np.concatenate(\n268                 (self.coef_, [self.intercept_, self.scale_]))\n269         else:\n270             if self.fit_intercept:\n271                 parameters = np.zeros(X.shape[1] + 2)\n272             else:\n273                 parameters = np.zeros(X.shape[1] + 1)\n274             # Make sure to initialize the scale parameter to a strictly\n275             # positive value:\n276             parameters[-1] = 1\n277 \n278         # Sigma or the scale factor should be non-negative.\n279         # Setting it to be zero might cause undefined bounds hence we set it\n280         # to a value close to zero.\n281         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))\n282         bounds[-1][0] = np.finfo(np.float64).eps * 10\n283 \n284         parameters, f, dict_ = optimize.fmin_l_bfgs_b(\n285             _huber_loss_and_gradient, parameters,\n286             args=(X, y, self.epsilon, self.alpha, sample_weight),\n287             maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,\n288             iprint=0)\n289         if dict_['warnflag'] == 2:\n290             raise ValueError(\"HuberRegressor convergence failed:\"\n291                              \" l-BFGS-b solver terminated with %s\"\n292                              % dict_['task'].decode('ascii'))\n293         # In scipy <= 1.0.0, nit may exceed maxiter.\n294         # See https://github.com/scipy/scipy/issues/7854.\n295         self.n_iter_ = min(dict_['nit'], self.max_iter)\n296         self.scale_ = parameters[-1]\n297         if self.fit_intercept:\n298             self.intercept_ = parameters[-2]\n299         else:\n300             self.intercept_ = 0.0\n301         self.coef_ = parameters[:X.shape[1]]\n302 \n303         residual = np.abs(\n304             y - safe_sparse_dot(X, self.coef_) - self.intercept_)\n305         self.outliers_ = residual > self.scale_ * self.epsilon\n306         return self\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]