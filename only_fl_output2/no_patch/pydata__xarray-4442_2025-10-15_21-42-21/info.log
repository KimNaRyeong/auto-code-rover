2025-10-15 21:42:21.297 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 21:42:22.905 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=751, output_tokens=12, cost=0.007870
2025-10-15 21:42:30.801 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=931, output_tokens=283, cost=0.017800
2025-10-15 21:42:32.555 | INFO     | registered non reproducing test 0
2025-10-15 21:42:45.702 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2057, output_tokens=489, cost=0.035240
2025-10-15 21:42:47.417 | INFO     | registered non reproducing test 1
2025-10-15 21:43:11.150 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2263, output_tokens=603, cost=0.040720
2025-10-15 21:43:12.854 | INFO     | registered non reproducing test 2
2025-10-15 21:43:12.854 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 21:43:12.860 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:43:21.006 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1048, output_tokens=258, cost=0.018220
2025-10-15 21:43:21.012 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:43:23.601 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=612, output_tokens=64, cost=0.008040
2025-10-15 21:43:23.603 | DEBUG    | Extracted a valid json.
2025-10-15 21:43:23.603 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DataArray\")",
        "search_method_in_class(\"to_series\", \"DataArray\")",
        "search_method_in_class(\"to_dataframe\", \"DataArray\")",
        "search_code(\"MultiIndex\")"
    ],
    "bug_locations": []
}
2025-10-15 21:43:23.743 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 21:43:23.744 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 21:43:52.017 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6768, output_tokens=872, cost=0.093840
2025-10-15 21:43:52.034 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:44:00.584 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7856, output_tokens=302, cost=0.087620
2025-10-15 21:44:00.591 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:44:08.033 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=656, output_tokens=279, cost=0.014930
2025-10-15 21:44:08.034 | DEBUG    | Extracted a valid json.
2025-10-15 21:44:08.034 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "to_series",
            "intended_behavior": "This method should be enhanced to correctly handle `DataArray` objects with both stacked (multi-indexed) and other dimensions, ensuring the conversion process accurately reflects a combined `MultiIndex` that incorporates both the stacked dimensions and any additional dimensions. The method should recognize and properly configure these dimensions into the resulting `pandas.Series` index, preserving the logical structure and relationships of the original data dimensions."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "to_dataframe",
            "intended_behavior": "Similar to `to_series`, `to_dataframe` needs to be adapted to manage `DataArray` objects that feature a mix of multi-indexed and regular dimensions, ensuring a seamless conversion into a `pandas.DataFrame`. The DataFrame's index should be a `MultiIndex` that accurately represents all dimensions involved, including the handling of any stacked dimensions alongside standard dimensions. This enhancement should ensure that the dimensionality of the original `DataArray` is fully and correctly transitioned into the DataFrame structure without loss of information or hierarchical integrity."
        }
    ]
}
2025-10-15 21:44:08.068 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>to_series</method>
<code>
2683     def to_series(self) -> pd.Series:
2684         """Convert this array into a pandas.Series.
2685 
2686         The Series is indexed by the Cartesian product of index coordinates
2687         (in the form of a :py:class:`pandas.MultiIndex`).
2688         """
2689         index = self.coords.to_index()
2690         return pd.Series(self.values.reshape(-1), index=index, name=self.name)

</code><intended_behavior>This method should be enhanced to correctly handle `DataArray` objects with both stacked (multi-indexed) and other dimensions, ensuring the conversion process accurately reflects a combined `MultiIndex` that incorporates both the stacked dimensions and any additional dimensions. The method should recognize and properly configure these dimensions into the resulting `pandas.Series` index, preserving the logical structure and relationships of the original data dimensions.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
221 class DataArray(AbstractArray, DataWithCoords):
222     """N-dimensional array with labeled coordinates and dimensions.
223 
224     DataArray provides a wrapper around numpy ndarrays that uses
225     labeled dimensions and coordinates to support metadata aware
226     operations. The API is similar to that for the pandas Series or
227     DataFrame, but DataArray objects can have any number of dimensions,
228     and their contents have fixed data types.
229 
230     Additional features over raw numpy arrays:
231 
232     - Apply operations over dimensions by name: ``x.sum('time')``.
233     - Select or assign values by integer location (like numpy):
234       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
235       ``x.sel(time='2014-01-01')``.
236     - Mathematical operations (e.g., ``x - y``) vectorize across
237       multiple dimensions (known in numpy as "broadcasting") based on
238       dimension names, regardless of their original order.
239     - Keep track of arbitrary metadata in the form of a Python
240       dictionary: ``x.attrs``
241     - Convert to a pandas Series: ``x.to_series()``.
242 
243     Getting items from or doing mathematical operations with a
244     DataArray always returns another DataArray.
245 
246     Parameters
247     ----------
248     data : array_like
249         Values for this array. Must be an ``numpy.ndarray``, ndarray
250         like, or castable to an ``ndarray``. If a self-described xarray
251         or pandas object, attempts are made to use this array's
252         metadata to fill in other unspecified arguments. A view of the
253         array's data is used instead of a copy if possible.
254     coords : sequence or dict of array_like, optional
255         Coordinates (tick labels) to use for indexing along each
256         dimension. The following notations are accepted:
257 
258         - mapping {dimension name: array-like}
259         - sequence of tuples that are valid arguments for
260           ``xarray.Variable()``
261           - (dims, data)
262           - (dims, data, attrs)
263           - (dims, data, attrs, encoding)
264 
265         Additionally, it is possible to define a coord whose name
266         does not match the dimension name, or a coord based on multiple
267         dimensions, with one of the following notations:
268 
269         - mapping {coord name: DataArray}
270         - mapping {coord name: Variable}
271         - mapping {coord name: (dimension name, array-like)}
272         - mapping {coord name: (tuple of dimension names, array-like)}
273 
274     dims : hashable or sequence of hashable, optional
275         Name(s) of the data dimension(s). Must be either a hashable
276         (only for 1D data) or a sequence of hashables with length equal
277         to the number of dimensions. If this argument is omitted,
278         dimension names default to ``['dim_0', ... 'dim_n']``.
279     name : str or None, optional
280         Name of this array.
281     attrs : dict_like or None, optional
282         Attributes to assign to the new instance. By default, an empty
283         attribute dictionary is initialized.
284 
285     Examples
286     --------
287     Create data:
288 
289     >>> np.random.seed(0)
290     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
291     >>> precipitation = 10 * np.random.rand(2, 2, 3)
292     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
293     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
294     >>> time = pd.date_range("2014-09-06", periods=3)
295     >>> reference_time = pd.Timestamp("2014-09-05")
296 
297     Initialize a dataarray with multiple dimensions:
298 
299     >>> da = xr.DataArray(
300     ...     data=temperature,
301     ...     dims=["x", "y", "time"],
302     ...     coords=dict(
303     ...         lon=(["x", "y"], lon),
304     ...         lat=(["x", "y"], lat),
305     ...         time=time,
306     ...         reference_time=reference_time,
307     ...     ),
308     ...     attrs=dict(
309     ...         description="Ambient temperature.",
310     ...         units="degC",
311     ...     ),
312     ... )
313     >>> da
314     <xarray.DataArray (x: 2, y: 2, time: 3)>
315     array([[[29.11241877, 18.20125767, 22.82990387],
316             [32.92714559, 29.94046392,  7.18177696]],
317     <BLANKLINE>
318            [[22.60070734, 13.78914233, 14.17424919],
319             [18.28478802, 16.15234857, 26.63418806]]])
320     Coordinates:
321         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
322         lat             (x, y) float64 42.25 42.21 42.63 42.59
323       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
324         reference_time  datetime64[ns] 2014-09-05
325     Dimensions without coordinates: x, y
326     Attributes:
327         description:  Ambient temperature.
328         units:        degC
329 
330     Find out where the coldest temperature was:
331 
332     >>> da.isel(da.argmin(...))
333     <xarray.DataArray ()>
334     array(7.18177696)
335     Coordinates:
336         lon             float64 -99.32
337         lat             float64 42.21
338         time            datetime64[ns] 2014-09-08
339         reference_time  datetime64[ns] 2014-09-05
340     Attributes:
341         description:  Ambient temperature.
342         units:        degC
343     """
344 
345     _cache: Dict[str, Any]
346     _coords: Dict[Any, Variable]
347     _close: Optional[Callable[[], None]]
348     _indexes: Optional[Dict[Hashable, pd.Index]]
349     _name: Optional[Hashable]
350     _variable: Variable
351 
352     __slots__ = (
353         "_cache",
354         "_coords",
355         "_close",
356         "_indexes",
357         "_name",
358         "_variable",
359         "__weakref__",
360     )
361 
362     _groupby_cls = groupby.DataArrayGroupBy
363     _rolling_cls = rolling.DataArrayRolling
364     _coarsen_cls = rolling.DataArrayCoarsen
365     _resample_cls = resample.DataArrayResample
366     _weighted_cls = weighted.DataArrayWeighted
367 
368     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
369 
370     def __init__(
371         self,
372         data: Any = dtypes.NA,
373         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
374         dims: Union[Hashable, Sequence[Hashable], None] = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: Dict[Hashable, pd.Index] = None,
379         fastpath: bool = False,
380     ):
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385         else:
386             # try to fill in arguments from data if they weren't supplied
387             if coords is None:
388 
389                 if isinstance(data, DataArray):
390                     coords = data.coords
391                 elif isinstance(data, pd.Series):
392                     coords = [data.index]
393                 elif isinstance(data, pd.DataFrame):
394                     coords = [data.index, data.columns]
395                 elif isinstance(data, (pd.Index, IndexVariable)):
396                     coords = [data]
397                 elif isinstance(data, pdcompat.Panel):
398                     coords = [data.items, data.major_axis, data.minor_axis]
399 
400             if dims is None:
401                 dims = getattr(data, "dims", getattr(coords, "dims", None))
402             if name is None:
403                 name = getattr(data, "name", None)
404             if attrs is None and not isinstance(data, PANDAS_TYPES):
405                 attrs = getattr(data, "attrs", None)
406 
407             data = _check_data_shape(data, coords, dims)
408             data = as_compatible_data(data)
409             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
410             variable = Variable(dims, data, attrs, fastpath=True)
411             indexes = dict(
412                 _extract_indexes_from_coords(coords)
413             )  # needed for to_dataset
414 
415         # These fully describe a DataArray
416         self._variable = variable
417         assert isinstance(coords, dict)
418         self._coords = coords
419         self._name = name
420 
421         # TODO(shoyer): document this argument, once it becomes part of the
422         # public interface.
423         self._indexes = indexes
424 
425         self._close = None
426 
427     def _replace(
428         self,
429         variable: Variable = None,
430         coords=None,
431         name: Union[Hashable, None, Default] = _default,
432         indexes=None,
433     ) -> "DataArray":
434         if variable is None:
435             variable = self.variable
436         if coords is None:
437             coords = self._coords
438         if name is _default:
439             name = self.name
440         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
441 
442     def _replace_maybe_drop_dims(
443         self, variable: Variable, name: Union[Hashable, None, Default] = _default
444     ) -> "DataArray":
445         if variable.dims == self.dims and variable.shape == self.shape:
446             coords = self._coords.copy()
447             indexes = self._indexes
448         elif variable.dims == self.dims:
449             # Shape has changed (e.g. from reduce(..., keepdims=True)
450             new_sizes = dict(zip(self.dims, variable.shape))
451             coords = {
452                 k: v
453                 for k, v in self._coords.items()
454                 if v.shape == tuple(new_sizes[d] for d in v.dims)
455             }
456             changed_dims = [
457                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
458             ]
459             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
460         else:
461             allowed_dims = set(variable.dims)
462             coords = {
463                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
464             }
465             indexes = propagate_indexes(
466                 self._indexes, exclude=(set(self.dims) - allowed_dims)
467             )
468         return self._replace(variable, coords, name, indexes=indexes)
469 
470     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
471         if not len(indexes):
472             return self
473         coords = self._coords.copy()
474         for name, idx in indexes.items():
475             coords[name] = IndexVariable(name, idx)
476         obj = self._replace(coords=coords)
477 
478         # switch from dimension to level names, if necessary
479         dim_names: Dict[Any, str] = {}
480         for dim, idx in indexes.items():
481             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
482                 dim_names[dim] = idx.name
483         if dim_names:
484             obj = obj.rename(dim_names)
485         return obj
486 
487     def _to_temp_dataset(self) -> Dataset:
488         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
489 
490     def _from_temp_dataset(
491         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
492     ) -> "DataArray":
493         variable = dataset._variables.pop(_THIS_ARRAY)
494         coords = dataset._variables
495         indexes = dataset._indexes
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _to_dataset_split(self, dim: Hashable) -> Dataset:
499         """ splits dataarray along dimension 'dim' """
500 
501         def subset(dim, label):
502             array = self.loc[{dim: label}]
503             array.attrs = {}
504             return as_variable(array)
505 
506         variables = {label: subset(dim, label) for label in self.get_index(dim)}
507         variables.update({k: v for k, v in self._coords.items() if k != dim})
508         indexes = propagate_indexes(self._indexes, exclude=dim)
509         coord_names = set(self._coords) - {dim}
510         dataset = Dataset._construct_direct(
511             variables, coord_names, indexes=indexes, attrs=self.attrs
512         )
513         return dataset
514 
515     def _to_dataset_whole(
516         self, name: Hashable = None, shallow_copy: bool = True
517     ) -> Dataset:
518         if name is None:
519             name = self.name
520         if name is None:
521             raise ValueError(
522                 "unable to convert unnamed DataArray to a "
523                 "Dataset without providing an explicit name"
524             )
525         if name in self.coords:
526             raise ValueError(
527                 "cannot create a Dataset from a DataArray with "
528                 "the same name as one of its coordinates"
529             )
530         # use private APIs for speed: this is called by _to_temp_dataset(),
531         # which is used in the guts of a lot of operations (e.g., reindex)
532         variables = self._coords.copy()
533         variables[name] = self.variable
534         if shallow_copy:
535             for k in variables:
536                 variables[k] = variables[k].copy(deep=False)
537         indexes = self._indexes
538 
539         coord_names = set(self._coords)
540         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
541         return dataset
542 
543     def to_dataset(
544         self,
545         dim: Hashable = None,
546         *,
547         name: Hashable = None,
548         promote_attrs: bool = False,
549     ) -> Dataset:
550         """Convert a DataArray to a Dataset.
551 
552         Parameters
553         ----------
554         dim : hashable, optional
555             Name of the dimension on this array along which to split this array
556             into separate variables. If not provided, this array is converted
557             into a Dataset of one variable.
558         name : hashable, optional
559             Name to substitute for this array's name. Only valid if ``dim`` is
560             not provided.
561         promote_attrs : bool, default: False
562             Set to True to shallow copy attrs of DataArray to returned Dataset.
563 
564         Returns
565         -------
566         dataset : Dataset
567         """
568         if dim is not None and dim not in self.dims:
569             raise TypeError(
570                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
571             )
572 
573         if dim is not None:
574             if name is not None:
575                 raise TypeError("cannot supply both dim and name arguments")
576             result = self._to_dataset_split(dim)
577         else:
578             result = self._to_dataset_whole(name)
579 
580         if promote_attrs:
581             result.attrs = dict(self.attrs)
582 
583         return result
584 
585     @property
586     def name(self) -> Optional[Hashable]:
587         """The name of this array."""
588         return self._name
589 
590     @name.setter
591     def name(self, value: Optional[Hashable]) -> None:
592         self._name = value
593 
594     @property
595     def variable(self) -> Variable:
596         """Low level interface to the Variable object for this DataArray."""
597         return self._variable
598 
599     @property
600     def dtype(self) -> np.dtype:
601         return self.variable.dtype
602 
603     @property
604     def shape(self) -> Tuple[int, ...]:
605         return self.variable.shape
606 
607     @property
608     def size(self) -> int:
609         return self.variable.size
610 
611     @property
612     def nbytes(self) -> int:
613         return self.variable.nbytes
614 
615     @property
616     def ndim(self) -> int:
617         return self.variable.ndim
618 
619     def __len__(self) -> int:
620         return len(self.variable)
621 
622     @property
623     def data(self) -> Any:
624         """The array's data as a dask or numpy array"""
625         return self.variable.data
626 
627     @data.setter
628     def data(self, value: Any) -> None:
629         self.variable.data = value
630 
631     @property
632     def values(self) -> np.ndarray:
633         """The array's data as a numpy.ndarray"""
634         return self.variable.values
635 
636     @values.setter
637     def values(self, value: Any) -> None:
638         self.variable.values = value
639 
640     @property
641     def _in_memory(self) -> bool:
642         return self.variable._in_memory
643 
644     def to_index(self) -> pd.Index:
645         """Convert this variable to a pandas.Index. Only possible for 1D
646         arrays.
647         """
648         return self.variable.to_index()
649 
650     @property
651     def dims(self) -> Tuple[Hashable, ...]:
652         """Tuple of dimension names associated with this array.
653 
654         Note that the type of this property is inconsistent with
655         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
656         consistently named properties.
657         """
658         return self.variable.dims
659 
660     @dims.setter
661     def dims(self, value):
662         raise AttributeError(
663             "you cannot assign dims on a DataArray. Use "
664             ".rename() or .swap_dims() instead."
665         )
666 
667     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
668         if utils.is_dict_like(key):
669             return key
670         else:
671             key = indexing.expanded_indexer(key, self.ndim)
672             return dict(zip(self.dims, key))
673 
674     @property
675     def _level_coords(self) -> Dict[Hashable, Hashable]:
676         """Return a mapping of all MultiIndex levels and their corresponding
677         coordinate name.
678         """
679         level_coords: Dict[Hashable, Hashable] = {}
680 
681         for cname, var in self._coords.items():
682             if var.ndim == 1 and isinstance(var, IndexVariable):
683                 level_names = var.level_names
684                 if level_names is not None:
685                     (dim,) = var.dims
686                     level_coords.update({lname: dim for lname in level_names})
687         return level_coords
688 
689     def _getitem_coord(self, key):
690         from .dataset import _get_virtual_variable
691 
692         try:
693             var = self._coords[key]
694         except KeyError:
695             dim_sizes = dict(zip(self.dims, self.shape))
696             _, key, var = _get_virtual_variable(
697                 self._coords, key, self._level_coords, dim_sizes
698             )
699 
700         return self._replace_maybe_drop_dims(var, name=key)
701 
702     def __getitem__(self, key: Any) -> "DataArray":
703         if isinstance(key, str):
704             return self._getitem_coord(key)
705         else:
706             # xarray-style array indexing
707             return self.isel(indexers=self._item_key_to_dict(key))
708 
709     def __setitem__(self, key: Any, value: Any) -> None:
710         if isinstance(key, str):
711             self.coords[key] = value
712         else:
713             # Coordinates in key, value and self[key] should be consistent.
714             # TODO Coordinate consistency in key is checked here, but it
715             # causes unnecessary indexing. It should be optimized.
716             obj = self[key]
717             if isinstance(value, DataArray):
718                 assert_coordinate_consistent(value, obj.coords.variables)
719             # DataArray key -> Variable key
720             key = {
721                 k: v.variable if isinstance(v, DataArray) else v
722                 for k, v in self._item_key_to_dict(key).items()
723             }
724             self.variable[key] = value
725 
726     def __delitem__(self, key: Any) -> None:
727         del self.coords[key]
728 
729     @property
730     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
731         """Places to look-up items for attribute-style access"""
732         yield from self._item_sources
733         yield self.attrs
734 
735     @property
736     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
737         """Places to look-up items for key-completion"""
738         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
739 
740         # virtual coordinates
741         # uses empty dict -- everything here can already be found in self.coords.
742         yield HybridMappingProxy(keys=self.dims, mapping={})
743         yield HybridMappingProxy(keys=self._level_coords, mapping={})
744 
745     def __contains__(self, key: Any) -> bool:
746         return key in self.data
747 
748     @property
749     def loc(self) -> _LocIndexer:
750         """Attribute for location based indexing like pandas."""
751         return _LocIndexer(self)
752 
753     @property
754     def attrs(self) -> Dict[Hashable, Any]:
755         """Dictionary storing arbitrary metadata with this array."""
756         return self.variable.attrs
757 
758     @attrs.setter
759     def attrs(self, value: Mapping[Hashable, Any]) -> None:
760         # Disable type checking to work around mypy bug - see mypy#4167
761         self.variable.attrs = value  # type: ignore
762 
763     @property
764     def encoding(self) -> Dict[Hashable, Any]:
765         """Dictionary of format-specific settings for how this array should be
766         serialized."""
767         return self.variable.encoding
768 
769     @encoding.setter
770     def encoding(self, value: Mapping[Hashable, Any]) -> None:
771         self.variable.encoding = value
772 
773     @property
774     def indexes(self) -> Indexes:
775         """Mapping of pandas.Index objects used for label based indexing"""
776         if self._indexes is None:
777             self._indexes = default_indexes(self._coords, self.dims)
778         return Indexes(self._indexes)
779 
780     @property
781     def coords(self) -> DataArrayCoordinates:
782         """Dictionary-like container of coordinate arrays."""
783         return DataArrayCoordinates(self)
784 
785     def reset_coords(
786         self,
787         names: Union[Iterable[Hashable], Hashable, None] = None,
788         drop: bool = False,
789     ) -> Union[None, "DataArray", Dataset]:
790         """Given names of coordinates, reset them to become variables.
791 
792         Parameters
793         ----------
794         names : hashable or iterable of hashable, optional
795             Name(s) of non-index coordinates in this dataset to reset into
796             variables. By default, all non-index coordinates are reset.
797         drop : bool, optional
798             If True, remove coordinates instead of converting them into
799             variables.
800 
801         Returns
802         -------
803         Dataset, or DataArray if ``drop == True``
804         """
805         if names is None:
806             names = set(self.coords) - set(self.dims)
807         dataset = self.coords.to_dataset().reset_coords(names, drop)
808         if drop:
809             return self._replace(coords=dataset._variables)
810         else:
811             if self.name is None:
812                 raise ValueError(
813                     "cannot reset_coords with drop=False on an unnamed DataArrray"
814                 )
815             dataset[self.name] = self.variable
816             return dataset
817 
818     def __dask_tokenize__(self):
819         from dask.base import normalize_token
820 
821         return normalize_token((type(self), self._variable, self._coords, self._name))
822 
823     def __dask_graph__(self):
824         return self._to_temp_dataset().__dask_graph__()
825 
826     def __dask_keys__(self):
827         return self._to_temp_dataset().__dask_keys__()
828 
829     def __dask_layers__(self):
830         return self._to_temp_dataset().__dask_layers__()
831 
832     @property
833     def __dask_optimize__(self):
834         return self._to_temp_dataset().__dask_optimize__
835 
836     @property
837     def __dask_scheduler__(self):
838         return self._to_temp_dataset().__dask_scheduler__
839 
840     def __dask_postcompute__(self):
841         func, args = self._to_temp_dataset().__dask_postcompute__()
842         return self._dask_finalize, (func, args, self.name)
843 
844     def __dask_postpersist__(self):
845         func, args = self._to_temp_dataset().__dask_postpersist__()
846         return self._dask_finalize, (func, args, self.name)
847 
848     @staticmethod
849     def _dask_finalize(results, func, args, name):
850         ds = func(results, *args)
851         variable = ds._variables.pop(_THIS_ARRAY)
852         coords = ds._variables
853         return DataArray(variable, coords, name=name, fastpath=True)
854 
855     def load(self, **kwargs) -> "DataArray":
856         """Manually trigger loading of this array's data from disk or a
857         remote source into memory and return this array.
858 
859         Normally, it should not be necessary to call this method in user code,
860         because all xarray functions should either work on deferred data or
861         load data automatically. However, this method can be necessary when
862         working with many file objects on disk.
863 
864         Parameters
865         ----------
866         **kwargs : dict
867             Additional keyword arguments passed on to ``dask.compute``.
868 
869         See Also
870         --------
871         dask.compute
872         """
873         ds = self._to_temp_dataset().load(**kwargs)
874         new = self._from_temp_dataset(ds)
875         self._variable = new._variable
876         self._coords = new._coords
877         return self
878 
879     def compute(self, **kwargs) -> "DataArray":
880         """Manually trigger loading of this array's data from disk or a
881         remote source into memory and return a new array. The original is
882         left unaltered.
883 
884         Normally, it should not be necessary to call this method in user code,
885         because all xarray functions should either work on deferred data or
886         load data automatically. However, this method can be necessary when
887         working with many file objects on disk.
888 
889         Parameters
890         ----------
891         **kwargs : dict
892             Additional keyword arguments passed on to ``dask.compute``.
893 
894         See Also
895         --------
896         dask.compute
897         """
898         new = self.copy(deep=False)
899         return new.load(**kwargs)
900 
901     def persist(self, **kwargs) -> "DataArray":
902         """Trigger computation in constituent dask arrays
903 
904         This keeps them as dask arrays but encourages them to keep data in
905         memory.  This is particularly useful when on a distributed machine.
906         When on a single machine consider using ``.compute()`` instead.
907 
908         Parameters
909         ----------
910         **kwargs : dict
911             Additional keyword arguments passed on to ``dask.persist``.
912 
913         See Also
914         --------
915         dask.persist
916         """
917         ds = self._to_temp_dataset().persist(**kwargs)
918         return self._from_temp_dataset(ds)
919 
920     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
921         """Returns a copy of this array.
922 
923         If `deep=True`, a deep copy is made of the data array.
924         Otherwise, a shallow copy is made, and the returned data array's
925         values are a new view of this data array's values.
926 
927         Use `data` to create a new object with the same structure as
928         original but entirely new data.
929 
930         Parameters
931         ----------
932         deep : bool, optional
933             Whether the data array and its coordinates are loaded into memory
934             and copied onto the new object. Default is True.
935         data : array_like, optional
936             Data to use in the new object. Must have same shape as original.
937             When `data` is used, `deep` is ignored for all data variables,
938             and only used for coords.
939 
940         Returns
941         -------
942         object : DataArray
943             New object with dimensions, attributes, coordinates, name,
944             encoding, and optionally data copied from original.
945 
946         Examples
947         --------
948         Shallow versus deep copy
949 
950         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
951         >>> array.copy()
952         <xarray.DataArray (x: 3)>
953         array([1, 2, 3])
954         Coordinates:
955           * x        (x) <U1 'a' 'b' 'c'
956         >>> array_0 = array.copy(deep=False)
957         >>> array_0[0] = 7
958         >>> array_0
959         <xarray.DataArray (x: 3)>
960         array([7, 2, 3])
961         Coordinates:
962           * x        (x) <U1 'a' 'b' 'c'
963         >>> array
964         <xarray.DataArray (x: 3)>
965         array([7, 2, 3])
966         Coordinates:
967           * x        (x) <U1 'a' 'b' 'c'
968 
969         Changing the data using the ``data`` argument maintains the
970         structure of the original object, but with the new data. Original
971         object is unaffected.
972 
973         >>> array.copy(data=[0.1, 0.2, 0.3])
974         <xarray.DataArray (x: 3)>
975         array([0.1, 0.2, 0.3])
976         Coordinates:
977           * x        (x) <U1 'a' 'b' 'c'
978         >>> array
979         <xarray.DataArray (x: 3)>
980         array([7, 2, 3])
981         Coordinates:
982           * x        (x) <U1 'a' 'b' 'c'
983 
984         See Also
985         --------
986         pandas.DataFrame.copy
987         """
988         variable = self.variable.copy(deep=deep, data=data)
989         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
990         if self._indexes is None:
991             indexes = self._indexes
992         else:
993             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
994         return self._replace(variable, coords, indexes=indexes)
995 
996     def __copy__(self) -> "DataArray":
997         return self.copy(deep=False)
998 
999     def __deepcopy__(self, memo=None) -> "DataArray":
1000         # memo does nothing but is required for compatibility with
1001         # copy.deepcopy
1002         return self.copy(deep=True)
1003 
1004     # mutable objects should not be hashable
1005     # https://github.com/python/mypy/issues/4266
1006     __hash__ = None  # type: ignore
1007 
1008     @property
1009     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1010         """Block dimensions for this array's data or None if it's not a dask
1011         array.
1012         """
1013         return self.variable.chunks
1014 
1015     def chunk(
1016         self,
1017         chunks: Union[
1018             Number,
1019             Tuple[Number, ...],
1020             Tuple[Tuple[Number, ...], ...],
1021             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
1022         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1023         name_prefix: str = "xarray-",
1024         token: str = None,
1025         lock: bool = False,
1026     ) -> "DataArray":
1027         """Coerce this array's data into a dask arrays with the given chunks.
1028 
1029         If this variable is a non-dask array, it will be converted to dask
1030         array. If it's a dask array, it will be rechunked to the given chunk
1031         sizes.
1032 
1033         If neither chunks is not provided for one or more dimensions, chunk
1034         sizes along that dimension will not be updated; non-dask arrays will be
1035         converted into dask arrays with a single block.
1036 
1037         Parameters
1038         ----------
1039         chunks : int, tuple of int or mapping of hashable to int, optional
1040             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1041             ``{'x': 5, 'y': 5}``.
1042         name_prefix : str, optional
1043             Prefix for the name of the new dask array.
1044         token : str, optional
1045             Token uniquely identifying this array.
1046         lock : optional
1047             Passed on to :py:func:`dask.array.from_array`, if the array is not
1048             already as dask array.
1049 
1050         Returns
1051         -------
1052         chunked : xarray.DataArray
1053         """
1054         if isinstance(chunks, (tuple, list)):
1055             chunks = dict(zip(self.dims, chunks))
1056 
1057         ds = self._to_temp_dataset().chunk(
1058             chunks, name_prefix=name_prefix, token=token, lock=lock
1059         )
1060         return self._from_temp_dataset(ds)
1061 
1062     def isel(
1063         self,
1064         indexers: Mapping[Hashable, Any] = None,
1065         drop: bool = False,
1066         missing_dims: str = "raise",
1067         **indexers_kwargs: Any,
1068     ) -> "DataArray":
1069         """Return a new DataArray whose data is given by integer indexing
1070         along the specified dimension(s).
1071 
1072         Parameters
1073         ----------
1074         indexers : dict, optional
1075             A dict with keys matching dimensions and values given
1076             by integers, slice objects or arrays.
1077             indexer can be a integer, slice, array-like or DataArray.
1078             If DataArrays are passed as indexers, xarray-style indexing will be
1079             carried out. See :ref:`indexing` for the details.
1080             One of indexers or indexers_kwargs must be provided.
1081         drop : bool, optional
1082             If ``drop=True``, drop coordinates variables indexed by integers
1083             instead of making them scalar.
1084         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1085             What to do if dimensions that should be selected from are not present in the
1086             DataArray:
1087             - "raise": raise an exception
1088             - "warning": raise a warning, and ignore the missing dimensions
1089             - "ignore": ignore the missing dimensions
1090         **indexers_kwargs : {dim: indexer, ...}, optional
1091             The keyword arguments form of ``indexers``.
1092 
1093         See Also
1094         --------
1095         Dataset.isel
1096         DataArray.sel
1097 
1098         Examples
1099         --------
1100         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1101         >>> da
1102         <xarray.DataArray (x: 5, y: 5)>
1103         array([[ 0,  1,  2,  3,  4],
1104                [ 5,  6,  7,  8,  9],
1105                [10, 11, 12, 13, 14],
1106                [15, 16, 17, 18, 19],
1107                [20, 21, 22, 23, 24]])
1108         Dimensions without coordinates: x, y
1109 
1110         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1111         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1112         >>> da = da.isel(x=tgt_x, y=tgt_y)
1113         >>> da
1114         <xarray.DataArray (points: 5)>
1115         array([ 0,  6, 12, 18, 24])
1116         Dimensions without coordinates: points
1117         """
1118 
1119         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1120 
1121         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1122             ds = self._to_temp_dataset()._isel_fancy(
1123                 indexers, drop=drop, missing_dims=missing_dims
1124             )
1125             return self._from_temp_dataset(ds)
1126 
1127         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1128         # lists, or zero or one-dimensional np.ndarray's
1129 
1130         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1131 
1132         coords = {}
1133         for coord_name, coord_value in self._coords.items():
1134             coord_indexers = {
1135                 k: v for k, v in indexers.items() if k in coord_value.dims
1136             }
1137             if coord_indexers:
1138                 coord_value = coord_value.isel(coord_indexers)
1139                 if drop and coord_value.ndim == 0:
1140                     continue
1141             coords[coord_name] = coord_value
1142 
1143         return self._replace(variable=variable, coords=coords)
1144 
1145     def sel(
1146         self,
1147         indexers: Mapping[Hashable, Any] = None,
1148         method: str = None,
1149         tolerance=None,
1150         drop: bool = False,
1151         **indexers_kwargs: Any,
1152     ) -> "DataArray":
1153         """Return a new DataArray whose data is given by selecting index
1154         labels along the specified dimension(s).
1155 
1156         In contrast to `DataArray.isel`, indexers for this method should use
1157         labels instead of integers.
1158 
1159         Under the hood, this method is powered by using pandas's powerful Index
1160         objects. This makes label based indexing essentially just as fast as
1161         using integer indexing.
1162 
1163         It also means this method uses pandas's (well documented) logic for
1164         indexing. This means you can use string shortcuts for datetime indexes
1165         (e.g., '2000-01' to select all values in January 2000). It also means
1166         that slices are treated as inclusive of both the start and stop values,
1167         unlike normal Python indexing.
1168 
1169         .. warning::
1170 
1171           Do not try to assign values when using any of the indexing methods
1172           ``isel`` or ``sel``::
1173 
1174             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1175             # DO NOT do this
1176             da.isel(x=[0, 1, 2])[1] = -1
1177 
1178           Assigning values with the chained indexing using ``.sel`` or
1179           ``.isel`` fails silently.
1180 
1181         Parameters
1182         ----------
1183         indexers : dict, optional
1184             A dict with keys matching dimensions and values given
1185             by scalars, slices or arrays of tick labels. For dimensions with
1186             multi-index, the indexer may also be a dict-like object with keys
1187             matching index level names.
1188             If DataArrays are passed as indexers, xarray-style indexing will be
1189             carried out. See :ref:`indexing` for the details.
1190             One of indexers or indexers_kwargs must be provided.
1191         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1192             Method to use for inexact matches:
1193 
1194             * None (default): only exact matches
1195             * pad / ffill: propagate last valid index value forward
1196             * backfill / bfill: propagate next valid index value backward
1197             * nearest: use nearest valid index value
1198         tolerance : optional
1199             Maximum distance between original and new labels for inexact
1200             matches. The values of the index at the matching locations must
1201             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1202         drop : bool, optional
1203             If ``drop=True``, drop coordinates variables in `indexers` instead
1204             of making them scalar.
1205         **indexers_kwargs : {dim: indexer, ...}, optional
1206             The keyword arguments form of ``indexers``.
1207             One of indexers or indexers_kwargs must be provided.
1208 
1209         Returns
1210         -------
1211         obj : DataArray
1212             A new DataArray with the same contents as this DataArray, except the
1213             data and each dimension is indexed by the appropriate indexers.
1214             If indexer DataArrays have coordinates that do not conflict with
1215             this object, then these coordinates will be attached.
1216             In general, each array's data will be a view of the array's data
1217             in this DataArray, unless vectorized indexing was triggered by using
1218             an array indexer, in which case the data will be a copy.
1219 
1220         See Also
1221         --------
1222         Dataset.sel
1223         DataArray.isel
1224 
1225         Examples
1226         --------
1227         >>> da = xr.DataArray(
1228         ...     np.arange(25).reshape(5, 5),
1229         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1230         ...     dims=("x", "y"),
1231         ... )
1232         >>> da
1233         <xarray.DataArray (x: 5, y: 5)>
1234         array([[ 0,  1,  2,  3,  4],
1235                [ 5,  6,  7,  8,  9],
1236                [10, 11, 12, 13, 14],
1237                [15, 16, 17, 18, 19],
1238                [20, 21, 22, 23, 24]])
1239         Coordinates:
1240           * x        (x) int64 0 1 2 3 4
1241           * y        (y) int64 0 1 2 3 4
1242 
1243         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1244         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1245         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1246         >>> da
1247         <xarray.DataArray (points: 5)>
1248         array([ 0,  6, 12, 18, 24])
1249         Coordinates:
1250             x        (points) int64 0 1 2 3 4
1251             y        (points) int64 0 1 2 3 4
1252         Dimensions without coordinates: points
1253         """
1254         ds = self._to_temp_dataset().sel(
1255             indexers=indexers,
1256             drop=drop,
1257             method=method,
1258             tolerance=tolerance,
1259             **indexers_kwargs,
1260         )
1261         return self._from_temp_dataset(ds)
1262 
1263     def head(
1264         self,
1265         indexers: Union[Mapping[Hashable, int], int] = None,
1266         **indexers_kwargs: Any,
1267     ) -> "DataArray":
1268         """Return a new DataArray whose data is given by the the first `n`
1269         values along the specified dimension(s). Default `n` = 5
1270 
1271         See Also
1272         --------
1273         Dataset.head
1274         DataArray.tail
1275         DataArray.thin
1276         """
1277         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1278         return self._from_temp_dataset(ds)
1279 
1280     def tail(
1281         self,
1282         indexers: Union[Mapping[Hashable, int], int] = None,
1283         **indexers_kwargs: Any,
1284     ) -> "DataArray":
1285         """Return a new DataArray whose data is given by the the last `n`
1286         values along the specified dimension(s). Default `n` = 5
1287 
1288         See Also
1289         --------
1290         Dataset.tail
1291         DataArray.head
1292         DataArray.thin
1293         """
1294         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1295         return self._from_temp_dataset(ds)
1296 
1297     def thin(
1298         self,
1299         indexers: Union[Mapping[Hashable, int], int] = None,
1300         **indexers_kwargs: Any,
1301     ) -> "DataArray":
1302         """Return a new DataArray whose data is given by each `n` value
1303         along the specified dimension(s).
1304 
1305         See Also
1306         --------
1307         Dataset.thin
1308         DataArray.head
1309         DataArray.tail
1310         """
1311         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1312         return self._from_temp_dataset(ds)
1313 
1314     def broadcast_like(
1315         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1316     ) -> "DataArray":
1317         """Broadcast this DataArray against another Dataset or DataArray.
1318 
1319         This is equivalent to xr.broadcast(other, self)[1]
1320 
1321         xarray objects are broadcast against each other in arithmetic
1322         operations, so this method is not be necessary for most uses.
1323 
1324         If no change is needed, the input data is returned to the output
1325         without being copied.
1326 
1327         If new coords are added by the broadcast, their values are
1328         NaN filled.
1329 
1330         Parameters
1331         ----------
1332         other : Dataset or DataArray
1333             Object against which to broadcast this array.
1334         exclude : iterable of hashable, optional
1335             Dimensions that must not be broadcasted
1336 
1337         Returns
1338         -------
1339         new_da : DataArray
1340             The caller broadcasted against ``other``.
1341 
1342         Examples
1343         --------
1344         >>> arr1 = xr.DataArray(
1345         ...     np.random.randn(2, 3),
1346         ...     dims=("x", "y"),
1347         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1348         ... )
1349         >>> arr2 = xr.DataArray(
1350         ...     np.random.randn(3, 2),
1351         ...     dims=("x", "y"),
1352         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1353         ... )
1354         >>> arr1
1355         <xarray.DataArray (x: 2, y: 3)>
1356         array([[ 1.76405235,  0.40015721,  0.97873798],
1357                [ 2.2408932 ,  1.86755799, -0.97727788]])
1358         Coordinates:
1359           * x        (x) <U1 'a' 'b'
1360           * y        (y) <U1 'a' 'b' 'c'
1361         >>> arr2
1362         <xarray.DataArray (x: 3, y: 2)>
1363         array([[ 0.95008842, -0.15135721],
1364                [-0.10321885,  0.4105985 ],
1365                [ 0.14404357,  1.45427351]])
1366         Coordinates:
1367           * x        (x) <U1 'a' 'b' 'c'
1368           * y        (y) <U1 'a' 'b'
1369         >>> arr1.broadcast_like(arr2)
1370         <xarray.DataArray (x: 3, y: 3)>
1371         array([[ 1.76405235,  0.40015721,  0.97873798],
1372                [ 2.2408932 ,  1.86755799, -0.97727788],
1373                [        nan,         nan,         nan]])
1374         Coordinates:
1375           * x        (x) <U1 'a' 'b' 'c'
1376           * y        (y) <U1 'a' 'b' 'c'
1377         """
1378         if exclude is None:
1379             exclude = set()
1380         else:
1381             exclude = set(exclude)
1382         args = align(other, self, join="outer", copy=False, exclude=exclude)
1383 
1384         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1385 
1386         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1387 
1388     def reindex_like(
1389         self,
1390         other: Union["DataArray", Dataset],
1391         method: str = None,
1392         tolerance=None,
1393         copy: bool = True,
1394         fill_value=dtypes.NA,
1395     ) -> "DataArray":
1396         """Conform this object onto the indexes of another object, filling in
1397         missing values with ``fill_value``. The default fill value is NaN.
1398 
1399         Parameters
1400         ----------
1401         other : Dataset or DataArray
1402             Object with an 'indexes' attribute giving a mapping from dimension
1403             names to pandas.Index objects, which provides coordinates upon
1404             which to index the variables in this dataset. The indexes on this
1405             other object need not be the same as the indexes on this
1406             dataset. Any mis-matched index values will be filled in with
1407             NaN, and any mis-matched dimension names will simply be ignored.
1408         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1409             Method to use for filling index values from other not found on this
1410             data array:
1411 
1412             * None (default): don't fill gaps
1413             * pad / ffill: propagate last valid index value forward
1414             * backfill / bfill: propagate next valid index value backward
1415             * nearest: use nearest valid index value
1416         tolerance : optional
1417             Maximum distance between original and new labels for inexact
1418             matches. The values of the index at the matching locations must
1419             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1420         copy : bool, optional
1421             If ``copy=True``, data in the return value is always copied. If
1422             ``copy=False`` and reindexing is unnecessary, or can be performed
1423             with only slice operations, then the output may share memory with
1424             the input. In either case, a new xarray object is always returned.
1425         fill_value : scalar or dict-like, optional
1426             Value to use for newly missing values. If a dict-like, maps
1427             variable names (including coordinates) to fill values. Use this
1428             data array's name to refer to the data array's values.
1429 
1430         Returns
1431         -------
1432         reindexed : DataArray
1433             Another dataset array, with this array's data but coordinates from
1434             the other object.
1435 
1436         See Also
1437         --------
1438         DataArray.reindex
1439         align
1440         """
1441         indexers = reindex_like_indexers(self, other)
1442         return self.reindex(
1443             indexers=indexers,
1444             method=method,
1445             tolerance=tolerance,
1446             copy=copy,
1447             fill_value=fill_value,
1448         )
1449 
1450     def reindex(
1451         self,
1452         indexers: Mapping[Hashable, Any] = None,
1453         method: str = None,
1454         tolerance=None,
1455         copy: bool = True,
1456         fill_value=dtypes.NA,
1457         **indexers_kwargs: Any,
1458     ) -> "DataArray":
1459         """Conform this object onto the indexes of another object, filling in
1460         missing values with ``fill_value``. The default fill value is NaN.
1461 
1462         Parameters
1463         ----------
1464         indexers : dict, optional
1465             Dictionary with keys given by dimension names and values given by
1466             arrays of coordinates tick labels. Any mis-matched coordinate
1467             values will be filled in with NaN, and any mis-matched dimension
1468             names will simply be ignored.
1469             One of indexers or indexers_kwargs must be provided.
1470         copy : bool, optional
1471             If ``copy=True``, data in the return value is always copied. If
1472             ``copy=False`` and reindexing is unnecessary, or can be performed
1473             with only slice operations, then the output may share memory with
1474             the input. In either case, a new xarray object is always returned.
1475         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1476             Method to use for filling index values in ``indexers`` not found on
1477             this data array:
1478 
1479             * None (default): don't fill gaps
1480             * pad / ffill: propagate last valid index value forward
1481             * backfill / bfill: propagate next valid index value backward
1482             * nearest: use nearest valid index value
1483         tolerance : optional
1484             Maximum distance between original and new labels for inexact
1485             matches. The values of the index at the matching locations must
1486             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1487         fill_value : scalar or dict-like, optional
1488             Value to use for newly missing values. If a dict-like, maps
1489             variable names (including coordinates) to fill values. Use this
1490             data array's name to refer to the data array's values.
1491         **indexers_kwargs : {dim: indexer, ...}, optional
1492             The keyword arguments form of ``indexers``.
1493             One of indexers or indexers_kwargs must be provided.
1494 
1495         Returns
1496         -------
1497         reindexed : DataArray
1498             Another dataset array, with this array's data but replaced
1499             coordinates.
1500 
1501         See Also
1502         --------
1503         DataArray.reindex_like
1504         align
1505         """
1506         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1507         if isinstance(fill_value, dict):
1508             fill_value = fill_value.copy()
1509             sentinel = object()
1510             value = fill_value.pop(self.name, sentinel)
1511             if value is not sentinel:
1512                 fill_value[_THIS_ARRAY] = value
1513 
1514         ds = self._to_temp_dataset().reindex(
1515             indexers=indexers,
1516             method=method,
1517             tolerance=tolerance,
1518             copy=copy,
1519             fill_value=fill_value,
1520         )
1521         return self._from_temp_dataset(ds)
1522 
1523     def interp(
1524         self,
1525         coords: Mapping[Hashable, Any] = None,
1526         method: str = "linear",
1527         assume_sorted: bool = False,
1528         kwargs: Mapping[str, Any] = None,
1529         **coords_kwargs: Any,
1530     ) -> "DataArray":
1531         """Multidimensional interpolation of variables.
1532 
1533         Parameters
1534         ----------
1535         coords : dict, optional
1536             Mapping from dimension names to the new coordinates.
1537             New coordinate can be an scalar, array-like or DataArray.
1538             If DataArrays are passed as new coordinates, their dimensions are
1539             used for the broadcasting. Missing values are skipped.
1540         method : str, default: "linear"
1541             The method used to interpolate. Choose from
1542 
1543             - {"linear", "nearest"} for multidimensional array,
1544             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1545         assume_sorted : bool, optional
1546             If False, values of x can be in any order and they are sorted
1547             first. If True, x has to be an array of monotonically increasing
1548             values.
1549         kwargs : dict
1550             Additional keyword arguments passed to scipy's interpolator. Valid
1551             options and their behavior depend on if 1-dimensional or
1552             multi-dimensional interpolation is used.
1553         **coords_kwargs : {dim: coordinate, ...}, optional
1554             The keyword arguments form of ``coords``.
1555             One of coords or coords_kwargs must be provided.
1556 
1557         Returns
1558         -------
1559         interpolated : DataArray
1560             New dataarray on the new coordinates.
1561 
1562         Notes
1563         -----
1564         scipy is required.
1565 
1566         See Also
1567         --------
1568         scipy.interpolate.interp1d
1569         scipy.interpolate.interpn
1570 
1571         Examples
1572         --------
1573         >>> da = xr.DataArray(
1574         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1575         ...     dims=("x", "y"),
1576         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1577         ... )
1578         >>> da
1579         <xarray.DataArray (x: 3, y: 4)>
1580         array([[ 1.,  4.,  2.,  9.],
1581                [ 2.,  7.,  6., nan],
1582                [ 6., nan,  5.,  8.]])
1583         Coordinates:
1584           * x        (x) int64 0 1 2
1585           * y        (y) int64 10 12 14 16
1586 
1587         1D linear interpolation (the default):
1588 
1589         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1590         <xarray.DataArray (x: 4, y: 4)>
1591         array([[1.  , 4.  , 2.  ,  nan],
1592                [1.75, 6.25, 5.  ,  nan],
1593                [3.  ,  nan, 5.75,  nan],
1594                [5.  ,  nan, 5.25,  nan]])
1595         Coordinates:
1596           * y        (y) int64 10 12 14 16
1597           * x        (x) float64 0.0 0.75 1.25 1.75
1598 
1599         1D nearest interpolation:
1600 
1601         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1602         <xarray.DataArray (x: 4, y: 4)>
1603         array([[ 1.,  4.,  2.,  9.],
1604                [ 2.,  7.,  6., nan],
1605                [ 2.,  7.,  6., nan],
1606                [ 6., nan,  5.,  8.]])
1607         Coordinates:
1608           * y        (y) int64 10 12 14 16
1609           * x        (x) float64 0.0 0.75 1.25 1.75
1610 
1611         1D linear extrapolation:
1612 
1613         >>> da.interp(
1614         ...     x=[1, 1.5, 2.5, 3.5],
1615         ...     method="linear",
1616         ...     kwargs={"fill_value": "extrapolate"},
1617         ... )
1618         <xarray.DataArray (x: 4, y: 4)>
1619         array([[ 2. ,  7. ,  6. ,  nan],
1620                [ 4. ,  nan,  5.5,  nan],
1621                [ 8. ,  nan,  4.5,  nan],
1622                [12. ,  nan,  3.5,  nan]])
1623         Coordinates:
1624           * y        (y) int64 10 12 14 16
1625           * x        (x) float64 1.0 1.5 2.5 3.5
1626 
1627         2D linear interpolation:
1628 
1629         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1630         <xarray.DataArray (x: 4, y: 3)>
1631         array([[2.5  , 3.   ,   nan],
1632                [4.   , 5.625,   nan],
1633                [  nan,   nan,   nan],
1634                [  nan,   nan,   nan]])
1635         Coordinates:
1636           * x        (x) float64 0.0 0.75 1.25 1.75
1637           * y        (y) int64 11 13 15
1638         """
1639         if self.dtype.kind not in "uifc":
1640             raise TypeError(
1641                 "interp only works for a numeric type array. "
1642                 "Given {}.".format(self.dtype)
1643             )
1644         ds = self._to_temp_dataset().interp(
1645             coords,
1646             method=method,
1647             kwargs=kwargs,
1648             assume_sorted=assume_sorted,
1649             **coords_kwargs,
1650         )
1651         return self._from_temp_dataset(ds)
1652 
1653     def interp_like(
1654         self,
1655         other: Union["DataArray", Dataset],
1656         method: str = "linear",
1657         assume_sorted: bool = False,
1658         kwargs: Mapping[str, Any] = None,
1659     ) -> "DataArray":
1660         """Interpolate this object onto the coordinates of another object,
1661         filling out of range values with NaN.
1662 
1663         Parameters
1664         ----------
1665         other : Dataset or DataArray
1666             Object with an 'indexes' attribute giving a mapping from dimension
1667             names to an 1d array-like, which provides coordinates upon
1668             which to index the variables in this dataset. Missing values are skipped.
1669         method : str, default: "linear"
1670             The method used to interpolate. Choose from
1671 
1672             - {"linear", "nearest"} for multidimensional array,
1673             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1674         assume_sorted : bool, optional
1675             If False, values of coordinates that are interpolated over can be
1676             in any order and they are sorted first. If True, interpolated
1677             coordinates are assumed to be an array of monotonically increasing
1678             values.
1679         kwargs : dict, optional
1680             Additional keyword passed to scipy's interpolator.
1681 
1682         Returns
1683         -------
1684         interpolated : DataArray
1685             Another dataarray by interpolating this dataarray's data along the
1686             coordinates of the other object.
1687 
1688         Notes
1689         -----
1690         scipy is required.
1691         If the dataarray has object-type coordinates, reindex is used for these
1692         coordinates instead of the interpolation.
1693 
1694         See Also
1695         --------
1696         DataArray.interp
1697         DataArray.reindex_like
1698         """
1699         if self.dtype.kind not in "uifc":
1700             raise TypeError(
1701                 "interp only works for a numeric type array. "
1702                 "Given {}.".format(self.dtype)
1703             )
1704         ds = self._to_temp_dataset().interp_like(
1705             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1706         )
1707         return self._from_temp_dataset(ds)
1708 
1709     def rename(
1710         self,
1711         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1712         **names: Hashable,
1713     ) -> "DataArray":
1714         """Returns a new DataArray with renamed coordinates or a new name.
1715 
1716         Parameters
1717         ----------
1718         new_name_or_name_dict : str or dict-like, optional
1719             If the argument is dict-like, it used as a mapping from old
1720             names to new names for coordinates. Otherwise, use the argument
1721             as the new name for this array.
1722         **names : hashable, optional
1723             The keyword arguments form of a mapping from old names to
1724             new names for coordinates.
1725             One of new_name_or_name_dict or names must be provided.
1726 
1727         Returns
1728         -------
1729         renamed : DataArray
1730             Renamed array or array with renamed coordinates.
1731 
1732         See Also
1733         --------
1734         Dataset.rename
1735         DataArray.swap_dims
1736         """
1737         if names or utils.is_dict_like(new_name_or_name_dict):
1738             new_name_or_name_dict = cast(
1739                 Mapping[Hashable, Hashable], new_name_or_name_dict
1740             )
1741             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1742             dataset = self._to_temp_dataset().rename(name_dict)
1743             return self._from_temp_dataset(dataset)
1744         else:
1745             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1746             return self._replace(name=new_name_or_name_dict)
1747 
1748     def swap_dims(
1749         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs
1750     ) -> "DataArray":
1751         """Returns a new DataArray with swapped dimensions.
1752 
1753         Parameters
1754         ----------
1755         dims_dict : dict-like
1756             Dictionary whose keys are current dimension names and whose values
1757             are new names.
1758         **dims_kwargs : {existing_dim: new_dim, ...}, optional
1759             The keyword arguments form of ``dims_dict``.
1760             One of dims_dict or dims_kwargs must be provided.
1761 
1762         Returns
1763         -------
1764         swapped : DataArray
1765             DataArray with swapped dimensions.
1766 
1767         Examples
1768         --------
1769         >>> arr = xr.DataArray(
1770         ...     data=[0, 1],
1771         ...     dims="x",
1772         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1773         ... )
1774         >>> arr
1775         <xarray.DataArray (x: 2)>
1776         array([0, 1])
1777         Coordinates:
1778           * x        (x) <U1 'a' 'b'
1779             y        (x) int64 0 1
1780 
1781         >>> arr.swap_dims({"x": "y"})
1782         <xarray.DataArray (y: 2)>
1783         array([0, 1])
1784         Coordinates:
1785             x        (y) <U1 'a' 'b'
1786           * y        (y) int64 0 1
1787 
1788         >>> arr.swap_dims({"x": "z"})
1789         <xarray.DataArray (z: 2)>
1790         array([0, 1])
1791         Coordinates:
1792             x        (z) <U1 'a' 'b'
1793             y        (z) int64 0 1
1794         Dimensions without coordinates: z
1795 
1796         See Also
1797         --------
1798         DataArray.rename
1799         Dataset.swap_dims
1800         """
1801         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
1802         ds = self._to_temp_dataset().swap_dims(dims_dict)
1803         return self._from_temp_dataset(ds)
1804 
1805     def expand_dims(
1806         self,
1807         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1808         axis=None,
1809         **dim_kwargs: Any,
1810     ) -> "DataArray":
1811         """Return a new object with an additional axis (or axes) inserted at
1812         the corresponding position in the array shape. The new object is a
1813         view into the underlying array, not a copy.
1814 
1815         If dim is already a scalar coordinate, it will be promoted to a 1D
1816         coordinate consisting of a single value.
1817 
1818         Parameters
1819         ----------
1820         dim : hashable, sequence of hashable, dict, or None, optional
1821             Dimensions to include on the new variable.
1822             If provided as str or sequence of str, then dimensions are inserted
1823             with length 1. If provided as a dict, then the keys are the new
1824             dimensions and the values are either integers (giving the length of
1825             the new dimensions) or sequence/ndarray (giving the coordinates of
1826             the new dimensions).
1827         axis : int, list of int or tuple of int, or None, default: None
1828             Axis position(s) where new axis is to be inserted (position(s) on
1829             the result array). If a list (or tuple) of integers is passed,
1830             multiple axes are inserted. In this case, dim arguments should be
1831             same length list. If axis=None is passed, all the axes will be
1832             inserted to the start of the result array.
1833         **dim_kwargs : int or sequence or ndarray
1834             The keywords are arbitrary dimensions being inserted and the values
1835             are either the lengths of the new dims (if int is given), or their
1836             coordinates. Note, this is an alternative to passing a dict to the
1837             dim kwarg and will only be used if dim is None.
1838 
1839         Returns
1840         -------
1841         expanded : same type as caller
1842             This object, but with an additional dimension(s).
1843         """
1844         if isinstance(dim, int):
1845             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1846         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1847             if len(dim) != len(set(dim)):
1848                 raise ValueError("dims should not contain duplicate values.")
1849             dim = dict.fromkeys(dim, 1)
1850         elif dim is not None and not isinstance(dim, Mapping):
1851             dim = {cast(Hashable, dim): 1}
1852 
1853         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1854         ds = self._to_temp_dataset().expand_dims(dim, axis)
1855         return self._from_temp_dataset(ds)
1856 
1857     def set_index(
1858         self,
1859         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1860         append: bool = False,
1861         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1862     ) -> Optional["DataArray"]:
1863         """Set DataArray (multi-)indexes using one or more existing
1864         coordinates.
1865 
1866         Parameters
1867         ----------
1868         indexes : {dim: index, ...}
1869             Mapping from names matching dimensions and values given
1870             by (lists of) the names of existing coordinates or variables to set
1871             as new (multi-)index.
1872         append : bool, optional
1873             If True, append the supplied index(es) to the existing index(es).
1874             Otherwise replace the existing index(es) (default).
1875         **indexes_kwargs : optional
1876             The keyword arguments form of ``indexes``.
1877             One of indexes or indexes_kwargs must be provided.
1878 
1879         Returns
1880         -------
1881         obj : DataArray
1882             Another DataArray, with this data but replaced coordinates.
1883 
1884         Examples
1885         --------
1886         >>> arr = xr.DataArray(
1887         ...     data=np.ones((2, 3)),
1888         ...     dims=["x", "y"],
1889         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1890         ... )
1891         >>> arr
1892         <xarray.DataArray (x: 2, y: 3)>
1893         array([[1., 1., 1.],
1894                [1., 1., 1.]])
1895         Coordinates:
1896           * x        (x) int64 0 1
1897           * y        (y) int64 0 1 2
1898             a        (x) int64 3 4
1899         >>> arr.set_index(x="a")
1900         <xarray.DataArray (x: 2, y: 3)>
1901         array([[1., 1., 1.],
1902                [1., 1., 1.]])
1903         Coordinates:
1904           * x        (x) int64 3 4
1905           * y        (y) int64 0 1 2
1906 
1907         See Also
1908         --------
1909         DataArray.reset_index
1910         """
1911         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
1912         return self._from_temp_dataset(ds)
1913 
1914     def reset_index(
1915         self,
1916         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1917         drop: bool = False,
1918     ) -> Optional["DataArray"]:
1919         """Reset the specified index(es) or multi-index level(s).
1920 
1921         Parameters
1922         ----------
1923         dims_or_levels : hashable or sequence of hashable
1924             Name(s) of the dimension(s) and/or multi-index level(s) that will
1925             be reset.
1926         drop : bool, optional
1927             If True, remove the specified indexes and/or multi-index levels
1928             instead of extracting them as new coordinates (default: False).
1929 
1930         Returns
1931         -------
1932         obj : DataArray
1933             Another dataarray, with this dataarray's data but replaced
1934             coordinates.
1935 
1936         See Also
1937         --------
1938         DataArray.set_index
1939         """
1940         coords, _ = split_indexes(
1941             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1942         )
1943         return self._replace(coords=coords)
1944 
1945     def reorder_levels(
1946         self,
1947         dim_order: Mapping[Hashable, Sequence[int]] = None,
1948         **dim_order_kwargs: Sequence[int],
1949     ) -> "DataArray":
1950         """Rearrange index levels using input order.
1951 
1952         Parameters
1953         ----------
1954         dim_order : optional
1955             Mapping from names matching dimensions and values given
1956             by lists representing new level orders. Every given dimension
1957             must have a multi-index.
1958         **dim_order_kwargs : optional
1959             The keyword arguments form of ``dim_order``.
1960             One of dim_order or dim_order_kwargs must be provided.
1961 
1962         Returns
1963         -------
1964         obj : DataArray
1965             Another dataarray, with this dataarray's data but replaced
1966             coordinates.
1967         """
1968         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1969         replace_coords = {}
1970         for dim, order in dim_order.items():
1971             coord = self._coords[dim]
1972             index = coord.to_index()
1973             if not isinstance(index, pd.MultiIndex):
1974                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1975             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1976         coords = self._coords.copy()
1977         coords.update(replace_coords)
1978         return self._replace(coords=coords)
1979 
1980     def stack(
1981         self,
1982         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1983         **dimensions_kwargs: Sequence[Hashable],
1984     ) -> "DataArray":
1985         """
1986         Stack any number of existing dimensions into a single new dimension.
1987 
1988         New dimensions will be added at the end, and the corresponding
1989         coordinate variables will be combined into a MultiIndex.
1990 
1991         Parameters
1992         ----------
1993         dimensions : mapping of hashable to sequence of hashable
1994             Mapping of the form `new_name=(dim1, dim2, ...)`.
1995             Names of new dimensions, and the existing dimensions that they
1996             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1997             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1998             all dimensions.
1999         **dimensions_kwargs
2000             The keyword arguments form of ``dimensions``.
2001             One of dimensions or dimensions_kwargs must be provided.
2002 
2003         Returns
2004         -------
2005         stacked : DataArray
2006             DataArray with stacked data.
2007 
2008         Examples
2009         --------
2010         >>> arr = xr.DataArray(
2011         ...     np.arange(6).reshape(2, 3),
2012         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2013         ... )
2014         >>> arr
2015         <xarray.DataArray (x: 2, y: 3)>
2016         array([[0, 1, 2],
2017                [3, 4, 5]])
2018         Coordinates:
2019           * x        (x) <U1 'a' 'b'
2020           * y        (y) int64 0 1 2
2021         >>> stacked = arr.stack(z=("x", "y"))
2022         >>> stacked.indexes["z"]
2023         MultiIndex([('a', 0),
2024                     ('a', 1),
2025                     ('a', 2),
2026                     ('b', 0),
2027                     ('b', 1),
2028                     ('b', 2)],
2029                    names=['x', 'y'])
2030 
2031         See Also
2032         --------
2033         DataArray.unstack
2034         """
2035         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
2036         return self._from_temp_dataset(ds)
2037 
2038     def unstack(
2039         self,
2040         dim: Union[Hashable, Sequence[Hashable], None] = None,
2041         fill_value: Any = dtypes.NA,
2042         sparse: bool = False,
2043     ) -> "DataArray":
2044         """
2045         Unstack existing dimensions corresponding to MultiIndexes into
2046         multiple new dimensions.
2047 
2048         New dimensions will be added at the end.
2049 
2050         Parameters
2051         ----------
2052         dim : hashable or sequence of hashable, optional
2053             Dimension(s) over which to unstack. By default unstacks all
2054             MultiIndexes.
2055         fill_value : scalar or dict-like, default: nan
2056             value to be filled. If a dict-like, maps variable names to
2057             fill values. Use the data array's name to refer to its
2058             name. If not provided or if the dict-like does not contain
2059             all variables, the dtype's NA value will be used.
2060         sparse : bool, default: False
2061             use sparse-array if True
2062 
2063         Returns
2064         -------
2065         unstacked : DataArray
2066             Array with unstacked data.
2067 
2068         Examples
2069         --------
2070         >>> arr = xr.DataArray(
2071         ...     np.arange(6).reshape(2, 3),
2072         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2073         ... )
2074         >>> arr
2075         <xarray.DataArray (x: 2, y: 3)>
2076         array([[0, 1, 2],
2077                [3, 4, 5]])
2078         Coordinates:
2079           * x        (x) <U1 'a' 'b'
2080           * y        (y) int64 0 1 2
2081         >>> stacked = arr.stack(z=("x", "y"))
2082         >>> stacked.indexes["z"]
2083         MultiIndex([('a', 0),
2084                     ('a', 1),
2085                     ('a', 2),
2086                     ('b', 0),
2087                     ('b', 1),
2088                     ('b', 2)],
2089                    names=['x', 'y'])
2090         >>> roundtripped = stacked.unstack()
2091         >>> arr.identical(roundtripped)
2092         True
2093 
2094         See Also
2095         --------
2096         DataArray.stack
2097         """
2098         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2099         return self._from_temp_dataset(ds)
2100 
2101     def to_unstacked_dataset(self, dim, level=0):
2102         """Unstack DataArray expanding to Dataset along a given level of a
2103         stacked coordinate.
2104 
2105         This is the inverse operation of Dataset.to_stacked_array.
2106 
2107         Parameters
2108         ----------
2109         dim : str
2110             Name of existing dimension to unstack
2111         level : int or str
2112             The MultiIndex level to expand to a dataset along. Can either be
2113             the integer index of the level or its name.
2114         label : int, default: 0
2115             Label of the level to expand dataset along. Overrides the label
2116             argument if given.
2117 
2118         Returns
2119         -------
2120         unstacked: Dataset
2121 
2122         Examples
2123         --------
2124         >>> import xarray as xr
2125         >>> arr = xr.DataArray(
2126         ...     np.arange(6).reshape(2, 3),
2127         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2128         ... )
2129         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2130         >>> data
2131         <xarray.Dataset>
2132         Dimensions:  (x: 2, y: 3)
2133         Coordinates:
2134           * x        (x) <U1 'a' 'b'
2135           * y        (y) int64 0 1 2
2136         Data variables:
2137             a        (x, y) int64 0 1 2 3 4 5
2138             b        (x) int64 0 3
2139         >>> stacked = data.to_stacked_array("z", ["x"])
2140         >>> stacked.indexes["z"]
2141         MultiIndex([('a', 0.0),
2142                     ('a', 1.0),
2143                     ('a', 2.0),
2144                     ('b', nan)],
2145                    names=['variable', 'y'])
2146         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2147         >>> data.identical(roundtripped)
2148         True
2149 
2150         See Also
2151         --------
2152         Dataset.to_stacked_array
2153         """
2154 
2155         idx = self.indexes[dim]
2156         if not isinstance(idx, pd.MultiIndex):
2157             raise ValueError(f"'{dim}' is not a stacked coordinate")
2158 
2159         level_number = idx._get_level_number(level)
2160         variables = idx.levels[level_number]
2161         variable_dim = idx.names[level_number]
2162 
2163         # pull variables out of datarray
2164         data_dict = {}
2165         for k in variables:
2166             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2167 
2168         # unstacked dataset
2169         return Dataset(data_dict)
2170 
2171     def transpose(
2172         self,
2173         *dims: Hashable,
2174         transpose_coords: bool = True,
2175         missing_dims: str = "raise",
2176     ) -> "DataArray":
2177         """Return a new DataArray object with transposed dimensions.
2178 
2179         Parameters
2180         ----------
2181         *dims : hashable, optional
2182             By default, reverse the dimensions. Otherwise, reorder the
2183             dimensions to this order.
2184         transpose_coords : bool, default: True
2185             If True, also transpose the coordinates of this DataArray.
2186         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2187             What to do if dimensions that should be selected from are not present in the
2188             DataArray:
2189             - "raise": raise an exception
2190             - "warning": raise a warning, and ignore the missing dimensions
2191             - "ignore": ignore the missing dimensions
2192 
2193         Returns
2194         -------
2195         transposed : DataArray
2196             The returned DataArray's array is transposed.
2197 
2198         Notes
2199         -----
2200         This operation returns a view of this array's data. It is
2201         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2202         -- the data will be fully loaded.
2203 
2204         See Also
2205         --------
2206         numpy.transpose
2207         Dataset.transpose
2208         """
2209         if dims:
2210             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2211         variable = self.variable.transpose(*dims)
2212         if transpose_coords:
2213             coords: Dict[Hashable, Variable] = {}
2214             for name, coord in self.coords.items():
2215                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2216                 coords[name] = coord.variable.transpose(*coord_dims)
2217             return self._replace(variable, coords)
2218         else:
2219             return self._replace(variable)
2220 
2221     @property
2222     def T(self) -> "DataArray":
2223         return self.transpose()
2224 
2225     def drop_vars(
2226         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2227     ) -> "DataArray":
2228         """Returns an array with dropped variables.
2229 
2230         Parameters
2231         ----------
2232         names : hashable or iterable of hashable
2233             Name(s) of variables to drop.
2234         errors : {"raise", "ignore"}, optional
2235             If 'raise' (default), raises a ValueError error if any of the variable
2236             passed are not in the dataset. If 'ignore', any given names that are in the
2237             DataArray are dropped and no error is raised.
2238 
2239         Returns
2240         -------
2241         dropped : Dataset
2242             New Dataset copied from `self` with variables removed.
2243         """
2244         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2245         return self._from_temp_dataset(ds)
2246 
2247     def drop(
2248         self,
2249         labels: Mapping = None,
2250         dim: Hashable = None,
2251         *,
2252         errors: str = "raise",
2253         **labels_kwargs,
2254     ) -> "DataArray":
2255         """Backward compatible method based on `drop_vars` and `drop_sel`
2256 
2257         Using either `drop_vars` or `drop_sel` is encouraged
2258 
2259         See Also
2260         --------
2261         DataArray.drop_vars
2262         DataArray.drop_sel
2263         """
2264         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2265         return self._from_temp_dataset(ds)
2266 
2267     def drop_sel(
2268         self,
2269         labels: Mapping[Hashable, Any] = None,
2270         *,
2271         errors: str = "raise",
2272         **labels_kwargs,
2273     ) -> "DataArray":
2274         """Drop index labels from this DataArray.
2275 
2276         Parameters
2277         ----------
2278         labels : mapping of hashable to Any
2279             Index labels to drop
2280         errors : {"raise", "ignore"}, optional
2281             If 'raise' (default), raises a ValueError error if
2282             any of the index labels passed are not
2283             in the dataset. If 'ignore', any given labels that are in the
2284             dataset are dropped and no error is raised.
2285         **labels_kwargs : {dim: label, ...}, optional
2286             The keyword arguments form of ``dim`` and ``labels``
2287 
2288         Returns
2289         -------
2290         dropped : DataArray
2291         """
2292         if labels_kwargs or isinstance(labels, dict):
2293             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2294 
2295         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2296         return self._from_temp_dataset(ds)
2297 
2298     def drop_isel(self, indexers=None, **indexers_kwargs):
2299         """Drop index positions from this DataArray.
2300 
2301         Parameters
2302         ----------
2303         indexers : mapping of hashable to Any
2304             Index locations to drop
2305         **indexers_kwargs : {dim: position, ...}, optional
2306             The keyword arguments form of ``dim`` and ``positions``
2307 
2308         Returns
2309         -------
2310         dropped : DataArray
2311 
2312         Raises
2313         ------
2314         IndexError
2315         """
2316         dataset = self._to_temp_dataset()
2317         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2318         return self._from_temp_dataset(dataset)
2319 
2320     def dropna(
2321         self, dim: Hashable, how: str = "any", thresh: int = None
2322     ) -> "DataArray":
2323         """Returns a new array with dropped labels for missing values along
2324         the provided dimension.
2325 
2326         Parameters
2327         ----------
2328         dim : hashable
2329             Dimension along which to drop missing values. Dropping along
2330             multiple dimensions simultaneously is not yet supported.
2331         how : {"any", "all"}, optional
2332             * any : if any NA values are present, drop that label
2333             * all : if all values are NA, drop that label
2334         thresh : int, default: None
2335             If supplied, require this many non-NA values.
2336 
2337         Returns
2338         -------
2339         DataArray
2340         """
2341         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2342         return self._from_temp_dataset(ds)
2343 
2344     def fillna(self, value: Any) -> "DataArray":
2345         """Fill missing values in this object.
2346 
2347         This operation follows the normal broadcasting and alignment rules that
2348         xarray uses for binary arithmetic, except the result is aligned to this
2349         object (``join='left'``) instead of aligned to the intersection of
2350         index coordinates (``join='inner'``).
2351 
2352         Parameters
2353         ----------
2354         value : scalar, ndarray or DataArray
2355             Used to fill all matching missing values in this array. If the
2356             argument is a DataArray, it is first aligned with (reindexed to)
2357             this array.
2358 
2359         Returns
2360         -------
2361         DataArray
2362         """
2363         if utils.is_dict_like(value):
2364             raise TypeError(
2365                 "cannot provide fill value as a dictionary with "
2366                 "fillna on a DataArray"
2367             )
2368         out = ops.fillna(self, value)
2369         return out
2370 
2371     def interpolate_na(
2372         self,
2373         dim: Hashable = None,
2374         method: str = "linear",
2375         limit: int = None,
2376         use_coordinate: Union[bool, str] = True,
2377         max_gap: Union[
2378             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2379         ] = None,
2380         keep_attrs: bool = None,
2381         **kwargs: Any,
2382     ) -> "DataArray":
2383         """Fill in NaNs by interpolating according to different methods.
2384 
2385         Parameters
2386         ----------
2387         dim : str
2388             Specifies the dimension along which to interpolate.
2389         method : str, optional
2390             String indicating which method to use for interpolation:
2391 
2392             - 'linear': linear interpolation (Default). Additional keyword
2393               arguments are passed to :py:func:`numpy.interp`
2394             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2395               are passed to :py:func:`scipy.interpolate.interp1d`. If
2396               ``method='polynomial'``, the ``order`` keyword argument must also be
2397               provided.
2398             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2399               respective :py:class:`scipy.interpolate` classes.
2400         use_coordinate : bool or str, default: True
2401             Specifies which index to use as the x values in the interpolation
2402             formulated as `y = f(x)`. If False, values are treated as if
2403             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2404             used. If ``use_coordinate`` is a string, it specifies the name of a
2405             coordinate variariable to use as the index.
2406         limit : int, default: None
2407             Maximum number of consecutive NaNs to fill. Must be greater than 0
2408             or None for no limit. This filling is done regardless of the size of
2409             the gap in the data. To only interpolate over gaps less than a given length,
2410             see ``max_gap``.
2411         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2412             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2413             Use None for no limit. When interpolating along a datetime64 dimension
2414             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2415 
2416             - a string that is valid input for pandas.to_timedelta
2417             - a :py:class:`numpy.timedelta64` object
2418             - a :py:class:`pandas.Timedelta` object
2419             - a :py:class:`datetime.timedelta` object
2420 
2421             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2422             dimensions has not been implemented yet. Gap length is defined as the difference
2423             between coordinate values at the first data point after a gap and the last value
2424             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2425             between coordinate values at the first (last) valid data point and the first (last) NaN.
2426             For example, consider::
2427 
2428                 <xarray.DataArray (x: 9)>
2429                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2430                 Coordinates:
2431                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2432 
2433             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2434         keep_attrs : bool, default: True
2435             If True, the dataarray's attributes (`attrs`) will be copied from
2436             the original object to the new one.  If False, the new
2437             object will be returned without attributes.
2438         kwargs : dict, optional
2439             parameters passed verbatim to the underlying interpolation function
2440 
2441         Returns
2442         -------
2443         interpolated: DataArray
2444             Filled in DataArray.
2445 
2446         See Also
2447         --------
2448         numpy.interp
2449         scipy.interpolate
2450 
2451         Examples
2452         --------
2453         >>> da = xr.DataArray(
2454         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2455         ... )
2456         >>> da
2457         <xarray.DataArray (x: 5)>
2458         array([nan,  2.,  3., nan,  0.])
2459         Coordinates:
2460           * x        (x) int64 0 1 2 3 4
2461 
2462         >>> da.interpolate_na(dim="x", method="linear")
2463         <xarray.DataArray (x: 5)>
2464         array([nan, 2. , 3. , 1.5, 0. ])
2465         Coordinates:
2466           * x        (x) int64 0 1 2 3 4
2467 
2468         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2469         <xarray.DataArray (x: 5)>
2470         array([1. , 2. , 3. , 1.5, 0. ])
2471         Coordinates:
2472           * x        (x) int64 0 1 2 3 4
2473         """
2474         from .missing import interp_na
2475 
2476         return interp_na(
2477             self,
2478             dim=dim,
2479             method=method,
2480             limit=limit,
2481             use_coordinate=use_coordinate,
2482             max_gap=max_gap,
2483             keep_attrs=keep_attrs,
2484             **kwargs,
2485         )
2486 
2487     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2488         """Fill NaN values by propogating values forward
2489 
2490         *Requires bottleneck.*
2491 
2492         Parameters
2493         ----------
2494         dim : hashable
2495             Specifies the dimension along which to propagate values when
2496             filling.
2497         limit : int, default: None
2498             The maximum number of consecutive NaN values to forward fill. In
2499             other words, if there is a gap with more than this number of
2500             consecutive NaNs, it will only be partially filled. Must be greater
2501             than 0 or None for no limit.
2502 
2503         Returns
2504         -------
2505         DataArray
2506         """
2507         from .missing import ffill
2508 
2509         return ffill(self, dim, limit=limit)
2510 
2511     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2512         """Fill NaN values by propogating values backward
2513 
2514         *Requires bottleneck.*
2515 
2516         Parameters
2517         ----------
2518         dim : str
2519             Specifies the dimension along which to propagate values when
2520             filling.
2521         limit : int, default: None
2522             The maximum number of consecutive NaN values to backward fill. In
2523             other words, if there is a gap with more than this number of
2524             consecutive NaNs, it will only be partially filled. Must be greater
2525             than 0 or None for no limit.
2526 
2527         Returns
2528         -------
2529         DataArray
2530         """
2531         from .missing import bfill
2532 
2533         return bfill(self, dim, limit=limit)
2534 
2535     def combine_first(self, other: "DataArray") -> "DataArray":
2536         """Combine two DataArray objects, with union of coordinates.
2537 
2538         This operation follows the normal broadcasting and alignment rules of
2539         ``join='outer'``.  Default to non-null values of array calling the
2540         method.  Use np.nan to fill in vacant cells after alignment.
2541 
2542         Parameters
2543         ----------
2544         other : DataArray
2545             Used to fill all matching missing values in this array.
2546 
2547         Returns
2548         -------
2549         DataArray
2550         """
2551         return ops.fillna(self, other, join="outer")
2552 
2553     def reduce(
2554         self,
2555         func: Callable[..., Any],
2556         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2557         axis: Union[None, int, Sequence[int]] = None,
2558         keep_attrs: bool = None,
2559         keepdims: bool = False,
2560         **kwargs: Any,
2561     ) -> "DataArray":
2562         """Reduce this array by applying `func` along some dimension(s).
2563 
2564         Parameters
2565         ----------
2566         func : callable
2567             Function which can be called in the form
2568             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2569             np.ndarray over an integer valued axis.
2570         dim : hashable or sequence of hashable, optional
2571             Dimension(s) over which to apply `func`.
2572         axis : int or sequence of int, optional
2573             Axis(es) over which to repeatedly apply `func`. Only one of the
2574             'dim' and 'axis' arguments can be supplied. If neither are
2575             supplied, then the reduction is calculated over the flattened array
2576             (by calling `f(x)` without an axis argument).
2577         keep_attrs : bool, optional
2578             If True, the variable's attributes (`attrs`) will be copied from
2579             the original object to the new one.  If False (default), the new
2580             object will be returned without attributes.
2581         keepdims : bool, default: False
2582             If True, the dimensions which are reduced are left in the result
2583             as dimensions of size one. Coordinates that use these dimensions
2584             are removed.
2585         **kwargs : dict
2586             Additional keyword arguments passed on to `func`.
2587 
2588         Returns
2589         -------
2590         reduced : DataArray
2591             DataArray with this object's array replaced with an array with
2592             summarized data and the indicated dimension(s) removed.
2593         """
2594 
2595         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2596         return self._replace_maybe_drop_dims(var)
2597 
2598     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2599         """Convert this array into a pandas object with the same shape.
2600 
2601         The type of the returned object depends on the number of DataArray
2602         dimensions:
2603 
2604         * 0D -> `xarray.DataArray`
2605         * 1D -> `pandas.Series`
2606         * 2D -> `pandas.DataFrame`
2607 
2608         Only works for arrays with 2 or fewer dimensions.
2609 
2610         The DataArray constructor performs the inverse transformation.
2611         """
2612         # TODO: consolidate the info about pandas constructors and the
2613         # attributes that correspond to their indexes into a separate module?
2614         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2615         try:
2616             constructor = constructors[self.ndim]
2617         except KeyError:
2618             raise ValueError(
2619                 "cannot convert arrays with %s dimensions into "
2620                 "pandas objects" % self.ndim
2621             )
2622         indexes = [self.get_index(dim) for dim in self.dims]
2623         return constructor(self.values, *indexes)
2624 
2625     def to_dataframe(
2626         self, name: Hashable = None, dim_order: List[Hashable] = None
2627     ) -> pd.DataFrame:
2628         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2629 
2630         The DataFrame is indexed by the Cartesian product of index coordinates
2631         (in the form of a :py:class:`pandas.MultiIndex`).
2632 
2633         Other coordinates are included as columns in the DataFrame.
2634 
2635         Parameters
2636         ----------
2637         name
2638             Name to give to this array (required if unnamed).
2639         dim_order
2640             Hierarchical dimension order for the resulting dataframe.
2641             Array content is transposed to this order and then written out as flat
2642             vectors in contiguous order, so the last dimension in this list
2643             will be contiguous in the resulting DataFrame. This has a major
2644             influence on which operations are efficient on the resulting
2645             dataframe.
2646 
2647             If provided, must include all dimensions of this DataArray. By default,
2648             dimensions are sorted according to the DataArray dimensions order.
2649 
2650         Returns
2651         -------
2652         result
2653             DataArray as a pandas DataFrame.
2654 
2655         """
2656         if name is None:
2657             name = self.name
2658         if name is None:
2659             raise ValueError(
2660                 "cannot convert an unnamed DataArray to a "
2661                 "DataFrame: use the ``name`` parameter"
2662             )
2663         if self.ndim == 0:
2664             raise ValueError("cannot convert a scalar to a DataFrame")
2665 
2666         # By using a unique name, we can convert a DataArray into a DataFrame
2667         # even if it shares a name with one of its coordinates.
2668         # I would normally use unique_name = object() but that results in a
2669         # dataframe with columns in the wrong order, for reasons I have not
2670         # been able to debug (possibly a pandas bug?).
2671         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2672         ds = self._to_dataset_whole(name=unique_name)
2673 
2674         if dim_order is None:
2675             ordered_dims = dict(zip(self.dims, self.shape))
2676         else:
2677             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2678 
2679         df = ds._to_dataframe(ordered_dims)
2680         df.columns = [name if c == unique_name else c for c in df.columns]
2681         return df
2682 
2683     def to_series(self) -> pd.Series:
2684         """Convert this array into a pandas.Series.
2685 
2686         The Series is indexed by the Cartesian product of index coordinates
2687         (in the form of a :py:class:`pandas.MultiIndex`).
2688         """
2689         index = self.coords.to_index()
2690         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2691 
2692     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2693         """Convert this array into a numpy.ma.MaskedArray
2694 
2695         Parameters
2696         ----------
2697         copy : bool, default: True
2698             If True make a copy of the array in the result. If False,
2699             a MaskedArray view of DataArray.values is returned.
2700 
2701         Returns
2702         -------
2703         result : MaskedArray
2704             Masked where invalid values (nan or inf) occur.
2705         """
2706         values = self.values  # only compute lazy arrays once
2707         isnull = pd.isnull(values)
2708         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2709 
2710     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2711         """Write DataArray contents to a netCDF file.
2712 
2713         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2714 
2715         Notes
2716         -----
2717         Only xarray.Dataset objects can be written to netCDF files, so
2718         the xarray.DataArray is converted to a xarray.Dataset object
2719         containing a single variable. If the DataArray has no name, or if the
2720         name is the same as a coordinate name, then it is given the name
2721         ``"__xarray_dataarray_variable__"``.
2722 
2723         See Also
2724         --------
2725         Dataset.to_netcdf
2726         """
2727         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2728 
2729         if self.name is None:
2730             # If no name is set then use a generic xarray name
2731             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2732         elif self.name in self.coords or self.name in self.dims:
2733             # The name is the same as one of the coords names, which netCDF
2734             # doesn't support, so rename it but keep track of the old name
2735             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2736             dataset.attrs[DATAARRAY_NAME] = self.name
2737         else:
2738             # No problems with the name - so we're fine!
2739             dataset = self.to_dataset()
2740 
2741         return dataset.to_netcdf(*args, **kwargs)
2742 
2743     def to_dict(self, data: bool = True) -> dict:
2744         """
2745         Convert this xarray.DataArray into a dictionary following xarray
2746         naming conventions.
2747 
2748         Converts all variables and attributes to native Python objects.
2749         Useful for converting to json. To avoid datetime incompatibility
2750         use decode_times=False kwarg in xarray.open_dataset.
2751 
2752         Parameters
2753         ----------
2754         data : bool, optional
2755             Whether to include the actual data in the dictionary. When set to
2756             False, returns just the schema.
2757 
2758         See Also
2759         --------
2760         DataArray.from_dict
2761         """
2762         d = self.variable.to_dict(data=data)
2763         d.update({"coords": {}, "name": self.name})
2764         for k in self.coords:
2765             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2766         return d
2767 
2768     @classmethod
2769     def from_dict(cls, d: dict) -> "DataArray":
2770         """
2771         Convert a dictionary into an xarray.DataArray
2772 
2773         Input dict can take several forms:
2774 
2775         .. code:: python
2776 
2777             d = {"dims": ("t"), "data": x}
2778 
2779             d = {
2780                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2781                 "attrs": {"title": "air temperature"},
2782                 "dims": "t",
2783                 "data": x,
2784                 "name": "a",
2785             }
2786 
2787         where "t" is the name of the dimesion, "a" is the name of the array,
2788         and x and t are lists, numpy.arrays, or pandas objects.
2789 
2790         Parameters
2791         ----------
2792         d : dict
2793             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2794 
2795         Returns
2796         -------
2797         obj : xarray.DataArray
2798 
2799         See Also
2800         --------
2801         DataArray.to_dict
2802         Dataset.from_dict
2803         """
2804         coords = None
2805         if "coords" in d:
2806             try:
2807                 coords = {
2808                     k: (v["dims"], v["data"], v.get("attrs"))
2809                     for k, v in d["coords"].items()
2810                 }
2811             except KeyError as e:
2812                 raise ValueError(
2813                     "cannot convert dict when coords are missing the key "
2814                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2815                 )
2816         try:
2817             data = d["data"]
2818         except KeyError:
2819             raise ValueError("cannot convert dict without the key 'data''")
2820         else:
2821             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2822         return obj
2823 
2824     @classmethod
2825     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2826         """Convert a pandas.Series into an xarray.DataArray.
2827 
2828         If the series's index is a MultiIndex, it will be expanded into a
2829         tensor product of one-dimensional coordinates (filling in missing
2830         values with NaN). Thus this operation should be the inverse of the
2831         `to_series` method.
2832 
2833         If sparse=True, creates a sparse array instead of a dense NumPy array.
2834         Requires the pydata/sparse package.
2835 
2836         See Also
2837         --------
2838         xarray.Dataset.from_dataframe
2839         """
2840         temp_name = "__temporary_name"
2841         df = pd.DataFrame({temp_name: series})
2842         ds = Dataset.from_dataframe(df, sparse=sparse)
2843         result = cast(DataArray, ds[temp_name])
2844         result.name = series.name
2845         return result
2846 
2847     def to_cdms2(self) -> "cdms2_Variable":
2848         """Convert this array into a cdms2.Variable"""
2849         from ..convert import to_cdms2
2850 
2851         return to_cdms2(self)
2852 
2853     @classmethod
2854     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2855         """Convert a cdms2.Variable into an xarray.DataArray"""
2856         from ..convert import from_cdms2
2857 
2858         return from_cdms2(variable)
2859 
2860     def to_iris(self) -> "iris_Cube":
2861         """Convert this array into a iris.cube.Cube"""
2862         from ..convert import to_iris
2863 
2864         return to_iris(self)
2865 
2866     @classmethod
2867     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2868         """Convert a iris.cube.Cube into an xarray.DataArray"""
2869         from ..convert import from_iris
2870 
2871         return from_iris(cube)
2872 
2873     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2874         """Helper function for equals, broadcast_equals, and identical"""
2875 
2876         def compat(x, y):
2877             return getattr(x.variable, compat_str)(y.variable)
2878 
2879         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2880             self, other
2881         )
2882 
2883     def broadcast_equals(self, other: "DataArray") -> bool:
2884         """Two DataArrays are broadcast equal if they are equal after
2885         broadcasting them against each other such that they have the same
2886         dimensions.
2887 
2888         See Also
2889         --------
2890         DataArray.equals
2891         DataArray.identical
2892         """
2893         try:
2894             return self._all_compat(other, "broadcast_equals")
2895         except (TypeError, AttributeError):
2896             return False
2897 
2898     def equals(self, other: "DataArray") -> bool:
2899         """True if two DataArrays have the same dimensions, coordinates and
2900         values; otherwise False.
2901 
2902         DataArrays can still be equal (like pandas objects) if they have NaN
2903         values in the same locations.
2904 
2905         This method is necessary because `v1 == v2` for ``DataArray``
2906         does element-wise comparisons (like numpy.ndarrays).
2907 
2908         See Also
2909         --------
2910         DataArray.broadcast_equals
2911         DataArray.identical
2912         """
2913         try:
2914             return self._all_compat(other, "equals")
2915         except (TypeError, AttributeError):
2916             return False
2917 
2918     def identical(self, other: "DataArray") -> bool:
2919         """Like equals, but also checks the array name and attributes, and
2920         attributes on all coordinates.
2921 
2922         See Also
2923         --------
2924         DataArray.broadcast_equals
2925         DataArray.equals
2926         """
2927         try:
2928             return self.name == other.name and self._all_compat(other, "identical")
2929         except (TypeError, AttributeError):
2930             return False
2931 
2932     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2933         # use the same naming heuristics as pandas:
2934         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2935         other_name = getattr(other, "name", _default)
2936         if other_name is _default or other_name == self.name:
2937             return self.name
2938         else:
2939             return None
2940 
2941     def __array_wrap__(self, obj, context=None) -> "DataArray":
2942         new_var = self.variable.__array_wrap__(obj, context)
2943         return self._replace(new_var)
2944 
2945     def __matmul__(self, obj):
2946         return self.dot(obj)
2947 
2948     def __rmatmul__(self, other):
2949         # currently somewhat duplicative, as only other DataArrays are
2950         # compatible with matmul
2951         return computation.dot(other, self)
2952 
2953     @staticmethod
2954     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2955         @functools.wraps(f)
2956         def func(self, *args, **kwargs):
2957             keep_attrs = kwargs.pop("keep_attrs", None)
2958             if keep_attrs is None:
2959                 keep_attrs = _get_keep_attrs(default=True)
2960             with warnings.catch_warnings():
2961                 warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
2962                 warnings.filterwarnings(
2963                     "ignore", r"Mean of empty slice", category=RuntimeWarning
2964                 )
2965                 with np.errstate(all="ignore"):
2966                     da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2967                 if keep_attrs:
2968                     da.attrs = self.attrs
2969                 return da
2970 
2971         return func
2972 
2973     @staticmethod
2974     def _binary_op(
2975         f: Callable[..., Any],
2976         reflexive: bool = False,
2977         join: str = None,  # see xarray.align
2978         **ignored_kwargs,
2979     ) -> Callable[..., "DataArray"]:
2980         @functools.wraps(f)
2981         def func(self, other):
2982             if isinstance(other, (Dataset, groupby.GroupBy)):
2983                 return NotImplemented
2984             if isinstance(other, DataArray):
2985                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2986                 self, other = align(self, other, join=align_type, copy=False)
2987             other_variable = getattr(other, "variable", other)
2988             other_coords = getattr(other, "coords", None)
2989 
2990             variable = (
2991                 f(self.variable, other_variable)
2992                 if not reflexive
2993                 else f(other_variable, self.variable)
2994             )
2995             coords, indexes = self.coords._merge_raw(other_coords)
2996             name = self._result_name(other)
2997 
2998             return self._replace(variable, coords, name, indexes=indexes)
2999 
3000         return func
3001 
3002     @staticmethod
3003     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
3004         @functools.wraps(f)
3005         def func(self, other):
3006             if isinstance(other, groupby.GroupBy):
3007                 raise TypeError(
3008                     "in-place operations between a DataArray and "
3009                     "a grouped object are not permitted"
3010                 )
3011             # n.b. we can't align other to self (with other.reindex_like(self))
3012             # because `other` may be converted into floats, which would cause
3013             # in-place arithmetic to fail unpredictably. Instead, we simply
3014             # don't support automatic alignment with in-place arithmetic.
3015             other_coords = getattr(other, "coords", None)
3016             other_variable = getattr(other, "variable", other)
3017             try:
3018                 with self.coords._merge_inplace(other_coords):
3019                     f(self.variable, other_variable)
3020             except MergeError as exc:
3021                 raise MergeError(
3022                     "Automatic alignment is not supported for in-place operations.\n"
3023                     "Consider aligning the indices manually or using a not-in-place operation.\n"
3024                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3025                 ) from exc
3026             return self
3027 
3028         return func
3029 
3030     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
3031         self.attrs = other.attrs
3032 
3033     plot = utils.UncachedAccessor(_PlotMethods)
3034 
3035     def _title_for_slice(self, truncate: int = 50) -> str:
3036         """
3037         If the dataarray has 1 dimensional coordinates or comes from a slice
3038         we can show that info in the title
3039 
3040         Parameters
3041         ----------
3042         truncate : int, default: 50
3043             maximum number of characters for title
3044 
3045         Returns
3046         -------
3047         title : string
3048             Can be used for plot titles
3049 
3050         """
3051         one_dims = []
3052         for dim, coord in self.coords.items():
3053             if coord.size == 1:
3054                 one_dims.append(
3055                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
3056                 )
3057 
3058         title = ", ".join(one_dims)
3059         if len(title) > truncate:
3060             title = title[: (truncate - 3)] + "..."
3061 
3062         return title
3063 
3064     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
3065         """Calculate the n-th order discrete difference along given axis.
3066 
3067         Parameters
3068         ----------
3069         dim : hashable
3070             Dimension over which to calculate the finite difference.
3071         n : int, optional
3072             The number of times values are differenced.
3073         label : hashable, optional
3074             The new coordinate in dimension ``dim`` will have the
3075             values of either the minuend's or subtrahend's coordinate
3076             for values 'upper' and 'lower', respectively.  Other
3077             values are not supported.
3078 
3079         Returns
3080         -------
3081         difference : same type as caller
3082             The n-th order finite difference of this object.
3083 
3084         Notes
3085         -----
3086         `n` matches numpy's behavior and is different from pandas' first argument named
3087         `periods`.
3088 
3089         Examples
3090         --------
3091         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3092         >>> arr.diff("x")
3093         <xarray.DataArray (x: 3)>
3094         array([0, 1, 0])
3095         Coordinates:
3096           * x        (x) int64 2 3 4
3097         >>> arr.diff("x", 2)
3098         <xarray.DataArray (x: 2)>
3099         array([ 1, -1])
3100         Coordinates:
3101           * x        (x) int64 3 4
3102 
3103         See Also
3104         --------
3105         DataArray.differentiate
3106         """
3107         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3108         return self._from_temp_dataset(ds)
3109 
3110     def shift(
3111         self,
3112         shifts: Mapping[Hashable, int] = None,
3113         fill_value: Any = dtypes.NA,
3114         **shifts_kwargs: int,
3115     ) -> "DataArray":
3116         """Shift this array by an offset along one or more dimensions.
3117 
3118         Only the data is moved; coordinates stay in place. Values shifted from
3119         beyond array bounds are replaced by NaN. This is consistent with the
3120         behavior of ``shift`` in pandas.
3121 
3122         Parameters
3123         ----------
3124         shifts : mapping of hashable to int, optional
3125             Integer offset to shift along each of the given dimensions.
3126             Positive offsets shift to the right; negative offsets shift to the
3127             left.
3128         fill_value : scalar, optional
3129             Value to use for newly missing values
3130         **shifts_kwargs
3131             The keyword arguments form of ``shifts``.
3132             One of shifts or shifts_kwargs must be provided.
3133 
3134         Returns
3135         -------
3136         shifted : DataArray
3137             DataArray with the same coordinates and attributes but shifted
3138             data.
3139 
3140         See Also
3141         --------
3142         roll
3143 
3144         Examples
3145         --------
3146         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3147         >>> arr.shift(x=1)
3148         <xarray.DataArray (x: 3)>
3149         array([nan,  5.,  6.])
3150         Dimensions without coordinates: x
3151         """
3152         variable = self.variable.shift(
3153             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3154         )
3155         return self._replace(variable=variable)
3156 
3157     def roll(
3158         self,
3159         shifts: Mapping[Hashable, int] = None,
3160         roll_coords: bool = None,
3161         **shifts_kwargs: int,
3162     ) -> "DataArray":
3163         """Roll this array by an offset along one or more dimensions.
3164 
3165         Unlike shift, roll may rotate all variables, including coordinates
3166         if specified. The direction of rotation is consistent with
3167         :py:func:`numpy.roll`.
3168 
3169         Parameters
3170         ----------
3171         shifts : mapping of hashable to int, optional
3172             Integer offset to rotate each of the given dimensions.
3173             Positive offsets roll to the right; negative offsets roll to the
3174             left.
3175         roll_coords : bool
3176             Indicates whether to roll the coordinates by the offset
3177             The current default of roll_coords (None, equivalent to True) is
3178             deprecated and will change to False in a future version.
3179             Explicitly pass roll_coords to silence the warning.
3180         **shifts_kwargs
3181             The keyword arguments form of ``shifts``.
3182             One of shifts or shifts_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         rolled : DataArray
3187             DataArray with the same attributes but rolled data and coordinates.
3188 
3189         See Also
3190         --------
3191         shift
3192 
3193         Examples
3194         --------
3195         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3196         >>> arr.roll(x=1)
3197         <xarray.DataArray (x: 3)>
3198         array([7, 5, 6])
3199         Dimensions without coordinates: x
3200         """
3201         ds = self._to_temp_dataset().roll(
3202             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3203         )
3204         return self._from_temp_dataset(ds)
3205 
3206     @property
3207     def real(self) -> "DataArray":
3208         return self._replace(self.variable.real)
3209 
3210     @property
3211     def imag(self) -> "DataArray":
3212         return self._replace(self.variable.imag)
3213 
3214     def dot(
3215         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3216     ) -> "DataArray":
3217         """Perform dot product of two DataArrays along their shared dims.
3218 
3219         Equivalent to taking taking tensordot over all shared dims.
3220 
3221         Parameters
3222         ----------
3223         other : DataArray
3224             The other array with which the dot product is performed.
3225         dims : ..., hashable or sequence of hashable, optional
3226             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3227             If not specified, then all the common dimensions are summed over.
3228 
3229         Returns
3230         -------
3231         result : DataArray
3232             Array resulting from the dot product over all shared dimensions.
3233 
3234         See Also
3235         --------
3236         dot
3237         numpy.tensordot
3238 
3239         Examples
3240         --------
3241         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3242         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3243         >>> dm_vals = np.arange(4)
3244         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3245 
3246         >>> dm.dims
3247         ('z',)
3248 
3249         >>> da.dims
3250         ('x', 'y', 'z')
3251 
3252         >>> dot_result = da.dot(dm)
3253         >>> dot_result.dims
3254         ('x', 'y')
3255 
3256         """
3257         if isinstance(other, Dataset):
3258             raise NotImplementedError(
3259                 "dot products are not yet supported with Dataset objects."
3260             )
3261         if not isinstance(other, DataArray):
3262             raise TypeError("dot only operates on DataArrays.")
3263 
3264         return computation.dot(self, other, dims=dims)
3265 
3266     def sortby(
3267         self,
3268         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3269         ascending: bool = True,
3270     ) -> "DataArray":
3271         """Sort object by labels or values (along an axis).
3272 
3273         Sorts the dataarray, either along specified dimensions,
3274         or according to values of 1-D dataarrays that share dimension
3275         with calling object.
3276 
3277         If the input variables are dataarrays, then the dataarrays are aligned
3278         (via left-join) to the calling object prior to sorting by cell values.
3279         NaNs are sorted to the end, following Numpy convention.
3280 
3281         If multiple sorts along the same dimension is
3282         given, numpy's lexsort is performed along that dimension:
3283         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3284         and the FIRST key in the sequence is used as the primary sort key,
3285         followed by the 2nd key, etc.
3286 
3287         Parameters
3288         ----------
3289         variables : hashable, DataArray, or sequence of hashable or DataArray
3290             1D DataArray objects or name(s) of 1D variable(s) in
3291             coords whose values are used to sort this array.
3292         ascending : bool, optional
3293             Whether to sort by ascending or descending order.
3294 
3295         Returns
3296         -------
3297         sorted : DataArray
3298             A new dataarray where all the specified dims are sorted by dim
3299             labels.
3300 
3301         Examples
3302         --------
3303         >>> da = xr.DataArray(
3304         ...     np.random.rand(5),
3305         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3306         ...     dims="time",
3307         ... )
3308         >>> da
3309         <xarray.DataArray (time: 5)>
3310         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3311         Coordinates:
3312           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3313 
3314         >>> da.sortby(da)
3315         <xarray.DataArray (time: 5)>
3316         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3317         Coordinates:
3318           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3319         """
3320         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3321         return self._from_temp_dataset(ds)
3322 
3323     def quantile(
3324         self,
3325         q: Any,
3326         dim: Union[Hashable, Sequence[Hashable], None] = None,
3327         interpolation: str = "linear",
3328         keep_attrs: bool = None,
3329         skipna: bool = True,
3330     ) -> "DataArray":
3331         """Compute the qth quantile of the data along the specified dimension.
3332 
3333         Returns the qth quantiles(s) of the array elements.
3334 
3335         Parameters
3336         ----------
3337         q : float or array-like of float
3338             Quantile to compute, which must be between 0 and 1 inclusive.
3339         dim : hashable or sequence of hashable, optional
3340             Dimension(s) over which to apply quantile.
3341         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3342             This optional parameter specifies the interpolation method to
3343             use when the desired quantile lies between two data points
3344             ``i < j``:
3345 
3346                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3347                   the fractional part of the index surrounded by ``i`` and
3348                   ``j``.
3349                 - lower: ``i``.
3350                 - higher: ``j``.
3351                 - nearest: ``i`` or ``j``, whichever is nearest.
3352                 - midpoint: ``(i + j) / 2``.
3353         keep_attrs : bool, optional
3354             If True, the dataset's attributes (`attrs`) will be copied from
3355             the original object to the new one.  If False (default), the new
3356             object will be returned without attributes.
3357         skipna : bool, optional
3358             Whether to skip missing values when aggregating.
3359 
3360         Returns
3361         -------
3362         quantiles : DataArray
3363             If `q` is a single quantile, then the result
3364             is a scalar. If multiple percentiles are given, first axis of
3365             the result corresponds to the quantile and a quantile dimension
3366             is added to the return array. The other dimensions are the
3367             dimensions that remain after the reduction of the array.
3368 
3369         See Also
3370         --------
3371         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3372 
3373         Examples
3374         --------
3375         >>> da = xr.DataArray(
3376         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3377         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3378         ...     dims=("x", "y"),
3379         ... )
3380         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3381         <xarray.DataArray ()>
3382         array(0.7)
3383         Coordinates:
3384             quantile  float64 0.0
3385         >>> da.quantile(0, dim="x")
3386         <xarray.DataArray (y: 4)>
3387         array([0.7, 4.2, 2.6, 1.5])
3388         Coordinates:
3389           * y         (y) float64 1.0 1.5 2.0 2.5
3390             quantile  float64 0.0
3391         >>> da.quantile([0, 0.5, 1])
3392         <xarray.DataArray (quantile: 3)>
3393         array([0.7, 3.4, 9.4])
3394         Coordinates:
3395           * quantile  (quantile) float64 0.0 0.5 1.0
3396         >>> da.quantile([0, 0.5, 1], dim="x")
3397         <xarray.DataArray (quantile: 3, y: 4)>
3398         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3399                [3.6 , 5.75, 6.  , 1.7 ],
3400                [6.5 , 7.3 , 9.4 , 1.9 ]])
3401         Coordinates:
3402           * y         (y) float64 1.0 1.5 2.0 2.5
3403           * quantile  (quantile) float64 0.0 0.5 1.0
3404         """
3405 
3406         ds = self._to_temp_dataset().quantile(
3407             q,
3408             dim=dim,
3409             keep_attrs=keep_attrs,
3410             interpolation=interpolation,
3411             skipna=skipna,
3412         )
3413         return self._from_temp_dataset(ds)
3414 
3415     def rank(
3416         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3417     ) -> "DataArray":
3418         """Ranks the data.
3419 
3420         Equal values are assigned a rank that is the average of the ranks that
3421         would have been otherwise assigned to all of the values within that
3422         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3423 
3424         NaNs in the input array are returned as NaNs.
3425 
3426         The `bottleneck` library is required.
3427 
3428         Parameters
3429         ----------
3430         dim : hashable
3431             Dimension over which to compute rank.
3432         pct : bool, optional
3433             If True, compute percentage ranks, otherwise compute integer ranks.
3434         keep_attrs : bool, optional
3435             If True, the dataset's attributes (`attrs`) will be copied from
3436             the original object to the new one.  If False (default), the new
3437             object will be returned without attributes.
3438 
3439         Returns
3440         -------
3441         ranked : DataArray
3442             DataArray with the same coordinates and dtype 'float64'.
3443 
3444         Examples
3445         --------
3446         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3447         >>> arr.rank("x")
3448         <xarray.DataArray (x: 3)>
3449         array([1., 2., 3.])
3450         Dimensions without coordinates: x
3451         """
3452 
3453         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3454         return self._from_temp_dataset(ds)
3455 
3456     def differentiate(
3457         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3458     ) -> "DataArray":
3459         """ Differentiate the array with the second order accurate central
3460         differences.
3461 
3462         .. note::
3463             This feature is limited to simple cartesian geometry, i.e. coord
3464             must be one dimensional.
3465 
3466         Parameters
3467         ----------
3468         coord : hashable
3469             The coordinate to be used to compute the gradient.
3470         edge_order : {1, 2}, default: 1
3471             N-th order accurate differences at the boundaries.
3472         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3473                          "us", "ns", "ps", "fs", "as"} or None, optional
3474             Unit to compute gradient. Only valid for datetime coordinate.
3475 
3476         Returns
3477         -------
3478         differentiated: DataArray
3479 
3480         See also
3481         --------
3482         numpy.gradient: corresponding numpy function
3483 
3484         Examples
3485         --------
3486 
3487         >>> da = xr.DataArray(
3488         ...     np.arange(12).reshape(4, 3),
3489         ...     dims=["x", "y"],
3490         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3491         ... )
3492         >>> da
3493         <xarray.DataArray (x: 4, y: 3)>
3494         array([[ 0,  1,  2],
3495                [ 3,  4,  5],
3496                [ 6,  7,  8],
3497                [ 9, 10, 11]])
3498         Coordinates:
3499           * x        (x) float64 0.0 0.1 1.1 1.2
3500         Dimensions without coordinates: y
3501         >>>
3502         >>> da.differentiate("x")
3503         <xarray.DataArray (x: 4, y: 3)>
3504         array([[30.        , 30.        , 30.        ],
3505                [27.54545455, 27.54545455, 27.54545455],
3506                [27.54545455, 27.54545455, 27.54545455],
3507                [30.        , 30.        , 30.        ]])
3508         Coordinates:
3509           * x        (x) float64 0.0 0.1 1.1 1.2
3510         Dimensions without coordinates: y
3511         """
3512         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3513         return self._from_temp_dataset(ds)
3514 
3515     def integrate(
3516         self,
3517         coord: Union[Hashable, Sequence[Hashable]] = None,
3518         datetime_unit: str = None,
3519         *,
3520         dim: Union[Hashable, Sequence[Hashable]] = None,
3521     ) -> "DataArray":
3522         """Integrate along the given coordinate using the trapezoidal rule.
3523 
3524         .. note::
3525             This feature is limited to simple cartesian geometry, i.e. coord
3526             must be one dimensional.
3527 
3528         Parameters
3529         ----------
3530         coord : hashable, or sequence of hashable
3531             Coordinate(s) used for the integration.
3532         dim : hashable, or sequence of hashable
3533             Coordinate(s) used for the integration.
3534         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3535                         'ps', 'fs', 'as'}, optional
3536             Specify the unit if a datetime coordinate is used.
3537 
3538         Returns
3539         -------
3540         integrated : DataArray
3541 
3542         See also
3543         --------
3544         Dataset.integrate
3545         numpy.trapz : corresponding numpy function
3546 
3547         Examples
3548         --------
3549 
3550         >>> da = xr.DataArray(
3551         ...     np.arange(12).reshape(4, 3),
3552         ...     dims=["x", "y"],
3553         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3554         ... )
3555         >>> da
3556         <xarray.DataArray (x: 4, y: 3)>
3557         array([[ 0,  1,  2],
3558                [ 3,  4,  5],
3559                [ 6,  7,  8],
3560                [ 9, 10, 11]])
3561         Coordinates:
3562           * x        (x) float64 0.0 0.1 1.1 1.2
3563         Dimensions without coordinates: y
3564         >>>
3565         >>> da.integrate("x")
3566         <xarray.DataArray (y: 3)>
3567         array([5.4, 6.6, 7.8])
3568         Dimensions without coordinates: y
3569         """
3570         if dim is not None and coord is not None:
3571             raise ValueError(
3572                 "Cannot pass both 'dim' and 'coord'. Please pass only 'coord' instead."
3573             )
3574 
3575         if dim is not None and coord is None:
3576             coord = dim
3577             msg = (
3578                 "The `dim` keyword argument to `DataArray.integrate` is "
3579                 "being replaced with `coord`, for consistency with "
3580                 "`Dataset.integrate`. Please pass `coord` instead."
3581                 " `dim` will be removed in version 0.19.0."
3582             )
3583             warnings.warn(msg, FutureWarning, stacklevel=2)
3584 
3585         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
3586         return self._from_temp_dataset(ds)
3587 
3588     def unify_chunks(self) -> "DataArray":
3589         """Unify chunk size along all chunked dimensions of this DataArray.
3590 
3591         Returns
3592         -------
3593         DataArray with consistent chunk sizes for all dask-array variables
3594 
3595         See Also
3596         --------
3597         dask.array.core.unify_chunks
3598         """
3599         ds = self._to_temp_dataset().unify_chunks()
3600         return self._from_temp_dataset(ds)
3601 
3602     def map_blocks(
3603         self,
3604         func: "Callable[..., T_DSorDA]",
3605         args: Sequence[Any] = (),
3606         kwargs: Mapping[str, Any] = None,
3607         template: Union["DataArray", "Dataset"] = None,
3608     ) -> "T_DSorDA":
3609         """
3610         Apply a function to each block of this DataArray.
3611 
3612         .. warning::
3613             This method is experimental and its signature may change.
3614 
3615         Parameters
3616         ----------
3617         func : callable
3618             User-provided function that accepts a DataArray as its first
3619             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3620             corresponding to one chunk along each chunked dimension. ``func`` will be
3621             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3622 
3623             This function must return either a single DataArray or a single Dataset.
3624 
3625             This function cannot add a new chunked dimension.
3626         args : sequence
3627             Passed to func after unpacking and subsetting any xarray objects by blocks.
3628             xarray objects in args must be aligned with this object, otherwise an error is raised.
3629         kwargs : mapping
3630             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3631             subset to blocks. Passing dask collections in kwargs is not allowed.
3632         template : DataArray or Dataset, optional
3633             xarray object representing the final result after compute is called. If not provided,
3634             the function will be first run on mocked-up data, that looks like this object but
3635             has sizes 0, to determine properties of the returned object such as dtype,
3636             variable names, attributes, new dimensions and new indexes (if any).
3637             ``template`` must be provided if the function changes the size of existing dimensions.
3638             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3639             ``attrs`` set by ``func`` will be ignored.
3640 
3641         Returns
3642         -------
3643         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3644         function.
3645 
3646         Notes
3647         -----
3648         This function is designed for when ``func`` needs to manipulate a whole xarray object
3649         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
3650         recommended to use ``apply_ufunc``.
3651 
3652         If none of the variables in this object is backed by dask arrays, calling this function is
3653         equivalent to calling ``func(obj, *args, **kwargs)``.
3654 
3655         See Also
3656         --------
3657         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
3658         xarray.DataArray.map_blocks
3659 
3660         Examples
3661         --------
3662         Calculate an anomaly from climatology using ``.groupby()``. Using
3663         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3664         its indices, and its methods like ``.groupby()``.
3665 
3666         >>> def calculate_anomaly(da, groupby_type="time.month"):
3667         ...     gb = da.groupby(groupby_type)
3668         ...     clim = gb.mean(dim="time")
3669         ...     return gb - clim
3670         ...
3671         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3672         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3673         >>> np.random.seed(123)
3674         >>> array = xr.DataArray(
3675         ...     np.random.rand(len(time)),
3676         ...     dims=["time"],
3677         ...     coords={"time": time, "month": month},
3678         ... ).chunk()
3679         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3680         <xarray.DataArray (time: 24)>
3681         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3682                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3683                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3684                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3685                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3686         Coordinates:
3687           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3688             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3689 
3690         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3691         to the function being applied in ``xr.map_blocks()``:
3692 
3693         >>> array.map_blocks(
3694         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3695         ... )  # doctest: +ELLIPSIS
3696         <xarray.DataArray (time: 24)>
3697         dask.array<calculate_anomaly-...-<this, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3698         Coordinates:
3699           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3700             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3701         """
3702         from .parallel import map_blocks
3703 
3704         return map_blocks(func, self, args, kwargs, template)
3705 
3706     def polyfit(
3707         self,
3708         dim: Hashable,
3709         deg: int,
3710         skipna: bool = None,
3711         rcond: float = None,
3712         w: Union[Hashable, Any] = None,
3713         full: bool = False,
3714         cov: bool = False,
3715     ):
3716         """
3717         Least squares polynomial fit.
3718 
3719         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3720         invalid values when `skipna = True`.
3721 
3722         Parameters
3723         ----------
3724         dim : hashable
3725             Coordinate along which to fit the polynomials.
3726         deg : int
3727             Degree of the fitting polynomial.
3728         skipna : bool, optional
3729             If True, removes all invalid values before fitting each 1D slices of the array.
3730             Default is True if data is stored in a dask.array or if there is any
3731             invalid values, False otherwise.
3732         rcond : float, optional
3733             Relative condition number to the fit.
3734         w : hashable or array-like, optional
3735             Weights to apply to the y-coordinate of the sample points.
3736             Can be an array-like object or the name of a coordinate in the dataset.
3737         full : bool, optional
3738             Whether to return the residuals, matrix rank and singular values in addition
3739             to the coefficients.
3740         cov : bool or str, optional
3741             Whether to return to the covariance matrix in addition to the coefficients.
3742             The matrix is not scaled if `cov='unscaled'`.
3743 
3744         Returns
3745         -------
3746         polyfit_results : Dataset
3747             A single dataset which contains:
3748 
3749             polyfit_coefficients
3750                 The coefficients of the best fit.
3751             polyfit_residuals
3752                 The residuals of the least-square computation (only included if `full=True`).
3753                 When the matrix rank is deficient, np.nan is returned.
3754             [dim]_matrix_rank
3755                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3756             [dim]_singular_value
3757                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3758             polyfit_covariance
3759                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3760 
3761         See Also
3762         --------
3763         numpy.polyfit
3764         """
3765         return self._to_temp_dataset().polyfit(
3766             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3767         )
3768 
3769     def pad(
3770         self,
3771         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3772         mode: str = "constant",
3773         stat_length: Union[
3774             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3775         ] = None,
3776         constant_values: Union[
3777             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3778         ] = None,
3779         end_values: Union[
3780             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3781         ] = None,
3782         reflect_type: str = None,
3783         **pad_width_kwargs: Any,
3784     ) -> "DataArray":
3785         """Pad this array along one or more dimensions.
3786 
3787         .. warning::
3788             This function is experimental and its behaviour is likely to change
3789             especially regarding padding of dimension coordinates (or IndexVariables).
3790 
3791         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3792         coordinates will be padded with the same mode, otherwise coordinates
3793         are padded using the "constant" mode with fill_value dtypes.NA.
3794 
3795         Parameters
3796         ----------
3797         pad_width : mapping of hashable to tuple of int
3798             Mapping with the form of {dim: (pad_before, pad_after)}
3799             describing the number of values padded along each dimension.
3800             {dim: pad} is a shortcut for pad_before = pad_after = pad
3801         mode : str, default: "constant"
3802             One of the following string values (taken from numpy docs)
3803 
3804             'constant' (default)
3805                 Pads with a constant value.
3806             'edge'
3807                 Pads with the edge values of array.
3808             'linear_ramp'
3809                 Pads with the linear ramp between end_value and the
3810                 array edge value.
3811             'maximum'
3812                 Pads with the maximum value of all or part of the
3813                 vector along each axis.
3814             'mean'
3815                 Pads with the mean value of all or part of the
3816                 vector along each axis.
3817             'median'
3818                 Pads with the median value of all or part of the
3819                 vector along each axis.
3820             'minimum'
3821                 Pads with the minimum value of all or part of the
3822                 vector along each axis.
3823             'reflect'
3824                 Pads with the reflection of the vector mirrored on
3825                 the first and last values of the vector along each
3826                 axis.
3827             'symmetric'
3828                 Pads with the reflection of the vector mirrored
3829                 along the edge of the array.
3830             'wrap'
3831                 Pads with the wrap of the vector along the axis.
3832                 The first values are used to pad the end and the
3833                 end values are used to pad the beginning.
3834         stat_length : int, tuple or mapping of hashable to tuple, default: None
3835             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3836             values at edge of each axis used to calculate the statistic value.
3837             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3838             statistic lengths along each dimension.
3839             ((before, after),) yields same before and after statistic lengths
3840             for each dimension.
3841             (stat_length,) or int is a shortcut for before = after = statistic
3842             length for all axes.
3843             Default is ``None``, to use the entire axis.
3844         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3845             Used in 'constant'.  The values to set the padded values for each
3846             axis.
3847             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3848             pad constants along each dimension.
3849             ``((before, after),)`` yields same before and after constants for each
3850             dimension.
3851             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3852             all dimensions.
3853             Default is 0.
3854         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3855             Used in 'linear_ramp'.  The values used for the ending value of the
3856             linear_ramp and that will form the edge of the padded array.
3857             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3858             end values along each dimension.
3859             ``((before, after),)`` yields same before and after end values for each
3860             axis.
3861             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3862             all axes.
3863             Default is 0.
3864         reflect_type : {"even", "odd"}, optional
3865             Used in "reflect", and "symmetric".  The "even" style is the
3866             default with an unaltered reflection around the edge value.  For
3867             the "odd" style, the extended part of the array is created by
3868             subtracting the reflected values from two times the edge value.
3869         **pad_width_kwargs
3870             The keyword arguments form of ``pad_width``.
3871             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3872 
3873         Returns
3874         -------
3875         padded : DataArray
3876             DataArray with the padded coordinates and data.
3877 
3878         See Also
3879         --------
3880         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3881 
3882         Notes
3883         -----
3884         For ``mode="constant"`` and ``constant_values=None``, integer types will be
3885         promoted to ``float`` and padded with ``np.nan``.
3886 
3887         Examples
3888         --------
3889         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
3890         >>> arr.pad(x=(1, 2), constant_values=0)
3891         <xarray.DataArray (x: 6)>
3892         array([0, 5, 6, 7, 0, 0])
3893         Coordinates:
3894           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3895 
3896         >>> da = xr.DataArray(
3897         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
3898         ...     dims=["x", "y"],
3899         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
3900         ... )
3901         >>> da.pad(x=1)
3902         <xarray.DataArray (x: 4, y: 4)>
3903         array([[nan, nan, nan, nan],
3904                [ 0.,  1.,  2.,  3.],
3905                [10., 11., 12., 13.],
3906                [nan, nan, nan, nan]])
3907         Coordinates:
3908           * x        (x) float64 nan 0.0 1.0 nan
3909           * y        (y) int64 10 20 30 40
3910             z        (x) float64 nan 100.0 200.0 nan
3911 
3912         Careful, ``constant_values`` are coerced to the data type of the array which may
3913         lead to a loss of precision:
3914 
3915         >>> da.pad(x=1, constant_values=1.23456789)
3916         <xarray.DataArray (x: 4, y: 4)>
3917         array([[ 1,  1,  1,  1],
3918                [ 0,  1,  2,  3],
3919                [10, 11, 12, 13],
3920                [ 1,  1,  1,  1]])
3921         Coordinates:
3922           * x        (x) float64 nan 0.0 1.0 nan
3923           * y        (y) int64 10 20 30 40
3924             z        (x) float64 nan 100.0 200.0 nan
3925         """
3926         ds = self._to_temp_dataset().pad(
3927             pad_width=pad_width,
3928             mode=mode,
3929             stat_length=stat_length,
3930             constant_values=constant_values,
3931             end_values=end_values,
3932             reflect_type=reflect_type,
3933             **pad_width_kwargs,
3934         )
3935         return self._from_temp_dataset(ds)
3936 
3937     def idxmin(
3938         self,
3939         dim: Hashable = None,
3940         skipna: bool = None,
3941         fill_value: Any = dtypes.NA,
3942         keep_attrs: bool = None,
3943     ) -> "DataArray":
3944         """Return the coordinate label of the minimum value along a dimension.
3945 
3946         Returns a new `DataArray` named after the dimension with the values of
3947         the coordinate labels along that dimension corresponding to minimum
3948         values along that dimension.
3949 
3950         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3951         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3952 
3953         Parameters
3954         ----------
3955         dim : str, optional
3956             Dimension over which to apply `idxmin`.  This is optional for 1D
3957             arrays, but required for arrays with 2 or more dimensions.
3958         skipna : bool or None, default: None
3959             If True, skip missing values (as marked by NaN). By default, only
3960             skips missing values for ``float``, ``complex``, and ``object``
3961             dtypes; other dtypes either do not have a sentinel missing value
3962             (``int``) or ``skipna=True`` has not been implemented
3963             (``datetime64`` or ``timedelta64``).
3964         fill_value : Any, default: NaN
3965             Value to be filled in case all of the values along a dimension are
3966             null.  By default this is NaN.  The fill value and result are
3967             automatically converted to a compatible dtype if possible.
3968             Ignored if ``skipna`` is False.
3969         keep_attrs : bool, default: False
3970             If True, the attributes (``attrs``) will be copied from the
3971             original object to the new one.  If False (default), the new object
3972             will be returned without attributes.
3973 
3974         Returns
3975         -------
3976         reduced : DataArray
3977             New `DataArray` object with `idxmin` applied to its data and the
3978             indicated dimension removed.
3979 
3980         See Also
3981         --------
3982         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3983 
3984         Examples
3985         --------
3986         >>> array = xr.DataArray(
3987         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3988         ... )
3989         >>> array.min()
3990         <xarray.DataArray ()>
3991         array(-2)
3992         >>> array.argmin()
3993         <xarray.DataArray ()>
3994         array(4)
3995         >>> array.idxmin()
3996         <xarray.DataArray 'x' ()>
3997         array('e', dtype='<U1')
3998 
3999         >>> array = xr.DataArray(
4000         ...     [
4001         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4002         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4003         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4004         ...     ],
4005         ...     dims=["y", "x"],
4006         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4007         ... )
4008         >>> array.min(dim="x")
4009         <xarray.DataArray (y: 3)>
4010         array([-2., -4.,  1.])
4011         Coordinates:
4012           * y        (y) int64 -1 0 1
4013         >>> array.argmin(dim="x")
4014         <xarray.DataArray (y: 3)>
4015         array([4, 0, 2])
4016         Coordinates:
4017           * y        (y) int64 -1 0 1
4018         >>> array.idxmin(dim="x")
4019         <xarray.DataArray 'x' (y: 3)>
4020         array([16.,  0.,  4.])
4021         Coordinates:
4022           * y        (y) int64 -1 0 1
4023         """
4024         return computation._calc_idxminmax(
4025             array=self,
4026             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4027             dim=dim,
4028             skipna=skipna,
4029             fill_value=fill_value,
4030             keep_attrs=keep_attrs,
4031         )
4032 
4033     def idxmax(
4034         self,
4035         dim: Hashable = None,
4036         skipna: bool = None,
4037         fill_value: Any = dtypes.NA,
4038         keep_attrs: bool = None,
4039     ) -> "DataArray":
4040         """Return the coordinate label of the maximum value along a dimension.
4041 
4042         Returns a new `DataArray` named after the dimension with the values of
4043         the coordinate labels along that dimension corresponding to maximum
4044         values along that dimension.
4045 
4046         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4047         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4048 
4049         Parameters
4050         ----------
4051         dim : hashable, optional
4052             Dimension over which to apply `idxmax`.  This is optional for 1D
4053             arrays, but required for arrays with 2 or more dimensions.
4054         skipna : bool or None, default: None
4055             If True, skip missing values (as marked by NaN). By default, only
4056             skips missing values for ``float``, ``complex``, and ``object``
4057             dtypes; other dtypes either do not have a sentinel missing value
4058             (``int``) or ``skipna=True`` has not been implemented
4059             (``datetime64`` or ``timedelta64``).
4060         fill_value : Any, default: NaN
4061             Value to be filled in case all of the values along a dimension are
4062             null.  By default this is NaN.  The fill value and result are
4063             automatically converted to a compatible dtype if possible.
4064             Ignored if ``skipna`` is False.
4065         keep_attrs : bool, default: False
4066             If True, the attributes (``attrs``) will be copied from the
4067             original object to the new one.  If False (default), the new object
4068             will be returned without attributes.
4069 
4070         Returns
4071         -------
4072         reduced : DataArray
4073             New `DataArray` object with `idxmax` applied to its data and the
4074             indicated dimension removed.
4075 
4076         See Also
4077         --------
4078         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4079 
4080         Examples
4081         --------
4082         >>> array = xr.DataArray(
4083         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4084         ... )
4085         >>> array.max()
4086         <xarray.DataArray ()>
4087         array(2)
4088         >>> array.argmax()
4089         <xarray.DataArray ()>
4090         array(1)
4091         >>> array.idxmax()
4092         <xarray.DataArray 'x' ()>
4093         array('b', dtype='<U1')
4094 
4095         >>> array = xr.DataArray(
4096         ...     [
4097         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4098         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4099         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4100         ...     ],
4101         ...     dims=["y", "x"],
4102         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4103         ... )
4104         >>> array.max(dim="x")
4105         <xarray.DataArray (y: 3)>
4106         array([2., 2., 1.])
4107         Coordinates:
4108           * y        (y) int64 -1 0 1
4109         >>> array.argmax(dim="x")
4110         <xarray.DataArray (y: 3)>
4111         array([0, 2, 2])
4112         Coordinates:
4113           * y        (y) int64 -1 0 1
4114         >>> array.idxmax(dim="x")
4115         <xarray.DataArray 'x' (y: 3)>
4116         array([0., 4., 4.])
4117         Coordinates:
4118           * y        (y) int64 -1 0 1
4119         """
4120         return computation._calc_idxminmax(
4121             array=self,
4122             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4123             dim=dim,
4124             skipna=skipna,
4125             fill_value=fill_value,
4126             keep_attrs=keep_attrs,
4127         )
4128 
4129     def argmin(
4130         self,
4131         dim: Union[Hashable, Sequence[Hashable]] = None,
4132         axis: int = None,
4133         keep_attrs: bool = None,
4134         skipna: bool = None,
4135     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4136         """Index or indices of the minimum of the DataArray over one or more dimensions.
4137 
4138         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4139         which can be passed directly to isel(). If a single str is passed to 'dim' then
4140         returns a DataArray with dtype int.
4141 
4142         If there are multiple minima, the indices of the first one found will be
4143         returned.
4144 
4145         Parameters
4146         ----------
4147         dim : hashable, sequence of hashable or ..., optional
4148             The dimensions over which to find the minimum. By default, finds minimum over
4149             all dimensions - for now returning an int for backward compatibility, but
4150             this is deprecated, in future will return a dict with indices for all
4151             dimensions; to return a dict with all dimensions now, pass '...'.
4152         axis : int, optional
4153             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4154             can be supplied.
4155         keep_attrs : bool, optional
4156             If True, the attributes (`attrs`) will be copied from the original
4157             object to the new one.  If False (default), the new object will be
4158             returned without attributes.
4159         skipna : bool, optional
4160             If True, skip missing values (as marked by NaN). By default, only
4161             skips missing values for float dtypes; other dtypes either do not
4162             have a sentinel missing value (int) or skipna=True has not been
4163             implemented (object, datetime64 or timedelta64).
4164 
4165         Returns
4166         -------
4167         result : DataArray or dict of DataArray
4168 
4169         See Also
4170         --------
4171         Variable.argmin, DataArray.idxmin
4172 
4173         Examples
4174         --------
4175         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4176         >>> array.min()
4177         <xarray.DataArray ()>
4178         array(-1)
4179         >>> array.argmin()
4180         <xarray.DataArray ()>
4181         array(2)
4182         >>> array.argmin(...)
4183         {'x': <xarray.DataArray ()>
4184         array(2)}
4185         >>> array.isel(array.argmin(...))
4186         <xarray.DataArray ()>
4187         array(-1)
4188 
4189         >>> array = xr.DataArray(
4190         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4191         ...     dims=("x", "y", "z"),
4192         ... )
4193         >>> array.min(dim="x")
4194         <xarray.DataArray (y: 3, z: 3)>
4195         array([[ 1,  2,  1],
4196                [ 2, -5,  1],
4197                [ 2,  1,  1]])
4198         Dimensions without coordinates: y, z
4199         >>> array.argmin(dim="x")
4200         <xarray.DataArray (y: 3, z: 3)>
4201         array([[1, 0, 0],
4202                [1, 1, 1],
4203                [0, 0, 1]])
4204         Dimensions without coordinates: y, z
4205         >>> array.argmin(dim=["x"])
4206         {'x': <xarray.DataArray (y: 3, z: 3)>
4207         array([[1, 0, 0],
4208                [1, 1, 1],
4209                [0, 0, 1]])
4210         Dimensions without coordinates: y, z}
4211         >>> array.min(dim=("x", "z"))
4212         <xarray.DataArray (y: 3)>
4213         array([ 1, -5,  1])
4214         Dimensions without coordinates: y
4215         >>> array.argmin(dim=["x", "z"])
4216         {'x': <xarray.DataArray (y: 3)>
4217         array([0, 1, 0])
4218         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4219         array([2, 1, 1])
4220         Dimensions without coordinates: y}
4221         >>> array.isel(array.argmin(dim=["x", "z"]))
4222         <xarray.DataArray (y: 3)>
4223         array([ 1, -5,  1])
4224         Dimensions without coordinates: y
4225         """
4226         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4227         if isinstance(result, dict):
4228             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4229         else:
4230             return self._replace_maybe_drop_dims(result)
4231 
4232     def argmax(
4233         self,
4234         dim: Union[Hashable, Sequence[Hashable]] = None,
4235         axis: int = None,
4236         keep_attrs: bool = None,
4237         skipna: bool = None,
4238     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4239         """Index or indices of the maximum of the DataArray over one or more dimensions.
4240 
4241         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4242         which can be passed directly to isel(). If a single str is passed to 'dim' then
4243         returns a DataArray with dtype int.
4244 
4245         If there are multiple maxima, the indices of the first one found will be
4246         returned.
4247 
4248         Parameters
4249         ----------
4250         dim : hashable, sequence of hashable or ..., optional
4251             The dimensions over which to find the maximum. By default, finds maximum over
4252             all dimensions - for now returning an int for backward compatibility, but
4253             this is deprecated, in future will return a dict with indices for all
4254             dimensions; to return a dict with all dimensions now, pass '...'.
4255         axis : int, optional
4256             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4257             can be supplied.
4258         keep_attrs : bool, optional
4259             If True, the attributes (`attrs`) will be copied from the original
4260             object to the new one.  If False (default), the new object will be
4261             returned without attributes.
4262         skipna : bool, optional
4263             If True, skip missing values (as marked by NaN). By default, only
4264             skips missing values for float dtypes; other dtypes either do not
4265             have a sentinel missing value (int) or skipna=True has not been
4266             implemented (object, datetime64 or timedelta64).
4267 
4268         Returns
4269         -------
4270         result : DataArray or dict of DataArray
4271 
4272         See Also
4273         --------
4274         Variable.argmax, DataArray.idxmax
4275 
4276         Examples
4277         --------
4278         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4279         >>> array.max()
4280         <xarray.DataArray ()>
4281         array(3)
4282         >>> array.argmax()
4283         <xarray.DataArray ()>
4284         array(3)
4285         >>> array.argmax(...)
4286         {'x': <xarray.DataArray ()>
4287         array(3)}
4288         >>> array.isel(array.argmax(...))
4289         <xarray.DataArray ()>
4290         array(3)
4291 
4292         >>> array = xr.DataArray(
4293         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4294         ...     dims=("x", "y", "z"),
4295         ... )
4296         >>> array.max(dim="x")
4297         <xarray.DataArray (y: 3, z: 3)>
4298         array([[3, 3, 2],
4299                [3, 5, 2],
4300                [2, 3, 3]])
4301         Dimensions without coordinates: y, z
4302         >>> array.argmax(dim="x")
4303         <xarray.DataArray (y: 3, z: 3)>
4304         array([[0, 1, 1],
4305                [0, 1, 0],
4306                [0, 1, 0]])
4307         Dimensions without coordinates: y, z
4308         >>> array.argmax(dim=["x"])
4309         {'x': <xarray.DataArray (y: 3, z: 3)>
4310         array([[0, 1, 1],
4311                [0, 1, 0],
4312                [0, 1, 0]])
4313         Dimensions without coordinates: y, z}
4314         >>> array.max(dim=("x", "z"))
4315         <xarray.DataArray (y: 3)>
4316         array([3, 5, 3])
4317         Dimensions without coordinates: y
4318         >>> array.argmax(dim=["x", "z"])
4319         {'x': <xarray.DataArray (y: 3)>
4320         array([0, 1, 0])
4321         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4322         array([0, 1, 2])
4323         Dimensions without coordinates: y}
4324         >>> array.isel(array.argmax(dim=["x", "z"]))
4325         <xarray.DataArray (y: 3)>
4326         array([3, 5, 3])
4327         Dimensions without coordinates: y
4328         """
4329         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4330         if isinstance(result, dict):
4331             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4332         else:
4333             return self._replace_maybe_drop_dims(result)
4334 
4335     # this needs to be at the end, or mypy will confuse with `str`
4336     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4337     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>to_dataframe</method>
<code>
2625     def to_dataframe(
2626         self, name: Hashable = None, dim_order: List[Hashable] = None
2627     ) -> pd.DataFrame:
2628         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2629 
2630         The DataFrame is indexed by the Cartesian product of index coordinates
2631         (in the form of a :py:class:`pandas.MultiIndex`).
2632 
2633         Other coordinates are included as columns in the DataFrame.
2634 
2635         Parameters
2636         ----------
2637         name
2638             Name to give to this array (required if unnamed).
2639         dim_order
2640             Hierarchical dimension order for the resulting dataframe.
2641             Array content is transposed to this order and then written out as flat
2642             vectors in contiguous order, so the last dimension in this list
2643             will be contiguous in the resulting DataFrame. This has a major
2644             influence on which operations are efficient on the resulting
2645             dataframe.
2646 
2647             If provided, must include all dimensions of this DataArray. By default,
2648             dimensions are sorted according to the DataArray dimensions order.
2649 
2650         Returns
2651         -------
2652         result
2653             DataArray as a pandas DataFrame.
2654 
2655         """
2656         if name is None:
2657             name = self.name
2658         if name is None:
2659             raise ValueError(
2660                 "cannot convert an unnamed DataArray to a "
2661                 "DataFrame: use the ``name`` parameter"
2662             )
2663         if self.ndim == 0:
2664             raise ValueError("cannot convert a scalar to a DataFrame")
2665 
2666         # By using a unique name, we can convert a DataArray into a DataFrame
2667         # even if it shares a name with one of its coordinates.
2668         # I would normally use unique_name = object() but that results in a
2669         # dataframe with columns in the wrong order, for reasons I have not
2670         # been able to debug (possibly a pandas bug?).
2671         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2672         ds = self._to_dataset_whole(name=unique_name)
2673 
2674         if dim_order is None:
2675             ordered_dims = dict(zip(self.dims, self.shape))
2676         else:
2677             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2678 
2679         df = ds._to_dataframe(ordered_dims)
2680         df.columns = [name if c == unique_name else c for c in df.columns]
2681         return df

</code><intended_behavior>Similar to `to_series`, `to_dataframe` needs to be adapted to manage `DataArray` objects that feature a mix of multi-indexed and regular dimensions, ensuring a seamless conversion into a `pandas.DataFrame`. The DataFrame's index should be a `MultiIndex` that accurately represents all dimensions involved, including the handling of any stacked dimensions alongside standard dimensions. This enhancement should ensure that the dimensionality of the original `DataArray` is fully and correctly transitioned into the DataFrame structure without loss of information or hierarchical integrity.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
221 class DataArray(AbstractArray, DataWithCoords):
222     """N-dimensional array with labeled coordinates and dimensions.
223 
224     DataArray provides a wrapper around numpy ndarrays that uses
225     labeled dimensions and coordinates to support metadata aware
226     operations. The API is similar to that for the pandas Series or
227     DataFrame, but DataArray objects can have any number of dimensions,
228     and their contents have fixed data types.
229 
230     Additional features over raw numpy arrays:
231 
232     - Apply operations over dimensions by name: ``x.sum('time')``.
233     - Select or assign values by integer location (like numpy):
234       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
235       ``x.sel(time='2014-01-01')``.
236     - Mathematical operations (e.g., ``x - y``) vectorize across
237       multiple dimensions (known in numpy as "broadcasting") based on
238       dimension names, regardless of their original order.
239     - Keep track of arbitrary metadata in the form of a Python
240       dictionary: ``x.attrs``
241     - Convert to a pandas Series: ``x.to_series()``.
242 
243     Getting items from or doing mathematical operations with a
244     DataArray always returns another DataArray.
245 
246     Parameters
247     ----------
248     data : array_like
249         Values for this array. Must be an ``numpy.ndarray``, ndarray
250         like, or castable to an ``ndarray``. If a self-described xarray
251         or pandas object, attempts are made to use this array's
252         metadata to fill in other unspecified arguments. A view of the
253         array's data is used instead of a copy if possible.
254     coords : sequence or dict of array_like, optional
255         Coordinates (tick labels) to use for indexing along each
256         dimension. The following notations are accepted:
257 
258         - mapping {dimension name: array-like}
259         - sequence of tuples that are valid arguments for
260           ``xarray.Variable()``
261           - (dims, data)
262           - (dims, data, attrs)
263           - (dims, data, attrs, encoding)
264 
265         Additionally, it is possible to define a coord whose name
266         does not match the dimension name, or a coord based on multiple
267         dimensions, with one of the following notations:
268 
269         - mapping {coord name: DataArray}
270         - mapping {coord name: Variable}
271         - mapping {coord name: (dimension name, array-like)}
272         - mapping {coord name: (tuple of dimension names, array-like)}
273 
274     dims : hashable or sequence of hashable, optional
275         Name(s) of the data dimension(s). Must be either a hashable
276         (only for 1D data) or a sequence of hashables with length equal
277         to the number of dimensions. If this argument is omitted,
278         dimension names default to ``['dim_0', ... 'dim_n']``.
279     name : str or None, optional
280         Name of this array.
281     attrs : dict_like or None, optional
282         Attributes to assign to the new instance. By default, an empty
283         attribute dictionary is initialized.
284 
285     Examples
286     --------
287     Create data:
288 
289     >>> np.random.seed(0)
290     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
291     >>> precipitation = 10 * np.random.rand(2, 2, 3)
292     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
293     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
294     >>> time = pd.date_range("2014-09-06", periods=3)
295     >>> reference_time = pd.Timestamp("2014-09-05")
296 
297     Initialize a dataarray with multiple dimensions:
298 
299     >>> da = xr.DataArray(
300     ...     data=temperature,
301     ...     dims=["x", "y", "time"],
302     ...     coords=dict(
303     ...         lon=(["x", "y"], lon),
304     ...         lat=(["x", "y"], lat),
305     ...         time=time,
306     ...         reference_time=reference_time,
307     ...     ),
308     ...     attrs=dict(
309     ...         description="Ambient temperature.",
310     ...         units="degC",
311     ...     ),
312     ... )
313     >>> da
314     <xarray.DataArray (x: 2, y: 2, time: 3)>
315     array([[[29.11241877, 18.20125767, 22.82990387],
316             [32.92714559, 29.94046392,  7.18177696]],
317     <BLANKLINE>
318            [[22.60070734, 13.78914233, 14.17424919],
319             [18.28478802, 16.15234857, 26.63418806]]])
320     Coordinates:
321         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
322         lat             (x, y) float64 42.25 42.21 42.63 42.59
323       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
324         reference_time  datetime64[ns] 2014-09-05
325     Dimensions without coordinates: x, y
326     Attributes:
327         description:  Ambient temperature.
328         units:        degC
329 
330     Find out where the coldest temperature was:
331 
332     >>> da.isel(da.argmin(...))
333     <xarray.DataArray ()>
334     array(7.18177696)
335     Coordinates:
336         lon             float64 -99.32
337         lat             float64 42.21
338         time            datetime64[ns] 2014-09-08
339         reference_time  datetime64[ns] 2014-09-05
340     Attributes:
341         description:  Ambient temperature.
342         units:        degC
343     """
344 
345     _cache: Dict[str, Any]
346     _coords: Dict[Any, Variable]
347     _close: Optional[Callable[[], None]]
348     _indexes: Optional[Dict[Hashable, pd.Index]]
349     _name: Optional[Hashable]
350     _variable: Variable
351 
352     __slots__ = (
353         "_cache",
354         "_coords",
355         "_close",
356         "_indexes",
357         "_name",
358         "_variable",
359         "__weakref__",
360     )
361 
362     _groupby_cls = groupby.DataArrayGroupBy
363     _rolling_cls = rolling.DataArrayRolling
364     _coarsen_cls = rolling.DataArrayCoarsen
365     _resample_cls = resample.DataArrayResample
366     _weighted_cls = weighted.DataArrayWeighted
367 
368     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
369 
370     def __init__(
371         self,
372         data: Any = dtypes.NA,
373         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
374         dims: Union[Hashable, Sequence[Hashable], None] = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: Dict[Hashable, pd.Index] = None,
379         fastpath: bool = False,
380     ):
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385         else:
386             # try to fill in arguments from data if they weren't supplied
387             if coords is None:
388 
389                 if isinstance(data, DataArray):
390                     coords = data.coords
391                 elif isinstance(data, pd.Series):
392                     coords = [data.index]
393                 elif isinstance(data, pd.DataFrame):
394                     coords = [data.index, data.columns]
395                 elif isinstance(data, (pd.Index, IndexVariable)):
396                     coords = [data]
397                 elif isinstance(data, pdcompat.Panel):
398                     coords = [data.items, data.major_axis, data.minor_axis]
399 
400             if dims is None:
401                 dims = getattr(data, "dims", getattr(coords, "dims", None))
402             if name is None:
403                 name = getattr(data, "name", None)
404             if attrs is None and not isinstance(data, PANDAS_TYPES):
405                 attrs = getattr(data, "attrs", None)
406 
407             data = _check_data_shape(data, coords, dims)
408             data = as_compatible_data(data)
409             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
410             variable = Variable(dims, data, attrs, fastpath=True)
411             indexes = dict(
412                 _extract_indexes_from_coords(coords)
413             )  # needed for to_dataset
414 
415         # These fully describe a DataArray
416         self._variable = variable
417         assert isinstance(coords, dict)
418         self._coords = coords
419         self._name = name
420 
421         # TODO(shoyer): document this argument, once it becomes part of the
422         # public interface.
423         self._indexes = indexes
424 
425         self._close = None
426 
427     def _replace(
428         self,
429         variable: Variable = None,
430         coords=None,
431         name: Union[Hashable, None, Default] = _default,
432         indexes=None,
433     ) -> "DataArray":
434         if variable is None:
435             variable = self.variable
436         if coords is None:
437             coords = self._coords
438         if name is _default:
439             name = self.name
440         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
441 
442     def _replace_maybe_drop_dims(
443         self, variable: Variable, name: Union[Hashable, None, Default] = _default
444     ) -> "DataArray":
445         if variable.dims == self.dims and variable.shape == self.shape:
446             coords = self._coords.copy()
447             indexes = self._indexes
448         elif variable.dims == self.dims:
449             # Shape has changed (e.g. from reduce(..., keepdims=True)
450             new_sizes = dict(zip(self.dims, variable.shape))
451             coords = {
452                 k: v
453                 for k, v in self._coords.items()
454                 if v.shape == tuple(new_sizes[d] for d in v.dims)
455             }
456             changed_dims = [
457                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
458             ]
459             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
460         else:
461             allowed_dims = set(variable.dims)
462             coords = {
463                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
464             }
465             indexes = propagate_indexes(
466                 self._indexes, exclude=(set(self.dims) - allowed_dims)
467             )
468         return self._replace(variable, coords, name, indexes=indexes)
469 
470     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
471         if not len(indexes):
472             return self
473         coords = self._coords.copy()
474         for name, idx in indexes.items():
475             coords[name] = IndexVariable(name, idx)
476         obj = self._replace(coords=coords)
477 
478         # switch from dimension to level names, if necessary
479         dim_names: Dict[Any, str] = {}
480         for dim, idx in indexes.items():
481             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
482                 dim_names[dim] = idx.name
483         if dim_names:
484             obj = obj.rename(dim_names)
485         return obj
486 
487     def _to_temp_dataset(self) -> Dataset:
488         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
489 
490     def _from_temp_dataset(
491         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
492     ) -> "DataArray":
493         variable = dataset._variables.pop(_THIS_ARRAY)
494         coords = dataset._variables
495         indexes = dataset._indexes
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _to_dataset_split(self, dim: Hashable) -> Dataset:
499         """ splits dataarray along dimension 'dim' """
500 
501         def subset(dim, label):
502             array = self.loc[{dim: label}]
503             array.attrs = {}
504             return as_variable(array)
505 
506         variables = {label: subset(dim, label) for label in self.get_index(dim)}
507         variables.update({k: v for k, v in self._coords.items() if k != dim})
508         indexes = propagate_indexes(self._indexes, exclude=dim)
509         coord_names = set(self._coords) - {dim}
510         dataset = Dataset._construct_direct(
511             variables, coord_names, indexes=indexes, attrs=self.attrs
512         )
513         return dataset
514 
515     def _to_dataset_whole(
516         self, name: Hashable = None, shallow_copy: bool = True
517     ) -> Dataset:
518         if name is None:
519             name = self.name
520         if name is None:
521             raise ValueError(
522                 "unable to convert unnamed DataArray to a "
523                 "Dataset without providing an explicit name"
524             )
525         if name in self.coords:
526             raise ValueError(
527                 "cannot create a Dataset from a DataArray with "
528                 "the same name as one of its coordinates"
529             )
530         # use private APIs for speed: this is called by _to_temp_dataset(),
531         # which is used in the guts of a lot of operations (e.g., reindex)
532         variables = self._coords.copy()
533         variables[name] = self.variable
534         if shallow_copy:
535             for k in variables:
536                 variables[k] = variables[k].copy(deep=False)
537         indexes = self._indexes
538 
539         coord_names = set(self._coords)
540         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
541         return dataset
542 
543     def to_dataset(
544         self,
545         dim: Hashable = None,
546         *,
547         name: Hashable = None,
548         promote_attrs: bool = False,
549     ) -> Dataset:
550         """Convert a DataArray to a Dataset.
551 
552         Parameters
553         ----------
554         dim : hashable, optional
555             Name of the dimension on this array along which to split this array
556             into separate variables. If not provided, this array is converted
557             into a Dataset of one variable.
558         name : hashable, optional
559             Name to substitute for this array's name. Only valid if ``dim`` is
560             not provided.
561         promote_attrs : bool, default: False
562             Set to True to shallow copy attrs of DataArray to returned Dataset.
563 
564         Returns
565         -------
566         dataset : Dataset
567         """
568         if dim is not None and dim not in self.dims:
569             raise TypeError(
570                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
571             )
572 
573         if dim is not None:
574             if name is not None:
575                 raise TypeError("cannot supply both dim and name arguments")
576             result = self._to_dataset_split(dim)
577         else:
578             result = self._to_dataset_whole(name)
579 
580         if promote_attrs:
581             result.attrs = dict(self.attrs)
582 
583         return result
584 
585     @property
586     def name(self) -> Optional[Hashable]:
587         """The name of this array."""
588         return self._name
589 
590     @name.setter
591     def name(self, value: Optional[Hashable]) -> None:
592         self._name = value
593 
594     @property
595     def variable(self) -> Variable:
596         """Low level interface to the Variable object for this DataArray."""
597         return self._variable
598 
599     @property
600     def dtype(self) -> np.dtype:
601         return self.variable.dtype
602 
603     @property
604     def shape(self) -> Tuple[int, ...]:
605         return self.variable.shape
606 
607     @property
608     def size(self) -> int:
609         return self.variable.size
610 
611     @property
612     def nbytes(self) -> int:
613         return self.variable.nbytes
614 
615     @property
616     def ndim(self) -> int:
617         return self.variable.ndim
618 
619     def __len__(self) -> int:
620         return len(self.variable)
621 
622     @property
623     def data(self) -> Any:
624         """The array's data as a dask or numpy array"""
625         return self.variable.data
626 
627     @data.setter
628     def data(self, value: Any) -> None:
629         self.variable.data = value
630 
631     @property
632     def values(self) -> np.ndarray:
633         """The array's data as a numpy.ndarray"""
634         return self.variable.values
635 
636     @values.setter
637     def values(self, value: Any) -> None:
638         self.variable.values = value
639 
640     @property
641     def _in_memory(self) -> bool:
642         return self.variable._in_memory
643 
644     def to_index(self) -> pd.Index:
645         """Convert this variable to a pandas.Index. Only possible for 1D
646         arrays.
647         """
648         return self.variable.to_index()
649 
650     @property
651     def dims(self) -> Tuple[Hashable, ...]:
652         """Tuple of dimension names associated with this array.
653 
654         Note that the type of this property is inconsistent with
655         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
656         consistently named properties.
657         """
658         return self.variable.dims
659 
660     @dims.setter
661     def dims(self, value):
662         raise AttributeError(
663             "you cannot assign dims on a DataArray. Use "
664             ".rename() or .swap_dims() instead."
665         )
666 
667     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
668         if utils.is_dict_like(key):
669             return key
670         else:
671             key = indexing.expanded_indexer(key, self.ndim)
672             return dict(zip(self.dims, key))
673 
674     @property
675     def _level_coords(self) -> Dict[Hashable, Hashable]:
676         """Return a mapping of all MultiIndex levels and their corresponding
677         coordinate name.
678         """
679         level_coords: Dict[Hashable, Hashable] = {}
680 
681         for cname, var in self._coords.items():
682             if var.ndim == 1 and isinstance(var, IndexVariable):
683                 level_names = var.level_names
684                 if level_names is not None:
685                     (dim,) = var.dims
686                     level_coords.update({lname: dim for lname in level_names})
687         return level_coords
688 
689     def _getitem_coord(self, key):
690         from .dataset import _get_virtual_variable
691 
692         try:
693             var = self._coords[key]
694         except KeyError:
695             dim_sizes = dict(zip(self.dims, self.shape))
696             _, key, var = _get_virtual_variable(
697                 self._coords, key, self._level_coords, dim_sizes
698             )
699 
700         return self._replace_maybe_drop_dims(var, name=key)
701 
702     def __getitem__(self, key: Any) -> "DataArray":
703         if isinstance(key, str):
704             return self._getitem_coord(key)
705         else:
706             # xarray-style array indexing
707             return self.isel(indexers=self._item_key_to_dict(key))
708 
709     def __setitem__(self, key: Any, value: Any) -> None:
710         if isinstance(key, str):
711             self.coords[key] = value
712         else:
713             # Coordinates in key, value and self[key] should be consistent.
714             # TODO Coordinate consistency in key is checked here, but it
715             # causes unnecessary indexing. It should be optimized.
716             obj = self[key]
717             if isinstance(value, DataArray):
718                 assert_coordinate_consistent(value, obj.coords.variables)
719             # DataArray key -> Variable key
720             key = {
721                 k: v.variable if isinstance(v, DataArray) else v
722                 for k, v in self._item_key_to_dict(key).items()
723             }
724             self.variable[key] = value
725 
726     def __delitem__(self, key: Any) -> None:
727         del self.coords[key]
728 
729     @property
730     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
731         """Places to look-up items for attribute-style access"""
732         yield from self._item_sources
733         yield self.attrs
734 
735     @property
736     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
737         """Places to look-up items for key-completion"""
738         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
739 
740         # virtual coordinates
741         # uses empty dict -- everything here can already be found in self.coords.
742         yield HybridMappingProxy(keys=self.dims, mapping={})
743         yield HybridMappingProxy(keys=self._level_coords, mapping={})
744 
745     def __contains__(self, key: Any) -> bool:
746         return key in self.data
747 
748     @property
749     def loc(self) -> _LocIndexer:
750         """Attribute for location based indexing like pandas."""
751         return _LocIndexer(self)
752 
753     @property
754     def attrs(self) -> Dict[Hashable, Any]:
755         """Dictionary storing arbitrary metadata with this array."""
756         return self.variable.attrs
757 
758     @attrs.setter
759     def attrs(self, value: Mapping[Hashable, Any]) -> None:
760         # Disable type checking to work around mypy bug - see mypy#4167
761         self.variable.attrs = value  # type: ignore
762 
763     @property
764     def encoding(self) -> Dict[Hashable, Any]:
765         """Dictionary of format-specific settings for how this array should be
766         serialized."""
767         return self.variable.encoding
768 
769     @encoding.setter
770     def encoding(self, value: Mapping[Hashable, Any]) -> None:
771         self.variable.encoding = value
772 
773     @property
774     def indexes(self) -> Indexes:
775         """Mapping of pandas.Index objects used for label based indexing"""
776         if self._indexes is None:
777             self._indexes = default_indexes(self._coords, self.dims)
778         return Indexes(self._indexes)
779 
780     @property
781     def coords(self) -> DataArrayCoordinates:
782         """Dictionary-like container of coordinate arrays."""
783         return DataArrayCoordinates(self)
784 
785     def reset_coords(
786         self,
787         names: Union[Iterable[Hashable], Hashable, None] = None,
788         drop: bool = False,
789     ) -> Union[None, "DataArray", Dataset]:
790         """Given names of coordinates, reset them to become variables.
791 
792         Parameters
793         ----------
794         names : hashable or iterable of hashable, optional
795             Name(s) of non-index coordinates in this dataset to reset into
796             variables. By default, all non-index coordinates are reset.
797         drop : bool, optional
798             If True, remove coordinates instead of converting them into
799             variables.
800 
801         Returns
802         -------
803         Dataset, or DataArray if ``drop == True``
804         """
805         if names is None:
806             names = set(self.coords) - set(self.dims)
807         dataset = self.coords.to_dataset().reset_coords(names, drop)
808         if drop:
809             return self._replace(coords=dataset._variables)
810         else:
811             if self.name is None:
812                 raise ValueError(
813                     "cannot reset_coords with drop=False on an unnamed DataArrray"
814                 )
815             dataset[self.name] = self.variable
816             return dataset
817 
818     def __dask_tokenize__(self):
819         from dask.base import normalize_token
820 
821         return normalize_token((type(self), self._variable, self._coords, self._name))
822 
823     def __dask_graph__(self):
824         return self._to_temp_dataset().__dask_graph__()
825 
826     def __dask_keys__(self):
827         return self._to_temp_dataset().__dask_keys__()
828 
829     def __dask_layers__(self):
830         return self._to_temp_dataset().__dask_layers__()
831 
832     @property
833     def __dask_optimize__(self):
834         return self._to_temp_dataset().__dask_optimize__
835 
836     @property
837     def __dask_scheduler__(self):
838         return self._to_temp_dataset().__dask_scheduler__
839 
840     def __dask_postcompute__(self):
841         func, args = self._to_temp_dataset().__dask_postcompute__()
842         return self._dask_finalize, (func, args, self.name)
843 
844     def __dask_postpersist__(self):
845         func, args = self._to_temp_dataset().__dask_postpersist__()
846         return self._dask_finalize, (func, args, self.name)
847 
848     @staticmethod
849     def _dask_finalize(results, func, args, name):
850         ds = func(results, *args)
851         variable = ds._variables.pop(_THIS_ARRAY)
852         coords = ds._variables
853         return DataArray(variable, coords, name=name, fastpath=True)
854 
855     def load(self, **kwargs) -> "DataArray":
856         """Manually trigger loading of this array's data from disk or a
857         remote source into memory and return this array.
858 
859         Normally, it should not be necessary to call this method in user code,
860         because all xarray functions should either work on deferred data or
861         load data automatically. However, this method can be necessary when
862         working with many file objects on disk.
863 
864         Parameters
865         ----------
866         **kwargs : dict
867             Additional keyword arguments passed on to ``dask.compute``.
868 
869         See Also
870         --------
871         dask.compute
872         """
873         ds = self._to_temp_dataset().load(**kwargs)
874         new = self._from_temp_dataset(ds)
875         self._variable = new._variable
876         self._coords = new._coords
877         return self
878 
879     def compute(self, **kwargs) -> "DataArray":
880         """Manually trigger loading of this array's data from disk or a
881         remote source into memory and return a new array. The original is
882         left unaltered.
883 
884         Normally, it should not be necessary to call this method in user code,
885         because all xarray functions should either work on deferred data or
886         load data automatically. However, this method can be necessary when
887         working with many file objects on disk.
888 
889         Parameters
890         ----------
891         **kwargs : dict
892             Additional keyword arguments passed on to ``dask.compute``.
893 
894         See Also
895         --------
896         dask.compute
897         """
898         new = self.copy(deep=False)
899         return new.load(**kwargs)
900 
901     def persist(self, **kwargs) -> "DataArray":
902         """Trigger computation in constituent dask arrays
903 
904         This keeps them as dask arrays but encourages them to keep data in
905         memory.  This is particularly useful when on a distributed machine.
906         When on a single machine consider using ``.compute()`` instead.
907 
908         Parameters
909         ----------
910         **kwargs : dict
911             Additional keyword arguments passed on to ``dask.persist``.
912 
913         See Also
914         --------
915         dask.persist
916         """
917         ds = self._to_temp_dataset().persist(**kwargs)
918         return self._from_temp_dataset(ds)
919 
920     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
921         """Returns a copy of this array.
922 
923         If `deep=True`, a deep copy is made of the data array.
924         Otherwise, a shallow copy is made, and the returned data array's
925         values are a new view of this data array's values.
926 
927         Use `data` to create a new object with the same structure as
928         original but entirely new data.
929 
930         Parameters
931         ----------
932         deep : bool, optional
933             Whether the data array and its coordinates are loaded into memory
934             and copied onto the new object. Default is True.
935         data : array_like, optional
936             Data to use in the new object. Must have same shape as original.
937             When `data` is used, `deep` is ignored for all data variables,
938             and only used for coords.
939 
940         Returns
941         -------
942         object : DataArray
943             New object with dimensions, attributes, coordinates, name,
944             encoding, and optionally data copied from original.
945 
946         Examples
947         --------
948         Shallow versus deep copy
949 
950         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
951         >>> array.copy()
952         <xarray.DataArray (x: 3)>
953         array([1, 2, 3])
954         Coordinates:
955           * x        (x) <U1 'a' 'b' 'c'
956         >>> array_0 = array.copy(deep=False)
957         >>> array_0[0] = 7
958         >>> array_0
959         <xarray.DataArray (x: 3)>
960         array([7, 2, 3])
961         Coordinates:
962           * x        (x) <U1 'a' 'b' 'c'
963         >>> array
964         <xarray.DataArray (x: 3)>
965         array([7, 2, 3])
966         Coordinates:
967           * x        (x) <U1 'a' 'b' 'c'
968 
969         Changing the data using the ``data`` argument maintains the
970         structure of the original object, but with the new data. Original
971         object is unaffected.
972 
973         >>> array.copy(data=[0.1, 0.2, 0.3])
974         <xarray.DataArray (x: 3)>
975         array([0.1, 0.2, 0.3])
976         Coordinates:
977           * x        (x) <U1 'a' 'b' 'c'
978         >>> array
979         <xarray.DataArray (x: 3)>
980         array([7, 2, 3])
981         Coordinates:
982           * x        (x) <U1 'a' 'b' 'c'
983 
984         See Also
985         --------
986         pandas.DataFrame.copy
987         """
988         variable = self.variable.copy(deep=deep, data=data)
989         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
990         if self._indexes is None:
991             indexes = self._indexes
992         else:
993             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
994         return self._replace(variable, coords, indexes=indexes)
995 
996     def __copy__(self) -> "DataArray":
997         return self.copy(deep=False)
998 
999     def __deepcopy__(self, memo=None) -> "DataArray":
1000         # memo does nothing but is required for compatibility with
1001         # copy.deepcopy
1002         return self.copy(deep=True)
1003 
1004     # mutable objects should not be hashable
1005     # https://github.com/python/mypy/issues/4266
1006     __hash__ = None  # type: ignore
1007 
1008     @property
1009     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1010         """Block dimensions for this array's data or None if it's not a dask
1011         array.
1012         """
1013         return self.variable.chunks
1014 
1015     def chunk(
1016         self,
1017         chunks: Union[
1018             Number,
1019             Tuple[Number, ...],
1020             Tuple[Tuple[Number, ...], ...],
1021             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
1022         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1023         name_prefix: str = "xarray-",
1024         token: str = None,
1025         lock: bool = False,
1026     ) -> "DataArray":
1027         """Coerce this array's data into a dask arrays with the given chunks.
1028 
1029         If this variable is a non-dask array, it will be converted to dask
1030         array. If it's a dask array, it will be rechunked to the given chunk
1031         sizes.
1032 
1033         If neither chunks is not provided for one or more dimensions, chunk
1034         sizes along that dimension will not be updated; non-dask arrays will be
1035         converted into dask arrays with a single block.
1036 
1037         Parameters
1038         ----------
1039         chunks : int, tuple of int or mapping of hashable to int, optional
1040             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1041             ``{'x': 5, 'y': 5}``.
1042         name_prefix : str, optional
1043             Prefix for the name of the new dask array.
1044         token : str, optional
1045             Token uniquely identifying this array.
1046         lock : optional
1047             Passed on to :py:func:`dask.array.from_array`, if the array is not
1048             already as dask array.
1049 
1050         Returns
1051         -------
1052         chunked : xarray.DataArray
1053         """
1054         if isinstance(chunks, (tuple, list)):
1055             chunks = dict(zip(self.dims, chunks))
1056 
1057         ds = self._to_temp_dataset().chunk(
1058             chunks, name_prefix=name_prefix, token=token, lock=lock
1059         )
1060         return self._from_temp_dataset(ds)
1061 
1062     def isel(
1063         self,
1064         indexers: Mapping[Hashable, Any] = None,
1065         drop: bool = False,
1066         missing_dims: str = "raise",
1067         **indexers_kwargs: Any,
1068     ) -> "DataArray":
1069         """Return a new DataArray whose data is given by integer indexing
1070         along the specified dimension(s).
1071 
1072         Parameters
1073         ----------
1074         indexers : dict, optional
1075             A dict with keys matching dimensions and values given
1076             by integers, slice objects or arrays.
1077             indexer can be a integer, slice, array-like or DataArray.
1078             If DataArrays are passed as indexers, xarray-style indexing will be
1079             carried out. See :ref:`indexing` for the details.
1080             One of indexers or indexers_kwargs must be provided.
1081         drop : bool, optional
1082             If ``drop=True``, drop coordinates variables indexed by integers
1083             instead of making them scalar.
1084         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1085             What to do if dimensions that should be selected from are not present in the
1086             DataArray:
1087             - "raise": raise an exception
1088             - "warning": raise a warning, and ignore the missing dimensions
1089             - "ignore": ignore the missing dimensions
1090         **indexers_kwargs : {dim: indexer, ...}, optional
1091             The keyword arguments form of ``indexers``.
1092 
1093         See Also
1094         --------
1095         Dataset.isel
1096         DataArray.sel
1097 
1098         Examples
1099         --------
1100         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1101         >>> da
1102         <xarray.DataArray (x: 5, y: 5)>
1103         array([[ 0,  1,  2,  3,  4],
1104                [ 5,  6,  7,  8,  9],
1105                [10, 11, 12, 13, 14],
1106                [15, 16, 17, 18, 19],
1107                [20, 21, 22, 23, 24]])
1108         Dimensions without coordinates: x, y
1109 
1110         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1111         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1112         >>> da = da.isel(x=tgt_x, y=tgt_y)
1113         >>> da
1114         <xarray.DataArray (points: 5)>
1115         array([ 0,  6, 12, 18, 24])
1116         Dimensions without coordinates: points
1117         """
1118 
1119         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1120 
1121         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1122             ds = self._to_temp_dataset()._isel_fancy(
1123                 indexers, drop=drop, missing_dims=missing_dims
1124             )
1125             return self._from_temp_dataset(ds)
1126 
1127         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1128         # lists, or zero or one-dimensional np.ndarray's
1129 
1130         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1131 
1132         coords = {}
1133         for coord_name, coord_value in self._coords.items():
1134             coord_indexers = {
1135                 k: v for k, v in indexers.items() if k in coord_value.dims
1136             }
1137             if coord_indexers:
1138                 coord_value = coord_value.isel(coord_indexers)
1139                 if drop and coord_value.ndim == 0:
1140                     continue
1141             coords[coord_name] = coord_value
1142 
1143         return self._replace(variable=variable, coords=coords)
1144 
1145     def sel(
1146         self,
1147         indexers: Mapping[Hashable, Any] = None,
1148         method: str = None,
1149         tolerance=None,
1150         drop: bool = False,
1151         **indexers_kwargs: Any,
1152     ) -> "DataArray":
1153         """Return a new DataArray whose data is given by selecting index
1154         labels along the specified dimension(s).
1155 
1156         In contrast to `DataArray.isel`, indexers for this method should use
1157         labels instead of integers.
1158 
1159         Under the hood, this method is powered by using pandas's powerful Index
1160         objects. This makes label based indexing essentially just as fast as
1161         using integer indexing.
1162 
1163         It also means this method uses pandas's (well documented) logic for
1164         indexing. This means you can use string shortcuts for datetime indexes
1165         (e.g., '2000-01' to select all values in January 2000). It also means
1166         that slices are treated as inclusive of both the start and stop values,
1167         unlike normal Python indexing.
1168 
1169         .. warning::
1170 
1171           Do not try to assign values when using any of the indexing methods
1172           ``isel`` or ``sel``::
1173 
1174             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1175             # DO NOT do this
1176             da.isel(x=[0, 1, 2])[1] = -1
1177 
1178           Assigning values with the chained indexing using ``.sel`` or
1179           ``.isel`` fails silently.
1180 
1181         Parameters
1182         ----------
1183         indexers : dict, optional
1184             A dict with keys matching dimensions and values given
1185             by scalars, slices or arrays of tick labels. For dimensions with
1186             multi-index, the indexer may also be a dict-like object with keys
1187             matching index level names.
1188             If DataArrays are passed as indexers, xarray-style indexing will be
1189             carried out. See :ref:`indexing` for the details.
1190             One of indexers or indexers_kwargs must be provided.
1191         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1192             Method to use for inexact matches:
1193 
1194             * None (default): only exact matches
1195             * pad / ffill: propagate last valid index value forward
1196             * backfill / bfill: propagate next valid index value backward
1197             * nearest: use nearest valid index value
1198         tolerance : optional
1199             Maximum distance between original and new labels for inexact
1200             matches. The values of the index at the matching locations must
1201             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1202         drop : bool, optional
1203             If ``drop=True``, drop coordinates variables in `indexers` instead
1204             of making them scalar.
1205         **indexers_kwargs : {dim: indexer, ...}, optional
1206             The keyword arguments form of ``indexers``.
1207             One of indexers or indexers_kwargs must be provided.
1208 
1209         Returns
1210         -------
1211         obj : DataArray
1212             A new DataArray with the same contents as this DataArray, except the
1213             data and each dimension is indexed by the appropriate indexers.
1214             If indexer DataArrays have coordinates that do not conflict with
1215             this object, then these coordinates will be attached.
1216             In general, each array's data will be a view of the array's data
1217             in this DataArray, unless vectorized indexing was triggered by using
1218             an array indexer, in which case the data will be a copy.
1219 
1220         See Also
1221         --------
1222         Dataset.sel
1223         DataArray.isel
1224 
1225         Examples
1226         --------
1227         >>> da = xr.DataArray(
1228         ...     np.arange(25).reshape(5, 5),
1229         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1230         ...     dims=("x", "y"),
1231         ... )
1232         >>> da
1233         <xarray.DataArray (x: 5, y: 5)>
1234         array([[ 0,  1,  2,  3,  4],
1235                [ 5,  6,  7,  8,  9],
1236                [10, 11, 12, 13, 14],
1237                [15, 16, 17, 18, 19],
1238                [20, 21, 22, 23, 24]])
1239         Coordinates:
1240           * x        (x) int64 0 1 2 3 4
1241           * y        (y) int64 0 1 2 3 4
1242 
1243         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1244         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1245         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1246         >>> da
1247         <xarray.DataArray (points: 5)>
1248         array([ 0,  6, 12, 18, 24])
1249         Coordinates:
1250             x        (points) int64 0 1 2 3 4
1251             y        (points) int64 0 1 2 3 4
1252         Dimensions without coordinates: points
1253         """
1254         ds = self._to_temp_dataset().sel(
1255             indexers=indexers,
1256             drop=drop,
1257             method=method,
1258             tolerance=tolerance,
1259             **indexers_kwargs,
1260         )
1261         return self._from_temp_dataset(ds)
1262 
1263     def head(
1264         self,
1265         indexers: Union[Mapping[Hashable, int], int] = None,
1266         **indexers_kwargs: Any,
1267     ) -> "DataArray":
1268         """Return a new DataArray whose data is given by the the first `n`
1269         values along the specified dimension(s). Default `n` = 5
1270 
1271         See Also
1272         --------
1273         Dataset.head
1274         DataArray.tail
1275         DataArray.thin
1276         """
1277         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1278         return self._from_temp_dataset(ds)
1279 
1280     def tail(
1281         self,
1282         indexers: Union[Mapping[Hashable, int], int] = None,
1283         **indexers_kwargs: Any,
1284     ) -> "DataArray":
1285         """Return a new DataArray whose data is given by the the last `n`
1286         values along the specified dimension(s). Default `n` = 5
1287 
1288         See Also
1289         --------
1290         Dataset.tail
1291         DataArray.head
1292         DataArray.thin
1293         """
1294         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1295         return self._from_temp_dataset(ds)
1296 
1297     def thin(
1298         self,
1299         indexers: Union[Mapping[Hashable, int], int] = None,
1300         **indexers_kwargs: Any,
1301     ) -> "DataArray":
1302         """Return a new DataArray whose data is given by each `n` value
1303         along the specified dimension(s).
1304 
1305         See Also
1306         --------
1307         Dataset.thin
1308         DataArray.head
1309         DataArray.tail
1310         """
1311         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1312         return self._from_temp_dataset(ds)
1313 
1314     def broadcast_like(
1315         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1316     ) -> "DataArray":
1317         """Broadcast this DataArray against another Dataset or DataArray.
1318 
1319         This is equivalent to xr.broadcast(other, self)[1]
1320 
1321         xarray objects are broadcast against each other in arithmetic
1322         operations, so this method is not be necessary for most uses.
1323 
1324         If no change is needed, the input data is returned to the output
1325         without being copied.
1326 
1327         If new coords are added by the broadcast, their values are
1328         NaN filled.
1329 
1330         Parameters
1331         ----------
1332         other : Dataset or DataArray
1333             Object against which to broadcast this array.
1334         exclude : iterable of hashable, optional
1335             Dimensions that must not be broadcasted
1336 
1337         Returns
1338         -------
1339         new_da : DataArray
1340             The caller broadcasted against ``other``.
1341 
1342         Examples
1343         --------
1344         >>> arr1 = xr.DataArray(
1345         ...     np.random.randn(2, 3),
1346         ...     dims=("x", "y"),
1347         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1348         ... )
1349         >>> arr2 = xr.DataArray(
1350         ...     np.random.randn(3, 2),
1351         ...     dims=("x", "y"),
1352         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1353         ... )
1354         >>> arr1
1355         <xarray.DataArray (x: 2, y: 3)>
1356         array([[ 1.76405235,  0.40015721,  0.97873798],
1357                [ 2.2408932 ,  1.86755799, -0.97727788]])
1358         Coordinates:
1359           * x        (x) <U1 'a' 'b'
1360           * y        (y) <U1 'a' 'b' 'c'
1361         >>> arr2
1362         <xarray.DataArray (x: 3, y: 2)>
1363         array([[ 0.95008842, -0.15135721],
1364                [-0.10321885,  0.4105985 ],
1365                [ 0.14404357,  1.45427351]])
1366         Coordinates:
1367           * x        (x) <U1 'a' 'b' 'c'
1368           * y        (y) <U1 'a' 'b'
1369         >>> arr1.broadcast_like(arr2)
1370         <xarray.DataArray (x: 3, y: 3)>
1371         array([[ 1.76405235,  0.40015721,  0.97873798],
1372                [ 2.2408932 ,  1.86755799, -0.97727788],
1373                [        nan,         nan,         nan]])
1374         Coordinates:
1375           * x        (x) <U1 'a' 'b' 'c'
1376           * y        (y) <U1 'a' 'b' 'c'
1377         """
1378         if exclude is None:
1379             exclude = set()
1380         else:
1381             exclude = set(exclude)
1382         args = align(other, self, join="outer", copy=False, exclude=exclude)
1383 
1384         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1385 
1386         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1387 
1388     def reindex_like(
1389         self,
1390         other: Union["DataArray", Dataset],
1391         method: str = None,
1392         tolerance=None,
1393         copy: bool = True,
1394         fill_value=dtypes.NA,
1395     ) -> "DataArray":
1396         """Conform this object onto the indexes of another object, filling in
1397         missing values with ``fill_value``. The default fill value is NaN.
1398 
1399         Parameters
1400         ----------
1401         other : Dataset or DataArray
1402             Object with an 'indexes' attribute giving a mapping from dimension
1403             names to pandas.Index objects, which provides coordinates upon
1404             which to index the variables in this dataset. The indexes on this
1405             other object need not be the same as the indexes on this
1406             dataset. Any mis-matched index values will be filled in with
1407             NaN, and any mis-matched dimension names will simply be ignored.
1408         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1409             Method to use for filling index values from other not found on this
1410             data array:
1411 
1412             * None (default): don't fill gaps
1413             * pad / ffill: propagate last valid index value forward
1414             * backfill / bfill: propagate next valid index value backward
1415             * nearest: use nearest valid index value
1416         tolerance : optional
1417             Maximum distance between original and new labels for inexact
1418             matches. The values of the index at the matching locations must
1419             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1420         copy : bool, optional
1421             If ``copy=True``, data in the return value is always copied. If
1422             ``copy=False`` and reindexing is unnecessary, or can be performed
1423             with only slice operations, then the output may share memory with
1424             the input. In either case, a new xarray object is always returned.
1425         fill_value : scalar or dict-like, optional
1426             Value to use for newly missing values. If a dict-like, maps
1427             variable names (including coordinates) to fill values. Use this
1428             data array's name to refer to the data array's values.
1429 
1430         Returns
1431         -------
1432         reindexed : DataArray
1433             Another dataset array, with this array's data but coordinates from
1434             the other object.
1435 
1436         See Also
1437         --------
1438         DataArray.reindex
1439         align
1440         """
1441         indexers = reindex_like_indexers(self, other)
1442         return self.reindex(
1443             indexers=indexers,
1444             method=method,
1445             tolerance=tolerance,
1446             copy=copy,
1447             fill_value=fill_value,
1448         )
1449 
1450     def reindex(
1451         self,
1452         indexers: Mapping[Hashable, Any] = None,
1453         method: str = None,
1454         tolerance=None,
1455         copy: bool = True,
1456         fill_value=dtypes.NA,
1457         **indexers_kwargs: Any,
1458     ) -> "DataArray":
1459         """Conform this object onto the indexes of another object, filling in
1460         missing values with ``fill_value``. The default fill value is NaN.
1461 
1462         Parameters
1463         ----------
1464         indexers : dict, optional
1465             Dictionary with keys given by dimension names and values given by
1466             arrays of coordinates tick labels. Any mis-matched coordinate
1467             values will be filled in with NaN, and any mis-matched dimension
1468             names will simply be ignored.
1469             One of indexers or indexers_kwargs must be provided.
1470         copy : bool, optional
1471             If ``copy=True``, data in the return value is always copied. If
1472             ``copy=False`` and reindexing is unnecessary, or can be performed
1473             with only slice operations, then the output may share memory with
1474             the input. In either case, a new xarray object is always returned.
1475         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1476             Method to use for filling index values in ``indexers`` not found on
1477             this data array:
1478 
1479             * None (default): don't fill gaps
1480             * pad / ffill: propagate last valid index value forward
1481             * backfill / bfill: propagate next valid index value backward
1482             * nearest: use nearest valid index value
1483         tolerance : optional
1484             Maximum distance between original and new labels for inexact
1485             matches. The values of the index at the matching locations must
1486             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1487         fill_value : scalar or dict-like, optional
1488             Value to use for newly missing values. If a dict-like, maps
1489             variable names (including coordinates) to fill values. Use this
1490             data array's name to refer to the data array's values.
1491         **indexers_kwargs : {dim: indexer, ...}, optional
1492             The keyword arguments form of ``indexers``.
1493             One of indexers or indexers_kwargs must be provided.
1494 
1495         Returns
1496         -------
1497         reindexed : DataArray
1498             Another dataset array, with this array's data but replaced
1499             coordinates.
1500 
1501         See Also
1502         --------
1503         DataArray.reindex_like
1504         align
1505         """
1506         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1507         if isinstance(fill_value, dict):
1508             fill_value = fill_value.copy()
1509             sentinel = object()
1510             value = fill_value.pop(self.name, sentinel)
1511             if value is not sentinel:
1512                 fill_value[_THIS_ARRAY] = value
1513 
1514         ds = self._to_temp_dataset().reindex(
1515             indexers=indexers,
1516             method=method,
1517             tolerance=tolerance,
1518             copy=copy,
1519             fill_value=fill_value,
1520         )
1521         return self._from_temp_dataset(ds)
1522 
1523     def interp(
1524         self,
1525         coords: Mapping[Hashable, Any] = None,
1526         method: str = "linear",
1527         assume_sorted: bool = False,
1528         kwargs: Mapping[str, Any] = None,
1529         **coords_kwargs: Any,
1530     ) -> "DataArray":
1531         """Multidimensional interpolation of variables.
1532 
1533         Parameters
1534         ----------
1535         coords : dict, optional
1536             Mapping from dimension names to the new coordinates.
1537             New coordinate can be an scalar, array-like or DataArray.
1538             If DataArrays are passed as new coordinates, their dimensions are
1539             used for the broadcasting. Missing values are skipped.
1540         method : str, default: "linear"
1541             The method used to interpolate. Choose from
1542 
1543             - {"linear", "nearest"} for multidimensional array,
1544             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1545         assume_sorted : bool, optional
1546             If False, values of x can be in any order and they are sorted
1547             first. If True, x has to be an array of monotonically increasing
1548             values.
1549         kwargs : dict
1550             Additional keyword arguments passed to scipy's interpolator. Valid
1551             options and their behavior depend on if 1-dimensional or
1552             multi-dimensional interpolation is used.
1553         **coords_kwargs : {dim: coordinate, ...}, optional
1554             The keyword arguments form of ``coords``.
1555             One of coords or coords_kwargs must be provided.
1556 
1557         Returns
1558         -------
1559         interpolated : DataArray
1560             New dataarray on the new coordinates.
1561 
1562         Notes
1563         -----
1564         scipy is required.
1565 
1566         See Also
1567         --------
1568         scipy.interpolate.interp1d
1569         scipy.interpolate.interpn
1570 
1571         Examples
1572         --------
1573         >>> da = xr.DataArray(
1574         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1575         ...     dims=("x", "y"),
1576         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1577         ... )
1578         >>> da
1579         <xarray.DataArray (x: 3, y: 4)>
1580         array([[ 1.,  4.,  2.,  9.],
1581                [ 2.,  7.,  6., nan],
1582                [ 6., nan,  5.,  8.]])
1583         Coordinates:
1584           * x        (x) int64 0 1 2
1585           * y        (y) int64 10 12 14 16
1586 
1587         1D linear interpolation (the default):
1588 
1589         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1590         <xarray.DataArray (x: 4, y: 4)>
1591         array([[1.  , 4.  , 2.  ,  nan],
1592                [1.75, 6.25, 5.  ,  nan],
1593                [3.  ,  nan, 5.75,  nan],
1594                [5.  ,  nan, 5.25,  nan]])
1595         Coordinates:
1596           * y        (y) int64 10 12 14 16
1597           * x        (x) float64 0.0 0.75 1.25 1.75
1598 
1599         1D nearest interpolation:
1600 
1601         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1602         <xarray.DataArray (x: 4, y: 4)>
1603         array([[ 1.,  4.,  2.,  9.],
1604                [ 2.,  7.,  6., nan],
1605                [ 2.,  7.,  6., nan],
1606                [ 6., nan,  5.,  8.]])
1607         Coordinates:
1608           * y        (y) int64 10 12 14 16
1609           * x        (x) float64 0.0 0.75 1.25 1.75
1610 
1611         1D linear extrapolation:
1612 
1613         >>> da.interp(
1614         ...     x=[1, 1.5, 2.5, 3.5],
1615         ...     method="linear",
1616         ...     kwargs={"fill_value": "extrapolate"},
1617         ... )
1618         <xarray.DataArray (x: 4, y: 4)>
1619         array([[ 2. ,  7. ,  6. ,  nan],
1620                [ 4. ,  nan,  5.5,  nan],
1621                [ 8. ,  nan,  4.5,  nan],
1622                [12. ,  nan,  3.5,  nan]])
1623         Coordinates:
1624           * y        (y) int64 10 12 14 16
1625           * x        (x) float64 1.0 1.5 2.5 3.5
1626 
1627         2D linear interpolation:
1628 
1629         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1630         <xarray.DataArray (x: 4, y: 3)>
1631         array([[2.5  , 3.   ,   nan],
1632                [4.   , 5.625,   nan],
1633                [  nan,   nan,   nan],
1634                [  nan,   nan,   nan]])
1635         Coordinates:
1636           * x        (x) float64 0.0 0.75 1.25 1.75
1637           * y        (y) int64 11 13 15
1638         """
1639         if self.dtype.kind not in "uifc":
1640             raise TypeError(
1641                 "interp only works for a numeric type array. "
1642                 "Given {}.".format(self.dtype)
1643             )
1644         ds = self._to_temp_dataset().interp(
1645             coords,
1646             method=method,
1647             kwargs=kwargs,
1648             assume_sorted=assume_sorted,
1649             **coords_kwargs,
1650         )
1651         return self._from_temp_dataset(ds)
1652 
1653     def interp_like(
1654         self,
1655         other: Union["DataArray", Dataset],
1656         method: str = "linear",
1657         assume_sorted: bool = False,
1658         kwargs: Mapping[str, Any] = None,
1659     ) -> "DataArray":
1660         """Interpolate this object onto the coordinates of another object,
1661         filling out of range values with NaN.
1662 
1663         Parameters
1664         ----------
1665         other : Dataset or DataArray
1666             Object with an 'indexes' attribute giving a mapping from dimension
1667             names to an 1d array-like, which provides coordinates upon
1668             which to index the variables in this dataset. Missing values are skipped.
1669         method : str, default: "linear"
1670             The method used to interpolate. Choose from
1671 
1672             - {"linear", "nearest"} for multidimensional array,
1673             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1674         assume_sorted : bool, optional
1675             If False, values of coordinates that are interpolated over can be
1676             in any order and they are sorted first. If True, interpolated
1677             coordinates are assumed to be an array of monotonically increasing
1678             values.
1679         kwargs : dict, optional
1680             Additional keyword passed to scipy's interpolator.
1681 
1682         Returns
1683         -------
1684         interpolated : DataArray
1685             Another dataarray by interpolating this dataarray's data along the
1686             coordinates of the other object.
1687 
1688         Notes
1689         -----
1690         scipy is required.
1691         If the dataarray has object-type coordinates, reindex is used for these
1692         coordinates instead of the interpolation.
1693 
1694         See Also
1695         --------
1696         DataArray.interp
1697         DataArray.reindex_like
1698         """
1699         if self.dtype.kind not in "uifc":
1700             raise TypeError(
1701                 "interp only works for a numeric type array. "
1702                 "Given {}.".format(self.dtype)
1703             )
1704         ds = self._to_temp_dataset().interp_like(
1705             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1706         )
1707         return self._from_temp_dataset(ds)
1708 
1709     def rename(
1710         self,
1711         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1712         **names: Hashable,
1713     ) -> "DataArray":
1714         """Returns a new DataArray with renamed coordinates or a new name.
1715 
1716         Parameters
1717         ----------
1718         new_name_or_name_dict : str or dict-like, optional
1719             If the argument is dict-like, it used as a mapping from old
1720             names to new names for coordinates. Otherwise, use the argument
1721             as the new name for this array.
1722         **names : hashable, optional
1723             The keyword arguments form of a mapping from old names to
1724             new names for coordinates.
1725             One of new_name_or_name_dict or names must be provided.
1726 
1727         Returns
1728         -------
1729         renamed : DataArray
1730             Renamed array or array with renamed coordinates.
1731 
1732         See Also
1733         --------
1734         Dataset.rename
1735         DataArray.swap_dims
1736         """
1737         if names or utils.is_dict_like(new_name_or_name_dict):
1738             new_name_or_name_dict = cast(
1739                 Mapping[Hashable, Hashable], new_name_or_name_dict
1740             )
1741             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1742             dataset = self._to_temp_dataset().rename(name_dict)
1743             return self._from_temp_dataset(dataset)
1744         else:
1745             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1746             return self._replace(name=new_name_or_name_dict)
1747 
1748     def swap_dims(
1749         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs
1750     ) -> "DataArray":
1751         """Returns a new DataArray with swapped dimensions.
1752 
1753         Parameters
1754         ----------
1755         dims_dict : dict-like
1756             Dictionary whose keys are current dimension names and whose values
1757             are new names.
1758         **dims_kwargs : {existing_dim: new_dim, ...}, optional
1759             The keyword arguments form of ``dims_dict``.
1760             One of dims_dict or dims_kwargs must be provided.
1761 
1762         Returns
1763         -------
1764         swapped : DataArray
1765             DataArray with swapped dimensions.
1766 
1767         Examples
1768         --------
1769         >>> arr = xr.DataArray(
1770         ...     data=[0, 1],
1771         ...     dims="x",
1772         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1773         ... )
1774         >>> arr
1775         <xarray.DataArray (x: 2)>
1776         array([0, 1])
1777         Coordinates:
1778           * x        (x) <U1 'a' 'b'
1779             y        (x) int64 0 1
1780 
1781         >>> arr.swap_dims({"x": "y"})
1782         <xarray.DataArray (y: 2)>
1783         array([0, 1])
1784         Coordinates:
1785             x        (y) <U1 'a' 'b'
1786           * y        (y) int64 0 1
1787 
1788         >>> arr.swap_dims({"x": "z"})
1789         <xarray.DataArray (z: 2)>
1790         array([0, 1])
1791         Coordinates:
1792             x        (z) <U1 'a' 'b'
1793             y        (z) int64 0 1
1794         Dimensions without coordinates: z
1795 
1796         See Also
1797         --------
1798         DataArray.rename
1799         Dataset.swap_dims
1800         """
1801         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
1802         ds = self._to_temp_dataset().swap_dims(dims_dict)
1803         return self._from_temp_dataset(ds)
1804 
1805     def expand_dims(
1806         self,
1807         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1808         axis=None,
1809         **dim_kwargs: Any,
1810     ) -> "DataArray":
1811         """Return a new object with an additional axis (or axes) inserted at
1812         the corresponding position in the array shape. The new object is a
1813         view into the underlying array, not a copy.
1814 
1815         If dim is already a scalar coordinate, it will be promoted to a 1D
1816         coordinate consisting of a single value.
1817 
1818         Parameters
1819         ----------
1820         dim : hashable, sequence of hashable, dict, or None, optional
1821             Dimensions to include on the new variable.
1822             If provided as str or sequence of str, then dimensions are inserted
1823             with length 1. If provided as a dict, then the keys are the new
1824             dimensions and the values are either integers (giving the length of
1825             the new dimensions) or sequence/ndarray (giving the coordinates of
1826             the new dimensions).
1827         axis : int, list of int or tuple of int, or None, default: None
1828             Axis position(s) where new axis is to be inserted (position(s) on
1829             the result array). If a list (or tuple) of integers is passed,
1830             multiple axes are inserted. In this case, dim arguments should be
1831             same length list. If axis=None is passed, all the axes will be
1832             inserted to the start of the result array.
1833         **dim_kwargs : int or sequence or ndarray
1834             The keywords are arbitrary dimensions being inserted and the values
1835             are either the lengths of the new dims (if int is given), or their
1836             coordinates. Note, this is an alternative to passing a dict to the
1837             dim kwarg and will only be used if dim is None.
1838 
1839         Returns
1840         -------
1841         expanded : same type as caller
1842             This object, but with an additional dimension(s).
1843         """
1844         if isinstance(dim, int):
1845             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1846         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1847             if len(dim) != len(set(dim)):
1848                 raise ValueError("dims should not contain duplicate values.")
1849             dim = dict.fromkeys(dim, 1)
1850         elif dim is not None and not isinstance(dim, Mapping):
1851             dim = {cast(Hashable, dim): 1}
1852 
1853         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1854         ds = self._to_temp_dataset().expand_dims(dim, axis)
1855         return self._from_temp_dataset(ds)
1856 
1857     def set_index(
1858         self,
1859         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1860         append: bool = False,
1861         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1862     ) -> Optional["DataArray"]:
1863         """Set DataArray (multi-)indexes using one or more existing
1864         coordinates.
1865 
1866         Parameters
1867         ----------
1868         indexes : {dim: index, ...}
1869             Mapping from names matching dimensions and values given
1870             by (lists of) the names of existing coordinates or variables to set
1871             as new (multi-)index.
1872         append : bool, optional
1873             If True, append the supplied index(es) to the existing index(es).
1874             Otherwise replace the existing index(es) (default).
1875         **indexes_kwargs : optional
1876             The keyword arguments form of ``indexes``.
1877             One of indexes or indexes_kwargs must be provided.
1878 
1879         Returns
1880         -------
1881         obj : DataArray
1882             Another DataArray, with this data but replaced coordinates.
1883 
1884         Examples
1885         --------
1886         >>> arr = xr.DataArray(
1887         ...     data=np.ones((2, 3)),
1888         ...     dims=["x", "y"],
1889         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1890         ... )
1891         >>> arr
1892         <xarray.DataArray (x: 2, y: 3)>
1893         array([[1., 1., 1.],
1894                [1., 1., 1.]])
1895         Coordinates:
1896           * x        (x) int64 0 1
1897           * y        (y) int64 0 1 2
1898             a        (x) int64 3 4
1899         >>> arr.set_index(x="a")
1900         <xarray.DataArray (x: 2, y: 3)>
1901         array([[1., 1., 1.],
1902                [1., 1., 1.]])
1903         Coordinates:
1904           * x        (x) int64 3 4
1905           * y        (y) int64 0 1 2
1906 
1907         See Also
1908         --------
1909         DataArray.reset_index
1910         """
1911         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
1912         return self._from_temp_dataset(ds)
1913 
1914     def reset_index(
1915         self,
1916         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1917         drop: bool = False,
1918     ) -> Optional["DataArray"]:
1919         """Reset the specified index(es) or multi-index level(s).
1920 
1921         Parameters
1922         ----------
1923         dims_or_levels : hashable or sequence of hashable
1924             Name(s) of the dimension(s) and/or multi-index level(s) that will
1925             be reset.
1926         drop : bool, optional
1927             If True, remove the specified indexes and/or multi-index levels
1928             instead of extracting them as new coordinates (default: False).
1929 
1930         Returns
1931         -------
1932         obj : DataArray
1933             Another dataarray, with this dataarray's data but replaced
1934             coordinates.
1935 
1936         See Also
1937         --------
1938         DataArray.set_index
1939         """
1940         coords, _ = split_indexes(
1941             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1942         )
1943         return self._replace(coords=coords)
1944 
1945     def reorder_levels(
1946         self,
1947         dim_order: Mapping[Hashable, Sequence[int]] = None,
1948         **dim_order_kwargs: Sequence[int],
1949     ) -> "DataArray":
1950         """Rearrange index levels using input order.
1951 
1952         Parameters
1953         ----------
1954         dim_order : optional
1955             Mapping from names matching dimensions and values given
1956             by lists representing new level orders. Every given dimension
1957             must have a multi-index.
1958         **dim_order_kwargs : optional
1959             The keyword arguments form of ``dim_order``.
1960             One of dim_order or dim_order_kwargs must be provided.
1961 
1962         Returns
1963         -------
1964         obj : DataArray
1965             Another dataarray, with this dataarray's data but replaced
1966             coordinates.
1967         """
1968         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1969         replace_coords = {}
1970         for dim, order in dim_order.items():
1971             coord = self._coords[dim]
1972             index = coord.to_index()
1973             if not isinstance(index, pd.MultiIndex):
1974                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1975             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1976         coords = self._coords.copy()
1977         coords.update(replace_coords)
1978         return self._replace(coords=coords)
1979 
1980     def stack(
1981         self,
1982         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1983         **dimensions_kwargs: Sequence[Hashable],
1984     ) -> "DataArray":
1985         """
1986         Stack any number of existing dimensions into a single new dimension.
1987 
1988         New dimensions will be added at the end, and the corresponding
1989         coordinate variables will be combined into a MultiIndex.
1990 
1991         Parameters
1992         ----------
1993         dimensions : mapping of hashable to sequence of hashable
1994             Mapping of the form `new_name=(dim1, dim2, ...)`.
1995             Names of new dimensions, and the existing dimensions that they
1996             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1997             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1998             all dimensions.
1999         **dimensions_kwargs
2000             The keyword arguments form of ``dimensions``.
2001             One of dimensions or dimensions_kwargs must be provided.
2002 
2003         Returns
2004         -------
2005         stacked : DataArray
2006             DataArray with stacked data.
2007 
2008         Examples
2009         --------
2010         >>> arr = xr.DataArray(
2011         ...     np.arange(6).reshape(2, 3),
2012         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2013         ... )
2014         >>> arr
2015         <xarray.DataArray (x: 2, y: 3)>
2016         array([[0, 1, 2],
2017                [3, 4, 5]])
2018         Coordinates:
2019           * x        (x) <U1 'a' 'b'
2020           * y        (y) int64 0 1 2
2021         >>> stacked = arr.stack(z=("x", "y"))
2022         >>> stacked.indexes["z"]
2023         MultiIndex([('a', 0),
2024                     ('a', 1),
2025                     ('a', 2),
2026                     ('b', 0),
2027                     ('b', 1),
2028                     ('b', 2)],
2029                    names=['x', 'y'])
2030 
2031         See Also
2032         --------
2033         DataArray.unstack
2034         """
2035         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
2036         return self._from_temp_dataset(ds)
2037 
2038     def unstack(
2039         self,
2040         dim: Union[Hashable, Sequence[Hashable], None] = None,
2041         fill_value: Any = dtypes.NA,
2042         sparse: bool = False,
2043     ) -> "DataArray":
2044         """
2045         Unstack existing dimensions corresponding to MultiIndexes into
2046         multiple new dimensions.
2047 
2048         New dimensions will be added at the end.
2049 
2050         Parameters
2051         ----------
2052         dim : hashable or sequence of hashable, optional
2053             Dimension(s) over which to unstack. By default unstacks all
2054             MultiIndexes.
2055         fill_value : scalar or dict-like, default: nan
2056             value to be filled. If a dict-like, maps variable names to
2057             fill values. Use the data array's name to refer to its
2058             name. If not provided or if the dict-like does not contain
2059             all variables, the dtype's NA value will be used.
2060         sparse : bool, default: False
2061             use sparse-array if True
2062 
2063         Returns
2064         -------
2065         unstacked : DataArray
2066             Array with unstacked data.
2067 
2068         Examples
2069         --------
2070         >>> arr = xr.DataArray(
2071         ...     np.arange(6).reshape(2, 3),
2072         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2073         ... )
2074         >>> arr
2075         <xarray.DataArray (x: 2, y: 3)>
2076         array([[0, 1, 2],
2077                [3, 4, 5]])
2078         Coordinates:
2079           * x        (x) <U1 'a' 'b'
2080           * y        (y) int64 0 1 2
2081         >>> stacked = arr.stack(z=("x", "y"))
2082         >>> stacked.indexes["z"]
2083         MultiIndex([('a', 0),
2084                     ('a', 1),
2085                     ('a', 2),
2086                     ('b', 0),
2087                     ('b', 1),
2088                     ('b', 2)],
2089                    names=['x', 'y'])
2090         >>> roundtripped = stacked.unstack()
2091         >>> arr.identical(roundtripped)
2092         True
2093 
2094         See Also
2095         --------
2096         DataArray.stack
2097         """
2098         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2099         return self._from_temp_dataset(ds)
2100 
2101     def to_unstacked_dataset(self, dim, level=0):
2102         """Unstack DataArray expanding to Dataset along a given level of a
2103         stacked coordinate.
2104 
2105         This is the inverse operation of Dataset.to_stacked_array.
2106 
2107         Parameters
2108         ----------
2109         dim : str
2110             Name of existing dimension to unstack
2111         level : int or str
2112             The MultiIndex level to expand to a dataset along. Can either be
2113             the integer index of the level or its name.
2114         label : int, default: 0
2115             Label of the level to expand dataset along. Overrides the label
2116             argument if given.
2117 
2118         Returns
2119         -------
2120         unstacked: Dataset
2121 
2122         Examples
2123         --------
2124         >>> import xarray as xr
2125         >>> arr = xr.DataArray(
2126         ...     np.arange(6).reshape(2, 3),
2127         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2128         ... )
2129         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2130         >>> data
2131         <xarray.Dataset>
2132         Dimensions:  (x: 2, y: 3)
2133         Coordinates:
2134           * x        (x) <U1 'a' 'b'
2135           * y        (y) int64 0 1 2
2136         Data variables:
2137             a        (x, y) int64 0 1 2 3 4 5
2138             b        (x) int64 0 3
2139         >>> stacked = data.to_stacked_array("z", ["x"])
2140         >>> stacked.indexes["z"]
2141         MultiIndex([('a', 0.0),
2142                     ('a', 1.0),
2143                     ('a', 2.0),
2144                     ('b', nan)],
2145                    names=['variable', 'y'])
2146         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2147         >>> data.identical(roundtripped)
2148         True
2149 
2150         See Also
2151         --------
2152         Dataset.to_stacked_array
2153         """
2154 
2155         idx = self.indexes[dim]
2156         if not isinstance(idx, pd.MultiIndex):
2157             raise ValueError(f"'{dim}' is not a stacked coordinate")
2158 
2159         level_number = idx._get_level_number(level)
2160         variables = idx.levels[level_number]
2161         variable_dim = idx.names[level_number]
2162 
2163         # pull variables out of datarray
2164         data_dict = {}
2165         for k in variables:
2166             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2167 
2168         # unstacked dataset
2169         return Dataset(data_dict)
2170 
2171     def transpose(
2172         self,
2173         *dims: Hashable,
2174         transpose_coords: bool = True,
2175         missing_dims: str = "raise",
2176     ) -> "DataArray":
2177         """Return a new DataArray object with transposed dimensions.
2178 
2179         Parameters
2180         ----------
2181         *dims : hashable, optional
2182             By default, reverse the dimensions. Otherwise, reorder the
2183             dimensions to this order.
2184         transpose_coords : bool, default: True
2185             If True, also transpose the coordinates of this DataArray.
2186         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2187             What to do if dimensions that should be selected from are not present in the
2188             DataArray:
2189             - "raise": raise an exception
2190             - "warning": raise a warning, and ignore the missing dimensions
2191             - "ignore": ignore the missing dimensions
2192 
2193         Returns
2194         -------
2195         transposed : DataArray
2196             The returned DataArray's array is transposed.
2197 
2198         Notes
2199         -----
2200         This operation returns a view of this array's data. It is
2201         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2202         -- the data will be fully loaded.
2203 
2204         See Also
2205         --------
2206         numpy.transpose
2207         Dataset.transpose
2208         """
2209         if dims:
2210             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2211         variable = self.variable.transpose(*dims)
2212         if transpose_coords:
2213             coords: Dict[Hashable, Variable] = {}
2214             for name, coord in self.coords.items():
2215                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2216                 coords[name] = coord.variable.transpose(*coord_dims)
2217             return self._replace(variable, coords)
2218         else:
2219             return self._replace(variable)
2220 
2221     @property
2222     def T(self) -> "DataArray":
2223         return self.transpose()
2224 
2225     def drop_vars(
2226         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2227     ) -> "DataArray":
2228         """Returns an array with dropped variables.
2229 
2230         Parameters
2231         ----------
2232         names : hashable or iterable of hashable
2233             Name(s) of variables to drop.
2234         errors : {"raise", "ignore"}, optional
2235             If 'raise' (default), raises a ValueError error if any of the variable
2236             passed are not in the dataset. If 'ignore', any given names that are in the
2237             DataArray are dropped and no error is raised.
2238 
2239         Returns
2240         -------
2241         dropped : Dataset
2242             New Dataset copied from `self` with variables removed.
2243         """
2244         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2245         return self._from_temp_dataset(ds)
2246 
2247     def drop(
2248         self,
2249         labels: Mapping = None,
2250         dim: Hashable = None,
2251         *,
2252         errors: str = "raise",
2253         **labels_kwargs,
2254     ) -> "DataArray":
2255         """Backward compatible method based on `drop_vars` and `drop_sel`
2256 
2257         Using either `drop_vars` or `drop_sel` is encouraged
2258 
2259         See Also
2260         --------
2261         DataArray.drop_vars
2262         DataArray.drop_sel
2263         """
2264         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2265         return self._from_temp_dataset(ds)
2266 
2267     def drop_sel(
2268         self,
2269         labels: Mapping[Hashable, Any] = None,
2270         *,
2271         errors: str = "raise",
2272         **labels_kwargs,
2273     ) -> "DataArray":
2274         """Drop index labels from this DataArray.
2275 
2276         Parameters
2277         ----------
2278         labels : mapping of hashable to Any
2279             Index labels to drop
2280         errors : {"raise", "ignore"}, optional
2281             If 'raise' (default), raises a ValueError error if
2282             any of the index labels passed are not
2283             in the dataset. If 'ignore', any given labels that are in the
2284             dataset are dropped and no error is raised.
2285         **labels_kwargs : {dim: label, ...}, optional
2286             The keyword arguments form of ``dim`` and ``labels``
2287 
2288         Returns
2289         -------
2290         dropped : DataArray
2291         """
2292         if labels_kwargs or isinstance(labels, dict):
2293             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2294 
2295         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2296         return self._from_temp_dataset(ds)
2297 
2298     def drop_isel(self, indexers=None, **indexers_kwargs):
2299         """Drop index positions from this DataArray.
2300 
2301         Parameters
2302         ----------
2303         indexers : mapping of hashable to Any
2304             Index locations to drop
2305         **indexers_kwargs : {dim: position, ...}, optional
2306             The keyword arguments form of ``dim`` and ``positions``
2307 
2308         Returns
2309         -------
2310         dropped : DataArray
2311 
2312         Raises
2313         ------
2314         IndexError
2315         """
2316         dataset = self._to_temp_dataset()
2317         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2318         return self._from_temp_dataset(dataset)
2319 
2320     def dropna(
2321         self, dim: Hashable, how: str = "any", thresh: int = None
2322     ) -> "DataArray":
2323         """Returns a new array with dropped labels for missing values along
2324         the provided dimension.
2325 
2326         Parameters
2327         ----------
2328         dim : hashable
2329             Dimension along which to drop missing values. Dropping along
2330             multiple dimensions simultaneously is not yet supported.
2331         how : {"any", "all"}, optional
2332             * any : if any NA values are present, drop that label
2333             * all : if all values are NA, drop that label
2334         thresh : int, default: None
2335             If supplied, require this many non-NA values.
2336 
2337         Returns
2338         -------
2339         DataArray
2340         """
2341         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2342         return self._from_temp_dataset(ds)
2343 
2344     def fillna(self, value: Any) -> "DataArray":
2345         """Fill missing values in this object.
2346 
2347         This operation follows the normal broadcasting and alignment rules that
2348         xarray uses for binary arithmetic, except the result is aligned to this
2349         object (``join='left'``) instead of aligned to the intersection of
2350         index coordinates (``join='inner'``).
2351 
2352         Parameters
2353         ----------
2354         value : scalar, ndarray or DataArray
2355             Used to fill all matching missing values in this array. If the
2356             argument is a DataArray, it is first aligned with (reindexed to)
2357             this array.
2358 
2359         Returns
2360         -------
2361         DataArray
2362         """
2363         if utils.is_dict_like(value):
2364             raise TypeError(
2365                 "cannot provide fill value as a dictionary with "
2366                 "fillna on a DataArray"
2367             )
2368         out = ops.fillna(self, value)
2369         return out
2370 
2371     def interpolate_na(
2372         self,
2373         dim: Hashable = None,
2374         method: str = "linear",
2375         limit: int = None,
2376         use_coordinate: Union[bool, str] = True,
2377         max_gap: Union[
2378             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2379         ] = None,
2380         keep_attrs: bool = None,
2381         **kwargs: Any,
2382     ) -> "DataArray":
2383         """Fill in NaNs by interpolating according to different methods.
2384 
2385         Parameters
2386         ----------
2387         dim : str
2388             Specifies the dimension along which to interpolate.
2389         method : str, optional
2390             String indicating which method to use for interpolation:
2391 
2392             - 'linear': linear interpolation (Default). Additional keyword
2393               arguments are passed to :py:func:`numpy.interp`
2394             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2395               are passed to :py:func:`scipy.interpolate.interp1d`. If
2396               ``method='polynomial'``, the ``order`` keyword argument must also be
2397               provided.
2398             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2399               respective :py:class:`scipy.interpolate` classes.
2400         use_coordinate : bool or str, default: True
2401             Specifies which index to use as the x values in the interpolation
2402             formulated as `y = f(x)`. If False, values are treated as if
2403             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2404             used. If ``use_coordinate`` is a string, it specifies the name of a
2405             coordinate variariable to use as the index.
2406         limit : int, default: None
2407             Maximum number of consecutive NaNs to fill. Must be greater than 0
2408             or None for no limit. This filling is done regardless of the size of
2409             the gap in the data. To only interpolate over gaps less than a given length,
2410             see ``max_gap``.
2411         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2412             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2413             Use None for no limit. When interpolating along a datetime64 dimension
2414             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2415 
2416             - a string that is valid input for pandas.to_timedelta
2417             - a :py:class:`numpy.timedelta64` object
2418             - a :py:class:`pandas.Timedelta` object
2419             - a :py:class:`datetime.timedelta` object
2420 
2421             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2422             dimensions has not been implemented yet. Gap length is defined as the difference
2423             between coordinate values at the first data point after a gap and the last value
2424             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2425             between coordinate values at the first (last) valid data point and the first (last) NaN.
2426             For example, consider::
2427 
2428                 <xarray.DataArray (x: 9)>
2429                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2430                 Coordinates:
2431                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2432 
2433             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2434         keep_attrs : bool, default: True
2435             If True, the dataarray's attributes (`attrs`) will be copied from
2436             the original object to the new one.  If False, the new
2437             object will be returned without attributes.
2438         kwargs : dict, optional
2439             parameters passed verbatim to the underlying interpolation function
2440 
2441         Returns
2442         -------
2443         interpolated: DataArray
2444             Filled in DataArray.
2445 
2446         See Also
2447         --------
2448         numpy.interp
2449         scipy.interpolate
2450 
2451         Examples
2452         --------
2453         >>> da = xr.DataArray(
2454         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2455         ... )
2456         >>> da
2457         <xarray.DataArray (x: 5)>
2458         array([nan,  2.,  3., nan,  0.])
2459         Coordinates:
2460           * x        (x) int64 0 1 2 3 4
2461 
2462         >>> da.interpolate_na(dim="x", method="linear")
2463         <xarray.DataArray (x: 5)>
2464         array([nan, 2. , 3. , 1.5, 0. ])
2465         Coordinates:
2466           * x        (x) int64 0 1 2 3 4
2467 
2468         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2469         <xarray.DataArray (x: 5)>
2470         array([1. , 2. , 3. , 1.5, 0. ])
2471         Coordinates:
2472           * x        (x) int64 0 1 2 3 4
2473         """
2474         from .missing import interp_na
2475 
2476         return interp_na(
2477             self,
2478             dim=dim,
2479             method=method,
2480             limit=limit,
2481             use_coordinate=use_coordinate,
2482             max_gap=max_gap,
2483             keep_attrs=keep_attrs,
2484             **kwargs,
2485         )
2486 
2487     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2488         """Fill NaN values by propogating values forward
2489 
2490         *Requires bottleneck.*
2491 
2492         Parameters
2493         ----------
2494         dim : hashable
2495             Specifies the dimension along which to propagate values when
2496             filling.
2497         limit : int, default: None
2498             The maximum number of consecutive NaN values to forward fill. In
2499             other words, if there is a gap with more than this number of
2500             consecutive NaNs, it will only be partially filled. Must be greater
2501             than 0 or None for no limit.
2502 
2503         Returns
2504         -------
2505         DataArray
2506         """
2507         from .missing import ffill
2508 
2509         return ffill(self, dim, limit=limit)
2510 
2511     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2512         """Fill NaN values by propogating values backward
2513 
2514         *Requires bottleneck.*
2515 
2516         Parameters
2517         ----------
2518         dim : str
2519             Specifies the dimension along which to propagate values when
2520             filling.
2521         limit : int, default: None
2522             The maximum number of consecutive NaN values to backward fill. In
2523             other words, if there is a gap with more than this number of
2524             consecutive NaNs, it will only be partially filled. Must be greater
2525             than 0 or None for no limit.
2526 
2527         Returns
2528         -------
2529         DataArray
2530         """
2531         from .missing import bfill
2532 
2533         return bfill(self, dim, limit=limit)
2534 
2535     def combine_first(self, other: "DataArray") -> "DataArray":
2536         """Combine two DataArray objects, with union of coordinates.
2537 
2538         This operation follows the normal broadcasting and alignment rules of
2539         ``join='outer'``.  Default to non-null values of array calling the
2540         method.  Use np.nan to fill in vacant cells after alignment.
2541 
2542         Parameters
2543         ----------
2544         other : DataArray
2545             Used to fill all matching missing values in this array.
2546 
2547         Returns
2548         -------
2549         DataArray
2550         """
2551         return ops.fillna(self, other, join="outer")
2552 
2553     def reduce(
2554         self,
2555         func: Callable[..., Any],
2556         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2557         axis: Union[None, int, Sequence[int]] = None,
2558         keep_attrs: bool = None,
2559         keepdims: bool = False,
2560         **kwargs: Any,
2561     ) -> "DataArray":
2562         """Reduce this array by applying `func` along some dimension(s).
2563 
2564         Parameters
2565         ----------
2566         func : callable
2567             Function which can be called in the form
2568             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2569             np.ndarray over an integer valued axis.
2570         dim : hashable or sequence of hashable, optional
2571             Dimension(s) over which to apply `func`.
2572         axis : int or sequence of int, optional
2573             Axis(es) over which to repeatedly apply `func`. Only one of the
2574             'dim' and 'axis' arguments can be supplied. If neither are
2575             supplied, then the reduction is calculated over the flattened array
2576             (by calling `f(x)` without an axis argument).
2577         keep_attrs : bool, optional
2578             If True, the variable's attributes (`attrs`) will be copied from
2579             the original object to the new one.  If False (default), the new
2580             object will be returned without attributes.
2581         keepdims : bool, default: False
2582             If True, the dimensions which are reduced are left in the result
2583             as dimensions of size one. Coordinates that use these dimensions
2584             are removed.
2585         **kwargs : dict
2586             Additional keyword arguments passed on to `func`.
2587 
2588         Returns
2589         -------
2590         reduced : DataArray
2591             DataArray with this object's array replaced with an array with
2592             summarized data and the indicated dimension(s) removed.
2593         """
2594 
2595         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2596         return self._replace_maybe_drop_dims(var)
2597 
2598     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2599         """Convert this array into a pandas object with the same shape.
2600 
2601         The type of the returned object depends on the number of DataArray
2602         dimensions:
2603 
2604         * 0D -> `xarray.DataArray`
2605         * 1D -> `pandas.Series`
2606         * 2D -> `pandas.DataFrame`
2607 
2608         Only works for arrays with 2 or fewer dimensions.
2609 
2610         The DataArray constructor performs the inverse transformation.
2611         """
2612         # TODO: consolidate the info about pandas constructors and the
2613         # attributes that correspond to their indexes into a separate module?
2614         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2615         try:
2616             constructor = constructors[self.ndim]
2617         except KeyError:
2618             raise ValueError(
2619                 "cannot convert arrays with %s dimensions into "
2620                 "pandas objects" % self.ndim
2621             )
2622         indexes = [self.get_index(dim) for dim in self.dims]
2623         return constructor(self.values, *indexes)
2624 
2625     def to_dataframe(
2626         self, name: Hashable = None, dim_order: List[Hashable] = None
2627     ) -> pd.DataFrame:
2628         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2629 
2630         The DataFrame is indexed by the Cartesian product of index coordinates
2631         (in the form of a :py:class:`pandas.MultiIndex`).
2632 
2633         Other coordinates are included as columns in the DataFrame.
2634 
2635         Parameters
2636         ----------
2637         name
2638             Name to give to this array (required if unnamed).
2639         dim_order
2640             Hierarchical dimension order for the resulting dataframe.
2641             Array content is transposed to this order and then written out as flat
2642             vectors in contiguous order, so the last dimension in this list
2643             will be contiguous in the resulting DataFrame. This has a major
2644             influence on which operations are efficient on the resulting
2645             dataframe.
2646 
2647             If provided, must include all dimensions of this DataArray. By default,
2648             dimensions are sorted according to the DataArray dimensions order.
2649 
2650         Returns
2651         -------
2652         result
2653             DataArray as a pandas DataFrame.
2654 
2655         """
2656         if name is None:
2657             name = self.name
2658         if name is None:
2659             raise ValueError(
2660                 "cannot convert an unnamed DataArray to a "
2661                 "DataFrame: use the ``name`` parameter"
2662             )
2663         if self.ndim == 0:
2664             raise ValueError("cannot convert a scalar to a DataFrame")
2665 
2666         # By using a unique name, we can convert a DataArray into a DataFrame
2667         # even if it shares a name with one of its coordinates.
2668         # I would normally use unique_name = object() but that results in a
2669         # dataframe with columns in the wrong order, for reasons I have not
2670         # been able to debug (possibly a pandas bug?).
2671         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2672         ds = self._to_dataset_whole(name=unique_name)
2673 
2674         if dim_order is None:
2675             ordered_dims = dict(zip(self.dims, self.shape))
2676         else:
2677             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2678 
2679         df = ds._to_dataframe(ordered_dims)
2680         df.columns = [name if c == unique_name else c for c in df.columns]
2681         return df
2682 
2683     def to_series(self) -> pd.Series:
2684         """Convert this array into a pandas.Series.
2685 
2686         The Series is indexed by the Cartesian product of index coordinates
2687         (in the form of a :py:class:`pandas.MultiIndex`).
2688         """
2689         index = self.coords.to_index()
2690         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2691 
2692     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2693         """Convert this array into a numpy.ma.MaskedArray
2694 
2695         Parameters
2696         ----------
2697         copy : bool, default: True
2698             If True make a copy of the array in the result. If False,
2699             a MaskedArray view of DataArray.values is returned.
2700 
2701         Returns
2702         -------
2703         result : MaskedArray
2704             Masked where invalid values (nan or inf) occur.
2705         """
2706         values = self.values  # only compute lazy arrays once
2707         isnull = pd.isnull(values)
2708         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2709 
2710     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2711         """Write DataArray contents to a netCDF file.
2712 
2713         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2714 
2715         Notes
2716         -----
2717         Only xarray.Dataset objects can be written to netCDF files, so
2718         the xarray.DataArray is converted to a xarray.Dataset object
2719         containing a single variable. If the DataArray has no name, or if the
2720         name is the same as a coordinate name, then it is given the name
2721         ``"__xarray_dataarray_variable__"``.
2722 
2723         See Also
2724         --------
2725         Dataset.to_netcdf
2726         """
2727         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2728 
2729         if self.name is None:
2730             # If no name is set then use a generic xarray name
2731             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2732         elif self.name in self.coords or self.name in self.dims:
2733             # The name is the same as one of the coords names, which netCDF
2734             # doesn't support, so rename it but keep track of the old name
2735             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2736             dataset.attrs[DATAARRAY_NAME] = self.name
2737         else:
2738             # No problems with the name - so we're fine!
2739             dataset = self.to_dataset()
2740 
2741         return dataset.to_netcdf(*args, **kwargs)
2742 
2743     def to_dict(self, data: bool = True) -> dict:
2744         """
2745         Convert this xarray.DataArray into a dictionary following xarray
2746         naming conventions.
2747 
2748         Converts all variables and attributes to native Python objects.
2749         Useful for converting to json. To avoid datetime incompatibility
2750         use decode_times=False kwarg in xarray.open_dataset.
2751 
2752         Parameters
2753         ----------
2754         data : bool, optional
2755             Whether to include the actual data in the dictionary. When set to
2756             False, returns just the schema.
2757 
2758         See Also
2759         --------
2760         DataArray.from_dict
2761         """
2762         d = self.variable.to_dict(data=data)
2763         d.update({"coords": {}, "name": self.name})
2764         for k in self.coords:
2765             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2766         return d
2767 
2768     @classmethod
2769     def from_dict(cls, d: dict) -> "DataArray":
2770         """
2771         Convert a dictionary into an xarray.DataArray
2772 
2773         Input dict can take several forms:
2774 
2775         .. code:: python
2776 
2777             d = {"dims": ("t"), "data": x}
2778 
2779             d = {
2780                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2781                 "attrs": {"title": "air temperature"},
2782                 "dims": "t",
2783                 "data": x,
2784                 "name": "a",
2785             }
2786 
2787         where "t" is the name of the dimesion, "a" is the name of the array,
2788         and x and t are lists, numpy.arrays, or pandas objects.
2789 
2790         Parameters
2791         ----------
2792         d : dict
2793             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2794 
2795         Returns
2796         -------
2797         obj : xarray.DataArray
2798 
2799         See Also
2800         --------
2801         DataArray.to_dict
2802         Dataset.from_dict
2803         """
2804         coords = None
2805         if "coords" in d:
2806             try:
2807                 coords = {
2808                     k: (v["dims"], v["data"], v.get("attrs"))
2809                     for k, v in d["coords"].items()
2810                 }
2811             except KeyError as e:
2812                 raise ValueError(
2813                     "cannot convert dict when coords are missing the key "
2814                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2815                 )
2816         try:
2817             data = d["data"]
2818         except KeyError:
2819             raise ValueError("cannot convert dict without the key 'data''")
2820         else:
2821             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2822         return obj
2823 
2824     @classmethod
2825     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2826         """Convert a pandas.Series into an xarray.DataArray.
2827 
2828         If the series's index is a MultiIndex, it will be expanded into a
2829         tensor product of one-dimensional coordinates (filling in missing
2830         values with NaN). Thus this operation should be the inverse of the
2831         `to_series` method.
2832 
2833         If sparse=True, creates a sparse array instead of a dense NumPy array.
2834         Requires the pydata/sparse package.
2835 
2836         See Also
2837         --------
2838         xarray.Dataset.from_dataframe
2839         """
2840         temp_name = "__temporary_name"
2841         df = pd.DataFrame({temp_name: series})
2842         ds = Dataset.from_dataframe(df, sparse=sparse)
2843         result = cast(DataArray, ds[temp_name])
2844         result.name = series.name
2845         return result
2846 
2847     def to_cdms2(self) -> "cdms2_Variable":
2848         """Convert this array into a cdms2.Variable"""
2849         from ..convert import to_cdms2
2850 
2851         return to_cdms2(self)
2852 
2853     @classmethod
2854     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2855         """Convert a cdms2.Variable into an xarray.DataArray"""
2856         from ..convert import from_cdms2
2857 
2858         return from_cdms2(variable)
2859 
2860     def to_iris(self) -> "iris_Cube":
2861         """Convert this array into a iris.cube.Cube"""
2862         from ..convert import to_iris
2863 
2864         return to_iris(self)
2865 
2866     @classmethod
2867     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2868         """Convert a iris.cube.Cube into an xarray.DataArray"""
2869         from ..convert import from_iris
2870 
2871         return from_iris(cube)
2872 
2873     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2874         """Helper function for equals, broadcast_equals, and identical"""
2875 
2876         def compat(x, y):
2877             return getattr(x.variable, compat_str)(y.variable)
2878 
2879         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2880             self, other
2881         )
2882 
2883     def broadcast_equals(self, other: "DataArray") -> bool:
2884         """Two DataArrays are broadcast equal if they are equal after
2885         broadcasting them against each other such that they have the same
2886         dimensions.
2887 
2888         See Also
2889         --------
2890         DataArray.equals
2891         DataArray.identical
2892         """
2893         try:
2894             return self._all_compat(other, "broadcast_equals")
2895         except (TypeError, AttributeError):
2896             return False
2897 
2898     def equals(self, other: "DataArray") -> bool:
2899         """True if two DataArrays have the same dimensions, coordinates and
2900         values; otherwise False.
2901 
2902         DataArrays can still be equal (like pandas objects) if they have NaN
2903         values in the same locations.
2904 
2905         This method is necessary because `v1 == v2` for ``DataArray``
2906         does element-wise comparisons (like numpy.ndarrays).
2907 
2908         See Also
2909         --------
2910         DataArray.broadcast_equals
2911         DataArray.identical
2912         """
2913         try:
2914             return self._all_compat(other, "equals")
2915         except (TypeError, AttributeError):
2916             return False
2917 
2918     def identical(self, other: "DataArray") -> bool:
2919         """Like equals, but also checks the array name and attributes, and
2920         attributes on all coordinates.
2921 
2922         See Also
2923         --------
2924         DataArray.broadcast_equals
2925         DataArray.equals
2926         """
2927         try:
2928             return self.name == other.name and self._all_compat(other, "identical")
2929         except (TypeError, AttributeError):
2930             return False
2931 
2932     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2933         # use the same naming heuristics as pandas:
2934         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2935         other_name = getattr(other, "name", _default)
2936         if other_name is _default or other_name == self.name:
2937             return self.name
2938         else:
2939             return None
2940 
2941     def __array_wrap__(self, obj, context=None) -> "DataArray":
2942         new_var = self.variable.__array_wrap__(obj, context)
2943         return self._replace(new_var)
2944 
2945     def __matmul__(self, obj):
2946         return self.dot(obj)
2947 
2948     def __rmatmul__(self, other):
2949         # currently somewhat duplicative, as only other DataArrays are
2950         # compatible with matmul
2951         return computation.dot(other, self)
2952 
2953     @staticmethod
2954     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2955         @functools.wraps(f)
2956         def func(self, *args, **kwargs):
2957             keep_attrs = kwargs.pop("keep_attrs", None)
2958             if keep_attrs is None:
2959                 keep_attrs = _get_keep_attrs(default=True)
2960             with warnings.catch_warnings():
2961                 warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
2962                 warnings.filterwarnings(
2963                     "ignore", r"Mean of empty slice", category=RuntimeWarning
2964                 )
2965                 with np.errstate(all="ignore"):
2966                     da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2967                 if keep_attrs:
2968                     da.attrs = self.attrs
2969                 return da
2970 
2971         return func
2972 
2973     @staticmethod
2974     def _binary_op(
2975         f: Callable[..., Any],
2976         reflexive: bool = False,
2977         join: str = None,  # see xarray.align
2978         **ignored_kwargs,
2979     ) -> Callable[..., "DataArray"]:
2980         @functools.wraps(f)
2981         def func(self, other):
2982             if isinstance(other, (Dataset, groupby.GroupBy)):
2983                 return NotImplemented
2984             if isinstance(other, DataArray):
2985                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2986                 self, other = align(self, other, join=align_type, copy=False)
2987             other_variable = getattr(other, "variable", other)
2988             other_coords = getattr(other, "coords", None)
2989 
2990             variable = (
2991                 f(self.variable, other_variable)
2992                 if not reflexive
2993                 else f(other_variable, self.variable)
2994             )
2995             coords, indexes = self.coords._merge_raw(other_coords)
2996             name = self._result_name(other)
2997 
2998             return self._replace(variable, coords, name, indexes=indexes)
2999 
3000         return func
3001 
3002     @staticmethod
3003     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
3004         @functools.wraps(f)
3005         def func(self, other):
3006             if isinstance(other, groupby.GroupBy):
3007                 raise TypeError(
3008                     "in-place operations between a DataArray and "
3009                     "a grouped object are not permitted"
3010                 )
3011             # n.b. we can't align other to self (with other.reindex_like(self))
3012             # because `other` may be converted into floats, which would cause
3013             # in-place arithmetic to fail unpredictably. Instead, we simply
3014             # don't support automatic alignment with in-place arithmetic.
3015             other_coords = getattr(other, "coords", None)
3016             other_variable = getattr(other, "variable", other)
3017             try:
3018                 with self.coords._merge_inplace(other_coords):
3019                     f(self.variable, other_variable)
3020             except MergeError as exc:
3021                 raise MergeError(
3022                     "Automatic alignment is not supported for in-place operations.\n"
3023                     "Consider aligning the indices manually or using a not-in-place operation.\n"
3024                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3025                 ) from exc
3026             return self
3027 
3028         return func
3029 
3030     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
3031         self.attrs = other.attrs
3032 
3033     plot = utils.UncachedAccessor(_PlotMethods)
3034 
3035     def _title_for_slice(self, truncate: int = 50) -> str:
3036         """
3037         If the dataarray has 1 dimensional coordinates or comes from a slice
3038         we can show that info in the title
3039 
3040         Parameters
3041         ----------
3042         truncate : int, default: 50
3043             maximum number of characters for title
3044 
3045         Returns
3046         -------
3047         title : string
3048             Can be used for plot titles
3049 
3050         """
3051         one_dims = []
3052         for dim, coord in self.coords.items():
3053             if coord.size == 1:
3054                 one_dims.append(
3055                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
3056                 )
3057 
3058         title = ", ".join(one_dims)
3059         if len(title) > truncate:
3060             title = title[: (truncate - 3)] + "..."
3061 
3062         return title
3063 
3064     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
3065         """Calculate the n-th order discrete difference along given axis.
3066 
3067         Parameters
3068         ----------
3069         dim : hashable
3070             Dimension over which to calculate the finite difference.
3071         n : int, optional
3072             The number of times values are differenced.
3073         label : hashable, optional
3074             The new coordinate in dimension ``dim`` will have the
3075             values of either the minuend's or subtrahend's coordinate
3076             for values 'upper' and 'lower', respectively.  Other
3077             values are not supported.
3078 
3079         Returns
3080         -------
3081         difference : same type as caller
3082             The n-th order finite difference of this object.
3083 
3084         Notes
3085         -----
3086         `n` matches numpy's behavior and is different from pandas' first argument named
3087         `periods`.
3088 
3089         Examples
3090         --------
3091         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3092         >>> arr.diff("x")
3093         <xarray.DataArray (x: 3)>
3094         array([0, 1, 0])
3095         Coordinates:
3096           * x        (x) int64 2 3 4
3097         >>> arr.diff("x", 2)
3098         <xarray.DataArray (x: 2)>
3099         array([ 1, -1])
3100         Coordinates:
3101           * x        (x) int64 3 4
3102 
3103         See Also
3104         --------
3105         DataArray.differentiate
3106         """
3107         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3108         return self._from_temp_dataset(ds)
3109 
3110     def shift(
3111         self,
3112         shifts: Mapping[Hashable, int] = None,
3113         fill_value: Any = dtypes.NA,
3114         **shifts_kwargs: int,
3115     ) -> "DataArray":
3116         """Shift this array by an offset along one or more dimensions.
3117 
3118         Only the data is moved; coordinates stay in place. Values shifted from
3119         beyond array bounds are replaced by NaN. This is consistent with the
3120         behavior of ``shift`` in pandas.
3121 
3122         Parameters
3123         ----------
3124         shifts : mapping of hashable to int, optional
3125             Integer offset to shift along each of the given dimensions.
3126             Positive offsets shift to the right; negative offsets shift to the
3127             left.
3128         fill_value : scalar, optional
3129             Value to use for newly missing values
3130         **shifts_kwargs
3131             The keyword arguments form of ``shifts``.
3132             One of shifts or shifts_kwargs must be provided.
3133 
3134         Returns
3135         -------
3136         shifted : DataArray
3137             DataArray with the same coordinates and attributes but shifted
3138             data.
3139 
3140         See Also
3141         --------
3142         roll
3143 
3144         Examples
3145         --------
3146         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3147         >>> arr.shift(x=1)
3148         <xarray.DataArray (x: 3)>
3149         array([nan,  5.,  6.])
3150         Dimensions without coordinates: x
3151         """
3152         variable = self.variable.shift(
3153             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3154         )
3155         return self._replace(variable=variable)
3156 
3157     def roll(
3158         self,
3159         shifts: Mapping[Hashable, int] = None,
3160         roll_coords: bool = None,
3161         **shifts_kwargs: int,
3162     ) -> "DataArray":
3163         """Roll this array by an offset along one or more dimensions.
3164 
3165         Unlike shift, roll may rotate all variables, including coordinates
3166         if specified. The direction of rotation is consistent with
3167         :py:func:`numpy.roll`.
3168 
3169         Parameters
3170         ----------
3171         shifts : mapping of hashable to int, optional
3172             Integer offset to rotate each of the given dimensions.
3173             Positive offsets roll to the right; negative offsets roll to the
3174             left.
3175         roll_coords : bool
3176             Indicates whether to roll the coordinates by the offset
3177             The current default of roll_coords (None, equivalent to True) is
3178             deprecated and will change to False in a future version.
3179             Explicitly pass roll_coords to silence the warning.
3180         **shifts_kwargs
3181             The keyword arguments form of ``shifts``.
3182             One of shifts or shifts_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         rolled : DataArray
3187             DataArray with the same attributes but rolled data and coordinates.
3188 
3189         See Also
3190         --------
3191         shift
3192 
3193         Examples
3194         --------
3195         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3196         >>> arr.roll(x=1)
3197         <xarray.DataArray (x: 3)>
3198         array([7, 5, 6])
3199         Dimensions without coordinates: x
3200         """
3201         ds = self._to_temp_dataset().roll(
3202             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3203         )
3204         return self._from_temp_dataset(ds)
3205 
3206     @property
3207     def real(self) -> "DataArray":
3208         return self._replace(self.variable.real)
3209 
3210     @property
3211     def imag(self) -> "DataArray":
3212         return self._replace(self.variable.imag)
3213 
3214     def dot(
3215         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3216     ) -> "DataArray":
3217         """Perform dot product of two DataArrays along their shared dims.
3218 
3219         Equivalent to taking taking tensordot over all shared dims.
3220 
3221         Parameters
3222         ----------
3223         other : DataArray
3224             The other array with which the dot product is performed.
3225         dims : ..., hashable or sequence of hashable, optional
3226             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3227             If not specified, then all the common dimensions are summed over.
3228 
3229         Returns
3230         -------
3231         result : DataArray
3232             Array resulting from the dot product over all shared dimensions.
3233 
3234         See Also
3235         --------
3236         dot
3237         numpy.tensordot
3238 
3239         Examples
3240         --------
3241         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3242         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3243         >>> dm_vals = np.arange(4)
3244         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3245 
3246         >>> dm.dims
3247         ('z',)
3248 
3249         >>> da.dims
3250         ('x', 'y', 'z')
3251 
3252         >>> dot_result = da.dot(dm)
3253         >>> dot_result.dims
3254         ('x', 'y')
3255 
3256         """
3257         if isinstance(other, Dataset):
3258             raise NotImplementedError(
3259                 "dot products are not yet supported with Dataset objects."
3260             )
3261         if not isinstance(other, DataArray):
3262             raise TypeError("dot only operates on DataArrays.")
3263 
3264         return computation.dot(self, other, dims=dims)
3265 
3266     def sortby(
3267         self,
3268         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3269         ascending: bool = True,
3270     ) -> "DataArray":
3271         """Sort object by labels or values (along an axis).
3272 
3273         Sorts the dataarray, either along specified dimensions,
3274         or according to values of 1-D dataarrays that share dimension
3275         with calling object.
3276 
3277         If the input variables are dataarrays, then the dataarrays are aligned
3278         (via left-join) to the calling object prior to sorting by cell values.
3279         NaNs are sorted to the end, following Numpy convention.
3280 
3281         If multiple sorts along the same dimension is
3282         given, numpy's lexsort is performed along that dimension:
3283         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3284         and the FIRST key in the sequence is used as the primary sort key,
3285         followed by the 2nd key, etc.
3286 
3287         Parameters
3288         ----------
3289         variables : hashable, DataArray, or sequence of hashable or DataArray
3290             1D DataArray objects or name(s) of 1D variable(s) in
3291             coords whose values are used to sort this array.
3292         ascending : bool, optional
3293             Whether to sort by ascending or descending order.
3294 
3295         Returns
3296         -------
3297         sorted : DataArray
3298             A new dataarray where all the specified dims are sorted by dim
3299             labels.
3300 
3301         Examples
3302         --------
3303         >>> da = xr.DataArray(
3304         ...     np.random.rand(5),
3305         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3306         ...     dims="time",
3307         ... )
3308         >>> da
3309         <xarray.DataArray (time: 5)>
3310         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3311         Coordinates:
3312           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3313 
3314         >>> da.sortby(da)
3315         <xarray.DataArray (time: 5)>
3316         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3317         Coordinates:
3318           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3319         """
3320         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3321         return self._from_temp_dataset(ds)
3322 
3323     def quantile(
3324         self,
3325         q: Any,
3326         dim: Union[Hashable, Sequence[Hashable], None] = None,
3327         interpolation: str = "linear",
3328         keep_attrs: bool = None,
3329         skipna: bool = True,
3330     ) -> "DataArray":
3331         """Compute the qth quantile of the data along the specified dimension.
3332 
3333         Returns the qth quantiles(s) of the array elements.
3334 
3335         Parameters
3336         ----------
3337         q : float or array-like of float
3338             Quantile to compute, which must be between 0 and 1 inclusive.
3339         dim : hashable or sequence of hashable, optional
3340             Dimension(s) over which to apply quantile.
3341         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3342             This optional parameter specifies the interpolation method to
3343             use when the desired quantile lies between two data points
3344             ``i < j``:
3345 
3346                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3347                   the fractional part of the index surrounded by ``i`` and
3348                   ``j``.
3349                 - lower: ``i``.
3350                 - higher: ``j``.
3351                 - nearest: ``i`` or ``j``, whichever is nearest.
3352                 - midpoint: ``(i + j) / 2``.
3353         keep_attrs : bool, optional
3354             If True, the dataset's attributes (`attrs`) will be copied from
3355             the original object to the new one.  If False (default), the new
3356             object will be returned without attributes.
3357         skipna : bool, optional
3358             Whether to skip missing values when aggregating.
3359 
3360         Returns
3361         -------
3362         quantiles : DataArray
3363             If `q` is a single quantile, then the result
3364             is a scalar. If multiple percentiles are given, first axis of
3365             the result corresponds to the quantile and a quantile dimension
3366             is added to the return array. The other dimensions are the
3367             dimensions that remain after the reduction of the array.
3368 
3369         See Also
3370         --------
3371         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3372 
3373         Examples
3374         --------
3375         >>> da = xr.DataArray(
3376         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3377         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3378         ...     dims=("x", "y"),
3379         ... )
3380         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3381         <xarray.DataArray ()>
3382         array(0.7)
3383         Coordinates:
3384             quantile  float64 0.0
3385         >>> da.quantile(0, dim="x")
3386         <xarray.DataArray (y: 4)>
3387         array([0.7, 4.2, 2.6, 1.5])
3388         Coordinates:
3389           * y         (y) float64 1.0 1.5 2.0 2.5
3390             quantile  float64 0.0
3391         >>> da.quantile([0, 0.5, 1])
3392         <xarray.DataArray (quantile: 3)>
3393         array([0.7, 3.4, 9.4])
3394         Coordinates:
3395           * quantile  (quantile) float64 0.0 0.5 1.0
3396         >>> da.quantile([0, 0.5, 1], dim="x")
3397         <xarray.DataArray (quantile: 3, y: 4)>
3398         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3399                [3.6 , 5.75, 6.  , 1.7 ],
3400                [6.5 , 7.3 , 9.4 , 1.9 ]])
3401         Coordinates:
3402           * y         (y) float64 1.0 1.5 2.0 2.5
3403           * quantile  (quantile) float64 0.0 0.5 1.0
3404         """
3405 
3406         ds = self._to_temp_dataset().quantile(
3407             q,
3408             dim=dim,
3409             keep_attrs=keep_attrs,
3410             interpolation=interpolation,
3411             skipna=skipna,
3412         )
3413         return self._from_temp_dataset(ds)
3414 
3415     def rank(
3416         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3417     ) -> "DataArray":
3418         """Ranks the data.
3419 
3420         Equal values are assigned a rank that is the average of the ranks that
3421         would have been otherwise assigned to all of the values within that
3422         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3423 
3424         NaNs in the input array are returned as NaNs.
3425 
3426         The `bottleneck` library is required.
3427 
3428         Parameters
3429         ----------
3430         dim : hashable
3431             Dimension over which to compute rank.
3432         pct : bool, optional
3433             If True, compute percentage ranks, otherwise compute integer ranks.
3434         keep_attrs : bool, optional
3435             If True, the dataset's attributes (`attrs`) will be copied from
3436             the original object to the new one.  If False (default), the new
3437             object will be returned without attributes.
3438 
3439         Returns
3440         -------
3441         ranked : DataArray
3442             DataArray with the same coordinates and dtype 'float64'.
3443 
3444         Examples
3445         --------
3446         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3447         >>> arr.rank("x")
3448         <xarray.DataArray (x: 3)>
3449         array([1., 2., 3.])
3450         Dimensions without coordinates: x
3451         """
3452 
3453         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3454         return self._from_temp_dataset(ds)
3455 
3456     def differentiate(
3457         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3458     ) -> "DataArray":
3459         """ Differentiate the array with the second order accurate central
3460         differences.
3461 
3462         .. note::
3463             This feature is limited to simple cartesian geometry, i.e. coord
3464             must be one dimensional.
3465 
3466         Parameters
3467         ----------
3468         coord : hashable
3469             The coordinate to be used to compute the gradient.
3470         edge_order : {1, 2}, default: 1
3471             N-th order accurate differences at the boundaries.
3472         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3473                          "us", "ns", "ps", "fs", "as"} or None, optional
3474             Unit to compute gradient. Only valid for datetime coordinate.
3475 
3476         Returns
3477         -------
3478         differentiated: DataArray
3479 
3480         See also
3481         --------
3482         numpy.gradient: corresponding numpy function
3483 
3484         Examples
3485         --------
3486 
3487         >>> da = xr.DataArray(
3488         ...     np.arange(12).reshape(4, 3),
3489         ...     dims=["x", "y"],
3490         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3491         ... )
3492         >>> da
3493         <xarray.DataArray (x: 4, y: 3)>
3494         array([[ 0,  1,  2],
3495                [ 3,  4,  5],
3496                [ 6,  7,  8],
3497                [ 9, 10, 11]])
3498         Coordinates:
3499           * x        (x) float64 0.0 0.1 1.1 1.2
3500         Dimensions without coordinates: y
3501         >>>
3502         >>> da.differentiate("x")
3503         <xarray.DataArray (x: 4, y: 3)>
3504         array([[30.        , 30.        , 30.        ],
3505                [27.54545455, 27.54545455, 27.54545455],
3506                [27.54545455, 27.54545455, 27.54545455],
3507                [30.        , 30.        , 30.        ]])
3508         Coordinates:
3509           * x        (x) float64 0.0 0.1 1.1 1.2
3510         Dimensions without coordinates: y
3511         """
3512         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3513         return self._from_temp_dataset(ds)
3514 
3515     def integrate(
3516         self,
3517         coord: Union[Hashable, Sequence[Hashable]] = None,
3518         datetime_unit: str = None,
3519         *,
3520         dim: Union[Hashable, Sequence[Hashable]] = None,
3521     ) -> "DataArray":
3522         """Integrate along the given coordinate using the trapezoidal rule.
3523 
3524         .. note::
3525             This feature is limited to simple cartesian geometry, i.e. coord
3526             must be one dimensional.
3527 
3528         Parameters
3529         ----------
3530         coord : hashable, or sequence of hashable
3531             Coordinate(s) used for the integration.
3532         dim : hashable, or sequence of hashable
3533             Coordinate(s) used for the integration.
3534         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3535                         'ps', 'fs', 'as'}, optional
3536             Specify the unit if a datetime coordinate is used.
3537 
3538         Returns
3539         -------
3540         integrated : DataArray
3541 
3542         See also
3543         --------
3544         Dataset.integrate
3545         numpy.trapz : corresponding numpy function
3546 
3547         Examples
3548         --------
3549 
3550         >>> da = xr.DataArray(
3551         ...     np.arange(12).reshape(4, 3),
3552         ...     dims=["x", "y"],
3553         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3554         ... )
3555         >>> da
3556         <xarray.DataArray (x: 4, y: 3)>
3557         array([[ 0,  1,  2],
3558                [ 3,  4,  5],
3559                [ 6,  7,  8],
3560                [ 9, 10, 11]])
3561         Coordinates:
3562           * x        (x) float64 0.0 0.1 1.1 1.2
3563         Dimensions without coordinates: y
3564         >>>
3565         >>> da.integrate("x")
3566         <xarray.DataArray (y: 3)>
3567         array([5.4, 6.6, 7.8])
3568         Dimensions without coordinates: y
3569         """
3570         if dim is not None and coord is not None:
3571             raise ValueError(
3572                 "Cannot pass both 'dim' and 'coord'. Please pass only 'coord' instead."
3573             )
3574 
3575         if dim is not None and coord is None:
3576             coord = dim
3577             msg = (
3578                 "The `dim` keyword argument to `DataArray.integrate` is "
3579                 "being replaced with `coord`, for consistency with "
3580                 "`Dataset.integrate`. Please pass `coord` instead."
3581                 " `dim` will be removed in version 0.19.0."
3582             )
3583             warnings.warn(msg, FutureWarning, stacklevel=2)
3584 
3585         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
3586         return self._from_temp_dataset(ds)
3587 
3588     def unify_chunks(self) -> "DataArray":
3589         """Unify chunk size along all chunked dimensions of this DataArray.
3590 
3591         Returns
3592         -------
3593         DataArray with consistent chunk sizes for all dask-array variables
3594 
3595         See Also
3596         --------
3597         dask.array.core.unify_chunks
3598         """
3599         ds = self._to_temp_dataset().unify_chunks()
3600         return self._from_temp_dataset(ds)
3601 
3602     def map_blocks(
3603         self,
3604         func: "Callable[..., T_DSorDA]",
3605         args: Sequence[Any] = (),
3606         kwargs: Mapping[str, Any] = None,
3607         template: Union["DataArray", "Dataset"] = None,
3608     ) -> "T_DSorDA":
3609         """
3610         Apply a function to each block of this DataArray.
3611 
3612         .. warning::
3613             This method is experimental and its signature may change.
3614 
3615         Parameters
3616         ----------
3617         func : callable
3618             User-provided function that accepts a DataArray as its first
3619             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3620             corresponding to one chunk along each chunked dimension. ``func`` will be
3621             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3622 
3623             This function must return either a single DataArray or a single Dataset.
3624 
3625             This function cannot add a new chunked dimension.
3626         args : sequence
3627             Passed to func after unpacking and subsetting any xarray objects by blocks.
3628             xarray objects in args must be aligned with this object, otherwise an error is raised.
3629         kwargs : mapping
3630             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3631             subset to blocks. Passing dask collections in kwargs is not allowed.
3632         template : DataArray or Dataset, optional
3633             xarray object representing the final result after compute is called. If not provided,
3634             the function will be first run on mocked-up data, that looks like this object but
3635             has sizes 0, to determine properties of the returned object such as dtype,
3636             variable names, attributes, new dimensions and new indexes (if any).
3637             ``template`` must be provided if the function changes the size of existing dimensions.
3638             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3639             ``attrs`` set by ``func`` will be ignored.
3640 
3641         Returns
3642         -------
3643         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3644         function.
3645 
3646         Notes
3647         -----
3648         This function is designed for when ``func`` needs to manipulate a whole xarray object
3649         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
3650         recommended to use ``apply_ufunc``.
3651 
3652         If none of the variables in this object is backed by dask arrays, calling this function is
3653         equivalent to calling ``func(obj, *args, **kwargs)``.
3654 
3655         See Also
3656         --------
3657         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
3658         xarray.DataArray.map_blocks
3659 
3660         Examples
3661         --------
3662         Calculate an anomaly from climatology using ``.groupby()``. Using
3663         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3664         its indices, and its methods like ``.groupby()``.
3665 
3666         >>> def calculate_anomaly(da, groupby_type="time.month"):
3667         ...     gb = da.groupby(groupby_type)
3668         ...     clim = gb.mean(dim="time")
3669         ...     return gb - clim
3670         ...
3671         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3672         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3673         >>> np.random.seed(123)
3674         >>> array = xr.DataArray(
3675         ...     np.random.rand(len(time)),
3676         ...     dims=["time"],
3677         ...     coords={"time": time, "month": month},
3678         ... ).chunk()
3679         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3680         <xarray.DataArray (time: 24)>
3681         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3682                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3683                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3684                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3685                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3686         Coordinates:
3687           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3688             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3689 
3690         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3691         to the function being applied in ``xr.map_blocks()``:
3692 
3693         >>> array.map_blocks(
3694         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3695         ... )  # doctest: +ELLIPSIS
3696         <xarray.DataArray (time: 24)>
3697         dask.array<calculate_anomaly-...-<this, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3698         Coordinates:
3699           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3700             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3701         """
3702         from .parallel import map_blocks
3703 
3704         return map_blocks(func, self, args, kwargs, template)
3705 
3706     def polyfit(
3707         self,
3708         dim: Hashable,
3709         deg: int,
3710         skipna: bool = None,
3711         rcond: float = None,
3712         w: Union[Hashable, Any] = None,
3713         full: bool = False,
3714         cov: bool = False,
3715     ):
3716         """
3717         Least squares polynomial fit.
3718 
3719         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3720         invalid values when `skipna = True`.
3721 
3722         Parameters
3723         ----------
3724         dim : hashable
3725             Coordinate along which to fit the polynomials.
3726         deg : int
3727             Degree of the fitting polynomial.
3728         skipna : bool, optional
3729             If True, removes all invalid values before fitting each 1D slices of the array.
3730             Default is True if data is stored in a dask.array or if there is any
3731             invalid values, False otherwise.
3732         rcond : float, optional
3733             Relative condition number to the fit.
3734         w : hashable or array-like, optional
3735             Weights to apply to the y-coordinate of the sample points.
3736             Can be an array-like object or the name of a coordinate in the dataset.
3737         full : bool, optional
3738             Whether to return the residuals, matrix rank and singular values in addition
3739             to the coefficients.
3740         cov : bool or str, optional
3741             Whether to return to the covariance matrix in addition to the coefficients.
3742             The matrix is not scaled if `cov='unscaled'`.
3743 
3744         Returns
3745         -------
3746         polyfit_results : Dataset
3747             A single dataset which contains:
3748 
3749             polyfit_coefficients
3750                 The coefficients of the best fit.
3751             polyfit_residuals
3752                 The residuals of the least-square computation (only included if `full=True`).
3753                 When the matrix rank is deficient, np.nan is returned.
3754             [dim]_matrix_rank
3755                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3756             [dim]_singular_value
3757                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3758             polyfit_covariance
3759                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3760 
3761         See Also
3762         --------
3763         numpy.polyfit
3764         """
3765         return self._to_temp_dataset().polyfit(
3766             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3767         )
3768 
3769     def pad(
3770         self,
3771         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3772         mode: str = "constant",
3773         stat_length: Union[
3774             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3775         ] = None,
3776         constant_values: Union[
3777             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3778         ] = None,
3779         end_values: Union[
3780             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3781         ] = None,
3782         reflect_type: str = None,
3783         **pad_width_kwargs: Any,
3784     ) -> "DataArray":
3785         """Pad this array along one or more dimensions.
3786 
3787         .. warning::
3788             This function is experimental and its behaviour is likely to change
3789             especially regarding padding of dimension coordinates (or IndexVariables).
3790 
3791         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3792         coordinates will be padded with the same mode, otherwise coordinates
3793         are padded using the "constant" mode with fill_value dtypes.NA.
3794 
3795         Parameters
3796         ----------
3797         pad_width : mapping of hashable to tuple of int
3798             Mapping with the form of {dim: (pad_before, pad_after)}
3799             describing the number of values padded along each dimension.
3800             {dim: pad} is a shortcut for pad_before = pad_after = pad
3801         mode : str, default: "constant"
3802             One of the following string values (taken from numpy docs)
3803 
3804             'constant' (default)
3805                 Pads with a constant value.
3806             'edge'
3807                 Pads with the edge values of array.
3808             'linear_ramp'
3809                 Pads with the linear ramp between end_value and the
3810                 array edge value.
3811             'maximum'
3812                 Pads with the maximum value of all or part of the
3813                 vector along each axis.
3814             'mean'
3815                 Pads with the mean value of all or part of the
3816                 vector along each axis.
3817             'median'
3818                 Pads with the median value of all or part of the
3819                 vector along each axis.
3820             'minimum'
3821                 Pads with the minimum value of all or part of the
3822                 vector along each axis.
3823             'reflect'
3824                 Pads with the reflection of the vector mirrored on
3825                 the first and last values of the vector along each
3826                 axis.
3827             'symmetric'
3828                 Pads with the reflection of the vector mirrored
3829                 along the edge of the array.
3830             'wrap'
3831                 Pads with the wrap of the vector along the axis.
3832                 The first values are used to pad the end and the
3833                 end values are used to pad the beginning.
3834         stat_length : int, tuple or mapping of hashable to tuple, default: None
3835             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3836             values at edge of each axis used to calculate the statistic value.
3837             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3838             statistic lengths along each dimension.
3839             ((before, after),) yields same before and after statistic lengths
3840             for each dimension.
3841             (stat_length,) or int is a shortcut for before = after = statistic
3842             length for all axes.
3843             Default is ``None``, to use the entire axis.
3844         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3845             Used in 'constant'.  The values to set the padded values for each
3846             axis.
3847             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3848             pad constants along each dimension.
3849             ``((before, after),)`` yields same before and after constants for each
3850             dimension.
3851             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3852             all dimensions.
3853             Default is 0.
3854         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3855             Used in 'linear_ramp'.  The values used for the ending value of the
3856             linear_ramp and that will form the edge of the padded array.
3857             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3858             end values along each dimension.
3859             ``((before, after),)`` yields same before and after end values for each
3860             axis.
3861             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3862             all axes.
3863             Default is 0.
3864         reflect_type : {"even", "odd"}, optional
3865             Used in "reflect", and "symmetric".  The "even" style is the
3866             default with an unaltered reflection around the edge value.  For
3867             the "odd" style, the extended part of the array is created by
3868             subtracting the reflected values from two times the edge value.
3869         **pad_width_kwargs
3870             The keyword arguments form of ``pad_width``.
3871             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3872 
3873         Returns
3874         -------
3875         padded : DataArray
3876             DataArray with the padded coordinates and data.
3877 
3878         See Also
3879         --------
3880         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3881 
3882         Notes
3883         -----
3884         For ``mode="constant"`` and ``constant_values=None``, integer types will be
3885         promoted to ``float`` and padded with ``np.nan``.
3886 
3887         Examples
3888         --------
3889         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
3890         >>> arr.pad(x=(1, 2), constant_values=0)
3891         <xarray.DataArray (x: 6)>
3892         array([0, 5, 6, 7, 0, 0])
3893         Coordinates:
3894           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3895 
3896         >>> da = xr.DataArray(
3897         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
3898         ...     dims=["x", "y"],
3899         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
3900         ... )
3901         >>> da.pad(x=1)
3902         <xarray.DataArray (x: 4, y: 4)>
3903         array([[nan, nan, nan, nan],
3904                [ 0.,  1.,  2.,  3.],
3905                [10., 11., 12., 13.],
3906                [nan, nan, nan, nan]])
3907         Coordinates:
3908           * x        (x) float64 nan 0.0 1.0 nan
3909           * y        (y) int64 10 20 30 40
3910             z        (x) float64 nan 100.0 200.0 nan
3911 
3912         Careful, ``constant_values`` are coerced to the data type of the array which may
3913         lead to a loss of precision:
3914 
3915         >>> da.pad(x=1, constant_values=1.23456789)
3916         <xarray.DataArray (x: 4, y: 4)>
3917         array([[ 1,  1,  1,  1],
3918                [ 0,  1,  2,  3],
3919                [10, 11, 12, 13],
3920                [ 1,  1,  1,  1]])
3921         Coordinates:
3922           * x        (x) float64 nan 0.0 1.0 nan
3923           * y        (y) int64 10 20 30 40
3924             z        (x) float64 nan 100.0 200.0 nan
3925         """
3926         ds = self._to_temp_dataset().pad(
3927             pad_width=pad_width,
3928             mode=mode,
3929             stat_length=stat_length,
3930             constant_values=constant_values,
3931             end_values=end_values,
3932             reflect_type=reflect_type,
3933             **pad_width_kwargs,
3934         )
3935         return self._from_temp_dataset(ds)
3936 
3937     def idxmin(
3938         self,
3939         dim: Hashable = None,
3940         skipna: bool = None,
3941         fill_value: Any = dtypes.NA,
3942         keep_attrs: bool = None,
3943     ) -> "DataArray":
3944         """Return the coordinate label of the minimum value along a dimension.
3945 
3946         Returns a new `DataArray` named after the dimension with the values of
3947         the coordinate labels along that dimension corresponding to minimum
3948         values along that dimension.
3949 
3950         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3951         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3952 
3953         Parameters
3954         ----------
3955         dim : str, optional
3956             Dimension over which to apply `idxmin`.  This is optional for 1D
3957             arrays, but required for arrays with 2 or more dimensions.
3958         skipna : bool or None, default: None
3959             If True, skip missing values (as marked by NaN). By default, only
3960             skips missing values for ``float``, ``complex``, and ``object``
3961             dtypes; other dtypes either do not have a sentinel missing value
3962             (``int``) or ``skipna=True`` has not been implemented
3963             (``datetime64`` or ``timedelta64``).
3964         fill_value : Any, default: NaN
3965             Value to be filled in case all of the values along a dimension are
3966             null.  By default this is NaN.  The fill value and result are
3967             automatically converted to a compatible dtype if possible.
3968             Ignored if ``skipna`` is False.
3969         keep_attrs : bool, default: False
3970             If True, the attributes (``attrs``) will be copied from the
3971             original object to the new one.  If False (default), the new object
3972             will be returned without attributes.
3973 
3974         Returns
3975         -------
3976         reduced : DataArray
3977             New `DataArray` object with `idxmin` applied to its data and the
3978             indicated dimension removed.
3979 
3980         See Also
3981         --------
3982         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3983 
3984         Examples
3985         --------
3986         >>> array = xr.DataArray(
3987         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3988         ... )
3989         >>> array.min()
3990         <xarray.DataArray ()>
3991         array(-2)
3992         >>> array.argmin()
3993         <xarray.DataArray ()>
3994         array(4)
3995         >>> array.idxmin()
3996         <xarray.DataArray 'x' ()>
3997         array('e', dtype='<U1')
3998 
3999         >>> array = xr.DataArray(
4000         ...     [
4001         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4002         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4003         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4004         ...     ],
4005         ...     dims=["y", "x"],
4006         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4007         ... )
4008         >>> array.min(dim="x")
4009         <xarray.DataArray (y: 3)>
4010         array([-2., -4.,  1.])
4011         Coordinates:
4012           * y        (y) int64 -1 0 1
4013         >>> array.argmin(dim="x")
4014         <xarray.DataArray (y: 3)>
4015         array([4, 0, 2])
4016         Coordinates:
4017           * y        (y) int64 -1 0 1
4018         >>> array.idxmin(dim="x")
4019         <xarray.DataArray 'x' (y: 3)>
4020         array([16.,  0.,  4.])
4021         Coordinates:
4022           * y        (y) int64 -1 0 1
4023         """
4024         return computation._calc_idxminmax(
4025             array=self,
4026             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4027             dim=dim,
4028             skipna=skipna,
4029             fill_value=fill_value,
4030             keep_attrs=keep_attrs,
4031         )
4032 
4033     def idxmax(
4034         self,
4035         dim: Hashable = None,
4036         skipna: bool = None,
4037         fill_value: Any = dtypes.NA,
4038         keep_attrs: bool = None,
4039     ) -> "DataArray":
4040         """Return the coordinate label of the maximum value along a dimension.
4041 
4042         Returns a new `DataArray` named after the dimension with the values of
4043         the coordinate labels along that dimension corresponding to maximum
4044         values along that dimension.
4045 
4046         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4047         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4048 
4049         Parameters
4050         ----------
4051         dim : hashable, optional
4052             Dimension over which to apply `idxmax`.  This is optional for 1D
4053             arrays, but required for arrays with 2 or more dimensions.
4054         skipna : bool or None, default: None
4055             If True, skip missing values (as marked by NaN). By default, only
4056             skips missing values for ``float``, ``complex``, and ``object``
4057             dtypes; other dtypes either do not have a sentinel missing value
4058             (``int``) or ``skipna=True`` has not been implemented
4059             (``datetime64`` or ``timedelta64``).
4060         fill_value : Any, default: NaN
4061             Value to be filled in case all of the values along a dimension are
4062             null.  By default this is NaN.  The fill value and result are
4063             automatically converted to a compatible dtype if possible.
4064             Ignored if ``skipna`` is False.
4065         keep_attrs : bool, default: False
4066             If True, the attributes (``attrs``) will be copied from the
4067             original object to the new one.  If False (default), the new object
4068             will be returned without attributes.
4069 
4070         Returns
4071         -------
4072         reduced : DataArray
4073             New `DataArray` object with `idxmax` applied to its data and the
4074             indicated dimension removed.
4075 
4076         See Also
4077         --------
4078         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4079 
4080         Examples
4081         --------
4082         >>> array = xr.DataArray(
4083         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4084         ... )
4085         >>> array.max()
4086         <xarray.DataArray ()>
4087         array(2)
4088         >>> array.argmax()
4089         <xarray.DataArray ()>
4090         array(1)
4091         >>> array.idxmax()
4092         <xarray.DataArray 'x' ()>
4093         array('b', dtype='<U1')
4094 
4095         >>> array = xr.DataArray(
4096         ...     [
4097         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4098         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4099         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4100         ...     ],
4101         ...     dims=["y", "x"],
4102         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4103         ... )
4104         >>> array.max(dim="x")
4105         <xarray.DataArray (y: 3)>
4106         array([2., 2., 1.])
4107         Coordinates:
4108           * y        (y) int64 -1 0 1
4109         >>> array.argmax(dim="x")
4110         <xarray.DataArray (y: 3)>
4111         array([0, 2, 2])
4112         Coordinates:
4113           * y        (y) int64 -1 0 1
4114         >>> array.idxmax(dim="x")
4115         <xarray.DataArray 'x' (y: 3)>
4116         array([0., 4., 4.])
4117         Coordinates:
4118           * y        (y) int64 -1 0 1
4119         """
4120         return computation._calc_idxminmax(
4121             array=self,
4122             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4123             dim=dim,
4124             skipna=skipna,
4125             fill_value=fill_value,
4126             keep_attrs=keep_attrs,
4127         )
4128 
4129     def argmin(
4130         self,
4131         dim: Union[Hashable, Sequence[Hashable]] = None,
4132         axis: int = None,
4133         keep_attrs: bool = None,
4134         skipna: bool = None,
4135     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4136         """Index or indices of the minimum of the DataArray over one or more dimensions.
4137 
4138         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4139         which can be passed directly to isel(). If a single str is passed to 'dim' then
4140         returns a DataArray with dtype int.
4141 
4142         If there are multiple minima, the indices of the first one found will be
4143         returned.
4144 
4145         Parameters
4146         ----------
4147         dim : hashable, sequence of hashable or ..., optional
4148             The dimensions over which to find the minimum. By default, finds minimum over
4149             all dimensions - for now returning an int for backward compatibility, but
4150             this is deprecated, in future will return a dict with indices for all
4151             dimensions; to return a dict with all dimensions now, pass '...'.
4152         axis : int, optional
4153             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4154             can be supplied.
4155         keep_attrs : bool, optional
4156             If True, the attributes (`attrs`) will be copied from the original
4157             object to the new one.  If False (default), the new object will be
4158             returned without attributes.
4159         skipna : bool, optional
4160             If True, skip missing values (as marked by NaN). By default, only
4161             skips missing values for float dtypes; other dtypes either do not
4162             have a sentinel missing value (int) or skipna=True has not been
4163             implemented (object, datetime64 or timedelta64).
4164 
4165         Returns
4166         -------
4167         result : DataArray or dict of DataArray
4168 
4169         See Also
4170         --------
4171         Variable.argmin, DataArray.idxmin
4172 
4173         Examples
4174         --------
4175         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4176         >>> array.min()
4177         <xarray.DataArray ()>
4178         array(-1)
4179         >>> array.argmin()
4180         <xarray.DataArray ()>
4181         array(2)
4182         >>> array.argmin(...)
4183         {'x': <xarray.DataArray ()>
4184         array(2)}
4185         >>> array.isel(array.argmin(...))
4186         <xarray.DataArray ()>
4187         array(-1)
4188 
4189         >>> array = xr.DataArray(
4190         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4191         ...     dims=("x", "y", "z"),
4192         ... )
4193         >>> array.min(dim="x")
4194         <xarray.DataArray (y: 3, z: 3)>
4195         array([[ 1,  2,  1],
4196                [ 2, -5,  1],
4197                [ 2,  1,  1]])
4198         Dimensions without coordinates: y, z
4199         >>> array.argmin(dim="x")
4200         <xarray.DataArray (y: 3, z: 3)>
4201         array([[1, 0, 0],
4202                [1, 1, 1],
4203                [0, 0, 1]])
4204         Dimensions without coordinates: y, z
4205         >>> array.argmin(dim=["x"])
4206         {'x': <xarray.DataArray (y: 3, z: 3)>
4207         array([[1, 0, 0],
4208                [1, 1, 1],
4209                [0, 0, 1]])
4210         Dimensions without coordinates: y, z}
4211         >>> array.min(dim=("x", "z"))
4212         <xarray.DataArray (y: 3)>
4213         array([ 1, -5,  1])
4214         Dimensions without coordinates: y
4215         >>> array.argmin(dim=["x", "z"])
4216         {'x': <xarray.DataArray (y: 3)>
4217         array([0, 1, 0])
4218         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4219         array([2, 1, 1])
4220         Dimensions without coordinates: y}
4221         >>> array.isel(array.argmin(dim=["x", "z"]))
4222         <xarray.DataArray (y: 3)>
4223         array([ 1, -5,  1])
4224         Dimensions without coordinates: y
4225         """
4226         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4227         if isinstance(result, dict):
4228             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4229         else:
4230             return self._replace_maybe_drop_dims(result)
4231 
4232     def argmax(
4233         self,
4234         dim: Union[Hashable, Sequence[Hashable]] = None,
4235         axis: int = None,
4236         keep_attrs: bool = None,
4237         skipna: bool = None,
4238     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4239         """Index or indices of the maximum of the DataArray over one or more dimensions.
4240 
4241         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4242         which can be passed directly to isel(). If a single str is passed to 'dim' then
4243         returns a DataArray with dtype int.
4244 
4245         If there are multiple maxima, the indices of the first one found will be
4246         returned.
4247 
4248         Parameters
4249         ----------
4250         dim : hashable, sequence of hashable or ..., optional
4251             The dimensions over which to find the maximum. By default, finds maximum over
4252             all dimensions - for now returning an int for backward compatibility, but
4253             this is deprecated, in future will return a dict with indices for all
4254             dimensions; to return a dict with all dimensions now, pass '...'.
4255         axis : int, optional
4256             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4257             can be supplied.
4258         keep_attrs : bool, optional
4259             If True, the attributes (`attrs`) will be copied from the original
4260             object to the new one.  If False (default), the new object will be
4261             returned without attributes.
4262         skipna : bool, optional
4263             If True, skip missing values (as marked by NaN). By default, only
4264             skips missing values for float dtypes; other dtypes either do not
4265             have a sentinel missing value (int) or skipna=True has not been
4266             implemented (object, datetime64 or timedelta64).
4267 
4268         Returns
4269         -------
4270         result : DataArray or dict of DataArray
4271 
4272         See Also
4273         --------
4274         Variable.argmax, DataArray.idxmax
4275 
4276         Examples
4277         --------
4278         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4279         >>> array.max()
4280         <xarray.DataArray ()>
4281         array(3)
4282         >>> array.argmax()
4283         <xarray.DataArray ()>
4284         array(3)
4285         >>> array.argmax(...)
4286         {'x': <xarray.DataArray ()>
4287         array(3)}
4288         >>> array.isel(array.argmax(...))
4289         <xarray.DataArray ()>
4290         array(3)
4291 
4292         >>> array = xr.DataArray(
4293         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4294         ...     dims=("x", "y", "z"),
4295         ... )
4296         >>> array.max(dim="x")
4297         <xarray.DataArray (y: 3, z: 3)>
4298         array([[3, 3, 2],
4299                [3, 5, 2],
4300                [2, 3, 3]])
4301         Dimensions without coordinates: y, z
4302         >>> array.argmax(dim="x")
4303         <xarray.DataArray (y: 3, z: 3)>
4304         array([[0, 1, 1],
4305                [0, 1, 0],
4306                [0, 1, 0]])
4307         Dimensions without coordinates: y, z
4308         >>> array.argmax(dim=["x"])
4309         {'x': <xarray.DataArray (y: 3, z: 3)>
4310         array([[0, 1, 1],
4311                [0, 1, 0],
4312                [0, 1, 0]])
4313         Dimensions without coordinates: y, z}
4314         >>> array.max(dim=("x", "z"))
4315         <xarray.DataArray (y: 3)>
4316         array([3, 5, 3])
4317         Dimensions without coordinates: y
4318         >>> array.argmax(dim=["x", "z"])
4319         {'x': <xarray.DataArray (y: 3)>
4320         array([0, 1, 0])
4321         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4322         array([0, 1, 2])
4323         Dimensions without coordinates: y}
4324         >>> array.isel(array.argmax(dim=["x", "z"]))
4325         <xarray.DataArray (y: 3)>
4326         array([3, 5, 3])
4327         Dimensions without coordinates: y
4328         """
4329         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4330         if isinstance(result, dict):
4331             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4332         else:
4333             return self._replace_maybe_drop_dims(result)
4334 
4335     # this needs to be at the end, or mypy will confuse with `str`
4336     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4337     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 21:44:08.069 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>to_series</method>
<code>
2683     def to_series(self) -> pd.Series:
2684         """Convert this array into a pandas.Series.
2685 
2686         The Series is indexed by the Cartesian product of index coordinates
2687         (in the form of a :py:class:`pandas.MultiIndex`).
2688         """
2689         index = self.coords.to_index()
2690         return pd.Series(self.values.reshape(-1), index=index, name=self.name)

</code><intended_behavior>This method should be enhanced to correctly handle `DataArray` objects with both stacked (multi-indexed) and other dimensions, ensuring the conversion process accurately reflects a combined `MultiIndex` that incorporates both the stacked dimensions and any additional dimensions. The method should recognize and properly configure these dimensions into the resulting `pandas.Series` index, preserving the logical structure and relationships of the original data dimensions.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
221 class DataArray(AbstractArray, DataWithCoords):
222     """N-dimensional array with labeled coordinates and dimensions.
223 
224     DataArray provides a wrapper around numpy ndarrays that uses
225     labeled dimensions and coordinates to support metadata aware
226     operations. The API is similar to that for the pandas Series or
227     DataFrame, but DataArray objects can have any number of dimensions,
228     and their contents have fixed data types.
229 
230     Additional features over raw numpy arrays:
231 
232     - Apply operations over dimensions by name: ``x.sum('time')``.
233     - Select or assign values by integer location (like numpy):
234       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
235       ``x.sel(time='2014-01-01')``.
236     - Mathematical operations (e.g., ``x - y``) vectorize across
237       multiple dimensions (known in numpy as "broadcasting") based on
238       dimension names, regardless of their original order.
239     - Keep track of arbitrary metadata in the form of a Python
240       dictionary: ``x.attrs``
241     - Convert to a pandas Series: ``x.to_series()``.
242 
243     Getting items from or doing mathematical operations with a
244     DataArray always returns another DataArray.
245 
246     Parameters
247     ----------
248     data : array_like
249         Values for this array. Must be an ``numpy.ndarray``, ndarray
250         like, or castable to an ``ndarray``. If a self-described xarray
251         or pandas object, attempts are made to use this array's
252         metadata to fill in other unspecified arguments. A view of the
253         array's data is used instead of a copy if possible.
254     coords : sequence or dict of array_like, optional
255         Coordinates (tick labels) to use for indexing along each
256         dimension. The following notations are accepted:
257 
258         - mapping {dimension name: array-like}
259         - sequence of tuples that are valid arguments for
260           ``xarray.Variable()``
261           - (dims, data)
262           - (dims, data, attrs)
263           - (dims, data, attrs, encoding)
264 
265         Additionally, it is possible to define a coord whose name
266         does not match the dimension name, or a coord based on multiple
267         dimensions, with one of the following notations:
268 
269         - mapping {coord name: DataArray}
270         - mapping {coord name: Variable}
271         - mapping {coord name: (dimension name, array-like)}
272         - mapping {coord name: (tuple of dimension names, array-like)}
273 
274     dims : hashable or sequence of hashable, optional
275         Name(s) of the data dimension(s). Must be either a hashable
276         (only for 1D data) or a sequence of hashables with length equal
277         to the number of dimensions. If this argument is omitted,
278         dimension names default to ``['dim_0', ... 'dim_n']``.
279     name : str or None, optional
280         Name of this array.
281     attrs : dict_like or None, optional
282         Attributes to assign to the new instance. By default, an empty
283         attribute dictionary is initialized.
284 
285     Examples
286     --------
287     Create data:
288 
289     >>> np.random.seed(0)
290     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
291     >>> precipitation = 10 * np.random.rand(2, 2, 3)
292     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
293     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
294     >>> time = pd.date_range("2014-09-06", periods=3)
295     >>> reference_time = pd.Timestamp("2014-09-05")
296 
297     Initialize a dataarray with multiple dimensions:
298 
299     >>> da = xr.DataArray(
300     ...     data=temperature,
301     ...     dims=["x", "y", "time"],
302     ...     coords=dict(
303     ...         lon=(["x", "y"], lon),
304     ...         lat=(["x", "y"], lat),
305     ...         time=time,
306     ...         reference_time=reference_time,
307     ...     ),
308     ...     attrs=dict(
309     ...         description="Ambient temperature.",
310     ...         units="degC",
311     ...     ),
312     ... )
313     >>> da
314     <xarray.DataArray (x: 2, y: 2, time: 3)>
315     array([[[29.11241877, 18.20125767, 22.82990387],
316             [32.92714559, 29.94046392,  7.18177696]],
317     <BLANKLINE>
318            [[22.60070734, 13.78914233, 14.17424919],
319             [18.28478802, 16.15234857, 26.63418806]]])
320     Coordinates:
321         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
322         lat             (x, y) float64 42.25 42.21 42.63 42.59
323       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
324         reference_time  datetime64[ns] 2014-09-05
325     Dimensions without coordinates: x, y
326     Attributes:
327         description:  Ambient temperature.
328         units:        degC
329 
330     Find out where the coldest temperature was:
331 
332     >>> da.isel(da.argmin(...))
333     <xarray.DataArray ()>
334     array(7.18177696)
335     Coordinates:
336         lon             float64 -99.32
337         lat             float64 42.21
338         time            datetime64[ns] 2014-09-08
339         reference_time  datetime64[ns] 2014-09-05
340     Attributes:
341         description:  Ambient temperature.
342         units:        degC
343     """
344 
345     _cache: Dict[str, Any]
346     _coords: Dict[Any, Variable]
347     _close: Optional[Callable[[], None]]
348     _indexes: Optional[Dict[Hashable, pd.Index]]
349     _name: Optional[Hashable]
350     _variable: Variable
351 
352     __slots__ = (
353         "_cache",
354         "_coords",
355         "_close",
356         "_indexes",
357         "_name",
358         "_variable",
359         "__weakref__",
360     )
361 
362     _groupby_cls = groupby.DataArrayGroupBy
363     _rolling_cls = rolling.DataArrayRolling
364     _coarsen_cls = rolling.DataArrayCoarsen
365     _resample_cls = resample.DataArrayResample
366     _weighted_cls = weighted.DataArrayWeighted
367 
368     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
369 
370     def __init__(
371         self,
372         data: Any = dtypes.NA,
373         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
374         dims: Union[Hashable, Sequence[Hashable], None] = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: Dict[Hashable, pd.Index] = None,
379         fastpath: bool = False,
380     ):
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385         else:
386             # try to fill in arguments from data if they weren't supplied
387             if coords is None:
388 
389                 if isinstance(data, DataArray):
390                     coords = data.coords
391                 elif isinstance(data, pd.Series):
392                     coords = [data.index]
393                 elif isinstance(data, pd.DataFrame):
394                     coords = [data.index, data.columns]
395                 elif isinstance(data, (pd.Index, IndexVariable)):
396                     coords = [data]
397                 elif isinstance(data, pdcompat.Panel):
398                     coords = [data.items, data.major_axis, data.minor_axis]
399 
400             if dims is None:
401                 dims = getattr(data, "dims", getattr(coords, "dims", None))
402             if name is None:
403                 name = getattr(data, "name", None)
404             if attrs is None and not isinstance(data, PANDAS_TYPES):
405                 attrs = getattr(data, "attrs", None)
406 
407             data = _check_data_shape(data, coords, dims)
408             data = as_compatible_data(data)
409             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
410             variable = Variable(dims, data, attrs, fastpath=True)
411             indexes = dict(
412                 _extract_indexes_from_coords(coords)
413             )  # needed for to_dataset
414 
415         # These fully describe a DataArray
416         self._variable = variable
417         assert isinstance(coords, dict)
418         self._coords = coords
419         self._name = name
420 
421         # TODO(shoyer): document this argument, once it becomes part of the
422         # public interface.
423         self._indexes = indexes
424 
425         self._close = None
426 
427     def _replace(
428         self,
429         variable: Variable = None,
430         coords=None,
431         name: Union[Hashable, None, Default] = _default,
432         indexes=None,
433     ) -> "DataArray":
434         if variable is None:
435             variable = self.variable
436         if coords is None:
437             coords = self._coords
438         if name is _default:
439             name = self.name
440         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
441 
442     def _replace_maybe_drop_dims(
443         self, variable: Variable, name: Union[Hashable, None, Default] = _default
444     ) -> "DataArray":
445         if variable.dims == self.dims and variable.shape == self.shape:
446             coords = self._coords.copy()
447             indexes = self._indexes
448         elif variable.dims == self.dims:
449             # Shape has changed (e.g. from reduce(..., keepdims=True)
450             new_sizes = dict(zip(self.dims, variable.shape))
451             coords = {
452                 k: v
453                 for k, v in self._coords.items()
454                 if v.shape == tuple(new_sizes[d] for d in v.dims)
455             }
456             changed_dims = [
457                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
458             ]
459             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
460         else:
461             allowed_dims = set(variable.dims)
462             coords = {
463                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
464             }
465             indexes = propagate_indexes(
466                 self._indexes, exclude=(set(self.dims) - allowed_dims)
467             )
468         return self._replace(variable, coords, name, indexes=indexes)
469 
470     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
471         if not len(indexes):
472             return self
473         coords = self._coords.copy()
474         for name, idx in indexes.items():
475             coords[name] = IndexVariable(name, idx)
476         obj = self._replace(coords=coords)
477 
478         # switch from dimension to level names, if necessary
479         dim_names: Dict[Any, str] = {}
480         for dim, idx in indexes.items():
481             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
482                 dim_names[dim] = idx.name
483         if dim_names:
484             obj = obj.rename(dim_names)
485         return obj
486 
487     def _to_temp_dataset(self) -> Dataset:
488         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
489 
490     def _from_temp_dataset(
491         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
492     ) -> "DataArray":
493         variable = dataset._variables.pop(_THIS_ARRAY)
494         coords = dataset._variables
495         indexes = dataset._indexes
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _to_dataset_split(self, dim: Hashable) -> Dataset:
499         """ splits dataarray along dimension 'dim' """
500 
501         def subset(dim, label):
502             array = self.loc[{dim: label}]
503             array.attrs = {}
504             return as_variable(array)
505 
506         variables = {label: subset(dim, label) for label in self.get_index(dim)}
507         variables.update({k: v for k, v in self._coords.items() if k != dim})
508         indexes = propagate_indexes(self._indexes, exclude=dim)
509         coord_names = set(self._coords) - {dim}
510         dataset = Dataset._construct_direct(
511             variables, coord_names, indexes=indexes, attrs=self.attrs
512         )
513         return dataset
514 
515     def _to_dataset_whole(
516         self, name: Hashable = None, shallow_copy: bool = True
517     ) -> Dataset:
518         if name is None:
519             name = self.name
520         if name is None:
521             raise ValueError(
522                 "unable to convert unnamed DataArray to a "
523                 "Dataset without providing an explicit name"
524             )
525         if name in self.coords:
526             raise ValueError(
527                 "cannot create a Dataset from a DataArray with "
528                 "the same name as one of its coordinates"
529             )
530         # use private APIs for speed: this is called by _to_temp_dataset(),
531         # which is used in the guts of a lot of operations (e.g., reindex)
532         variables = self._coords.copy()
533         variables[name] = self.variable
534         if shallow_copy:
535             for k in variables:
536                 variables[k] = variables[k].copy(deep=False)
537         indexes = self._indexes
538 
539         coord_names = set(self._coords)
540         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
541         return dataset
542 
543     def to_dataset(
544         self,
545         dim: Hashable = None,
546         *,
547         name: Hashable = None,
548         promote_attrs: bool = False,
549     ) -> Dataset:
550         """Convert a DataArray to a Dataset.
551 
552         Parameters
553         ----------
554         dim : hashable, optional
555             Name of the dimension on this array along which to split this array
556             into separate variables. If not provided, this array is converted
557             into a Dataset of one variable.
558         name : hashable, optional
559             Name to substitute for this array's name. Only valid if ``dim`` is
560             not provided.
561         promote_attrs : bool, default: False
562             Set to True to shallow copy attrs of DataArray to returned Dataset.
563 
564         Returns
565         -------
566         dataset : Dataset
567         """
568         if dim is not None and dim not in self.dims:
569             raise TypeError(
570                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
571             )
572 
573         if dim is not None:
574             if name is not None:
575                 raise TypeError("cannot supply both dim and name arguments")
576             result = self._to_dataset_split(dim)
577         else:
578             result = self._to_dataset_whole(name)
579 
580         if promote_attrs:
581             result.attrs = dict(self.attrs)
582 
583         return result
584 
585     @property
586     def name(self) -> Optional[Hashable]:
587         """The name of this array."""
588         return self._name
589 
590     @name.setter
591     def name(self, value: Optional[Hashable]) -> None:
592         self._name = value
593 
594     @property
595     def variable(self) -> Variable:
596         """Low level interface to the Variable object for this DataArray."""
597         return self._variable
598 
599     @property
600     def dtype(self) -> np.dtype:
601         return self.variable.dtype
602 
603     @property
604     def shape(self) -> Tuple[int, ...]:
605         return self.variable.shape
606 
607     @property
608     def size(self) -> int:
609         return self.variable.size
610 
611     @property
612     def nbytes(self) -> int:
613         return self.variable.nbytes
614 
615     @property
616     def ndim(self) -> int:
617         return self.variable.ndim
618 
619     def __len__(self) -> int:
620         return len(self.variable)
621 
622     @property
623     def data(self) -> Any:
624         """The array's data as a dask or numpy array"""
625         return self.variable.data
626 
627     @data.setter
628     def data(self, value: Any) -> None:
629         self.variable.data = value
630 
631     @property
632     def values(self) -> np.ndarray:
633         """The array's data as a numpy.ndarray"""
634         return self.variable.values
635 
636     @values.setter
637     def values(self, value: Any) -> None:
638         self.variable.values = value
639 
640     @property
641     def _in_memory(self) -> bool:
642         return self.variable._in_memory
643 
644     def to_index(self) -> pd.Index:
645         """Convert this variable to a pandas.Index. Only possible for 1D
646         arrays.
647         """
648         return self.variable.to_index()
649 
650     @property
651     def dims(self) -> Tuple[Hashable, ...]:
652         """Tuple of dimension names associated with this array.
653 
654         Note that the type of this property is inconsistent with
655         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
656         consistently named properties.
657         """
658         return self.variable.dims
659 
660     @dims.setter
661     def dims(self, value):
662         raise AttributeError(
663             "you cannot assign dims on a DataArray. Use "
664             ".rename() or .swap_dims() instead."
665         )
666 
667     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
668         if utils.is_dict_like(key):
669             return key
670         else:
671             key = indexing.expanded_indexer(key, self.ndim)
672             return dict(zip(self.dims, key))
673 
674     @property
675     def _level_coords(self) -> Dict[Hashable, Hashable]:
676         """Return a mapping of all MultiIndex levels and their corresponding
677         coordinate name.
678         """
679         level_coords: Dict[Hashable, Hashable] = {}
680 
681         for cname, var in self._coords.items():
682             if var.ndim == 1 and isinstance(var, IndexVariable):
683                 level_names = var.level_names
684                 if level_names is not None:
685                     (dim,) = var.dims
686                     level_coords.update({lname: dim for lname in level_names})
687         return level_coords
688 
689     def _getitem_coord(self, key):
690         from .dataset import _get_virtual_variable
691 
692         try:
693             var = self._coords[key]
694         except KeyError:
695             dim_sizes = dict(zip(self.dims, self.shape))
696             _, key, var = _get_virtual_variable(
697                 self._coords, key, self._level_coords, dim_sizes
698             )
699 
700         return self._replace_maybe_drop_dims(var, name=key)
701 
702     def __getitem__(self, key: Any) -> "DataArray":
703         if isinstance(key, str):
704             return self._getitem_coord(key)
705         else:
706             # xarray-style array indexing
707             return self.isel(indexers=self._item_key_to_dict(key))
708 
709     def __setitem__(self, key: Any, value: Any) -> None:
710         if isinstance(key, str):
711             self.coords[key] = value
712         else:
713             # Coordinates in key, value and self[key] should be consistent.
714             # TODO Coordinate consistency in key is checked here, but it
715             # causes unnecessary indexing. It should be optimized.
716             obj = self[key]
717             if isinstance(value, DataArray):
718                 assert_coordinate_consistent(value, obj.coords.variables)
719             # DataArray key -> Variable key
720             key = {
721                 k: v.variable if isinstance(v, DataArray) else v
722                 for k, v in self._item_key_to_dict(key).items()
723             }
724             self.variable[key] = value
725 
726     def __delitem__(self, key: Any) -> None:
727         del self.coords[key]
728 
729     @property
730     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
731         """Places to look-up items for attribute-style access"""
732         yield from self._item_sources
733         yield self.attrs
734 
735     @property
736     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
737         """Places to look-up items for key-completion"""
738         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
739 
740         # virtual coordinates
741         # uses empty dict -- everything here can already be found in self.coords.
742         yield HybridMappingProxy(keys=self.dims, mapping={})
743         yield HybridMappingProxy(keys=self._level_coords, mapping={})
744 
745     def __contains__(self, key: Any) -> bool:
746         return key in self.data
747 
748     @property
749     def loc(self) -> _LocIndexer:
750         """Attribute for location based indexing like pandas."""
751         return _LocIndexer(self)
752 
753     @property
754     def attrs(self) -> Dict[Hashable, Any]:
755         """Dictionary storing arbitrary metadata with this array."""
756         return self.variable.attrs
757 
758     @attrs.setter
759     def attrs(self, value: Mapping[Hashable, Any]) -> None:
760         # Disable type checking to work around mypy bug - see mypy#4167
761         self.variable.attrs = value  # type: ignore
762 
763     @property
764     def encoding(self) -> Dict[Hashable, Any]:
765         """Dictionary of format-specific settings for how this array should be
766         serialized."""
767         return self.variable.encoding
768 
769     @encoding.setter
770     def encoding(self, value: Mapping[Hashable, Any]) -> None:
771         self.variable.encoding = value
772 
773     @property
774     def indexes(self) -> Indexes:
775         """Mapping of pandas.Index objects used for label based indexing"""
776         if self._indexes is None:
777             self._indexes = default_indexes(self._coords, self.dims)
778         return Indexes(self._indexes)
779 
780     @property
781     def coords(self) -> DataArrayCoordinates:
782         """Dictionary-like container of coordinate arrays."""
783         return DataArrayCoordinates(self)
784 
785     def reset_coords(
786         self,
787         names: Union[Iterable[Hashable], Hashable, None] = None,
788         drop: bool = False,
789     ) -> Union[None, "DataArray", Dataset]:
790         """Given names of coordinates, reset them to become variables.
791 
792         Parameters
793         ----------
794         names : hashable or iterable of hashable, optional
795             Name(s) of non-index coordinates in this dataset to reset into
796             variables. By default, all non-index coordinates are reset.
797         drop : bool, optional
798             If True, remove coordinates instead of converting them into
799             variables.
800 
801         Returns
802         -------
803         Dataset, or DataArray if ``drop == True``
804         """
805         if names is None:
806             names = set(self.coords) - set(self.dims)
807         dataset = self.coords.to_dataset().reset_coords(names, drop)
808         if drop:
809             return self._replace(coords=dataset._variables)
810         else:
811             if self.name is None:
812                 raise ValueError(
813                     "cannot reset_coords with drop=False on an unnamed DataArrray"
814                 )
815             dataset[self.name] = self.variable
816             return dataset
817 
818     def __dask_tokenize__(self):
819         from dask.base import normalize_token
820 
821         return normalize_token((type(self), self._variable, self._coords, self._name))
822 
823     def __dask_graph__(self):
824         return self._to_temp_dataset().__dask_graph__()
825 
826     def __dask_keys__(self):
827         return self._to_temp_dataset().__dask_keys__()
828 
829     def __dask_layers__(self):
830         return self._to_temp_dataset().__dask_layers__()
831 
832     @property
833     def __dask_optimize__(self):
834         return self._to_temp_dataset().__dask_optimize__
835 
836     @property
837     def __dask_scheduler__(self):
838         return self._to_temp_dataset().__dask_scheduler__
839 
840     def __dask_postcompute__(self):
841         func, args = self._to_temp_dataset().__dask_postcompute__()
842         return self._dask_finalize, (func, args, self.name)
843 
844     def __dask_postpersist__(self):
845         func, args = self._to_temp_dataset().__dask_postpersist__()
846         return self._dask_finalize, (func, args, self.name)
847 
848     @staticmethod
849     def _dask_finalize(results, func, args, name):
850         ds = func(results, *args)
851         variable = ds._variables.pop(_THIS_ARRAY)
852         coords = ds._variables
853         return DataArray(variable, coords, name=name, fastpath=True)
854 
855     def load(self, **kwargs) -> "DataArray":
856         """Manually trigger loading of this array's data from disk or a
857         remote source into memory and return this array.
858 
859         Normally, it should not be necessary to call this method in user code,
860         because all xarray functions should either work on deferred data or
861         load data automatically. However, this method can be necessary when
862         working with many file objects on disk.
863 
864         Parameters
865         ----------
866         **kwargs : dict
867             Additional keyword arguments passed on to ``dask.compute``.
868 
869         See Also
870         --------
871         dask.compute
872         """
873         ds = self._to_temp_dataset().load(**kwargs)
874         new = self._from_temp_dataset(ds)
875         self._variable = new._variable
876         self._coords = new._coords
877         return self
878 
879     def compute(self, **kwargs) -> "DataArray":
880         """Manually trigger loading of this array's data from disk or a
881         remote source into memory and return a new array. The original is
882         left unaltered.
883 
884         Normally, it should not be necessary to call this method in user code,
885         because all xarray functions should either work on deferred data or
886         load data automatically. However, this method can be necessary when
887         working with many file objects on disk.
888 
889         Parameters
890         ----------
891         **kwargs : dict
892             Additional keyword arguments passed on to ``dask.compute``.
893 
894         See Also
895         --------
896         dask.compute
897         """
898         new = self.copy(deep=False)
899         return new.load(**kwargs)
900 
901     def persist(self, **kwargs) -> "DataArray":
902         """Trigger computation in constituent dask arrays
903 
904         This keeps them as dask arrays but encourages them to keep data in
905         memory.  This is particularly useful when on a distributed machine.
906         When on a single machine consider using ``.compute()`` instead.
907 
908         Parameters
909         ----------
910         **kwargs : dict
911             Additional keyword arguments passed on to ``dask.persist``.
912 
913         See Also
914         --------
915         dask.persist
916         """
917         ds = self._to_temp_dataset().persist(**kwargs)
918         return self._from_temp_dataset(ds)
919 
920     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
921         """Returns a copy of this array.
922 
923         If `deep=True`, a deep copy is made of the data array.
924         Otherwise, a shallow copy is made, and the returned data array's
925         values are a new view of this data array's values.
926 
927         Use `data` to create a new object with the same structure as
928         original but entirely new data.
929 
930         Parameters
931         ----------
932         deep : bool, optional
933             Whether the data array and its coordinates are loaded into memory
934             and copied onto the new object. Default is True.
935         data : array_like, optional
936             Data to use in the new object. Must have same shape as original.
937             When `data` is used, `deep` is ignored for all data variables,
938             and only used for coords.
939 
940         Returns
941         -------
942         object : DataArray
943             New object with dimensions, attributes, coordinates, name,
944             encoding, and optionally data copied from original.
945 
946         Examples
947         --------
948         Shallow versus deep copy
949 
950         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
951         >>> array.copy()
952         <xarray.DataArray (x: 3)>
953         array([1, 2, 3])
954         Coordinates:
955           * x        (x) <U1 'a' 'b' 'c'
956         >>> array_0 = array.copy(deep=False)
957         >>> array_0[0] = 7
958         >>> array_0
959         <xarray.DataArray (x: 3)>
960         array([7, 2, 3])
961         Coordinates:
962           * x        (x) <U1 'a' 'b' 'c'
963         >>> array
964         <xarray.DataArray (x: 3)>
965         array([7, 2, 3])
966         Coordinates:
967           * x        (x) <U1 'a' 'b' 'c'
968 
969         Changing the data using the ``data`` argument maintains the
970         structure of the original object, but with the new data. Original
971         object is unaffected.
972 
973         >>> array.copy(data=[0.1, 0.2, 0.3])
974         <xarray.DataArray (x: 3)>
975         array([0.1, 0.2, 0.3])
976         Coordinates:
977           * x        (x) <U1 'a' 'b' 'c'
978         >>> array
979         <xarray.DataArray (x: 3)>
980         array([7, 2, 3])
981         Coordinates:
982           * x        (x) <U1 'a' 'b' 'c'
983 
984         See Also
985         --------
986         pandas.DataFrame.copy
987         """
988         variable = self.variable.copy(deep=deep, data=data)
989         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
990         if self._indexes is None:
991             indexes = self._indexes
992         else:
993             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
994         return self._replace(variable, coords, indexes=indexes)
995 
996     def __copy__(self) -> "DataArray":
997         return self.copy(deep=False)
998 
999     def __deepcopy__(self, memo=None) -> "DataArray":
1000         # memo does nothing but is required for compatibility with
1001         # copy.deepcopy
1002         return self.copy(deep=True)
1003 
1004     # mutable objects should not be hashable
1005     # https://github.com/python/mypy/issues/4266
1006     __hash__ = None  # type: ignore
1007 
1008     @property
1009     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1010         """Block dimensions for this array's data or None if it's not a dask
1011         array.
1012         """
1013         return self.variable.chunks
1014 
1015     def chunk(
1016         self,
1017         chunks: Union[
1018             Number,
1019             Tuple[Number, ...],
1020             Tuple[Tuple[Number, ...], ...],
1021             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
1022         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1023         name_prefix: str = "xarray-",
1024         token: str = None,
1025         lock: bool = False,
1026     ) -> "DataArray":
1027         """Coerce this array's data into a dask arrays with the given chunks.
1028 
1029         If this variable is a non-dask array, it will be converted to dask
1030         array. If it's a dask array, it will be rechunked to the given chunk
1031         sizes.
1032 
1033         If neither chunks is not provided for one or more dimensions, chunk
1034         sizes along that dimension will not be updated; non-dask arrays will be
1035         converted into dask arrays with a single block.
1036 
1037         Parameters
1038         ----------
1039         chunks : int, tuple of int or mapping of hashable to int, optional
1040             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1041             ``{'x': 5, 'y': 5}``.
1042         name_prefix : str, optional
1043             Prefix for the name of the new dask array.
1044         token : str, optional
1045             Token uniquely identifying this array.
1046         lock : optional
1047             Passed on to :py:func:`dask.array.from_array`, if the array is not
1048             already as dask array.
1049 
1050         Returns
1051         -------
1052         chunked : xarray.DataArray
1053         """
1054         if isinstance(chunks, (tuple, list)):
1055             chunks = dict(zip(self.dims, chunks))
1056 
1057         ds = self._to_temp_dataset().chunk(
1058             chunks, name_prefix=name_prefix, token=token, lock=lock
1059         )
1060         return self._from_temp_dataset(ds)
1061 
1062     def isel(
1063         self,
1064         indexers: Mapping[Hashable, Any] = None,
1065         drop: bool = False,
1066         missing_dims: str = "raise",
1067         **indexers_kwargs: Any,
1068     ) -> "DataArray":
1069         """Return a new DataArray whose data is given by integer indexing
1070         along the specified dimension(s).
1071 
1072         Parameters
1073         ----------
1074         indexers : dict, optional
1075             A dict with keys matching dimensions and values given
1076             by integers, slice objects or arrays.
1077             indexer can be a integer, slice, array-like or DataArray.
1078             If DataArrays are passed as indexers, xarray-style indexing will be
1079             carried out. See :ref:`indexing` for the details.
1080             One of indexers or indexers_kwargs must be provided.
1081         drop : bool, optional
1082             If ``drop=True``, drop coordinates variables indexed by integers
1083             instead of making them scalar.
1084         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1085             What to do if dimensions that should be selected from are not present in the
1086             DataArray:
1087             - "raise": raise an exception
1088             - "warning": raise a warning, and ignore the missing dimensions
1089             - "ignore": ignore the missing dimensions
1090         **indexers_kwargs : {dim: indexer, ...}, optional
1091             The keyword arguments form of ``indexers``.
1092 
1093         See Also
1094         --------
1095         Dataset.isel
1096         DataArray.sel
1097 
1098         Examples
1099         --------
1100         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1101         >>> da
1102         <xarray.DataArray (x: 5, y: 5)>
1103         array([[ 0,  1,  2,  3,  4],
1104                [ 5,  6,  7,  8,  9],
1105                [10, 11, 12, 13, 14],
1106                [15, 16, 17, 18, 19],
1107                [20, 21, 22, 23, 24]])
1108         Dimensions without coordinates: x, y
1109 
1110         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1111         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1112         >>> da = da.isel(x=tgt_x, y=tgt_y)
1113         >>> da
1114         <xarray.DataArray (points: 5)>
1115         array([ 0,  6, 12, 18, 24])
1116         Dimensions without coordinates: points
1117         """
1118 
1119         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1120 
1121         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1122             ds = self._to_temp_dataset()._isel_fancy(
1123                 indexers, drop=drop, missing_dims=missing_dims
1124             )
1125             return self._from_temp_dataset(ds)
1126 
1127         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1128         # lists, or zero or one-dimensional np.ndarray's
1129 
1130         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1131 
1132         coords = {}
1133         for coord_name, coord_value in self._coords.items():
1134             coord_indexers = {
1135                 k: v for k, v in indexers.items() if k in coord_value.dims
1136             }
1137             if coord_indexers:
1138                 coord_value = coord_value.isel(coord_indexers)
1139                 if drop and coord_value.ndim == 0:
1140                     continue
1141             coords[coord_name] = coord_value
1142 
1143         return self._replace(variable=variable, coords=coords)
1144 
1145     def sel(
1146         self,
1147         indexers: Mapping[Hashable, Any] = None,
1148         method: str = None,
1149         tolerance=None,
1150         drop: bool = False,
1151         **indexers_kwargs: Any,
1152     ) -> "DataArray":
1153         """Return a new DataArray whose data is given by selecting index
1154         labels along the specified dimension(s).
1155 
1156         In contrast to `DataArray.isel`, indexers for this method should use
1157         labels instead of integers.
1158 
1159         Under the hood, this method is powered by using pandas's powerful Index
1160         objects. This makes label based indexing essentially just as fast as
1161         using integer indexing.
1162 
1163         It also means this method uses pandas's (well documented) logic for
1164         indexing. This means you can use string shortcuts for datetime indexes
1165         (e.g., '2000-01' to select all values in January 2000). It also means
1166         that slices are treated as inclusive of both the start and stop values,
1167         unlike normal Python indexing.
1168 
1169         .. warning::
1170 
1171           Do not try to assign values when using any of the indexing methods
1172           ``isel`` or ``sel``::
1173 
1174             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1175             # DO NOT do this
1176             da.isel(x=[0, 1, 2])[1] = -1
1177 
1178           Assigning values with the chained indexing using ``.sel`` or
1179           ``.isel`` fails silently.
1180 
1181         Parameters
1182         ----------
1183         indexers : dict, optional
1184             A dict with keys matching dimensions and values given
1185             by scalars, slices or arrays of tick labels. For dimensions with
1186             multi-index, the indexer may also be a dict-like object with keys
1187             matching index level names.
1188             If DataArrays are passed as indexers, xarray-style indexing will be
1189             carried out. See :ref:`indexing` for the details.
1190             One of indexers or indexers_kwargs must be provided.
1191         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1192             Method to use for inexact matches:
1193 
1194             * None (default): only exact matches
1195             * pad / ffill: propagate last valid index value forward
1196             * backfill / bfill: propagate next valid index value backward
1197             * nearest: use nearest valid index value
1198         tolerance : optional
1199             Maximum distance between original and new labels for inexact
1200             matches. The values of the index at the matching locations must
1201             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1202         drop : bool, optional
1203             If ``drop=True``, drop coordinates variables in `indexers` instead
1204             of making them scalar.
1205         **indexers_kwargs : {dim: indexer, ...}, optional
1206             The keyword arguments form of ``indexers``.
1207             One of indexers or indexers_kwargs must be provided.
1208 
1209         Returns
1210         -------
1211         obj : DataArray
1212             A new DataArray with the same contents as this DataArray, except the
1213             data and each dimension is indexed by the appropriate indexers.
1214             If indexer DataArrays have coordinates that do not conflict with
1215             this object, then these coordinates will be attached.
1216             In general, each array's data will be a view of the array's data
1217             in this DataArray, unless vectorized indexing was triggered by using
1218             an array indexer, in which case the data will be a copy.
1219 
1220         See Also
1221         --------
1222         Dataset.sel
1223         DataArray.isel
1224 
1225         Examples
1226         --------
1227         >>> da = xr.DataArray(
1228         ...     np.arange(25).reshape(5, 5),
1229         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1230         ...     dims=("x", "y"),
1231         ... )
1232         >>> da
1233         <xarray.DataArray (x: 5, y: 5)>
1234         array([[ 0,  1,  2,  3,  4],
1235                [ 5,  6,  7,  8,  9],
1236                [10, 11, 12, 13, 14],
1237                [15, 16, 17, 18, 19],
1238                [20, 21, 22, 23, 24]])
1239         Coordinates:
1240           * x        (x) int64 0 1 2 3 4
1241           * y        (y) int64 0 1 2 3 4
1242 
1243         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1244         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1245         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1246         >>> da
1247         <xarray.DataArray (points: 5)>
1248         array([ 0,  6, 12, 18, 24])
1249         Coordinates:
1250             x        (points) int64 0 1 2 3 4
1251             y        (points) int64 0 1 2 3 4
1252         Dimensions without coordinates: points
1253         """
1254         ds = self._to_temp_dataset().sel(
1255             indexers=indexers,
1256             drop=drop,
1257             method=method,
1258             tolerance=tolerance,
1259             **indexers_kwargs,
1260         )
1261         return self._from_temp_dataset(ds)
1262 
1263     def head(
1264         self,
1265         indexers: Union[Mapping[Hashable, int], int] = None,
1266         **indexers_kwargs: Any,
1267     ) -> "DataArray":
1268         """Return a new DataArray whose data is given by the the first `n`
1269         values along the specified dimension(s). Default `n` = 5
1270 
1271         See Also
1272         --------
1273         Dataset.head
1274         DataArray.tail
1275         DataArray.thin
1276         """
1277         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1278         return self._from_temp_dataset(ds)
1279 
1280     def tail(
1281         self,
1282         indexers: Union[Mapping[Hashable, int], int] = None,
1283         **indexers_kwargs: Any,
1284     ) -> "DataArray":
1285         """Return a new DataArray whose data is given by the the last `n`
1286         values along the specified dimension(s). Default `n` = 5
1287 
1288         See Also
1289         --------
1290         Dataset.tail
1291         DataArray.head
1292         DataArray.thin
1293         """
1294         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1295         return self._from_temp_dataset(ds)
1296 
1297     def thin(
1298         self,
1299         indexers: Union[Mapping[Hashable, int], int] = None,
1300         **indexers_kwargs: Any,
1301     ) -> "DataArray":
1302         """Return a new DataArray whose data is given by each `n` value
1303         along the specified dimension(s).
1304 
1305         See Also
1306         --------
1307         Dataset.thin
1308         DataArray.head
1309         DataArray.tail
1310         """
1311         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1312         return self._from_temp_dataset(ds)
1313 
1314     def broadcast_like(
1315         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1316     ) -> "DataArray":
1317         """Broadcast this DataArray against another Dataset or DataArray.
1318 
1319         This is equivalent to xr.broadcast(other, self)[1]
1320 
1321         xarray objects are broadcast against each other in arithmetic
1322         operations, so this method is not be necessary for most uses.
1323 
1324         If no change is needed, the input data is returned to the output
1325         without being copied.
1326 
1327         If new coords are added by the broadcast, their values are
1328         NaN filled.
1329 
1330         Parameters
1331         ----------
1332         other : Dataset or DataArray
1333             Object against which to broadcast this array.
1334         exclude : iterable of hashable, optional
1335             Dimensions that must not be broadcasted
1336 
1337         Returns
1338         -------
1339         new_da : DataArray
1340             The caller broadcasted against ``other``.
1341 
1342         Examples
1343         --------
1344         >>> arr1 = xr.DataArray(
1345         ...     np.random.randn(2, 3),
1346         ...     dims=("x", "y"),
1347         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1348         ... )
1349         >>> arr2 = xr.DataArray(
1350         ...     np.random.randn(3, 2),
1351         ...     dims=("x", "y"),
1352         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1353         ... )
1354         >>> arr1
1355         <xarray.DataArray (x: 2, y: 3)>
1356         array([[ 1.76405235,  0.40015721,  0.97873798],
1357                [ 2.2408932 ,  1.86755799, -0.97727788]])
1358         Coordinates:
1359           * x        (x) <U1 'a' 'b'
1360           * y        (y) <U1 'a' 'b' 'c'
1361         >>> arr2
1362         <xarray.DataArray (x: 3, y: 2)>
1363         array([[ 0.95008842, -0.15135721],
1364                [-0.10321885,  0.4105985 ],
1365                [ 0.14404357,  1.45427351]])
1366         Coordinates:
1367           * x        (x) <U1 'a' 'b' 'c'
1368           * y        (y) <U1 'a' 'b'
1369         >>> arr1.broadcast_like(arr2)
1370         <xarray.DataArray (x: 3, y: 3)>
1371         array([[ 1.76405235,  0.40015721,  0.97873798],
1372                [ 2.2408932 ,  1.86755799, -0.97727788],
1373                [        nan,         nan,         nan]])
1374         Coordinates:
1375           * x        (x) <U1 'a' 'b' 'c'
1376           * y        (y) <U1 'a' 'b' 'c'
1377         """
1378         if exclude is None:
1379             exclude = set()
1380         else:
1381             exclude = set(exclude)
1382         args = align(other, self, join="outer", copy=False, exclude=exclude)
1383 
1384         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1385 
1386         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1387 
1388     def reindex_like(
1389         self,
1390         other: Union["DataArray", Dataset],
1391         method: str = None,
1392         tolerance=None,
1393         copy: bool = True,
1394         fill_value=dtypes.NA,
1395     ) -> "DataArray":
1396         """Conform this object onto the indexes of another object, filling in
1397         missing values with ``fill_value``. The default fill value is NaN.
1398 
1399         Parameters
1400         ----------
1401         other : Dataset or DataArray
1402             Object with an 'indexes' attribute giving a mapping from dimension
1403             names to pandas.Index objects, which provides coordinates upon
1404             which to index the variables in this dataset. The indexes on this
1405             other object need not be the same as the indexes on this
1406             dataset. Any mis-matched index values will be filled in with
1407             NaN, and any mis-matched dimension names will simply be ignored.
1408         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1409             Method to use for filling index values from other not found on this
1410             data array:
1411 
1412             * None (default): don't fill gaps
1413             * pad / ffill: propagate last valid index value forward
1414             * backfill / bfill: propagate next valid index value backward
1415             * nearest: use nearest valid index value
1416         tolerance : optional
1417             Maximum distance between original and new labels for inexact
1418             matches. The values of the index at the matching locations must
1419             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1420         copy : bool, optional
1421             If ``copy=True``, data in the return value is always copied. If
1422             ``copy=False`` and reindexing is unnecessary, or can be performed
1423             with only slice operations, then the output may share memory with
1424             the input. In either case, a new xarray object is always returned.
1425         fill_value : scalar or dict-like, optional
1426             Value to use for newly missing values. If a dict-like, maps
1427             variable names (including coordinates) to fill values. Use this
1428             data array's name to refer to the data array's values.
1429 
1430         Returns
1431         -------
1432         reindexed : DataArray
1433             Another dataset array, with this array's data but coordinates from
1434             the other object.
1435 
1436         See Also
1437         --------
1438         DataArray.reindex
1439         align
1440         """
1441         indexers = reindex_like_indexers(self, other)
1442         return self.reindex(
1443             indexers=indexers,
1444             method=method,
1445             tolerance=tolerance,
1446             copy=copy,
1447             fill_value=fill_value,
1448         )
1449 
1450     def reindex(
1451         self,
1452         indexers: Mapping[Hashable, Any] = None,
1453         method: str = None,
1454         tolerance=None,
1455         copy: bool = True,
1456         fill_value=dtypes.NA,
1457         **indexers_kwargs: Any,
1458     ) -> "DataArray":
1459         """Conform this object onto the indexes of another object, filling in
1460         missing values with ``fill_value``. The default fill value is NaN.
1461 
1462         Parameters
1463         ----------
1464         indexers : dict, optional
1465             Dictionary with keys given by dimension names and values given by
1466             arrays of coordinates tick labels. Any mis-matched coordinate
1467             values will be filled in with NaN, and any mis-matched dimension
1468             names will simply be ignored.
1469             One of indexers or indexers_kwargs must be provided.
1470         copy : bool, optional
1471             If ``copy=True``, data in the return value is always copied. If
1472             ``copy=False`` and reindexing is unnecessary, or can be performed
1473             with only slice operations, then the output may share memory with
1474             the input. In either case, a new xarray object is always returned.
1475         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1476             Method to use for filling index values in ``indexers`` not found on
1477             this data array:
1478 
1479             * None (default): don't fill gaps
1480             * pad / ffill: propagate last valid index value forward
1481             * backfill / bfill: propagate next valid index value backward
1482             * nearest: use nearest valid index value
1483         tolerance : optional
1484             Maximum distance between original and new labels for inexact
1485             matches. The values of the index at the matching locations must
1486             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1487         fill_value : scalar or dict-like, optional
1488             Value to use for newly missing values. If a dict-like, maps
1489             variable names (including coordinates) to fill values. Use this
1490             data array's name to refer to the data array's values.
1491         **indexers_kwargs : {dim: indexer, ...}, optional
1492             The keyword arguments form of ``indexers``.
1493             One of indexers or indexers_kwargs must be provided.
1494 
1495         Returns
1496         -------
1497         reindexed : DataArray
1498             Another dataset array, with this array's data but replaced
1499             coordinates.
1500 
1501         See Also
1502         --------
1503         DataArray.reindex_like
1504         align
1505         """
1506         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1507         if isinstance(fill_value, dict):
1508             fill_value = fill_value.copy()
1509             sentinel = object()
1510             value = fill_value.pop(self.name, sentinel)
1511             if value is not sentinel:
1512                 fill_value[_THIS_ARRAY] = value
1513 
1514         ds = self._to_temp_dataset().reindex(
1515             indexers=indexers,
1516             method=method,
1517             tolerance=tolerance,
1518             copy=copy,
1519             fill_value=fill_value,
1520         )
1521         return self._from_temp_dataset(ds)
1522 
1523     def interp(
1524         self,
1525         coords: Mapping[Hashable, Any] = None,
1526         method: str = "linear",
1527         assume_sorted: bool = False,
1528         kwargs: Mapping[str, Any] = None,
1529         **coords_kwargs: Any,
1530     ) -> "DataArray":
1531         """Multidimensional interpolation of variables.
1532 
1533         Parameters
1534         ----------
1535         coords : dict, optional
1536             Mapping from dimension names to the new coordinates.
1537             New coordinate can be an scalar, array-like or DataArray.
1538             If DataArrays are passed as new coordinates, their dimensions are
1539             used for the broadcasting. Missing values are skipped.
1540         method : str, default: "linear"
1541             The method used to interpolate. Choose from
1542 
1543             - {"linear", "nearest"} for multidimensional array,
1544             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1545         assume_sorted : bool, optional
1546             If False, values of x can be in any order and they are sorted
1547             first. If True, x has to be an array of monotonically increasing
1548             values.
1549         kwargs : dict
1550             Additional keyword arguments passed to scipy's interpolator. Valid
1551             options and their behavior depend on if 1-dimensional or
1552             multi-dimensional interpolation is used.
1553         **coords_kwargs : {dim: coordinate, ...}, optional
1554             The keyword arguments form of ``coords``.
1555             One of coords or coords_kwargs must be provided.
1556 
1557         Returns
1558         -------
1559         interpolated : DataArray
1560             New dataarray on the new coordinates.
1561 
1562         Notes
1563         -----
1564         scipy is required.
1565 
1566         See Also
1567         --------
1568         scipy.interpolate.interp1d
1569         scipy.interpolate.interpn
1570 
1571         Examples
1572         --------
1573         >>> da = xr.DataArray(
1574         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1575         ...     dims=("x", "y"),
1576         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1577         ... )
1578         >>> da
1579         <xarray.DataArray (x: 3, y: 4)>
1580         array([[ 1.,  4.,  2.,  9.],
1581                [ 2.,  7.,  6., nan],
1582                [ 6., nan,  5.,  8.]])
1583         Coordinates:
1584           * x        (x) int64 0 1 2
1585           * y        (y) int64 10 12 14 16
1586 
1587         1D linear interpolation (the default):
1588 
1589         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1590         <xarray.DataArray (x: 4, y: 4)>
1591         array([[1.  , 4.  , 2.  ,  nan],
1592                [1.75, 6.25, 5.  ,  nan],
1593                [3.  ,  nan, 5.75,  nan],
1594                [5.  ,  nan, 5.25,  nan]])
1595         Coordinates:
1596           * y        (y) int64 10 12 14 16
1597           * x        (x) float64 0.0 0.75 1.25 1.75
1598 
1599         1D nearest interpolation:
1600 
1601         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1602         <xarray.DataArray (x: 4, y: 4)>
1603         array([[ 1.,  4.,  2.,  9.],
1604                [ 2.,  7.,  6., nan],
1605                [ 2.,  7.,  6., nan],
1606                [ 6., nan,  5.,  8.]])
1607         Coordinates:
1608           * y        (y) int64 10 12 14 16
1609           * x        (x) float64 0.0 0.75 1.25 1.75
1610 
1611         1D linear extrapolation:
1612 
1613         >>> da.interp(
1614         ...     x=[1, 1.5, 2.5, 3.5],
1615         ...     method="linear",
1616         ...     kwargs={"fill_value": "extrapolate"},
1617         ... )
1618         <xarray.DataArray (x: 4, y: 4)>
1619         array([[ 2. ,  7. ,  6. ,  nan],
1620                [ 4. ,  nan,  5.5,  nan],
1621                [ 8. ,  nan,  4.5,  nan],
1622                [12. ,  nan,  3.5,  nan]])
1623         Coordinates:
1624           * y        (y) int64 10 12 14 16
1625           * x        (x) float64 1.0 1.5 2.5 3.5
1626 
1627         2D linear interpolation:
1628 
1629         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1630         <xarray.DataArray (x: 4, y: 3)>
1631         array([[2.5  , 3.   ,   nan],
1632                [4.   , 5.625,   nan],
1633                [  nan,   nan,   nan],
1634                [  nan,   nan,   nan]])
1635         Coordinates:
1636           * x        (x) float64 0.0 0.75 1.25 1.75
1637           * y        (y) int64 11 13 15
1638         """
1639         if self.dtype.kind not in "uifc":
1640             raise TypeError(
1641                 "interp only works for a numeric type array. "
1642                 "Given {}.".format(self.dtype)
1643             )
1644         ds = self._to_temp_dataset().interp(
1645             coords,
1646             method=method,
1647             kwargs=kwargs,
1648             assume_sorted=assume_sorted,
1649             **coords_kwargs,
1650         )
1651         return self._from_temp_dataset(ds)
1652 
1653     def interp_like(
1654         self,
1655         other: Union["DataArray", Dataset],
1656         method: str = "linear",
1657         assume_sorted: bool = False,
1658         kwargs: Mapping[str, Any] = None,
1659     ) -> "DataArray":
1660         """Interpolate this object onto the coordinates of another object,
1661         filling out of range values with NaN.
1662 
1663         Parameters
1664         ----------
1665         other : Dataset or DataArray
1666             Object with an 'indexes' attribute giving a mapping from dimension
1667             names to an 1d array-like, which provides coordinates upon
1668             which to index the variables in this dataset. Missing values are skipped.
1669         method : str, default: "linear"
1670             The method used to interpolate. Choose from
1671 
1672             - {"linear", "nearest"} for multidimensional array,
1673             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1674         assume_sorted : bool, optional
1675             If False, values of coordinates that are interpolated over can be
1676             in any order and they are sorted first. If True, interpolated
1677             coordinates are assumed to be an array of monotonically increasing
1678             values.
1679         kwargs : dict, optional
1680             Additional keyword passed to scipy's interpolator.
1681 
1682         Returns
1683         -------
1684         interpolated : DataArray
1685             Another dataarray by interpolating this dataarray's data along the
1686             coordinates of the other object.
1687 
1688         Notes
1689         -----
1690         scipy is required.
1691         If the dataarray has object-type coordinates, reindex is used for these
1692         coordinates instead of the interpolation.
1693 
1694         See Also
1695         --------
1696         DataArray.interp
1697         DataArray.reindex_like
1698         """
1699         if self.dtype.kind not in "uifc":
1700             raise TypeError(
1701                 "interp only works for a numeric type array. "
1702                 "Given {}.".format(self.dtype)
1703             )
1704         ds = self._to_temp_dataset().interp_like(
1705             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1706         )
1707         return self._from_temp_dataset(ds)
1708 
1709     def rename(
1710         self,
1711         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1712         **names: Hashable,
1713     ) -> "DataArray":
1714         """Returns a new DataArray with renamed coordinates or a new name.
1715 
1716         Parameters
1717         ----------
1718         new_name_or_name_dict : str or dict-like, optional
1719             If the argument is dict-like, it used as a mapping from old
1720             names to new names for coordinates. Otherwise, use the argument
1721             as the new name for this array.
1722         **names : hashable, optional
1723             The keyword arguments form of a mapping from old names to
1724             new names for coordinates.
1725             One of new_name_or_name_dict or names must be provided.
1726 
1727         Returns
1728         -------
1729         renamed : DataArray
1730             Renamed array or array with renamed coordinates.
1731 
1732         See Also
1733         --------
1734         Dataset.rename
1735         DataArray.swap_dims
1736         """
1737         if names or utils.is_dict_like(new_name_or_name_dict):
1738             new_name_or_name_dict = cast(
1739                 Mapping[Hashable, Hashable], new_name_or_name_dict
1740             )
1741             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1742             dataset = self._to_temp_dataset().rename(name_dict)
1743             return self._from_temp_dataset(dataset)
1744         else:
1745             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1746             return self._replace(name=new_name_or_name_dict)
1747 
1748     def swap_dims(
1749         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs
1750     ) -> "DataArray":
1751         """Returns a new DataArray with swapped dimensions.
1752 
1753         Parameters
1754         ----------
1755         dims_dict : dict-like
1756             Dictionary whose keys are current dimension names and whose values
1757             are new names.
1758         **dims_kwargs : {existing_dim: new_dim, ...}, optional
1759             The keyword arguments form of ``dims_dict``.
1760             One of dims_dict or dims_kwargs must be provided.
1761 
1762         Returns
1763         -------
1764         swapped : DataArray
1765             DataArray with swapped dimensions.
1766 
1767         Examples
1768         --------
1769         >>> arr = xr.DataArray(
1770         ...     data=[0, 1],
1771         ...     dims="x",
1772         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1773         ... )
1774         >>> arr
1775         <xarray.DataArray (x: 2)>
1776         array([0, 1])
1777         Coordinates:
1778           * x        (x) <U1 'a' 'b'
1779             y        (x) int64 0 1
1780 
1781         >>> arr.swap_dims({"x": "y"})
1782         <xarray.DataArray (y: 2)>
1783         array([0, 1])
1784         Coordinates:
1785             x        (y) <U1 'a' 'b'
1786           * y        (y) int64 0 1
1787 
1788         >>> arr.swap_dims({"x": "z"})
1789         <xarray.DataArray (z: 2)>
1790         array([0, 1])
1791         Coordinates:
1792             x        (z) <U1 'a' 'b'
1793             y        (z) int64 0 1
1794         Dimensions without coordinates: z
1795 
1796         See Also
1797         --------
1798         DataArray.rename
1799         Dataset.swap_dims
1800         """
1801         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
1802         ds = self._to_temp_dataset().swap_dims(dims_dict)
1803         return self._from_temp_dataset(ds)
1804 
1805     def expand_dims(
1806         self,
1807         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1808         axis=None,
1809         **dim_kwargs: Any,
1810     ) -> "DataArray":
1811         """Return a new object with an additional axis (or axes) inserted at
1812         the corresponding position in the array shape. The new object is a
1813         view into the underlying array, not a copy.
1814 
1815         If dim is already a scalar coordinate, it will be promoted to a 1D
1816         coordinate consisting of a single value.
1817 
1818         Parameters
1819         ----------
1820         dim : hashable, sequence of hashable, dict, or None, optional
1821             Dimensions to include on the new variable.
1822             If provided as str or sequence of str, then dimensions are inserted
1823             with length 1. If provided as a dict, then the keys are the new
1824             dimensions and the values are either integers (giving the length of
1825             the new dimensions) or sequence/ndarray (giving the coordinates of
1826             the new dimensions).
1827         axis : int, list of int or tuple of int, or None, default: None
1828             Axis position(s) where new axis is to be inserted (position(s) on
1829             the result array). If a list (or tuple) of integers is passed,
1830             multiple axes are inserted. In this case, dim arguments should be
1831             same length list. If axis=None is passed, all the axes will be
1832             inserted to the start of the result array.
1833         **dim_kwargs : int or sequence or ndarray
1834             The keywords are arbitrary dimensions being inserted and the values
1835             are either the lengths of the new dims (if int is given), or their
1836             coordinates. Note, this is an alternative to passing a dict to the
1837             dim kwarg and will only be used if dim is None.
1838 
1839         Returns
1840         -------
1841         expanded : same type as caller
1842             This object, but with an additional dimension(s).
1843         """
1844         if isinstance(dim, int):
1845             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1846         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1847             if len(dim) != len(set(dim)):
1848                 raise ValueError("dims should not contain duplicate values.")
1849             dim = dict.fromkeys(dim, 1)
1850         elif dim is not None and not isinstance(dim, Mapping):
1851             dim = {cast(Hashable, dim): 1}
1852 
1853         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1854         ds = self._to_temp_dataset().expand_dims(dim, axis)
1855         return self._from_temp_dataset(ds)
1856 
1857     def set_index(
1858         self,
1859         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1860         append: bool = False,
1861         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1862     ) -> Optional["DataArray"]:
1863         """Set DataArray (multi-)indexes using one or more existing
1864         coordinates.
1865 
1866         Parameters
1867         ----------
1868         indexes : {dim: index, ...}
1869             Mapping from names matching dimensions and values given
1870             by (lists of) the names of existing coordinates or variables to set
1871             as new (multi-)index.
1872         append : bool, optional
1873             If True, append the supplied index(es) to the existing index(es).
1874             Otherwise replace the existing index(es) (default).
1875         **indexes_kwargs : optional
1876             The keyword arguments form of ``indexes``.
1877             One of indexes or indexes_kwargs must be provided.
1878 
1879         Returns
1880         -------
1881         obj : DataArray
1882             Another DataArray, with this data but replaced coordinates.
1883 
1884         Examples
1885         --------
1886         >>> arr = xr.DataArray(
1887         ...     data=np.ones((2, 3)),
1888         ...     dims=["x", "y"],
1889         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1890         ... )
1891         >>> arr
1892         <xarray.DataArray (x: 2, y: 3)>
1893         array([[1., 1., 1.],
1894                [1., 1., 1.]])
1895         Coordinates:
1896           * x        (x) int64 0 1
1897           * y        (y) int64 0 1 2
1898             a        (x) int64 3 4
1899         >>> arr.set_index(x="a")
1900         <xarray.DataArray (x: 2, y: 3)>
1901         array([[1., 1., 1.],
1902                [1., 1., 1.]])
1903         Coordinates:
1904           * x        (x) int64 3 4
1905           * y        (y) int64 0 1 2
1906 
1907         See Also
1908         --------
1909         DataArray.reset_index
1910         """
1911         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
1912         return self._from_temp_dataset(ds)
1913 
1914     def reset_index(
1915         self,
1916         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1917         drop: bool = False,
1918     ) -> Optional["DataArray"]:
1919         """Reset the specified index(es) or multi-index level(s).
1920 
1921         Parameters
1922         ----------
1923         dims_or_levels : hashable or sequence of hashable
1924             Name(s) of the dimension(s) and/or multi-index level(s) that will
1925             be reset.
1926         drop : bool, optional
1927             If True, remove the specified indexes and/or multi-index levels
1928             instead of extracting them as new coordinates (default: False).
1929 
1930         Returns
1931         -------
1932         obj : DataArray
1933             Another dataarray, with this dataarray's data but replaced
1934             coordinates.
1935 
1936         See Also
1937         --------
1938         DataArray.set_index
1939         """
1940         coords, _ = split_indexes(
1941             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1942         )
1943         return self._replace(coords=coords)
1944 
1945     def reorder_levels(
1946         self,
1947         dim_order: Mapping[Hashable, Sequence[int]] = None,
1948         **dim_order_kwargs: Sequence[int],
1949     ) -> "DataArray":
1950         """Rearrange index levels using input order.
1951 
1952         Parameters
1953         ----------
1954         dim_order : optional
1955             Mapping from names matching dimensions and values given
1956             by lists representing new level orders. Every given dimension
1957             must have a multi-index.
1958         **dim_order_kwargs : optional
1959             The keyword arguments form of ``dim_order``.
1960             One of dim_order or dim_order_kwargs must be provided.
1961 
1962         Returns
1963         -------
1964         obj : DataArray
1965             Another dataarray, with this dataarray's data but replaced
1966             coordinates.
1967         """
1968         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1969         replace_coords = {}
1970         for dim, order in dim_order.items():
1971             coord = self._coords[dim]
1972             index = coord.to_index()
1973             if not isinstance(index, pd.MultiIndex):
1974                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1975             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1976         coords = self._coords.copy()
1977         coords.update(replace_coords)
1978         return self._replace(coords=coords)
1979 
1980     def stack(
1981         self,
1982         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1983         **dimensions_kwargs: Sequence[Hashable],
1984     ) -> "DataArray":
1985         """
1986         Stack any number of existing dimensions into a single new dimension.
1987 
1988         New dimensions will be added at the end, and the corresponding
1989         coordinate variables will be combined into a MultiIndex.
1990 
1991         Parameters
1992         ----------
1993         dimensions : mapping of hashable to sequence of hashable
1994             Mapping of the form `new_name=(dim1, dim2, ...)`.
1995             Names of new dimensions, and the existing dimensions that they
1996             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1997             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1998             all dimensions.
1999         **dimensions_kwargs
2000             The keyword arguments form of ``dimensions``.
2001             One of dimensions or dimensions_kwargs must be provided.
2002 
2003         Returns
2004         -------
2005         stacked : DataArray
2006             DataArray with stacked data.
2007 
2008         Examples
2009         --------
2010         >>> arr = xr.DataArray(
2011         ...     np.arange(6).reshape(2, 3),
2012         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2013         ... )
2014         >>> arr
2015         <xarray.DataArray (x: 2, y: 3)>
2016         array([[0, 1, 2],
2017                [3, 4, 5]])
2018         Coordinates:
2019           * x        (x) <U1 'a' 'b'
2020           * y        (y) int64 0 1 2
2021         >>> stacked = arr.stack(z=("x", "y"))
2022         >>> stacked.indexes["z"]
2023         MultiIndex([('a', 0),
2024                     ('a', 1),
2025                     ('a', 2),
2026                     ('b', 0),
2027                     ('b', 1),
2028                     ('b', 2)],
2029                    names=['x', 'y'])
2030 
2031         See Also
2032         --------
2033         DataArray.unstack
2034         """
2035         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
2036         return self._from_temp_dataset(ds)
2037 
2038     def unstack(
2039         self,
2040         dim: Union[Hashable, Sequence[Hashable], None] = None,
2041         fill_value: Any = dtypes.NA,
2042         sparse: bool = False,
2043     ) -> "DataArray":
2044         """
2045         Unstack existing dimensions corresponding to MultiIndexes into
2046         multiple new dimensions.
2047 
2048         New dimensions will be added at the end.
2049 
2050         Parameters
2051         ----------
2052         dim : hashable or sequence of hashable, optional
2053             Dimension(s) over which to unstack. By default unstacks all
2054             MultiIndexes.
2055         fill_value : scalar or dict-like, default: nan
2056             value to be filled. If a dict-like, maps variable names to
2057             fill values. Use the data array's name to refer to its
2058             name. If not provided or if the dict-like does not contain
2059             all variables, the dtype's NA value will be used.
2060         sparse : bool, default: False
2061             use sparse-array if True
2062 
2063         Returns
2064         -------
2065         unstacked : DataArray
2066             Array with unstacked data.
2067 
2068         Examples
2069         --------
2070         >>> arr = xr.DataArray(
2071         ...     np.arange(6).reshape(2, 3),
2072         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2073         ... )
2074         >>> arr
2075         <xarray.DataArray (x: 2, y: 3)>
2076         array([[0, 1, 2],
2077                [3, 4, 5]])
2078         Coordinates:
2079           * x        (x) <U1 'a' 'b'
2080           * y        (y) int64 0 1 2
2081         >>> stacked = arr.stack(z=("x", "y"))
2082         >>> stacked.indexes["z"]
2083         MultiIndex([('a', 0),
2084                     ('a', 1),
2085                     ('a', 2),
2086                     ('b', 0),
2087                     ('b', 1),
2088                     ('b', 2)],
2089                    names=['x', 'y'])
2090         >>> roundtripped = stacked.unstack()
2091         >>> arr.identical(roundtripped)
2092         True
2093 
2094         See Also
2095         --------
2096         DataArray.stack
2097         """
2098         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2099         return self._from_temp_dataset(ds)
2100 
2101     def to_unstacked_dataset(self, dim, level=0):
2102         """Unstack DataArray expanding to Dataset along a given level of a
2103         stacked coordinate.
2104 
2105         This is the inverse operation of Dataset.to_stacked_array.
2106 
2107         Parameters
2108         ----------
2109         dim : str
2110             Name of existing dimension to unstack
2111         level : int or str
2112             The MultiIndex level to expand to a dataset along. Can either be
2113             the integer index of the level or its name.
2114         label : int, default: 0
2115             Label of the level to expand dataset along. Overrides the label
2116             argument if given.
2117 
2118         Returns
2119         -------
2120         unstacked: Dataset
2121 
2122         Examples
2123         --------
2124         >>> import xarray as xr
2125         >>> arr = xr.DataArray(
2126         ...     np.arange(6).reshape(2, 3),
2127         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2128         ... )
2129         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2130         >>> data
2131         <xarray.Dataset>
2132         Dimensions:  (x: 2, y: 3)
2133         Coordinates:
2134           * x        (x) <U1 'a' 'b'
2135           * y        (y) int64 0 1 2
2136         Data variables:
2137             a        (x, y) int64 0 1 2 3 4 5
2138             b        (x) int64 0 3
2139         >>> stacked = data.to_stacked_array("z", ["x"])
2140         >>> stacked.indexes["z"]
2141         MultiIndex([('a', 0.0),
2142                     ('a', 1.0),
2143                     ('a', 2.0),
2144                     ('b', nan)],
2145                    names=['variable', 'y'])
2146         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2147         >>> data.identical(roundtripped)
2148         True
2149 
2150         See Also
2151         --------
2152         Dataset.to_stacked_array
2153         """
2154 
2155         idx = self.indexes[dim]
2156         if not isinstance(idx, pd.MultiIndex):
2157             raise ValueError(f"'{dim}' is not a stacked coordinate")
2158 
2159         level_number = idx._get_level_number(level)
2160         variables = idx.levels[level_number]
2161         variable_dim = idx.names[level_number]
2162 
2163         # pull variables out of datarray
2164         data_dict = {}
2165         for k in variables:
2166             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2167 
2168         # unstacked dataset
2169         return Dataset(data_dict)
2170 
2171     def transpose(
2172         self,
2173         *dims: Hashable,
2174         transpose_coords: bool = True,
2175         missing_dims: str = "raise",
2176     ) -> "DataArray":
2177         """Return a new DataArray object with transposed dimensions.
2178 
2179         Parameters
2180         ----------
2181         *dims : hashable, optional
2182             By default, reverse the dimensions. Otherwise, reorder the
2183             dimensions to this order.
2184         transpose_coords : bool, default: True
2185             If True, also transpose the coordinates of this DataArray.
2186         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2187             What to do if dimensions that should be selected from are not present in the
2188             DataArray:
2189             - "raise": raise an exception
2190             - "warning": raise a warning, and ignore the missing dimensions
2191             - "ignore": ignore the missing dimensions
2192 
2193         Returns
2194         -------
2195         transposed : DataArray
2196             The returned DataArray's array is transposed.
2197 
2198         Notes
2199         -----
2200         This operation returns a view of this array's data. It is
2201         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2202         -- the data will be fully loaded.
2203 
2204         See Also
2205         --------
2206         numpy.transpose
2207         Dataset.transpose
2208         """
2209         if dims:
2210             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2211         variable = self.variable.transpose(*dims)
2212         if transpose_coords:
2213             coords: Dict[Hashable, Variable] = {}
2214             for name, coord in self.coords.items():
2215                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2216                 coords[name] = coord.variable.transpose(*coord_dims)
2217             return self._replace(variable, coords)
2218         else:
2219             return self._replace(variable)
2220 
2221     @property
2222     def T(self) -> "DataArray":
2223         return self.transpose()
2224 
2225     def drop_vars(
2226         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2227     ) -> "DataArray":
2228         """Returns an array with dropped variables.
2229 
2230         Parameters
2231         ----------
2232         names : hashable or iterable of hashable
2233             Name(s) of variables to drop.
2234         errors : {"raise", "ignore"}, optional
2235             If 'raise' (default), raises a ValueError error if any of the variable
2236             passed are not in the dataset. If 'ignore', any given names that are in the
2237             DataArray are dropped and no error is raised.
2238 
2239         Returns
2240         -------
2241         dropped : Dataset
2242             New Dataset copied from `self` with variables removed.
2243         """
2244         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2245         return self._from_temp_dataset(ds)
2246 
2247     def drop(
2248         self,
2249         labels: Mapping = None,
2250         dim: Hashable = None,
2251         *,
2252         errors: str = "raise",
2253         **labels_kwargs,
2254     ) -> "DataArray":
2255         """Backward compatible method based on `drop_vars` and `drop_sel`
2256 
2257         Using either `drop_vars` or `drop_sel` is encouraged
2258 
2259         See Also
2260         --------
2261         DataArray.drop_vars
2262         DataArray.drop_sel
2263         """
2264         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2265         return self._from_temp_dataset(ds)
2266 
2267     def drop_sel(
2268         self,
2269         labels: Mapping[Hashable, Any] = None,
2270         *,
2271         errors: str = "raise",
2272         **labels_kwargs,
2273     ) -> "DataArray":
2274         """Drop index labels from this DataArray.
2275 
2276         Parameters
2277         ----------
2278         labels : mapping of hashable to Any
2279             Index labels to drop
2280         errors : {"raise", "ignore"}, optional
2281             If 'raise' (default), raises a ValueError error if
2282             any of the index labels passed are not
2283             in the dataset. If 'ignore', any given labels that are in the
2284             dataset are dropped and no error is raised.
2285         **labels_kwargs : {dim: label, ...}, optional
2286             The keyword arguments form of ``dim`` and ``labels``
2287 
2288         Returns
2289         -------
2290         dropped : DataArray
2291         """
2292         if labels_kwargs or isinstance(labels, dict):
2293             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2294 
2295         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2296         return self._from_temp_dataset(ds)
2297 
2298     def drop_isel(self, indexers=None, **indexers_kwargs):
2299         """Drop index positions from this DataArray.
2300 
2301         Parameters
2302         ----------
2303         indexers : mapping of hashable to Any
2304             Index locations to drop
2305         **indexers_kwargs : {dim: position, ...}, optional
2306             The keyword arguments form of ``dim`` and ``positions``
2307 
2308         Returns
2309         -------
2310         dropped : DataArray
2311 
2312         Raises
2313         ------
2314         IndexError
2315         """
2316         dataset = self._to_temp_dataset()
2317         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2318         return self._from_temp_dataset(dataset)
2319 
2320     def dropna(
2321         self, dim: Hashable, how: str = "any", thresh: int = None
2322     ) -> "DataArray":
2323         """Returns a new array with dropped labels for missing values along
2324         the provided dimension.
2325 
2326         Parameters
2327         ----------
2328         dim : hashable
2329             Dimension along which to drop missing values. Dropping along
2330             multiple dimensions simultaneously is not yet supported.
2331         how : {"any", "all"}, optional
2332             * any : if any NA values are present, drop that label
2333             * all : if all values are NA, drop that label
2334         thresh : int, default: None
2335             If supplied, require this many non-NA values.
2336 
2337         Returns
2338         -------
2339         DataArray
2340         """
2341         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2342         return self._from_temp_dataset(ds)
2343 
2344     def fillna(self, value: Any) -> "DataArray":
2345         """Fill missing values in this object.
2346 
2347         This operation follows the normal broadcasting and alignment rules that
2348         xarray uses for binary arithmetic, except the result is aligned to this
2349         object (``join='left'``) instead of aligned to the intersection of
2350         index coordinates (``join='inner'``).
2351 
2352         Parameters
2353         ----------
2354         value : scalar, ndarray or DataArray
2355             Used to fill all matching missing values in this array. If the
2356             argument is a DataArray, it is first aligned with (reindexed to)
2357             this array.
2358 
2359         Returns
2360         -------
2361         DataArray
2362         """
2363         if utils.is_dict_like(value):
2364             raise TypeError(
2365                 "cannot provide fill value as a dictionary with "
2366                 "fillna on a DataArray"
2367             )
2368         out = ops.fillna(self, value)
2369         return out
2370 
2371     def interpolate_na(
2372         self,
2373         dim: Hashable = None,
2374         method: str = "linear",
2375         limit: int = None,
2376         use_coordinate: Union[bool, str] = True,
2377         max_gap: Union[
2378             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2379         ] = None,
2380         keep_attrs: bool = None,
2381         **kwargs: Any,
2382     ) -> "DataArray":
2383         """Fill in NaNs by interpolating according to different methods.
2384 
2385         Parameters
2386         ----------
2387         dim : str
2388             Specifies the dimension along which to interpolate.
2389         method : str, optional
2390             String indicating which method to use for interpolation:
2391 
2392             - 'linear': linear interpolation (Default). Additional keyword
2393               arguments are passed to :py:func:`numpy.interp`
2394             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2395               are passed to :py:func:`scipy.interpolate.interp1d`. If
2396               ``method='polynomial'``, the ``order`` keyword argument must also be
2397               provided.
2398             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2399               respective :py:class:`scipy.interpolate` classes.
2400         use_coordinate : bool or str, default: True
2401             Specifies which index to use as the x values in the interpolation
2402             formulated as `y = f(x)`. If False, values are treated as if
2403             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2404             used. If ``use_coordinate`` is a string, it specifies the name of a
2405             coordinate variariable to use as the index.
2406         limit : int, default: None
2407             Maximum number of consecutive NaNs to fill. Must be greater than 0
2408             or None for no limit. This filling is done regardless of the size of
2409             the gap in the data. To only interpolate over gaps less than a given length,
2410             see ``max_gap``.
2411         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2412             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2413             Use None for no limit. When interpolating along a datetime64 dimension
2414             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2415 
2416             - a string that is valid input for pandas.to_timedelta
2417             - a :py:class:`numpy.timedelta64` object
2418             - a :py:class:`pandas.Timedelta` object
2419             - a :py:class:`datetime.timedelta` object
2420 
2421             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2422             dimensions has not been implemented yet. Gap length is defined as the difference
2423             between coordinate values at the first data point after a gap and the last value
2424             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2425             between coordinate values at the first (last) valid data point and the first (last) NaN.
2426             For example, consider::
2427 
2428                 <xarray.DataArray (x: 9)>
2429                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2430                 Coordinates:
2431                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2432 
2433             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2434         keep_attrs : bool, default: True
2435             If True, the dataarray's attributes (`attrs`) will be copied from
2436             the original object to the new one.  If False, the new
2437             object will be returned without attributes.
2438         kwargs : dict, optional
2439             parameters passed verbatim to the underlying interpolation function
2440 
2441         Returns
2442         -------
2443         interpolated: DataArray
2444             Filled in DataArray.
2445 
2446         See Also
2447         --------
2448         numpy.interp
2449         scipy.interpolate
2450 
2451         Examples
2452         --------
2453         >>> da = xr.DataArray(
2454         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2455         ... )
2456         >>> da
2457         <xarray.DataArray (x: 5)>
2458         array([nan,  2.,  3., nan,  0.])
2459         Coordinates:
2460           * x        (x) int64 0 1 2 3 4
2461 
2462         >>> da.interpolate_na(dim="x", method="linear")
2463         <xarray.DataArray (x: 5)>
2464         array([nan, 2. , 3. , 1.5, 0. ])
2465         Coordinates:
2466           * x        (x) int64 0 1 2 3 4
2467 
2468         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2469         <xarray.DataArray (x: 5)>
2470         array([1. , 2. , 3. , 1.5, 0. ])
2471         Coordinates:
2472           * x        (x) int64 0 1 2 3 4
2473         """
2474         from .missing import interp_na
2475 
2476         return interp_na(
2477             self,
2478             dim=dim,
2479             method=method,
2480             limit=limit,
2481             use_coordinate=use_coordinate,
2482             max_gap=max_gap,
2483             keep_attrs=keep_attrs,
2484             **kwargs,
2485         )
2486 
2487     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2488         """Fill NaN values by propogating values forward
2489 
2490         *Requires bottleneck.*
2491 
2492         Parameters
2493         ----------
2494         dim : hashable
2495             Specifies the dimension along which to propagate values when
2496             filling.
2497         limit : int, default: None
2498             The maximum number of consecutive NaN values to forward fill. In
2499             other words, if there is a gap with more than this number of
2500             consecutive NaNs, it will only be partially filled. Must be greater
2501             than 0 or None for no limit.
2502 
2503         Returns
2504         -------
2505         DataArray
2506         """
2507         from .missing import ffill
2508 
2509         return ffill(self, dim, limit=limit)
2510 
2511     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2512         """Fill NaN values by propogating values backward
2513 
2514         *Requires bottleneck.*
2515 
2516         Parameters
2517         ----------
2518         dim : str
2519             Specifies the dimension along which to propagate values when
2520             filling.
2521         limit : int, default: None
2522             The maximum number of consecutive NaN values to backward fill. In
2523             other words, if there is a gap with more than this number of
2524             consecutive NaNs, it will only be partially filled. Must be greater
2525             than 0 or None for no limit.
2526 
2527         Returns
2528         -------
2529         DataArray
2530         """
2531         from .missing import bfill
2532 
2533         return bfill(self, dim, limit=limit)
2534 
2535     def combine_first(self, other: "DataArray") -> "DataArray":
2536         """Combine two DataArray objects, with union of coordinates.
2537 
2538         This operation follows the normal broadcasting and alignment rules of
2539         ``join='outer'``.  Default to non-null values of array calling the
2540         method.  Use np.nan to fill in vacant cells after alignment.
2541 
2542         Parameters
2543         ----------
2544         other : DataArray
2545             Used to fill all matching missing values in this array.
2546 
2547         Returns
2548         -------
2549         DataArray
2550         """
2551         return ops.fillna(self, other, join="outer")
2552 
2553     def reduce(
2554         self,
2555         func: Callable[..., Any],
2556         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2557         axis: Union[None, int, Sequence[int]] = None,
2558         keep_attrs: bool = None,
2559         keepdims: bool = False,
2560         **kwargs: Any,
2561     ) -> "DataArray":
2562         """Reduce this array by applying `func` along some dimension(s).
2563 
2564         Parameters
2565         ----------
2566         func : callable
2567             Function which can be called in the form
2568             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2569             np.ndarray over an integer valued axis.
2570         dim : hashable or sequence of hashable, optional
2571             Dimension(s) over which to apply `func`.
2572         axis : int or sequence of int, optional
2573             Axis(es) over which to repeatedly apply `func`. Only one of the
2574             'dim' and 'axis' arguments can be supplied. If neither are
2575             supplied, then the reduction is calculated over the flattened array
2576             (by calling `f(x)` without an axis argument).
2577         keep_attrs : bool, optional
2578             If True, the variable's attributes (`attrs`) will be copied from
2579             the original object to the new one.  If False (default), the new
2580             object will be returned without attributes.
2581         keepdims : bool, default: False
2582             If True, the dimensions which are reduced are left in the result
2583             as dimensions of size one. Coordinates that use these dimensions
2584             are removed.
2585         **kwargs : dict
2586             Additional keyword arguments passed on to `func`.
2587 
2588         Returns
2589         -------
2590         reduced : DataArray
2591             DataArray with this object's array replaced with an array with
2592             summarized data and the indicated dimension(s) removed.
2593         """
2594 
2595         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2596         return self._replace_maybe_drop_dims(var)
2597 
2598     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2599         """Convert this array into a pandas object with the same shape.
2600 
2601         The type of the returned object depends on the number of DataArray
2602         dimensions:
2603 
2604         * 0D -> `xarray.DataArray`
2605         * 1D -> `pandas.Series`
2606         * 2D -> `pandas.DataFrame`
2607 
2608         Only works for arrays with 2 or fewer dimensions.
2609 
2610         The DataArray constructor performs the inverse transformation.
2611         """
2612         # TODO: consolidate the info about pandas constructors and the
2613         # attributes that correspond to their indexes into a separate module?
2614         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2615         try:
2616             constructor = constructors[self.ndim]
2617         except KeyError:
2618             raise ValueError(
2619                 "cannot convert arrays with %s dimensions into "
2620                 "pandas objects" % self.ndim
2621             )
2622         indexes = [self.get_index(dim) for dim in self.dims]
2623         return constructor(self.values, *indexes)
2624 
2625     def to_dataframe(
2626         self, name: Hashable = None, dim_order: List[Hashable] = None
2627     ) -> pd.DataFrame:
2628         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2629 
2630         The DataFrame is indexed by the Cartesian product of index coordinates
2631         (in the form of a :py:class:`pandas.MultiIndex`).
2632 
2633         Other coordinates are included as columns in the DataFrame.
2634 
2635         Parameters
2636         ----------
2637         name
2638             Name to give to this array (required if unnamed).
2639         dim_order
2640             Hierarchical dimension order for the resulting dataframe.
2641             Array content is transposed to this order and then written out as flat
2642             vectors in contiguous order, so the last dimension in this list
2643             will be contiguous in the resulting DataFrame. This has a major
2644             influence on which operations are efficient on the resulting
2645             dataframe.
2646 
2647             If provided, must include all dimensions of this DataArray. By default,
2648             dimensions are sorted according to the DataArray dimensions order.
2649 
2650         Returns
2651         -------
2652         result
2653             DataArray as a pandas DataFrame.
2654 
2655         """
2656         if name is None:
2657             name = self.name
2658         if name is None:
2659             raise ValueError(
2660                 "cannot convert an unnamed DataArray to a "
2661                 "DataFrame: use the ``name`` parameter"
2662             )
2663         if self.ndim == 0:
2664             raise ValueError("cannot convert a scalar to a DataFrame")
2665 
2666         # By using a unique name, we can convert a DataArray into a DataFrame
2667         # even if it shares a name with one of its coordinates.
2668         # I would normally use unique_name = object() but that results in a
2669         # dataframe with columns in the wrong order, for reasons I have not
2670         # been able to debug (possibly a pandas bug?).
2671         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2672         ds = self._to_dataset_whole(name=unique_name)
2673 
2674         if dim_order is None:
2675             ordered_dims = dict(zip(self.dims, self.shape))
2676         else:
2677             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2678 
2679         df = ds._to_dataframe(ordered_dims)
2680         df.columns = [name if c == unique_name else c for c in df.columns]
2681         return df
2682 
2683     def to_series(self) -> pd.Series:
2684         """Convert this array into a pandas.Series.
2685 
2686         The Series is indexed by the Cartesian product of index coordinates
2687         (in the form of a :py:class:`pandas.MultiIndex`).
2688         """
2689         index = self.coords.to_index()
2690         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2691 
2692     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2693         """Convert this array into a numpy.ma.MaskedArray
2694 
2695         Parameters
2696         ----------
2697         copy : bool, default: True
2698             If True make a copy of the array in the result. If False,
2699             a MaskedArray view of DataArray.values is returned.
2700 
2701         Returns
2702         -------
2703         result : MaskedArray
2704             Masked where invalid values (nan or inf) occur.
2705         """
2706         values = self.values  # only compute lazy arrays once
2707         isnull = pd.isnull(values)
2708         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2709 
2710     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2711         """Write DataArray contents to a netCDF file.
2712 
2713         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2714 
2715         Notes
2716         -----
2717         Only xarray.Dataset objects can be written to netCDF files, so
2718         the xarray.DataArray is converted to a xarray.Dataset object
2719         containing a single variable. If the DataArray has no name, or if the
2720         name is the same as a coordinate name, then it is given the name
2721         ``"__xarray_dataarray_variable__"``.
2722 
2723         See Also
2724         --------
2725         Dataset.to_netcdf
2726         """
2727         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2728 
2729         if self.name is None:
2730             # If no name is set then use a generic xarray name
2731             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2732         elif self.name in self.coords or self.name in self.dims:
2733             # The name is the same as one of the coords names, which netCDF
2734             # doesn't support, so rename it but keep track of the old name
2735             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2736             dataset.attrs[DATAARRAY_NAME] = self.name
2737         else:
2738             # No problems with the name - so we're fine!
2739             dataset = self.to_dataset()
2740 
2741         return dataset.to_netcdf(*args, **kwargs)
2742 
2743     def to_dict(self, data: bool = True) -> dict:
2744         """
2745         Convert this xarray.DataArray into a dictionary following xarray
2746         naming conventions.
2747 
2748         Converts all variables and attributes to native Python objects.
2749         Useful for converting to json. To avoid datetime incompatibility
2750         use decode_times=False kwarg in xarray.open_dataset.
2751 
2752         Parameters
2753         ----------
2754         data : bool, optional
2755             Whether to include the actual data in the dictionary. When set to
2756             False, returns just the schema.
2757 
2758         See Also
2759         --------
2760         DataArray.from_dict
2761         """
2762         d = self.variable.to_dict(data=data)
2763         d.update({"coords": {}, "name": self.name})
2764         for k in self.coords:
2765             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2766         return d
2767 
2768     @classmethod
2769     def from_dict(cls, d: dict) -> "DataArray":
2770         """
2771         Convert a dictionary into an xarray.DataArray
2772 
2773         Input dict can take several forms:
2774 
2775         .. code:: python
2776 
2777             d = {"dims": ("t"), "data": x}
2778 
2779             d = {
2780                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2781                 "attrs": {"title": "air temperature"},
2782                 "dims": "t",
2783                 "data": x,
2784                 "name": "a",
2785             }
2786 
2787         where "t" is the name of the dimesion, "a" is the name of the array,
2788         and x and t are lists, numpy.arrays, or pandas objects.
2789 
2790         Parameters
2791         ----------
2792         d : dict
2793             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2794 
2795         Returns
2796         -------
2797         obj : xarray.DataArray
2798 
2799         See Also
2800         --------
2801         DataArray.to_dict
2802         Dataset.from_dict
2803         """
2804         coords = None
2805         if "coords" in d:
2806             try:
2807                 coords = {
2808                     k: (v["dims"], v["data"], v.get("attrs"))
2809                     for k, v in d["coords"].items()
2810                 }
2811             except KeyError as e:
2812                 raise ValueError(
2813                     "cannot convert dict when coords are missing the key "
2814                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2815                 )
2816         try:
2817             data = d["data"]
2818         except KeyError:
2819             raise ValueError("cannot convert dict without the key 'data''")
2820         else:
2821             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2822         return obj
2823 
2824     @classmethod
2825     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2826         """Convert a pandas.Series into an xarray.DataArray.
2827 
2828         If the series's index is a MultiIndex, it will be expanded into a
2829         tensor product of one-dimensional coordinates (filling in missing
2830         values with NaN). Thus this operation should be the inverse of the
2831         `to_series` method.
2832 
2833         If sparse=True, creates a sparse array instead of a dense NumPy array.
2834         Requires the pydata/sparse package.
2835 
2836         See Also
2837         --------
2838         xarray.Dataset.from_dataframe
2839         """
2840         temp_name = "__temporary_name"
2841         df = pd.DataFrame({temp_name: series})
2842         ds = Dataset.from_dataframe(df, sparse=sparse)
2843         result = cast(DataArray, ds[temp_name])
2844         result.name = series.name
2845         return result
2846 
2847     def to_cdms2(self) -> "cdms2_Variable":
2848         """Convert this array into a cdms2.Variable"""
2849         from ..convert import to_cdms2
2850 
2851         return to_cdms2(self)
2852 
2853     @classmethod
2854     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2855         """Convert a cdms2.Variable into an xarray.DataArray"""
2856         from ..convert import from_cdms2
2857 
2858         return from_cdms2(variable)
2859 
2860     def to_iris(self) -> "iris_Cube":
2861         """Convert this array into a iris.cube.Cube"""
2862         from ..convert import to_iris
2863 
2864         return to_iris(self)
2865 
2866     @classmethod
2867     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2868         """Convert a iris.cube.Cube into an xarray.DataArray"""
2869         from ..convert import from_iris
2870 
2871         return from_iris(cube)
2872 
2873     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2874         """Helper function for equals, broadcast_equals, and identical"""
2875 
2876         def compat(x, y):
2877             return getattr(x.variable, compat_str)(y.variable)
2878 
2879         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2880             self, other
2881         )
2882 
2883     def broadcast_equals(self, other: "DataArray") -> bool:
2884         """Two DataArrays are broadcast equal if they are equal after
2885         broadcasting them against each other such that they have the same
2886         dimensions.
2887 
2888         See Also
2889         --------
2890         DataArray.equals
2891         DataArray.identical
2892         """
2893         try:
2894             return self._all_compat(other, "broadcast_equals")
2895         except (TypeError, AttributeError):
2896             return False
2897 
2898     def equals(self, other: "DataArray") -> bool:
2899         """True if two DataArrays have the same dimensions, coordinates and
2900         values; otherwise False.
2901 
2902         DataArrays can still be equal (like pandas objects) if they have NaN
2903         values in the same locations.
2904 
2905         This method is necessary because `v1 == v2` for ``DataArray``
2906         does element-wise comparisons (like numpy.ndarrays).
2907 
2908         See Also
2909         --------
2910         DataArray.broadcast_equals
2911         DataArray.identical
2912         """
2913         try:
2914             return self._all_compat(other, "equals")
2915         except (TypeError, AttributeError):
2916             return False
2917 
2918     def identical(self, other: "DataArray") -> bool:
2919         """Like equals, but also checks the array name and attributes, and
2920         attributes on all coordinates.
2921 
2922         See Also
2923         --------
2924         DataArray.broadcast_equals
2925         DataArray.equals
2926         """
2927         try:
2928             return self.name == other.name and self._all_compat(other, "identical")
2929         except (TypeError, AttributeError):
2930             return False
2931 
2932     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2933         # use the same naming heuristics as pandas:
2934         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2935         other_name = getattr(other, "name", _default)
2936         if other_name is _default or other_name == self.name:
2937             return self.name
2938         else:
2939             return None
2940 
2941     def __array_wrap__(self, obj, context=None) -> "DataArray":
2942         new_var = self.variable.__array_wrap__(obj, context)
2943         return self._replace(new_var)
2944 
2945     def __matmul__(self, obj):
2946         return self.dot(obj)
2947 
2948     def __rmatmul__(self, other):
2949         # currently somewhat duplicative, as only other DataArrays are
2950         # compatible with matmul
2951         return computation.dot(other, self)
2952 
2953     @staticmethod
2954     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2955         @functools.wraps(f)
2956         def func(self, *args, **kwargs):
2957             keep_attrs = kwargs.pop("keep_attrs", None)
2958             if keep_attrs is None:
2959                 keep_attrs = _get_keep_attrs(default=True)
2960             with warnings.catch_warnings():
2961                 warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
2962                 warnings.filterwarnings(
2963                     "ignore", r"Mean of empty slice", category=RuntimeWarning
2964                 )
2965                 with np.errstate(all="ignore"):
2966                     da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2967                 if keep_attrs:
2968                     da.attrs = self.attrs
2969                 return da
2970 
2971         return func
2972 
2973     @staticmethod
2974     def _binary_op(
2975         f: Callable[..., Any],
2976         reflexive: bool = False,
2977         join: str = None,  # see xarray.align
2978         **ignored_kwargs,
2979     ) -> Callable[..., "DataArray"]:
2980         @functools.wraps(f)
2981         def func(self, other):
2982             if isinstance(other, (Dataset, groupby.GroupBy)):
2983                 return NotImplemented
2984             if isinstance(other, DataArray):
2985                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2986                 self, other = align(self, other, join=align_type, copy=False)
2987             other_variable = getattr(other, "variable", other)
2988             other_coords = getattr(other, "coords", None)
2989 
2990             variable = (
2991                 f(self.variable, other_variable)
2992                 if not reflexive
2993                 else f(other_variable, self.variable)
2994             )
2995             coords, indexes = self.coords._merge_raw(other_coords)
2996             name = self._result_name(other)
2997 
2998             return self._replace(variable, coords, name, indexes=indexes)
2999 
3000         return func
3001 
3002     @staticmethod
3003     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
3004         @functools.wraps(f)
3005         def func(self, other):
3006             if isinstance(other, groupby.GroupBy):
3007                 raise TypeError(
3008                     "in-place operations between a DataArray and "
3009                     "a grouped object are not permitted"
3010                 )
3011             # n.b. we can't align other to self (with other.reindex_like(self))
3012             # because `other` may be converted into floats, which would cause
3013             # in-place arithmetic to fail unpredictably. Instead, we simply
3014             # don't support automatic alignment with in-place arithmetic.
3015             other_coords = getattr(other, "coords", None)
3016             other_variable = getattr(other, "variable", other)
3017             try:
3018                 with self.coords._merge_inplace(other_coords):
3019                     f(self.variable, other_variable)
3020             except MergeError as exc:
3021                 raise MergeError(
3022                     "Automatic alignment is not supported for in-place operations.\n"
3023                     "Consider aligning the indices manually or using a not-in-place operation.\n"
3024                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3025                 ) from exc
3026             return self
3027 
3028         return func
3029 
3030     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
3031         self.attrs = other.attrs
3032 
3033     plot = utils.UncachedAccessor(_PlotMethods)
3034 
3035     def _title_for_slice(self, truncate: int = 50) -> str:
3036         """
3037         If the dataarray has 1 dimensional coordinates or comes from a slice
3038         we can show that info in the title
3039 
3040         Parameters
3041         ----------
3042         truncate : int, default: 50
3043             maximum number of characters for title
3044 
3045         Returns
3046         -------
3047         title : string
3048             Can be used for plot titles
3049 
3050         """
3051         one_dims = []
3052         for dim, coord in self.coords.items():
3053             if coord.size == 1:
3054                 one_dims.append(
3055                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
3056                 )
3057 
3058         title = ", ".join(one_dims)
3059         if len(title) > truncate:
3060             title = title[: (truncate - 3)] + "..."
3061 
3062         return title
3063 
3064     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
3065         """Calculate the n-th order discrete difference along given axis.
3066 
3067         Parameters
3068         ----------
3069         dim : hashable
3070             Dimension over which to calculate the finite difference.
3071         n : int, optional
3072             The number of times values are differenced.
3073         label : hashable, optional
3074             The new coordinate in dimension ``dim`` will have the
3075             values of either the minuend's or subtrahend's coordinate
3076             for values 'upper' and 'lower', respectively.  Other
3077             values are not supported.
3078 
3079         Returns
3080         -------
3081         difference : same type as caller
3082             The n-th order finite difference of this object.
3083 
3084         Notes
3085         -----
3086         `n` matches numpy's behavior and is different from pandas' first argument named
3087         `periods`.
3088 
3089         Examples
3090         --------
3091         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3092         >>> arr.diff("x")
3093         <xarray.DataArray (x: 3)>
3094         array([0, 1, 0])
3095         Coordinates:
3096           * x        (x) int64 2 3 4
3097         >>> arr.diff("x", 2)
3098         <xarray.DataArray (x: 2)>
3099         array([ 1, -1])
3100         Coordinates:
3101           * x        (x) int64 3 4
3102 
3103         See Also
3104         --------
3105         DataArray.differentiate
3106         """
3107         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3108         return self._from_temp_dataset(ds)
3109 
3110     def shift(
3111         self,
3112         shifts: Mapping[Hashable, int] = None,
3113         fill_value: Any = dtypes.NA,
3114         **shifts_kwargs: int,
3115     ) -> "DataArray":
3116         """Shift this array by an offset along one or more dimensions.
3117 
3118         Only the data is moved; coordinates stay in place. Values shifted from
3119         beyond array bounds are replaced by NaN. This is consistent with the
3120         behavior of ``shift`` in pandas.
3121 
3122         Parameters
3123         ----------
3124         shifts : mapping of hashable to int, optional
3125             Integer offset to shift along each of the given dimensions.
3126             Positive offsets shift to the right; negative offsets shift to the
3127             left.
3128         fill_value : scalar, optional
3129             Value to use for newly missing values
3130         **shifts_kwargs
3131             The keyword arguments form of ``shifts``.
3132             One of shifts or shifts_kwargs must be provided.
3133 
3134         Returns
3135         -------
3136         shifted : DataArray
3137             DataArray with the same coordinates and attributes but shifted
3138             data.
3139 
3140         See Also
3141         --------
3142         roll
3143 
3144         Examples
3145         --------
3146         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3147         >>> arr.shift(x=1)
3148         <xarray.DataArray (x: 3)>
3149         array([nan,  5.,  6.])
3150         Dimensions without coordinates: x
3151         """
3152         variable = self.variable.shift(
3153             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3154         )
3155         return self._replace(variable=variable)
3156 
3157     def roll(
3158         self,
3159         shifts: Mapping[Hashable, int] = None,
3160         roll_coords: bool = None,
3161         **shifts_kwargs: int,
3162     ) -> "DataArray":
3163         """Roll this array by an offset along one or more dimensions.
3164 
3165         Unlike shift, roll may rotate all variables, including coordinates
3166         if specified. The direction of rotation is consistent with
3167         :py:func:`numpy.roll`.
3168 
3169         Parameters
3170         ----------
3171         shifts : mapping of hashable to int, optional
3172             Integer offset to rotate each of the given dimensions.
3173             Positive offsets roll to the right; negative offsets roll to the
3174             left.
3175         roll_coords : bool
3176             Indicates whether to roll the coordinates by the offset
3177             The current default of roll_coords (None, equivalent to True) is
3178             deprecated and will change to False in a future version.
3179             Explicitly pass roll_coords to silence the warning.
3180         **shifts_kwargs
3181             The keyword arguments form of ``shifts``.
3182             One of shifts or shifts_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         rolled : DataArray
3187             DataArray with the same attributes but rolled data and coordinates.
3188 
3189         See Also
3190         --------
3191         shift
3192 
3193         Examples
3194         --------
3195         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3196         >>> arr.roll(x=1)
3197         <xarray.DataArray (x: 3)>
3198         array([7, 5, 6])
3199         Dimensions without coordinates: x
3200         """
3201         ds = self._to_temp_dataset().roll(
3202             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3203         )
3204         return self._from_temp_dataset(ds)
3205 
3206     @property
3207     def real(self) -> "DataArray":
3208         return self._replace(self.variable.real)
3209 
3210     @property
3211     def imag(self) -> "DataArray":
3212         return self._replace(self.variable.imag)
3213 
3214     def dot(
3215         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3216     ) -> "DataArray":
3217         """Perform dot product of two DataArrays along their shared dims.
3218 
3219         Equivalent to taking taking tensordot over all shared dims.
3220 
3221         Parameters
3222         ----------
3223         other : DataArray
3224             The other array with which the dot product is performed.
3225         dims : ..., hashable or sequence of hashable, optional
3226             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3227             If not specified, then all the common dimensions are summed over.
3228 
3229         Returns
3230         -------
3231         result : DataArray
3232             Array resulting from the dot product over all shared dimensions.
3233 
3234         See Also
3235         --------
3236         dot
3237         numpy.tensordot
3238 
3239         Examples
3240         --------
3241         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3242         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3243         >>> dm_vals = np.arange(4)
3244         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3245 
3246         >>> dm.dims
3247         ('z',)
3248 
3249         >>> da.dims
3250         ('x', 'y', 'z')
3251 
3252         >>> dot_result = da.dot(dm)
3253         >>> dot_result.dims
3254         ('x', 'y')
3255 
3256         """
3257         if isinstance(other, Dataset):
3258             raise NotImplementedError(
3259                 "dot products are not yet supported with Dataset objects."
3260             )
3261         if not isinstance(other, DataArray):
3262             raise TypeError("dot only operates on DataArrays.")
3263 
3264         return computation.dot(self, other, dims=dims)
3265 
3266     def sortby(
3267         self,
3268         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3269         ascending: bool = True,
3270     ) -> "DataArray":
3271         """Sort object by labels or values (along an axis).
3272 
3273         Sorts the dataarray, either along specified dimensions,
3274         or according to values of 1-D dataarrays that share dimension
3275         with calling object.
3276 
3277         If the input variables are dataarrays, then the dataarrays are aligned
3278         (via left-join) to the calling object prior to sorting by cell values.
3279         NaNs are sorted to the end, following Numpy convention.
3280 
3281         If multiple sorts along the same dimension is
3282         given, numpy's lexsort is performed along that dimension:
3283         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3284         and the FIRST key in the sequence is used as the primary sort key,
3285         followed by the 2nd key, etc.
3286 
3287         Parameters
3288         ----------
3289         variables : hashable, DataArray, or sequence of hashable or DataArray
3290             1D DataArray objects or name(s) of 1D variable(s) in
3291             coords whose values are used to sort this array.
3292         ascending : bool, optional
3293             Whether to sort by ascending or descending order.
3294 
3295         Returns
3296         -------
3297         sorted : DataArray
3298             A new dataarray where all the specified dims are sorted by dim
3299             labels.
3300 
3301         Examples
3302         --------
3303         >>> da = xr.DataArray(
3304         ...     np.random.rand(5),
3305         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3306         ...     dims="time",
3307         ... )
3308         >>> da
3309         <xarray.DataArray (time: 5)>
3310         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3311         Coordinates:
3312           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3313 
3314         >>> da.sortby(da)
3315         <xarray.DataArray (time: 5)>
3316         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3317         Coordinates:
3318           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3319         """
3320         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3321         return self._from_temp_dataset(ds)
3322 
3323     def quantile(
3324         self,
3325         q: Any,
3326         dim: Union[Hashable, Sequence[Hashable], None] = None,
3327         interpolation: str = "linear",
3328         keep_attrs: bool = None,
3329         skipna: bool = True,
3330     ) -> "DataArray":
3331         """Compute the qth quantile of the data along the specified dimension.
3332 
3333         Returns the qth quantiles(s) of the array elements.
3334 
3335         Parameters
3336         ----------
3337         q : float or array-like of float
3338             Quantile to compute, which must be between 0 and 1 inclusive.
3339         dim : hashable or sequence of hashable, optional
3340             Dimension(s) over which to apply quantile.
3341         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3342             This optional parameter specifies the interpolation method to
3343             use when the desired quantile lies between two data points
3344             ``i < j``:
3345 
3346                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3347                   the fractional part of the index surrounded by ``i`` and
3348                   ``j``.
3349                 - lower: ``i``.
3350                 - higher: ``j``.
3351                 - nearest: ``i`` or ``j``, whichever is nearest.
3352                 - midpoint: ``(i + j) / 2``.
3353         keep_attrs : bool, optional
3354             If True, the dataset's attributes (`attrs`) will be copied from
3355             the original object to the new one.  If False (default), the new
3356             object will be returned without attributes.
3357         skipna : bool, optional
3358             Whether to skip missing values when aggregating.
3359 
3360         Returns
3361         -------
3362         quantiles : DataArray
3363             If `q` is a single quantile, then the result
3364             is a scalar. If multiple percentiles are given, first axis of
3365             the result corresponds to the quantile and a quantile dimension
3366             is added to the return array. The other dimensions are the
3367             dimensions that remain after the reduction of the array.
3368 
3369         See Also
3370         --------
3371         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3372 
3373         Examples
3374         --------
3375         >>> da = xr.DataArray(
3376         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3377         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3378         ...     dims=("x", "y"),
3379         ... )
3380         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3381         <xarray.DataArray ()>
3382         array(0.7)
3383         Coordinates:
3384             quantile  float64 0.0
3385         >>> da.quantile(0, dim="x")
3386         <xarray.DataArray (y: 4)>
3387         array([0.7, 4.2, 2.6, 1.5])
3388         Coordinates:
3389           * y         (y) float64 1.0 1.5 2.0 2.5
3390             quantile  float64 0.0
3391         >>> da.quantile([0, 0.5, 1])
3392         <xarray.DataArray (quantile: 3)>
3393         array([0.7, 3.4, 9.4])
3394         Coordinates:
3395           * quantile  (quantile) float64 0.0 0.5 1.0
3396         >>> da.quantile([0, 0.5, 1], dim="x")
3397         <xarray.DataArray (quantile: 3, y: 4)>
3398         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3399                [3.6 , 5.75, 6.  , 1.7 ],
3400                [6.5 , 7.3 , 9.4 , 1.9 ]])
3401         Coordinates:
3402           * y         (y) float64 1.0 1.5 2.0 2.5
3403           * quantile  (quantile) float64 0.0 0.5 1.0
3404         """
3405 
3406         ds = self._to_temp_dataset().quantile(
3407             q,
3408             dim=dim,
3409             keep_attrs=keep_attrs,
3410             interpolation=interpolation,
3411             skipna=skipna,
3412         )
3413         return self._from_temp_dataset(ds)
3414 
3415     def rank(
3416         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3417     ) -> "DataArray":
3418         """Ranks the data.
3419 
3420         Equal values are assigned a rank that is the average of the ranks that
3421         would have been otherwise assigned to all of the values within that
3422         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3423 
3424         NaNs in the input array are returned as NaNs.
3425 
3426         The `bottleneck` library is required.
3427 
3428         Parameters
3429         ----------
3430         dim : hashable
3431             Dimension over which to compute rank.
3432         pct : bool, optional
3433             If True, compute percentage ranks, otherwise compute integer ranks.
3434         keep_attrs : bool, optional
3435             If True, the dataset's attributes (`attrs`) will be copied from
3436             the original object to the new one.  If False (default), the new
3437             object will be returned without attributes.
3438 
3439         Returns
3440         -------
3441         ranked : DataArray
3442             DataArray with the same coordinates and dtype 'float64'.
3443 
3444         Examples
3445         --------
3446         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3447         >>> arr.rank("x")
3448         <xarray.DataArray (x: 3)>
3449         array([1., 2., 3.])
3450         Dimensions without coordinates: x
3451         """
3452 
3453         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3454         return self._from_temp_dataset(ds)
3455 
3456     def differentiate(
3457         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3458     ) -> "DataArray":
3459         """ Differentiate the array with the second order accurate central
3460         differences.
3461 
3462         .. note::
3463             This feature is limited to simple cartesian geometry, i.e. coord
3464             must be one dimensional.
3465 
3466         Parameters
3467         ----------
3468         coord : hashable
3469             The coordinate to be used to compute the gradient.
3470         edge_order : {1, 2}, default: 1
3471             N-th order accurate differences at the boundaries.
3472         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3473                          "us", "ns", "ps", "fs", "as"} or None, optional
3474             Unit to compute gradient. Only valid for datetime coordinate.
3475 
3476         Returns
3477         -------
3478         differentiated: DataArray
3479 
3480         See also
3481         --------
3482         numpy.gradient: corresponding numpy function
3483 
3484         Examples
3485         --------
3486 
3487         >>> da = xr.DataArray(
3488         ...     np.arange(12).reshape(4, 3),
3489         ...     dims=["x", "y"],
3490         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3491         ... )
3492         >>> da
3493         <xarray.DataArray (x: 4, y: 3)>
3494         array([[ 0,  1,  2],
3495                [ 3,  4,  5],
3496                [ 6,  7,  8],
3497                [ 9, 10, 11]])
3498         Coordinates:
3499           * x        (x) float64 0.0 0.1 1.1 1.2
3500         Dimensions without coordinates: y
3501         >>>
3502         >>> da.differentiate("x")
3503         <xarray.DataArray (x: 4, y: 3)>
3504         array([[30.        , 30.        , 30.        ],
3505                [27.54545455, 27.54545455, 27.54545455],
3506                [27.54545455, 27.54545455, 27.54545455],
3507                [30.        , 30.        , 30.        ]])
3508         Coordinates:
3509           * x        (x) float64 0.0 0.1 1.1 1.2
3510         Dimensions without coordinates: y
3511         """
3512         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3513         return self._from_temp_dataset(ds)
3514 
3515     def integrate(
3516         self,
3517         coord: Union[Hashable, Sequence[Hashable]] = None,
3518         datetime_unit: str = None,
3519         *,
3520         dim: Union[Hashable, Sequence[Hashable]] = None,
3521     ) -> "DataArray":
3522         """Integrate along the given coordinate using the trapezoidal rule.
3523 
3524         .. note::
3525             This feature is limited to simple cartesian geometry, i.e. coord
3526             must be one dimensional.
3527 
3528         Parameters
3529         ----------
3530         coord : hashable, or sequence of hashable
3531             Coordinate(s) used for the integration.
3532         dim : hashable, or sequence of hashable
3533             Coordinate(s) used for the integration.
3534         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3535                         'ps', 'fs', 'as'}, optional
3536             Specify the unit if a datetime coordinate is used.
3537 
3538         Returns
3539         -------
3540         integrated : DataArray
3541 
3542         See also
3543         --------
3544         Dataset.integrate
3545         numpy.trapz : corresponding numpy function
3546 
3547         Examples
3548         --------
3549 
3550         >>> da = xr.DataArray(
3551         ...     np.arange(12).reshape(4, 3),
3552         ...     dims=["x", "y"],
3553         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3554         ... )
3555         >>> da
3556         <xarray.DataArray (x: 4, y: 3)>
3557         array([[ 0,  1,  2],
3558                [ 3,  4,  5],
3559                [ 6,  7,  8],
3560                [ 9, 10, 11]])
3561         Coordinates:
3562           * x        (x) float64 0.0 0.1 1.1 1.2
3563         Dimensions without coordinates: y
3564         >>>
3565         >>> da.integrate("x")
3566         <xarray.DataArray (y: 3)>
3567         array([5.4, 6.6, 7.8])
3568         Dimensions without coordinates: y
3569         """
3570         if dim is not None and coord is not None:
3571             raise ValueError(
3572                 "Cannot pass both 'dim' and 'coord'. Please pass only 'coord' instead."
3573             )
3574 
3575         if dim is not None and coord is None:
3576             coord = dim
3577             msg = (
3578                 "The `dim` keyword argument to `DataArray.integrate` is "
3579                 "being replaced with `coord`, for consistency with "
3580                 "`Dataset.integrate`. Please pass `coord` instead."
3581                 " `dim` will be removed in version 0.19.0."
3582             )
3583             warnings.warn(msg, FutureWarning, stacklevel=2)
3584 
3585         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
3586         return self._from_temp_dataset(ds)
3587 
3588     def unify_chunks(self) -> "DataArray":
3589         """Unify chunk size along all chunked dimensions of this DataArray.
3590 
3591         Returns
3592         -------
3593         DataArray with consistent chunk sizes for all dask-array variables
3594 
3595         See Also
3596         --------
3597         dask.array.core.unify_chunks
3598         """
3599         ds = self._to_temp_dataset().unify_chunks()
3600         return self._from_temp_dataset(ds)
3601 
3602     def map_blocks(
3603         self,
3604         func: "Callable[..., T_DSorDA]",
3605         args: Sequence[Any] = (),
3606         kwargs: Mapping[str, Any] = None,
3607         template: Union["DataArray", "Dataset"] = None,
3608     ) -> "T_DSorDA":
3609         """
3610         Apply a function to each block of this DataArray.
3611 
3612         .. warning::
3613             This method is experimental and its signature may change.
3614 
3615         Parameters
3616         ----------
3617         func : callable
3618             User-provided function that accepts a DataArray as its first
3619             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3620             corresponding to one chunk along each chunked dimension. ``func`` will be
3621             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3622 
3623             This function must return either a single DataArray or a single Dataset.
3624 
3625             This function cannot add a new chunked dimension.
3626         args : sequence
3627             Passed to func after unpacking and subsetting any xarray objects by blocks.
3628             xarray objects in args must be aligned with this object, otherwise an error is raised.
3629         kwargs : mapping
3630             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3631             subset to blocks. Passing dask collections in kwargs is not allowed.
3632         template : DataArray or Dataset, optional
3633             xarray object representing the final result after compute is called. If not provided,
3634             the function will be first run on mocked-up data, that looks like this object but
3635             has sizes 0, to determine properties of the returned object such as dtype,
3636             variable names, attributes, new dimensions and new indexes (if any).
3637             ``template`` must be provided if the function changes the size of existing dimensions.
3638             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3639             ``attrs`` set by ``func`` will be ignored.
3640 
3641         Returns
3642         -------
3643         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3644         function.
3645 
3646         Notes
3647         -----
3648         This function is designed for when ``func`` needs to manipulate a whole xarray object
3649         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
3650         recommended to use ``apply_ufunc``.
3651 
3652         If none of the variables in this object is backed by dask arrays, calling this function is
3653         equivalent to calling ``func(obj, *args, **kwargs)``.
3654 
3655         See Also
3656         --------
3657         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
3658         xarray.DataArray.map_blocks
3659 
3660         Examples
3661         --------
3662         Calculate an anomaly from climatology using ``.groupby()``. Using
3663         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3664         its indices, and its methods like ``.groupby()``.
3665 
3666         >>> def calculate_anomaly(da, groupby_type="time.month"):
3667         ...     gb = da.groupby(groupby_type)
3668         ...     clim = gb.mean(dim="time")
3669         ...     return gb - clim
3670         ...
3671         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3672         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3673         >>> np.random.seed(123)
3674         >>> array = xr.DataArray(
3675         ...     np.random.rand(len(time)),
3676         ...     dims=["time"],
3677         ...     coords={"time": time, "month": month},
3678         ... ).chunk()
3679         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3680         <xarray.DataArray (time: 24)>
3681         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3682                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3683                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3684                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3685                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3686         Coordinates:
3687           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3688             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3689 
3690         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3691         to the function being applied in ``xr.map_blocks()``:
3692 
3693         >>> array.map_blocks(
3694         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3695         ... )  # doctest: +ELLIPSIS
3696         <xarray.DataArray (time: 24)>
3697         dask.array<calculate_anomaly-...-<this, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3698         Coordinates:
3699           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3700             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3701         """
3702         from .parallel import map_blocks
3703 
3704         return map_blocks(func, self, args, kwargs, template)
3705 
3706     def polyfit(
3707         self,
3708         dim: Hashable,
3709         deg: int,
3710         skipna: bool = None,
3711         rcond: float = None,
3712         w: Union[Hashable, Any] = None,
3713         full: bool = False,
3714         cov: bool = False,
3715     ):
3716         """
3717         Least squares polynomial fit.
3718 
3719         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3720         invalid values when `skipna = True`.
3721 
3722         Parameters
3723         ----------
3724         dim : hashable
3725             Coordinate along which to fit the polynomials.
3726         deg : int
3727             Degree of the fitting polynomial.
3728         skipna : bool, optional
3729             If True, removes all invalid values before fitting each 1D slices of the array.
3730             Default is True if data is stored in a dask.array or if there is any
3731             invalid values, False otherwise.
3732         rcond : float, optional
3733             Relative condition number to the fit.
3734         w : hashable or array-like, optional
3735             Weights to apply to the y-coordinate of the sample points.
3736             Can be an array-like object or the name of a coordinate in the dataset.
3737         full : bool, optional
3738             Whether to return the residuals, matrix rank and singular values in addition
3739             to the coefficients.
3740         cov : bool or str, optional
3741             Whether to return to the covariance matrix in addition to the coefficients.
3742             The matrix is not scaled if `cov='unscaled'`.
3743 
3744         Returns
3745         -------
3746         polyfit_results : Dataset
3747             A single dataset which contains:
3748 
3749             polyfit_coefficients
3750                 The coefficients of the best fit.
3751             polyfit_residuals
3752                 The residuals of the least-square computation (only included if `full=True`).
3753                 When the matrix rank is deficient, np.nan is returned.
3754             [dim]_matrix_rank
3755                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3756             [dim]_singular_value
3757                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3758             polyfit_covariance
3759                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3760 
3761         See Also
3762         --------
3763         numpy.polyfit
3764         """
3765         return self._to_temp_dataset().polyfit(
3766             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3767         )
3768 
3769     def pad(
3770         self,
3771         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3772         mode: str = "constant",
3773         stat_length: Union[
3774             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3775         ] = None,
3776         constant_values: Union[
3777             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3778         ] = None,
3779         end_values: Union[
3780             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3781         ] = None,
3782         reflect_type: str = None,
3783         **pad_width_kwargs: Any,
3784     ) -> "DataArray":
3785         """Pad this array along one or more dimensions.
3786 
3787         .. warning::
3788             This function is experimental and its behaviour is likely to change
3789             especially regarding padding of dimension coordinates (or IndexVariables).
3790 
3791         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3792         coordinates will be padded with the same mode, otherwise coordinates
3793         are padded using the "constant" mode with fill_value dtypes.NA.
3794 
3795         Parameters
3796         ----------
3797         pad_width : mapping of hashable to tuple of int
3798             Mapping with the form of {dim: (pad_before, pad_after)}
3799             describing the number of values padded along each dimension.
3800             {dim: pad} is a shortcut for pad_before = pad_after = pad
3801         mode : str, default: "constant"
3802             One of the following string values (taken from numpy docs)
3803 
3804             'constant' (default)
3805                 Pads with a constant value.
3806             'edge'
3807                 Pads with the edge values of array.
3808             'linear_ramp'
3809                 Pads with the linear ramp between end_value and the
3810                 array edge value.
3811             'maximum'
3812                 Pads with the maximum value of all or part of the
3813                 vector along each axis.
3814             'mean'
3815                 Pads with the mean value of all or part of the
3816                 vector along each axis.
3817             'median'
3818                 Pads with the median value of all or part of the
3819                 vector along each axis.
3820             'minimum'
3821                 Pads with the minimum value of all or part of the
3822                 vector along each axis.
3823             'reflect'
3824                 Pads with the reflection of the vector mirrored on
3825                 the first and last values of the vector along each
3826                 axis.
3827             'symmetric'
3828                 Pads with the reflection of the vector mirrored
3829                 along the edge of the array.
3830             'wrap'
3831                 Pads with the wrap of the vector along the axis.
3832                 The first values are used to pad the end and the
3833                 end values are used to pad the beginning.
3834         stat_length : int, tuple or mapping of hashable to tuple, default: None
3835             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3836             values at edge of each axis used to calculate the statistic value.
3837             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3838             statistic lengths along each dimension.
3839             ((before, after),) yields same before and after statistic lengths
3840             for each dimension.
3841             (stat_length,) or int is a shortcut for before = after = statistic
3842             length for all axes.
3843             Default is ``None``, to use the entire axis.
3844         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3845             Used in 'constant'.  The values to set the padded values for each
3846             axis.
3847             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3848             pad constants along each dimension.
3849             ``((before, after),)`` yields same before and after constants for each
3850             dimension.
3851             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3852             all dimensions.
3853             Default is 0.
3854         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3855             Used in 'linear_ramp'.  The values used for the ending value of the
3856             linear_ramp and that will form the edge of the padded array.
3857             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3858             end values along each dimension.
3859             ``((before, after),)`` yields same before and after end values for each
3860             axis.
3861             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3862             all axes.
3863             Default is 0.
3864         reflect_type : {"even", "odd"}, optional
3865             Used in "reflect", and "symmetric".  The "even" style is the
3866             default with an unaltered reflection around the edge value.  For
3867             the "odd" style, the extended part of the array is created by
3868             subtracting the reflected values from two times the edge value.
3869         **pad_width_kwargs
3870             The keyword arguments form of ``pad_width``.
3871             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3872 
3873         Returns
3874         -------
3875         padded : DataArray
3876             DataArray with the padded coordinates and data.
3877 
3878         See Also
3879         --------
3880         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3881 
3882         Notes
3883         -----
3884         For ``mode="constant"`` and ``constant_values=None``, integer types will be
3885         promoted to ``float`` and padded with ``np.nan``.
3886 
3887         Examples
3888         --------
3889         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
3890         >>> arr.pad(x=(1, 2), constant_values=0)
3891         <xarray.DataArray (x: 6)>
3892         array([0, 5, 6, 7, 0, 0])
3893         Coordinates:
3894           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3895 
3896         >>> da = xr.DataArray(
3897         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
3898         ...     dims=["x", "y"],
3899         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
3900         ... )
3901         >>> da.pad(x=1)
3902         <xarray.DataArray (x: 4, y: 4)>
3903         array([[nan, nan, nan, nan],
3904                [ 0.,  1.,  2.,  3.],
3905                [10., 11., 12., 13.],
3906                [nan, nan, nan, nan]])
3907         Coordinates:
3908           * x        (x) float64 nan 0.0 1.0 nan
3909           * y        (y) int64 10 20 30 40
3910             z        (x) float64 nan 100.0 200.0 nan
3911 
3912         Careful, ``constant_values`` are coerced to the data type of the array which may
3913         lead to a loss of precision:
3914 
3915         >>> da.pad(x=1, constant_values=1.23456789)
3916         <xarray.DataArray (x: 4, y: 4)>
3917         array([[ 1,  1,  1,  1],
3918                [ 0,  1,  2,  3],
3919                [10, 11, 12, 13],
3920                [ 1,  1,  1,  1]])
3921         Coordinates:
3922           * x        (x) float64 nan 0.0 1.0 nan
3923           * y        (y) int64 10 20 30 40
3924             z        (x) float64 nan 100.0 200.0 nan
3925         """
3926         ds = self._to_temp_dataset().pad(
3927             pad_width=pad_width,
3928             mode=mode,
3929             stat_length=stat_length,
3930             constant_values=constant_values,
3931             end_values=end_values,
3932             reflect_type=reflect_type,
3933             **pad_width_kwargs,
3934         )
3935         return self._from_temp_dataset(ds)
3936 
3937     def idxmin(
3938         self,
3939         dim: Hashable = None,
3940         skipna: bool = None,
3941         fill_value: Any = dtypes.NA,
3942         keep_attrs: bool = None,
3943     ) -> "DataArray":
3944         """Return the coordinate label of the minimum value along a dimension.
3945 
3946         Returns a new `DataArray` named after the dimension with the values of
3947         the coordinate labels along that dimension corresponding to minimum
3948         values along that dimension.
3949 
3950         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3951         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3952 
3953         Parameters
3954         ----------
3955         dim : str, optional
3956             Dimension over which to apply `idxmin`.  This is optional for 1D
3957             arrays, but required for arrays with 2 or more dimensions.
3958         skipna : bool or None, default: None
3959             If True, skip missing values (as marked by NaN). By default, only
3960             skips missing values for ``float``, ``complex``, and ``object``
3961             dtypes; other dtypes either do not have a sentinel missing value
3962             (``int``) or ``skipna=True`` has not been implemented
3963             (``datetime64`` or ``timedelta64``).
3964         fill_value : Any, default: NaN
3965             Value to be filled in case all of the values along a dimension are
3966             null.  By default this is NaN.  The fill value and result are
3967             automatically converted to a compatible dtype if possible.
3968             Ignored if ``skipna`` is False.
3969         keep_attrs : bool, default: False
3970             If True, the attributes (``attrs``) will be copied from the
3971             original object to the new one.  If False (default), the new object
3972             will be returned without attributes.
3973 
3974         Returns
3975         -------
3976         reduced : DataArray
3977             New `DataArray` object with `idxmin` applied to its data and the
3978             indicated dimension removed.
3979 
3980         See Also
3981         --------
3982         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3983 
3984         Examples
3985         --------
3986         >>> array = xr.DataArray(
3987         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3988         ... )
3989         >>> array.min()
3990         <xarray.DataArray ()>
3991         array(-2)
3992         >>> array.argmin()
3993         <xarray.DataArray ()>
3994         array(4)
3995         >>> array.idxmin()
3996         <xarray.DataArray 'x' ()>
3997         array('e', dtype='<U1')
3998 
3999         >>> array = xr.DataArray(
4000         ...     [
4001         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4002         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4003         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4004         ...     ],
4005         ...     dims=["y", "x"],
4006         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4007         ... )
4008         >>> array.min(dim="x")
4009         <xarray.DataArray (y: 3)>
4010         array([-2., -4.,  1.])
4011         Coordinates:
4012           * y        (y) int64 -1 0 1
4013         >>> array.argmin(dim="x")
4014         <xarray.DataArray (y: 3)>
4015         array([4, 0, 2])
4016         Coordinates:
4017           * y        (y) int64 -1 0 1
4018         >>> array.idxmin(dim="x")
4019         <xarray.DataArray 'x' (y: 3)>
4020         array([16.,  0.,  4.])
4021         Coordinates:
4022           * y        (y) int64 -1 0 1
4023         """
4024         return computation._calc_idxminmax(
4025             array=self,
4026             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4027             dim=dim,
4028             skipna=skipna,
4029             fill_value=fill_value,
4030             keep_attrs=keep_attrs,
4031         )
4032 
4033     def idxmax(
4034         self,
4035         dim: Hashable = None,
4036         skipna: bool = None,
4037         fill_value: Any = dtypes.NA,
4038         keep_attrs: bool = None,
4039     ) -> "DataArray":
4040         """Return the coordinate label of the maximum value along a dimension.
4041 
4042         Returns a new `DataArray` named after the dimension with the values of
4043         the coordinate labels along that dimension corresponding to maximum
4044         values along that dimension.
4045 
4046         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4047         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4048 
4049         Parameters
4050         ----------
4051         dim : hashable, optional
4052             Dimension over which to apply `idxmax`.  This is optional for 1D
4053             arrays, but required for arrays with 2 or more dimensions.
4054         skipna : bool or None, default: None
4055             If True, skip missing values (as marked by NaN). By default, only
4056             skips missing values for ``float``, ``complex``, and ``object``
4057             dtypes; other dtypes either do not have a sentinel missing value
4058             (``int``) or ``skipna=True`` has not been implemented
4059             (``datetime64`` or ``timedelta64``).
4060         fill_value : Any, default: NaN
4061             Value to be filled in case all of the values along a dimension are
4062             null.  By default this is NaN.  The fill value and result are
4063             automatically converted to a compatible dtype if possible.
4064             Ignored if ``skipna`` is False.
4065         keep_attrs : bool, default: False
4066             If True, the attributes (``attrs``) will be copied from the
4067             original object to the new one.  If False (default), the new object
4068             will be returned without attributes.
4069 
4070         Returns
4071         -------
4072         reduced : DataArray
4073             New `DataArray` object with `idxmax` applied to its data and the
4074             indicated dimension removed.
4075 
4076         See Also
4077         --------
4078         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4079 
4080         Examples
4081         --------
4082         >>> array = xr.DataArray(
4083         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4084         ... )
4085         >>> array.max()
4086         <xarray.DataArray ()>
4087         array(2)
4088         >>> array.argmax()
4089         <xarray.DataArray ()>
4090         array(1)
4091         >>> array.idxmax()
4092         <xarray.DataArray 'x' ()>
4093         array('b', dtype='<U1')
4094 
4095         >>> array = xr.DataArray(
4096         ...     [
4097         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4098         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4099         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4100         ...     ],
4101         ...     dims=["y", "x"],
4102         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4103         ... )
4104         >>> array.max(dim="x")
4105         <xarray.DataArray (y: 3)>
4106         array([2., 2., 1.])
4107         Coordinates:
4108           * y        (y) int64 -1 0 1
4109         >>> array.argmax(dim="x")
4110         <xarray.DataArray (y: 3)>
4111         array([0, 2, 2])
4112         Coordinates:
4113           * y        (y) int64 -1 0 1
4114         >>> array.idxmax(dim="x")
4115         <xarray.DataArray 'x' (y: 3)>
4116         array([0., 4., 4.])
4117         Coordinates:
4118           * y        (y) int64 -1 0 1
4119         """
4120         return computation._calc_idxminmax(
4121             array=self,
4122             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4123             dim=dim,
4124             skipna=skipna,
4125             fill_value=fill_value,
4126             keep_attrs=keep_attrs,
4127         )
4128 
4129     def argmin(
4130         self,
4131         dim: Union[Hashable, Sequence[Hashable]] = None,
4132         axis: int = None,
4133         keep_attrs: bool = None,
4134         skipna: bool = None,
4135     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4136         """Index or indices of the minimum of the DataArray over one or more dimensions.
4137 
4138         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4139         which can be passed directly to isel(). If a single str is passed to 'dim' then
4140         returns a DataArray with dtype int.
4141 
4142         If there are multiple minima, the indices of the first one found will be
4143         returned.
4144 
4145         Parameters
4146         ----------
4147         dim : hashable, sequence of hashable or ..., optional
4148             The dimensions over which to find the minimum. By default, finds minimum over
4149             all dimensions - for now returning an int for backward compatibility, but
4150             this is deprecated, in future will return a dict with indices for all
4151             dimensions; to return a dict with all dimensions now, pass '...'.
4152         axis : int, optional
4153             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4154             can be supplied.
4155         keep_attrs : bool, optional
4156             If True, the attributes (`attrs`) will be copied from the original
4157             object to the new one.  If False (default), the new object will be
4158             returned without attributes.
4159         skipna : bool, optional
4160             If True, skip missing values (as marked by NaN). By default, only
4161             skips missing values for float dtypes; other dtypes either do not
4162             have a sentinel missing value (int) or skipna=True has not been
4163             implemented (object, datetime64 or timedelta64).
4164 
4165         Returns
4166         -------
4167         result : DataArray or dict of DataArray
4168 
4169         See Also
4170         --------
4171         Variable.argmin, DataArray.idxmin
4172 
4173         Examples
4174         --------
4175         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4176         >>> array.min()
4177         <xarray.DataArray ()>
4178         array(-1)
4179         >>> array.argmin()
4180         <xarray.DataArray ()>
4181         array(2)
4182         >>> array.argmin(...)
4183         {'x': <xarray.DataArray ()>
4184         array(2)}
4185         >>> array.isel(array.argmin(...))
4186         <xarray.DataArray ()>
4187         array(-1)
4188 
4189         >>> array = xr.DataArray(
4190         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4191         ...     dims=("x", "y", "z"),
4192         ... )
4193         >>> array.min(dim="x")
4194         <xarray.DataArray (y: 3, z: 3)>
4195         array([[ 1,  2,  1],
4196                [ 2, -5,  1],
4197                [ 2,  1,  1]])
4198         Dimensions without coordinates: y, z
4199         >>> array.argmin(dim="x")
4200         <xarray.DataArray (y: 3, z: 3)>
4201         array([[1, 0, 0],
4202                [1, 1, 1],
4203                [0, 0, 1]])
4204         Dimensions without coordinates: y, z
4205         >>> array.argmin(dim=["x"])
4206         {'x': <xarray.DataArray (y: 3, z: 3)>
4207         array([[1, 0, 0],
4208                [1, 1, 1],
4209                [0, 0, 1]])
4210         Dimensions without coordinates: y, z}
4211         >>> array.min(dim=("x", "z"))
4212         <xarray.DataArray (y: 3)>
4213         array([ 1, -5,  1])
4214         Dimensions without coordinates: y
4215         >>> array.argmin(dim=["x", "z"])
4216         {'x': <xarray.DataArray (y: 3)>
4217         array([0, 1, 0])
4218         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4219         array([2, 1, 1])
4220         Dimensions without coordinates: y}
4221         >>> array.isel(array.argmin(dim=["x", "z"]))
4222         <xarray.DataArray (y: 3)>
4223         array([ 1, -5,  1])
4224         Dimensions without coordinates: y
4225         """
4226         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4227         if isinstance(result, dict):
4228             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4229         else:
4230             return self._replace_maybe_drop_dims(result)
4231 
4232     def argmax(
4233         self,
4234         dim: Union[Hashable, Sequence[Hashable]] = None,
4235         axis: int = None,
4236         keep_attrs: bool = None,
4237         skipna: bool = None,
4238     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4239         """Index or indices of the maximum of the DataArray over one or more dimensions.
4240 
4241         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4242         which can be passed directly to isel(). If a single str is passed to 'dim' then
4243         returns a DataArray with dtype int.
4244 
4245         If there are multiple maxima, the indices of the first one found will be
4246         returned.
4247 
4248         Parameters
4249         ----------
4250         dim : hashable, sequence of hashable or ..., optional
4251             The dimensions over which to find the maximum. By default, finds maximum over
4252             all dimensions - for now returning an int for backward compatibility, but
4253             this is deprecated, in future will return a dict with indices for all
4254             dimensions; to return a dict with all dimensions now, pass '...'.
4255         axis : int, optional
4256             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4257             can be supplied.
4258         keep_attrs : bool, optional
4259             If True, the attributes (`attrs`) will be copied from the original
4260             object to the new one.  If False (default), the new object will be
4261             returned without attributes.
4262         skipna : bool, optional
4263             If True, skip missing values (as marked by NaN). By default, only
4264             skips missing values for float dtypes; other dtypes either do not
4265             have a sentinel missing value (int) or skipna=True has not been
4266             implemented (object, datetime64 or timedelta64).
4267 
4268         Returns
4269         -------
4270         result : DataArray or dict of DataArray
4271 
4272         See Also
4273         --------
4274         Variable.argmax, DataArray.idxmax
4275 
4276         Examples
4277         --------
4278         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4279         >>> array.max()
4280         <xarray.DataArray ()>
4281         array(3)
4282         >>> array.argmax()
4283         <xarray.DataArray ()>
4284         array(3)
4285         >>> array.argmax(...)
4286         {'x': <xarray.DataArray ()>
4287         array(3)}
4288         >>> array.isel(array.argmax(...))
4289         <xarray.DataArray ()>
4290         array(3)
4291 
4292         >>> array = xr.DataArray(
4293         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4294         ...     dims=("x", "y", "z"),
4295         ... )
4296         >>> array.max(dim="x")
4297         <xarray.DataArray (y: 3, z: 3)>
4298         array([[3, 3, 2],
4299                [3, 5, 2],
4300                [2, 3, 3]])
4301         Dimensions without coordinates: y, z
4302         >>> array.argmax(dim="x")
4303         <xarray.DataArray (y: 3, z: 3)>
4304         array([[0, 1, 1],
4305                [0, 1, 0],
4306                [0, 1, 0]])
4307         Dimensions without coordinates: y, z
4308         >>> array.argmax(dim=["x"])
4309         {'x': <xarray.DataArray (y: 3, z: 3)>
4310         array([[0, 1, 1],
4311                [0, 1, 0],
4312                [0, 1, 0]])
4313         Dimensions without coordinates: y, z}
4314         >>> array.max(dim=("x", "z"))
4315         <xarray.DataArray (y: 3)>
4316         array([3, 5, 3])
4317         Dimensions without coordinates: y
4318         >>> array.argmax(dim=["x", "z"])
4319         {'x': <xarray.DataArray (y: 3)>
4320         array([0, 1, 0])
4321         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4322         array([0, 1, 2])
4323         Dimensions without coordinates: y}
4324         >>> array.isel(array.argmax(dim=["x", "z"]))
4325         <xarray.DataArray (y: 3)>
4326         array([3, 5, 3])
4327         Dimensions without coordinates: y
4328         """
4329         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4330         if isinstance(result, dict):
4331             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4332         else:
4333             return self._replace_maybe_drop_dims(result)
4334 
4335     # this needs to be at the end, or mypy will confuse with `str`
4336     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4337     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>to_dataframe</method>
<code>
2625     def to_dataframe(
2626         self, name: Hashable = None, dim_order: List[Hashable] = None
2627     ) -> pd.DataFrame:
2628         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2629 
2630         The DataFrame is indexed by the Cartesian product of index coordinates
2631         (in the form of a :py:class:`pandas.MultiIndex`).
2632 
2633         Other coordinates are included as columns in the DataFrame.
2634 
2635         Parameters
2636         ----------
2637         name
2638             Name to give to this array (required if unnamed).
2639         dim_order
2640             Hierarchical dimension order for the resulting dataframe.
2641             Array content is transposed to this order and then written out as flat
2642             vectors in contiguous order, so the last dimension in this list
2643             will be contiguous in the resulting DataFrame. This has a major
2644             influence on which operations are efficient on the resulting
2645             dataframe.
2646 
2647             If provided, must include all dimensions of this DataArray. By default,
2648             dimensions are sorted according to the DataArray dimensions order.
2649 
2650         Returns
2651         -------
2652         result
2653             DataArray as a pandas DataFrame.
2654 
2655         """
2656         if name is None:
2657             name = self.name
2658         if name is None:
2659             raise ValueError(
2660                 "cannot convert an unnamed DataArray to a "
2661                 "DataFrame: use the ``name`` parameter"
2662             )
2663         if self.ndim == 0:
2664             raise ValueError("cannot convert a scalar to a DataFrame")
2665 
2666         # By using a unique name, we can convert a DataArray into a DataFrame
2667         # even if it shares a name with one of its coordinates.
2668         # I would normally use unique_name = object() but that results in a
2669         # dataframe with columns in the wrong order, for reasons I have not
2670         # been able to debug (possibly a pandas bug?).
2671         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2672         ds = self._to_dataset_whole(name=unique_name)
2673 
2674         if dim_order is None:
2675             ordered_dims = dict(zip(self.dims, self.shape))
2676         else:
2677             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2678 
2679         df = ds._to_dataframe(ordered_dims)
2680         df.columns = [name if c == unique_name else c for c in df.columns]
2681         return df

</code><intended_behavior>Similar to `to_series`, `to_dataframe` needs to be adapted to manage `DataArray` objects that feature a mix of multi-indexed and regular dimensions, ensuring a seamless conversion into a `pandas.DataFrame`. The DataFrame's index should be a `MultiIndex` that accurately represents all dimensions involved, including the handling of any stacked dimensions alongside standard dimensions. This enhancement should ensure that the dimensionality of the original `DataArray` is fully and correctly transitioned into the DataFrame structure without loss of information or hierarchical integrity.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
221 class DataArray(AbstractArray, DataWithCoords):
222     """N-dimensional array with labeled coordinates and dimensions.
223 
224     DataArray provides a wrapper around numpy ndarrays that uses
225     labeled dimensions and coordinates to support metadata aware
226     operations. The API is similar to that for the pandas Series or
227     DataFrame, but DataArray objects can have any number of dimensions,
228     and their contents have fixed data types.
229 
230     Additional features over raw numpy arrays:
231 
232     - Apply operations over dimensions by name: ``x.sum('time')``.
233     - Select or assign values by integer location (like numpy):
234       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
235       ``x.sel(time='2014-01-01')``.
236     - Mathematical operations (e.g., ``x - y``) vectorize across
237       multiple dimensions (known in numpy as "broadcasting") based on
238       dimension names, regardless of their original order.
239     - Keep track of arbitrary metadata in the form of a Python
240       dictionary: ``x.attrs``
241     - Convert to a pandas Series: ``x.to_series()``.
242 
243     Getting items from or doing mathematical operations with a
244     DataArray always returns another DataArray.
245 
246     Parameters
247     ----------
248     data : array_like
249         Values for this array. Must be an ``numpy.ndarray``, ndarray
250         like, or castable to an ``ndarray``. If a self-described xarray
251         or pandas object, attempts are made to use this array's
252         metadata to fill in other unspecified arguments. A view of the
253         array's data is used instead of a copy if possible.
254     coords : sequence or dict of array_like, optional
255         Coordinates (tick labels) to use for indexing along each
256         dimension. The following notations are accepted:
257 
258         - mapping {dimension name: array-like}
259         - sequence of tuples that are valid arguments for
260           ``xarray.Variable()``
261           - (dims, data)
262           - (dims, data, attrs)
263           - (dims, data, attrs, encoding)
264 
265         Additionally, it is possible to define a coord whose name
266         does not match the dimension name, or a coord based on multiple
267         dimensions, with one of the following notations:
268 
269         - mapping {coord name: DataArray}
270         - mapping {coord name: Variable}
271         - mapping {coord name: (dimension name, array-like)}
272         - mapping {coord name: (tuple of dimension names, array-like)}
273 
274     dims : hashable or sequence of hashable, optional
275         Name(s) of the data dimension(s). Must be either a hashable
276         (only for 1D data) or a sequence of hashables with length equal
277         to the number of dimensions. If this argument is omitted,
278         dimension names default to ``['dim_0', ... 'dim_n']``.
279     name : str or None, optional
280         Name of this array.
281     attrs : dict_like or None, optional
282         Attributes to assign to the new instance. By default, an empty
283         attribute dictionary is initialized.
284 
285     Examples
286     --------
287     Create data:
288 
289     >>> np.random.seed(0)
290     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
291     >>> precipitation = 10 * np.random.rand(2, 2, 3)
292     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
293     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
294     >>> time = pd.date_range("2014-09-06", periods=3)
295     >>> reference_time = pd.Timestamp("2014-09-05")
296 
297     Initialize a dataarray with multiple dimensions:
298 
299     >>> da = xr.DataArray(
300     ...     data=temperature,
301     ...     dims=["x", "y", "time"],
302     ...     coords=dict(
303     ...         lon=(["x", "y"], lon),
304     ...         lat=(["x", "y"], lat),
305     ...         time=time,
306     ...         reference_time=reference_time,
307     ...     ),
308     ...     attrs=dict(
309     ...         description="Ambient temperature.",
310     ...         units="degC",
311     ...     ),
312     ... )
313     >>> da
314     <xarray.DataArray (x: 2, y: 2, time: 3)>
315     array([[[29.11241877, 18.20125767, 22.82990387],
316             [32.92714559, 29.94046392,  7.18177696]],
317     <BLANKLINE>
318            [[22.60070734, 13.78914233, 14.17424919],
319             [18.28478802, 16.15234857, 26.63418806]]])
320     Coordinates:
321         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
322         lat             (x, y) float64 42.25 42.21 42.63 42.59
323       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
324         reference_time  datetime64[ns] 2014-09-05
325     Dimensions without coordinates: x, y
326     Attributes:
327         description:  Ambient temperature.
328         units:        degC
329 
330     Find out where the coldest temperature was:
331 
332     >>> da.isel(da.argmin(...))
333     <xarray.DataArray ()>
334     array(7.18177696)
335     Coordinates:
336         lon             float64 -99.32
337         lat             float64 42.21
338         time            datetime64[ns] 2014-09-08
339         reference_time  datetime64[ns] 2014-09-05
340     Attributes:
341         description:  Ambient temperature.
342         units:        degC
343     """
344 
345     _cache: Dict[str, Any]
346     _coords: Dict[Any, Variable]
347     _close: Optional[Callable[[], None]]
348     _indexes: Optional[Dict[Hashable, pd.Index]]
349     _name: Optional[Hashable]
350     _variable: Variable
351 
352     __slots__ = (
353         "_cache",
354         "_coords",
355         "_close",
356         "_indexes",
357         "_name",
358         "_variable",
359         "__weakref__",
360     )
361 
362     _groupby_cls = groupby.DataArrayGroupBy
363     _rolling_cls = rolling.DataArrayRolling
364     _coarsen_cls = rolling.DataArrayCoarsen
365     _resample_cls = resample.DataArrayResample
366     _weighted_cls = weighted.DataArrayWeighted
367 
368     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
369 
370     def __init__(
371         self,
372         data: Any = dtypes.NA,
373         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,
374         dims: Union[Hashable, Sequence[Hashable], None] = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: Dict[Hashable, pd.Index] = None,
379         fastpath: bool = False,
380     ):
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385         else:
386             # try to fill in arguments from data if they weren't supplied
387             if coords is None:
388 
389                 if isinstance(data, DataArray):
390                     coords = data.coords
391                 elif isinstance(data, pd.Series):
392                     coords = [data.index]
393                 elif isinstance(data, pd.DataFrame):
394                     coords = [data.index, data.columns]
395                 elif isinstance(data, (pd.Index, IndexVariable)):
396                     coords = [data]
397                 elif isinstance(data, pdcompat.Panel):
398                     coords = [data.items, data.major_axis, data.minor_axis]
399 
400             if dims is None:
401                 dims = getattr(data, "dims", getattr(coords, "dims", None))
402             if name is None:
403                 name = getattr(data, "name", None)
404             if attrs is None and not isinstance(data, PANDAS_TYPES):
405                 attrs = getattr(data, "attrs", None)
406 
407             data = _check_data_shape(data, coords, dims)
408             data = as_compatible_data(data)
409             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
410             variable = Variable(dims, data, attrs, fastpath=True)
411             indexes = dict(
412                 _extract_indexes_from_coords(coords)
413             )  # needed for to_dataset
414 
415         # These fully describe a DataArray
416         self._variable = variable
417         assert isinstance(coords, dict)
418         self._coords = coords
419         self._name = name
420 
421         # TODO(shoyer): document this argument, once it becomes part of the
422         # public interface.
423         self._indexes = indexes
424 
425         self._close = None
426 
427     def _replace(
428         self,
429         variable: Variable = None,
430         coords=None,
431         name: Union[Hashable, None, Default] = _default,
432         indexes=None,
433     ) -> "DataArray":
434         if variable is None:
435             variable = self.variable
436         if coords is None:
437             coords = self._coords
438         if name is _default:
439             name = self.name
440         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
441 
442     def _replace_maybe_drop_dims(
443         self, variable: Variable, name: Union[Hashable, None, Default] = _default
444     ) -> "DataArray":
445         if variable.dims == self.dims and variable.shape == self.shape:
446             coords = self._coords.copy()
447             indexes = self._indexes
448         elif variable.dims == self.dims:
449             # Shape has changed (e.g. from reduce(..., keepdims=True)
450             new_sizes = dict(zip(self.dims, variable.shape))
451             coords = {
452                 k: v
453                 for k, v in self._coords.items()
454                 if v.shape == tuple(new_sizes[d] for d in v.dims)
455             }
456             changed_dims = [
457                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
458             ]
459             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
460         else:
461             allowed_dims = set(variable.dims)
462             coords = {
463                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
464             }
465             indexes = propagate_indexes(
466                 self._indexes, exclude=(set(self.dims) - allowed_dims)
467             )
468         return self._replace(variable, coords, name, indexes=indexes)
469 
470     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> "DataArray":
471         if not len(indexes):
472             return self
473         coords = self._coords.copy()
474         for name, idx in indexes.items():
475             coords[name] = IndexVariable(name, idx)
476         obj = self._replace(coords=coords)
477 
478         # switch from dimension to level names, if necessary
479         dim_names: Dict[Any, str] = {}
480         for dim, idx in indexes.items():
481             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
482                 dim_names[dim] = idx.name
483         if dim_names:
484             obj = obj.rename(dim_names)
485         return obj
486 
487     def _to_temp_dataset(self) -> Dataset:
488         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
489 
490     def _from_temp_dataset(
491         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
492     ) -> "DataArray":
493         variable = dataset._variables.pop(_THIS_ARRAY)
494         coords = dataset._variables
495         indexes = dataset._indexes
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _to_dataset_split(self, dim: Hashable) -> Dataset:
499         """ splits dataarray along dimension 'dim' """
500 
501         def subset(dim, label):
502             array = self.loc[{dim: label}]
503             array.attrs = {}
504             return as_variable(array)
505 
506         variables = {label: subset(dim, label) for label in self.get_index(dim)}
507         variables.update({k: v for k, v in self._coords.items() if k != dim})
508         indexes = propagate_indexes(self._indexes, exclude=dim)
509         coord_names = set(self._coords) - {dim}
510         dataset = Dataset._construct_direct(
511             variables, coord_names, indexes=indexes, attrs=self.attrs
512         )
513         return dataset
514 
515     def _to_dataset_whole(
516         self, name: Hashable = None, shallow_copy: bool = True
517     ) -> Dataset:
518         if name is None:
519             name = self.name
520         if name is None:
521             raise ValueError(
522                 "unable to convert unnamed DataArray to a "
523                 "Dataset without providing an explicit name"
524             )
525         if name in self.coords:
526             raise ValueError(
527                 "cannot create a Dataset from a DataArray with "
528                 "the same name as one of its coordinates"
529             )
530         # use private APIs for speed: this is called by _to_temp_dataset(),
531         # which is used in the guts of a lot of operations (e.g., reindex)
532         variables = self._coords.copy()
533         variables[name] = self.variable
534         if shallow_copy:
535             for k in variables:
536                 variables[k] = variables[k].copy(deep=False)
537         indexes = self._indexes
538 
539         coord_names = set(self._coords)
540         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)
541         return dataset
542 
543     def to_dataset(
544         self,
545         dim: Hashable = None,
546         *,
547         name: Hashable = None,
548         promote_attrs: bool = False,
549     ) -> Dataset:
550         """Convert a DataArray to a Dataset.
551 
552         Parameters
553         ----------
554         dim : hashable, optional
555             Name of the dimension on this array along which to split this array
556             into separate variables. If not provided, this array is converted
557             into a Dataset of one variable.
558         name : hashable, optional
559             Name to substitute for this array's name. Only valid if ``dim`` is
560             not provided.
561         promote_attrs : bool, default: False
562             Set to True to shallow copy attrs of DataArray to returned Dataset.
563 
564         Returns
565         -------
566         dataset : Dataset
567         """
568         if dim is not None and dim not in self.dims:
569             raise TypeError(
570                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
571             )
572 
573         if dim is not None:
574             if name is not None:
575                 raise TypeError("cannot supply both dim and name arguments")
576             result = self._to_dataset_split(dim)
577         else:
578             result = self._to_dataset_whole(name)
579 
580         if promote_attrs:
581             result.attrs = dict(self.attrs)
582 
583         return result
584 
585     @property
586     def name(self) -> Optional[Hashable]:
587         """The name of this array."""
588         return self._name
589 
590     @name.setter
591     def name(self, value: Optional[Hashable]) -> None:
592         self._name = value
593 
594     @property
595     def variable(self) -> Variable:
596         """Low level interface to the Variable object for this DataArray."""
597         return self._variable
598 
599     @property
600     def dtype(self) -> np.dtype:
601         return self.variable.dtype
602 
603     @property
604     def shape(self) -> Tuple[int, ...]:
605         return self.variable.shape
606 
607     @property
608     def size(self) -> int:
609         return self.variable.size
610 
611     @property
612     def nbytes(self) -> int:
613         return self.variable.nbytes
614 
615     @property
616     def ndim(self) -> int:
617         return self.variable.ndim
618 
619     def __len__(self) -> int:
620         return len(self.variable)
621 
622     @property
623     def data(self) -> Any:
624         """The array's data as a dask or numpy array"""
625         return self.variable.data
626 
627     @data.setter
628     def data(self, value: Any) -> None:
629         self.variable.data = value
630 
631     @property
632     def values(self) -> np.ndarray:
633         """The array's data as a numpy.ndarray"""
634         return self.variable.values
635 
636     @values.setter
637     def values(self, value: Any) -> None:
638         self.variable.values = value
639 
640     @property
641     def _in_memory(self) -> bool:
642         return self.variable._in_memory
643 
644     def to_index(self) -> pd.Index:
645         """Convert this variable to a pandas.Index. Only possible for 1D
646         arrays.
647         """
648         return self.variable.to_index()
649 
650     @property
651     def dims(self) -> Tuple[Hashable, ...]:
652         """Tuple of dimension names associated with this array.
653 
654         Note that the type of this property is inconsistent with
655         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
656         consistently named properties.
657         """
658         return self.variable.dims
659 
660     @dims.setter
661     def dims(self, value):
662         raise AttributeError(
663             "you cannot assign dims on a DataArray. Use "
664             ".rename() or .swap_dims() instead."
665         )
666 
667     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
668         if utils.is_dict_like(key):
669             return key
670         else:
671             key = indexing.expanded_indexer(key, self.ndim)
672             return dict(zip(self.dims, key))
673 
674     @property
675     def _level_coords(self) -> Dict[Hashable, Hashable]:
676         """Return a mapping of all MultiIndex levels and their corresponding
677         coordinate name.
678         """
679         level_coords: Dict[Hashable, Hashable] = {}
680 
681         for cname, var in self._coords.items():
682             if var.ndim == 1 and isinstance(var, IndexVariable):
683                 level_names = var.level_names
684                 if level_names is not None:
685                     (dim,) = var.dims
686                     level_coords.update({lname: dim for lname in level_names})
687         return level_coords
688 
689     def _getitem_coord(self, key):
690         from .dataset import _get_virtual_variable
691 
692         try:
693             var = self._coords[key]
694         except KeyError:
695             dim_sizes = dict(zip(self.dims, self.shape))
696             _, key, var = _get_virtual_variable(
697                 self._coords, key, self._level_coords, dim_sizes
698             )
699 
700         return self._replace_maybe_drop_dims(var, name=key)
701 
702     def __getitem__(self, key: Any) -> "DataArray":
703         if isinstance(key, str):
704             return self._getitem_coord(key)
705         else:
706             # xarray-style array indexing
707             return self.isel(indexers=self._item_key_to_dict(key))
708 
709     def __setitem__(self, key: Any, value: Any) -> None:
710         if isinstance(key, str):
711             self.coords[key] = value
712         else:
713             # Coordinates in key, value and self[key] should be consistent.
714             # TODO Coordinate consistency in key is checked here, but it
715             # causes unnecessary indexing. It should be optimized.
716             obj = self[key]
717             if isinstance(value, DataArray):
718                 assert_coordinate_consistent(value, obj.coords.variables)
719             # DataArray key -> Variable key
720             key = {
721                 k: v.variable if isinstance(v, DataArray) else v
722                 for k, v in self._item_key_to_dict(key).items()
723             }
724             self.variable[key] = value
725 
726     def __delitem__(self, key: Any) -> None:
727         del self.coords[key]
728 
729     @property
730     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
731         """Places to look-up items for attribute-style access"""
732         yield from self._item_sources
733         yield self.attrs
734 
735     @property
736     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
737         """Places to look-up items for key-completion"""
738         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
739 
740         # virtual coordinates
741         # uses empty dict -- everything here can already be found in self.coords.
742         yield HybridMappingProxy(keys=self.dims, mapping={})
743         yield HybridMappingProxy(keys=self._level_coords, mapping={})
744 
745     def __contains__(self, key: Any) -> bool:
746         return key in self.data
747 
748     @property
749     def loc(self) -> _LocIndexer:
750         """Attribute for location based indexing like pandas."""
751         return _LocIndexer(self)
752 
753     @property
754     def attrs(self) -> Dict[Hashable, Any]:
755         """Dictionary storing arbitrary metadata with this array."""
756         return self.variable.attrs
757 
758     @attrs.setter
759     def attrs(self, value: Mapping[Hashable, Any]) -> None:
760         # Disable type checking to work around mypy bug - see mypy#4167
761         self.variable.attrs = value  # type: ignore
762 
763     @property
764     def encoding(self) -> Dict[Hashable, Any]:
765         """Dictionary of format-specific settings for how this array should be
766         serialized."""
767         return self.variable.encoding
768 
769     @encoding.setter
770     def encoding(self, value: Mapping[Hashable, Any]) -> None:
771         self.variable.encoding = value
772 
773     @property
774     def indexes(self) -> Indexes:
775         """Mapping of pandas.Index objects used for label based indexing"""
776         if self._indexes is None:
777             self._indexes = default_indexes(self._coords, self.dims)
778         return Indexes(self._indexes)
779 
780     @property
781     def coords(self) -> DataArrayCoordinates:
782         """Dictionary-like container of coordinate arrays."""
783         return DataArrayCoordinates(self)
784 
785     def reset_coords(
786         self,
787         names: Union[Iterable[Hashable], Hashable, None] = None,
788         drop: bool = False,
789     ) -> Union[None, "DataArray", Dataset]:
790         """Given names of coordinates, reset them to become variables.
791 
792         Parameters
793         ----------
794         names : hashable or iterable of hashable, optional
795             Name(s) of non-index coordinates in this dataset to reset into
796             variables. By default, all non-index coordinates are reset.
797         drop : bool, optional
798             If True, remove coordinates instead of converting them into
799             variables.
800 
801         Returns
802         -------
803         Dataset, or DataArray if ``drop == True``
804         """
805         if names is None:
806             names = set(self.coords) - set(self.dims)
807         dataset = self.coords.to_dataset().reset_coords(names, drop)
808         if drop:
809             return self._replace(coords=dataset._variables)
810         else:
811             if self.name is None:
812                 raise ValueError(
813                     "cannot reset_coords with drop=False on an unnamed DataArrray"
814                 )
815             dataset[self.name] = self.variable
816             return dataset
817 
818     def __dask_tokenize__(self):
819         from dask.base import normalize_token
820 
821         return normalize_token((type(self), self._variable, self._coords, self._name))
822 
823     def __dask_graph__(self):
824         return self._to_temp_dataset().__dask_graph__()
825 
826     def __dask_keys__(self):
827         return self._to_temp_dataset().__dask_keys__()
828 
829     def __dask_layers__(self):
830         return self._to_temp_dataset().__dask_layers__()
831 
832     @property
833     def __dask_optimize__(self):
834         return self._to_temp_dataset().__dask_optimize__
835 
836     @property
837     def __dask_scheduler__(self):
838         return self._to_temp_dataset().__dask_scheduler__
839 
840     def __dask_postcompute__(self):
841         func, args = self._to_temp_dataset().__dask_postcompute__()
842         return self._dask_finalize, (func, args, self.name)
843 
844     def __dask_postpersist__(self):
845         func, args = self._to_temp_dataset().__dask_postpersist__()
846         return self._dask_finalize, (func, args, self.name)
847 
848     @staticmethod
849     def _dask_finalize(results, func, args, name):
850         ds = func(results, *args)
851         variable = ds._variables.pop(_THIS_ARRAY)
852         coords = ds._variables
853         return DataArray(variable, coords, name=name, fastpath=True)
854 
855     def load(self, **kwargs) -> "DataArray":
856         """Manually trigger loading of this array's data from disk or a
857         remote source into memory and return this array.
858 
859         Normally, it should not be necessary to call this method in user code,
860         because all xarray functions should either work on deferred data or
861         load data automatically. However, this method can be necessary when
862         working with many file objects on disk.
863 
864         Parameters
865         ----------
866         **kwargs : dict
867             Additional keyword arguments passed on to ``dask.compute``.
868 
869         See Also
870         --------
871         dask.compute
872         """
873         ds = self._to_temp_dataset().load(**kwargs)
874         new = self._from_temp_dataset(ds)
875         self._variable = new._variable
876         self._coords = new._coords
877         return self
878 
879     def compute(self, **kwargs) -> "DataArray":
880         """Manually trigger loading of this array's data from disk or a
881         remote source into memory and return a new array. The original is
882         left unaltered.
883 
884         Normally, it should not be necessary to call this method in user code,
885         because all xarray functions should either work on deferred data or
886         load data automatically. However, this method can be necessary when
887         working with many file objects on disk.
888 
889         Parameters
890         ----------
891         **kwargs : dict
892             Additional keyword arguments passed on to ``dask.compute``.
893 
894         See Also
895         --------
896         dask.compute
897         """
898         new = self.copy(deep=False)
899         return new.load(**kwargs)
900 
901     def persist(self, **kwargs) -> "DataArray":
902         """Trigger computation in constituent dask arrays
903 
904         This keeps them as dask arrays but encourages them to keep data in
905         memory.  This is particularly useful when on a distributed machine.
906         When on a single machine consider using ``.compute()`` instead.
907 
908         Parameters
909         ----------
910         **kwargs : dict
911             Additional keyword arguments passed on to ``dask.persist``.
912 
913         See Also
914         --------
915         dask.persist
916         """
917         ds = self._to_temp_dataset().persist(**kwargs)
918         return self._from_temp_dataset(ds)
919 
920     def copy(self, deep: bool = True, data: Any = None) -> "DataArray":
921         """Returns a copy of this array.
922 
923         If `deep=True`, a deep copy is made of the data array.
924         Otherwise, a shallow copy is made, and the returned data array's
925         values are a new view of this data array's values.
926 
927         Use `data` to create a new object with the same structure as
928         original but entirely new data.
929 
930         Parameters
931         ----------
932         deep : bool, optional
933             Whether the data array and its coordinates are loaded into memory
934             and copied onto the new object. Default is True.
935         data : array_like, optional
936             Data to use in the new object. Must have same shape as original.
937             When `data` is used, `deep` is ignored for all data variables,
938             and only used for coords.
939 
940         Returns
941         -------
942         object : DataArray
943             New object with dimensions, attributes, coordinates, name,
944             encoding, and optionally data copied from original.
945 
946         Examples
947         --------
948         Shallow versus deep copy
949 
950         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
951         >>> array.copy()
952         <xarray.DataArray (x: 3)>
953         array([1, 2, 3])
954         Coordinates:
955           * x        (x) <U1 'a' 'b' 'c'
956         >>> array_0 = array.copy(deep=False)
957         >>> array_0[0] = 7
958         >>> array_0
959         <xarray.DataArray (x: 3)>
960         array([7, 2, 3])
961         Coordinates:
962           * x        (x) <U1 'a' 'b' 'c'
963         >>> array
964         <xarray.DataArray (x: 3)>
965         array([7, 2, 3])
966         Coordinates:
967           * x        (x) <U1 'a' 'b' 'c'
968 
969         Changing the data using the ``data`` argument maintains the
970         structure of the original object, but with the new data. Original
971         object is unaffected.
972 
973         >>> array.copy(data=[0.1, 0.2, 0.3])
974         <xarray.DataArray (x: 3)>
975         array([0.1, 0.2, 0.3])
976         Coordinates:
977           * x        (x) <U1 'a' 'b' 'c'
978         >>> array
979         <xarray.DataArray (x: 3)>
980         array([7, 2, 3])
981         Coordinates:
982           * x        (x) <U1 'a' 'b' 'c'
983 
984         See Also
985         --------
986         pandas.DataFrame.copy
987         """
988         variable = self.variable.copy(deep=deep, data=data)
989         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
990         if self._indexes is None:
991             indexes = self._indexes
992         else:
993             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
994         return self._replace(variable, coords, indexes=indexes)
995 
996     def __copy__(self) -> "DataArray":
997         return self.copy(deep=False)
998 
999     def __deepcopy__(self, memo=None) -> "DataArray":
1000         # memo does nothing but is required for compatibility with
1001         # copy.deepcopy
1002         return self.copy(deep=True)
1003 
1004     # mutable objects should not be hashable
1005     # https://github.com/python/mypy/issues/4266
1006     __hash__ = None  # type: ignore
1007 
1008     @property
1009     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1010         """Block dimensions for this array's data or None if it's not a dask
1011         array.
1012         """
1013         return self.variable.chunks
1014 
1015     def chunk(
1016         self,
1017         chunks: Union[
1018             Number,
1019             Tuple[Number, ...],
1020             Tuple[Tuple[Number, ...], ...],
1021             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],
1022         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1023         name_prefix: str = "xarray-",
1024         token: str = None,
1025         lock: bool = False,
1026     ) -> "DataArray":
1027         """Coerce this array's data into a dask arrays with the given chunks.
1028 
1029         If this variable is a non-dask array, it will be converted to dask
1030         array. If it's a dask array, it will be rechunked to the given chunk
1031         sizes.
1032 
1033         If neither chunks is not provided for one or more dimensions, chunk
1034         sizes along that dimension will not be updated; non-dask arrays will be
1035         converted into dask arrays with a single block.
1036 
1037         Parameters
1038         ----------
1039         chunks : int, tuple of int or mapping of hashable to int, optional
1040             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1041             ``{'x': 5, 'y': 5}``.
1042         name_prefix : str, optional
1043             Prefix for the name of the new dask array.
1044         token : str, optional
1045             Token uniquely identifying this array.
1046         lock : optional
1047             Passed on to :py:func:`dask.array.from_array`, if the array is not
1048             already as dask array.
1049 
1050         Returns
1051         -------
1052         chunked : xarray.DataArray
1053         """
1054         if isinstance(chunks, (tuple, list)):
1055             chunks = dict(zip(self.dims, chunks))
1056 
1057         ds = self._to_temp_dataset().chunk(
1058             chunks, name_prefix=name_prefix, token=token, lock=lock
1059         )
1060         return self._from_temp_dataset(ds)
1061 
1062     def isel(
1063         self,
1064         indexers: Mapping[Hashable, Any] = None,
1065         drop: bool = False,
1066         missing_dims: str = "raise",
1067         **indexers_kwargs: Any,
1068     ) -> "DataArray":
1069         """Return a new DataArray whose data is given by integer indexing
1070         along the specified dimension(s).
1071 
1072         Parameters
1073         ----------
1074         indexers : dict, optional
1075             A dict with keys matching dimensions and values given
1076             by integers, slice objects or arrays.
1077             indexer can be a integer, slice, array-like or DataArray.
1078             If DataArrays are passed as indexers, xarray-style indexing will be
1079             carried out. See :ref:`indexing` for the details.
1080             One of indexers or indexers_kwargs must be provided.
1081         drop : bool, optional
1082             If ``drop=True``, drop coordinates variables indexed by integers
1083             instead of making them scalar.
1084         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1085             What to do if dimensions that should be selected from are not present in the
1086             DataArray:
1087             - "raise": raise an exception
1088             - "warning": raise a warning, and ignore the missing dimensions
1089             - "ignore": ignore the missing dimensions
1090         **indexers_kwargs : {dim: indexer, ...}, optional
1091             The keyword arguments form of ``indexers``.
1092 
1093         See Also
1094         --------
1095         Dataset.isel
1096         DataArray.sel
1097 
1098         Examples
1099         --------
1100         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1101         >>> da
1102         <xarray.DataArray (x: 5, y: 5)>
1103         array([[ 0,  1,  2,  3,  4],
1104                [ 5,  6,  7,  8,  9],
1105                [10, 11, 12, 13, 14],
1106                [15, 16, 17, 18, 19],
1107                [20, 21, 22, 23, 24]])
1108         Dimensions without coordinates: x, y
1109 
1110         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1111         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1112         >>> da = da.isel(x=tgt_x, y=tgt_y)
1113         >>> da
1114         <xarray.DataArray (points: 5)>
1115         array([ 0,  6, 12, 18, 24])
1116         Dimensions without coordinates: points
1117         """
1118 
1119         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1120 
1121         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1122             ds = self._to_temp_dataset()._isel_fancy(
1123                 indexers, drop=drop, missing_dims=missing_dims
1124             )
1125             return self._from_temp_dataset(ds)
1126 
1127         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1128         # lists, or zero or one-dimensional np.ndarray's
1129 
1130         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1131 
1132         coords = {}
1133         for coord_name, coord_value in self._coords.items():
1134             coord_indexers = {
1135                 k: v for k, v in indexers.items() if k in coord_value.dims
1136             }
1137             if coord_indexers:
1138                 coord_value = coord_value.isel(coord_indexers)
1139                 if drop and coord_value.ndim == 0:
1140                     continue
1141             coords[coord_name] = coord_value
1142 
1143         return self._replace(variable=variable, coords=coords)
1144 
1145     def sel(
1146         self,
1147         indexers: Mapping[Hashable, Any] = None,
1148         method: str = None,
1149         tolerance=None,
1150         drop: bool = False,
1151         **indexers_kwargs: Any,
1152     ) -> "DataArray":
1153         """Return a new DataArray whose data is given by selecting index
1154         labels along the specified dimension(s).
1155 
1156         In contrast to `DataArray.isel`, indexers for this method should use
1157         labels instead of integers.
1158 
1159         Under the hood, this method is powered by using pandas's powerful Index
1160         objects. This makes label based indexing essentially just as fast as
1161         using integer indexing.
1162 
1163         It also means this method uses pandas's (well documented) logic for
1164         indexing. This means you can use string shortcuts for datetime indexes
1165         (e.g., '2000-01' to select all values in January 2000). It also means
1166         that slices are treated as inclusive of both the start and stop values,
1167         unlike normal Python indexing.
1168 
1169         .. warning::
1170 
1171           Do not try to assign values when using any of the indexing methods
1172           ``isel`` or ``sel``::
1173 
1174             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1175             # DO NOT do this
1176             da.isel(x=[0, 1, 2])[1] = -1
1177 
1178           Assigning values with the chained indexing using ``.sel`` or
1179           ``.isel`` fails silently.
1180 
1181         Parameters
1182         ----------
1183         indexers : dict, optional
1184             A dict with keys matching dimensions and values given
1185             by scalars, slices or arrays of tick labels. For dimensions with
1186             multi-index, the indexer may also be a dict-like object with keys
1187             matching index level names.
1188             If DataArrays are passed as indexers, xarray-style indexing will be
1189             carried out. See :ref:`indexing` for the details.
1190             One of indexers or indexers_kwargs must be provided.
1191         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1192             Method to use for inexact matches:
1193 
1194             * None (default): only exact matches
1195             * pad / ffill: propagate last valid index value forward
1196             * backfill / bfill: propagate next valid index value backward
1197             * nearest: use nearest valid index value
1198         tolerance : optional
1199             Maximum distance between original and new labels for inexact
1200             matches. The values of the index at the matching locations must
1201             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1202         drop : bool, optional
1203             If ``drop=True``, drop coordinates variables in `indexers` instead
1204             of making them scalar.
1205         **indexers_kwargs : {dim: indexer, ...}, optional
1206             The keyword arguments form of ``indexers``.
1207             One of indexers or indexers_kwargs must be provided.
1208 
1209         Returns
1210         -------
1211         obj : DataArray
1212             A new DataArray with the same contents as this DataArray, except the
1213             data and each dimension is indexed by the appropriate indexers.
1214             If indexer DataArrays have coordinates that do not conflict with
1215             this object, then these coordinates will be attached.
1216             In general, each array's data will be a view of the array's data
1217             in this DataArray, unless vectorized indexing was triggered by using
1218             an array indexer, in which case the data will be a copy.
1219 
1220         See Also
1221         --------
1222         Dataset.sel
1223         DataArray.isel
1224 
1225         Examples
1226         --------
1227         >>> da = xr.DataArray(
1228         ...     np.arange(25).reshape(5, 5),
1229         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1230         ...     dims=("x", "y"),
1231         ... )
1232         >>> da
1233         <xarray.DataArray (x: 5, y: 5)>
1234         array([[ 0,  1,  2,  3,  4],
1235                [ 5,  6,  7,  8,  9],
1236                [10, 11, 12, 13, 14],
1237                [15, 16, 17, 18, 19],
1238                [20, 21, 22, 23, 24]])
1239         Coordinates:
1240           * x        (x) int64 0 1 2 3 4
1241           * y        (y) int64 0 1 2 3 4
1242 
1243         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1244         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1245         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1246         >>> da
1247         <xarray.DataArray (points: 5)>
1248         array([ 0,  6, 12, 18, 24])
1249         Coordinates:
1250             x        (points) int64 0 1 2 3 4
1251             y        (points) int64 0 1 2 3 4
1252         Dimensions without coordinates: points
1253         """
1254         ds = self._to_temp_dataset().sel(
1255             indexers=indexers,
1256             drop=drop,
1257             method=method,
1258             tolerance=tolerance,
1259             **indexers_kwargs,
1260         )
1261         return self._from_temp_dataset(ds)
1262 
1263     def head(
1264         self,
1265         indexers: Union[Mapping[Hashable, int], int] = None,
1266         **indexers_kwargs: Any,
1267     ) -> "DataArray":
1268         """Return a new DataArray whose data is given by the the first `n`
1269         values along the specified dimension(s). Default `n` = 5
1270 
1271         See Also
1272         --------
1273         Dataset.head
1274         DataArray.tail
1275         DataArray.thin
1276         """
1277         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1278         return self._from_temp_dataset(ds)
1279 
1280     def tail(
1281         self,
1282         indexers: Union[Mapping[Hashable, int], int] = None,
1283         **indexers_kwargs: Any,
1284     ) -> "DataArray":
1285         """Return a new DataArray whose data is given by the the last `n`
1286         values along the specified dimension(s). Default `n` = 5
1287 
1288         See Also
1289         --------
1290         Dataset.tail
1291         DataArray.head
1292         DataArray.thin
1293         """
1294         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1295         return self._from_temp_dataset(ds)
1296 
1297     def thin(
1298         self,
1299         indexers: Union[Mapping[Hashable, int], int] = None,
1300         **indexers_kwargs: Any,
1301     ) -> "DataArray":
1302         """Return a new DataArray whose data is given by each `n` value
1303         along the specified dimension(s).
1304 
1305         See Also
1306         --------
1307         Dataset.thin
1308         DataArray.head
1309         DataArray.tail
1310         """
1311         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1312         return self._from_temp_dataset(ds)
1313 
1314     def broadcast_like(
1315         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1316     ) -> "DataArray":
1317         """Broadcast this DataArray against another Dataset or DataArray.
1318 
1319         This is equivalent to xr.broadcast(other, self)[1]
1320 
1321         xarray objects are broadcast against each other in arithmetic
1322         operations, so this method is not be necessary for most uses.
1323 
1324         If no change is needed, the input data is returned to the output
1325         without being copied.
1326 
1327         If new coords are added by the broadcast, their values are
1328         NaN filled.
1329 
1330         Parameters
1331         ----------
1332         other : Dataset or DataArray
1333             Object against which to broadcast this array.
1334         exclude : iterable of hashable, optional
1335             Dimensions that must not be broadcasted
1336 
1337         Returns
1338         -------
1339         new_da : DataArray
1340             The caller broadcasted against ``other``.
1341 
1342         Examples
1343         --------
1344         >>> arr1 = xr.DataArray(
1345         ...     np.random.randn(2, 3),
1346         ...     dims=("x", "y"),
1347         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1348         ... )
1349         >>> arr2 = xr.DataArray(
1350         ...     np.random.randn(3, 2),
1351         ...     dims=("x", "y"),
1352         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1353         ... )
1354         >>> arr1
1355         <xarray.DataArray (x: 2, y: 3)>
1356         array([[ 1.76405235,  0.40015721,  0.97873798],
1357                [ 2.2408932 ,  1.86755799, -0.97727788]])
1358         Coordinates:
1359           * x        (x) <U1 'a' 'b'
1360           * y        (y) <U1 'a' 'b' 'c'
1361         >>> arr2
1362         <xarray.DataArray (x: 3, y: 2)>
1363         array([[ 0.95008842, -0.15135721],
1364                [-0.10321885,  0.4105985 ],
1365                [ 0.14404357,  1.45427351]])
1366         Coordinates:
1367           * x        (x) <U1 'a' 'b' 'c'
1368           * y        (y) <U1 'a' 'b'
1369         >>> arr1.broadcast_like(arr2)
1370         <xarray.DataArray (x: 3, y: 3)>
1371         array([[ 1.76405235,  0.40015721,  0.97873798],
1372                [ 2.2408932 ,  1.86755799, -0.97727788],
1373                [        nan,         nan,         nan]])
1374         Coordinates:
1375           * x        (x) <U1 'a' 'b' 'c'
1376           * y        (y) <U1 'a' 'b' 'c'
1377         """
1378         if exclude is None:
1379             exclude = set()
1380         else:
1381             exclude = set(exclude)
1382         args = align(other, self, join="outer", copy=False, exclude=exclude)
1383 
1384         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1385 
1386         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1387 
1388     def reindex_like(
1389         self,
1390         other: Union["DataArray", Dataset],
1391         method: str = None,
1392         tolerance=None,
1393         copy: bool = True,
1394         fill_value=dtypes.NA,
1395     ) -> "DataArray":
1396         """Conform this object onto the indexes of another object, filling in
1397         missing values with ``fill_value``. The default fill value is NaN.
1398 
1399         Parameters
1400         ----------
1401         other : Dataset or DataArray
1402             Object with an 'indexes' attribute giving a mapping from dimension
1403             names to pandas.Index objects, which provides coordinates upon
1404             which to index the variables in this dataset. The indexes on this
1405             other object need not be the same as the indexes on this
1406             dataset. Any mis-matched index values will be filled in with
1407             NaN, and any mis-matched dimension names will simply be ignored.
1408         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1409             Method to use for filling index values from other not found on this
1410             data array:
1411 
1412             * None (default): don't fill gaps
1413             * pad / ffill: propagate last valid index value forward
1414             * backfill / bfill: propagate next valid index value backward
1415             * nearest: use nearest valid index value
1416         tolerance : optional
1417             Maximum distance between original and new labels for inexact
1418             matches. The values of the index at the matching locations must
1419             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1420         copy : bool, optional
1421             If ``copy=True``, data in the return value is always copied. If
1422             ``copy=False`` and reindexing is unnecessary, or can be performed
1423             with only slice operations, then the output may share memory with
1424             the input. In either case, a new xarray object is always returned.
1425         fill_value : scalar or dict-like, optional
1426             Value to use for newly missing values. If a dict-like, maps
1427             variable names (including coordinates) to fill values. Use this
1428             data array's name to refer to the data array's values.
1429 
1430         Returns
1431         -------
1432         reindexed : DataArray
1433             Another dataset array, with this array's data but coordinates from
1434             the other object.
1435 
1436         See Also
1437         --------
1438         DataArray.reindex
1439         align
1440         """
1441         indexers = reindex_like_indexers(self, other)
1442         return self.reindex(
1443             indexers=indexers,
1444             method=method,
1445             tolerance=tolerance,
1446             copy=copy,
1447             fill_value=fill_value,
1448         )
1449 
1450     def reindex(
1451         self,
1452         indexers: Mapping[Hashable, Any] = None,
1453         method: str = None,
1454         tolerance=None,
1455         copy: bool = True,
1456         fill_value=dtypes.NA,
1457         **indexers_kwargs: Any,
1458     ) -> "DataArray":
1459         """Conform this object onto the indexes of another object, filling in
1460         missing values with ``fill_value``. The default fill value is NaN.
1461 
1462         Parameters
1463         ----------
1464         indexers : dict, optional
1465             Dictionary with keys given by dimension names and values given by
1466             arrays of coordinates tick labels. Any mis-matched coordinate
1467             values will be filled in with NaN, and any mis-matched dimension
1468             names will simply be ignored.
1469             One of indexers or indexers_kwargs must be provided.
1470         copy : bool, optional
1471             If ``copy=True``, data in the return value is always copied. If
1472             ``copy=False`` and reindexing is unnecessary, or can be performed
1473             with only slice operations, then the output may share memory with
1474             the input. In either case, a new xarray object is always returned.
1475         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1476             Method to use for filling index values in ``indexers`` not found on
1477             this data array:
1478 
1479             * None (default): don't fill gaps
1480             * pad / ffill: propagate last valid index value forward
1481             * backfill / bfill: propagate next valid index value backward
1482             * nearest: use nearest valid index value
1483         tolerance : optional
1484             Maximum distance between original and new labels for inexact
1485             matches. The values of the index at the matching locations must
1486             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1487         fill_value : scalar or dict-like, optional
1488             Value to use for newly missing values. If a dict-like, maps
1489             variable names (including coordinates) to fill values. Use this
1490             data array's name to refer to the data array's values.
1491         **indexers_kwargs : {dim: indexer, ...}, optional
1492             The keyword arguments form of ``indexers``.
1493             One of indexers or indexers_kwargs must be provided.
1494 
1495         Returns
1496         -------
1497         reindexed : DataArray
1498             Another dataset array, with this array's data but replaced
1499             coordinates.
1500 
1501         See Also
1502         --------
1503         DataArray.reindex_like
1504         align
1505         """
1506         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1507         if isinstance(fill_value, dict):
1508             fill_value = fill_value.copy()
1509             sentinel = object()
1510             value = fill_value.pop(self.name, sentinel)
1511             if value is not sentinel:
1512                 fill_value[_THIS_ARRAY] = value
1513 
1514         ds = self._to_temp_dataset().reindex(
1515             indexers=indexers,
1516             method=method,
1517             tolerance=tolerance,
1518             copy=copy,
1519             fill_value=fill_value,
1520         )
1521         return self._from_temp_dataset(ds)
1522 
1523     def interp(
1524         self,
1525         coords: Mapping[Hashable, Any] = None,
1526         method: str = "linear",
1527         assume_sorted: bool = False,
1528         kwargs: Mapping[str, Any] = None,
1529         **coords_kwargs: Any,
1530     ) -> "DataArray":
1531         """Multidimensional interpolation of variables.
1532 
1533         Parameters
1534         ----------
1535         coords : dict, optional
1536             Mapping from dimension names to the new coordinates.
1537             New coordinate can be an scalar, array-like or DataArray.
1538             If DataArrays are passed as new coordinates, their dimensions are
1539             used for the broadcasting. Missing values are skipped.
1540         method : str, default: "linear"
1541             The method used to interpolate. Choose from
1542 
1543             - {"linear", "nearest"} for multidimensional array,
1544             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1545         assume_sorted : bool, optional
1546             If False, values of x can be in any order and they are sorted
1547             first. If True, x has to be an array of monotonically increasing
1548             values.
1549         kwargs : dict
1550             Additional keyword arguments passed to scipy's interpolator. Valid
1551             options and their behavior depend on if 1-dimensional or
1552             multi-dimensional interpolation is used.
1553         **coords_kwargs : {dim: coordinate, ...}, optional
1554             The keyword arguments form of ``coords``.
1555             One of coords or coords_kwargs must be provided.
1556 
1557         Returns
1558         -------
1559         interpolated : DataArray
1560             New dataarray on the new coordinates.
1561 
1562         Notes
1563         -----
1564         scipy is required.
1565 
1566         See Also
1567         --------
1568         scipy.interpolate.interp1d
1569         scipy.interpolate.interpn
1570 
1571         Examples
1572         --------
1573         >>> da = xr.DataArray(
1574         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1575         ...     dims=("x", "y"),
1576         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1577         ... )
1578         >>> da
1579         <xarray.DataArray (x: 3, y: 4)>
1580         array([[ 1.,  4.,  2.,  9.],
1581                [ 2.,  7.,  6., nan],
1582                [ 6., nan,  5.,  8.]])
1583         Coordinates:
1584           * x        (x) int64 0 1 2
1585           * y        (y) int64 10 12 14 16
1586 
1587         1D linear interpolation (the default):
1588 
1589         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1590         <xarray.DataArray (x: 4, y: 4)>
1591         array([[1.  , 4.  , 2.  ,  nan],
1592                [1.75, 6.25, 5.  ,  nan],
1593                [3.  ,  nan, 5.75,  nan],
1594                [5.  ,  nan, 5.25,  nan]])
1595         Coordinates:
1596           * y        (y) int64 10 12 14 16
1597           * x        (x) float64 0.0 0.75 1.25 1.75
1598 
1599         1D nearest interpolation:
1600 
1601         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1602         <xarray.DataArray (x: 4, y: 4)>
1603         array([[ 1.,  4.,  2.,  9.],
1604                [ 2.,  7.,  6., nan],
1605                [ 2.,  7.,  6., nan],
1606                [ 6., nan,  5.,  8.]])
1607         Coordinates:
1608           * y        (y) int64 10 12 14 16
1609           * x        (x) float64 0.0 0.75 1.25 1.75
1610 
1611         1D linear extrapolation:
1612 
1613         >>> da.interp(
1614         ...     x=[1, 1.5, 2.5, 3.5],
1615         ...     method="linear",
1616         ...     kwargs={"fill_value": "extrapolate"},
1617         ... )
1618         <xarray.DataArray (x: 4, y: 4)>
1619         array([[ 2. ,  7. ,  6. ,  nan],
1620                [ 4. ,  nan,  5.5,  nan],
1621                [ 8. ,  nan,  4.5,  nan],
1622                [12. ,  nan,  3.5,  nan]])
1623         Coordinates:
1624           * y        (y) int64 10 12 14 16
1625           * x        (x) float64 1.0 1.5 2.5 3.5
1626 
1627         2D linear interpolation:
1628 
1629         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1630         <xarray.DataArray (x: 4, y: 3)>
1631         array([[2.5  , 3.   ,   nan],
1632                [4.   , 5.625,   nan],
1633                [  nan,   nan,   nan],
1634                [  nan,   nan,   nan]])
1635         Coordinates:
1636           * x        (x) float64 0.0 0.75 1.25 1.75
1637           * y        (y) int64 11 13 15
1638         """
1639         if self.dtype.kind not in "uifc":
1640             raise TypeError(
1641                 "interp only works for a numeric type array. "
1642                 "Given {}.".format(self.dtype)
1643             )
1644         ds = self._to_temp_dataset().interp(
1645             coords,
1646             method=method,
1647             kwargs=kwargs,
1648             assume_sorted=assume_sorted,
1649             **coords_kwargs,
1650         )
1651         return self._from_temp_dataset(ds)
1652 
1653     def interp_like(
1654         self,
1655         other: Union["DataArray", Dataset],
1656         method: str = "linear",
1657         assume_sorted: bool = False,
1658         kwargs: Mapping[str, Any] = None,
1659     ) -> "DataArray":
1660         """Interpolate this object onto the coordinates of another object,
1661         filling out of range values with NaN.
1662 
1663         Parameters
1664         ----------
1665         other : Dataset or DataArray
1666             Object with an 'indexes' attribute giving a mapping from dimension
1667             names to an 1d array-like, which provides coordinates upon
1668             which to index the variables in this dataset. Missing values are skipped.
1669         method : str, default: "linear"
1670             The method used to interpolate. Choose from
1671 
1672             - {"linear", "nearest"} for multidimensional array,
1673             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1674         assume_sorted : bool, optional
1675             If False, values of coordinates that are interpolated over can be
1676             in any order and they are sorted first. If True, interpolated
1677             coordinates are assumed to be an array of monotonically increasing
1678             values.
1679         kwargs : dict, optional
1680             Additional keyword passed to scipy's interpolator.
1681 
1682         Returns
1683         -------
1684         interpolated : DataArray
1685             Another dataarray by interpolating this dataarray's data along the
1686             coordinates of the other object.
1687 
1688         Notes
1689         -----
1690         scipy is required.
1691         If the dataarray has object-type coordinates, reindex is used for these
1692         coordinates instead of the interpolation.
1693 
1694         See Also
1695         --------
1696         DataArray.interp
1697         DataArray.reindex_like
1698         """
1699         if self.dtype.kind not in "uifc":
1700             raise TypeError(
1701                 "interp only works for a numeric type array. "
1702                 "Given {}.".format(self.dtype)
1703             )
1704         ds = self._to_temp_dataset().interp_like(
1705             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1706         )
1707         return self._from_temp_dataset(ds)
1708 
1709     def rename(
1710         self,
1711         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,
1712         **names: Hashable,
1713     ) -> "DataArray":
1714         """Returns a new DataArray with renamed coordinates or a new name.
1715 
1716         Parameters
1717         ----------
1718         new_name_or_name_dict : str or dict-like, optional
1719             If the argument is dict-like, it used as a mapping from old
1720             names to new names for coordinates. Otherwise, use the argument
1721             as the new name for this array.
1722         **names : hashable, optional
1723             The keyword arguments form of a mapping from old names to
1724             new names for coordinates.
1725             One of new_name_or_name_dict or names must be provided.
1726 
1727         Returns
1728         -------
1729         renamed : DataArray
1730             Renamed array or array with renamed coordinates.
1731 
1732         See Also
1733         --------
1734         Dataset.rename
1735         DataArray.swap_dims
1736         """
1737         if names or utils.is_dict_like(new_name_or_name_dict):
1738             new_name_or_name_dict = cast(
1739                 Mapping[Hashable, Hashable], new_name_or_name_dict
1740             )
1741             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1742             dataset = self._to_temp_dataset().rename(name_dict)
1743             return self._from_temp_dataset(dataset)
1744         else:
1745             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1746             return self._replace(name=new_name_or_name_dict)
1747 
1748     def swap_dims(
1749         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims_kwargs
1750     ) -> "DataArray":
1751         """Returns a new DataArray with swapped dimensions.
1752 
1753         Parameters
1754         ----------
1755         dims_dict : dict-like
1756             Dictionary whose keys are current dimension names and whose values
1757             are new names.
1758         **dims_kwargs : {existing_dim: new_dim, ...}, optional
1759             The keyword arguments form of ``dims_dict``.
1760             One of dims_dict or dims_kwargs must be provided.
1761 
1762         Returns
1763         -------
1764         swapped : DataArray
1765             DataArray with swapped dimensions.
1766 
1767         Examples
1768         --------
1769         >>> arr = xr.DataArray(
1770         ...     data=[0, 1],
1771         ...     dims="x",
1772         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1773         ... )
1774         >>> arr
1775         <xarray.DataArray (x: 2)>
1776         array([0, 1])
1777         Coordinates:
1778           * x        (x) <U1 'a' 'b'
1779             y        (x) int64 0 1
1780 
1781         >>> arr.swap_dims({"x": "y"})
1782         <xarray.DataArray (y: 2)>
1783         array([0, 1])
1784         Coordinates:
1785             x        (y) <U1 'a' 'b'
1786           * y        (y) int64 0 1
1787 
1788         >>> arr.swap_dims({"x": "z"})
1789         <xarray.DataArray (z: 2)>
1790         array([0, 1])
1791         Coordinates:
1792             x        (z) <U1 'a' 'b'
1793             y        (z) int64 0 1
1794         Dimensions without coordinates: z
1795 
1796         See Also
1797         --------
1798         DataArray.rename
1799         Dataset.swap_dims
1800         """
1801         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
1802         ds = self._to_temp_dataset().swap_dims(dims_dict)
1803         return self._from_temp_dataset(ds)
1804 
1805     def expand_dims(
1806         self,
1807         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
1808         axis=None,
1809         **dim_kwargs: Any,
1810     ) -> "DataArray":
1811         """Return a new object with an additional axis (or axes) inserted at
1812         the corresponding position in the array shape. The new object is a
1813         view into the underlying array, not a copy.
1814 
1815         If dim is already a scalar coordinate, it will be promoted to a 1D
1816         coordinate consisting of a single value.
1817 
1818         Parameters
1819         ----------
1820         dim : hashable, sequence of hashable, dict, or None, optional
1821             Dimensions to include on the new variable.
1822             If provided as str or sequence of str, then dimensions are inserted
1823             with length 1. If provided as a dict, then the keys are the new
1824             dimensions and the values are either integers (giving the length of
1825             the new dimensions) or sequence/ndarray (giving the coordinates of
1826             the new dimensions).
1827         axis : int, list of int or tuple of int, or None, default: None
1828             Axis position(s) where new axis is to be inserted (position(s) on
1829             the result array). If a list (or tuple) of integers is passed,
1830             multiple axes are inserted. In this case, dim arguments should be
1831             same length list. If axis=None is passed, all the axes will be
1832             inserted to the start of the result array.
1833         **dim_kwargs : int or sequence or ndarray
1834             The keywords are arbitrary dimensions being inserted and the values
1835             are either the lengths of the new dims (if int is given), or their
1836             coordinates. Note, this is an alternative to passing a dict to the
1837             dim kwarg and will only be used if dim is None.
1838 
1839         Returns
1840         -------
1841         expanded : same type as caller
1842             This object, but with an additional dimension(s).
1843         """
1844         if isinstance(dim, int):
1845             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1846         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1847             if len(dim) != len(set(dim)):
1848                 raise ValueError("dims should not contain duplicate values.")
1849             dim = dict.fromkeys(dim, 1)
1850         elif dim is not None and not isinstance(dim, Mapping):
1851             dim = {cast(Hashable, dim): 1}
1852 
1853         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1854         ds = self._to_temp_dataset().expand_dims(dim, axis)
1855         return self._from_temp_dataset(ds)
1856 
1857     def set_index(
1858         self,
1859         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
1860         append: bool = False,
1861         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1862     ) -> Optional["DataArray"]:
1863         """Set DataArray (multi-)indexes using one or more existing
1864         coordinates.
1865 
1866         Parameters
1867         ----------
1868         indexes : {dim: index, ...}
1869             Mapping from names matching dimensions and values given
1870             by (lists of) the names of existing coordinates or variables to set
1871             as new (multi-)index.
1872         append : bool, optional
1873             If True, append the supplied index(es) to the existing index(es).
1874             Otherwise replace the existing index(es) (default).
1875         **indexes_kwargs : optional
1876             The keyword arguments form of ``indexes``.
1877             One of indexes or indexes_kwargs must be provided.
1878 
1879         Returns
1880         -------
1881         obj : DataArray
1882             Another DataArray, with this data but replaced coordinates.
1883 
1884         Examples
1885         --------
1886         >>> arr = xr.DataArray(
1887         ...     data=np.ones((2, 3)),
1888         ...     dims=["x", "y"],
1889         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1890         ... )
1891         >>> arr
1892         <xarray.DataArray (x: 2, y: 3)>
1893         array([[1., 1., 1.],
1894                [1., 1., 1.]])
1895         Coordinates:
1896           * x        (x) int64 0 1
1897           * y        (y) int64 0 1 2
1898             a        (x) int64 3 4
1899         >>> arr.set_index(x="a")
1900         <xarray.DataArray (x: 2, y: 3)>
1901         array([[1., 1., 1.],
1902                [1., 1., 1.]])
1903         Coordinates:
1904           * x        (x) int64 3 4
1905           * y        (y) int64 0 1 2
1906 
1907         See Also
1908         --------
1909         DataArray.reset_index
1910         """
1911         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
1912         return self._from_temp_dataset(ds)
1913 
1914     def reset_index(
1915         self,
1916         dims_or_levels: Union[Hashable, Sequence[Hashable]],
1917         drop: bool = False,
1918     ) -> Optional["DataArray"]:
1919         """Reset the specified index(es) or multi-index level(s).
1920 
1921         Parameters
1922         ----------
1923         dims_or_levels : hashable or sequence of hashable
1924             Name(s) of the dimension(s) and/or multi-index level(s) that will
1925             be reset.
1926         drop : bool, optional
1927             If True, remove the specified indexes and/or multi-index levels
1928             instead of extracting them as new coordinates (default: False).
1929 
1930         Returns
1931         -------
1932         obj : DataArray
1933             Another dataarray, with this dataarray's data but replaced
1934             coordinates.
1935 
1936         See Also
1937         --------
1938         DataArray.set_index
1939         """
1940         coords, _ = split_indexes(
1941             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
1942         )
1943         return self._replace(coords=coords)
1944 
1945     def reorder_levels(
1946         self,
1947         dim_order: Mapping[Hashable, Sequence[int]] = None,
1948         **dim_order_kwargs: Sequence[int],
1949     ) -> "DataArray":
1950         """Rearrange index levels using input order.
1951 
1952         Parameters
1953         ----------
1954         dim_order : optional
1955             Mapping from names matching dimensions and values given
1956             by lists representing new level orders. Every given dimension
1957             must have a multi-index.
1958         **dim_order_kwargs : optional
1959             The keyword arguments form of ``dim_order``.
1960             One of dim_order or dim_order_kwargs must be provided.
1961 
1962         Returns
1963         -------
1964         obj : DataArray
1965             Another dataarray, with this dataarray's data but replaced
1966             coordinates.
1967         """
1968         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
1969         replace_coords = {}
1970         for dim, order in dim_order.items():
1971             coord = self._coords[dim]
1972             index = coord.to_index()
1973             if not isinstance(index, pd.MultiIndex):
1974                 raise ValueError("coordinate %r has no MultiIndex" % dim)
1975             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
1976         coords = self._coords.copy()
1977         coords.update(replace_coords)
1978         return self._replace(coords=coords)
1979 
1980     def stack(
1981         self,
1982         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
1983         **dimensions_kwargs: Sequence[Hashable],
1984     ) -> "DataArray":
1985         """
1986         Stack any number of existing dimensions into a single new dimension.
1987 
1988         New dimensions will be added at the end, and the corresponding
1989         coordinate variables will be combined into a MultiIndex.
1990 
1991         Parameters
1992         ----------
1993         dimensions : mapping of hashable to sequence of hashable
1994             Mapping of the form `new_name=(dim1, dim2, ...)`.
1995             Names of new dimensions, and the existing dimensions that they
1996             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
1997             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
1998             all dimensions.
1999         **dimensions_kwargs
2000             The keyword arguments form of ``dimensions``.
2001             One of dimensions or dimensions_kwargs must be provided.
2002 
2003         Returns
2004         -------
2005         stacked : DataArray
2006             DataArray with stacked data.
2007 
2008         Examples
2009         --------
2010         >>> arr = xr.DataArray(
2011         ...     np.arange(6).reshape(2, 3),
2012         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2013         ... )
2014         >>> arr
2015         <xarray.DataArray (x: 2, y: 3)>
2016         array([[0, 1, 2],
2017                [3, 4, 5]])
2018         Coordinates:
2019           * x        (x) <U1 'a' 'b'
2020           * y        (y) int64 0 1 2
2021         >>> stacked = arr.stack(z=("x", "y"))
2022         >>> stacked.indexes["z"]
2023         MultiIndex([('a', 0),
2024                     ('a', 1),
2025                     ('a', 2),
2026                     ('b', 0),
2027                     ('b', 1),
2028                     ('b', 2)],
2029                    names=['x', 'y'])
2030 
2031         See Also
2032         --------
2033         DataArray.unstack
2034         """
2035         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
2036         return self._from_temp_dataset(ds)
2037 
2038     def unstack(
2039         self,
2040         dim: Union[Hashable, Sequence[Hashable], None] = None,
2041         fill_value: Any = dtypes.NA,
2042         sparse: bool = False,
2043     ) -> "DataArray":
2044         """
2045         Unstack existing dimensions corresponding to MultiIndexes into
2046         multiple new dimensions.
2047 
2048         New dimensions will be added at the end.
2049 
2050         Parameters
2051         ----------
2052         dim : hashable or sequence of hashable, optional
2053             Dimension(s) over which to unstack. By default unstacks all
2054             MultiIndexes.
2055         fill_value : scalar or dict-like, default: nan
2056             value to be filled. If a dict-like, maps variable names to
2057             fill values. Use the data array's name to refer to its
2058             name. If not provided or if the dict-like does not contain
2059             all variables, the dtype's NA value will be used.
2060         sparse : bool, default: False
2061             use sparse-array if True
2062 
2063         Returns
2064         -------
2065         unstacked : DataArray
2066             Array with unstacked data.
2067 
2068         Examples
2069         --------
2070         >>> arr = xr.DataArray(
2071         ...     np.arange(6).reshape(2, 3),
2072         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2073         ... )
2074         >>> arr
2075         <xarray.DataArray (x: 2, y: 3)>
2076         array([[0, 1, 2],
2077                [3, 4, 5]])
2078         Coordinates:
2079           * x        (x) <U1 'a' 'b'
2080           * y        (y) int64 0 1 2
2081         >>> stacked = arr.stack(z=("x", "y"))
2082         >>> stacked.indexes["z"]
2083         MultiIndex([('a', 0),
2084                     ('a', 1),
2085                     ('a', 2),
2086                     ('b', 0),
2087                     ('b', 1),
2088                     ('b', 2)],
2089                    names=['x', 'y'])
2090         >>> roundtripped = stacked.unstack()
2091         >>> arr.identical(roundtripped)
2092         True
2093 
2094         See Also
2095         --------
2096         DataArray.stack
2097         """
2098         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2099         return self._from_temp_dataset(ds)
2100 
2101     def to_unstacked_dataset(self, dim, level=0):
2102         """Unstack DataArray expanding to Dataset along a given level of a
2103         stacked coordinate.
2104 
2105         This is the inverse operation of Dataset.to_stacked_array.
2106 
2107         Parameters
2108         ----------
2109         dim : str
2110             Name of existing dimension to unstack
2111         level : int or str
2112             The MultiIndex level to expand to a dataset along. Can either be
2113             the integer index of the level or its name.
2114         label : int, default: 0
2115             Label of the level to expand dataset along. Overrides the label
2116             argument if given.
2117 
2118         Returns
2119         -------
2120         unstacked: Dataset
2121 
2122         Examples
2123         --------
2124         >>> import xarray as xr
2125         >>> arr = xr.DataArray(
2126         ...     np.arange(6).reshape(2, 3),
2127         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2128         ... )
2129         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2130         >>> data
2131         <xarray.Dataset>
2132         Dimensions:  (x: 2, y: 3)
2133         Coordinates:
2134           * x        (x) <U1 'a' 'b'
2135           * y        (y) int64 0 1 2
2136         Data variables:
2137             a        (x, y) int64 0 1 2 3 4 5
2138             b        (x) int64 0 3
2139         >>> stacked = data.to_stacked_array("z", ["x"])
2140         >>> stacked.indexes["z"]
2141         MultiIndex([('a', 0.0),
2142                     ('a', 1.0),
2143                     ('a', 2.0),
2144                     ('b', nan)],
2145                    names=['variable', 'y'])
2146         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2147         >>> data.identical(roundtripped)
2148         True
2149 
2150         See Also
2151         --------
2152         Dataset.to_stacked_array
2153         """
2154 
2155         idx = self.indexes[dim]
2156         if not isinstance(idx, pd.MultiIndex):
2157             raise ValueError(f"'{dim}' is not a stacked coordinate")
2158 
2159         level_number = idx._get_level_number(level)
2160         variables = idx.levels[level_number]
2161         variable_dim = idx.names[level_number]
2162 
2163         # pull variables out of datarray
2164         data_dict = {}
2165         for k in variables:
2166             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2167 
2168         # unstacked dataset
2169         return Dataset(data_dict)
2170 
2171     def transpose(
2172         self,
2173         *dims: Hashable,
2174         transpose_coords: bool = True,
2175         missing_dims: str = "raise",
2176     ) -> "DataArray":
2177         """Return a new DataArray object with transposed dimensions.
2178 
2179         Parameters
2180         ----------
2181         *dims : hashable, optional
2182             By default, reverse the dimensions. Otherwise, reorder the
2183             dimensions to this order.
2184         transpose_coords : bool, default: True
2185             If True, also transpose the coordinates of this DataArray.
2186         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2187             What to do if dimensions that should be selected from are not present in the
2188             DataArray:
2189             - "raise": raise an exception
2190             - "warning": raise a warning, and ignore the missing dimensions
2191             - "ignore": ignore the missing dimensions
2192 
2193         Returns
2194         -------
2195         transposed : DataArray
2196             The returned DataArray's array is transposed.
2197 
2198         Notes
2199         -----
2200         This operation returns a view of this array's data. It is
2201         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2202         -- the data will be fully loaded.
2203 
2204         See Also
2205         --------
2206         numpy.transpose
2207         Dataset.transpose
2208         """
2209         if dims:
2210             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2211         variable = self.variable.transpose(*dims)
2212         if transpose_coords:
2213             coords: Dict[Hashable, Variable] = {}
2214             for name, coord in self.coords.items():
2215                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2216                 coords[name] = coord.variable.transpose(*coord_dims)
2217             return self._replace(variable, coords)
2218         else:
2219             return self._replace(variable)
2220 
2221     @property
2222     def T(self) -> "DataArray":
2223         return self.transpose()
2224 
2225     def drop_vars(
2226         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2227     ) -> "DataArray":
2228         """Returns an array with dropped variables.
2229 
2230         Parameters
2231         ----------
2232         names : hashable or iterable of hashable
2233             Name(s) of variables to drop.
2234         errors : {"raise", "ignore"}, optional
2235             If 'raise' (default), raises a ValueError error if any of the variable
2236             passed are not in the dataset. If 'ignore', any given names that are in the
2237             DataArray are dropped and no error is raised.
2238 
2239         Returns
2240         -------
2241         dropped : Dataset
2242             New Dataset copied from `self` with variables removed.
2243         """
2244         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2245         return self._from_temp_dataset(ds)
2246 
2247     def drop(
2248         self,
2249         labels: Mapping = None,
2250         dim: Hashable = None,
2251         *,
2252         errors: str = "raise",
2253         **labels_kwargs,
2254     ) -> "DataArray":
2255         """Backward compatible method based on `drop_vars` and `drop_sel`
2256 
2257         Using either `drop_vars` or `drop_sel` is encouraged
2258 
2259         See Also
2260         --------
2261         DataArray.drop_vars
2262         DataArray.drop_sel
2263         """
2264         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2265         return self._from_temp_dataset(ds)
2266 
2267     def drop_sel(
2268         self,
2269         labels: Mapping[Hashable, Any] = None,
2270         *,
2271         errors: str = "raise",
2272         **labels_kwargs,
2273     ) -> "DataArray":
2274         """Drop index labels from this DataArray.
2275 
2276         Parameters
2277         ----------
2278         labels : mapping of hashable to Any
2279             Index labels to drop
2280         errors : {"raise", "ignore"}, optional
2281             If 'raise' (default), raises a ValueError error if
2282             any of the index labels passed are not
2283             in the dataset. If 'ignore', any given labels that are in the
2284             dataset are dropped and no error is raised.
2285         **labels_kwargs : {dim: label, ...}, optional
2286             The keyword arguments form of ``dim`` and ``labels``
2287 
2288         Returns
2289         -------
2290         dropped : DataArray
2291         """
2292         if labels_kwargs or isinstance(labels, dict):
2293             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2294 
2295         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2296         return self._from_temp_dataset(ds)
2297 
2298     def drop_isel(self, indexers=None, **indexers_kwargs):
2299         """Drop index positions from this DataArray.
2300 
2301         Parameters
2302         ----------
2303         indexers : mapping of hashable to Any
2304             Index locations to drop
2305         **indexers_kwargs : {dim: position, ...}, optional
2306             The keyword arguments form of ``dim`` and ``positions``
2307 
2308         Returns
2309         -------
2310         dropped : DataArray
2311 
2312         Raises
2313         ------
2314         IndexError
2315         """
2316         dataset = self._to_temp_dataset()
2317         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2318         return self._from_temp_dataset(dataset)
2319 
2320     def dropna(
2321         self, dim: Hashable, how: str = "any", thresh: int = None
2322     ) -> "DataArray":
2323         """Returns a new array with dropped labels for missing values along
2324         the provided dimension.
2325 
2326         Parameters
2327         ----------
2328         dim : hashable
2329             Dimension along which to drop missing values. Dropping along
2330             multiple dimensions simultaneously is not yet supported.
2331         how : {"any", "all"}, optional
2332             * any : if any NA values are present, drop that label
2333             * all : if all values are NA, drop that label
2334         thresh : int, default: None
2335             If supplied, require this many non-NA values.
2336 
2337         Returns
2338         -------
2339         DataArray
2340         """
2341         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2342         return self._from_temp_dataset(ds)
2343 
2344     def fillna(self, value: Any) -> "DataArray":
2345         """Fill missing values in this object.
2346 
2347         This operation follows the normal broadcasting and alignment rules that
2348         xarray uses for binary arithmetic, except the result is aligned to this
2349         object (``join='left'``) instead of aligned to the intersection of
2350         index coordinates (``join='inner'``).
2351 
2352         Parameters
2353         ----------
2354         value : scalar, ndarray or DataArray
2355             Used to fill all matching missing values in this array. If the
2356             argument is a DataArray, it is first aligned with (reindexed to)
2357             this array.
2358 
2359         Returns
2360         -------
2361         DataArray
2362         """
2363         if utils.is_dict_like(value):
2364             raise TypeError(
2365                 "cannot provide fill value as a dictionary with "
2366                 "fillna on a DataArray"
2367             )
2368         out = ops.fillna(self, value)
2369         return out
2370 
2371     def interpolate_na(
2372         self,
2373         dim: Hashable = None,
2374         method: str = "linear",
2375         limit: int = None,
2376         use_coordinate: Union[bool, str] = True,
2377         max_gap: Union[
2378             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2379         ] = None,
2380         keep_attrs: bool = None,
2381         **kwargs: Any,
2382     ) -> "DataArray":
2383         """Fill in NaNs by interpolating according to different methods.
2384 
2385         Parameters
2386         ----------
2387         dim : str
2388             Specifies the dimension along which to interpolate.
2389         method : str, optional
2390             String indicating which method to use for interpolation:
2391 
2392             - 'linear': linear interpolation (Default). Additional keyword
2393               arguments are passed to :py:func:`numpy.interp`
2394             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2395               are passed to :py:func:`scipy.interpolate.interp1d`. If
2396               ``method='polynomial'``, the ``order`` keyword argument must also be
2397               provided.
2398             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2399               respective :py:class:`scipy.interpolate` classes.
2400         use_coordinate : bool or str, default: True
2401             Specifies which index to use as the x values in the interpolation
2402             formulated as `y = f(x)`. If False, values are treated as if
2403             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2404             used. If ``use_coordinate`` is a string, it specifies the name of a
2405             coordinate variariable to use as the index.
2406         limit : int, default: None
2407             Maximum number of consecutive NaNs to fill. Must be greater than 0
2408             or None for no limit. This filling is done regardless of the size of
2409             the gap in the data. To only interpolate over gaps less than a given length,
2410             see ``max_gap``.
2411         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2412             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2413             Use None for no limit. When interpolating along a datetime64 dimension
2414             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2415 
2416             - a string that is valid input for pandas.to_timedelta
2417             - a :py:class:`numpy.timedelta64` object
2418             - a :py:class:`pandas.Timedelta` object
2419             - a :py:class:`datetime.timedelta` object
2420 
2421             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2422             dimensions has not been implemented yet. Gap length is defined as the difference
2423             between coordinate values at the first data point after a gap and the last value
2424             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2425             between coordinate values at the first (last) valid data point and the first (last) NaN.
2426             For example, consider::
2427 
2428                 <xarray.DataArray (x: 9)>
2429                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2430                 Coordinates:
2431                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2432 
2433             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2434         keep_attrs : bool, default: True
2435             If True, the dataarray's attributes (`attrs`) will be copied from
2436             the original object to the new one.  If False, the new
2437             object will be returned without attributes.
2438         kwargs : dict, optional
2439             parameters passed verbatim to the underlying interpolation function
2440 
2441         Returns
2442         -------
2443         interpolated: DataArray
2444             Filled in DataArray.
2445 
2446         See Also
2447         --------
2448         numpy.interp
2449         scipy.interpolate
2450 
2451         Examples
2452         --------
2453         >>> da = xr.DataArray(
2454         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2455         ... )
2456         >>> da
2457         <xarray.DataArray (x: 5)>
2458         array([nan,  2.,  3., nan,  0.])
2459         Coordinates:
2460           * x        (x) int64 0 1 2 3 4
2461 
2462         >>> da.interpolate_na(dim="x", method="linear")
2463         <xarray.DataArray (x: 5)>
2464         array([nan, 2. , 3. , 1.5, 0. ])
2465         Coordinates:
2466           * x        (x) int64 0 1 2 3 4
2467 
2468         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2469         <xarray.DataArray (x: 5)>
2470         array([1. , 2. , 3. , 1.5, 0. ])
2471         Coordinates:
2472           * x        (x) int64 0 1 2 3 4
2473         """
2474         from .missing import interp_na
2475 
2476         return interp_na(
2477             self,
2478             dim=dim,
2479             method=method,
2480             limit=limit,
2481             use_coordinate=use_coordinate,
2482             max_gap=max_gap,
2483             keep_attrs=keep_attrs,
2484             **kwargs,
2485         )
2486 
2487     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2488         """Fill NaN values by propogating values forward
2489 
2490         *Requires bottleneck.*
2491 
2492         Parameters
2493         ----------
2494         dim : hashable
2495             Specifies the dimension along which to propagate values when
2496             filling.
2497         limit : int, default: None
2498             The maximum number of consecutive NaN values to forward fill. In
2499             other words, if there is a gap with more than this number of
2500             consecutive NaNs, it will only be partially filled. Must be greater
2501             than 0 or None for no limit.
2502 
2503         Returns
2504         -------
2505         DataArray
2506         """
2507         from .missing import ffill
2508 
2509         return ffill(self, dim, limit=limit)
2510 
2511     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2512         """Fill NaN values by propogating values backward
2513 
2514         *Requires bottleneck.*
2515 
2516         Parameters
2517         ----------
2518         dim : str
2519             Specifies the dimension along which to propagate values when
2520             filling.
2521         limit : int, default: None
2522             The maximum number of consecutive NaN values to backward fill. In
2523             other words, if there is a gap with more than this number of
2524             consecutive NaNs, it will only be partially filled. Must be greater
2525             than 0 or None for no limit.
2526 
2527         Returns
2528         -------
2529         DataArray
2530         """
2531         from .missing import bfill
2532 
2533         return bfill(self, dim, limit=limit)
2534 
2535     def combine_first(self, other: "DataArray") -> "DataArray":
2536         """Combine two DataArray objects, with union of coordinates.
2537 
2538         This operation follows the normal broadcasting and alignment rules of
2539         ``join='outer'``.  Default to non-null values of array calling the
2540         method.  Use np.nan to fill in vacant cells after alignment.
2541 
2542         Parameters
2543         ----------
2544         other : DataArray
2545             Used to fill all matching missing values in this array.
2546 
2547         Returns
2548         -------
2549         DataArray
2550         """
2551         return ops.fillna(self, other, join="outer")
2552 
2553     def reduce(
2554         self,
2555         func: Callable[..., Any],
2556         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2557         axis: Union[None, int, Sequence[int]] = None,
2558         keep_attrs: bool = None,
2559         keepdims: bool = False,
2560         **kwargs: Any,
2561     ) -> "DataArray":
2562         """Reduce this array by applying `func` along some dimension(s).
2563 
2564         Parameters
2565         ----------
2566         func : callable
2567             Function which can be called in the form
2568             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2569             np.ndarray over an integer valued axis.
2570         dim : hashable or sequence of hashable, optional
2571             Dimension(s) over which to apply `func`.
2572         axis : int or sequence of int, optional
2573             Axis(es) over which to repeatedly apply `func`. Only one of the
2574             'dim' and 'axis' arguments can be supplied. If neither are
2575             supplied, then the reduction is calculated over the flattened array
2576             (by calling `f(x)` without an axis argument).
2577         keep_attrs : bool, optional
2578             If True, the variable's attributes (`attrs`) will be copied from
2579             the original object to the new one.  If False (default), the new
2580             object will be returned without attributes.
2581         keepdims : bool, default: False
2582             If True, the dimensions which are reduced are left in the result
2583             as dimensions of size one. Coordinates that use these dimensions
2584             are removed.
2585         **kwargs : dict
2586             Additional keyword arguments passed on to `func`.
2587 
2588         Returns
2589         -------
2590         reduced : DataArray
2591             DataArray with this object's array replaced with an array with
2592             summarized data and the indicated dimension(s) removed.
2593         """
2594 
2595         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2596         return self._replace_maybe_drop_dims(var)
2597 
2598     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2599         """Convert this array into a pandas object with the same shape.
2600 
2601         The type of the returned object depends on the number of DataArray
2602         dimensions:
2603 
2604         * 0D -> `xarray.DataArray`
2605         * 1D -> `pandas.Series`
2606         * 2D -> `pandas.DataFrame`
2607 
2608         Only works for arrays with 2 or fewer dimensions.
2609 
2610         The DataArray constructor performs the inverse transformation.
2611         """
2612         # TODO: consolidate the info about pandas constructors and the
2613         # attributes that correspond to their indexes into a separate module?
2614         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2615         try:
2616             constructor = constructors[self.ndim]
2617         except KeyError:
2618             raise ValueError(
2619                 "cannot convert arrays with %s dimensions into "
2620                 "pandas objects" % self.ndim
2621             )
2622         indexes = [self.get_index(dim) for dim in self.dims]
2623         return constructor(self.values, *indexes)
2624 
2625     def to_dataframe(
2626         self, name: Hashable = None, dim_order: List[Hashable] = None
2627     ) -> pd.DataFrame:
2628         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2629 
2630         The DataFrame is indexed by the Cartesian product of index coordinates
2631         (in the form of a :py:class:`pandas.MultiIndex`).
2632 
2633         Other coordinates are included as columns in the DataFrame.
2634 
2635         Parameters
2636         ----------
2637         name
2638             Name to give to this array (required if unnamed).
2639         dim_order
2640             Hierarchical dimension order for the resulting dataframe.
2641             Array content is transposed to this order and then written out as flat
2642             vectors in contiguous order, so the last dimension in this list
2643             will be contiguous in the resulting DataFrame. This has a major
2644             influence on which operations are efficient on the resulting
2645             dataframe.
2646 
2647             If provided, must include all dimensions of this DataArray. By default,
2648             dimensions are sorted according to the DataArray dimensions order.
2649 
2650         Returns
2651         -------
2652         result
2653             DataArray as a pandas DataFrame.
2654 
2655         """
2656         if name is None:
2657             name = self.name
2658         if name is None:
2659             raise ValueError(
2660                 "cannot convert an unnamed DataArray to a "
2661                 "DataFrame: use the ``name`` parameter"
2662             )
2663         if self.ndim == 0:
2664             raise ValueError("cannot convert a scalar to a DataFrame")
2665 
2666         # By using a unique name, we can convert a DataArray into a DataFrame
2667         # even if it shares a name with one of its coordinates.
2668         # I would normally use unique_name = object() but that results in a
2669         # dataframe with columns in the wrong order, for reasons I have not
2670         # been able to debug (possibly a pandas bug?).
2671         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2672         ds = self._to_dataset_whole(name=unique_name)
2673 
2674         if dim_order is None:
2675             ordered_dims = dict(zip(self.dims, self.shape))
2676         else:
2677             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2678 
2679         df = ds._to_dataframe(ordered_dims)
2680         df.columns = [name if c == unique_name else c for c in df.columns]
2681         return df
2682 
2683     def to_series(self) -> pd.Series:
2684         """Convert this array into a pandas.Series.
2685 
2686         The Series is indexed by the Cartesian product of index coordinates
2687         (in the form of a :py:class:`pandas.MultiIndex`).
2688         """
2689         index = self.coords.to_index()
2690         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2691 
2692     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2693         """Convert this array into a numpy.ma.MaskedArray
2694 
2695         Parameters
2696         ----------
2697         copy : bool, default: True
2698             If True make a copy of the array in the result. If False,
2699             a MaskedArray view of DataArray.values is returned.
2700 
2701         Returns
2702         -------
2703         result : MaskedArray
2704             Masked where invalid values (nan or inf) occur.
2705         """
2706         values = self.values  # only compute lazy arrays once
2707         isnull = pd.isnull(values)
2708         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2709 
2710     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2711         """Write DataArray contents to a netCDF file.
2712 
2713         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2714 
2715         Notes
2716         -----
2717         Only xarray.Dataset objects can be written to netCDF files, so
2718         the xarray.DataArray is converted to a xarray.Dataset object
2719         containing a single variable. If the DataArray has no name, or if the
2720         name is the same as a coordinate name, then it is given the name
2721         ``"__xarray_dataarray_variable__"``.
2722 
2723         See Also
2724         --------
2725         Dataset.to_netcdf
2726         """
2727         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2728 
2729         if self.name is None:
2730             # If no name is set then use a generic xarray name
2731             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2732         elif self.name in self.coords or self.name in self.dims:
2733             # The name is the same as one of the coords names, which netCDF
2734             # doesn't support, so rename it but keep track of the old name
2735             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2736             dataset.attrs[DATAARRAY_NAME] = self.name
2737         else:
2738             # No problems with the name - so we're fine!
2739             dataset = self.to_dataset()
2740 
2741         return dataset.to_netcdf(*args, **kwargs)
2742 
2743     def to_dict(self, data: bool = True) -> dict:
2744         """
2745         Convert this xarray.DataArray into a dictionary following xarray
2746         naming conventions.
2747 
2748         Converts all variables and attributes to native Python objects.
2749         Useful for converting to json. To avoid datetime incompatibility
2750         use decode_times=False kwarg in xarray.open_dataset.
2751 
2752         Parameters
2753         ----------
2754         data : bool, optional
2755             Whether to include the actual data in the dictionary. When set to
2756             False, returns just the schema.
2757 
2758         See Also
2759         --------
2760         DataArray.from_dict
2761         """
2762         d = self.variable.to_dict(data=data)
2763         d.update({"coords": {}, "name": self.name})
2764         for k in self.coords:
2765             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2766         return d
2767 
2768     @classmethod
2769     def from_dict(cls, d: dict) -> "DataArray":
2770         """
2771         Convert a dictionary into an xarray.DataArray
2772 
2773         Input dict can take several forms:
2774 
2775         .. code:: python
2776 
2777             d = {"dims": ("t"), "data": x}
2778 
2779             d = {
2780                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2781                 "attrs": {"title": "air temperature"},
2782                 "dims": "t",
2783                 "data": x,
2784                 "name": "a",
2785             }
2786 
2787         where "t" is the name of the dimesion, "a" is the name of the array,
2788         and x and t are lists, numpy.arrays, or pandas objects.
2789 
2790         Parameters
2791         ----------
2792         d : dict
2793             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2794 
2795         Returns
2796         -------
2797         obj : xarray.DataArray
2798 
2799         See Also
2800         --------
2801         DataArray.to_dict
2802         Dataset.from_dict
2803         """
2804         coords = None
2805         if "coords" in d:
2806             try:
2807                 coords = {
2808                     k: (v["dims"], v["data"], v.get("attrs"))
2809                     for k, v in d["coords"].items()
2810                 }
2811             except KeyError as e:
2812                 raise ValueError(
2813                     "cannot convert dict when coords are missing the key "
2814                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2815                 )
2816         try:
2817             data = d["data"]
2818         except KeyError:
2819             raise ValueError("cannot convert dict without the key 'data''")
2820         else:
2821             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2822         return obj
2823 
2824     @classmethod
2825     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2826         """Convert a pandas.Series into an xarray.DataArray.
2827 
2828         If the series's index is a MultiIndex, it will be expanded into a
2829         tensor product of one-dimensional coordinates (filling in missing
2830         values with NaN). Thus this operation should be the inverse of the
2831         `to_series` method.
2832 
2833         If sparse=True, creates a sparse array instead of a dense NumPy array.
2834         Requires the pydata/sparse package.
2835 
2836         See Also
2837         --------
2838         xarray.Dataset.from_dataframe
2839         """
2840         temp_name = "__temporary_name"
2841         df = pd.DataFrame({temp_name: series})
2842         ds = Dataset.from_dataframe(df, sparse=sparse)
2843         result = cast(DataArray, ds[temp_name])
2844         result.name = series.name
2845         return result
2846 
2847     def to_cdms2(self) -> "cdms2_Variable":
2848         """Convert this array into a cdms2.Variable"""
2849         from ..convert import to_cdms2
2850 
2851         return to_cdms2(self)
2852 
2853     @classmethod
2854     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2855         """Convert a cdms2.Variable into an xarray.DataArray"""
2856         from ..convert import from_cdms2
2857 
2858         return from_cdms2(variable)
2859 
2860     def to_iris(self) -> "iris_Cube":
2861         """Convert this array into a iris.cube.Cube"""
2862         from ..convert import to_iris
2863 
2864         return to_iris(self)
2865 
2866     @classmethod
2867     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2868         """Convert a iris.cube.Cube into an xarray.DataArray"""
2869         from ..convert import from_iris
2870 
2871         return from_iris(cube)
2872 
2873     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2874         """Helper function for equals, broadcast_equals, and identical"""
2875 
2876         def compat(x, y):
2877             return getattr(x.variable, compat_str)(y.variable)
2878 
2879         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2880             self, other
2881         )
2882 
2883     def broadcast_equals(self, other: "DataArray") -> bool:
2884         """Two DataArrays are broadcast equal if they are equal after
2885         broadcasting them against each other such that they have the same
2886         dimensions.
2887 
2888         See Also
2889         --------
2890         DataArray.equals
2891         DataArray.identical
2892         """
2893         try:
2894             return self._all_compat(other, "broadcast_equals")
2895         except (TypeError, AttributeError):
2896             return False
2897 
2898     def equals(self, other: "DataArray") -> bool:
2899         """True if two DataArrays have the same dimensions, coordinates and
2900         values; otherwise False.
2901 
2902         DataArrays can still be equal (like pandas objects) if they have NaN
2903         values in the same locations.
2904 
2905         This method is necessary because `v1 == v2` for ``DataArray``
2906         does element-wise comparisons (like numpy.ndarrays).
2907 
2908         See Also
2909         --------
2910         DataArray.broadcast_equals
2911         DataArray.identical
2912         """
2913         try:
2914             return self._all_compat(other, "equals")
2915         except (TypeError, AttributeError):
2916             return False
2917 
2918     def identical(self, other: "DataArray") -> bool:
2919         """Like equals, but also checks the array name and attributes, and
2920         attributes on all coordinates.
2921 
2922         See Also
2923         --------
2924         DataArray.broadcast_equals
2925         DataArray.equals
2926         """
2927         try:
2928             return self.name == other.name and self._all_compat(other, "identical")
2929         except (TypeError, AttributeError):
2930             return False
2931 
2932     def _result_name(self, other: Any = None) -> Optional[Hashable]:
2933         # use the same naming heuristics as pandas:
2934         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
2935         other_name = getattr(other, "name", _default)
2936         if other_name is _default or other_name == self.name:
2937             return self.name
2938         else:
2939             return None
2940 
2941     def __array_wrap__(self, obj, context=None) -> "DataArray":
2942         new_var = self.variable.__array_wrap__(obj, context)
2943         return self._replace(new_var)
2944 
2945     def __matmul__(self, obj):
2946         return self.dot(obj)
2947 
2948     def __rmatmul__(self, other):
2949         # currently somewhat duplicative, as only other DataArrays are
2950         # compatible with matmul
2951         return computation.dot(other, self)
2952 
2953     @staticmethod
2954     def _unary_op(f: Callable[..., Any]) -> Callable[..., "DataArray"]:
2955         @functools.wraps(f)
2956         def func(self, *args, **kwargs):
2957             keep_attrs = kwargs.pop("keep_attrs", None)
2958             if keep_attrs is None:
2959                 keep_attrs = _get_keep_attrs(default=True)
2960             with warnings.catch_warnings():
2961                 warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
2962                 warnings.filterwarnings(
2963                     "ignore", r"Mean of empty slice", category=RuntimeWarning
2964                 )
2965                 with np.errstate(all="ignore"):
2966                     da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
2967                 if keep_attrs:
2968                     da.attrs = self.attrs
2969                 return da
2970 
2971         return func
2972 
2973     @staticmethod
2974     def _binary_op(
2975         f: Callable[..., Any],
2976         reflexive: bool = False,
2977         join: str = None,  # see xarray.align
2978         **ignored_kwargs,
2979     ) -> Callable[..., "DataArray"]:
2980         @functools.wraps(f)
2981         def func(self, other):
2982             if isinstance(other, (Dataset, groupby.GroupBy)):
2983                 return NotImplemented
2984             if isinstance(other, DataArray):
2985                 align_type = OPTIONS["arithmetic_join"] if join is None else join
2986                 self, other = align(self, other, join=align_type, copy=False)
2987             other_variable = getattr(other, "variable", other)
2988             other_coords = getattr(other, "coords", None)
2989 
2990             variable = (
2991                 f(self.variable, other_variable)
2992                 if not reflexive
2993                 else f(other_variable, self.variable)
2994             )
2995             coords, indexes = self.coords._merge_raw(other_coords)
2996             name = self._result_name(other)
2997 
2998             return self._replace(variable, coords, name, indexes=indexes)
2999 
3000         return func
3001 
3002     @staticmethod
3003     def _inplace_binary_op(f: Callable) -> Callable[..., "DataArray"]:
3004         @functools.wraps(f)
3005         def func(self, other):
3006             if isinstance(other, groupby.GroupBy):
3007                 raise TypeError(
3008                     "in-place operations between a DataArray and "
3009                     "a grouped object are not permitted"
3010                 )
3011             # n.b. we can't align other to self (with other.reindex_like(self))
3012             # because `other` may be converted into floats, which would cause
3013             # in-place arithmetic to fail unpredictably. Instead, we simply
3014             # don't support automatic alignment with in-place arithmetic.
3015             other_coords = getattr(other, "coords", None)
3016             other_variable = getattr(other, "variable", other)
3017             try:
3018                 with self.coords._merge_inplace(other_coords):
3019                     f(self.variable, other_variable)
3020             except MergeError as exc:
3021                 raise MergeError(
3022                     "Automatic alignment is not supported for in-place operations.\n"
3023                     "Consider aligning the indices manually or using a not-in-place operation.\n"
3024                     "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3025                 ) from exc
3026             return self
3027 
3028         return func
3029 
3030     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
3031         self.attrs = other.attrs
3032 
3033     plot = utils.UncachedAccessor(_PlotMethods)
3034 
3035     def _title_for_slice(self, truncate: int = 50) -> str:
3036         """
3037         If the dataarray has 1 dimensional coordinates or comes from a slice
3038         we can show that info in the title
3039 
3040         Parameters
3041         ----------
3042         truncate : int, default: 50
3043             maximum number of characters for title
3044 
3045         Returns
3046         -------
3047         title : string
3048             Can be used for plot titles
3049 
3050         """
3051         one_dims = []
3052         for dim, coord in self.coords.items():
3053             if coord.size == 1:
3054                 one_dims.append(
3055                     "{dim} = {v}".format(dim=dim, v=format_item(coord.values))
3056                 )
3057 
3058         title = ", ".join(one_dims)
3059         if len(title) > truncate:
3060             title = title[: (truncate - 3)] + "..."
3061 
3062         return title
3063 
3064     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
3065         """Calculate the n-th order discrete difference along given axis.
3066 
3067         Parameters
3068         ----------
3069         dim : hashable
3070             Dimension over which to calculate the finite difference.
3071         n : int, optional
3072             The number of times values are differenced.
3073         label : hashable, optional
3074             The new coordinate in dimension ``dim`` will have the
3075             values of either the minuend's or subtrahend's coordinate
3076             for values 'upper' and 'lower', respectively.  Other
3077             values are not supported.
3078 
3079         Returns
3080         -------
3081         difference : same type as caller
3082             The n-th order finite difference of this object.
3083 
3084         Notes
3085         -----
3086         `n` matches numpy's behavior and is different from pandas' first argument named
3087         `periods`.
3088 
3089         Examples
3090         --------
3091         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3092         >>> arr.diff("x")
3093         <xarray.DataArray (x: 3)>
3094         array([0, 1, 0])
3095         Coordinates:
3096           * x        (x) int64 2 3 4
3097         >>> arr.diff("x", 2)
3098         <xarray.DataArray (x: 2)>
3099         array([ 1, -1])
3100         Coordinates:
3101           * x        (x) int64 3 4
3102 
3103         See Also
3104         --------
3105         DataArray.differentiate
3106         """
3107         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3108         return self._from_temp_dataset(ds)
3109 
3110     def shift(
3111         self,
3112         shifts: Mapping[Hashable, int] = None,
3113         fill_value: Any = dtypes.NA,
3114         **shifts_kwargs: int,
3115     ) -> "DataArray":
3116         """Shift this array by an offset along one or more dimensions.
3117 
3118         Only the data is moved; coordinates stay in place. Values shifted from
3119         beyond array bounds are replaced by NaN. This is consistent with the
3120         behavior of ``shift`` in pandas.
3121 
3122         Parameters
3123         ----------
3124         shifts : mapping of hashable to int, optional
3125             Integer offset to shift along each of the given dimensions.
3126             Positive offsets shift to the right; negative offsets shift to the
3127             left.
3128         fill_value : scalar, optional
3129             Value to use for newly missing values
3130         **shifts_kwargs
3131             The keyword arguments form of ``shifts``.
3132             One of shifts or shifts_kwargs must be provided.
3133 
3134         Returns
3135         -------
3136         shifted : DataArray
3137             DataArray with the same coordinates and attributes but shifted
3138             data.
3139 
3140         See Also
3141         --------
3142         roll
3143 
3144         Examples
3145         --------
3146         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3147         >>> arr.shift(x=1)
3148         <xarray.DataArray (x: 3)>
3149         array([nan,  5.,  6.])
3150         Dimensions without coordinates: x
3151         """
3152         variable = self.variable.shift(
3153             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3154         )
3155         return self._replace(variable=variable)
3156 
3157     def roll(
3158         self,
3159         shifts: Mapping[Hashable, int] = None,
3160         roll_coords: bool = None,
3161         **shifts_kwargs: int,
3162     ) -> "DataArray":
3163         """Roll this array by an offset along one or more dimensions.
3164 
3165         Unlike shift, roll may rotate all variables, including coordinates
3166         if specified. The direction of rotation is consistent with
3167         :py:func:`numpy.roll`.
3168 
3169         Parameters
3170         ----------
3171         shifts : mapping of hashable to int, optional
3172             Integer offset to rotate each of the given dimensions.
3173             Positive offsets roll to the right; negative offsets roll to the
3174             left.
3175         roll_coords : bool
3176             Indicates whether to roll the coordinates by the offset
3177             The current default of roll_coords (None, equivalent to True) is
3178             deprecated and will change to False in a future version.
3179             Explicitly pass roll_coords to silence the warning.
3180         **shifts_kwargs
3181             The keyword arguments form of ``shifts``.
3182             One of shifts or shifts_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         rolled : DataArray
3187             DataArray with the same attributes but rolled data and coordinates.
3188 
3189         See Also
3190         --------
3191         shift
3192 
3193         Examples
3194         --------
3195         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3196         >>> arr.roll(x=1)
3197         <xarray.DataArray (x: 3)>
3198         array([7, 5, 6])
3199         Dimensions without coordinates: x
3200         """
3201         ds = self._to_temp_dataset().roll(
3202             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3203         )
3204         return self._from_temp_dataset(ds)
3205 
3206     @property
3207     def real(self) -> "DataArray":
3208         return self._replace(self.variable.real)
3209 
3210     @property
3211     def imag(self) -> "DataArray":
3212         return self._replace(self.variable.imag)
3213 
3214     def dot(
3215         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3216     ) -> "DataArray":
3217         """Perform dot product of two DataArrays along their shared dims.
3218 
3219         Equivalent to taking taking tensordot over all shared dims.
3220 
3221         Parameters
3222         ----------
3223         other : DataArray
3224             The other array with which the dot product is performed.
3225         dims : ..., hashable or sequence of hashable, optional
3226             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3227             If not specified, then all the common dimensions are summed over.
3228 
3229         Returns
3230         -------
3231         result : DataArray
3232             Array resulting from the dot product over all shared dimensions.
3233 
3234         See Also
3235         --------
3236         dot
3237         numpy.tensordot
3238 
3239         Examples
3240         --------
3241         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3242         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3243         >>> dm_vals = np.arange(4)
3244         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3245 
3246         >>> dm.dims
3247         ('z',)
3248 
3249         >>> da.dims
3250         ('x', 'y', 'z')
3251 
3252         >>> dot_result = da.dot(dm)
3253         >>> dot_result.dims
3254         ('x', 'y')
3255 
3256         """
3257         if isinstance(other, Dataset):
3258             raise NotImplementedError(
3259                 "dot products are not yet supported with Dataset objects."
3260             )
3261         if not isinstance(other, DataArray):
3262             raise TypeError("dot only operates on DataArrays.")
3263 
3264         return computation.dot(self, other, dims=dims)
3265 
3266     def sortby(
3267         self,
3268         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3269         ascending: bool = True,
3270     ) -> "DataArray":
3271         """Sort object by labels or values (along an axis).
3272 
3273         Sorts the dataarray, either along specified dimensions,
3274         or according to values of 1-D dataarrays that share dimension
3275         with calling object.
3276 
3277         If the input variables are dataarrays, then the dataarrays are aligned
3278         (via left-join) to the calling object prior to sorting by cell values.
3279         NaNs are sorted to the end, following Numpy convention.
3280 
3281         If multiple sorts along the same dimension is
3282         given, numpy's lexsort is performed along that dimension:
3283         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3284         and the FIRST key in the sequence is used as the primary sort key,
3285         followed by the 2nd key, etc.
3286 
3287         Parameters
3288         ----------
3289         variables : hashable, DataArray, or sequence of hashable or DataArray
3290             1D DataArray objects or name(s) of 1D variable(s) in
3291             coords whose values are used to sort this array.
3292         ascending : bool, optional
3293             Whether to sort by ascending or descending order.
3294 
3295         Returns
3296         -------
3297         sorted : DataArray
3298             A new dataarray where all the specified dims are sorted by dim
3299             labels.
3300 
3301         Examples
3302         --------
3303         >>> da = xr.DataArray(
3304         ...     np.random.rand(5),
3305         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3306         ...     dims="time",
3307         ... )
3308         >>> da
3309         <xarray.DataArray (time: 5)>
3310         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3311         Coordinates:
3312           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3313 
3314         >>> da.sortby(da)
3315         <xarray.DataArray (time: 5)>
3316         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3317         Coordinates:
3318           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3319         """
3320         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3321         return self._from_temp_dataset(ds)
3322 
3323     def quantile(
3324         self,
3325         q: Any,
3326         dim: Union[Hashable, Sequence[Hashable], None] = None,
3327         interpolation: str = "linear",
3328         keep_attrs: bool = None,
3329         skipna: bool = True,
3330     ) -> "DataArray":
3331         """Compute the qth quantile of the data along the specified dimension.
3332 
3333         Returns the qth quantiles(s) of the array elements.
3334 
3335         Parameters
3336         ----------
3337         q : float or array-like of float
3338             Quantile to compute, which must be between 0 and 1 inclusive.
3339         dim : hashable or sequence of hashable, optional
3340             Dimension(s) over which to apply quantile.
3341         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3342             This optional parameter specifies the interpolation method to
3343             use when the desired quantile lies between two data points
3344             ``i < j``:
3345 
3346                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3347                   the fractional part of the index surrounded by ``i`` and
3348                   ``j``.
3349                 - lower: ``i``.
3350                 - higher: ``j``.
3351                 - nearest: ``i`` or ``j``, whichever is nearest.
3352                 - midpoint: ``(i + j) / 2``.
3353         keep_attrs : bool, optional
3354             If True, the dataset's attributes (`attrs`) will be copied from
3355             the original object to the new one.  If False (default), the new
3356             object will be returned without attributes.
3357         skipna : bool, optional
3358             Whether to skip missing values when aggregating.
3359 
3360         Returns
3361         -------
3362         quantiles : DataArray
3363             If `q` is a single quantile, then the result
3364             is a scalar. If multiple percentiles are given, first axis of
3365             the result corresponds to the quantile and a quantile dimension
3366             is added to the return array. The other dimensions are the
3367             dimensions that remain after the reduction of the array.
3368 
3369         See Also
3370         --------
3371         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3372 
3373         Examples
3374         --------
3375         >>> da = xr.DataArray(
3376         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3377         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3378         ...     dims=("x", "y"),
3379         ... )
3380         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3381         <xarray.DataArray ()>
3382         array(0.7)
3383         Coordinates:
3384             quantile  float64 0.0
3385         >>> da.quantile(0, dim="x")
3386         <xarray.DataArray (y: 4)>
3387         array([0.7, 4.2, 2.6, 1.5])
3388         Coordinates:
3389           * y         (y) float64 1.0 1.5 2.0 2.5
3390             quantile  float64 0.0
3391         >>> da.quantile([0, 0.5, 1])
3392         <xarray.DataArray (quantile: 3)>
3393         array([0.7, 3.4, 9.4])
3394         Coordinates:
3395           * quantile  (quantile) float64 0.0 0.5 1.0
3396         >>> da.quantile([0, 0.5, 1], dim="x")
3397         <xarray.DataArray (quantile: 3, y: 4)>
3398         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3399                [3.6 , 5.75, 6.  , 1.7 ],
3400                [6.5 , 7.3 , 9.4 , 1.9 ]])
3401         Coordinates:
3402           * y         (y) float64 1.0 1.5 2.0 2.5
3403           * quantile  (quantile) float64 0.0 0.5 1.0
3404         """
3405 
3406         ds = self._to_temp_dataset().quantile(
3407             q,
3408             dim=dim,
3409             keep_attrs=keep_attrs,
3410             interpolation=interpolation,
3411             skipna=skipna,
3412         )
3413         return self._from_temp_dataset(ds)
3414 
3415     def rank(
3416         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3417     ) -> "DataArray":
3418         """Ranks the data.
3419 
3420         Equal values are assigned a rank that is the average of the ranks that
3421         would have been otherwise assigned to all of the values within that
3422         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3423 
3424         NaNs in the input array are returned as NaNs.
3425 
3426         The `bottleneck` library is required.
3427 
3428         Parameters
3429         ----------
3430         dim : hashable
3431             Dimension over which to compute rank.
3432         pct : bool, optional
3433             If True, compute percentage ranks, otherwise compute integer ranks.
3434         keep_attrs : bool, optional
3435             If True, the dataset's attributes (`attrs`) will be copied from
3436             the original object to the new one.  If False (default), the new
3437             object will be returned without attributes.
3438 
3439         Returns
3440         -------
3441         ranked : DataArray
3442             DataArray with the same coordinates and dtype 'float64'.
3443 
3444         Examples
3445         --------
3446         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3447         >>> arr.rank("x")
3448         <xarray.DataArray (x: 3)>
3449         array([1., 2., 3.])
3450         Dimensions without coordinates: x
3451         """
3452 
3453         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3454         return self._from_temp_dataset(ds)
3455 
3456     def differentiate(
3457         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3458     ) -> "DataArray":
3459         """ Differentiate the array with the second order accurate central
3460         differences.
3461 
3462         .. note::
3463             This feature is limited to simple cartesian geometry, i.e. coord
3464             must be one dimensional.
3465 
3466         Parameters
3467         ----------
3468         coord : hashable
3469             The coordinate to be used to compute the gradient.
3470         edge_order : {1, 2}, default: 1
3471             N-th order accurate differences at the boundaries.
3472         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3473                          "us", "ns", "ps", "fs", "as"} or None, optional
3474             Unit to compute gradient. Only valid for datetime coordinate.
3475 
3476         Returns
3477         -------
3478         differentiated: DataArray
3479 
3480         See also
3481         --------
3482         numpy.gradient: corresponding numpy function
3483 
3484         Examples
3485         --------
3486 
3487         >>> da = xr.DataArray(
3488         ...     np.arange(12).reshape(4, 3),
3489         ...     dims=["x", "y"],
3490         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3491         ... )
3492         >>> da
3493         <xarray.DataArray (x: 4, y: 3)>
3494         array([[ 0,  1,  2],
3495                [ 3,  4,  5],
3496                [ 6,  7,  8],
3497                [ 9, 10, 11]])
3498         Coordinates:
3499           * x        (x) float64 0.0 0.1 1.1 1.2
3500         Dimensions without coordinates: y
3501         >>>
3502         >>> da.differentiate("x")
3503         <xarray.DataArray (x: 4, y: 3)>
3504         array([[30.        , 30.        , 30.        ],
3505                [27.54545455, 27.54545455, 27.54545455],
3506                [27.54545455, 27.54545455, 27.54545455],
3507                [30.        , 30.        , 30.        ]])
3508         Coordinates:
3509           * x        (x) float64 0.0 0.1 1.1 1.2
3510         Dimensions without coordinates: y
3511         """
3512         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3513         return self._from_temp_dataset(ds)
3514 
3515     def integrate(
3516         self,
3517         coord: Union[Hashable, Sequence[Hashable]] = None,
3518         datetime_unit: str = None,
3519         *,
3520         dim: Union[Hashable, Sequence[Hashable]] = None,
3521     ) -> "DataArray":
3522         """Integrate along the given coordinate using the trapezoidal rule.
3523 
3524         .. note::
3525             This feature is limited to simple cartesian geometry, i.e. coord
3526             must be one dimensional.
3527 
3528         Parameters
3529         ----------
3530         coord : hashable, or sequence of hashable
3531             Coordinate(s) used for the integration.
3532         dim : hashable, or sequence of hashable
3533             Coordinate(s) used for the integration.
3534         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3535                         'ps', 'fs', 'as'}, optional
3536             Specify the unit if a datetime coordinate is used.
3537 
3538         Returns
3539         -------
3540         integrated : DataArray
3541 
3542         See also
3543         --------
3544         Dataset.integrate
3545         numpy.trapz : corresponding numpy function
3546 
3547         Examples
3548         --------
3549 
3550         >>> da = xr.DataArray(
3551         ...     np.arange(12).reshape(4, 3),
3552         ...     dims=["x", "y"],
3553         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3554         ... )
3555         >>> da
3556         <xarray.DataArray (x: 4, y: 3)>
3557         array([[ 0,  1,  2],
3558                [ 3,  4,  5],
3559                [ 6,  7,  8],
3560                [ 9, 10, 11]])
3561         Coordinates:
3562           * x        (x) float64 0.0 0.1 1.1 1.2
3563         Dimensions without coordinates: y
3564         >>>
3565         >>> da.integrate("x")
3566         <xarray.DataArray (y: 3)>
3567         array([5.4, 6.6, 7.8])
3568         Dimensions without coordinates: y
3569         """
3570         if dim is not None and coord is not None:
3571             raise ValueError(
3572                 "Cannot pass both 'dim' and 'coord'. Please pass only 'coord' instead."
3573             )
3574 
3575         if dim is not None and coord is None:
3576             coord = dim
3577             msg = (
3578                 "The `dim` keyword argument to `DataArray.integrate` is "
3579                 "being replaced with `coord`, for consistency with "
3580                 "`Dataset.integrate`. Please pass `coord` instead."
3581                 " `dim` will be removed in version 0.19.0."
3582             )
3583             warnings.warn(msg, FutureWarning, stacklevel=2)
3584 
3585         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
3586         return self._from_temp_dataset(ds)
3587 
3588     def unify_chunks(self) -> "DataArray":
3589         """Unify chunk size along all chunked dimensions of this DataArray.
3590 
3591         Returns
3592         -------
3593         DataArray with consistent chunk sizes for all dask-array variables
3594 
3595         See Also
3596         --------
3597         dask.array.core.unify_chunks
3598         """
3599         ds = self._to_temp_dataset().unify_chunks()
3600         return self._from_temp_dataset(ds)
3601 
3602     def map_blocks(
3603         self,
3604         func: "Callable[..., T_DSorDA]",
3605         args: Sequence[Any] = (),
3606         kwargs: Mapping[str, Any] = None,
3607         template: Union["DataArray", "Dataset"] = None,
3608     ) -> "T_DSorDA":
3609         """
3610         Apply a function to each block of this DataArray.
3611 
3612         .. warning::
3613             This method is experimental and its signature may change.
3614 
3615         Parameters
3616         ----------
3617         func : callable
3618             User-provided function that accepts a DataArray as its first
3619             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3620             corresponding to one chunk along each chunked dimension. ``func`` will be
3621             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3622 
3623             This function must return either a single DataArray or a single Dataset.
3624 
3625             This function cannot add a new chunked dimension.
3626         args : sequence
3627             Passed to func after unpacking and subsetting any xarray objects by blocks.
3628             xarray objects in args must be aligned with this object, otherwise an error is raised.
3629         kwargs : mapping
3630             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3631             subset to blocks. Passing dask collections in kwargs is not allowed.
3632         template : DataArray or Dataset, optional
3633             xarray object representing the final result after compute is called. If not provided,
3634             the function will be first run on mocked-up data, that looks like this object but
3635             has sizes 0, to determine properties of the returned object such as dtype,
3636             variable names, attributes, new dimensions and new indexes (if any).
3637             ``template`` must be provided if the function changes the size of existing dimensions.
3638             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3639             ``attrs`` set by ``func`` will be ignored.
3640 
3641         Returns
3642         -------
3643         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3644         function.
3645 
3646         Notes
3647         -----
3648         This function is designed for when ``func`` needs to manipulate a whole xarray object
3649         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
3650         recommended to use ``apply_ufunc``.
3651 
3652         If none of the variables in this object is backed by dask arrays, calling this function is
3653         equivalent to calling ``func(obj, *args, **kwargs)``.
3654 
3655         See Also
3656         --------
3657         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
3658         xarray.DataArray.map_blocks
3659 
3660         Examples
3661         --------
3662         Calculate an anomaly from climatology using ``.groupby()``. Using
3663         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3664         its indices, and its methods like ``.groupby()``.
3665 
3666         >>> def calculate_anomaly(da, groupby_type="time.month"):
3667         ...     gb = da.groupby(groupby_type)
3668         ...     clim = gb.mean(dim="time")
3669         ...     return gb - clim
3670         ...
3671         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3672         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3673         >>> np.random.seed(123)
3674         >>> array = xr.DataArray(
3675         ...     np.random.rand(len(time)),
3676         ...     dims=["time"],
3677         ...     coords={"time": time, "month": month},
3678         ... ).chunk()
3679         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3680         <xarray.DataArray (time: 24)>
3681         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3682                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3683                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3684                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3685                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3686         Coordinates:
3687           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3688             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3689 
3690         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3691         to the function being applied in ``xr.map_blocks()``:
3692 
3693         >>> array.map_blocks(
3694         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3695         ... )  # doctest: +ELLIPSIS
3696         <xarray.DataArray (time: 24)>
3697         dask.array<calculate_anomaly-...-<this, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3698         Coordinates:
3699           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3700             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3701         """
3702         from .parallel import map_blocks
3703 
3704         return map_blocks(func, self, args, kwargs, template)
3705 
3706     def polyfit(
3707         self,
3708         dim: Hashable,
3709         deg: int,
3710         skipna: bool = None,
3711         rcond: float = None,
3712         w: Union[Hashable, Any] = None,
3713         full: bool = False,
3714         cov: bool = False,
3715     ):
3716         """
3717         Least squares polynomial fit.
3718 
3719         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3720         invalid values when `skipna = True`.
3721 
3722         Parameters
3723         ----------
3724         dim : hashable
3725             Coordinate along which to fit the polynomials.
3726         deg : int
3727             Degree of the fitting polynomial.
3728         skipna : bool, optional
3729             If True, removes all invalid values before fitting each 1D slices of the array.
3730             Default is True if data is stored in a dask.array or if there is any
3731             invalid values, False otherwise.
3732         rcond : float, optional
3733             Relative condition number to the fit.
3734         w : hashable or array-like, optional
3735             Weights to apply to the y-coordinate of the sample points.
3736             Can be an array-like object or the name of a coordinate in the dataset.
3737         full : bool, optional
3738             Whether to return the residuals, matrix rank and singular values in addition
3739             to the coefficients.
3740         cov : bool or str, optional
3741             Whether to return to the covariance matrix in addition to the coefficients.
3742             The matrix is not scaled if `cov='unscaled'`.
3743 
3744         Returns
3745         -------
3746         polyfit_results : Dataset
3747             A single dataset which contains:
3748 
3749             polyfit_coefficients
3750                 The coefficients of the best fit.
3751             polyfit_residuals
3752                 The residuals of the least-square computation (only included if `full=True`).
3753                 When the matrix rank is deficient, np.nan is returned.
3754             [dim]_matrix_rank
3755                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3756             [dim]_singular_value
3757                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3758             polyfit_covariance
3759                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3760 
3761         See Also
3762         --------
3763         numpy.polyfit
3764         """
3765         return self._to_temp_dataset().polyfit(
3766             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3767         )
3768 
3769     def pad(
3770         self,
3771         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
3772         mode: str = "constant",
3773         stat_length: Union[
3774             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3775         ] = None,
3776         constant_values: Union[
3777             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3778         ] = None,
3779         end_values: Union[
3780             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
3781         ] = None,
3782         reflect_type: str = None,
3783         **pad_width_kwargs: Any,
3784     ) -> "DataArray":
3785         """Pad this array along one or more dimensions.
3786 
3787         .. warning::
3788             This function is experimental and its behaviour is likely to change
3789             especially regarding padding of dimension coordinates (or IndexVariables).
3790 
3791         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3792         coordinates will be padded with the same mode, otherwise coordinates
3793         are padded using the "constant" mode with fill_value dtypes.NA.
3794 
3795         Parameters
3796         ----------
3797         pad_width : mapping of hashable to tuple of int
3798             Mapping with the form of {dim: (pad_before, pad_after)}
3799             describing the number of values padded along each dimension.
3800             {dim: pad} is a shortcut for pad_before = pad_after = pad
3801         mode : str, default: "constant"
3802             One of the following string values (taken from numpy docs)
3803 
3804             'constant' (default)
3805                 Pads with a constant value.
3806             'edge'
3807                 Pads with the edge values of array.
3808             'linear_ramp'
3809                 Pads with the linear ramp between end_value and the
3810                 array edge value.
3811             'maximum'
3812                 Pads with the maximum value of all or part of the
3813                 vector along each axis.
3814             'mean'
3815                 Pads with the mean value of all or part of the
3816                 vector along each axis.
3817             'median'
3818                 Pads with the median value of all or part of the
3819                 vector along each axis.
3820             'minimum'
3821                 Pads with the minimum value of all or part of the
3822                 vector along each axis.
3823             'reflect'
3824                 Pads with the reflection of the vector mirrored on
3825                 the first and last values of the vector along each
3826                 axis.
3827             'symmetric'
3828                 Pads with the reflection of the vector mirrored
3829                 along the edge of the array.
3830             'wrap'
3831                 Pads with the wrap of the vector along the axis.
3832                 The first values are used to pad the end and the
3833                 end values are used to pad the beginning.
3834         stat_length : int, tuple or mapping of hashable to tuple, default: None
3835             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3836             values at edge of each axis used to calculate the statistic value.
3837             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3838             statistic lengths along each dimension.
3839             ((before, after),) yields same before and after statistic lengths
3840             for each dimension.
3841             (stat_length,) or int is a shortcut for before = after = statistic
3842             length for all axes.
3843             Default is ``None``, to use the entire axis.
3844         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3845             Used in 'constant'.  The values to set the padded values for each
3846             axis.
3847             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3848             pad constants along each dimension.
3849             ``((before, after),)`` yields same before and after constants for each
3850             dimension.
3851             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3852             all dimensions.
3853             Default is 0.
3854         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3855             Used in 'linear_ramp'.  The values used for the ending value of the
3856             linear_ramp and that will form the edge of the padded array.
3857             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3858             end values along each dimension.
3859             ``((before, after),)`` yields same before and after end values for each
3860             axis.
3861             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3862             all axes.
3863             Default is 0.
3864         reflect_type : {"even", "odd"}, optional
3865             Used in "reflect", and "symmetric".  The "even" style is the
3866             default with an unaltered reflection around the edge value.  For
3867             the "odd" style, the extended part of the array is created by
3868             subtracting the reflected values from two times the edge value.
3869         **pad_width_kwargs
3870             The keyword arguments form of ``pad_width``.
3871             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
3872 
3873         Returns
3874         -------
3875         padded : DataArray
3876             DataArray with the padded coordinates and data.
3877 
3878         See Also
3879         --------
3880         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
3881 
3882         Notes
3883         -----
3884         For ``mode="constant"`` and ``constant_values=None``, integer types will be
3885         promoted to ``float`` and padded with ``np.nan``.
3886 
3887         Examples
3888         --------
3889         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
3890         >>> arr.pad(x=(1, 2), constant_values=0)
3891         <xarray.DataArray (x: 6)>
3892         array([0, 5, 6, 7, 0, 0])
3893         Coordinates:
3894           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
3895 
3896         >>> da = xr.DataArray(
3897         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
3898         ...     dims=["x", "y"],
3899         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
3900         ... )
3901         >>> da.pad(x=1)
3902         <xarray.DataArray (x: 4, y: 4)>
3903         array([[nan, nan, nan, nan],
3904                [ 0.,  1.,  2.,  3.],
3905                [10., 11., 12., 13.],
3906                [nan, nan, nan, nan]])
3907         Coordinates:
3908           * x        (x) float64 nan 0.0 1.0 nan
3909           * y        (y) int64 10 20 30 40
3910             z        (x) float64 nan 100.0 200.0 nan
3911 
3912         Careful, ``constant_values`` are coerced to the data type of the array which may
3913         lead to a loss of precision:
3914 
3915         >>> da.pad(x=1, constant_values=1.23456789)
3916         <xarray.DataArray (x: 4, y: 4)>
3917         array([[ 1,  1,  1,  1],
3918                [ 0,  1,  2,  3],
3919                [10, 11, 12, 13],
3920                [ 1,  1,  1,  1]])
3921         Coordinates:
3922           * x        (x) float64 nan 0.0 1.0 nan
3923           * y        (y) int64 10 20 30 40
3924             z        (x) float64 nan 100.0 200.0 nan
3925         """
3926         ds = self._to_temp_dataset().pad(
3927             pad_width=pad_width,
3928             mode=mode,
3929             stat_length=stat_length,
3930             constant_values=constant_values,
3931             end_values=end_values,
3932             reflect_type=reflect_type,
3933             **pad_width_kwargs,
3934         )
3935         return self._from_temp_dataset(ds)
3936 
3937     def idxmin(
3938         self,
3939         dim: Hashable = None,
3940         skipna: bool = None,
3941         fill_value: Any = dtypes.NA,
3942         keep_attrs: bool = None,
3943     ) -> "DataArray":
3944         """Return the coordinate label of the minimum value along a dimension.
3945 
3946         Returns a new `DataArray` named after the dimension with the values of
3947         the coordinate labels along that dimension corresponding to minimum
3948         values along that dimension.
3949 
3950         In comparison to :py:meth:`~DataArray.argmin`, this returns the
3951         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
3952 
3953         Parameters
3954         ----------
3955         dim : str, optional
3956             Dimension over which to apply `idxmin`.  This is optional for 1D
3957             arrays, but required for arrays with 2 or more dimensions.
3958         skipna : bool or None, default: None
3959             If True, skip missing values (as marked by NaN). By default, only
3960             skips missing values for ``float``, ``complex``, and ``object``
3961             dtypes; other dtypes either do not have a sentinel missing value
3962             (``int``) or ``skipna=True`` has not been implemented
3963             (``datetime64`` or ``timedelta64``).
3964         fill_value : Any, default: NaN
3965             Value to be filled in case all of the values along a dimension are
3966             null.  By default this is NaN.  The fill value and result are
3967             automatically converted to a compatible dtype if possible.
3968             Ignored if ``skipna`` is False.
3969         keep_attrs : bool, default: False
3970             If True, the attributes (``attrs``) will be copied from the
3971             original object to the new one.  If False (default), the new object
3972             will be returned without attributes.
3973 
3974         Returns
3975         -------
3976         reduced : DataArray
3977             New `DataArray` object with `idxmin` applied to its data and the
3978             indicated dimension removed.
3979 
3980         See Also
3981         --------
3982         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
3983 
3984         Examples
3985         --------
3986         >>> array = xr.DataArray(
3987         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
3988         ... )
3989         >>> array.min()
3990         <xarray.DataArray ()>
3991         array(-2)
3992         >>> array.argmin()
3993         <xarray.DataArray ()>
3994         array(4)
3995         >>> array.idxmin()
3996         <xarray.DataArray 'x' ()>
3997         array('e', dtype='<U1')
3998 
3999         >>> array = xr.DataArray(
4000         ...     [
4001         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4002         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4003         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4004         ...     ],
4005         ...     dims=["y", "x"],
4006         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4007         ... )
4008         >>> array.min(dim="x")
4009         <xarray.DataArray (y: 3)>
4010         array([-2., -4.,  1.])
4011         Coordinates:
4012           * y        (y) int64 -1 0 1
4013         >>> array.argmin(dim="x")
4014         <xarray.DataArray (y: 3)>
4015         array([4, 0, 2])
4016         Coordinates:
4017           * y        (y) int64 -1 0 1
4018         >>> array.idxmin(dim="x")
4019         <xarray.DataArray 'x' (y: 3)>
4020         array([16.,  0.,  4.])
4021         Coordinates:
4022           * y        (y) int64 -1 0 1
4023         """
4024         return computation._calc_idxminmax(
4025             array=self,
4026             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4027             dim=dim,
4028             skipna=skipna,
4029             fill_value=fill_value,
4030             keep_attrs=keep_attrs,
4031         )
4032 
4033     def idxmax(
4034         self,
4035         dim: Hashable = None,
4036         skipna: bool = None,
4037         fill_value: Any = dtypes.NA,
4038         keep_attrs: bool = None,
4039     ) -> "DataArray":
4040         """Return the coordinate label of the maximum value along a dimension.
4041 
4042         Returns a new `DataArray` named after the dimension with the values of
4043         the coordinate labels along that dimension corresponding to maximum
4044         values along that dimension.
4045 
4046         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4047         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4048 
4049         Parameters
4050         ----------
4051         dim : hashable, optional
4052             Dimension over which to apply `idxmax`.  This is optional for 1D
4053             arrays, but required for arrays with 2 or more dimensions.
4054         skipna : bool or None, default: None
4055             If True, skip missing values (as marked by NaN). By default, only
4056             skips missing values for ``float``, ``complex``, and ``object``
4057             dtypes; other dtypes either do not have a sentinel missing value
4058             (``int``) or ``skipna=True`` has not been implemented
4059             (``datetime64`` or ``timedelta64``).
4060         fill_value : Any, default: NaN
4061             Value to be filled in case all of the values along a dimension are
4062             null.  By default this is NaN.  The fill value and result are
4063             automatically converted to a compatible dtype if possible.
4064             Ignored if ``skipna`` is False.
4065         keep_attrs : bool, default: False
4066             If True, the attributes (``attrs``) will be copied from the
4067             original object to the new one.  If False (default), the new object
4068             will be returned without attributes.
4069 
4070         Returns
4071         -------
4072         reduced : DataArray
4073             New `DataArray` object with `idxmax` applied to its data and the
4074             indicated dimension removed.
4075 
4076         See Also
4077         --------
4078         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4079 
4080         Examples
4081         --------
4082         >>> array = xr.DataArray(
4083         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4084         ... )
4085         >>> array.max()
4086         <xarray.DataArray ()>
4087         array(2)
4088         >>> array.argmax()
4089         <xarray.DataArray ()>
4090         array(1)
4091         >>> array.idxmax()
4092         <xarray.DataArray 'x' ()>
4093         array('b', dtype='<U1')
4094 
4095         >>> array = xr.DataArray(
4096         ...     [
4097         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4098         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4099         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4100         ...     ],
4101         ...     dims=["y", "x"],
4102         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4103         ... )
4104         >>> array.max(dim="x")
4105         <xarray.DataArray (y: 3)>
4106         array([2., 2., 1.])
4107         Coordinates:
4108           * y        (y) int64 -1 0 1
4109         >>> array.argmax(dim="x")
4110         <xarray.DataArray (y: 3)>
4111         array([0, 2, 2])
4112         Coordinates:
4113           * y        (y) int64 -1 0 1
4114         >>> array.idxmax(dim="x")
4115         <xarray.DataArray 'x' (y: 3)>
4116         array([0., 4., 4.])
4117         Coordinates:
4118           * y        (y) int64 -1 0 1
4119         """
4120         return computation._calc_idxminmax(
4121             array=self,
4122             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4123             dim=dim,
4124             skipna=skipna,
4125             fill_value=fill_value,
4126             keep_attrs=keep_attrs,
4127         )
4128 
4129     def argmin(
4130         self,
4131         dim: Union[Hashable, Sequence[Hashable]] = None,
4132         axis: int = None,
4133         keep_attrs: bool = None,
4134         skipna: bool = None,
4135     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4136         """Index or indices of the minimum of the DataArray over one or more dimensions.
4137 
4138         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4139         which can be passed directly to isel(). If a single str is passed to 'dim' then
4140         returns a DataArray with dtype int.
4141 
4142         If there are multiple minima, the indices of the first one found will be
4143         returned.
4144 
4145         Parameters
4146         ----------
4147         dim : hashable, sequence of hashable or ..., optional
4148             The dimensions over which to find the minimum. By default, finds minimum over
4149             all dimensions - for now returning an int for backward compatibility, but
4150             this is deprecated, in future will return a dict with indices for all
4151             dimensions; to return a dict with all dimensions now, pass '...'.
4152         axis : int, optional
4153             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4154             can be supplied.
4155         keep_attrs : bool, optional
4156             If True, the attributes (`attrs`) will be copied from the original
4157             object to the new one.  If False (default), the new object will be
4158             returned without attributes.
4159         skipna : bool, optional
4160             If True, skip missing values (as marked by NaN). By default, only
4161             skips missing values for float dtypes; other dtypes either do not
4162             have a sentinel missing value (int) or skipna=True has not been
4163             implemented (object, datetime64 or timedelta64).
4164 
4165         Returns
4166         -------
4167         result : DataArray or dict of DataArray
4168 
4169         See Also
4170         --------
4171         Variable.argmin, DataArray.idxmin
4172 
4173         Examples
4174         --------
4175         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4176         >>> array.min()
4177         <xarray.DataArray ()>
4178         array(-1)
4179         >>> array.argmin()
4180         <xarray.DataArray ()>
4181         array(2)
4182         >>> array.argmin(...)
4183         {'x': <xarray.DataArray ()>
4184         array(2)}
4185         >>> array.isel(array.argmin(...))
4186         <xarray.DataArray ()>
4187         array(-1)
4188 
4189         >>> array = xr.DataArray(
4190         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4191         ...     dims=("x", "y", "z"),
4192         ... )
4193         >>> array.min(dim="x")
4194         <xarray.DataArray (y: 3, z: 3)>
4195         array([[ 1,  2,  1],
4196                [ 2, -5,  1],
4197                [ 2,  1,  1]])
4198         Dimensions without coordinates: y, z
4199         >>> array.argmin(dim="x")
4200         <xarray.DataArray (y: 3, z: 3)>
4201         array([[1, 0, 0],
4202                [1, 1, 1],
4203                [0, 0, 1]])
4204         Dimensions without coordinates: y, z
4205         >>> array.argmin(dim=["x"])
4206         {'x': <xarray.DataArray (y: 3, z: 3)>
4207         array([[1, 0, 0],
4208                [1, 1, 1],
4209                [0, 0, 1]])
4210         Dimensions without coordinates: y, z}
4211         >>> array.min(dim=("x", "z"))
4212         <xarray.DataArray (y: 3)>
4213         array([ 1, -5,  1])
4214         Dimensions without coordinates: y
4215         >>> array.argmin(dim=["x", "z"])
4216         {'x': <xarray.DataArray (y: 3)>
4217         array([0, 1, 0])
4218         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4219         array([2, 1, 1])
4220         Dimensions without coordinates: y}
4221         >>> array.isel(array.argmin(dim=["x", "z"]))
4222         <xarray.DataArray (y: 3)>
4223         array([ 1, -5,  1])
4224         Dimensions without coordinates: y
4225         """
4226         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4227         if isinstance(result, dict):
4228             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4229         else:
4230             return self._replace_maybe_drop_dims(result)
4231 
4232     def argmax(
4233         self,
4234         dim: Union[Hashable, Sequence[Hashable]] = None,
4235         axis: int = None,
4236         keep_attrs: bool = None,
4237         skipna: bool = None,
4238     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4239         """Index or indices of the maximum of the DataArray over one or more dimensions.
4240 
4241         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4242         which can be passed directly to isel(). If a single str is passed to 'dim' then
4243         returns a DataArray with dtype int.
4244 
4245         If there are multiple maxima, the indices of the first one found will be
4246         returned.
4247 
4248         Parameters
4249         ----------
4250         dim : hashable, sequence of hashable or ..., optional
4251             The dimensions over which to find the maximum. By default, finds maximum over
4252             all dimensions - for now returning an int for backward compatibility, but
4253             this is deprecated, in future will return a dict with indices for all
4254             dimensions; to return a dict with all dimensions now, pass '...'.
4255         axis : int, optional
4256             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4257             can be supplied.
4258         keep_attrs : bool, optional
4259             If True, the attributes (`attrs`) will be copied from the original
4260             object to the new one.  If False (default), the new object will be
4261             returned without attributes.
4262         skipna : bool, optional
4263             If True, skip missing values (as marked by NaN). By default, only
4264             skips missing values for float dtypes; other dtypes either do not
4265             have a sentinel missing value (int) or skipna=True has not been
4266             implemented (object, datetime64 or timedelta64).
4267 
4268         Returns
4269         -------
4270         result : DataArray or dict of DataArray
4271 
4272         See Also
4273         --------
4274         Variable.argmax, DataArray.idxmax
4275 
4276         Examples
4277         --------
4278         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4279         >>> array.max()
4280         <xarray.DataArray ()>
4281         array(3)
4282         >>> array.argmax()
4283         <xarray.DataArray ()>
4284         array(3)
4285         >>> array.argmax(...)
4286         {'x': <xarray.DataArray ()>
4287         array(3)}
4288         >>> array.isel(array.argmax(...))
4289         <xarray.DataArray ()>
4290         array(3)
4291 
4292         >>> array = xr.DataArray(
4293         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4294         ...     dims=("x", "y", "z"),
4295         ... )
4296         >>> array.max(dim="x")
4297         <xarray.DataArray (y: 3, z: 3)>
4298         array([[3, 3, 2],
4299                [3, 5, 2],
4300                [2, 3, 3]])
4301         Dimensions without coordinates: y, z
4302         >>> array.argmax(dim="x")
4303         <xarray.DataArray (y: 3, z: 3)>
4304         array([[0, 1, 1],
4305                [0, 1, 0],
4306                [0, 1, 0]])
4307         Dimensions without coordinates: y, z
4308         >>> array.argmax(dim=["x"])
4309         {'x': <xarray.DataArray (y: 3, z: 3)>
4310         array([[0, 1, 1],
4311                [0, 1, 0],
4312                [0, 1, 0]])
4313         Dimensions without coordinates: y, z}
4314         >>> array.max(dim=("x", "z"))
4315         <xarray.DataArray (y: 3)>
4316         array([3, 5, 3])
4317         Dimensions without coordinates: y
4318         >>> array.argmax(dim=["x", "z"])
4319         {'x': <xarray.DataArray (y: 3)>
4320         array([0, 1, 0])
4321         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4322         array([0, 1, 2])
4323         Dimensions without coordinates: y}
4324         >>> array.isel(array.argmax(dim=["x", "z"]))
4325         <xarray.DataArray (y: 3)>
4326         array([3, 5, 3])
4327         Dimensions without coordinates: y
4328         """
4329         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4330         if isinstance(result, dict):
4331             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4332         else:
4333             return self._replace_maybe_drop_dims(result)
4334 
4335     # this needs to be at the end, or mypy will confuse with `str`
4336     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4337     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 21:44:08.070 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 21:44:08.070 | INFO     | Search-only: skipping patch selection
2025-10-15 21:44:08.156 | INFO     | Task pydata__xarray-4442 completed successfully.
2025-10-15 21:44:08.157 | INFO     | No patch generated. You can try running ACR again.
