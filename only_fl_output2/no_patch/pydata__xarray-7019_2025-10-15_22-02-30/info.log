2025-10-15 22:02:30.464 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 22:02:32.287 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=756, output_tokens=12, cost=0.007920
2025-10-15 22:02:32.288 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 22:02:32.292 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 22:02:40.686 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1063, output_tokens=347, cost=0.021040
2025-10-15 22:02:40.881 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 22:02:43.727 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=701, output_tokens=79, cost=0.009380
2025-10-15 22:02:43.728 | DEBUG    | Extracted a valid json.
2025-10-15 22:02:43.728 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"dask.array\")",
        "search_method(\"chunk\")",
        "search_method(\"compute\")",
        "search_method_in_class(\"chunk\", \"DataArray\")",
        "search_method_in_class(\"compute\", \"DataArray\")",
        "search_class(\"ArrayWrapper\")"
    ],
    "bug_locations": []
}
2025-10-15 22:02:43.859 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 22:02:43.860 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 22:03:04.781 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3356, output_tokens=941, cost=0.061790
2025-10-15 22:03:04.790 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 22:03:17.376 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4513, output_tokens=557, cost=0.061840
2025-10-15 22:03:17.387 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 22:03:27.675 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=911, output_tokens=324, cost=0.018830
2025-10-15 22:03:27.676 | DEBUG    | Extracted a valid json.
2025-10-15 22:03:27.676 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class_in_file(\"DataArray\", \"xarray/core/dataarray.py\")",
        "search_method_in_class(\"chunk\", \"DataArray\")",
        "search_method_in_class(\"compute\", \"DataArray\")"
    ],
    "bug_locations": [
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "chunk",
            "intended_behavior": "This method, which currently handles chunking for dask arrays, should be refactored to abstract away direct dask calls. Instead, it should dynamically dispatch chunk operations based on the backend library specified (e.g., dask, cubed). The underlying chunking mechanism should detect the specified manager (\"dask\", \"cubed\", etc.) and invoke the corresponding chunk method from the appropriate library. This change would maintain existing functionality for dask while extending support to cubed and potentially other libraries."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "compute",
            "intended_behavior": "Similar to the `chunk` method, `compute` should also be refactored to support dynamic dispatch based on the specified backend library for computation. The method should abstract the compute calls, allowing for a seamless interchange between dask and other conforming libraries like cubed. The functionality for dask computations remains intact, while the method extends support for alternative libraries following the same or compatible API patterns."
        }
    ]
}
2025-10-15 22:03:28.582 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 22:03:28.583 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 22:04:09.369 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=81815, output_tokens=626, cost=0.836930
2025-10-15 22:04:09.384 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 22:04:34.566 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=82657, output_tokens=451, cost=0.840100
2025-10-15 22:04:34.579 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 22:04:43.211 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=805, output_tokens=371, cost=0.019180
2025-10-15 22:04:43.212 | DEBUG    | Extracted a valid json.
2025-10-15 22:04:43.212 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "chunk",
            "intended_behavior": "The method should include support for a more general chunking mechanism that is not tightly coupled with dask. This entails creating an abstraction layer that can interpret chunks arguments for different frameworks like dask or cubed. The method should dynamically determine the chunking strategy based on the specified backend engine."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "compute",
            "intended_behavior": "Similar to chunk, the compute method should be refactored to support computations through alternative backends. This requires abstracting away dask-specific call to a compute action that’s applicable to the selected backend, including a backend management system that handles the method calls to the appropriate library."
        },
        {
            "file": "Not specified, could be newly created or integrated within an existing module like xarray/backends/",
            "class": "",
            "method": "",
            "intended_behavior": "Implement a new backend abstraction layer that standardizes the interface for chunking, computing, and any other operation that may leverage parallel execution frameworks. This includes: 1. Defining a base class or interface for backend operations. 2. Implementing subclass(es) for each supported backend (e.g., dask, cubed). 3. Providing a mechanism (e.g., a backend registry) that allows for selecting and dynamically switching between backends based on user preferences or configuration. 4. Ensuring backward compatibility with existing dask operations while allowing for the integration of new backends."
        }
    ]
}
2025-10-15 22:04:43.248 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>chunk</method>
<code>
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)

</code><intended_behavior>The method should include support for a more general chunking mechanism that is not tightly coupled with dask. This entails creating an abstraction layer that can interpret chunks arguments for different frameworks like dask or cubed. The method should dynamically determine the chunking strategy based on the specified backend engine.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
229 class DataArray(
230     AbstractArray,
231     DataWithCoords,
232     DataArrayArithmetic,
233     DataArrayAggregations,
234 ):
235     """N-dimensional array with labeled coordinates and dimensions.
236 
237     DataArray provides a wrapper around numpy ndarrays that uses
238     labeled dimensions and coordinates to support metadata aware
239     operations. The API is similar to that for the pandas Series or
240     DataFrame, but DataArray objects can have any number of dimensions,
241     and their contents have fixed data types.
242 
243     Additional features over raw numpy arrays:
244 
245     - Apply operations over dimensions by name: ``x.sum('time')``.
246     - Select or assign values by integer location (like numpy):
247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
248       ``x.sel(time='2014-01-01')``.
249     - Mathematical operations (e.g., ``x - y``) vectorize across
250       multiple dimensions (known in numpy as "broadcasting") based on
251       dimension names, regardless of their original order.
252     - Keep track of arbitrary metadata in the form of a Python
253       dictionary: ``x.attrs``
254     - Convert to a pandas Series: ``x.to_series()``.
255 
256     Getting items from or doing mathematical operations with a
257     DataArray always returns another DataArray.
258 
259     Parameters
260     ----------
261     data : array_like
262         Values for this array. Must be an ``numpy.ndarray``, ndarray
263         like, or castable to an ``ndarray``. If a self-described xarray
264         or pandas object, attempts are made to use this array's
265         metadata to fill in other unspecified arguments. A view of the
266         array's data is used instead of a copy if possible.
267     coords : sequence or dict of array_like, optional
268         Coordinates (tick labels) to use for indexing along each
269         dimension. The following notations are accepted:
270 
271         - mapping {dimension name: array-like}
272         - sequence of tuples that are valid arguments for
273           ``xarray.Variable()``
274           - (dims, data)
275           - (dims, data, attrs)
276           - (dims, data, attrs, encoding)
277 
278         Additionally, it is possible to define a coord whose name
279         does not match the dimension name, or a coord based on multiple
280         dimensions, with one of the following notations:
281 
282         - mapping {coord name: DataArray}
283         - mapping {coord name: Variable}
284         - mapping {coord name: (dimension name, array-like)}
285         - mapping {coord name: (tuple of dimension names, array-like)}
286 
287     dims : Hashable or sequence of Hashable, optional
288         Name(s) of the data dimension(s). Must be either a Hashable
289         (only for 1D data) or a sequence of Hashables with length equal
290         to the number of dimensions. If this argument is omitted,
291         dimension names are taken from ``coords`` (if possible) and
292         otherwise default to ``['dim_0', ... 'dim_n']``.
293     name : str or None, optional
294         Name of this array.
295     attrs : dict_like or None, optional
296         Attributes to assign to the new instance. By default, an empty
297         attribute dictionary is initialized.
298 
299     Examples
300     --------
301     Create data:
302 
303     >>> np.random.seed(0)
304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
307     >>> time = pd.date_range("2014-09-06", periods=3)
308     >>> reference_time = pd.Timestamp("2014-09-05")
309 
310     Initialize a dataarray with multiple dimensions:
311 
312     >>> da = xr.DataArray(
313     ...     data=temperature,
314     ...     dims=["x", "y", "time"],
315     ...     coords=dict(
316     ...         lon=(["x", "y"], lon),
317     ...         lat=(["x", "y"], lat),
318     ...         time=time,
319     ...         reference_time=reference_time,
320     ...     ),
321     ...     attrs=dict(
322     ...         description="Ambient temperature.",
323     ...         units="degC",
324     ...     ),
325     ... )
326     >>> da
327     <xarray.DataArray (x: 2, y: 2, time: 3)>
328     array([[[29.11241877, 18.20125767, 22.82990387],
329             [32.92714559, 29.94046392,  7.18177696]],
330     <BLANKLINE>
331            [[22.60070734, 13.78914233, 14.17424919],
332             [18.28478802, 16.15234857, 26.63418806]]])
333     Coordinates:
334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
335         lat             (x, y) float64 42.25 42.21 42.63 42.59
336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
337         reference_time  datetime64[ns] 2014-09-05
338     Dimensions without coordinates: x, y
339     Attributes:
340         description:  Ambient temperature.
341         units:        degC
342 
343     Find out where the coldest temperature was:
344 
345     >>> da.isel(da.argmin(...))
346     <xarray.DataArray ()>
347     array(7.18177696)
348     Coordinates:
349         lon             float64 -99.32
350         lat             float64 42.21
351         time            datetime64[ns] 2014-09-08
352         reference_time  datetime64[ns] 2014-09-05
353     Attributes:
354         description:  Ambient temperature.
355         units:        degC
356     """
357 
358     _cache: dict[str, Any]
359     _coords: dict[Any, Variable]
360     _close: Callable[[], None] | None
361     _indexes: dict[Hashable, Index]
362     _name: Hashable | None
363     _variable: Variable
364 
365     __slots__ = (
366         "_cache",
367         "_coords",
368         "_close",
369         "_indexes",
370         "_name",
371         "_variable",
372         "__weakref__",
373     )
374 
375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
376 
377     def __init__(
378         self,
379         data: Any = dtypes.NA,
380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
381         | Mapping[Any, Any]
382         | None = None,
383         dims: Hashable | Sequence[Hashable] | None = None,
384         name: Hashable | None = None,
385         attrs: Mapping | None = None,
386         # internal parameters
387         indexes: dict[Hashable, Index] | None = None,
388         fastpath: bool = False,
389     ) -> None:
390         if fastpath:
391             variable = data
392             assert dims is None
393             assert attrs is None
394             assert indexes is not None
395         else:
396             # TODO: (benbovy - explicit indexes) remove
397             # once it becomes part of the public interface
398             if indexes is not None:
399                 raise ValueError("Providing explicit indexes is not supported yet")
400 
401             # try to fill in arguments from data if they weren't supplied
402             if coords is None:
403                 if isinstance(data, DataArray):
404                     coords = data.coords
405                 elif isinstance(data, pd.Series):
406                     coords = [data.index]
407                 elif isinstance(data, pd.DataFrame):
408                     coords = [data.index, data.columns]
409                 elif isinstance(data, (pd.Index, IndexVariable)):
410                     coords = [data]
411 
412             if dims is None:
413                 dims = getattr(data, "dims", getattr(coords, "dims", None))
414             if name is None:
415                 name = getattr(data, "name", None)
416             if attrs is None and not isinstance(data, PANDAS_TYPES):
417                 attrs = getattr(data, "attrs", None)
418 
419             data = _check_data_shape(data, coords, dims)
420             data = as_compatible_data(data)
421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
422             variable = Variable(dims, data, attrs, fastpath=True)
423             indexes, coords = _create_indexes_from_coords(coords)
424 
425         # These fully describe a DataArray
426         self._variable = variable
427         assert isinstance(coords, dict)
428         self._coords = coords
429         self._name = name
430 
431         # TODO(shoyer): document this argument, once it becomes part of the
432         # public interface.
433         self._indexes = indexes
434 
435         self._close = None
436 
437     @classmethod
438     def _construct_direct(
439         cls: type[T_DataArray],
440         variable: Variable,
441         coords: dict[Any, Variable],
442         name: Hashable,
443         indexes: dict[Hashable, Index],
444     ) -> T_DataArray:
445         """Shortcut around __init__ for internal use when we want to skip
446         costly validation
447         """
448         obj = object.__new__(cls)
449         obj._variable = variable
450         obj._coords = coords
451         obj._name = name
452         obj._indexes = indexes
453         obj._close = None
454         return obj
455 
456     def _replace(
457         self: T_DataArray,
458         variable: Variable | None = None,
459         coords=None,
460         name: Hashable | None | Default = _default,
461         indexes=None,
462     ) -> T_DataArray:
463         if variable is None:
464             variable = self.variable
465         if coords is None:
466             coords = self._coords
467         if indexes is None:
468             indexes = self._indexes
469         if name is _default:
470             name = self.name
471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
472 
473     def _replace_maybe_drop_dims(
474         self: T_DataArray,
475         variable: Variable,
476         name: Hashable | None | Default = _default,
477     ) -> T_DataArray:
478         if variable.dims == self.dims and variable.shape == self.shape:
479             coords = self._coords.copy()
480             indexes = self._indexes
481         elif variable.dims == self.dims:
482             # Shape has changed (e.g. from reduce(..., keepdims=True)
483             new_sizes = dict(zip(self.dims, variable.shape))
484             coords = {
485                 k: v
486                 for k, v in self._coords.items()
487                 if v.shape == tuple(new_sizes[d] for d in v.dims)
488             }
489             indexes = filter_indexes_from_coords(self._indexes, set(coords))
490         else:
491             allowed_dims = set(variable.dims)
492             coords = {
493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
494             }
495             indexes = filter_indexes_from_coords(self._indexes, set(coords))
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _overwrite_indexes(
499         self: T_DataArray,
500         indexes: Mapping[Any, Index],
501         coords: Mapping[Any, Variable] | None = None,
502         drop_coords: list[Hashable] | None = None,
503         rename_dims: Mapping[Any, Any] | None = None,
504     ) -> T_DataArray:
505         """Maybe replace indexes and their corresponding coordinates."""
506         if not indexes:
507             return self
508 
509         if coords is None:
510             coords = {}
511         if drop_coords is None:
512             drop_coords = []
513 
514         new_variable = self.variable.copy()
515         new_coords = self._coords.copy()
516         new_indexes = dict(self._indexes)
517 
518         for name in indexes:
519             new_coords[name] = coords[name]
520             new_indexes[name] = indexes[name]
521 
522         for name in drop_coords:
523             new_coords.pop(name)
524             new_indexes.pop(name)
525 
526         if rename_dims:
527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
528 
529         return self._replace(
530             variable=new_variable, coords=new_coords, indexes=new_indexes
531         )
532 
533     def _to_temp_dataset(self) -> Dataset:
534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
535 
536     def _from_temp_dataset(
537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
538     ) -> T_DataArray:
539         variable = dataset._variables.pop(_THIS_ARRAY)
540         coords = dataset._variables
541         indexes = dataset._indexes
542         return self._replace(variable, coords, name, indexes=indexes)
543 
544     def _to_dataset_split(self, dim: Hashable) -> Dataset:
545         """splits dataarray along dimension 'dim'"""
546 
547         def subset(dim, label):
548             array = self.loc[{dim: label}]
549             array.attrs = {}
550             return as_variable(array)
551 
552         variables = {label: subset(dim, label) for label in self.get_index(dim)}
553         variables.update({k: v for k, v in self._coords.items() if k != dim})
554         coord_names = set(self._coords) - {dim}
555         indexes = filter_indexes_from_coords(self._indexes, coord_names)
556         dataset = Dataset._construct_direct(
557             variables, coord_names, indexes=indexes, attrs=self.attrs
558         )
559         return dataset
560 
561     def _to_dataset_whole(
562         self, name: Hashable = None, shallow_copy: bool = True
563     ) -> Dataset:
564         if name is None:
565             name = self.name
566         if name is None:
567             raise ValueError(
568                 "unable to convert unnamed DataArray to a "
569                 "Dataset without providing an explicit name"
570             )
571         if name in self.coords:
572             raise ValueError(
573                 "cannot create a Dataset from a DataArray with "
574                 "the same name as one of its coordinates"
575             )
576         # use private APIs for speed: this is called by _to_temp_dataset(),
577         # which is used in the guts of a lot of operations (e.g., reindex)
578         variables = self._coords.copy()
579         variables[name] = self.variable
580         if shallow_copy:
581             for k in variables:
582                 variables[k] = variables[k].copy(deep=False)
583         indexes = self._indexes
584 
585         coord_names = set(self._coords)
586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
587 
588     def to_dataset(
589         self,
590         dim: Hashable = None,
591         *,
592         name: Hashable = None,
593         promote_attrs: bool = False,
594     ) -> Dataset:
595         """Convert a DataArray to a Dataset.
596 
597         Parameters
598         ----------
599         dim : Hashable, optional
600             Name of the dimension on this array along which to split this array
601             into separate variables. If not provided, this array is converted
602             into a Dataset of one variable.
603         name : Hashable, optional
604             Name to substitute for this array's name. Only valid if ``dim`` is
605             not provided.
606         promote_attrs : bool, default: False
607             Set to True to shallow copy attrs of DataArray to returned Dataset.
608 
609         Returns
610         -------
611         dataset : Dataset
612         """
613         if dim is not None and dim not in self.dims:
614             raise TypeError(
615                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
616             )
617 
618         if dim is not None:
619             if name is not None:
620                 raise TypeError("cannot supply both dim and name arguments")
621             result = self._to_dataset_split(dim)
622         else:
623             result = self._to_dataset_whole(name)
624 
625         if promote_attrs:
626             result.attrs = dict(self.attrs)
627 
628         return result
629 
630     @property
631     def name(self) -> Hashable | None:
632         """The name of this array."""
633         return self._name
634 
635     @name.setter
636     def name(self, value: Hashable | None) -> None:
637         self._name = value
638 
639     @property
640     def variable(self) -> Variable:
641         """Low level interface to the Variable object for this DataArray."""
642         return self._variable
643 
644     @property
645     def dtype(self) -> np.dtype:
646         """
647         Data-type of the array’s elements.
648 
649         See Also
650         --------
651         ndarray.dtype
652         numpy.dtype
653         """
654         return self.variable.dtype
655 
656     @property
657     def shape(self) -> tuple[int, ...]:
658         """
659         Tuple of array dimensions.
660 
661         See Also
662         --------
663         numpy.ndarray.shape
664         """
665         return self.variable.shape
666 
667     @property
668     def size(self) -> int:
669         """
670         Number of elements in the array.
671 
672         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
673 
674         See Also
675         --------
676         numpy.ndarray.size
677         """
678         return self.variable.size
679 
680     @property
681     def nbytes(self) -> int:
682         """
683         Total bytes consumed by the elements of this DataArray's data.
684 
685         If the underlying data array does not include ``nbytes``, estimates
686         the bytes consumed based on the ``size`` and ``dtype``.
687         """
688         return self.variable.nbytes
689 
690     @property
691     def ndim(self) -> int:
692         """
693         Number of array dimensions.
694 
695         See Also
696         --------
697         numpy.ndarray.ndim
698         """
699         return self.variable.ndim
700 
701     def __len__(self) -> int:
702         return len(self.variable)
703 
704     @property
705     def data(self) -> Any:
706         """
707         The DataArray's data as an array. The underlying array type
708         (e.g. dask, sparse, pint) is preserved.
709 
710         See Also
711         --------
712         DataArray.to_numpy
713         DataArray.as_numpy
714         DataArray.values
715         """
716         return self.variable.data
717 
718     @data.setter
719     def data(self, value: Any) -> None:
720         self.variable.data = value
721 
722     @property
723     def values(self) -> np.ndarray:
724         """
725         The array's data as a numpy.ndarray.
726 
727         If the array's data is not a numpy.ndarray this will attempt to convert
728         it naively using np.array(), which will raise an error if the array
729         type does not support coercion like this (e.g. cupy).
730         """
731         return self.variable.values
732 
733     @values.setter
734     def values(self, value: Any) -> None:
735         self.variable.values = value
736 
737     def to_numpy(self) -> np.ndarray:
738         """
739         Coerces wrapped data to numpy and returns a numpy.ndarray.
740 
741         See Also
742         --------
743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
744         Dataset.as_numpy
745         DataArray.values
746         DataArray.data
747         """
748         return self.variable.to_numpy()
749 
750     def as_numpy(self: T_DataArray) -> T_DataArray:
751         """
752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
753 
754         See Also
755         --------
756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
757         Dataset.as_numpy : Converts all variables in a Dataset.
758         DataArray.values
759         DataArray.data
760         """
761         coords = {k: v.as_numpy() for k, v in self._coords.items()}
762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
763 
764     @property
765     def _in_memory(self) -> bool:
766         return self.variable._in_memory
767 
768     def _to_index(self) -> pd.Index:
769         return self.variable._to_index()
770 
771     def to_index(self) -> pd.Index:
772         """Convert this variable to a pandas.Index. Only possible for 1D
773         arrays.
774         """
775         return self.variable.to_index()
776 
777     @property
778     def dims(self) -> tuple[Hashable, ...]:
779         """Tuple of dimension names associated with this array.
780 
781         Note that the type of this property is inconsistent with
782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
783         consistently named properties.
784 
785         See Also
786         --------
787         DataArray.sizes
788         Dataset.dims
789         """
790         return self.variable.dims
791 
792     @dims.setter
793     def dims(self, value: Any) -> NoReturn:
794         raise AttributeError(
795             "you cannot assign dims on a DataArray. Use "
796             ".rename() or .swap_dims() instead."
797         )
798 
799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
800         if utils.is_dict_like(key):
801             return key
802         key = indexing.expanded_indexer(key, self.ndim)
803         return dict(zip(self.dims, key))
804 
805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
806         from xarray.core.dataset import _get_virtual_variable
807 
808         try:
809             var = self._coords[key]
810         except KeyError:
811             dim_sizes = dict(zip(self.dims, self.shape))
812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
813 
814         return self._replace_maybe_drop_dims(var, name=key)
815 
816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
817         if isinstance(key, str):
818             return self._getitem_coord(key)
819         else:
820             # xarray-style array indexing
821             return self.isel(indexers=self._item_key_to_dict(key))
822 
823     def __setitem__(self, key: Any, value: Any) -> None:
824         if isinstance(key, str):
825             self.coords[key] = value
826         else:
827             # Coordinates in key, value and self[key] should be consistent.
828             # TODO Coordinate consistency in key is checked here, but it
829             # causes unnecessary indexing. It should be optimized.
830             obj = self[key]
831             if isinstance(value, DataArray):
832                 assert_coordinate_consistent(value, obj.coords.variables)
833             # DataArray key -> Variable key
834             key = {
835                 k: v.variable if isinstance(v, DataArray) else v
836                 for k, v in self._item_key_to_dict(key).items()
837             }
838             self.variable[key] = value
839 
840     def __delitem__(self, key: Any) -> None:
841         del self.coords[key]
842 
843     @property
844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
845         """Places to look-up items for attribute-style access"""
846         yield from self._item_sources
847         yield self.attrs
848 
849     @property
850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
851         """Places to look-up items for key-completion"""
852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
853 
854         # virtual coordinates
855         # uses empty dict -- everything here can already be found in self.coords.
856         yield HybridMappingProxy(keys=self.dims, mapping={})
857 
858     def __contains__(self, key: Any) -> bool:
859         return key in self.data
860 
861     @property
862     def loc(self) -> _LocIndexer:
863         """Attribute for location based indexing like pandas."""
864         return _LocIndexer(self)
865 
866     @property
867     def attrs(self) -> dict[Any, Any]:
868         """Dictionary storing arbitrary metadata with this array."""
869         return self.variable.attrs
870 
871     @attrs.setter
872     def attrs(self, value: Mapping[Any, Any]) -> None:
873         self.variable.attrs = dict(value)
874 
875     @property
876     def encoding(self) -> dict[Any, Any]:
877         """Dictionary of format-specific settings for how this array should be
878         serialized."""
879         return self.variable.encoding
880 
881     @encoding.setter
882     def encoding(self, value: Mapping[Any, Any]) -> None:
883         self.variable.encoding = dict(value)
884 
885     def reset_encoding(self: T_DataArray) -> T_DataArray:
886         """Return a new DataArray without encoding on the array or any attached
887         coords."""
888         ds = self._to_temp_dataset().reset_encoding()
889         return self._from_temp_dataset(ds)
890 
891     @property
892     def indexes(self) -> Indexes:
893         """Mapping of pandas.Index objects used for label based indexing.
894 
895         Raises an error if this Dataset has indexes that cannot be coerced
896         to pandas.Index objects.
897 
898         See Also
899         --------
900         DataArray.xindexes
901 
902         """
903         return self.xindexes.to_pandas_indexes()
904 
905     @property
906     def xindexes(self) -> Indexes:
907         """Mapping of xarray Index objects used for label based indexing."""
908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
909 
910     @property
911     def coords(self) -> DataArrayCoordinates:
912         """Dictionary-like container of coordinate arrays."""
913         return DataArrayCoordinates(self)
914 
915     @overload
916     def reset_coords(
917         self: T_DataArray,
918         names: Dims = None,
919         drop: Literal[False] = False,
920     ) -> Dataset:
921         ...
922 
923     @overload
924     def reset_coords(
925         self: T_DataArray,
926         names: Dims = None,
927         *,
928         drop: Literal[True],
929     ) -> T_DataArray:
930         ...
931 
932     def reset_coords(
933         self: T_DataArray,
934         names: Dims = None,
935         drop: bool = False,
936     ) -> T_DataArray | Dataset:
937         """Given names of coordinates, reset them to become variables.
938 
939         Parameters
940         ----------
941         names : str, Iterable of Hashable or None, optional
942             Name(s) of non-index coordinates in this dataset to reset into
943             variables. By default, all non-index coordinates are reset.
944         drop : bool, default: False
945             If True, remove coordinates instead of converting them into
946             variables.
947 
948         Returns
949         -------
950         Dataset, or DataArray if ``drop == True``
951 
952         Examples
953         --------
954         >>> temperature = np.arange(25).reshape(5, 5)
955         >>> pressure = np.arange(50, 75).reshape(5, 5)
956         >>> da = xr.DataArray(
957         ...     data=temperature,
958         ...     dims=["x", "y"],
959         ...     coords=dict(
960         ...         lon=("x", np.arange(10, 15)),
961         ...         lat=("y", np.arange(20, 25)),
962         ...         Pressure=(["x", "y"], pressure),
963         ...     ),
964         ...     name="Temperature",
965         ... )
966         >>> da
967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
968         array([[ 0,  1,  2,  3,  4],
969                [ 5,  6,  7,  8,  9],
970                [10, 11, 12, 13, 14],
971                [15, 16, 17, 18, 19],
972                [20, 21, 22, 23, 24]])
973         Coordinates:
974             lon       (x) int64 10 11 12 13 14
975             lat       (y) int64 20 21 22 23 24
976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
977         Dimensions without coordinates: x, y
978 
979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
980 
981         >>> da.reset_coords(names="Pressure")
982         <xarray.Dataset>
983         Dimensions:      (x: 5, y: 5)
984         Coordinates:
985             lon          (x) int64 10 11 12 13 14
986             lat          (y) int64 20 21 22 23 24
987         Dimensions without coordinates: x, y
988         Data variables:
989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
991 
992         Return DataArray without targeted coordinate:
993 
994         >>> da.reset_coords(names="Pressure", drop=True)
995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
996         array([[ 0,  1,  2,  3,  4],
997                [ 5,  6,  7,  8,  9],
998                [10, 11, 12, 13, 14],
999                [15, 16, 17, 18, 19],
1000                [20, 21, 22, 23, 24]])
1001         Coordinates:
1002             lon      (x) int64 10 11 12 13 14
1003             lat      (y) int64 20 21 22 23 24
1004         Dimensions without coordinates: x, y
1005         """
1006         if names is None:
1007             names = set(self.coords) - set(self._indexes)
1008         dataset = self.coords.to_dataset().reset_coords(names, drop)
1009         if drop:
1010             return self._replace(coords=dataset._variables)
1011         if self.name is None:
1012             raise ValueError(
1013                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1014             )
1015         dataset[self.name] = self.variable
1016         return dataset
1017 
1018     def __dask_tokenize__(self):
1019         from dask.base import normalize_token
1020 
1021         return normalize_token((type(self), self._variable, self._coords, self._name))
1022 
1023     def __dask_graph__(self):
1024         return self._to_temp_dataset().__dask_graph__()
1025 
1026     def __dask_keys__(self):
1027         return self._to_temp_dataset().__dask_keys__()
1028 
1029     def __dask_layers__(self):
1030         return self._to_temp_dataset().__dask_layers__()
1031 
1032     @property
1033     def __dask_optimize__(self):
1034         return self._to_temp_dataset().__dask_optimize__
1035 
1036     @property
1037     def __dask_scheduler__(self):
1038         return self._to_temp_dataset().__dask_scheduler__
1039 
1040     def __dask_postcompute__(self):
1041         func, args = self._to_temp_dataset().__dask_postcompute__()
1042         return self._dask_finalize, (self.name, func) + args
1043 
1044     def __dask_postpersist__(self):
1045         func, args = self._to_temp_dataset().__dask_postpersist__()
1046         return self._dask_finalize, (self.name, func) + args
1047 
1048     @staticmethod
1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1050         ds = func(results, *args, **kwargs)
1051         variable = ds._variables.pop(_THIS_ARRAY)
1052         coords = ds._variables
1053         indexes = ds._indexes
1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1055 
1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1057         """Manually trigger loading of this array's data from disk or a
1058         remote source into memory and return this array.
1059 
1060         Normally, it should not be necessary to call this method in user code,
1061         because all xarray functions should either work on deferred data or
1062         load data automatically. However, this method can be necessary when
1063         working with many file objects on disk.
1064 
1065         Parameters
1066         ----------
1067         **kwargs : dict
1068             Additional keyword arguments passed on to ``dask.compute``.
1069 
1070         See Also
1071         --------
1072         dask.compute
1073         """
1074         ds = self._to_temp_dataset().load(**kwargs)
1075         new = self._from_temp_dataset(ds)
1076         self._variable = new._variable
1077         self._coords = new._coords
1078         return self
1079 
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)
1101 
1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1103         """Trigger computation in constituent dask arrays
1104 
1105         This keeps them as dask arrays but encourages them to keep data in
1106         memory.  This is particularly useful when on a distributed machine.
1107         When on a single machine consider using ``.compute()`` instead.
1108 
1109         Parameters
1110         ----------
1111         **kwargs : dict
1112             Additional keyword arguments passed on to ``dask.persist``.
1113 
1114         See Also
1115         --------
1116         dask.persist
1117         """
1118         ds = self._to_temp_dataset().persist(**kwargs)
1119         return self._from_temp_dataset(ds)
1120 
1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1122         """Returns a copy of this array.
1123 
1124         If `deep=True`, a deep copy is made of the data array.
1125         Otherwise, a shallow copy is made, and the returned data array's
1126         values are a new view of this data array's values.
1127 
1128         Use `data` to create a new object with the same structure as
1129         original but entirely new data.
1130 
1131         Parameters
1132         ----------
1133         deep : bool, optional
1134             Whether the data array and its coordinates are loaded into memory
1135             and copied onto the new object. Default is True.
1136         data : array_like, optional
1137             Data to use in the new object. Must have same shape as original.
1138             When `data` is used, `deep` is ignored for all data variables,
1139             and only used for coords.
1140 
1141         Returns
1142         -------
1143         copy : DataArray
1144             New object with dimensions, attributes, coordinates, name,
1145             encoding, and optionally data copied from original.
1146 
1147         Examples
1148         --------
1149         Shallow versus deep copy
1150 
1151         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1152         >>> array.copy()
1153         <xarray.DataArray (x: 3)>
1154         array([1, 2, 3])
1155         Coordinates:
1156           * x        (x) <U1 'a' 'b' 'c'
1157         >>> array_0 = array.copy(deep=False)
1158         >>> array_0[0] = 7
1159         >>> array_0
1160         <xarray.DataArray (x: 3)>
1161         array([7, 2, 3])
1162         Coordinates:
1163           * x        (x) <U1 'a' 'b' 'c'
1164         >>> array
1165         <xarray.DataArray (x: 3)>
1166         array([7, 2, 3])
1167         Coordinates:
1168           * x        (x) <U1 'a' 'b' 'c'
1169 
1170         Changing the data using the ``data`` argument maintains the
1171         structure of the original object, but with the new data. Original
1172         object is unaffected.
1173 
1174         >>> array.copy(data=[0.1, 0.2, 0.3])
1175         <xarray.DataArray (x: 3)>
1176         array([0.1, 0.2, 0.3])
1177         Coordinates:
1178           * x        (x) <U1 'a' 'b' 'c'
1179         >>> array
1180         <xarray.DataArray (x: 3)>
1181         array([7, 2, 3])
1182         Coordinates:
1183           * x        (x) <U1 'a' 'b' 'c'
1184 
1185         See Also
1186         --------
1187         pandas.DataFrame.copy
1188         """
1189         return self._copy(deep=deep, data=data)
1190 
1191     def _copy(
1192         self: T_DataArray,
1193         deep: bool = True,
1194         data: Any = None,
1195         memo: dict[int, Any] | None = None,
1196     ) -> T_DataArray:
1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1199 
1200         coords = {}
1201         for k, v in self._coords.items():
1202             if k in index_vars:
1203                 coords[k] = index_vars[k]
1204             else:
1205                 coords[k] = v._copy(deep=deep, memo=memo)
1206 
1207         return self._replace(variable, coords, indexes=indexes)
1208 
1209     def __copy__(self: T_DataArray) -> T_DataArray:
1210         return self._copy(deep=False)
1211 
1212     def __deepcopy__(
1213         self: T_DataArray, memo: dict[int, Any] | None = None
1214     ) -> T_DataArray:
1215         return self._copy(deep=True, memo=memo)
1216 
1217     # mutable objects should not be Hashable
1218     # https://github.com/python/mypy/issues/4266
1219     __hash__ = None  # type: ignore[assignment]
1220 
1221     @property
1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1223         """
1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1225         the underlying data is not a dask array.
1226 
1227         See Also
1228         --------
1229         DataArray.chunk
1230         DataArray.chunksizes
1231         xarray.unify_chunks
1232         """
1233         return self.variable.chunks
1234 
1235     @property
1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1237         """
1238         Mapping from dimension names to block lengths for this dataarray's data, or None if
1239         the underlying data is not a dask array.
1240         Cannot be modified directly, but can be modified by calling .chunk().
1241 
1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1243         instead of a tuple of chunk shapes.
1244 
1245         See Also
1246         --------
1247         DataArray.chunk
1248         DataArray.chunks
1249         xarray.unify_chunks
1250         """
1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1252         return get_chunksizes(all_variables)
1253 
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)
1333 
1334     def isel(
1335         self: T_DataArray,
1336         indexers: Mapping[Any, Any] | None = None,
1337         drop: bool = False,
1338         missing_dims: ErrorOptionsWithWarn = "raise",
1339         **indexers_kwargs: Any,
1340     ) -> T_DataArray:
1341         """Return a new DataArray whose data is given by selecting indexes
1342         along the specified dimension(s).
1343 
1344         Parameters
1345         ----------
1346         indexers : dict, optional
1347             A dict with keys matching dimensions and values given
1348             by integers, slice objects or arrays.
1349             indexer can be a integer, slice, array-like or DataArray.
1350             If DataArrays are passed as indexers, xarray-style indexing will be
1351             carried out. See :ref:`indexing` for the details.
1352             One of indexers or indexers_kwargs must be provided.
1353         drop : bool, default: False
1354             If ``drop=True``, drop coordinates variables indexed by integers
1355             instead of making them scalar.
1356         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1357             What to do if dimensions that should be selected from are not present in the
1358             DataArray:
1359             - "raise": raise an exception
1360             - "warn": raise a warning, and ignore the missing dimensions
1361             - "ignore": ignore the missing dimensions
1362         **indexers_kwargs : {dim: indexer, ...}, optional
1363             The keyword arguments form of ``indexers``.
1364 
1365         Returns
1366         -------
1367         indexed : xarray.DataArray
1368 
1369         See Also
1370         --------
1371         Dataset.isel
1372         DataArray.sel
1373 
1374         Examples
1375         --------
1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1377         >>> da
1378         <xarray.DataArray (x: 5, y: 5)>
1379         array([[ 0,  1,  2,  3,  4],
1380                [ 5,  6,  7,  8,  9],
1381                [10, 11, 12, 13, 14],
1382                [15, 16, 17, 18, 19],
1383                [20, 21, 22, 23, 24]])
1384         Dimensions without coordinates: x, y
1385 
1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> da = da.isel(x=tgt_x, y=tgt_y)
1389         >>> da
1390         <xarray.DataArray (points: 5)>
1391         array([ 0,  6, 12, 18, 24])
1392         Dimensions without coordinates: points
1393         """
1394 
1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1396 
1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1398             ds = self._to_temp_dataset()._isel_fancy(
1399                 indexers, drop=drop, missing_dims=missing_dims
1400             )
1401             return self._from_temp_dataset(ds)
1402 
1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1404         # lists, or zero or one-dimensional np.ndarray's
1405 
1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1408 
1409         coords = {}
1410         for coord_name, coord_value in self._coords.items():
1411             if coord_name in index_variables:
1412                 coord_value = index_variables[coord_name]
1413             else:
1414                 coord_indexers = {
1415                     k: v for k, v in indexers.items() if k in coord_value.dims
1416                 }
1417                 if coord_indexers:
1418                     coord_value = coord_value.isel(coord_indexers)
1419                     if drop and coord_value.ndim == 0:
1420                         continue
1421             coords[coord_name] = coord_value
1422 
1423         return self._replace(variable=variable, coords=coords, indexes=indexes)
1424 
1425     def sel(
1426         self: T_DataArray,
1427         indexers: Mapping[Any, Any] | None = None,
1428         method: str | None = None,
1429         tolerance=None,
1430         drop: bool = False,
1431         **indexers_kwargs: Any,
1432     ) -> T_DataArray:
1433         """Return a new DataArray whose data is given by selecting index
1434         labels along the specified dimension(s).
1435 
1436         In contrast to `DataArray.isel`, indexers for this method should use
1437         labels instead of integers.
1438 
1439         Under the hood, this method is powered by using pandas's powerful Index
1440         objects. This makes label based indexing essentially just as fast as
1441         using integer indexing.
1442 
1443         It also means this method uses pandas's (well documented) logic for
1444         indexing. This means you can use string shortcuts for datetime indexes
1445         (e.g., '2000-01' to select all values in January 2000). It also means
1446         that slices are treated as inclusive of both the start and stop values,
1447         unlike normal Python indexing.
1448 
1449         .. warning::
1450 
1451           Do not try to assign values when using any of the indexing methods
1452           ``isel`` or ``sel``::
1453 
1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1455             # DO NOT do this
1456             da.isel(x=[0, 1, 2])[1] = -1
1457 
1458           Assigning values with the chained indexing using ``.sel`` or
1459           ``.isel`` fails silently.
1460 
1461         Parameters
1462         ----------
1463         indexers : dict, optional
1464             A dict with keys matching dimensions and values given
1465             by scalars, slices or arrays of tick labels. For dimensions with
1466             multi-index, the indexer may also be a dict-like object with keys
1467             matching index level names.
1468             If DataArrays are passed as indexers, xarray-style indexing will be
1469             carried out. See :ref:`indexing` for the details.
1470             One of indexers or indexers_kwargs must be provided.
1471         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1472             Method to use for inexact matches:
1473 
1474             - None (default): only exact matches
1475             - pad / ffill: propagate last valid index value forward
1476             - backfill / bfill: propagate next valid index value backward
1477             - nearest: use nearest valid index value
1478 
1479         tolerance : optional
1480             Maximum distance between original and new labels for inexact
1481             matches. The values of the index at the matching locations must
1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1483         drop : bool, optional
1484             If ``drop=True``, drop coordinates variables in `indexers` instead
1485             of making them scalar.
1486         **indexers_kwargs : {dim: indexer, ...}, optional
1487             The keyword arguments form of ``indexers``.
1488             One of indexers or indexers_kwargs must be provided.
1489 
1490         Returns
1491         -------
1492         obj : DataArray
1493             A new DataArray with the same contents as this DataArray, except the
1494             data and each dimension is indexed by the appropriate indexers.
1495             If indexer DataArrays have coordinates that do not conflict with
1496             this object, then these coordinates will be attached.
1497             In general, each array's data will be a view of the array's data
1498             in this DataArray, unless vectorized indexing was triggered by using
1499             an array indexer, in which case the data will be a copy.
1500 
1501         See Also
1502         --------
1503         Dataset.sel
1504         DataArray.isel
1505 
1506         Examples
1507         --------
1508         >>> da = xr.DataArray(
1509         ...     np.arange(25).reshape(5, 5),
1510         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1511         ...     dims=("x", "y"),
1512         ... )
1513         >>> da
1514         <xarray.DataArray (x: 5, y: 5)>
1515         array([[ 0,  1,  2,  3,  4],
1516                [ 5,  6,  7,  8,  9],
1517                [10, 11, 12, 13, 14],
1518                [15, 16, 17, 18, 19],
1519                [20, 21, 22, 23, 24]])
1520         Coordinates:
1521           * x        (x) int64 0 1 2 3 4
1522           * y        (y) int64 0 1 2 3 4
1523 
1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1527         >>> da
1528         <xarray.DataArray (points: 5)>
1529         array([ 0,  6, 12, 18, 24])
1530         Coordinates:
1531             x        (points) int64 0 1 2 3 4
1532             y        (points) int64 0 1 2 3 4
1533         Dimensions without coordinates: points
1534         """
1535         ds = self._to_temp_dataset().sel(
1536             indexers=indexers,
1537             drop=drop,
1538             method=method,
1539             tolerance=tolerance,
1540             **indexers_kwargs,
1541         )
1542         return self._from_temp_dataset(ds)
1543 
1544     def head(
1545         self: T_DataArray,
1546         indexers: Mapping[Any, int] | int | None = None,
1547         **indexers_kwargs: Any,
1548     ) -> T_DataArray:
1549         """Return a new DataArray whose data is given by the the first `n`
1550         values along the specified dimension(s). Default `n` = 5
1551 
1552         See Also
1553         --------
1554         Dataset.head
1555         DataArray.tail
1556         DataArray.thin
1557 
1558         Examples
1559         --------
1560         >>> da = xr.DataArray(
1561         ...     np.arange(25).reshape(5, 5),
1562         ...     dims=("x", "y"),
1563         ... )
1564         >>> da
1565         <xarray.DataArray (x: 5, y: 5)>
1566         array([[ 0,  1,  2,  3,  4],
1567                [ 5,  6,  7,  8,  9],
1568                [10, 11, 12, 13, 14],
1569                [15, 16, 17, 18, 19],
1570                [20, 21, 22, 23, 24]])
1571         Dimensions without coordinates: x, y
1572 
1573         >>> da.head(x=1)
1574         <xarray.DataArray (x: 1, y: 5)>
1575         array([[0, 1, 2, 3, 4]])
1576         Dimensions without coordinates: x, y
1577 
1578         >>> da.head({"x": 2, "y": 2})
1579         <xarray.DataArray (x: 2, y: 2)>
1580         array([[0, 1],
1581                [5, 6]])
1582         Dimensions without coordinates: x, y
1583         """
1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1585         return self._from_temp_dataset(ds)
1586 
1587     def tail(
1588         self: T_DataArray,
1589         indexers: Mapping[Any, int] | int | None = None,
1590         **indexers_kwargs: Any,
1591     ) -> T_DataArray:
1592         """Return a new DataArray whose data is given by the the last `n`
1593         values along the specified dimension(s). Default `n` = 5
1594 
1595         See Also
1596         --------
1597         Dataset.tail
1598         DataArray.head
1599         DataArray.thin
1600 
1601         Examples
1602         --------
1603         >>> da = xr.DataArray(
1604         ...     np.arange(25).reshape(5, 5),
1605         ...     dims=("x", "y"),
1606         ... )
1607         >>> da
1608         <xarray.DataArray (x: 5, y: 5)>
1609         array([[ 0,  1,  2,  3,  4],
1610                [ 5,  6,  7,  8,  9],
1611                [10, 11, 12, 13, 14],
1612                [15, 16, 17, 18, 19],
1613                [20, 21, 22, 23, 24]])
1614         Dimensions without coordinates: x, y
1615 
1616         >>> da.tail(y=1)
1617         <xarray.DataArray (x: 5, y: 1)>
1618         array([[ 4],
1619                [ 9],
1620                [14],
1621                [19],
1622                [24]])
1623         Dimensions without coordinates: x, y
1624 
1625         >>> da.tail({"x": 2, "y": 2})
1626         <xarray.DataArray (x: 2, y: 2)>
1627         array([[18, 19],
1628                [23, 24]])
1629         Dimensions without coordinates: x, y
1630         """
1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1632         return self._from_temp_dataset(ds)
1633 
1634     def thin(
1635         self: T_DataArray,
1636         indexers: Mapping[Any, int] | int | None = None,
1637         **indexers_kwargs: Any,
1638     ) -> T_DataArray:
1639         """Return a new DataArray whose data is given by each `n` value
1640         along the specified dimension(s).
1641 
1642         Examples
1643         --------
1644         >>> x_arr = np.arange(0, 26)
1645         >>> x_arr
1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1647                17, 18, 19, 20, 21, 22, 23, 24, 25])
1648         >>> x = xr.DataArray(
1649         ...     np.reshape(x_arr, (2, 13)),
1650         ...     dims=("x", "y"),
1651         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1652         ... )
1653         >>> x
1654         <xarray.DataArray (x: 2, y: 13)>
1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1657         Coordinates:
1658           * x        (x) int64 0 1
1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1660 
1661         >>>
1662         >>> x.thin(3)
1663         <xarray.DataArray (x: 1, y: 5)>
1664         array([[ 0,  3,  6,  9, 12]])
1665         Coordinates:
1666           * x        (x) int64 0
1667           * y        (y) int64 0 3 6 9 12
1668         >>> x.thin({"x": 2, "y": 5})
1669         <xarray.DataArray (x: 1, y: 3)>
1670         array([[ 0,  5, 10]])
1671         Coordinates:
1672           * x        (x) int64 0
1673           * y        (y) int64 0 5 10
1674 
1675         See Also
1676         --------
1677         Dataset.thin
1678         DataArray.head
1679         DataArray.tail
1680         """
1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1682         return self._from_temp_dataset(ds)
1683 
1684     def broadcast_like(
1685         self: T_DataArray,
1686         other: DataArray | Dataset,
1687         exclude: Iterable[Hashable] | None = None,
1688     ) -> T_DataArray:
1689         """Broadcast this DataArray against another Dataset or DataArray.
1690 
1691         This is equivalent to xr.broadcast(other, self)[1]
1692 
1693         xarray objects are broadcast against each other in arithmetic
1694         operations, so this method is not be necessary for most uses.
1695 
1696         If no change is needed, the input data is returned to the output
1697         without being copied.
1698 
1699         If new coords are added by the broadcast, their values are
1700         NaN filled.
1701 
1702         Parameters
1703         ----------
1704         other : Dataset or DataArray
1705             Object against which to broadcast this array.
1706         exclude : iterable of Hashable, optional
1707             Dimensions that must not be broadcasted
1708 
1709         Returns
1710         -------
1711         new_da : DataArray
1712             The caller broadcasted against ``other``.
1713 
1714         Examples
1715         --------
1716         >>> arr1 = xr.DataArray(
1717         ...     np.random.randn(2, 3),
1718         ...     dims=("x", "y"),
1719         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1720         ... )
1721         >>> arr2 = xr.DataArray(
1722         ...     np.random.randn(3, 2),
1723         ...     dims=("x", "y"),
1724         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1725         ... )
1726         >>> arr1
1727         <xarray.DataArray (x: 2, y: 3)>
1728         array([[ 1.76405235,  0.40015721,  0.97873798],
1729                [ 2.2408932 ,  1.86755799, -0.97727788]])
1730         Coordinates:
1731           * x        (x) <U1 'a' 'b'
1732           * y        (y) <U1 'a' 'b' 'c'
1733         >>> arr2
1734         <xarray.DataArray (x: 3, y: 2)>
1735         array([[ 0.95008842, -0.15135721],
1736                [-0.10321885,  0.4105985 ],
1737                [ 0.14404357,  1.45427351]])
1738         Coordinates:
1739           * x        (x) <U1 'a' 'b' 'c'
1740           * y        (y) <U1 'a' 'b'
1741         >>> arr1.broadcast_like(arr2)
1742         <xarray.DataArray (x: 3, y: 3)>
1743         array([[ 1.76405235,  0.40015721,  0.97873798],
1744                [ 2.2408932 ,  1.86755799, -0.97727788],
1745                [        nan,         nan,         nan]])
1746         Coordinates:
1747           * x        (x) <U1 'a' 'b' 'c'
1748           * y        (y) <U1 'a' 'b' 'c'
1749         """
1750         if exclude is None:
1751             exclude = set()
1752         else:
1753             exclude = set(exclude)
1754         args = align(other, self, join="outer", copy=False, exclude=exclude)
1755 
1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1757 
1758         return _broadcast_helper(
1759             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1760         )
1761 
1762     def _reindex_callback(
1763         self: T_DataArray,
1764         aligner: alignment.Aligner,
1765         dim_pos_indexers: dict[Hashable, Any],
1766         variables: dict[Hashable, Variable],
1767         indexes: dict[Hashable, Index],
1768         fill_value: Any,
1769         exclude_dims: frozenset[Hashable],
1770         exclude_vars: frozenset[Hashable],
1771     ) -> T_DataArray:
1772         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1773 
1774         if isinstance(fill_value, dict):
1775             fill_value = fill_value.copy()
1776             sentinel = object()
1777             value = fill_value.pop(self.name, sentinel)
1778             if value is not sentinel:
1779                 fill_value[_THIS_ARRAY] = value
1780 
1781         ds = self._to_temp_dataset()
1782         reindexed = ds._reindex_callback(
1783             aligner,
1784             dim_pos_indexers,
1785             variables,
1786             indexes,
1787             fill_value,
1788             exclude_dims,
1789             exclude_vars,
1790         )
1791         return self._from_temp_dataset(reindexed)
1792 
1793     def reindex_like(
1794         self: T_DataArray,
1795         other: DataArray | Dataset,
1796         method: ReindexMethodOptions = None,
1797         tolerance: int | float | Iterable[int | float] | None = None,
1798         copy: bool = True,
1799         fill_value=dtypes.NA,
1800     ) -> T_DataArray:
1801         """Conform this object onto the indexes of another object, filling in
1802         missing values with ``fill_value``. The default fill value is NaN.
1803 
1804         Parameters
1805         ----------
1806         other : Dataset or DataArray
1807             Object with an 'indexes' attribute giving a mapping from dimension
1808             names to pandas.Index objects, which provides coordinates upon
1809             which to index the variables in this dataset. The indexes on this
1810             other object need not be the same as the indexes on this
1811             dataset. Any mis-matched index values will be filled in with
1812             NaN, and any mis-matched dimension names will simply be ignored.
1813         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1814             Method to use for filling index values from other not found on this
1815             data array:
1816 
1817             - None (default): don't fill gaps
1818             - pad / ffill: propagate last valid index value forward
1819             - backfill / bfill: propagate next valid index value backward
1820             - nearest: use nearest valid index value
1821 
1822         tolerance : optional
1823             Maximum distance between original and new labels for inexact
1824             matches. The values of the index at the matching locations must
1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1826             Tolerance may be a scalar value, which applies the same tolerance
1827             to all values, or list-like, which applies variable tolerance per
1828             element. List-like must be the same size as the index and its dtype
1829             must exactly match the index’s type.
1830         copy : bool, default: True
1831             If ``copy=True``, data in the return value is always copied. If
1832             ``copy=False`` and reindexing is unnecessary, or can be performed
1833             with only slice operations, then the output may share memory with
1834             the input. In either case, a new xarray object is always returned.
1835         fill_value : scalar or dict-like, optional
1836             Value to use for newly missing values. If a dict-like, maps
1837             variable names (including coordinates) to fill values. Use this
1838             data array's name to refer to the data array's values.
1839 
1840         Returns
1841         -------
1842         reindexed : DataArray
1843             Another dataset array, with this array's data but coordinates from
1844             the other object.
1845 
1846         Examples
1847         --------
1848         >>> data = np.arange(12).reshape(4, 3)
1849         >>> da1 = xr.DataArray(
1850         ...     data=data,
1851         ...     dims=["x", "y"],
1852         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1853         ... )
1854         >>> da1
1855         <xarray.DataArray (x: 4, y: 3)>
1856         array([[ 0,  1,  2],
1857                [ 3,  4,  5],
1858                [ 6,  7,  8],
1859                [ 9, 10, 11]])
1860         Coordinates:
1861           * x        (x) int64 10 20 30 40
1862           * y        (y) int64 70 80 90
1863         >>> da2 = xr.DataArray(
1864         ...     data=data,
1865         ...     dims=["x", "y"],
1866         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1867         ... )
1868         >>> da2
1869         <xarray.DataArray (x: 4, y: 3)>
1870         array([[ 0,  1,  2],
1871                [ 3,  4,  5],
1872                [ 6,  7,  8],
1873                [ 9, 10, 11]])
1874         Coordinates:
1875           * x        (x) int64 40 30 20 10
1876           * y        (y) int64 90 80 70
1877 
1878         Reindexing with both DataArrays having the same coordinates set, but in different order:
1879 
1880         >>> da1.reindex_like(da2)
1881         <xarray.DataArray (x: 4, y: 3)>
1882         array([[11, 10,  9],
1883                [ 8,  7,  6],
1884                [ 5,  4,  3],
1885                [ 2,  1,  0]])
1886         Coordinates:
1887           * x        (x) int64 40 30 20 10
1888           * y        (y) int64 90 80 70
1889 
1890         Reindexing with the other array having coordinates which the source array doesn't have:
1891 
1892         >>> data = np.arange(12).reshape(4, 3)
1893         >>> da1 = xr.DataArray(
1894         ...     data=data,
1895         ...     dims=["x", "y"],
1896         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1897         ... )
1898         >>> da2 = xr.DataArray(
1899         ...     data=data,
1900         ...     dims=["x", "y"],
1901         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1902         ... )
1903         >>> da1.reindex_like(da2)
1904         <xarray.DataArray (x: 4, y: 3)>
1905         array([[ 3.,  4.,  5.],
1906                [ 0.,  1.,  2.],
1907                [nan, nan, nan],
1908                [nan, nan, nan]])
1909         Coordinates:
1910           * x        (x) int64 20 10 29 39
1911           * y        (y) int64 70 80 90
1912 
1913         Filling missing values with the previous valid index with respect to the coordinates' value:
1914 
1915         >>> da1.reindex_like(da2, method="ffill")
1916         <xarray.DataArray (x: 4, y: 3)>
1917         array([[3, 4, 5],
1918                [0, 1, 2],
1919                [3, 4, 5],
1920                [6, 7, 8]])
1921         Coordinates:
1922           * x        (x) int64 20 10 29 39
1923           * y        (y) int64 70 80 90
1924 
1925         Filling missing values while tolerating specified error for inexact matches:
1926 
1927         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1928         <xarray.DataArray (x: 4, y: 3)>
1929         array([[ 3.,  4.,  5.],
1930                [ 0.,  1.,  2.],
1931                [nan, nan, nan],
1932                [nan, nan, nan]])
1933         Coordinates:
1934           * x        (x) int64 20 10 29 39
1935           * y        (y) int64 70 80 90
1936 
1937         Filling missing values with manually specified values:
1938 
1939         >>> da1.reindex_like(da2, fill_value=19)
1940         <xarray.DataArray (x: 4, y: 3)>
1941         array([[ 3,  4,  5],
1942                [ 0,  1,  2],
1943                [19, 19, 19],
1944                [19, 19, 19]])
1945         Coordinates:
1946           * x        (x) int64 20 10 29 39
1947           * y        (y) int64 70 80 90
1948 
1949         See Also
1950         --------
1951         DataArray.reindex
1952         align
1953         """
1954         return alignment.reindex_like(
1955             self,
1956             other=other,
1957             method=method,
1958             tolerance=tolerance,
1959             copy=copy,
1960             fill_value=fill_value,
1961         )
1962 
1963     def reindex(
1964         self: T_DataArray,
1965         indexers: Mapping[Any, Any] | None = None,
1966         method: ReindexMethodOptions = None,
1967         tolerance: float | Iterable[float] | None = None,
1968         copy: bool = True,
1969         fill_value=dtypes.NA,
1970         **indexers_kwargs: Any,
1971     ) -> T_DataArray:
1972         """Conform this object onto the indexes of another object, filling in
1973         missing values with ``fill_value``. The default fill value is NaN.
1974 
1975         Parameters
1976         ----------
1977         indexers : dict, optional
1978             Dictionary with keys given by dimension names and values given by
1979             arrays of coordinates tick labels. Any mis-matched coordinate
1980             values will be filled in with NaN, and any mis-matched dimension
1981             names will simply be ignored.
1982             One of indexers or indexers_kwargs must be provided.
1983         copy : bool, optional
1984             If ``copy=True``, data in the return value is always copied. If
1985             ``copy=False`` and reindexing is unnecessary, or can be performed
1986             with only slice operations, then the output may share memory with
1987             the input. In either case, a new xarray object is always returned.
1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1989             Method to use for filling index values in ``indexers`` not found on
1990             this data array:
1991 
1992             - None (default): don't fill gaps
1993             - pad / ffill: propagate last valid index value forward
1994             - backfill / bfill: propagate next valid index value backward
1995             - nearest: use nearest valid index value
1996 
1997         tolerance : float | Iterable[float] | None, default: None
1998             Maximum distance between original and new labels for inexact
1999             matches. The values of the index at the matching locations must
2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2001             Tolerance may be a scalar value, which applies the same tolerance
2002             to all values, or list-like, which applies variable tolerance per
2003             element. List-like must be the same size as the index and its dtype
2004             must exactly match the index’s type.
2005         fill_value : scalar or dict-like, optional
2006             Value to use for newly missing values. If a dict-like, maps
2007             variable names (including coordinates) to fill values. Use this
2008             data array's name to refer to the data array's values.
2009         **indexers_kwargs : {dim: indexer, ...}, optional
2010             The keyword arguments form of ``indexers``.
2011             One of indexers or indexers_kwargs must be provided.
2012 
2013         Returns
2014         -------
2015         reindexed : DataArray
2016             Another dataset array, with this array's data but replaced
2017             coordinates.
2018 
2019         Examples
2020         --------
2021         Reverse latitude:
2022 
2023         >>> da = xr.DataArray(
2024         ...     np.arange(4),
2025         ...     coords=[np.array([90, 89, 88, 87])],
2026         ...     dims="lat",
2027         ... )
2028         >>> da
2029         <xarray.DataArray (lat: 4)>
2030         array([0, 1, 2, 3])
2031         Coordinates:
2032           * lat      (lat) int64 90 89 88 87
2033         >>> da.reindex(lat=da.lat[::-1])
2034         <xarray.DataArray (lat: 4)>
2035         array([3, 2, 1, 0])
2036         Coordinates:
2037           * lat      (lat) int64 87 88 89 90
2038 
2039         See Also
2040         --------
2041         DataArray.reindex_like
2042         align
2043         """
2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2045         return alignment.reindex(
2046             self,
2047             indexers=indexers,
2048             method=method,
2049             tolerance=tolerance,
2050             copy=copy,
2051             fill_value=fill_value,
2052         )
2053 
2054     def interp(
2055         self: T_DataArray,
2056         coords: Mapping[Any, Any] | None = None,
2057         method: InterpOptions = "linear",
2058         assume_sorted: bool = False,
2059         kwargs: Mapping[str, Any] | None = None,
2060         **coords_kwargs: Any,
2061     ) -> T_DataArray:
2062         """Interpolate a DataArray onto new coordinates
2063 
2064         Performs univariate or multivariate interpolation of a DataArray onto
2065         new coordinates using scipy's interpolation routines. If interpolating
2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2067         called. When interpolating along multiple existing dimensions, an
2068         attempt is made to decompose the interpolation into multiple
2069         1-dimensional interpolations. If this is possible,
2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2071         :py:func:`scipy.interpolate.interpn` is called.
2072 
2073         Parameters
2074         ----------
2075         coords : dict, optional
2076             Mapping from dimension names to the new coordinates.
2077             New coordinate can be a scalar, array-like or DataArray.
2078             If DataArrays are passed as new coordinates, their dimensions are
2079             used for the broadcasting. Missing values are skipped.
2080         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2081             The method used to interpolate. The method should be supported by
2082             the scipy interpolator:
2083 
2084             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2085               "quadratic", "cubic", "polynomial"}
2086             - ``interpn``: {"linear", "nearest"}
2087 
2088             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2089             also be provided.
2090         assume_sorted : bool, default: False
2091             If False, values of x can be in any order and they are sorted
2092             first. If True, x has to be an array of monotonically increasing
2093             values.
2094         kwargs : dict-like or None, default: None
2095             Additional keyword arguments passed to scipy's interpolator. Valid
2096             options and their behavior depend whether ``interp1d`` or
2097             ``interpn`` is used.
2098         **coords_kwargs : {dim: coordinate, ...}, optional
2099             The keyword arguments form of ``coords``.
2100             One of coords or coords_kwargs must be provided.
2101 
2102         Returns
2103         -------
2104         interpolated : DataArray
2105             New dataarray on the new coordinates.
2106 
2107         Notes
2108         -----
2109         scipy is required.
2110 
2111         See Also
2112         --------
2113         scipy.interpolate.interp1d
2114         scipy.interpolate.interpn
2115 
2116         Examples
2117         --------
2118         >>> da = xr.DataArray(
2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2120         ...     dims=("x", "y"),
2121         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2122         ... )
2123         >>> da
2124         <xarray.DataArray (x: 3, y: 4)>
2125         array([[ 1.,  4.,  2.,  9.],
2126                [ 2.,  7.,  6., nan],
2127                [ 6., nan,  5.,  8.]])
2128         Coordinates:
2129           * x        (x) int64 0 1 2
2130           * y        (y) int64 10 12 14 16
2131 
2132         1D linear interpolation (the default):
2133 
2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2135         <xarray.DataArray (x: 4, y: 4)>
2136         array([[1.  , 4.  , 2.  ,  nan],
2137                [1.75, 6.25, 5.  ,  nan],
2138                [3.  ,  nan, 5.75,  nan],
2139                [5.  ,  nan, 5.25,  nan]])
2140         Coordinates:
2141           * y        (y) int64 10 12 14 16
2142           * x        (x) float64 0.0 0.75 1.25 1.75
2143 
2144         1D nearest interpolation:
2145 
2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2147         <xarray.DataArray (x: 4, y: 4)>
2148         array([[ 1.,  4.,  2.,  9.],
2149                [ 2.,  7.,  6., nan],
2150                [ 2.,  7.,  6., nan],
2151                [ 6., nan,  5.,  8.]])
2152         Coordinates:
2153           * y        (y) int64 10 12 14 16
2154           * x        (x) float64 0.0 0.75 1.25 1.75
2155 
2156         1D linear extrapolation:
2157 
2158         >>> da.interp(
2159         ...     x=[1, 1.5, 2.5, 3.5],
2160         ...     method="linear",
2161         ...     kwargs={"fill_value": "extrapolate"},
2162         ... )
2163         <xarray.DataArray (x: 4, y: 4)>
2164         array([[ 2. ,  7. ,  6. ,  nan],
2165                [ 4. ,  nan,  5.5,  nan],
2166                [ 8. ,  nan,  4.5,  nan],
2167                [12. ,  nan,  3.5,  nan]])
2168         Coordinates:
2169           * y        (y) int64 10 12 14 16
2170           * x        (x) float64 1.0 1.5 2.5 3.5
2171 
2172         2D linear interpolation:
2173 
2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2175         <xarray.DataArray (x: 4, y: 3)>
2176         array([[2.5  , 3.   ,   nan],
2177                [4.   , 5.625,   nan],
2178                [  nan,   nan,   nan],
2179                [  nan,   nan,   nan]])
2180         Coordinates:
2181           * x        (x) float64 0.0 0.75 1.25 1.75
2182           * y        (y) int64 11 13 15
2183         """
2184         if self.dtype.kind not in "uifc":
2185             raise TypeError(
2186                 "interp only works for a numeric type array. "
2187                 "Given {}.".format(self.dtype)
2188             )
2189         ds = self._to_temp_dataset().interp(
2190             coords,
2191             method=method,
2192             kwargs=kwargs,
2193             assume_sorted=assume_sorted,
2194             **coords_kwargs,
2195         )
2196         return self._from_temp_dataset(ds)
2197 
2198     def interp_like(
2199         self: T_DataArray,
2200         other: DataArray | Dataset,
2201         method: InterpOptions = "linear",
2202         assume_sorted: bool = False,
2203         kwargs: Mapping[str, Any] | None = None,
2204     ) -> T_DataArray:
2205         """Interpolate this object onto the coordinates of another object,
2206         filling out of range values with NaN.
2207 
2208         If interpolating along a single existing dimension,
2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2210         along multiple existing dimensions, an attempt is made to decompose the
2211         interpolation into multiple 1-dimensional interpolations. If this is
2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2213         :py:func:`scipy.interpolate.interpn` is called.
2214 
2215         Parameters
2216         ----------
2217         other : Dataset or DataArray
2218             Object with an 'indexes' attribute giving a mapping from dimension
2219             names to an 1d array-like, which provides coordinates upon
2220             which to index the variables in this dataset. Missing values are skipped.
2221         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2222             The method used to interpolate. The method should be supported by
2223             the scipy interpolator:
2224 
2225             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2226               "polynomial"} when ``interp1d`` is called.
2227             - {"linear", "nearest"} when ``interpn`` is called.
2228 
2229             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2230             also be provided.
2231         assume_sorted : bool, default: False
2232             If False, values of coordinates that are interpolated over can be
2233             in any order and they are sorted first. If True, interpolated
2234             coordinates are assumed to be an array of monotonically increasing
2235             values.
2236         kwargs : dict, optional
2237             Additional keyword passed to scipy's interpolator.
2238 
2239         Returns
2240         -------
2241         interpolated : DataArray
2242             Another dataarray by interpolating this dataarray's data along the
2243             coordinates of the other object.
2244 
2245         Examples
2246         --------
2247         >>> data = np.arange(12).reshape(4, 3)
2248         >>> da1 = xr.DataArray(
2249         ...     data=data,
2250         ...     dims=["x", "y"],
2251         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2252         ... )
2253         >>> da1
2254         <xarray.DataArray (x: 4, y: 3)>
2255         array([[ 0,  1,  2],
2256                [ 3,  4,  5],
2257                [ 6,  7,  8],
2258                [ 9, 10, 11]])
2259         Coordinates:
2260           * x        (x) int64 10 20 30 40
2261           * y        (y) int64 70 80 90
2262         >>> da2 = xr.DataArray(
2263         ...     data=data,
2264         ...     dims=["x", "y"],
2265         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2266         ... )
2267         >>> da2
2268         <xarray.DataArray (x: 4, y: 3)>
2269         array([[ 0,  1,  2],
2270                [ 3,  4,  5],
2271                [ 6,  7,  8],
2272                [ 9, 10, 11]])
2273         Coordinates:
2274           * x        (x) int64 10 20 29 39
2275           * y        (y) int64 70 80 90
2276 
2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2278 
2279         >>> da2.interp_like(da1)
2280         <xarray.DataArray (x: 4, y: 3)>
2281         array([[0. , 1. , 2. ],
2282                [3. , 4. , 5. ],
2283                [6.3, 7.3, 8.3],
2284                [nan, nan, nan]])
2285         Coordinates:
2286           * x        (x) int64 10 20 30 40
2287           * y        (y) int64 70 80 90
2288 
2289         Could also extrapolate missing values:
2290 
2291         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2292         <xarray.DataArray (x: 4, y: 3)>
2293         array([[ 0. ,  1. ,  2. ],
2294                [ 3. ,  4. ,  5. ],
2295                [ 6.3,  7.3,  8.3],
2296                [ 9.3, 10.3, 11.3]])
2297         Coordinates:
2298           * x        (x) int64 10 20 30 40
2299           * y        (y) int64 70 80 90
2300 
2301         Notes
2302         -----
2303         scipy is required.
2304         If the dataarray has object-type coordinates, reindex is used for these
2305         coordinates instead of the interpolation.
2306 
2307         See Also
2308         --------
2309         DataArray.interp
2310         DataArray.reindex_like
2311         """
2312         if self.dtype.kind not in "uifc":
2313             raise TypeError(
2314                 "interp only works for a numeric type array. "
2315                 "Given {}.".format(self.dtype)
2316             )
2317         ds = self._to_temp_dataset().interp_like(
2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2319         )
2320         return self._from_temp_dataset(ds)
2321 
2322     # change type of self and return to T_DataArray once
2323     # https://github.com/python/mypy/issues/12846 is resolved
2324     def rename(
2325         self,
2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2327         **names: Hashable,
2328     ) -> DataArray:
2329         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2330 
2331         Parameters
2332         ----------
2333         new_name_or_name_dict : str or dict-like, optional
2334             If the argument is dict-like, it used as a mapping from old
2335             names to new names for coordinates or dimensions. Otherwise,
2336             use the argument as the new name for this array.
2337         **names : Hashable, optional
2338             The keyword arguments form of a mapping from old names to
2339             new names for coordinates or dimensions.
2340             One of new_name_or_name_dict or names must be provided.
2341 
2342         Returns
2343         -------
2344         renamed : DataArray
2345             Renamed array or array with renamed coordinates.
2346 
2347         See Also
2348         --------
2349         Dataset.rename
2350         DataArray.swap_dims
2351         """
2352         if new_name_or_name_dict is None and not names:
2353             # change name to None?
2354             return self._replace(name=None)
2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2356             # change dims/coords
2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2358             dataset = self._to_temp_dataset()._rename(name_dict)
2359             return self._from_temp_dataset(dataset)
2360         if utils.hashable(new_name_or_name_dict) and names:
2361             # change name + dims/coords
2362             dataset = self._to_temp_dataset()._rename(names)
2363             dataarray = self._from_temp_dataset(dataset)
2364             return dataarray._replace(name=new_name_or_name_dict)
2365         # only change name
2366         return self._replace(name=new_name_or_name_dict)
2367 
2368     def swap_dims(
2369         self: T_DataArray,
2370         dims_dict: Mapping[Any, Hashable] | None = None,
2371         **dims_kwargs,
2372     ) -> T_DataArray:
2373         """Returns a new DataArray with swapped dimensions.
2374 
2375         Parameters
2376         ----------
2377         dims_dict : dict-like
2378             Dictionary whose keys are current dimension names and whose values
2379             are new names.
2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2381             The keyword arguments form of ``dims_dict``.
2382             One of dims_dict or dims_kwargs must be provided.
2383 
2384         Returns
2385         -------
2386         swapped : DataArray
2387             DataArray with swapped dimensions.
2388 
2389         Examples
2390         --------
2391         >>> arr = xr.DataArray(
2392         ...     data=[0, 1],
2393         ...     dims="x",
2394         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2395         ... )
2396         >>> arr
2397         <xarray.DataArray (x: 2)>
2398         array([0, 1])
2399         Coordinates:
2400           * x        (x) <U1 'a' 'b'
2401             y        (x) int64 0 1
2402 
2403         >>> arr.swap_dims({"x": "y"})
2404         <xarray.DataArray (y: 2)>
2405         array([0, 1])
2406         Coordinates:
2407             x        (y) <U1 'a' 'b'
2408           * y        (y) int64 0 1
2409 
2410         >>> arr.swap_dims({"x": "z"})
2411         <xarray.DataArray (z: 2)>
2412         array([0, 1])
2413         Coordinates:
2414             x        (z) <U1 'a' 'b'
2415             y        (z) int64 0 1
2416         Dimensions without coordinates: z
2417 
2418         See Also
2419         --------
2420         DataArray.rename
2421         Dataset.swap_dims
2422         """
2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2424         ds = self._to_temp_dataset().swap_dims(dims_dict)
2425         return self._from_temp_dataset(ds)
2426 
2427     # change type of self and return to T_DataArray once
2428     # https://github.com/python/mypy/issues/12846 is resolved
2429     def expand_dims(
2430         self,
2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2432         axis: None | int | Sequence[int] = None,
2433         **dim_kwargs: Any,
2434     ) -> DataArray:
2435         """Return a new object with an additional axis (or axes) inserted at
2436         the corresponding position in the array shape. The new object is a
2437         view into the underlying array, not a copy.
2438 
2439         If dim is already a scalar coordinate, it will be promoted to a 1D
2440         coordinate consisting of a single value.
2441 
2442         Parameters
2443         ----------
2444         dim : Hashable, sequence of Hashable, dict, or None, optional
2445             Dimensions to include on the new variable.
2446             If provided as str or sequence of str, then dimensions are inserted
2447             with length 1. If provided as a dict, then the keys are the new
2448             dimensions and the values are either integers (giving the length of
2449             the new dimensions) or sequence/ndarray (giving the coordinates of
2450             the new dimensions).
2451         axis : int, sequence of int, or None, default: None
2452             Axis position(s) where new axis is to be inserted (position(s) on
2453             the result array). If a sequence of integers is passed,
2454             multiple axes are inserted. In this case, dim arguments should be
2455             same length list. If axis=None is passed, all the axes will be
2456             inserted to the start of the result array.
2457         **dim_kwargs : int or sequence or ndarray
2458             The keywords are arbitrary dimensions being inserted and the values
2459             are either the lengths of the new dims (if int is given), or their
2460             coordinates. Note, this is an alternative to passing a dict to the
2461             dim kwarg and will only be used if dim is None.
2462 
2463         Returns
2464         -------
2465         expanded : DataArray
2466             This object, but with additional dimension(s).
2467 
2468         See Also
2469         --------
2470         Dataset.expand_dims
2471 
2472         Examples
2473         --------
2474         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2475         >>> da
2476         <xarray.DataArray (x: 5)>
2477         array([0, 1, 2, 3, 4])
2478         Dimensions without coordinates: x
2479 
2480         Add new dimension of length 2:
2481 
2482         >>> da.expand_dims(dim={"y": 2})
2483         <xarray.DataArray (y: 2, x: 5)>
2484         array([[0, 1, 2, 3, 4],
2485                [0, 1, 2, 3, 4]])
2486         Dimensions without coordinates: y, x
2487 
2488         >>> da.expand_dims(dim={"y": 2}, axis=1)
2489         <xarray.DataArray (x: 5, y: 2)>
2490         array([[0, 0],
2491                [1, 1],
2492                [2, 2],
2493                [3, 3],
2494                [4, 4]])
2495         Dimensions without coordinates: x, y
2496 
2497         Add a new dimension with coordinates from array:
2498 
2499         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2500         <xarray.DataArray (y: 5, x: 5)>
2501         array([[0, 1, 2, 3, 4],
2502                [0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4]])
2506         Coordinates:
2507           * y        (y) int64 0 1 2 3 4
2508         Dimensions without coordinates: x
2509         """
2510         if isinstance(dim, int):
2511             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2513             if len(dim) != len(set(dim)):
2514                 raise ValueError("dims should not contain duplicate values.")
2515             dim = dict.fromkeys(dim, 1)
2516         elif dim is not None and not isinstance(dim, Mapping):
2517             dim = {cast(Hashable, dim): 1}
2518 
2519         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2520         ds = self._to_temp_dataset().expand_dims(dim, axis)
2521         return self._from_temp_dataset(ds)
2522 
2523     # change type of self and return to T_DataArray once
2524     # https://github.com/python/mypy/issues/12846 is resolved
2525     def set_index(
2526         self,
2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2528         append: bool = False,
2529         **indexes_kwargs: Hashable | Sequence[Hashable],
2530     ) -> DataArray:
2531         """Set DataArray (multi-)indexes using one or more existing
2532         coordinates.
2533 
2534         This legacy method is limited to pandas (multi-)indexes and
2535         1-dimensional "dimension" coordinates. See
2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2537         Xarray-compatible index from one or more arbitrary coordinates.
2538 
2539         Parameters
2540         ----------
2541         indexes : {dim: index, ...}
2542             Mapping from names matching dimensions and values given
2543             by (lists of) the names of existing coordinates or variables to set
2544             as new (multi-)index.
2545         append : bool, default: False
2546             If True, append the supplied index(es) to the existing index(es).
2547             Otherwise replace the existing index(es).
2548         **indexes_kwargs : optional
2549             The keyword arguments form of ``indexes``.
2550             One of indexes or indexes_kwargs must be provided.
2551 
2552         Returns
2553         -------
2554         obj : DataArray
2555             Another DataArray, with this data but replaced coordinates.
2556 
2557         Examples
2558         --------
2559         >>> arr = xr.DataArray(
2560         ...     data=np.ones((2, 3)),
2561         ...     dims=["x", "y"],
2562         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2563         ... )
2564         >>> arr
2565         <xarray.DataArray (x: 2, y: 3)>
2566         array([[1., 1., 1.],
2567                [1., 1., 1.]])
2568         Coordinates:
2569           * x        (x) int64 0 1
2570           * y        (y) int64 0 1 2
2571             a        (x) int64 3 4
2572         >>> arr.set_index(x="a")
2573         <xarray.DataArray (x: 2, y: 3)>
2574         array([[1., 1., 1.],
2575                [1., 1., 1.]])
2576         Coordinates:
2577           * x        (x) int64 3 4
2578           * y        (y) int64 0 1 2
2579 
2580         See Also
2581         --------
2582         DataArray.reset_index
2583         DataArray.set_xindex
2584         """
2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2586         return self._from_temp_dataset(ds)
2587 
2588     # change type of self and return to T_DataArray once
2589     # https://github.com/python/mypy/issues/12846 is resolved
2590     def reset_index(
2591         self,
2592         dims_or_levels: Hashable | Sequence[Hashable],
2593         drop: bool = False,
2594     ) -> DataArray:
2595         """Reset the specified index(es) or multi-index level(s).
2596 
2597         This legacy method is specific to pandas (multi-)indexes and
2598         1-dimensional "dimension" coordinates. See the more generic
2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2600         method to respectively drop and set pandas or custom indexes for
2601         arbitrary coordinates.
2602 
2603         Parameters
2604         ----------
2605         dims_or_levels : Hashable or sequence of Hashable
2606             Name(s) of the dimension(s) and/or multi-index level(s) that will
2607             be reset.
2608         drop : bool, default: False
2609             If True, remove the specified indexes and/or multi-index levels
2610             instead of extracting them as new coordinates (default: False).
2611 
2612         Returns
2613         -------
2614         obj : DataArray
2615             Another dataarray, with this dataarray's data but replaced
2616             coordinates.
2617 
2618         See Also
2619         --------
2620         DataArray.set_index
2621         DataArray.set_xindex
2622         DataArray.drop_indexes
2623         """
2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2625         return self._from_temp_dataset(ds)
2626 
2627     def set_xindex(
2628         self: T_DataArray,
2629         coord_names: str | Sequence[Hashable],
2630         index_cls: type[Index] | None = None,
2631         **options,
2632     ) -> T_DataArray:
2633         """Set a new, Xarray-compatible index from one or more existing
2634         coordinate(s).
2635 
2636         Parameters
2637         ----------
2638         coord_names : str or list
2639             Name(s) of the coordinate(s) used to build the index.
2640             If several names are given, their order matters.
2641         index_cls : subclass of :class:`~xarray.indexes.Index`
2642             The type of index to create. By default, try setting
2643             a pandas (multi-)index from the supplied coordinates.
2644         **options
2645             Options passed to the index constructor.
2646 
2647         Returns
2648         -------
2649         obj : DataArray
2650             Another dataarray, with this dataarray's data and with a new index.
2651 
2652         """
2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2654         return self._from_temp_dataset(ds)
2655 
2656     def reorder_levels(
2657         self: T_DataArray,
2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2659         **dim_order_kwargs: Sequence[int | Hashable],
2660     ) -> T_DataArray:
2661         """Rearrange index levels using input order.
2662 
2663         Parameters
2664         ----------
2665         dim_order dict-like of Hashable to int or Hashable: optional
2666             Mapping from names matching dimensions and values given
2667             by lists representing new level orders. Every given dimension
2668             must have a multi-index.
2669         **dim_order_kwargs : optional
2670             The keyword arguments form of ``dim_order``.
2671             One of dim_order or dim_order_kwargs must be provided.
2672 
2673         Returns
2674         -------
2675         obj : DataArray
2676             Another dataarray, with this dataarray's data but replaced
2677             coordinates.
2678         """
2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2680         return self._from_temp_dataset(ds)
2681 
2682     def stack(
2683         self: T_DataArray,
2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2685         create_index: bool | None = True,
2686         index_cls: type[Index] = PandasMultiIndex,
2687         **dimensions_kwargs: Sequence[Hashable],
2688     ) -> T_DataArray:
2689         """
2690         Stack any number of existing dimensions into a single new dimension.
2691 
2692         New dimensions will be added at the end, and the corresponding
2693         coordinate variables will be combined into a MultiIndex.
2694 
2695         Parameters
2696         ----------
2697         dimensions : mapping of Hashable to sequence of Hashable
2698             Mapping of the form `new_name=(dim1, dim2, ...)`.
2699             Names of new dimensions, and the existing dimensions that they
2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2702             all dimensions.
2703         create_index : bool or None, default: True
2704             If True, create a multi-index for each of the stacked dimensions.
2705             If False, don't create any index.
2706             If None, create a multi-index only if exactly one single (1-d) coordinate
2707             index is found for every dimension to stack.
2708         index_cls: class, optional
2709             Can be used to pass a custom multi-index type. Must be an Xarray index that
2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2711         **dimensions_kwargs
2712             The keyword arguments form of ``dimensions``.
2713             One of dimensions or dimensions_kwargs must be provided.
2714 
2715         Returns
2716         -------
2717         stacked : DataArray
2718             DataArray with stacked data.
2719 
2720         Examples
2721         --------
2722         >>> arr = xr.DataArray(
2723         ...     np.arange(6).reshape(2, 3),
2724         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2725         ... )
2726         >>> arr
2727         <xarray.DataArray (x: 2, y: 3)>
2728         array([[0, 1, 2],
2729                [3, 4, 5]])
2730         Coordinates:
2731           * x        (x) <U1 'a' 'b'
2732           * y        (y) int64 0 1 2
2733         >>> stacked = arr.stack(z=("x", "y"))
2734         >>> stacked.indexes["z"]
2735         MultiIndex([('a', 0),
2736                     ('a', 1),
2737                     ('a', 2),
2738                     ('b', 0),
2739                     ('b', 1),
2740                     ('b', 2)],
2741                    name='z')
2742 
2743         See Also
2744         --------
2745         DataArray.unstack
2746         """
2747         ds = self._to_temp_dataset().stack(
2748             dimensions,
2749             create_index=create_index,
2750             index_cls=index_cls,
2751             **dimensions_kwargs,
2752         )
2753         return self._from_temp_dataset(ds)
2754 
2755     # change type of self and return to T_DataArray once
2756     # https://github.com/python/mypy/issues/12846 is resolved
2757     def unstack(
2758         self,
2759         dim: Dims = None,
2760         fill_value: Any = dtypes.NA,
2761         sparse: bool = False,
2762     ) -> DataArray:
2763         """
2764         Unstack existing dimensions corresponding to MultiIndexes into
2765         multiple new dimensions.
2766 
2767         New dimensions will be added at the end.
2768 
2769         Parameters
2770         ----------
2771         dim : str, Iterable of Hashable or None, optional
2772             Dimension(s) over which to unstack. By default unstacks all
2773             MultiIndexes.
2774         fill_value : scalar or dict-like, default: nan
2775             Value to be filled. If a dict-like, maps variable names to
2776             fill values. Use the data array's name to refer to its
2777             name. If not provided or if the dict-like does not contain
2778             all variables, the dtype's NA value will be used.
2779         sparse : bool, default: False
2780             Use sparse-array if True
2781 
2782         Returns
2783         -------
2784         unstacked : DataArray
2785             Array with unstacked data.
2786 
2787         Examples
2788         --------
2789         >>> arr = xr.DataArray(
2790         ...     np.arange(6).reshape(2, 3),
2791         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2792         ... )
2793         >>> arr
2794         <xarray.DataArray (x: 2, y: 3)>
2795         array([[0, 1, 2],
2796                [3, 4, 5]])
2797         Coordinates:
2798           * x        (x) <U1 'a' 'b'
2799           * y        (y) int64 0 1 2
2800         >>> stacked = arr.stack(z=("x", "y"))
2801         >>> stacked.indexes["z"]
2802         MultiIndex([('a', 0),
2803                     ('a', 1),
2804                     ('a', 2),
2805                     ('b', 0),
2806                     ('b', 1),
2807                     ('b', 2)],
2808                    name='z')
2809         >>> roundtripped = stacked.unstack()
2810         >>> arr.identical(roundtripped)
2811         True
2812 
2813         See Also
2814         --------
2815         DataArray.stack
2816         """
2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2818         return self._from_temp_dataset(ds)
2819 
2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2821         """Unstack DataArray expanding to Dataset along a given level of a
2822         stacked coordinate.
2823 
2824         This is the inverse operation of Dataset.to_stacked_array.
2825 
2826         Parameters
2827         ----------
2828         dim : Hashable
2829             Name of existing dimension to unstack
2830         level : int or Hashable, default: 0
2831             The MultiIndex level to expand to a dataset along. Can either be
2832             the integer index of the level or its name.
2833 
2834         Returns
2835         -------
2836         unstacked: Dataset
2837 
2838         Examples
2839         --------
2840         >>> arr = xr.DataArray(
2841         ...     np.arange(6).reshape(2, 3),
2842         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2843         ... )
2844         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2845         >>> data
2846         <xarray.Dataset>
2847         Dimensions:  (x: 2, y: 3)
2848         Coordinates:
2849           * x        (x) <U1 'a' 'b'
2850           * y        (y) int64 0 1 2
2851         Data variables:
2852             a        (x, y) int64 0 1 2 3 4 5
2853             b        (x) int64 0 3
2854         >>> stacked = data.to_stacked_array("z", ["x"])
2855         >>> stacked.indexes["z"]
2856         MultiIndex([('a', 0.0),
2857                     ('a', 1.0),
2858                     ('a', 2.0),
2859                     ('b', nan)],
2860                    name='z')
2861         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2862         >>> data.identical(roundtripped)
2863         True
2864 
2865         See Also
2866         --------
2867         Dataset.to_stacked_array
2868         """
2869         idx = self._indexes[dim].to_pandas_index()
2870         if not isinstance(idx, pd.MultiIndex):
2871             raise ValueError(f"'{dim}' is not a stacked coordinate")
2872 
2873         level_number = idx._get_level_number(level)
2874         variables = idx.levels[level_number]
2875         variable_dim = idx.names[level_number]
2876 
2877         # pull variables out of datarray
2878         data_dict = {}
2879         for k in variables:
2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2881 
2882         # unstacked dataset
2883         return Dataset(data_dict)
2884 
2885     def transpose(
2886         self: T_DataArray,
2887         *dims: Hashable,
2888         transpose_coords: bool = True,
2889         missing_dims: ErrorOptionsWithWarn = "raise",
2890     ) -> T_DataArray:
2891         """Return a new DataArray object with transposed dimensions.
2892 
2893         Parameters
2894         ----------
2895         *dims : Hashable, optional
2896             By default, reverse the dimensions. Otherwise, reorder the
2897             dimensions to this order.
2898         transpose_coords : bool, default: True
2899             If True, also transpose the coordinates of this DataArray.
2900         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2901             What to do if dimensions that should be selected from are not present in the
2902             DataArray:
2903             - "raise": raise an exception
2904             - "warn": raise a warning, and ignore the missing dimensions
2905             - "ignore": ignore the missing dimensions
2906 
2907         Returns
2908         -------
2909         transposed : DataArray
2910             The returned DataArray's array is transposed.
2911 
2912         Notes
2913         -----
2914         This operation returns a view of this array's data. It is
2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2916         -- the data will be fully loaded.
2917 
2918         See Also
2919         --------
2920         numpy.transpose
2921         Dataset.transpose
2922         """
2923         if dims:
2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2925         variable = self.variable.transpose(*dims)
2926         if transpose_coords:
2927             coords: dict[Hashable, Variable] = {}
2928             for name, coord in self.coords.items():
2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2930                 coords[name] = coord.variable.transpose(*coord_dims)
2931             return self._replace(variable, coords)
2932         else:
2933             return self._replace(variable)
2934 
2935     @property
2936     def T(self: T_DataArray) -> T_DataArray:
2937         return self.transpose()
2938 
2939     # change type of self and return to T_DataArray once
2940     # https://github.com/python/mypy/issues/12846 is resolved
2941     def drop_vars(
2942         self,
2943         names: Hashable | Iterable[Hashable],
2944         *,
2945         errors: ErrorOptions = "raise",
2946     ) -> DataArray:
2947         """Returns an array with dropped variables.
2948 
2949         Parameters
2950         ----------
2951         names : Hashable or iterable of Hashable
2952             Name(s) of variables to drop.
2953         errors : {"raise", "ignore"}, default: "raise"
2954             If 'raise', raises a ValueError error if any of the variable
2955             passed are not in the dataset. If 'ignore', any given names that are in the
2956             DataArray are dropped and no error is raised.
2957 
2958         Returns
2959         -------
2960         dropped : Dataset
2961             New Dataset copied from `self` with variables removed.
2962 
2963         Examples
2964         -------
2965         >>> data = np.arange(12).reshape(4, 3)
2966         >>> da = xr.DataArray(
2967         ...     data=data,
2968         ...     dims=["x", "y"],
2969         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2970         ... )
2971         >>> da
2972         <xarray.DataArray (x: 4, y: 3)>
2973         array([[ 0,  1,  2],
2974                [ 3,  4,  5],
2975                [ 6,  7,  8],
2976                [ 9, 10, 11]])
2977         Coordinates:
2978           * x        (x) int64 10 20 30 40
2979           * y        (y) int64 70 80 90
2980 
2981         Removing a single variable:
2982 
2983         >>> da.drop_vars("x")
2984         <xarray.DataArray (x: 4, y: 3)>
2985         array([[ 0,  1,  2],
2986                [ 3,  4,  5],
2987                [ 6,  7,  8],
2988                [ 9, 10, 11]])
2989         Coordinates:
2990           * y        (y) int64 70 80 90
2991         Dimensions without coordinates: x
2992 
2993         Removing a list of variables:
2994 
2995         >>> da.drop_vars(["x", "y"])
2996         <xarray.DataArray (x: 4, y: 3)>
2997         array([[ 0,  1,  2],
2998                [ 3,  4,  5],
2999                [ 6,  7,  8],
3000                [ 9, 10, 11]])
3001         Dimensions without coordinates: x, y
3002         """
3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3004         return self._from_temp_dataset(ds)
3005 
3006     def drop_indexes(
3007         self: T_DataArray,
3008         coord_names: Hashable | Iterable[Hashable],
3009         *,
3010         errors: ErrorOptions = "raise",
3011     ) -> T_DataArray:
3012         """Drop the indexes assigned to the given coordinates.
3013 
3014         Parameters
3015         ----------
3016         coord_names : hashable or iterable of hashable
3017             Name(s) of the coordinate(s) for which to drop the index.
3018         errors : {"raise", "ignore"}, default: "raise"
3019             If 'raise', raises a ValueError error if any of the coordinates
3020             passed have no index or are not in the dataset.
3021             If 'ignore', no error is raised.
3022 
3023         Returns
3024         -------
3025         dropped : DataArray
3026             A new dataarray with dropped indexes.
3027         """
3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3029         return self._from_temp_dataset(ds)
3030 
3031     def drop(
3032         self: T_DataArray,
3033         labels: Mapping[Any, Any] | None = None,
3034         dim: Hashable | None = None,
3035         *,
3036         errors: ErrorOptions = "raise",
3037         **labels_kwargs,
3038     ) -> T_DataArray:
3039         """Backward compatible method based on `drop_vars` and `drop_sel`
3040 
3041         Using either `drop_vars` or `drop_sel` is encouraged
3042 
3043         See Also
3044         --------
3045         DataArray.drop_vars
3046         DataArray.drop_sel
3047         """
3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3049         return self._from_temp_dataset(ds)
3050 
3051     def drop_sel(
3052         self: T_DataArray,
3053         labels: Mapping[Any, Any] | None = None,
3054         *,
3055         errors: ErrorOptions = "raise",
3056         **labels_kwargs,
3057     ) -> T_DataArray:
3058         """Drop index labels from this DataArray.
3059 
3060         Parameters
3061         ----------
3062         labels : mapping of Hashable to Any
3063             Index labels to drop
3064         errors : {"raise", "ignore"}, default: "raise"
3065             If 'raise', raises a ValueError error if
3066             any of the index labels passed are not
3067             in the dataset. If 'ignore', any given labels that are in the
3068             dataset are dropped and no error is raised.
3069         **labels_kwargs : {dim: label, ...}, optional
3070             The keyword arguments form of ``dim`` and ``labels``
3071 
3072         Returns
3073         -------
3074         dropped : DataArray
3075 
3076         Examples
3077         --------
3078         >>> da = xr.DataArray(
3079         ...     np.arange(25).reshape(5, 5),
3080         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3081         ...     dims=("x", "y"),
3082         ... )
3083         >>> da
3084         <xarray.DataArray (x: 5, y: 5)>
3085         array([[ 0,  1,  2,  3,  4],
3086                [ 5,  6,  7,  8,  9],
3087                [10, 11, 12, 13, 14],
3088                [15, 16, 17, 18, 19],
3089                [20, 21, 22, 23, 24]])
3090         Coordinates:
3091           * x        (x) int64 0 2 4 6 8
3092           * y        (y) int64 0 3 6 9 12
3093 
3094         >>> da.drop_sel(x=[0, 2], y=9)
3095         <xarray.DataArray (x: 3, y: 4)>
3096         array([[10, 11, 12, 14],
3097                [15, 16, 17, 19],
3098                [20, 21, 22, 24]])
3099         Coordinates:
3100           * x        (x) int64 4 6 8
3101           * y        (y) int64 0 3 6 12
3102 
3103         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3104         <xarray.DataArray (x: 4, y: 3)>
3105         array([[ 2,  3,  4],
3106                [ 7,  8,  9],
3107                [12, 13, 14],
3108                [22, 23, 24]])
3109         Coordinates:
3110           * x        (x) int64 0 2 4 8
3111           * y        (y) int64 6 9 12
3112         """
3113         if labels_kwargs or isinstance(labels, dict):
3114             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3115 
3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3117         return self._from_temp_dataset(ds)
3118 
3119     def drop_isel(
3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3121     ) -> T_DataArray:
3122         """Drop index positions from this DataArray.
3123 
3124         Parameters
3125         ----------
3126         indexers : mapping of Hashable to Any or None, default: None
3127             Index locations to drop
3128         **indexers_kwargs : {dim: position, ...}, optional
3129             The keyword arguments form of ``dim`` and ``positions``
3130 
3131         Returns
3132         -------
3133         dropped : DataArray
3134 
3135         Raises
3136         ------
3137         IndexError
3138 
3139         Examples
3140         --------
3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3142         >>> da
3143         <xarray.DataArray (X: 5, Y: 5)>
3144         array([[ 0,  1,  2,  3,  4],
3145                [ 5,  6,  7,  8,  9],
3146                [10, 11, 12, 13, 14],
3147                [15, 16, 17, 18, 19],
3148                [20, 21, 22, 23, 24]])
3149         Dimensions without coordinates: X, Y
3150 
3151         >>> da.drop_isel(X=[0, 4], Y=2)
3152         <xarray.DataArray (X: 3, Y: 4)>
3153         array([[ 5,  6,  8,  9],
3154                [10, 11, 13, 14],
3155                [15, 16, 18, 19]])
3156         Dimensions without coordinates: X, Y
3157 
3158         >>> da.drop_isel({"X": 3, "Y": 3})
3159         <xarray.DataArray (X: 4, Y: 4)>
3160         array([[ 0,  1,  2,  4],
3161                [ 5,  6,  7,  9],
3162                [10, 11, 12, 14],
3163                [20, 21, 22, 24]])
3164         Dimensions without coordinates: X, Y
3165         """
3166         dataset = self._to_temp_dataset()
3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3168         return self._from_temp_dataset(dataset)
3169 
3170     def dropna(
3171         self: T_DataArray,
3172         dim: Hashable,
3173         how: Literal["any", "all"] = "any",
3174         thresh: int | None = None,
3175     ) -> T_DataArray:
3176         """Returns a new array with dropped labels for missing values along
3177         the provided dimension.
3178 
3179         Parameters
3180         ----------
3181         dim : Hashable
3182             Dimension along which to drop missing values. Dropping along
3183             multiple dimensions simultaneously is not yet supported.
3184         how : {"any", "all"}, default: "any"
3185             - any : if any NA values are present, drop that label
3186             - all : if all values are NA, drop that label
3187 
3188         thresh : int or None, default: None
3189             If supplied, require this many non-NA values.
3190 
3191         Returns
3192         -------
3193         dropped : DataArray
3194 
3195         Examples
3196         --------
3197         >>> temperature = [
3198         ...     [0, 4, 2, 9],
3199         ...     [np.nan, np.nan, np.nan, np.nan],
3200         ...     [np.nan, 4, 2, 0],
3201         ...     [3, 1, 0, 0],
3202         ... ]
3203         >>> da = xr.DataArray(
3204         ...     data=temperature,
3205         ...     dims=["Y", "X"],
3206         ...     coords=dict(
3207         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3208         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3209         ...     ),
3210         ... )
3211         >>> da
3212         <xarray.DataArray (Y: 4, X: 4)>
3213         array([[ 0.,  4.,  2.,  9.],
3214                [nan, nan, nan, nan],
3215                [nan,  4.,  2.,  0.],
3216                [ 3.,  1.,  0.,  0.]])
3217         Coordinates:
3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3219             lon      (X) float64 10.0 10.25 10.5 10.75
3220         Dimensions without coordinates: Y, X
3221 
3222         >>> da.dropna(dim="Y", how="any")
3223         <xarray.DataArray (Y: 2, X: 4)>
3224         array([[0., 4., 2., 9.],
3225                [3., 1., 0., 0.]])
3226         Coordinates:
3227             lat      (Y) float64 -20.0 -20.75
3228             lon      (X) float64 10.0 10.25 10.5 10.75
3229         Dimensions without coordinates: Y, X
3230 
3231         Drop values only if all values along the dimension are NaN:
3232 
3233         >>> da.dropna(dim="Y", how="all")
3234         <xarray.DataArray (Y: 3, X: 4)>
3235         array([[ 0.,  4.,  2.,  9.],
3236                [nan,  4.,  2.,  0.],
3237                [ 3.,  1.,  0.,  0.]])
3238         Coordinates:
3239             lat      (Y) float64 -20.0 -20.5 -20.75
3240             lon      (X) float64 10.0 10.25 10.5 10.75
3241         Dimensions without coordinates: Y, X
3242         """
3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3244         return self._from_temp_dataset(ds)
3245 
3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3247         """Fill missing values in this object.
3248 
3249         This operation follows the normal broadcasting and alignment rules that
3250         xarray uses for binary arithmetic, except the result is aligned to this
3251         object (``join='left'``) instead of aligned to the intersection of
3252         index coordinates (``join='inner'``).
3253 
3254         Parameters
3255         ----------
3256         value : scalar, ndarray or DataArray
3257             Used to fill all matching missing values in this array. If the
3258             argument is a DataArray, it is first aligned with (reindexed to)
3259             this array.
3260 
3261         Returns
3262         -------
3263         filled : DataArray
3264 
3265         Examples
3266         --------
3267         >>> da = xr.DataArray(
3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3269         ...     dims="Z",
3270         ...     coords=dict(
3271         ...         Z=("Z", np.arange(6)),
3272         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3273         ...     ),
3274         ... )
3275         >>> da
3276         <xarray.DataArray (Z: 6)>
3277         array([ 1.,  4., nan,  0.,  3., nan])
3278         Coordinates:
3279           * Z        (Z) int64 0 1 2 3 4 5
3280             height   (Z) int64 0 10 20 30 40 50
3281 
3282         Fill all NaN values with 0:
3283 
3284         >>> da.fillna(0)
3285         <xarray.DataArray (Z: 6)>
3286         array([1., 4., 0., 0., 3., 0.])
3287         Coordinates:
3288           * Z        (Z) int64 0 1 2 3 4 5
3289             height   (Z) int64 0 10 20 30 40 50
3290 
3291         Fill NaN values with corresponding values in array:
3292 
3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3294         <xarray.DataArray (Z: 6)>
3295         array([1., 4., 4., 0., 3., 9.])
3296         Coordinates:
3297           * Z        (Z) int64 0 1 2 3 4 5
3298             height   (Z) int64 0 10 20 30 40 50
3299         """
3300         if utils.is_dict_like(value):
3301             raise TypeError(
3302                 "cannot provide fill value as a dictionary with "
3303                 "fillna on a DataArray"
3304             )
3305         out = ops.fillna(self, value)
3306         return out
3307 
3308     def interpolate_na(
3309         self: T_DataArray,
3310         dim: Hashable | None = None,
3311         method: InterpOptions = "linear",
3312         limit: int | None = None,
3313         use_coordinate: bool | str = True,
3314         max_gap: (
3315             None
3316             | int
3317             | float
3318             | str
3319             | pd.Timedelta
3320             | np.timedelta64
3321             | datetime.timedelta
3322         ) = None,
3323         keep_attrs: bool | None = None,
3324         **kwargs: Any,
3325     ) -> T_DataArray:
3326         """Fill in NaNs by interpolating according to different methods.
3327 
3328         Parameters
3329         ----------
3330         dim : Hashable or None, optional
3331             Specifies the dimension along which to interpolate.
3332         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3333             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3334             String indicating which method to use for interpolation:
3335 
3336             - 'linear': linear interpolation. Additional keyword
3337               arguments are passed to :py:func:`numpy.interp`
3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3339               are passed to :py:func:`scipy.interpolate.interp1d`. If
3340               ``method='polynomial'``, the ``order`` keyword argument must also be
3341               provided.
3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3343               respective :py:class:`scipy.interpolate` classes.
3344 
3345         use_coordinate : bool or str, default: True
3346             Specifies which index to use as the x values in the interpolation
3347             formulated as `y = f(x)`. If False, values are treated as if
3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
3349             used. If ``use_coordinate`` is a string, it specifies the name of a
3350             coordinate variable to use as the index.
3351         limit : int or None, default: None
3352             Maximum number of consecutive NaNs to fill. Must be greater than 0
3353             or None for no limit. This filling is done regardless of the size of
3354             the gap in the data. To only interpolate over gaps less than a given length,
3355             see ``max_gap``.
3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3358             Use None for no limit. When interpolating along a datetime64 dimension
3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3360 
3361             - a string that is valid input for pandas.to_timedelta
3362             - a :py:class:`numpy.timedelta64` object
3363             - a :py:class:`pandas.Timedelta` object
3364             - a :py:class:`datetime.timedelta` object
3365 
3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3367             dimensions has not been implemented yet. Gap length is defined as the difference
3368             between coordinate values at the first data point after a gap and the last value
3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3370             between coordinate values at the first (last) valid data point and the first (last) NaN.
3371             For example, consider::
3372 
3373                 <xarray.DataArray (x: 9)>
3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3375                 Coordinates:
3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3377 
3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3379         keep_attrs : bool or None, default: None
3380             If True, the dataarray's attributes (`attrs`) will be copied from
3381             the original object to the new one.  If False, the new
3382             object will be returned without attributes.
3383         **kwargs : dict, optional
3384             parameters passed verbatim to the underlying interpolation function
3385 
3386         Returns
3387         -------
3388         interpolated: DataArray
3389             Filled in DataArray.
3390 
3391         See Also
3392         --------
3393         numpy.interp
3394         scipy.interpolate
3395 
3396         Examples
3397         --------
3398         >>> da = xr.DataArray(
3399         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3400         ... )
3401         >>> da
3402         <xarray.DataArray (x: 5)>
3403         array([nan,  2.,  3., nan,  0.])
3404         Coordinates:
3405           * x        (x) int64 0 1 2 3 4
3406 
3407         >>> da.interpolate_na(dim="x", method="linear")
3408         <xarray.DataArray (x: 5)>
3409         array([nan, 2. , 3. , 1.5, 0. ])
3410         Coordinates:
3411           * x        (x) int64 0 1 2 3 4
3412 
3413         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3414         <xarray.DataArray (x: 5)>
3415         array([1. , 2. , 3. , 1.5, 0. ])
3416         Coordinates:
3417           * x        (x) int64 0 1 2 3 4
3418         """
3419         from xarray.core.missing import interp_na
3420 
3421         return interp_na(
3422             self,
3423             dim=dim,
3424             method=method,
3425             limit=limit,
3426             use_coordinate=use_coordinate,
3427             max_gap=max_gap,
3428             keep_attrs=keep_attrs,
3429             **kwargs,
3430         )
3431 
3432     def ffill(
3433         self: T_DataArray, dim: Hashable, limit: int | None = None
3434     ) -> T_DataArray:
3435         """Fill NaN values by propagating values forward
3436 
3437         *Requires bottleneck.*
3438 
3439         Parameters
3440         ----------
3441         dim : Hashable
3442             Specifies the dimension along which to propagate values when
3443             filling.
3444         limit : int or None, default: None
3445             The maximum number of consecutive NaN values to forward fill. In
3446             other words, if there is a gap with more than this number of
3447             consecutive NaNs, it will only be partially filled. Must be greater
3448             than 0 or None for no limit. Must be None or greater than or equal
3449             to axis length if filling along chunked axes (dimensions).
3450 
3451         Returns
3452         -------
3453         filled : DataArray
3454 
3455         Examples
3456         --------
3457         >>> temperature = np.array(
3458         ...     [
3459         ...         [np.nan, 1, 3],
3460         ...         [0, np.nan, 5],
3461         ...         [5, np.nan, np.nan],
3462         ...         [3, np.nan, np.nan],
3463         ...         [0, 2, 0],
3464         ...     ]
3465         ... )
3466         >>> da = xr.DataArray(
3467         ...     data=temperature,
3468         ...     dims=["Y", "X"],
3469         ...     coords=dict(
3470         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3471         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3472         ...     ),
3473         ... )
3474         >>> da
3475         <xarray.DataArray (Y: 5, X: 3)>
3476         array([[nan,  1.,  3.],
3477                [ 0., nan,  5.],
3478                [ 5., nan, nan],
3479                [ 3., nan, nan],
3480                [ 0.,  2.,  0.]])
3481         Coordinates:
3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3483             lon      (X) float64 10.0 10.25 10.5
3484         Dimensions without coordinates: Y, X
3485 
3486         Fill all NaN values:
3487 
3488         >>> da.ffill(dim="Y", limit=None)
3489         <xarray.DataArray (Y: 5, X: 3)>
3490         array([[nan,  1.,  3.],
3491                [ 0.,  1.,  5.],
3492                [ 5.,  1.,  5.],
3493                [ 3.,  1.,  5.],
3494                [ 0.,  2.,  0.]])
3495         Coordinates:
3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3497             lon      (X) float64 10.0 10.25 10.5
3498         Dimensions without coordinates: Y, X
3499 
3500         Fill only the first of consecutive NaN values:
3501 
3502         >>> da.ffill(dim="Y", limit=1)
3503         <xarray.DataArray (Y: 5, X: 3)>
3504         array([[nan,  1.,  3.],
3505                [ 0.,  1.,  5.],
3506                [ 5., nan,  5.],
3507                [ 3., nan, nan],
3508                [ 0.,  2.,  0.]])
3509         Coordinates:
3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3511             lon      (X) float64 10.0 10.25 10.5
3512         Dimensions without coordinates: Y, X
3513         """
3514         from xarray.core.missing import ffill
3515 
3516         return ffill(self, dim, limit=limit)
3517 
3518     def bfill(
3519         self: T_DataArray, dim: Hashable, limit: int | None = None
3520     ) -> T_DataArray:
3521         """Fill NaN values by propagating values backward
3522 
3523         *Requires bottleneck.*
3524 
3525         Parameters
3526         ----------
3527         dim : str
3528             Specifies the dimension along which to propagate values when
3529             filling.
3530         limit : int or None, default: None
3531             The maximum number of consecutive NaN values to backward fill. In
3532             other words, if there is a gap with more than this number of
3533             consecutive NaNs, it will only be partially filled. Must be greater
3534             than 0 or None for no limit. Must be None or greater than or equal
3535             to axis length if filling along chunked axes (dimensions).
3536 
3537         Returns
3538         -------
3539         filled : DataArray
3540 
3541         Examples
3542         --------
3543         >>> temperature = np.array(
3544         ...     [
3545         ...         [0, 1, 3],
3546         ...         [0, np.nan, 5],
3547         ...         [5, np.nan, np.nan],
3548         ...         [3, np.nan, np.nan],
3549         ...         [np.nan, 2, 0],
3550         ...     ]
3551         ... )
3552         >>> da = xr.DataArray(
3553         ...     data=temperature,
3554         ...     dims=["Y", "X"],
3555         ...     coords=dict(
3556         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3557         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3558         ...     ),
3559         ... )
3560         >>> da
3561         <xarray.DataArray (Y: 5, X: 3)>
3562         array([[ 0.,  1.,  3.],
3563                [ 0., nan,  5.],
3564                [ 5., nan, nan],
3565                [ 3., nan, nan],
3566                [nan,  2.,  0.]])
3567         Coordinates:
3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3569             lon      (X) float64 10.0 10.25 10.5
3570         Dimensions without coordinates: Y, X
3571 
3572         Fill all NaN values:
3573 
3574         >>> da.bfill(dim="Y", limit=None)
3575         <xarray.DataArray (Y: 5, X: 3)>
3576         array([[ 0.,  1.,  3.],
3577                [ 0.,  2.,  5.],
3578                [ 5.,  2.,  0.],
3579                [ 3.,  2.,  0.],
3580                [nan,  2.,  0.]])
3581         Coordinates:
3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3583             lon      (X) float64 10.0 10.25 10.5
3584         Dimensions without coordinates: Y, X
3585 
3586         Fill only the first of consecutive NaN values:
3587 
3588         >>> da.bfill(dim="Y", limit=1)
3589         <xarray.DataArray (Y: 5, X: 3)>
3590         array([[ 0.,  1.,  3.],
3591                [ 0., nan,  5.],
3592                [ 5., nan, nan],
3593                [ 3.,  2.,  0.],
3594                [nan,  2.,  0.]])
3595         Coordinates:
3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3597             lon      (X) float64 10.0 10.25 10.5
3598         Dimensions without coordinates: Y, X
3599         """
3600         from xarray.core.missing import bfill
3601 
3602         return bfill(self, dim, limit=limit)
3603 
3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3605         """Combine two DataArray objects, with union of coordinates.
3606 
3607         This operation follows the normal broadcasting and alignment rules of
3608         ``join='outer'``.  Default to non-null values of array calling the
3609         method.  Use np.nan to fill in vacant cells after alignment.
3610 
3611         Parameters
3612         ----------
3613         other : DataArray
3614             Used to fill all matching missing values in this array.
3615 
3616         Returns
3617         -------
3618         DataArray
3619         """
3620         return ops.fillna(self, other, join="outer")
3621 
3622     def reduce(
3623         self: T_DataArray,
3624         func: Callable[..., Any],
3625         dim: Dims = None,
3626         *,
3627         axis: int | Sequence[int] | None = None,
3628         keep_attrs: bool | None = None,
3629         keepdims: bool = False,
3630         **kwargs: Any,
3631     ) -> T_DataArray:
3632         """Reduce this array by applying `func` along some dimension(s).
3633 
3634         Parameters
3635         ----------
3636         func : callable
3637             Function which can be called in the form
3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3639             np.ndarray over an integer valued axis.
3640         dim : "...", str, Iterable of Hashable or None, optional
3641             Dimension(s) over which to apply `func`. By default `func` is
3642             applied over all dimensions.
3643         axis : int or sequence of int, optional
3644             Axis(es) over which to repeatedly apply `func`. Only one of the
3645             'dim' and 'axis' arguments can be supplied. If neither are
3646             supplied, then the reduction is calculated over the flattened array
3647             (by calling `f(x)` without an axis argument).
3648         keep_attrs : bool or None, optional
3649             If True, the variable's attributes (`attrs`) will be copied from
3650             the original object to the new one.  If False (default), the new
3651             object will be returned without attributes.
3652         keepdims : bool, default: False
3653             If True, the dimensions which are reduced are left in the result
3654             as dimensions of size one. Coordinates that use these dimensions
3655             are removed.
3656         **kwargs : dict
3657             Additional keyword arguments passed on to `func`.
3658 
3659         Returns
3660         -------
3661         reduced : DataArray
3662             DataArray with this object's array replaced with an array with
3663             summarized data and the indicated dimension(s) removed.
3664         """
3665 
3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3667         return self._replace_maybe_drop_dims(var)
3668 
3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3670         """Convert this array into a pandas object with the same shape.
3671 
3672         The type of the returned object depends on the number of DataArray
3673         dimensions:
3674 
3675         * 0D -> `xarray.DataArray`
3676         * 1D -> `pandas.Series`
3677         * 2D -> `pandas.DataFrame`
3678 
3679         Only works for arrays with 2 or fewer dimensions.
3680 
3681         The DataArray constructor performs the inverse transformation.
3682 
3683         Returns
3684         -------
3685         result : DataArray | Series | DataFrame
3686             DataArray, pandas Series or pandas DataFrame.
3687         """
3688         # TODO: consolidate the info about pandas constructors and the
3689         # attributes that correspond to their indexes into a separate module?
3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3691         try:
3692             constructor = constructors[self.ndim]
3693         except KeyError:
3694             raise ValueError(
3695                 f"Cannot convert arrays with {self.ndim} dimensions into "
3696                 "pandas objects. Requires 2 or fewer dimensions."
3697             )
3698         indexes = [self.get_index(dim) for dim in self.dims]
3699         return constructor(self.values, *indexes)
3700 
3701     def to_dataframe(
3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3703     ) -> pd.DataFrame:
3704         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3705 
3706         The DataFrame is indexed by the Cartesian product of index coordinates
3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3708         included as columns in the DataFrame.
3709 
3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3711         doesn't rely on a MultiIndex to build the DataFrame.
3712 
3713         Parameters
3714         ----------
3715         name: Hashable or None, optional
3716             Name to give to this array (required if unnamed).
3717         dim_order: Sequence of Hashable or None, optional
3718             Hierarchical dimension order for the resulting dataframe.
3719             Array content is transposed to this order and then written out as flat
3720             vectors in contiguous order, so the last dimension in this list
3721             will be contiguous in the resulting DataFrame. This has a major
3722             influence on which operations are efficient on the resulting
3723             dataframe.
3724 
3725             If provided, must include all dimensions of this DataArray. By default,
3726             dimensions are sorted according to the DataArray dimensions order.
3727 
3728         Returns
3729         -------
3730         result: DataFrame
3731             DataArray as a pandas DataFrame.
3732 
3733         See also
3734         --------
3735         DataArray.to_pandas
3736         DataArray.to_series
3737         """
3738         if name is None:
3739             name = self.name
3740         if name is None:
3741             raise ValueError(
3742                 "cannot convert an unnamed DataArray to a "
3743                 "DataFrame: use the ``name`` parameter"
3744             )
3745         if self.ndim == 0:
3746             raise ValueError("cannot convert a scalar to a DataFrame")
3747 
3748         # By using a unique name, we can convert a DataArray into a DataFrame
3749         # even if it shares a name with one of its coordinates.
3750         # I would normally use unique_name = object() but that results in a
3751         # dataframe with columns in the wrong order, for reasons I have not
3752         # been able to debug (possibly a pandas bug?).
3753         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3754         ds = self._to_dataset_whole(name=unique_name)
3755 
3756         if dim_order is None:
3757             ordered_dims = dict(zip(self.dims, self.shape))
3758         else:
3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3760 
3761         df = ds._to_dataframe(ordered_dims)
3762         df.columns = [name if c == unique_name else c for c in df.columns]
3763         return df
3764 
3765     def to_series(self) -> pd.Series:
3766         """Convert this array into a pandas.Series.
3767 
3768         The Series is indexed by the Cartesian product of index coordinates
3769         (in the form of a :py:class:`pandas.MultiIndex`).
3770 
3771         Returns
3772         -------
3773         result : Series
3774             DataArray as a pandas Series.
3775 
3776         See also
3777         --------
3778         DataArray.to_pandas
3779         DataArray.to_dataframe
3780         """
3781         index = self.coords.to_index()
3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3783 
3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3785         """Convert this array into a numpy.ma.MaskedArray
3786 
3787         Parameters
3788         ----------
3789         copy : bool, default: True
3790             If True make a copy of the array in the result. If False,
3791             a MaskedArray view of DataArray.values is returned.
3792 
3793         Returns
3794         -------
3795         result : MaskedArray
3796             Masked where invalid values (nan or inf) occur.
3797         """
3798         values = self.to_numpy()  # only compute lazy arrays once
3799         isnull = pd.isnull(values)
3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3801 
3802     # path=None writes to bytes
3803     @overload
3804     def to_netcdf(
3805         self,
3806         path: None = None,
3807         mode: Literal["w", "a"] = "w",
3808         format: T_NetcdfTypes | None = None,
3809         group: str | None = None,
3810         engine: T_NetcdfEngine | None = None,
3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3812         unlimited_dims: Iterable[Hashable] | None = None,
3813         compute: bool = True,
3814         invalid_netcdf: bool = False,
3815     ) -> bytes:
3816         ...
3817 
3818     # default return None
3819     @overload
3820     def to_netcdf(
3821         self,
3822         path: str | PathLike,
3823         mode: Literal["w", "a"] = "w",
3824         format: T_NetcdfTypes | None = None,
3825         group: str | None = None,
3826         engine: T_NetcdfEngine | None = None,
3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3828         unlimited_dims: Iterable[Hashable] | None = None,
3829         compute: Literal[True] = True,
3830         invalid_netcdf: bool = False,
3831     ) -> None:
3832         ...
3833 
3834     # compute=False returns dask.Delayed
3835     @overload
3836     def to_netcdf(
3837         self,
3838         path: str | PathLike,
3839         mode: Literal["w", "a"] = "w",
3840         format: T_NetcdfTypes | None = None,
3841         group: str | None = None,
3842         engine: T_NetcdfEngine | None = None,
3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3844         unlimited_dims: Iterable[Hashable] | None = None,
3845         *,
3846         compute: Literal[False],
3847         invalid_netcdf: bool = False,
3848     ) -> Delayed:
3849         ...
3850 
3851     def to_netcdf(
3852         self,
3853         path: str | PathLike | None = None,
3854         mode: Literal["w", "a"] = "w",
3855         format: T_NetcdfTypes | None = None,
3856         group: str | None = None,
3857         engine: T_NetcdfEngine | None = None,
3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3859         unlimited_dims: Iterable[Hashable] | None = None,
3860         compute: bool = True,
3861         invalid_netcdf: bool = False,
3862     ) -> bytes | Delayed | None:
3863         """Write DataArray contents to a netCDF file.
3864 
3865         Parameters
3866         ----------
3867         path : str, path-like or None, optional
3868             Path to which to save this dataset. File-like objects are only
3869             supported by the scipy engine. If no path is provided, this
3870             function returns the resulting netCDF file as bytes; in this case,
3871             we need to use scipy, which does not support netCDF version 4 (the
3872             default format becomes NETCDF3_64BIT).
3873         mode : {"w", "a"}, default: "w"
3874             Write ('w') or append ('a') mode. If mode='w', any existing file at
3875             this location will be overwritten. If mode='a', existing variables
3876             will be overwritten.
3877         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3878                   "NETCDF3_CLASSIC"}, optional
3879             File format for the resulting netCDF file:
3880 
3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3882               features.
3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3884               netCDF 3 compatible API features.
3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3886               which fully supports 2+ GB files, but is only compatible with
3887               clients linked against netCDF version 3.6.0 or later.
3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3889               handle 2+ GB files very well.
3890 
3891             All formats are supported by the netCDF4-python library.
3892             scipy.io.netcdf only supports the last two formats.
3893 
3894             The default format is NETCDF4 if you are saving a file to disk and
3895             have the netCDF4-python library available. Otherwise, xarray falls
3896             back to using scipy to write netCDF files and defaults to the
3897             NETCDF3_64BIT format (scipy does not support netCDF4).
3898         group : str, optional
3899             Path to the netCDF4 group in the given file to open (only works for
3900             format='NETCDF4'). The group(s) will be created if necessary.
3901         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3902             Engine to use when writing netCDF files. If not provided, the
3903             default engine is chosen based on available dependencies, with a
3904             preference for 'netcdf4' if writing to a file on disk.
3905         encoding : dict, optional
3906             Nested dictionary with variable names as keys and dictionaries of
3907             variable specific encodings as values, e.g.,
3908             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3909             "zlib": True}, ...}``
3910 
3911             The `h5netcdf` engine supports both the NetCDF4-style compression
3912             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3913             ones ``{"compression": "gzip", "compression_opts": 9}``.
3914             This allows using any compression plugin installed in the HDF5
3915             library, e.g. LZF.
3916 
3917         unlimited_dims : iterable of Hashable, optional
3918             Dimension(s) that should be serialized as unlimited dimensions.
3919             By default, no dimensions are treated as unlimited dimensions.
3920             Note that unlimited_dims may also be set via
3921             ``dataset.encoding["unlimited_dims"]``.
3922         compute: bool, default: True
3923             If true compute immediately, otherwise return a
3924             ``dask.delayed.Delayed`` object that can be computed later.
3925         invalid_netcdf: bool, default: False
3926             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3927             hdf5 files which are invalid netcdf as described in
3928             https://github.com/h5netcdf/h5netcdf.
3929 
3930         Returns
3931         -------
3932         store: bytes or Delayed or None
3933             * ``bytes`` if path is None
3934             * ``dask.delayed.Delayed`` if compute is False
3935             * None otherwise
3936 
3937         Notes
3938         -----
3939         Only xarray.Dataset objects can be written to netCDF files, so
3940         the xarray.DataArray is converted to a xarray.Dataset object
3941         containing a single variable. If the DataArray has no name, or if the
3942         name is the same as a coordinate name, then it is given the name
3943         ``"__xarray_dataarray_variable__"``.
3944 
3945         See Also
3946         --------
3947         Dataset.to_netcdf
3948         """
3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3950 
3951         if self.name is None:
3952             # If no name is set then use a generic xarray name
3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3954         elif self.name in self.coords or self.name in self.dims:
3955             # The name is the same as one of the coords names, which netCDF
3956             # doesn't support, so rename it but keep track of the old name
3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3958             dataset.attrs[DATAARRAY_NAME] = self.name
3959         else:
3960             # No problems with the name - so we're fine!
3961             dataset = self.to_dataset()
3962 
3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3964             dataset,
3965             path,
3966             mode=mode,
3967             format=format,
3968             group=group,
3969             engine=engine,
3970             encoding=encoding,
3971             unlimited_dims=unlimited_dims,
3972             compute=compute,
3973             multifile=False,
3974             invalid_netcdf=invalid_netcdf,
3975         )
3976 
3977     # compute=True (default) returns ZarrStore
3978     @overload
3979     def to_zarr(
3980         self,
3981         store: MutableMapping | str | PathLike[str] | None = None,
3982         chunk_store: MutableMapping | str | PathLike | None = None,
3983         mode: Literal["w", "w-", "a", "r+", None] = None,
3984         synchronizer=None,
3985         group: str | None = None,
3986         encoding: Mapping | None = None,
3987         compute: Literal[True] = True,
3988         consolidated: bool | None = None,
3989         append_dim: Hashable | None = None,
3990         region: Mapping[str, slice] | None = None,
3991         safe_chunks: bool = True,
3992         storage_options: dict[str, str] | None = None,
3993         zarr_version: int | None = None,
3994     ) -> ZarrStore:
3995         ...
3996 
3997     # compute=False returns dask.Delayed
3998     @overload
3999     def to_zarr(
4000         self,
4001         store: MutableMapping | str | PathLike[str] | None = None,
4002         chunk_store: MutableMapping | str | PathLike | None = None,
4003         mode: Literal["w", "w-", "a", "r+", None] = None,
4004         synchronizer=None,
4005         group: str | None = None,
4006         encoding: Mapping | None = None,
4007         *,
4008         compute: Literal[False],
4009         consolidated: bool | None = None,
4010         append_dim: Hashable | None = None,
4011         region: Mapping[str, slice] | None = None,
4012         safe_chunks: bool = True,
4013         storage_options: dict[str, str] | None = None,
4014         zarr_version: int | None = None,
4015     ) -> Delayed:
4016         ...
4017 
4018     def to_zarr(
4019         self,
4020         store: MutableMapping | str | PathLike[str] | None = None,
4021         chunk_store: MutableMapping | str | PathLike | None = None,
4022         mode: Literal["w", "w-", "a", "r+", None] = None,
4023         synchronizer=None,
4024         group: str | None = None,
4025         encoding: Mapping | None = None,
4026         compute: bool = True,
4027         consolidated: bool | None = None,
4028         append_dim: Hashable | None = None,
4029         region: Mapping[str, slice] | None = None,
4030         safe_chunks: bool = True,
4031         storage_options: dict[str, str] | None = None,
4032         zarr_version: int | None = None,
4033     ) -> ZarrStore | Delayed:
4034         """Write DataArray contents to a Zarr store
4035 
4036         Zarr chunks are determined in the following way:
4037 
4038         - From the ``chunks`` attribute in each variable's ``encoding``
4039           (can be set via `DataArray.chunk`).
4040         - If the variable is a Dask array, from the dask chunks
4041         - If neither Dask chunks nor encoding chunks are present, chunks will
4042           be determined automatically by Zarr
4043         - If both Dask chunks and encoding chunks are present, encoding chunks
4044           will be used, provided that there is a many-to-one relationship between
4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.
4047           This restriction ensures that no synchronization / locks are required
4048           when writing. To disable this restriction, use ``safe_chunks=False``.
4049 
4050         Parameters
4051         ----------
4052         store : MutableMapping, str or path-like, optional
4053             Store or path to directory in local or remote file system.
4054         chunk_store : MutableMapping, str or path-like, optional
4055             Store or path to directory in local or remote file system only for Zarr
4056             array chunks. Requires zarr-python v2.4.0 or later.
4057         mode : {"w", "w-", "a", "r+", None}, optional
4058             Persistence mode: "w" means create (overwrite if exists);
4059             "w-" means create (fail if exists);
4060             "a" means override existing variables (create if does not exist);
4061             "r+" means modify existing array *values* only (raise an error if
4062             any metadata or shapes would change).
4063             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
4064             "r+" if ``region`` is set and ``w-`` otherwise.
4065         synchronizer : object, optional
4066             Zarr array synchronizer.
4067         group : str, optional
4068             Group path. (a.k.a. `path` in zarr terminology.)
4069         encoding : dict, optional
4070             Nested dictionary with variable names as keys and dictionaries of
4071             variable specific encodings as values, e.g.,
4072             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
4073         compute : bool, default: True
4074             If True write array data immediately, otherwise return a
4075             ``dask.delayed.Delayed`` object that can be computed to write
4076             array data later. Metadata is always updated eagerly.
4077         consolidated : bool, optional
4078             If True, apply zarr's `consolidate_metadata` function to the store
4079             after writing metadata and read existing stores with consolidated
4080             metadata; if False, do not. The default (`consolidated=None`) means
4081             write consolidated metadata and attempt to read consolidated
4082             metadata for existing stores (falling back to non-consolidated).
4083 
4084             When the experimental ``zarr_version=3``, ``consolidated`` must be
4085             either be ``None`` or ``False``.
4086         append_dim : hashable, optional
4087             If set, the dimension along which the data will be appended. All
4088             other dimensions on overridden variables must remain the same size.
4089         region : dict, optional
4090             Optional mapping from dimension names to integer slices along
4091             dataarray dimensions to indicate the region of existing zarr array(s)
4092             in which to write this datarray's data. For example,
4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
4094             that values should be written to the region ``0:1000`` along ``x``
4095             and ``10000:11000`` along ``y``.
4096 
4097             Two restrictions apply to the use of ``region``:
4098 
4099             - If ``region`` is set, _all_ variables in a dataarray must have at
4100               least one dimension in common with the region. Other variables
4101               should be written in a separate call to ``to_zarr()``.
4102             - Dimensions cannot be included in both ``region`` and
4103               ``append_dim`` at the same time. To create empty arrays to fill
4104               in with ``region``, use a separate call to ``to_zarr()`` with
4105               ``compute=False``. See "Appending to existing Zarr stores" in
4106               the reference documentation for full details.
4107         safe_chunks : bool, default: True
4108             If True, only allow writes to when there is a many-to-one relationship
4109             between Zarr chunks (specified in encoding) and Dask chunks.
4110             Set False to override this restriction; however, data may become corrupted
4111             if Zarr arrays are written in parallel. This option may be useful in combination
4112             with ``compute=False`` to initialize a Zarr store from an existing
4113             DataArray with arbitrary chunk structure.
4114         storage_options : dict, optional
4115             Any additional parameters for the storage backend (ignored for local
4116             paths).
4117         zarr_version : int or None, optional
4118             The desired zarr spec version to target (currently 2 or 3). The
4119             default of None will attempt to determine the zarr version from
4120             ``store`` when possible, otherwise defaulting to 2.
4121 
4122         Returns
4123         -------
4124             * ``dask.delayed.Delayed`` if compute is False
4125             * ZarrStore otherwise
4126 
4127         References
4128         ----------
4129         https://zarr.readthedocs.io/
4130 
4131         Notes
4132         -----
4133         Zarr chunking behavior:
4134             If chunks are found in the encoding argument or attribute
4135             corresponding to any DataArray, those chunks are used.
4136             If a DataArray is a dask array, it is written with those chunks.
4137             If not other chunks are found, Zarr uses its own heuristics to
4138             choose automatic chunk sizes.
4139 
4140         encoding:
4141             The encoding attribute (if exists) of the DataArray(s) will be
4142             used. Override any existing encodings by providing the ``encoding`` kwarg.
4143 
4144         See Also
4145         --------
4146         Dataset.to_zarr
4147         :ref:`io.zarr`
4148             The I/O user guide, with more details and examples.
4149         """
4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr
4151 
4152         if self.name is None:
4153             # If no name is set then use a generic xarray name
4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4155         elif self.name in self.coords or self.name in self.dims:
4156             # The name is the same as one of the coords names, which the netCDF data model
4157             # does not support, so rename it but keep track of the old name
4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4159             dataset.attrs[DATAARRAY_NAME] = self.name
4160         else:
4161             # No problems with the name - so we're fine!
4162             dataset = self.to_dataset()
4163 
4164         return to_zarr(  # type: ignore[call-overload,misc]
4165             dataset,
4166             store=store,
4167             chunk_store=chunk_store,
4168             mode=mode,
4169             synchronizer=synchronizer,
4170             group=group,
4171             encoding=encoding,
4172             compute=compute,
4173             consolidated=consolidated,
4174             append_dim=append_dim,
4175             region=region,
4176             safe_chunks=safe_chunks,
4177             storage_options=storage_options,
4178             zarr_version=zarr_version,
4179         )
4180 
4181     def to_dict(
4182         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
4183     ) -> dict[str, Any]:
4184         """
4185         Convert this xarray.DataArray into a dictionary following xarray
4186         naming conventions.
4187 
4188         Converts all variables and attributes to native Python objects.
4189         Useful for converting to json. To avoid datetime incompatibility
4190         use decode_times=False kwarg in xarray.open_dataset.
4191 
4192         Parameters
4193         ----------
4194         data : bool or {"list", "array"}, default: "list"
4195             Whether to include the actual data in the dictionary. When set to
4196             False, returns just the schema. If set to "array", returns data as
4197             underlying array type. If set to "list" (or True for backwards
4198             compatibility), returns data in lists of Python data types. Note
4199             that for obtaining the "list" output efficiently, use
4200             `da.compute().to_dict(data="list")`.
4201 
4202         encoding : bool, default: False
4203             Whether to include the Dataset's encoding in the dictionary.
4204 
4205         Returns
4206         -------
4207         dict: dict
4208 
4209         See Also
4210         --------
4211         DataArray.from_dict
4212         Dataset.to_dict
4213         """
4214         d = self.variable.to_dict(data=data)
4215         d.update({"coords": {}, "name": self.name})
4216         for k, coord in self.coords.items():
4217             d["coords"][k] = coord.variable.to_dict(data=data)
4218         if encoding:
4219             d["encoding"] = dict(self.encoding)
4220         return d
4221 
4222     @classmethod
4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4224         """Convert a dictionary into an xarray.DataArray
4225 
4226         Parameters
4227         ----------
4228         d : dict
4229             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4230 
4231         Returns
4232         -------
4233         obj : xarray.DataArray
4234 
4235         See Also
4236         --------
4237         DataArray.to_dict
4238         Dataset.from_dict
4239 
4240         Examples
4241         --------
4242         >>> d = {"dims": "t", "data": [1, 2, 3]}
4243         >>> da = xr.DataArray.from_dict(d)
4244         >>> da
4245         <xarray.DataArray (t: 3)>
4246         array([1, 2, 3])
4247         Dimensions without coordinates: t
4248 
4249         >>> d = {
4250         ...     "coords": {
4251         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4252         ...     },
4253         ...     "attrs": {"title": "air temperature"},
4254         ...     "dims": "t",
4255         ...     "data": [10, 20, 30],
4256         ...     "name": "a",
4257         ... }
4258         >>> da = xr.DataArray.from_dict(d)
4259         >>> da
4260         <xarray.DataArray 'a' (t: 3)>
4261         array([10, 20, 30])
4262         Coordinates:
4263           * t        (t) int64 0 1 2
4264         Attributes:
4265             title:    air temperature
4266         """
4267         coords = None
4268         if "coords" in d:
4269             try:
4270                 coords = {
4271                     k: (v["dims"], v["data"], v.get("attrs"))
4272                     for k, v in d["coords"].items()
4273                 }
4274             except KeyError as e:
4275                 raise ValueError(
4276                     "cannot convert dict when coords are missing the key "
4277                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4278                 )
4279         try:
4280             data = d["data"]
4281         except KeyError:
4282             raise ValueError("cannot convert dict without the key 'data''")
4283         else:
4284             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4285 
4286         obj.encoding.update(d.get("encoding", {}))
4287 
4288         return obj
4289 
4290     @classmethod
4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4292         """Convert a pandas.Series into an xarray.DataArray.
4293 
4294         If the series's index is a MultiIndex, it will be expanded into a
4295         tensor product of one-dimensional coordinates (filling in missing
4296         values with NaN). Thus this operation should be the inverse of the
4297         `to_series` method.
4298 
4299         Parameters
4300         ----------
4301         series : Series
4302             Pandas Series object to convert.
4303         sparse : bool, default: False
4304             If sparse=True, creates a sparse array instead of a dense NumPy array.
4305             Requires the pydata/sparse package.
4306 
4307         See Also
4308         --------
4309         DataArray.to_series
4310         Dataset.from_dataframe
4311         """
4312         temp_name = "__temporary_name"
4313         df = pd.DataFrame({temp_name: series})
4314         ds = Dataset.from_dataframe(df, sparse=sparse)
4315         result = cast(DataArray, ds[temp_name])
4316         result.name = series.name
4317         return result
4318 
4319     def to_cdms2(self) -> cdms2_Variable:
4320         """Convert this array into a cdms2.Variable"""
4321         from xarray.convert import to_cdms2
4322 
4323         return to_cdms2(self)
4324 
4325     @classmethod
4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4327         """Convert a cdms2.Variable into an xarray.DataArray"""
4328         from xarray.convert import from_cdms2
4329 
4330         return from_cdms2(variable)
4331 
4332     def to_iris(self) -> iris_Cube:
4333         """Convert this array into a iris.cube.Cube"""
4334         from xarray.convert import to_iris
4335 
4336         return to_iris(self)
4337 
4338     @classmethod
4339     def from_iris(cls, cube: iris_Cube) -> DataArray:
4340         """Convert a iris.cube.Cube into an xarray.DataArray"""
4341         from xarray.convert import from_iris
4342 
4343         return from_iris(cube)
4344 
4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4346         """Helper function for equals, broadcast_equals, and identical"""
4347 
4348         def compat(x, y):
4349             return getattr(x.variable, compat_str)(y.variable)
4350 
4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4352             self, other
4353         )
4354 
4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4356         """Two DataArrays are broadcast equal if they are equal after
4357         broadcasting them against each other such that they have the same
4358         dimensions.
4359 
4360         Parameters
4361         ----------
4362         other : DataArray
4363             DataArray to compare to.
4364 
4365         Returns
4366         ----------
4367         equal : bool
4368             True if the two DataArrays are broadcast equal.
4369 
4370         See Also
4371         --------
4372         DataArray.equals
4373         DataArray.identical
4374 
4375         Examples
4376         --------
4377         >>> a = xr.DataArray([1, 2], dims="X")
4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4379         >>> a
4380         <xarray.DataArray (X: 2)>
4381         array([1, 2])
4382         Dimensions without coordinates: X
4383         >>> b
4384         <xarray.DataArray (X: 2, Y: 2)>
4385         array([[1, 1],
4386                [2, 2]])
4387         Dimensions without coordinates: X, Y
4388 
4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4390 
4391         >>> a.equals(b)
4392         False
4393         >>> a2, b2 = xr.broadcast(a, b)
4394         >>> a2.equals(b2)
4395         True
4396         >>> a.broadcast_equals(b)
4397         True
4398         """
4399         try:
4400             return self._all_compat(other, "broadcast_equals")
4401         except (TypeError, AttributeError):
4402             return False
4403 
4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4405         """True if two DataArrays have the same dimensions, coordinates and
4406         values; otherwise False.
4407 
4408         DataArrays can still be equal (like pandas objects) if they have NaN
4409         values in the same locations.
4410 
4411         This method is necessary because `v1 == v2` for ``DataArray``
4412         does element-wise comparisons (like numpy.ndarrays).
4413 
4414         Parameters
4415         ----------
4416         other : DataArray
4417             DataArray to compare to.
4418 
4419         Returns
4420         ----------
4421         equal : bool
4422             True if the two DataArrays are equal.
4423 
4424         See Also
4425         --------
4426         DataArray.broadcast_equals
4427         DataArray.identical
4428 
4429         Examples
4430         --------
4431         >>> a = xr.DataArray([1, 2, 3], dims="X")
4432         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4433         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4434         >>> d = xr.DataArray([3, 2, 1], dims="X")
4435         >>> a
4436         <xarray.DataArray (X: 3)>
4437         array([1, 2, 3])
4438         Dimensions without coordinates: X
4439         >>> b
4440         <xarray.DataArray (X: 3)>
4441         array([1, 2, 3])
4442         Dimensions without coordinates: X
4443         Attributes:
4444             units:    m
4445         >>> c
4446         <xarray.DataArray (Y: 3)>
4447         array([1, 2, 3])
4448         Dimensions without coordinates: Y
4449         >>> d
4450         <xarray.DataArray (X: 3)>
4451         array([3, 2, 1])
4452         Dimensions without coordinates: X
4453 
4454         >>> a.equals(b)
4455         True
4456         >>> a.equals(c)
4457         False
4458         >>> a.equals(d)
4459         False
4460         """
4461         try:
4462             return self._all_compat(other, "equals")
4463         except (TypeError, AttributeError):
4464             return False
4465 
4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4467         """Like equals, but also checks the array name and attributes, and
4468         attributes on all coordinates.
4469 
4470         Parameters
4471         ----------
4472         other : DataArray
4473             DataArray to compare to.
4474 
4475         Returns
4476         ----------
4477         equal : bool
4478             True if the two DataArrays are identical.
4479 
4480         See Also
4481         --------
4482         DataArray.broadcast_equals
4483         DataArray.equals
4484 
4485         Examples
4486         --------
4487         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4488         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4489         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4490         >>> a
4491         <xarray.DataArray 'Width' (X: 3)>
4492         array([1, 2, 3])
4493         Dimensions without coordinates: X
4494         Attributes:
4495             units:    m
4496         >>> b
4497         <xarray.DataArray 'Width' (X: 3)>
4498         array([1, 2, 3])
4499         Dimensions without coordinates: X
4500         Attributes:
4501             units:    m
4502         >>> c
4503         <xarray.DataArray 'Width' (X: 3)>
4504         array([1, 2, 3])
4505         Dimensions without coordinates: X
4506         Attributes:
4507             units:    ft
4508 
4509         >>> a.equals(b)
4510         True
4511         >>> a.identical(b)
4512         True
4513 
4514         >>> a.equals(c)
4515         True
4516         >>> a.identical(c)
4517         False
4518         """
4519         try:
4520             return self.name == other.name and self._all_compat(other, "identical")
4521         except (TypeError, AttributeError):
4522             return False
4523 
4524     def _result_name(self, other: Any = None) -> Hashable | None:
4525         # use the same naming heuristics as pandas:
4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4527         other_name = getattr(other, "name", _default)
4528         if other_name is _default or other_name == self.name:
4529             return self.name
4530         else:
4531             return None
4532 
4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4534         new_var = self.variable.__array_wrap__(obj, context)
4535         return self._replace(new_var)
4536 
4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4538         return self.dot(obj)
4539 
4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4541         # currently somewhat duplicative, as only other DataArrays are
4542         # compatible with matmul
4543         return computation.dot(other, self)
4544 
4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4546         keep_attrs = kwargs.pop("keep_attrs", None)
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=True)
4549         with warnings.catch_warnings():
4550             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4551             warnings.filterwarnings(
4552                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4553             )
4554             with np.errstate(all="ignore"):
4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4556             if keep_attrs:
4557                 da.attrs = self.attrs
4558             return da
4559 
4560     def _binary_op(
4561         self: T_DataArray,
4562         other: Any,
4563         f: Callable,
4564         reflexive: bool = False,
4565     ) -> T_DataArray:
4566         from xarray.core.groupby import GroupBy
4567 
4568         if isinstance(other, (Dataset, GroupBy)):
4569             return NotImplemented
4570         if isinstance(other, DataArray):
4571             align_type = OPTIONS["arithmetic_join"]
4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4573         other_variable = getattr(other, "variable", other)
4574         other_coords = getattr(other, "coords", None)
4575 
4576         variable = (
4577             f(self.variable, other_variable)
4578             if not reflexive
4579             else f(other_variable, self.variable)
4580         )
4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4582         name = self._result_name(other)
4583 
4584         return self._replace(variable, coords, name, indexes=indexes)
4585 
4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4587         from xarray.core.groupby import GroupBy
4588 
4589         if isinstance(other, GroupBy):
4590             raise TypeError(
4591                 "in-place operations between a DataArray and "
4592                 "a grouped object are not permitted"
4593             )
4594         # n.b. we can't align other to self (with other.reindex_like(self))
4595         # because `other` may be converted into floats, which would cause
4596         # in-place arithmetic to fail unpredictably. Instead, we simply
4597         # don't support automatic alignment with in-place arithmetic.
4598         other_coords = getattr(other, "coords", None)
4599         other_variable = getattr(other, "variable", other)
4600         try:
4601             with self.coords._merge_inplace(other_coords):
4602                 f(self.variable, other_variable)
4603         except MergeError as exc:
4604             raise MergeError(
4605                 "Automatic alignment is not supported for in-place operations.\n"
4606                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4607                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4608             ) from exc
4609         return self
4610 
4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4612         self.attrs = other.attrs
4613 
4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4615 
4616     def _title_for_slice(self, truncate: int = 50) -> str:
4617         """
4618         If the dataarray has 1 dimensional coordinates or comes from a slice
4619         we can show that info in the title
4620 
4621         Parameters
4622         ----------
4623         truncate : int, default: 50
4624             maximum number of characters for title
4625 
4626         Returns
4627         -------
4628         title : string
4629             Can be used for plot titles
4630 
4631         """
4632         one_dims = []
4633         for dim, coord in self.coords.items():
4634             if coord.size == 1:
4635                 one_dims.append(
4636                     "{dim} = {v}{unit}".format(
4637                         dim=dim,
4638                         v=format_item(coord.values),
4639                         unit=_get_units_from_attrs(coord),
4640                     )
4641                 )
4642 
4643         title = ", ".join(one_dims)
4644         if len(title) > truncate:
4645             title = title[: (truncate - 3)] + "..."
4646 
4647         return title
4648 
4649     def diff(
4650         self: T_DataArray,
4651         dim: Hashable,
4652         n: int = 1,
4653         label: Literal["upper", "lower"] = "upper",
4654     ) -> T_DataArray:
4655         """Calculate the n-th order discrete difference along given axis.
4656 
4657         Parameters
4658         ----------
4659         dim : Hashable
4660             Dimension over which to calculate the finite difference.
4661         n : int, default: 1
4662             The number of times values are differenced.
4663         label : {"upper", "lower"}, default: "upper"
4664             The new coordinate in dimension ``dim`` will have the
4665             values of either the minuend's or subtrahend's coordinate
4666             for values 'upper' and 'lower', respectively.
4667 
4668         Returns
4669         -------
4670         difference : DataArray
4671             The n-th order finite difference of this object.
4672 
4673         Notes
4674         -----
4675         `n` matches numpy's behavior and is different from pandas' first argument named
4676         `periods`.
4677 
4678         Examples
4679         --------
4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4681         >>> arr.diff("x")
4682         <xarray.DataArray (x: 3)>
4683         array([0, 1, 0])
4684         Coordinates:
4685           * x        (x) int64 2 3 4
4686         >>> arr.diff("x", 2)
4687         <xarray.DataArray (x: 2)>
4688         array([ 1, -1])
4689         Coordinates:
4690           * x        (x) int64 3 4
4691 
4692         See Also
4693         --------
4694         DataArray.differentiate
4695         """
4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4697         return self._from_temp_dataset(ds)
4698 
4699     def shift(
4700         self: T_DataArray,
4701         shifts: Mapping[Any, int] | None = None,
4702         fill_value: Any = dtypes.NA,
4703         **shifts_kwargs: int,
4704     ) -> T_DataArray:
4705         """Shift this DataArray by an offset along one or more dimensions.
4706 
4707         Only the data is moved; coordinates stay in place. This is consistent
4708         with the behavior of ``shift`` in pandas.
4709 
4710         Values shifted from beyond array bounds will appear at one end of
4711         each dimension, which are filled according to `fill_value`. For periodic
4712         offsets instead see `roll`.
4713 
4714         Parameters
4715         ----------
4716         shifts : mapping of Hashable to int or None, optional
4717             Integer offset to shift along each of the given dimensions.
4718             Positive offsets shift to the right; negative offsets shift to the
4719             left.
4720         fill_value : scalar, optional
4721             Value to use for newly missing values
4722         **shifts_kwargs
4723             The keyword arguments form of ``shifts``.
4724             One of shifts or shifts_kwargs must be provided.
4725 
4726         Returns
4727         -------
4728         shifted : DataArray
4729             DataArray with the same coordinates and attributes but shifted
4730             data.
4731 
4732         See Also
4733         --------
4734         roll
4735 
4736         Examples
4737         --------
4738         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4739         >>> arr.shift(x=1)
4740         <xarray.DataArray (x: 3)>
4741         array([nan,  5.,  6.])
4742         Dimensions without coordinates: x
4743         """
4744         variable = self.variable.shift(
4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4746         )
4747         return self._replace(variable=variable)
4748 
4749     def roll(
4750         self: T_DataArray,
4751         shifts: Mapping[Hashable, int] | None = None,
4752         roll_coords: bool = False,
4753         **shifts_kwargs: int,
4754     ) -> T_DataArray:
4755         """Roll this array by an offset along one or more dimensions.
4756 
4757         Unlike shift, roll treats the given dimensions as periodic, so will not
4758         create any missing values to be filled.
4759 
4760         Unlike shift, roll may rotate all variables, including coordinates
4761         if specified. The direction of rotation is consistent with
4762         :py:func:`numpy.roll`.
4763 
4764         Parameters
4765         ----------
4766         shifts : mapping of Hashable to int, optional
4767             Integer offset to rotate each of the given dimensions.
4768             Positive offsets roll to the right; negative offsets roll to the
4769             left.
4770         roll_coords : bool, default: False
4771             Indicates whether to roll the coordinates by the offset too.
4772         **shifts_kwargs : {dim: offset, ...}, optional
4773             The keyword arguments form of ``shifts``.
4774             One of shifts or shifts_kwargs must be provided.
4775 
4776         Returns
4777         -------
4778         rolled : DataArray
4779             DataArray with the same attributes but rolled data and coordinates.
4780 
4781         See Also
4782         --------
4783         shift
4784 
4785         Examples
4786         --------
4787         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4788         >>> arr.roll(x=1)
4789         <xarray.DataArray (x: 3)>
4790         array([7, 5, 6])
4791         Dimensions without coordinates: x
4792         """
4793         ds = self._to_temp_dataset().roll(
4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4795         )
4796         return self._from_temp_dataset(ds)
4797 
4798     @property
4799     def real(self: T_DataArray) -> T_DataArray:
4800         """
4801         The real part of the array.
4802 
4803         See Also
4804         --------
4805         numpy.ndarray.real
4806         """
4807         return self._replace(self.variable.real)
4808 
4809     @property
4810     def imag(self: T_DataArray) -> T_DataArray:
4811         """
4812         The imaginary part of the array.
4813 
4814         See Also
4815         --------
4816         numpy.ndarray.imag
4817         """
4818         return self._replace(self.variable.imag)
4819 
4820     def dot(
4821         self: T_DataArray,
4822         other: T_DataArray,
4823         dims: Dims = None,
4824     ) -> T_DataArray:
4825         """Perform dot product of two DataArrays along their shared dims.
4826 
4827         Equivalent to taking taking tensordot over all shared dims.
4828 
4829         Parameters
4830         ----------
4831         other : DataArray
4832             The other array with which the dot product is performed.
4833         dims : ..., str, Iterable of Hashable or None, optional
4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4835             If not specified, then all the common dimensions are summed over.
4836 
4837         Returns
4838         -------
4839         result : DataArray
4840             Array resulting from the dot product over all shared dimensions.
4841 
4842         See Also
4843         --------
4844         dot
4845         numpy.tensordot
4846 
4847         Examples
4848         --------
4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4850         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4851         >>> dm_vals = np.arange(4)
4852         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4853 
4854         >>> dm.dims
4855         ('z',)
4856 
4857         >>> da.dims
4858         ('x', 'y', 'z')
4859 
4860         >>> dot_result = da.dot(dm)
4861         >>> dot_result.dims
4862         ('x', 'y')
4863 
4864         """
4865         if isinstance(other, Dataset):
4866             raise NotImplementedError(
4867                 "dot products are not yet supported with Dataset objects."
4868             )
4869         if not isinstance(other, DataArray):
4870             raise TypeError("dot only operates on DataArrays.")
4871 
4872         return computation.dot(self, other, dims=dims)
4873 
4874     # change type of self and return to T_DataArray once
4875     # https://github.com/python/mypy/issues/12846 is resolved
4876     def sortby(
4877         self,
4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4879         ascending: bool = True,
4880     ) -> DataArray:
4881         """Sort object by labels or values (along an axis).
4882 
4883         Sorts the dataarray, either along specified dimensions,
4884         or according to values of 1-D dataarrays that share dimension
4885         with calling object.
4886 
4887         If the input variables are dataarrays, then the dataarrays are aligned
4888         (via left-join) to the calling object prior to sorting by cell values.
4889         NaNs are sorted to the end, following Numpy convention.
4890 
4891         If multiple sorts along the same dimension is
4892         given, numpy's lexsort is performed along that dimension:
4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4894         and the FIRST key in the sequence is used as the primary sort key,
4895         followed by the 2nd key, etc.
4896 
4897         Parameters
4898         ----------
4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4900             1D DataArray objects or name(s) of 1D variable(s) in
4901             coords whose values are used to sort this array.
4902         ascending : bool, default: True
4903             Whether to sort by ascending or descending order.
4904 
4905         Returns
4906         -------
4907         sorted : DataArray
4908             A new dataarray where all the specified dims are sorted by dim
4909             labels.
4910 
4911         See Also
4912         --------
4913         Dataset.sortby
4914         numpy.sort
4915         pandas.sort_values
4916         pandas.sort_index
4917 
4918         Examples
4919         --------
4920         >>> da = xr.DataArray(
4921         ...     np.random.rand(5),
4922         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4923         ...     dims="time",
4924         ... )
4925         >>> da
4926         <xarray.DataArray (time: 5)>
4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4928         Coordinates:
4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4930 
4931         >>> da.sortby(da)
4932         <xarray.DataArray (time: 5)>
4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4934         Coordinates:
4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4936         """
4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4938         return self._from_temp_dataset(ds)
4939 
4940     def quantile(
4941         self: T_DataArray,
4942         q: ArrayLike,
4943         dim: Dims = None,
4944         method: QuantileMethods = "linear",
4945         keep_attrs: bool | None = None,
4946         skipna: bool | None = None,
4947         interpolation: QuantileMethods | None = None,
4948     ) -> T_DataArray:
4949         """Compute the qth quantile of the data along the specified dimension.
4950 
4951         Returns the qth quantiles(s) of the array elements.
4952 
4953         Parameters
4954         ----------
4955         q : float or array-like of float
4956             Quantile to compute, which must be between 0 and 1 inclusive.
4957         dim : str or Iterable of Hashable, optional
4958             Dimension(s) over which to apply quantile.
4959         method : str, default: "linear"
4960             This optional parameter specifies the interpolation method to use when the
4961             desired quantile lies between two data points. The options sorted by their R
4962             type as summarized in the H&F paper [1]_ are:
4963 
4964                 1. "inverted_cdf" (*)
4965                 2. "averaged_inverted_cdf" (*)
4966                 3. "closest_observation" (*)
4967                 4. "interpolated_inverted_cdf" (*)
4968                 5. "hazen" (*)
4969                 6. "weibull" (*)
4970                 7. "linear"  (default)
4971                 8. "median_unbiased" (*)
4972                 9. "normal_unbiased" (*)
4973 
4974             The first three methods are discontiuous. The following discontinuous
4975             variations of the default "linear" (7.) option are also available:
4976 
4977                 * "lower"
4978                 * "higher"
4979                 * "midpoint"
4980                 * "nearest"
4981 
4982             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4983             was previously called "interpolation", renamed in accordance with numpy
4984             version 1.22.0.
4985 
4986             (*) These methods require numpy version 1.22 or newer.
4987 
4988         keep_attrs : bool or None, optional
4989             If True, the dataset's attributes (`attrs`) will be copied from
4990             the original object to the new one.  If False (default), the new
4991             object will be returned without attributes.
4992         skipna : bool or None, optional
4993             If True, skip missing values (as marked by NaN). By default, only
4994             skips missing values for float dtypes; other dtypes either do not
4995             have a sentinel missing value (int) or skipna=True has not been
4996             implemented (object, datetime64 or timedelta64).
4997 
4998         Returns
4999         -------
5000         quantiles : DataArray
5001             If `q` is a single quantile, then the result
5002             is a scalar. If multiple percentiles are given, first axis of
5003             the result corresponds to the quantile and a quantile dimension
5004             is added to the return array. The other dimensions are the
5005             dimensions that remain after the reduction of the array.
5006 
5007         See Also
5008         --------
5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
5010 
5011         Examples
5012         --------
5013         >>> da = xr.DataArray(
5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
5015         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5016         ...     dims=("x", "y"),
5017         ... )
5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)
5019         <xarray.DataArray ()>
5020         array(0.7)
5021         Coordinates:
5022             quantile  float64 0.0
5023         >>> da.quantile(0, dim="x")
5024         <xarray.DataArray (y: 4)>
5025         array([0.7, 4.2, 2.6, 1.5])
5026         Coordinates:
5027           * y         (y) float64 1.0 1.5 2.0 2.5
5028             quantile  float64 0.0
5029         >>> da.quantile([0, 0.5, 1])
5030         <xarray.DataArray (quantile: 3)>
5031         array([0.7, 3.4, 9.4])
5032         Coordinates:
5033           * quantile  (quantile) float64 0.0 0.5 1.0
5034         >>> da.quantile([0, 0.5, 1], dim="x")
5035         <xarray.DataArray (quantile: 3, y: 4)>
5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],
5037                [3.6 , 5.75, 6.  , 1.7 ],
5038                [6.5 , 7.3 , 9.4 , 1.9 ]])
5039         Coordinates:
5040           * y         (y) float64 1.0 1.5 2.0 2.5
5041           * quantile  (quantile) float64 0.0 0.5 1.0
5042 
5043         References
5044         ----------
5045         .. [1] R. J. Hyndman and Y. Fan,
5046            "Sample quantiles in statistical packages,"
5047            The American Statistician, 50(4), pp. 361-365, 1996
5048         """
5049 
5050         ds = self._to_temp_dataset().quantile(
5051             q,
5052             dim=dim,
5053             keep_attrs=keep_attrs,
5054             method=method,
5055             skipna=skipna,
5056             interpolation=interpolation,
5057         )
5058         return self._from_temp_dataset(ds)
5059 
5060     def rank(
5061         self: T_DataArray,
5062         dim: Hashable,
5063         pct: bool = False,
5064         keep_attrs: bool | None = None,
5065     ) -> T_DataArray:
5066         """Ranks the data.
5067 
5068         Equal values are assigned a rank that is the average of the ranks that
5069         would have been otherwise assigned to all of the values within that
5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
5071 
5072         NaNs in the input array are returned as NaNs.
5073 
5074         The `bottleneck` library is required.
5075 
5076         Parameters
5077         ----------
5078         dim : Hashable
5079             Dimension over which to compute rank.
5080         pct : bool, default: False
5081             If True, compute percentage ranks, otherwise compute integer ranks.
5082         keep_attrs : bool or None, optional
5083             If True, the dataset's attributes (`attrs`) will be copied from
5084             the original object to the new one.  If False (default), the new
5085             object will be returned without attributes.
5086 
5087         Returns
5088         -------
5089         ranked : DataArray
5090             DataArray with the same coordinates and dtype 'float64'.
5091 
5092         Examples
5093         --------
5094         >>> arr = xr.DataArray([5, 6, 7], dims="x")
5095         >>> arr.rank("x")
5096         <xarray.DataArray (x: 3)>
5097         array([1., 2., 3.])
5098         Dimensions without coordinates: x
5099         """
5100 
5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
5102         return self._from_temp_dataset(ds)
5103 
5104     def differentiate(
5105         self: T_DataArray,
5106         coord: Hashable,
5107         edge_order: Literal[1, 2] = 1,
5108         datetime_unit: DatetimeUnitOptions = None,
5109     ) -> T_DataArray:
5110         """ Differentiate the array with the second order accurate central
5111         differences.
5112 
5113         .. note::
5114             This feature is limited to simple cartesian geometry, i.e. coord
5115             must be one dimensional.
5116 
5117         Parameters
5118         ----------
5119         coord : Hashable
5120             The coordinate to be used to compute the gradient.
5121         edge_order : {1, 2}, default: 1
5122             N-th order accurate differences at the boundaries.
5123         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5124                          "us", "ns", "ps", "fs", "as", None}, optional
5125             Unit to compute gradient. Only valid for datetime coordinate.
5126 
5127         Returns
5128         -------
5129         differentiated: DataArray
5130 
5131         See also
5132         --------
5133         numpy.gradient: corresponding numpy function
5134 
5135         Examples
5136         --------
5137 
5138         >>> da = xr.DataArray(
5139         ...     np.arange(12).reshape(4, 3),
5140         ...     dims=["x", "y"],
5141         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5142         ... )
5143         >>> da
5144         <xarray.DataArray (x: 4, y: 3)>
5145         array([[ 0,  1,  2],
5146                [ 3,  4,  5],
5147                [ 6,  7,  8],
5148                [ 9, 10, 11]])
5149         Coordinates:
5150           * x        (x) float64 0.0 0.1 1.1 1.2
5151         Dimensions without coordinates: y
5152         >>>
5153         >>> da.differentiate("x")
5154         <xarray.DataArray (x: 4, y: 3)>
5155         array([[30.        , 30.        , 30.        ],
5156                [27.54545455, 27.54545455, 27.54545455],
5157                [27.54545455, 27.54545455, 27.54545455],
5158                [30.        , 30.        , 30.        ]])
5159         Coordinates:
5160           * x        (x) float64 0.0 0.1 1.1 1.2
5161         Dimensions without coordinates: y
5162         """
5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
5164         return self._from_temp_dataset(ds)
5165 
5166     # change type of self and return to T_DataArray once
5167     # https://github.com/python/mypy/issues/12846 is resolved
5168     def integrate(
5169         self,
5170         coord: Hashable | Sequence[Hashable] = None,
5171         datetime_unit: DatetimeUnitOptions = None,
5172     ) -> DataArray:
5173         """Integrate along the given coordinate using the trapezoidal rule.
5174 
5175         .. note::
5176             This feature is limited to simple cartesian geometry, i.e. coord
5177             must be one dimensional.
5178 
5179         Parameters
5180         ----------
5181         coord : Hashable, or sequence of Hashable
5182             Coordinate(s) used for the integration.
5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5184                         'ps', 'fs', 'as', None}, optional
5185             Specify the unit if a datetime coordinate is used.
5186 
5187         Returns
5188         -------
5189         integrated : DataArray
5190 
5191         See also
5192         --------
5193         Dataset.integrate
5194         numpy.trapz : corresponding numpy function
5195 
5196         Examples
5197         --------
5198 
5199         >>> da = xr.DataArray(
5200         ...     np.arange(12).reshape(4, 3),
5201         ...     dims=["x", "y"],
5202         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5203         ... )
5204         >>> da
5205         <xarray.DataArray (x: 4, y: 3)>
5206         array([[ 0,  1,  2],
5207                [ 3,  4,  5],
5208                [ 6,  7,  8],
5209                [ 9, 10, 11]])
5210         Coordinates:
5211           * x        (x) float64 0.0 0.1 1.1 1.2
5212         Dimensions without coordinates: y
5213         >>>
5214         >>> da.integrate("x")
5215         <xarray.DataArray (y: 3)>
5216         array([5.4, 6.6, 7.8])
5217         Dimensions without coordinates: y
5218         """
5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5220         return self._from_temp_dataset(ds)
5221 
5222     # change type of self and return to T_DataArray once
5223     # https://github.com/python/mypy/issues/12846 is resolved
5224     def cumulative_integrate(
5225         self,
5226         coord: Hashable | Sequence[Hashable] = None,
5227         datetime_unit: DatetimeUnitOptions = None,
5228     ) -> DataArray:
5229         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5230 
5231         .. note::
5232             This feature is limited to simple cartesian geometry, i.e. coord
5233             must be one dimensional.
5234 
5235             The first entry of the cumulative integral is always 0, in order to keep the
5236             length of the dimension unchanged between input and output.
5237 
5238         Parameters
5239         ----------
5240         coord : Hashable, or sequence of Hashable
5241             Coordinate(s) used for the integration.
5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5243                         'ps', 'fs', 'as', None}, optional
5244             Specify the unit if a datetime coordinate is used.
5245 
5246         Returns
5247         -------
5248         integrated : DataArray
5249 
5250         See also
5251         --------
5252         Dataset.cumulative_integrate
5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5254 
5255         Examples
5256         --------
5257 
5258         >>> da = xr.DataArray(
5259         ...     np.arange(12).reshape(4, 3),
5260         ...     dims=["x", "y"],
5261         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5262         ... )
5263         >>> da
5264         <xarray.DataArray (x: 4, y: 3)>
5265         array([[ 0,  1,  2],
5266                [ 3,  4,  5],
5267                [ 6,  7,  8],
5268                [ 9, 10, 11]])
5269         Coordinates:
5270           * x        (x) float64 0.0 0.1 1.1 1.2
5271         Dimensions without coordinates: y
5272         >>>
5273         >>> da.cumulative_integrate("x")
5274         <xarray.DataArray (x: 4, y: 3)>
5275         array([[0.  , 0.  , 0.  ],
5276                [0.15, 0.25, 0.35],
5277                [4.65, 5.75, 6.85],
5278                [5.4 , 6.6 , 7.8 ]])
5279         Coordinates:
5280           * x        (x) float64 0.0 0.1 1.1 1.2
5281         Dimensions without coordinates: y
5282         """
5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5284         return self._from_temp_dataset(ds)
5285 
5286     def unify_chunks(self) -> DataArray:
5287         """Unify chunk size along all chunked dimensions of this DataArray.
5288 
5289         Returns
5290         -------
5291         DataArray with consistent chunk sizes for all dask-array variables
5292 
5293         See Also
5294         --------
5295         dask.array.core.unify_chunks
5296         """
5297 
5298         return unify_chunks(self)[0]
5299 
5300     def map_blocks(
5301         self,
5302         func: Callable[..., T_Xarray],
5303         args: Sequence[Any] = (),
5304         kwargs: Mapping[str, Any] | None = None,
5305         template: DataArray | Dataset | None = None,
5306     ) -> T_Xarray:
5307         """
5308         Apply a function to each block of this DataArray.
5309 
5310         .. warning::
5311             This method is experimental and its signature may change.
5312 
5313         Parameters
5314         ----------
5315         func : callable
5316             User-provided function that accepts a DataArray as its first
5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5318             corresponding to one chunk along each chunked dimension. ``func`` will be
5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5320 
5321             This function must return either a single DataArray or a single Dataset.
5322 
5323             This function cannot add a new chunked dimension.
5324         args : sequence
5325             Passed to func after unpacking and subsetting any xarray objects by blocks.
5326             xarray objects in args must be aligned with this object, otherwise an error is raised.
5327         kwargs : mapping
5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5329             subset to blocks. Passing dask collections in kwargs is not allowed.
5330         template : DataArray or Dataset, optional
5331             xarray object representing the final result after compute is called. If not provided,
5332             the function will be first run on mocked-up data, that looks like this object but
5333             has sizes 0, to determine properties of the returned object such as dtype,
5334             variable names, attributes, new dimensions and new indexes (if any).
5335             ``template`` must be provided if the function changes the size of existing dimensions.
5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5337             ``attrs`` set by ``func`` will be ignored.
5338 
5339         Returns
5340         -------
5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5342         function.
5343 
5344         Notes
5345         -----
5346         This function is designed for when ``func`` needs to manipulate a whole xarray object
5347         subset to each block. Each block is loaded into memory. In the more common case where
5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5349 
5350         If none of the variables in this object is backed by dask arrays, calling this function is
5351         equivalent to calling ``func(obj, *args, **kwargs)``.
5352 
5353         See Also
5354         --------
5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5356         xarray.DataArray.map_blocks
5357 
5358         Examples
5359         --------
5360         Calculate an anomaly from climatology using ``.groupby()``. Using
5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5362         its indices, and its methods like ``.groupby()``.
5363 
5364         >>> def calculate_anomaly(da, groupby_type="time.month"):
5365         ...     gb = da.groupby(groupby_type)
5366         ...     clim = gb.mean(dim="time")
5367         ...     return gb - clim
5368         ...
5369         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5370         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5371         >>> np.random.seed(123)
5372         >>> array = xr.DataArray(
5373         ...     np.random.rand(len(time)),
5374         ...     dims=["time"],
5375         ...     coords={"time": time, "month": month},
5376         ... ).chunk()
5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5378         <xarray.DataArray (time: 24)>
5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5384         Coordinates:
5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5387 
5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5389         to the function being applied in ``xr.map_blocks()``:
5390 
5391         >>> array.map_blocks(
5392         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5393         ... )  # doctest: +ELLIPSIS
5394         <xarray.DataArray (time: 24)>
5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5396         Coordinates:
5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5399         """
5400         from xarray.core.parallel import map_blocks
5401 
5402         return map_blocks(func, self, args, kwargs, template)
5403 
5404     def polyfit(
5405         self,
5406         dim: Hashable,
5407         deg: int,
5408         skipna: bool | None = None,
5409         rcond: float | None = None,
5410         w: Hashable | Any | None = None,
5411         full: bool = False,
5412         cov: bool | Literal["unscaled"] = False,
5413     ) -> Dataset:
5414         """
5415         Least squares polynomial fit.
5416 
5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5418         invalid values when `skipna = True`.
5419 
5420         Parameters
5421         ----------
5422         dim : Hashable
5423             Coordinate along which to fit the polynomials.
5424         deg : int
5425             Degree of the fitting polynomial.
5426         skipna : bool or None, optional
5427             If True, removes all invalid values before fitting each 1D slices of the array.
5428             Default is True if data is stored in a dask.array or if there is any
5429             invalid values, False otherwise.
5430         rcond : float or None, optional
5431             Relative condition number to the fit.
5432         w : Hashable, array-like or None, optional
5433             Weights to apply to the y-coordinate of the sample points.
5434             Can be an array-like object or the name of a coordinate in the dataset.
5435         full : bool, default: False
5436             Whether to return the residuals, matrix rank and singular values in addition
5437             to the coefficients.
5438         cov : bool or "unscaled", default: False
5439             Whether to return to the covariance matrix in addition to the coefficients.
5440             The matrix is not scaled if `cov='unscaled'`.
5441 
5442         Returns
5443         -------
5444         polyfit_results : Dataset
5445             A single dataset which contains:
5446 
5447             polyfit_coefficients
5448                 The coefficients of the best fit.
5449             polyfit_residuals
5450                 The residuals of the least-square computation (only included if `full=True`).
5451                 When the matrix rank is deficient, np.nan is returned.
5452             [dim]_matrix_rank
5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5454             [dim]_singular_value
5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5456             polyfit_covariance
5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5458 
5459         See Also
5460         --------
5461         numpy.polyfit
5462         numpy.polyval
5463         xarray.polyval
5464         """
5465         return self._to_temp_dataset().polyfit(
5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5467         )
5468 
5469     def pad(
5470         self: T_DataArray,
5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5472         mode: PadModeOptions = "constant",
5473         stat_length: int
5474         | tuple[int, int]
5475         | Mapping[Any, tuple[int, int]]
5476         | None = None,
5477         constant_values: float
5478         | tuple[float, float]
5479         | Mapping[Any, tuple[float, float]]
5480         | None = None,
5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5482         reflect_type: PadReflectOptions = None,
5483         keep_attrs: bool | None = None,
5484         **pad_width_kwargs: Any,
5485     ) -> T_DataArray:
5486         """Pad this array along one or more dimensions.
5487 
5488         .. warning::
5489             This function is experimental and its behaviour is likely to change
5490             especially regarding padding of dimension coordinates (or IndexVariables).
5491 
5492         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5493         coordinates will be padded with the same mode, otherwise coordinates
5494         are padded using the "constant" mode with fill_value dtypes.NA.
5495 
5496         Parameters
5497         ----------
5498         pad_width : mapping of Hashable to tuple of int
5499             Mapping with the form of {dim: (pad_before, pad_after)}
5500             describing the number of values padded along each dimension.
5501             {dim: pad} is a shortcut for pad_before = pad_after = pad
5502         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5503             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5504             How to pad the DataArray (taken from numpy docs):
5505 
5506             - "constant": Pads with a constant value.
5507             - "edge": Pads with the edge values of array.
5508             - "linear_ramp": Pads with the linear ramp between end_value and the
5509               array edge value.
5510             - "maximum": Pads with the maximum value of all or part of the
5511               vector along each axis.
5512             - "mean": Pads with the mean value of all or part of the
5513               vector along each axis.
5514             - "median": Pads with the median value of all or part of the
5515               vector along each axis.
5516             - "minimum": Pads with the minimum value of all or part of the
5517               vector along each axis.
5518             - "reflect": Pads with the reflection of the vector mirrored on
5519               the first and last values of the vector along each axis.
5520             - "symmetric": Pads with the reflection of the vector mirrored
5521               along the edge of the array.
5522             - "wrap": Pads with the wrap of the vector along the axis.
5523               The first values are used to pad the end and the
5524               end values are used to pad the beginning.
5525 
5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5528             values at edge of each axis used to calculate the statistic value.
5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5530             statistic lengths along each dimension.
5531             ((before, after),) yields same before and after statistic lengths
5532             for each dimension.
5533             (stat_length,) or int is a shortcut for before = after = statistic
5534             length for all axes.
5535             Default is ``None``, to use the entire axis.
5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5537             Used in 'constant'.  The values to set the padded values for each
5538             axis.
5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5540             pad constants along each dimension.
5541             ``((before, after),)`` yields same before and after constants for each
5542             dimension.
5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5544             all dimensions.
5545             Default is 0.
5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5547             Used in 'linear_ramp'.  The values used for the ending value of the
5548             linear_ramp and that will form the edge of the padded array.
5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5550             end values along each dimension.
5551             ``((before, after),)`` yields same before and after end values for each
5552             axis.
5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5554             all axes.
5555             Default is 0.
5556         reflect_type : {"even", "odd", None}, optional
5557             Used in "reflect", and "symmetric". The "even" style is the
5558             default with an unaltered reflection around the edge value. For
5559             the "odd" style, the extended part of the array is created by
5560             subtracting the reflected values from two times the edge value.
5561         keep_attrs : bool or None, optional
5562             If True, the attributes (``attrs``) will be copied from the
5563             original object to the new one. If False, the new object
5564             will be returned without attributes.
5565         **pad_width_kwargs
5566             The keyword arguments form of ``pad_width``.
5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5568 
5569         Returns
5570         -------
5571         padded : DataArray
5572             DataArray with the padded coordinates and data.
5573 
5574         See Also
5575         --------
5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5577 
5578         Notes
5579         -----
5580         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5581         promoted to ``float`` and padded with ``np.nan``.
5582 
5583         Padding coordinates will drop their corresponding index (if any) and will reset default
5584         indexes for dimension coordinates.
5585 
5586         Examples
5587         --------
5588         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5589         >>> arr.pad(x=(1, 2), constant_values=0)
5590         <xarray.DataArray (x: 6)>
5591         array([0, 5, 6, 7, 0, 0])
5592         Coordinates:
5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5594 
5595         >>> da = xr.DataArray(
5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5597         ...     dims=["x", "y"],
5598         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5599         ... )
5600         >>> da.pad(x=1)
5601         <xarray.DataArray (x: 4, y: 4)>
5602         array([[nan, nan, nan, nan],
5603                [ 0.,  1.,  2.,  3.],
5604                [10., 11., 12., 13.],
5605                [nan, nan, nan, nan]])
5606         Coordinates:
5607           * x        (x) float64 nan 0.0 1.0 nan
5608           * y        (y) int64 10 20 30 40
5609             z        (x) float64 nan 100.0 200.0 nan
5610 
5611         Careful, ``constant_values`` are coerced to the data type of the array which may
5612         lead to a loss of precision:
5613 
5614         >>> da.pad(x=1, constant_values=1.23456789)
5615         <xarray.DataArray (x: 4, y: 4)>
5616         array([[ 1,  1,  1,  1],
5617                [ 0,  1,  2,  3],
5618                [10, 11, 12, 13],
5619                [ 1,  1,  1,  1]])
5620         Coordinates:
5621           * x        (x) float64 nan 0.0 1.0 nan
5622           * y        (y) int64 10 20 30 40
5623             z        (x) float64 nan 100.0 200.0 nan
5624         """
5625         ds = self._to_temp_dataset().pad(
5626             pad_width=pad_width,
5627             mode=mode,
5628             stat_length=stat_length,
5629             constant_values=constant_values,
5630             end_values=end_values,
5631             reflect_type=reflect_type,
5632             keep_attrs=keep_attrs,
5633             **pad_width_kwargs,
5634         )
5635         return self._from_temp_dataset(ds)
5636 
5637     def idxmin(
5638         self,
5639         dim: Hashable | None = None,
5640         skipna: bool | None = None,
5641         fill_value: Any = dtypes.NA,
5642         keep_attrs: bool | None = None,
5643     ) -> DataArray:
5644         """Return the coordinate label of the minimum value along a dimension.
5645 
5646         Returns a new `DataArray` named after the dimension with the values of
5647         the coordinate labels along that dimension corresponding to minimum
5648         values along that dimension.
5649 
5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5652 
5653         Parameters
5654         ----------
5655         dim : str, optional
5656             Dimension over which to apply `idxmin`.  This is optional for 1D
5657             arrays, but required for arrays with 2 or more dimensions.
5658         skipna : bool or None, default: None
5659             If True, skip missing values (as marked by NaN). By default, only
5660             skips missing values for ``float``, ``complex``, and ``object``
5661             dtypes; other dtypes either do not have a sentinel missing value
5662             (``int``) or ``skipna=True`` has not been implemented
5663             (``datetime64`` or ``timedelta64``).
5664         fill_value : Any, default: NaN
5665             Value to be filled in case all of the values along a dimension are
5666             null.  By default this is NaN.  The fill value and result are
5667             automatically converted to a compatible dtype if possible.
5668             Ignored if ``skipna`` is False.
5669         keep_attrs : bool or None, optional
5670             If True, the attributes (``attrs``) will be copied from the
5671             original object to the new one. If False, the new object
5672             will be returned without attributes.
5673 
5674         Returns
5675         -------
5676         reduced : DataArray
5677             New `DataArray` object with `idxmin` applied to its data and the
5678             indicated dimension removed.
5679 
5680         See Also
5681         --------
5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5683 
5684         Examples
5685         --------
5686         >>> array = xr.DataArray(
5687         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5688         ... )
5689         >>> array.min()
5690         <xarray.DataArray ()>
5691         array(-2)
5692         >>> array.argmin(...)
5693         {'x': <xarray.DataArray ()>
5694         array(4)}
5695         >>> array.idxmin()
5696         <xarray.DataArray 'x' ()>
5697         array('e', dtype='<U1')
5698 
5699         >>> array = xr.DataArray(
5700         ...     [
5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5704         ...     ],
5705         ...     dims=["y", "x"],
5706         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5707         ... )
5708         >>> array.min(dim="x")
5709         <xarray.DataArray (y: 3)>
5710         array([-2., -4.,  1.])
5711         Coordinates:
5712           * y        (y) int64 -1 0 1
5713         >>> array.argmin(dim="x")
5714         <xarray.DataArray (y: 3)>
5715         array([4, 0, 2])
5716         Coordinates:
5717           * y        (y) int64 -1 0 1
5718         >>> array.idxmin(dim="x")
5719         <xarray.DataArray 'x' (y: 3)>
5720         array([16.,  0.,  4.])
5721         Coordinates:
5722           * y        (y) int64 -1 0 1
5723         """
5724         return computation._calc_idxminmax(
5725             array=self,
5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5727             dim=dim,
5728             skipna=skipna,
5729             fill_value=fill_value,
5730             keep_attrs=keep_attrs,
5731         )
5732 
5733     def idxmax(
5734         self,
5735         dim: Hashable = None,
5736         skipna: bool | None = None,
5737         fill_value: Any = dtypes.NA,
5738         keep_attrs: bool | None = None,
5739     ) -> DataArray:
5740         """Return the coordinate label of the maximum value along a dimension.
5741 
5742         Returns a new `DataArray` named after the dimension with the values of
5743         the coordinate labels along that dimension corresponding to maximum
5744         values along that dimension.
5745 
5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5748 
5749         Parameters
5750         ----------
5751         dim : Hashable, optional
5752             Dimension over which to apply `idxmax`.  This is optional for 1D
5753             arrays, but required for arrays with 2 or more dimensions.
5754         skipna : bool or None, default: None
5755             If True, skip missing values (as marked by NaN). By default, only
5756             skips missing values for ``float``, ``complex``, and ``object``
5757             dtypes; other dtypes either do not have a sentinel missing value
5758             (``int``) or ``skipna=True`` has not been implemented
5759             (``datetime64`` or ``timedelta64``).
5760         fill_value : Any, default: NaN
5761             Value to be filled in case all of the values along a dimension are
5762             null.  By default this is NaN.  The fill value and result are
5763             automatically converted to a compatible dtype if possible.
5764             Ignored if ``skipna`` is False.
5765         keep_attrs : bool or None, optional
5766             If True, the attributes (``attrs``) will be copied from the
5767             original object to the new one. If False, the new object
5768             will be returned without attributes.
5769 
5770         Returns
5771         -------
5772         reduced : DataArray
5773             New `DataArray` object with `idxmax` applied to its data and the
5774             indicated dimension removed.
5775 
5776         See Also
5777         --------
5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5779 
5780         Examples
5781         --------
5782         >>> array = xr.DataArray(
5783         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5784         ... )
5785         >>> array.max()
5786         <xarray.DataArray ()>
5787         array(2)
5788         >>> array.argmax(...)
5789         {'x': <xarray.DataArray ()>
5790         array(1)}
5791         >>> array.idxmax()
5792         <xarray.DataArray 'x' ()>
5793         array('b', dtype='<U1')
5794 
5795         >>> array = xr.DataArray(
5796         ...     [
5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5800         ...     ],
5801         ...     dims=["y", "x"],
5802         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5803         ... )
5804         >>> array.max(dim="x")
5805         <xarray.DataArray (y: 3)>
5806         array([2., 2., 1.])
5807         Coordinates:
5808           * y        (y) int64 -1 0 1
5809         >>> array.argmax(dim="x")
5810         <xarray.DataArray (y: 3)>
5811         array([0, 2, 2])
5812         Coordinates:
5813           * y        (y) int64 -1 0 1
5814         >>> array.idxmax(dim="x")
5815         <xarray.DataArray 'x' (y: 3)>
5816         array([0., 4., 4.])
5817         Coordinates:
5818           * y        (y) int64 -1 0 1
5819         """
5820         return computation._calc_idxminmax(
5821             array=self,
5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5823             dim=dim,
5824             skipna=skipna,
5825             fill_value=fill_value,
5826             keep_attrs=keep_attrs,
5827         )
5828 
5829     # change type of self and return to T_DataArray once
5830     # https://github.com/python/mypy/issues/12846 is resolved
5831     def argmin(
5832         self,
5833         dim: Dims = None,
5834         axis: int | None = None,
5835         keep_attrs: bool | None = None,
5836         skipna: bool | None = None,
5837     ) -> DataArray | dict[Hashable, DataArray]:
5838         """Index or indices of the minimum of the DataArray over one or more dimensions.
5839 
5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5841         which can be passed directly to isel(). If a single str is passed to 'dim' then
5842         returns a DataArray with dtype int.
5843 
5844         If there are multiple minima, the indices of the first one found will be
5845         returned.
5846 
5847         Parameters
5848         ----------
5849         dim : "...", str, Iterable of Hashable or None, optional
5850             The dimensions over which to find the minimum. By default, finds minimum over
5851             all dimensions - for now returning an int for backward compatibility, but
5852             this is deprecated, in future will return a dict with indices for all
5853             dimensions; to return a dict with all dimensions now, pass '...'.
5854         axis : int or None, optional
5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5856             can be supplied.
5857         keep_attrs : bool or None, optional
5858             If True, the attributes (`attrs`) will be copied from the original
5859             object to the new one. If False, the new object will be
5860             returned without attributes.
5861         skipna : bool or None, optional
5862             If True, skip missing values (as marked by NaN). By default, only
5863             skips missing values for float dtypes; other dtypes either do not
5864             have a sentinel missing value (int) or skipna=True has not been
5865             implemented (object, datetime64 or timedelta64).
5866 
5867         Returns
5868         -------
5869         result : DataArray or dict of DataArray
5870 
5871         See Also
5872         --------
5873         Variable.argmin, DataArray.idxmin
5874 
5875         Examples
5876         --------
5877         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5878         >>> array.min()
5879         <xarray.DataArray ()>
5880         array(-1)
5881         >>> array.argmin(...)
5882         {'x': <xarray.DataArray ()>
5883         array(2)}
5884         >>> array.isel(array.argmin(...))
5885         <xarray.DataArray ()>
5886         array(-1)
5887 
5888         >>> array = xr.DataArray(
5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5890         ...     dims=("x", "y", "z"),
5891         ... )
5892         >>> array.min(dim="x")
5893         <xarray.DataArray (y: 3, z: 3)>
5894         array([[ 1,  2,  1],
5895                [ 2, -5,  1],
5896                [ 2,  1,  1]])
5897         Dimensions without coordinates: y, z
5898         >>> array.argmin(dim="x")
5899         <xarray.DataArray (y: 3, z: 3)>
5900         array([[1, 0, 0],
5901                [1, 1, 1],
5902                [0, 0, 1]])
5903         Dimensions without coordinates: y, z
5904         >>> array.argmin(dim=["x"])
5905         {'x': <xarray.DataArray (y: 3, z: 3)>
5906         array([[1, 0, 0],
5907                [1, 1, 1],
5908                [0, 0, 1]])
5909         Dimensions without coordinates: y, z}
5910         >>> array.min(dim=("x", "z"))
5911         <xarray.DataArray (y: 3)>
5912         array([ 1, -5,  1])
5913         Dimensions without coordinates: y
5914         >>> array.argmin(dim=["x", "z"])
5915         {'x': <xarray.DataArray (y: 3)>
5916         array([0, 1, 0])
5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5918         array([2, 1, 1])
5919         Dimensions without coordinates: y}
5920         >>> array.isel(array.argmin(dim=["x", "z"]))
5921         <xarray.DataArray (y: 3)>
5922         array([ 1, -5,  1])
5923         Dimensions without coordinates: y
5924         """
5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5926         if isinstance(result, dict):
5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5928         else:
5929             return self._replace_maybe_drop_dims(result)
5930 
5931     # change type of self and return to T_DataArray once
5932     # https://github.com/python/mypy/issues/12846 is resolved
5933     def argmax(
5934         self,
5935         dim: Dims = None,
5936         axis: int | None = None,
5937         keep_attrs: bool | None = None,
5938         skipna: bool | None = None,
5939     ) -> DataArray | dict[Hashable, DataArray]:
5940         """Index or indices of the maximum of the DataArray over one or more dimensions.
5941 
5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5943         which can be passed directly to isel(). If a single str is passed to 'dim' then
5944         returns a DataArray with dtype int.
5945 
5946         If there are multiple maxima, the indices of the first one found will be
5947         returned.
5948 
5949         Parameters
5950         ----------
5951         dim : "...", str, Iterable of Hashable or None, optional
5952             The dimensions over which to find the maximum. By default, finds maximum over
5953             all dimensions - for now returning an int for backward compatibility, but
5954             this is deprecated, in future will return a dict with indices for all
5955             dimensions; to return a dict with all dimensions now, pass '...'.
5956         axis : int or None, optional
5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5958             can be supplied.
5959         keep_attrs : bool or None, optional
5960             If True, the attributes (`attrs`) will be copied from the original
5961             object to the new one. If False, the new object will be
5962             returned without attributes.
5963         skipna : bool or None, optional
5964             If True, skip missing values (as marked by NaN). By default, only
5965             skips missing values for float dtypes; other dtypes either do not
5966             have a sentinel missing value (int) or skipna=True has not been
5967             implemented (object, datetime64 or timedelta64).
5968 
5969         Returns
5970         -------
5971         result : DataArray or dict of DataArray
5972 
5973         See Also
5974         --------
5975         Variable.argmax, DataArray.idxmax
5976 
5977         Examples
5978         --------
5979         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5980         >>> array.max()
5981         <xarray.DataArray ()>
5982         array(3)
5983         >>> array.argmax(...)
5984         {'x': <xarray.DataArray ()>
5985         array(3)}
5986         >>> array.isel(array.argmax(...))
5987         <xarray.DataArray ()>
5988         array(3)
5989 
5990         >>> array = xr.DataArray(
5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5992         ...     dims=("x", "y", "z"),
5993         ... )
5994         >>> array.max(dim="x")
5995         <xarray.DataArray (y: 3, z: 3)>
5996         array([[3, 3, 2],
5997                [3, 5, 2],
5998                [2, 3, 3]])
5999         Dimensions without coordinates: y, z
6000         >>> array.argmax(dim="x")
6001         <xarray.DataArray (y: 3, z: 3)>
6002         array([[0, 1, 1],
6003                [0, 1, 0],
6004                [0, 1, 0]])
6005         Dimensions without coordinates: y, z
6006         >>> array.argmax(dim=["x"])
6007         {'x': <xarray.DataArray (y: 3, z: 3)>
6008         array([[0, 1, 1],
6009                [0, 1, 0],
6010                [0, 1, 0]])
6011         Dimensions without coordinates: y, z}
6012         >>> array.max(dim=("x", "z"))
6013         <xarray.DataArray (y: 3)>
6014         array([3, 5, 3])
6015         Dimensions without coordinates: y
6016         >>> array.argmax(dim=["x", "z"])
6017         {'x': <xarray.DataArray (y: 3)>
6018         array([0, 1, 0])
6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
6020         array([0, 1, 2])
6021         Dimensions without coordinates: y}
6022         >>> array.isel(array.argmax(dim=["x", "z"]))
6023         <xarray.DataArray (y: 3)>
6024         array([3, 5, 3])
6025         Dimensions without coordinates: y
6026         """
6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
6028         if isinstance(result, dict):
6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
6030         else:
6031             return self._replace_maybe_drop_dims(result)
6032 
6033     def query(
6034         self,
6035         queries: Mapping[Any, Any] | None = None,
6036         parser: QueryParserOptions = "pandas",
6037         engine: QueryEngineOptions = None,
6038         missing_dims: ErrorOptionsWithWarn = "raise",
6039         **queries_kwargs: Any,
6040     ) -> DataArray:
6041         """Return a new data array indexed along the specified
6042         dimension(s), where the indexers are given as strings containing
6043         Python expressions to be evaluated against the values in the array.
6044 
6045         Parameters
6046         ----------
6047         queries : dict-like or None, optional
6048             A dict-like with keys matching dimensions and values given by strings
6049             containing Python expressions to be evaluated against the data variables
6050             in the dataset. The expressions will be evaluated using the pandas
6051             eval() function, and can contain any valid Python expressions but cannot
6052             contain any Python statements.
6053         parser : {"pandas", "python"}, default: "pandas"
6054             The parser to use to construct the syntax tree from the expression.
6055             The default of 'pandas' parses code slightly different than standard
6056             Python. Alternatively, you can parse an expression using the 'python'
6057             parser to retain strict Python semantics.
6058         engine : {"python", "numexpr", None}, default: None
6059             The engine used to evaluate the expression. Supported engines are:
6060 
6061             - None: tries to use numexpr, falls back to python
6062             - "numexpr": evaluates expressions using numexpr
6063             - "python": performs operations as if you had eval’d in top level python
6064 
6065         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
6066             What to do if dimensions that should be selected from are not present in the
6067             DataArray:
6068 
6069             - "raise": raise an exception
6070             - "warn": raise a warning, and ignore the missing dimensions
6071             - "ignore": ignore the missing dimensions
6072 
6073         **queries_kwargs : {dim: query, ...}, optional
6074             The keyword arguments form of ``queries``.
6075             One of queries or queries_kwargs must be provided.
6076 
6077         Returns
6078         -------
6079         obj : DataArray
6080             A new DataArray with the same contents as this dataset, indexed by
6081             the results of the appropriate queries.
6082 
6083         See Also
6084         --------
6085         DataArray.isel
6086         Dataset.query
6087         pandas.eval
6088 
6089         Examples
6090         --------
6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
6092         >>> da
6093         <xarray.DataArray 'a' (x: 5)>
6094         array([0, 1, 2, 3, 4])
6095         Dimensions without coordinates: x
6096         >>> da.query(x="a > 2")
6097         <xarray.DataArray 'a' (x: 2)>
6098         array([3, 4])
6099         Dimensions without coordinates: x
6100         """
6101 
6102         ds = self._to_dataset_whole(shallow_copy=True)
6103         ds = ds.query(
6104             queries=queries,
6105             parser=parser,
6106             engine=engine,
6107             missing_dims=missing_dims,
6108             **queries_kwargs,
6109         )
6110         return ds[self.name]
6111 
6112     def curvefit(
6113         self,
6114         coords: str | DataArray | Iterable[str | DataArray],
6115         func: Callable[..., Any],
6116         reduce_dims: Dims = None,
6117         skipna: bool = True,
6118         p0: dict[str, Any] | None = None,
6119         bounds: dict[str, Any] | None = None,
6120         param_names: Sequence[str] | None = None,
6121         kwargs: dict[str, Any] | None = None,
6122     ) -> Dataset:
6123         """
6124         Curve fitting optimization for arbitrary functions.
6125 
6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
6127 
6128         Parameters
6129         ----------
6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable
6131             Independent coordinate(s) over which to perform the curve fitting. Must share
6132             at least one dimension with the calling object. When fitting multi-dimensional
6133             functions, supply `coords` as a sequence in the same order as arguments in
6134             `func`. To fit along existing dimensions of the calling object, `coords` can
6135             also be specified as a str or sequence of strs.
6136         func : callable
6137             User specified function in the form `f(x, *params)` which returns a numpy
6138             array of length `len(x)`. `params` are the fittable parameters which are optimized
6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
6140             coordinates, e.g. `f((x0, x1), *params)`.
6141         reduce_dims : str, Iterable of Hashable or None, optional
6142             Additional dimension(s) over which to aggregate while fitting. For example,
6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
6144             aggregate all lat and lon points and fit the specified function along the
6145             time dimension.
6146         skipna : bool, default: True
6147             Whether to skip missing values when fitting. Default is True.
6148         p0 : dict-like or None, optional
6149             Optional dictionary of parameter names to initial guesses passed to the
6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
6151             be assigned initial values following the default scipy behavior.
6152         bounds : dict-like or None, optional
6153             Optional dictionary of parameter names to bounding values passed to the
6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
6155             will be unbounded following the default scipy behavior.
6156         param_names : sequence of Hashable or None, optional
6157             Sequence of names for the fittable parameters of `func`. If not supplied,
6158             this will be automatically determined by arguments of `func`. `param_names`
6159             should be manually supplied when fitting a function that takes a variable
6160             number of parameters.
6161         **kwargs : optional
6162             Additional keyword arguments to passed to scipy curve_fit.
6163 
6164         Returns
6165         -------
6166         curvefit_results : Dataset
6167             A single dataset which contains:
6168 
6169             [var]_curvefit_coefficients
6170                 The coefficients of the best fit.
6171             [var]_curvefit_covariance
6172                 The covariance matrix of the coefficient estimates.
6173 
6174         See Also
6175         --------
6176         DataArray.polyfit
6177         scipy.optimize.curve_fit
6178         """
6179         return self._to_temp_dataset().curvefit(
6180             coords,
6181             func,
6182             reduce_dims=reduce_dims,
6183             skipna=skipna,
6184             p0=p0,
6185             bounds=bounds,
6186             param_names=param_names,
6187             kwargs=kwargs,
6188         )
6189 
6190     def drop_duplicates(
6191         self: T_DataArray,
6192         dim: Hashable | Iterable[Hashable],
6193         keep: Literal["first", "last", False] = "first",
6194     ) -> T_DataArray:
6195         """Returns a new DataArray with duplicate dimension values removed.
6196 
6197         Parameters
6198         ----------
6199         dim : dimension label or labels
6200             Pass `...` to drop duplicates along all dimensions.
6201         keep : {"first", "last", False}, default: "first"
6202             Determines which duplicates (if any) to keep.
6203 
6204             - ``"first"`` : Drop duplicates except for the first occurrence.
6205             - ``"last"`` : Drop duplicates except for the last occurrence.
6206             - False : Drop all duplicates.
6207 
6208         Returns
6209         -------
6210         DataArray
6211 
6212         See Also
6213         --------
6214         Dataset.drop_duplicates
6215 
6216         Examples
6217         --------
6218         >>> da = xr.DataArray(
6219         ...     np.arange(25).reshape(5, 5),
6220         ...     dims=("x", "y"),
6221         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6222         ... )
6223         >>> da
6224         <xarray.DataArray (x: 5, y: 5)>
6225         array([[ 0,  1,  2,  3,  4],
6226                [ 5,  6,  7,  8,  9],
6227                [10, 11, 12, 13, 14],
6228                [15, 16, 17, 18, 19],
6229                [20, 21, 22, 23, 24]])
6230         Coordinates:
6231           * x        (x) int64 0 0 1 2 3
6232           * y        (y) int64 0 1 2 3 3
6233 
6234         >>> da.drop_duplicates(dim="x")
6235         <xarray.DataArray (x: 4, y: 5)>
6236         array([[ 0,  1,  2,  3,  4],
6237                [10, 11, 12, 13, 14],
6238                [15, 16, 17, 18, 19],
6239                [20, 21, 22, 23, 24]])
6240         Coordinates:
6241           * x        (x) int64 0 1 2 3
6242           * y        (y) int64 0 1 2 3 3
6243 
6244         >>> da.drop_duplicates(dim="x", keep="last")
6245         <xarray.DataArray (x: 4, y: 5)>
6246         array([[ 5,  6,  7,  8,  9],
6247                [10, 11, 12, 13, 14],
6248                [15, 16, 17, 18, 19],
6249                [20, 21, 22, 23, 24]])
6250         Coordinates:
6251           * x        (x) int64 0 1 2 3
6252           * y        (y) int64 0 1 2 3 3
6253 
6254         Drop all duplicate dimension values:
6255 
6256         >>> da.drop_duplicates(dim=...)
6257         <xarray.DataArray (x: 4, y: 4)>
6258         array([[ 0,  1,  2,  3],
6259                [10, 11, 12, 13],
6260                [15, 16, 17, 18],
6261                [20, 21, 22, 23]])
6262         Coordinates:
6263           * x        (x) int64 0 1 2 3
6264           * y        (y) int64 0 1 2 3
6265         """
6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6267         return self._from_temp_dataset(deduplicated)
6268 
6269     def convert_calendar(
6270         self,
6271         calendar: str,
6272         dim: str = "time",
6273         align_on: str | None = None,
6274         missing: Any | None = None,
6275         use_cftime: bool | None = None,
6276     ) -> DataArray:
6277         """Convert the DataArray to another calendar.
6278 
6279         Only converts the individual timestamps, does not modify any data except
6280         in dropping invalid/surplus dates or inserting missing dates.
6281 
6282         If the source and target calendars are either no_leap, all_leap or a
6283         standard type, only the type of the time array is modified.
6284         When converting to a leap year from a non-leap year, the 29th of February
6285         is removed from the array. In the other direction the 29th of February
6286         will be missing in the output, unless `missing` is specified,
6287         in which case that value is inserted.
6288 
6289         For conversions involving `360_day` calendars, see Notes.
6290 
6291         This method is safe to use with sub-daily data as it doesn't touch the
6292         time part of the timestamps.
6293 
6294         Parameters
6295         ---------
6296         calendar : str
6297             The target calendar name.
6298         dim : str
6299             Name of the time coordinate.
6300         align_on : {None, 'date', 'year'}
6301             Must be specified when either source or target is a `360_day` calendar,
6302            ignored otherwise. See Notes.
6303         missing : Optional[any]
6304             By default, i.e. if the value is None, this method will simply attempt
6305             to convert the dates in the source calendar to the same dates in the
6306             target calendar, and drop any of those that are not possible to
6307             represent.  If a value is provided, a new time coordinate will be
6308             created in the target calendar with the same frequency as the original
6309             time coordinate; for any dates that are not present in the source, the
6310             data will be filled with this value.  Note that using this mode requires
6311             that the source data have an inferable frequency; for more information
6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6313             target calendar combinations, this could result in many missing values, see notes.
6314         use_cftime : boolean, optional
6315             Whether to use cftime objects in the output, only used if `calendar`
6316             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6317             If True, the new time axis uses cftime objects.
6318             If None (default), it uses :py:class:`numpy.datetime64` values if the
6319             date range permits it, and :py:class:`cftime.datetime` objects if not.
6320             If False, it uses :py:class:`numpy.datetime64`  or fails.
6321 
6322         Returns
6323         -------
6324         DataArray
6325             Copy of the dataarray with the time coordinate converted to the
6326             target calendar. If 'missing' was None (default), invalid dates in
6327             the new calendar are dropped, but missing dates are not inserted.
6328             If `missing` was given, the new data is reindexed to have a time axis
6329             with the same frequency as the source, but in the new calendar; any
6330             missing datapoints are filled with `missing`.
6331 
6332         Notes
6333         -----
6334         Passing a value to `missing` is only usable if the source's time coordinate as an
6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6336         if the target coordinate, generated from this frequency, has dates equivalent to the
6337         source. It is usually **not** appropriate to use this mode with:
6338 
6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6341             or 'mH' where 24 % m != 0).
6342 
6343         If one of the source or target calendars is `"360_day"`, `align_on` must
6344         be specified and two options are offered.
6345 
6346         - "year"
6347             The dates are translated according to their relative position in the year,
6348             ignoring their original month and day information, meaning that the
6349             missing/surplus days are added/removed at regular intervals.
6350 
6351             From a `360_day` to a standard calendar, the output will be missing the
6352             following dates (day of year in parentheses):
6353 
6354             To a leap year:
6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6356                 September 31st (275) and November 30th (335).
6357             To a non-leap year:
6358                 February 6th (36), April 19th (109), July 2nd (183),
6359                 September 12th (255), November 25th (329).
6360 
6361             From a standard calendar to a `"360_day"`, the following dates in the
6362             source array will be dropped:
6363 
6364             From a leap year:
6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6366                 September 31st (275), December 1st (336)
6367             From a non-leap year:
6368                 February 6th (37), April 20th (110), July 2nd (183),
6369                 September 13th (256), November 25th (329)
6370 
6371             This option is best used on daily and subdaily data.
6372 
6373         - "date"
6374             The month/day information is conserved and invalid dates are dropped
6375             from the output. This means that when converting from a `"360_day"` to a
6376             standard calendar, all 31st (Jan, March, May, July, August, October and
6377             December) will be missing as there is no equivalent dates in the
6378             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6379             will be dropped as there are no equivalent dates in a standard calendar.
6380 
6381             This option is best used with data on a frequency coarser than daily.
6382         """
6383         return convert_calendar(
6384             self,
6385             calendar,
6386             dim=dim,
6387             align_on=align_on,
6388             missing=missing,
6389             use_cftime=use_cftime,
6390         )
6391 
6392     def interp_calendar(
6393         self,
6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6395         dim: str = "time",
6396     ) -> DataArray:
6397         """Interpolates the DataArray to another calendar based on decimal year measure.
6398 
6399         Each timestamp in `source` and `target` are first converted to their decimal
6400         year equivalent then `source` is interpolated on the target coordinate.
6401         The decimal year of a timestamp is its year plus its sub-year component
6402         converted to the fraction of its year. For example "2000-03-01 12:00" is
6403         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6404 
6405         This method should only be used when the time (HH:MM:SS) information of
6406         time coordinate is not important.
6407 
6408         Parameters
6409         ----------
6410         target: DataArray or DatetimeIndex or CFTimeIndex
6411             The target time coordinate of a valid dtype
6412             (np.datetime64 or cftime objects)
6413         dim : str
6414             The time coordinate name.
6415 
6416         Return
6417         ------
6418         DataArray
6419             The source interpolated on the decimal years of target,
6420         """
6421         return interp_calendar(self, target, dim=dim)
6422 
6423     def groupby(
6424         self,
6425         group: Hashable | DataArray | IndexVariable,
6426         squeeze: bool = True,
6427         restore_coord_dims: bool = False,
6428     ) -> DataArrayGroupBy:
6429         """Returns a DataArrayGroupBy object for performing grouped operations.
6430 
6431         Parameters
6432         ----------
6433         group : Hashable, DataArray or IndexVariable
6434             Array whose unique values should be used to group this array. If a
6435             Hashable, must be the name of a coordinate contained in this dataarray.
6436         squeeze : bool, default: True
6437             If "group" is a dimension of any arrays in this dataset, `squeeze`
6438             controls whether the subarrays have a dimension of length 1 along
6439             that dimension or if the dimension is squeezed out.
6440         restore_coord_dims : bool, default: False
6441             If True, also restore the dimension order of multi-dimensional
6442             coordinates.
6443 
6444         Returns
6445         -------
6446         grouped : DataArrayGroupBy
6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6448             iterated over in the form of `(unique_value, grouped_array)` pairs.
6449 
6450         Examples
6451         --------
6452         Calculate daily anomalies for daily data:
6453 
6454         >>> da = xr.DataArray(
6455         ...     np.linspace(0, 1826, num=1827),
6456         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6457         ...     dims="time",
6458         ... )
6459         >>> da
6460         <xarray.DataArray (time: 1827)>
6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6462                1.826e+03])
6463         Coordinates:
6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6465         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6466         <xarray.DataArray (time: 1827)>
6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6468         Coordinates:
6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6471 
6472         See Also
6473         --------
6474         :ref:`groupby`
6475             Users guide explanation of how to group and bin data.
6476         DataArray.groupby_bins
6477         Dataset.groupby
6478         core.groupby.DataArrayGroupBy
6479         pandas.DataFrame.groupby
6480         """
6481         from xarray.core.groupby import (
6482             DataArrayGroupBy,
6483             ResolvedUniqueGrouper,
6484             UniqueGrouper,
6485             _validate_groupby_squeeze,
6486         )
6487 
6488         _validate_groupby_squeeze(squeeze)
6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
6490         return DataArrayGroupBy(
6491             self,
6492             (rgrouper,),
6493             squeeze=squeeze,
6494             restore_coord_dims=restore_coord_dims,
6495         )
6496 
6497     def groupby_bins(
6498         self,
6499         group: Hashable | DataArray | IndexVariable,
6500         bins: ArrayLike,
6501         right: bool = True,
6502         labels: ArrayLike | Literal[False] | None = None,
6503         precision: int = 3,
6504         include_lowest: bool = False,
6505         squeeze: bool = True,
6506         restore_coord_dims: bool = False,
6507     ) -> DataArrayGroupBy:
6508         """Returns a DataArrayGroupBy object for performing grouped operations.
6509 
6510         Rather than using all unique values of `group`, the values are discretized
6511         first by applying `pandas.cut` [1]_ to `group`.
6512 
6513         Parameters
6514         ----------
6515         group : Hashable, DataArray or IndexVariable
6516             Array whose binned values should be used to group this array. If a
6517             Hashable, must be the name of a coordinate contained in this dataarray.
6518         bins : int or array-like
6519             If bins is an int, it defines the number of equal-width bins in the
6520             range of x. However, in this case, the range of x is extended by .1%
6521             on each side to include the min or max values of x. If bins is a
6522             sequence it defines the bin edges allowing for non-uniform bin
6523             width. No extension of the range of x is done in this case.
6524         right : bool, default: True
6525             Indicates whether the bins include the rightmost edge or not. If
6526             right == True (the default), then the bins [1,2,3,4] indicate
6527             (1,2], (2,3], (3,4].
6528         labels : array-like, False or None, default: None
6529             Used as labels for the resulting bins. Must be of the same length as
6530             the resulting bins. If False, string bin labels are assigned by
6531             `pandas.cut`.
6532         precision : int, default: 3
6533             The precision at which to store and display the bins labels.
6534         include_lowest : bool, default: False
6535             Whether the first interval should be left-inclusive or not.
6536         squeeze : bool, default: True
6537             If "group" is a dimension of any arrays in this dataset, `squeeze`
6538             controls whether the subarrays have a dimension of length 1 along
6539             that dimension or if the dimension is squeezed out.
6540         restore_coord_dims : bool, default: False
6541             If True, also restore the dimension order of multi-dimensional
6542             coordinates.
6543 
6544         Returns
6545         -------
6546         grouped : DataArrayGroupBy
6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6548             iterated over in the form of `(unique_value, grouped_array)` pairs.
6549             The name of the group has the added suffix `_bins` in order to
6550             distinguish it from the original variable.
6551 
6552         See Also
6553         --------
6554         :ref:`groupby`
6555             Users guide explanation of how to group and bin data.
6556         DataArray.groupby
6557         Dataset.groupby_bins
6558         core.groupby.DataArrayGroupBy
6559         pandas.DataFrame.groupby
6560 
6561         References
6562         ----------
6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6564         """
6565         from xarray.core.groupby import (
6566             BinGrouper,
6567             DataArrayGroupBy,
6568             ResolvedBinGrouper,
6569             _validate_groupby_squeeze,
6570         )
6571 
6572         _validate_groupby_squeeze(squeeze)
6573         grouper = BinGrouper(
6574             bins=bins,
6575             cut_kwargs={
6576                 "right": right,
6577                 "labels": labels,
6578                 "precision": precision,
6579                 "include_lowest": include_lowest,
6580             },
6581         )
6582         rgrouper = ResolvedBinGrouper(grouper, group, self)
6583 
6584         return DataArrayGroupBy(
6585             self,
6586             (rgrouper,),
6587             squeeze=squeeze,
6588             restore_coord_dims=restore_coord_dims,
6589         )
6590 
6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6592         """
6593         Weighted DataArray operations.
6594 
6595         Parameters
6596         ----------
6597         weights : DataArray
6598             An array of weights associated with the values in this Dataset.
6599             Each value in the data contributes to the reduction operation
6600             according to its associated weight.
6601 
6602         Notes
6603         -----
6604         ``weights`` must be a DataArray and cannot contain missing values.
6605         Missing values can be replaced by ``weights.fillna(0)``.
6606 
6607         Returns
6608         -------
6609         core.weighted.DataArrayWeighted
6610 
6611         See Also
6612         --------
6613         Dataset.weighted
6614         """
6615         from xarray.core.weighted import DataArrayWeighted
6616 
6617         return DataArrayWeighted(self, weights)
6618 
6619     def rolling(
6620         self,
6621         dim: Mapping[Any, int] | None = None,
6622         min_periods: int | None = None,
6623         center: bool | Mapping[Any, bool] = False,
6624         **window_kwargs: int,
6625     ) -> DataArrayRolling:
6626         """
6627         Rolling window object for DataArrays.
6628 
6629         Parameters
6630         ----------
6631         dim : dict, optional
6632             Mapping from the dimension name to create the rolling iterator
6633             along (e.g. `time`) to its moving window size.
6634         min_periods : int or None, default: None
6635             Minimum number of observations in window required to have a value
6636             (otherwise result is NA). The default, None, is equivalent to
6637             setting min_periods equal to the size of the window.
6638         center : bool or Mapping to int, default: False
6639             Set the labels at the center of the window.
6640         **window_kwargs : optional
6641             The keyword arguments form of ``dim``.
6642             One of dim or window_kwargs must be provided.
6643 
6644         Returns
6645         -------
6646         core.rolling.DataArrayRolling
6647 
6648         Examples
6649         --------
6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6651 
6652         >>> da = xr.DataArray(
6653         ...     np.linspace(0, 11, num=12),
6654         ...     coords=[
6655         ...         pd.date_range(
6656         ...             "1999-12-15",
6657         ...             periods=12,
6658         ...             freq=pd.DateOffset(months=1),
6659         ...         )
6660         ...     ],
6661         ...     dims="time",
6662         ... )
6663         >>> da
6664         <xarray.DataArray (time: 12)>
6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6666         Coordinates:
6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6668         >>> da.rolling(time=3, center=True).mean()
6669         <xarray.DataArray (time: 12)>
6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6671         Coordinates:
6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6673 
6674         Remove the NaNs using ``dropna()``:
6675 
6676         >>> da.rolling(time=3, center=True).mean().dropna("time")
6677         <xarray.DataArray (time: 10)>
6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6679         Coordinates:
6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6681 
6682         See Also
6683         --------
6684         core.rolling.DataArrayRolling
6685         Dataset.rolling
6686         """
6687         from xarray.core.rolling import DataArrayRolling
6688 
6689         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6691 
6692     def coarsen(
6693         self,
6694         dim: Mapping[Any, int] | None = None,
6695         boundary: CoarsenBoundaryOptions = "exact",
6696         side: SideOptions | Mapping[Any, SideOptions] = "left",
6697         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6698         **window_kwargs: int,
6699     ) -> DataArrayCoarsen:
6700         """
6701         Coarsen object for DataArrays.
6702 
6703         Parameters
6704         ----------
6705         dim : mapping of hashable to int, optional
6706             Mapping from the dimension name to the window size.
6707         boundary : {"exact", "trim", "pad"}, default: "exact"
6708             If 'exact', a ValueError will be raised if dimension size is not a
6709             multiple of the window size. If 'trim', the excess entries are
6710             dropped. If 'pad', NA will be padded.
6711         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6712         coord_func : str or mapping of hashable to str, default: "mean"
6713             function (name) that is applied to the coordinates,
6714             or a mapping from coordinate name to function (name).
6715 
6716         Returns
6717         -------
6718         core.rolling.DataArrayCoarsen
6719 
6720         Examples
6721         --------
6722         Coarsen the long time series by averaging over every three days.
6723 
6724         >>> da = xr.DataArray(
6725         ...     np.linspace(0, 364, num=364),
6726         ...     dims="time",
6727         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6728         ... )
6729         >>> da  # +doctest: ELLIPSIS
6730         <xarray.DataArray (time: 364)>
6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6734         ...
6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6736                360.99173554, 361.99449036, 362.99724518, 364.        ])
6737         Coordinates:
6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6739         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6740         <xarray.DataArray (time: 121)>
6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6744         ...
6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6746                361.99449036])
6747         Coordinates:
6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6749         >>>
6750 
6751         See Also
6752         --------
6753         core.rolling.DataArrayCoarsen
6754         Dataset.coarsen
6755         """
6756         from xarray.core.rolling import DataArrayCoarsen
6757 
6758         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6759         return DataArrayCoarsen(
6760             self,
6761             dim,
6762             boundary=boundary,
6763             side=side,
6764             coord_func=coord_func,
6765         )
6766 
6767     def resample(
6768         self,
6769         indexer: Mapping[Any, str] | None = None,
6770         skipna: bool | None = None,
6771         closed: SideOptions | None = None,
6772         label: SideOptions | None = None,
6773         base: int | None = None,
6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6775         origin: str | DatetimeLike = "start_day",
6776         keep_attrs: bool | None = None,
6777         loffset: datetime.timedelta | str | None = None,
6778         restore_coord_dims: bool | None = None,
6779         **indexer_kwargs: str,
6780     ) -> DataArrayResample:
6781         """Returns a Resample object for performing resampling operations.
6782 
6783         Handles both downsampling and upsampling. The resampled
6784         dimension must be a datetime-like coordinate. If any intervals
6785         contain no values from the original object, they will be given
6786         the value ``NaN``.
6787 
6788         Parameters
6789         ----------
6790         indexer : Mapping of Hashable to str, optional
6791             Mapping from the dimension name to resample frequency [1]_. The
6792             dimension must be datetime-like.
6793         skipna : bool, optional
6794             Whether to skip missing values when aggregating in downsampling.
6795         closed : {"left", "right"}, optional
6796             Side of each interval to treat as closed.
6797         label : {"left", "right"}, optional
6798             Side of each interval to use for labeling.
6799         base : int, optional
6800             For frequencies that evenly subdivide 1 day, the "origin" of the
6801             aggregated intervals. For example, for "24H" frequency, base could
6802             range from 0 through 23.
6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6804             The datetime on which to adjust the grouping. The timezone of origin
6805             must match the timezone of the index.
6806 
6807             If a datetime is not used, these values are also supported:
6808             - 'epoch': `origin` is 1970-01-01
6809             - 'start': `origin` is the first value of the timeseries
6810             - 'start_day': `origin` is the first day at midnight of the timeseries
6811             - 'end': `origin` is the last value of the timeseries
6812             - 'end_day': `origin` is the ceiling midnight of the last day
6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6814             An offset timedelta added to the origin.
6815         loffset : timedelta or str, optional
6816             Offset used to adjust the resampled time labels. Some pandas date
6817             offset strings are supported.
6818         restore_coord_dims : bool, optional
6819             If True, also restore the dimension order of multi-dimensional
6820             coordinates.
6821         **indexer_kwargs : str
6822             The keyword arguments form of ``indexer``.
6823             One of indexer or indexer_kwargs must be provided.
6824 
6825         Returns
6826         -------
6827         resampled : core.resample.DataArrayResample
6828             This object resampled.
6829 
6830         Examples
6831         --------
6832         Downsample monthly time-series data to seasonal data:
6833 
6834         >>> da = xr.DataArray(
6835         ...     np.linspace(0, 11, num=12),
6836         ...     coords=[
6837         ...         pd.date_range(
6838         ...             "1999-12-15",
6839         ...             periods=12,
6840         ...             freq=pd.DateOffset(months=1),
6841         ...         )
6842         ...     ],
6843         ...     dims="time",
6844         ... )
6845         >>> da
6846         <xarray.DataArray (time: 12)>
6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6848         Coordinates:
6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6850         >>> da.resample(time="QS-DEC").mean()
6851         <xarray.DataArray (time: 4)>
6852         array([ 1.,  4.,  7., 10.])
6853         Coordinates:
6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6855 
6856         Upsample monthly time-series data to daily data:
6857 
6858         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6859         <xarray.DataArray (time: 337)>
6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6863         ...
6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6865                10.96774194, 11.        ])
6866         Coordinates:
6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6868 
6869         Limit scope of upsampling method
6870 
6871         >>> da.resample(time="1D").nearest(tolerance="1D")
6872         <xarray.DataArray (time: 337)>
6873         array([ 0.,  0., nan, ..., nan, 11., 11.])
6874         Coordinates:
6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6876 
6877         See Also
6878         --------
6879         Dataset.resample
6880         pandas.Series.resample
6881         pandas.DataFrame.resample
6882 
6883         References
6884         ----------
6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6886         """
6887         from xarray.core.resample import DataArrayResample
6888 
6889         return self._resample(
6890             resample_cls=DataArrayResample,
6891             indexer=indexer,
6892             skipna=skipna,
6893             closed=closed,
6894             label=label,
6895             base=base,
6896             offset=offset,
6897             origin=origin,
6898             keep_attrs=keep_attrs,
6899             loffset=loffset,
6900             restore_coord_dims=restore_coord_dims,
6901             **indexer_kwargs,
6902         )
6903 
6904     def to_dask_dataframe(
6905         self,
6906         dim_order: Sequence[Hashable] | None = None,
6907         set_index: bool = False,
6908     ) -> DaskDataFrame:
6909         """Convert this array into a dask.dataframe.DataFrame.
6910 
6911         Parameters
6912         ----------
6913         dim_order : Sequence of Hashable or None , optional
6914             Hierarchical dimension order for the resulting dataframe.
6915             Array content is transposed to this order and then written out as flat
6916             vectors in contiguous order, so the last dimension in this list
6917             will be contiguous in the resulting DataFrame. This has a major influence
6918             on which operations are efficient on the resulting dask dataframe.
6919         set_index : bool, default: False
6920             If set_index=True, the dask DataFrame is indexed by this dataset's
6921             coordinate. Since dask DataFrames do not support multi-indexes,
6922             set_index only works if the dataset only contains one dimension.
6923 
6924         Returns
6925         -------
6926         dask.dataframe.DataFrame
6927 
6928         Examples
6929         --------
6930         >>> da = xr.DataArray(
6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),
6932         ...     dims=("time", "lat", "lon"),
6933         ...     coords={
6934         ...         "time": np.arange(4),
6935         ...         "lat": [-30, -20],
6936         ...         "lon": [120, 130],
6937         ...     },
6938         ...     name="eg_dataarray",
6939         ...     attrs={"units": "Celsius", "description": "Random temperature data"},
6940         ... )
6941         >>> da.to_dask_dataframe(["lat", "lon", "time"]).compute()
6942             lat  lon  time  eg_dataarray
6943         0   -30  120     0             0
6944         1   -30  120     1             4
6945         2   -30  120     2             8
6946         3   -30  120     3            12
6947         4   -30  130     0             1
6948         5   -30  130     1             5
6949         6   -30  130     2             9
6950         7   -30  130     3            13
6951         8   -20  120     0             2
6952         9   -20  120     1             6
6953         10  -20  120     2            10
6954         11  -20  120     3            14
6955         12  -20  130     0             3
6956         13  -20  130     1             7
6957         14  -20  130     2            11
6958         15  -20  130     3            15
6959         """
6960         if self.name is None:
6961             raise ValueError(
6962                 "Cannot convert an unnamed DataArray to a "
6963                 "dask dataframe : use the ``.rename`` method to assign a name."
6964             )
6965         name = self.name
6966         ds = self._to_dataset_whole(name, shallow_copy=False)
6967         return ds.to_dask_dataframe(dim_order, set_index)
6968 
6969     # this needs to be at the end, or mypy will confuse with `str`
6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6971     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>compute</method>
<code>
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)

</code><intended_behavior>Similar to chunk, the compute method should be refactored to support computations through alternative backends. This requires abstracting away dask-specific call to a compute action that’s applicable to the selected backend, including a backend management system that handles the method calls to the appropriate library.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
229 class DataArray(
230     AbstractArray,
231     DataWithCoords,
232     DataArrayArithmetic,
233     DataArrayAggregations,
234 ):
235     """N-dimensional array with labeled coordinates and dimensions.
236 
237     DataArray provides a wrapper around numpy ndarrays that uses
238     labeled dimensions and coordinates to support metadata aware
239     operations. The API is similar to that for the pandas Series or
240     DataFrame, but DataArray objects can have any number of dimensions,
241     and their contents have fixed data types.
242 
243     Additional features over raw numpy arrays:
244 
245     - Apply operations over dimensions by name: ``x.sum('time')``.
246     - Select or assign values by integer location (like numpy):
247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
248       ``x.sel(time='2014-01-01')``.
249     - Mathematical operations (e.g., ``x - y``) vectorize across
250       multiple dimensions (known in numpy as "broadcasting") based on
251       dimension names, regardless of their original order.
252     - Keep track of arbitrary metadata in the form of a Python
253       dictionary: ``x.attrs``
254     - Convert to a pandas Series: ``x.to_series()``.
255 
256     Getting items from or doing mathematical operations with a
257     DataArray always returns another DataArray.
258 
259     Parameters
260     ----------
261     data : array_like
262         Values for this array. Must be an ``numpy.ndarray``, ndarray
263         like, or castable to an ``ndarray``. If a self-described xarray
264         or pandas object, attempts are made to use this array's
265         metadata to fill in other unspecified arguments. A view of the
266         array's data is used instead of a copy if possible.
267     coords : sequence or dict of array_like, optional
268         Coordinates (tick labels) to use for indexing along each
269         dimension. The following notations are accepted:
270 
271         - mapping {dimension name: array-like}
272         - sequence of tuples that are valid arguments for
273           ``xarray.Variable()``
274           - (dims, data)
275           - (dims, data, attrs)
276           - (dims, data, attrs, encoding)
277 
278         Additionally, it is possible to define a coord whose name
279         does not match the dimension name, or a coord based on multiple
280         dimensions, with one of the following notations:
281 
282         - mapping {coord name: DataArray}
283         - mapping {coord name: Variable}
284         - mapping {coord name: (dimension name, array-like)}
285         - mapping {coord name: (tuple of dimension names, array-like)}
286 
287     dims : Hashable or sequence of Hashable, optional
288         Name(s) of the data dimension(s). Must be either a Hashable
289         (only for 1D data) or a sequence of Hashables with length equal
290         to the number of dimensions. If this argument is omitted,
291         dimension names are taken from ``coords`` (if possible) and
292         otherwise default to ``['dim_0', ... 'dim_n']``.
293     name : str or None, optional
294         Name of this array.
295     attrs : dict_like or None, optional
296         Attributes to assign to the new instance. By default, an empty
297         attribute dictionary is initialized.
298 
299     Examples
300     --------
301     Create data:
302 
303     >>> np.random.seed(0)
304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
307     >>> time = pd.date_range("2014-09-06", periods=3)
308     >>> reference_time = pd.Timestamp("2014-09-05")
309 
310     Initialize a dataarray with multiple dimensions:
311 
312     >>> da = xr.DataArray(
313     ...     data=temperature,
314     ...     dims=["x", "y", "time"],
315     ...     coords=dict(
316     ...         lon=(["x", "y"], lon),
317     ...         lat=(["x", "y"], lat),
318     ...         time=time,
319     ...         reference_time=reference_time,
320     ...     ),
321     ...     attrs=dict(
322     ...         description="Ambient temperature.",
323     ...         units="degC",
324     ...     ),
325     ... )
326     >>> da
327     <xarray.DataArray (x: 2, y: 2, time: 3)>
328     array([[[29.11241877, 18.20125767, 22.82990387],
329             [32.92714559, 29.94046392,  7.18177696]],
330     <BLANKLINE>
331            [[22.60070734, 13.78914233, 14.17424919],
332             [18.28478802, 16.15234857, 26.63418806]]])
333     Coordinates:
334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
335         lat             (x, y) float64 42.25 42.21 42.63 42.59
336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
337         reference_time  datetime64[ns] 2014-09-05
338     Dimensions without coordinates: x, y
339     Attributes:
340         description:  Ambient temperature.
341         units:        degC
342 
343     Find out where the coldest temperature was:
344 
345     >>> da.isel(da.argmin(...))
346     <xarray.DataArray ()>
347     array(7.18177696)
348     Coordinates:
349         lon             float64 -99.32
350         lat             float64 42.21
351         time            datetime64[ns] 2014-09-08
352         reference_time  datetime64[ns] 2014-09-05
353     Attributes:
354         description:  Ambient temperature.
355         units:        degC
356     """
357 
358     _cache: dict[str, Any]
359     _coords: dict[Any, Variable]
360     _close: Callable[[], None] | None
361     _indexes: dict[Hashable, Index]
362     _name: Hashable | None
363     _variable: Variable
364 
365     __slots__ = (
366         "_cache",
367         "_coords",
368         "_close",
369         "_indexes",
370         "_name",
371         "_variable",
372         "__weakref__",
373     )
374 
375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
376 
377     def __init__(
378         self,
379         data: Any = dtypes.NA,
380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
381         | Mapping[Any, Any]
382         | None = None,
383         dims: Hashable | Sequence[Hashable] | None = None,
384         name: Hashable | None = None,
385         attrs: Mapping | None = None,
386         # internal parameters
387         indexes: dict[Hashable, Index] | None = None,
388         fastpath: bool = False,
389     ) -> None:
390         if fastpath:
391             variable = data
392             assert dims is None
393             assert attrs is None
394             assert indexes is not None
395         else:
396             # TODO: (benbovy - explicit indexes) remove
397             # once it becomes part of the public interface
398             if indexes is not None:
399                 raise ValueError("Providing explicit indexes is not supported yet")
400 
401             # try to fill in arguments from data if they weren't supplied
402             if coords is None:
403                 if isinstance(data, DataArray):
404                     coords = data.coords
405                 elif isinstance(data, pd.Series):
406                     coords = [data.index]
407                 elif isinstance(data, pd.DataFrame):
408                     coords = [data.index, data.columns]
409                 elif isinstance(data, (pd.Index, IndexVariable)):
410                     coords = [data]
411 
412             if dims is None:
413                 dims = getattr(data, "dims", getattr(coords, "dims", None))
414             if name is None:
415                 name = getattr(data, "name", None)
416             if attrs is None and not isinstance(data, PANDAS_TYPES):
417                 attrs = getattr(data, "attrs", None)
418 
419             data = _check_data_shape(data, coords, dims)
420             data = as_compatible_data(data)
421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
422             variable = Variable(dims, data, attrs, fastpath=True)
423             indexes, coords = _create_indexes_from_coords(coords)
424 
425         # These fully describe a DataArray
426         self._variable = variable
427         assert isinstance(coords, dict)
428         self._coords = coords
429         self._name = name
430 
431         # TODO(shoyer): document this argument, once it becomes part of the
432         # public interface.
433         self._indexes = indexes
434 
435         self._close = None
436 
437     @classmethod
438     def _construct_direct(
439         cls: type[T_DataArray],
440         variable: Variable,
441         coords: dict[Any, Variable],
442         name: Hashable,
443         indexes: dict[Hashable, Index],
444     ) -> T_DataArray:
445         """Shortcut around __init__ for internal use when we want to skip
446         costly validation
447         """
448         obj = object.__new__(cls)
449         obj._variable = variable
450         obj._coords = coords
451         obj._name = name
452         obj._indexes = indexes
453         obj._close = None
454         return obj
455 
456     def _replace(
457         self: T_DataArray,
458         variable: Variable | None = None,
459         coords=None,
460         name: Hashable | None | Default = _default,
461         indexes=None,
462     ) -> T_DataArray:
463         if variable is None:
464             variable = self.variable
465         if coords is None:
466             coords = self._coords
467         if indexes is None:
468             indexes = self._indexes
469         if name is _default:
470             name = self.name
471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
472 
473     def _replace_maybe_drop_dims(
474         self: T_DataArray,
475         variable: Variable,
476         name: Hashable | None | Default = _default,
477     ) -> T_DataArray:
478         if variable.dims == self.dims and variable.shape == self.shape:
479             coords = self._coords.copy()
480             indexes = self._indexes
481         elif variable.dims == self.dims:
482             # Shape has changed (e.g. from reduce(..., keepdims=True)
483             new_sizes = dict(zip(self.dims, variable.shape))
484             coords = {
485                 k: v
486                 for k, v in self._coords.items()
487                 if v.shape == tuple(new_sizes[d] for d in v.dims)
488             }
489             indexes = filter_indexes_from_coords(self._indexes, set(coords))
490         else:
491             allowed_dims = set(variable.dims)
492             coords = {
493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
494             }
495             indexes = filter_indexes_from_coords(self._indexes, set(coords))
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _overwrite_indexes(
499         self: T_DataArray,
500         indexes: Mapping[Any, Index],
501         coords: Mapping[Any, Variable] | None = None,
502         drop_coords: list[Hashable] | None = None,
503         rename_dims: Mapping[Any, Any] | None = None,
504     ) -> T_DataArray:
505         """Maybe replace indexes and their corresponding coordinates."""
506         if not indexes:
507             return self
508 
509         if coords is None:
510             coords = {}
511         if drop_coords is None:
512             drop_coords = []
513 
514         new_variable = self.variable.copy()
515         new_coords = self._coords.copy()
516         new_indexes = dict(self._indexes)
517 
518         for name in indexes:
519             new_coords[name] = coords[name]
520             new_indexes[name] = indexes[name]
521 
522         for name in drop_coords:
523             new_coords.pop(name)
524             new_indexes.pop(name)
525 
526         if rename_dims:
527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
528 
529         return self._replace(
530             variable=new_variable, coords=new_coords, indexes=new_indexes
531         )
532 
533     def _to_temp_dataset(self) -> Dataset:
534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
535 
536     def _from_temp_dataset(
537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
538     ) -> T_DataArray:
539         variable = dataset._variables.pop(_THIS_ARRAY)
540         coords = dataset._variables
541         indexes = dataset._indexes
542         return self._replace(variable, coords, name, indexes=indexes)
543 
544     def _to_dataset_split(self, dim: Hashable) -> Dataset:
545         """splits dataarray along dimension 'dim'"""
546 
547         def subset(dim, label):
548             array = self.loc[{dim: label}]
549             array.attrs = {}
550             return as_variable(array)
551 
552         variables = {label: subset(dim, label) for label in self.get_index(dim)}
553         variables.update({k: v for k, v in self._coords.items() if k != dim})
554         coord_names = set(self._coords) - {dim}
555         indexes = filter_indexes_from_coords(self._indexes, coord_names)
556         dataset = Dataset._construct_direct(
557             variables, coord_names, indexes=indexes, attrs=self.attrs
558         )
559         return dataset
560 
561     def _to_dataset_whole(
562         self, name: Hashable = None, shallow_copy: bool = True
563     ) -> Dataset:
564         if name is None:
565             name = self.name
566         if name is None:
567             raise ValueError(
568                 "unable to convert unnamed DataArray to a "
569                 "Dataset without providing an explicit name"
570             )
571         if name in self.coords:
572             raise ValueError(
573                 "cannot create a Dataset from a DataArray with "
574                 "the same name as one of its coordinates"
575             )
576         # use private APIs for speed: this is called by _to_temp_dataset(),
577         # which is used in the guts of a lot of operations (e.g., reindex)
578         variables = self._coords.copy()
579         variables[name] = self.variable
580         if shallow_copy:
581             for k in variables:
582                 variables[k] = variables[k].copy(deep=False)
583         indexes = self._indexes
584 
585         coord_names = set(self._coords)
586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
587 
588     def to_dataset(
589         self,
590         dim: Hashable = None,
591         *,
592         name: Hashable = None,
593         promote_attrs: bool = False,
594     ) -> Dataset:
595         """Convert a DataArray to a Dataset.
596 
597         Parameters
598         ----------
599         dim : Hashable, optional
600             Name of the dimension on this array along which to split this array
601             into separate variables. If not provided, this array is converted
602             into a Dataset of one variable.
603         name : Hashable, optional
604             Name to substitute for this array's name. Only valid if ``dim`` is
605             not provided.
606         promote_attrs : bool, default: False
607             Set to True to shallow copy attrs of DataArray to returned Dataset.
608 
609         Returns
610         -------
611         dataset : Dataset
612         """
613         if dim is not None and dim not in self.dims:
614             raise TypeError(
615                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
616             )
617 
618         if dim is not None:
619             if name is not None:
620                 raise TypeError("cannot supply both dim and name arguments")
621             result = self._to_dataset_split(dim)
622         else:
623             result = self._to_dataset_whole(name)
624 
625         if promote_attrs:
626             result.attrs = dict(self.attrs)
627 
628         return result
629 
630     @property
631     def name(self) -> Hashable | None:
632         """The name of this array."""
633         return self._name
634 
635     @name.setter
636     def name(self, value: Hashable | None) -> None:
637         self._name = value
638 
639     @property
640     def variable(self) -> Variable:
641         """Low level interface to the Variable object for this DataArray."""
642         return self._variable
643 
644     @property
645     def dtype(self) -> np.dtype:
646         """
647         Data-type of the array’s elements.
648 
649         See Also
650         --------
651         ndarray.dtype
652         numpy.dtype
653         """
654         return self.variable.dtype
655 
656     @property
657     def shape(self) -> tuple[int, ...]:
658         """
659         Tuple of array dimensions.
660 
661         See Also
662         --------
663         numpy.ndarray.shape
664         """
665         return self.variable.shape
666 
667     @property
668     def size(self) -> int:
669         """
670         Number of elements in the array.
671 
672         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
673 
674         See Also
675         --------
676         numpy.ndarray.size
677         """
678         return self.variable.size
679 
680     @property
681     def nbytes(self) -> int:
682         """
683         Total bytes consumed by the elements of this DataArray's data.
684 
685         If the underlying data array does not include ``nbytes``, estimates
686         the bytes consumed based on the ``size`` and ``dtype``.
687         """
688         return self.variable.nbytes
689 
690     @property
691     def ndim(self) -> int:
692         """
693         Number of array dimensions.
694 
695         See Also
696         --------
697         numpy.ndarray.ndim
698         """
699         return self.variable.ndim
700 
701     def __len__(self) -> int:
702         return len(self.variable)
703 
704     @property
705     def data(self) -> Any:
706         """
707         The DataArray's data as an array. The underlying array type
708         (e.g. dask, sparse, pint) is preserved.
709 
710         See Also
711         --------
712         DataArray.to_numpy
713         DataArray.as_numpy
714         DataArray.values
715         """
716         return self.variable.data
717 
718     @data.setter
719     def data(self, value: Any) -> None:
720         self.variable.data = value
721 
722     @property
723     def values(self) -> np.ndarray:
724         """
725         The array's data as a numpy.ndarray.
726 
727         If the array's data is not a numpy.ndarray this will attempt to convert
728         it naively using np.array(), which will raise an error if the array
729         type does not support coercion like this (e.g. cupy).
730         """
731         return self.variable.values
732 
733     @values.setter
734     def values(self, value: Any) -> None:
735         self.variable.values = value
736 
737     def to_numpy(self) -> np.ndarray:
738         """
739         Coerces wrapped data to numpy and returns a numpy.ndarray.
740 
741         See Also
742         --------
743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
744         Dataset.as_numpy
745         DataArray.values
746         DataArray.data
747         """
748         return self.variable.to_numpy()
749 
750     def as_numpy(self: T_DataArray) -> T_DataArray:
751         """
752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
753 
754         See Also
755         --------
756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
757         Dataset.as_numpy : Converts all variables in a Dataset.
758         DataArray.values
759         DataArray.data
760         """
761         coords = {k: v.as_numpy() for k, v in self._coords.items()}
762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
763 
764     @property
765     def _in_memory(self) -> bool:
766         return self.variable._in_memory
767 
768     def _to_index(self) -> pd.Index:
769         return self.variable._to_index()
770 
771     def to_index(self) -> pd.Index:
772         """Convert this variable to a pandas.Index. Only possible for 1D
773         arrays.
774         """
775         return self.variable.to_index()
776 
777     @property
778     def dims(self) -> tuple[Hashable, ...]:
779         """Tuple of dimension names associated with this array.
780 
781         Note that the type of this property is inconsistent with
782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
783         consistently named properties.
784 
785         See Also
786         --------
787         DataArray.sizes
788         Dataset.dims
789         """
790         return self.variable.dims
791 
792     @dims.setter
793     def dims(self, value: Any) -> NoReturn:
794         raise AttributeError(
795             "you cannot assign dims on a DataArray. Use "
796             ".rename() or .swap_dims() instead."
797         )
798 
799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
800         if utils.is_dict_like(key):
801             return key
802         key = indexing.expanded_indexer(key, self.ndim)
803         return dict(zip(self.dims, key))
804 
805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
806         from xarray.core.dataset import _get_virtual_variable
807 
808         try:
809             var = self._coords[key]
810         except KeyError:
811             dim_sizes = dict(zip(self.dims, self.shape))
812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
813 
814         return self._replace_maybe_drop_dims(var, name=key)
815 
816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
817         if isinstance(key, str):
818             return self._getitem_coord(key)
819         else:
820             # xarray-style array indexing
821             return self.isel(indexers=self._item_key_to_dict(key))
822 
823     def __setitem__(self, key: Any, value: Any) -> None:
824         if isinstance(key, str):
825             self.coords[key] = value
826         else:
827             # Coordinates in key, value and self[key] should be consistent.
828             # TODO Coordinate consistency in key is checked here, but it
829             # causes unnecessary indexing. It should be optimized.
830             obj = self[key]
831             if isinstance(value, DataArray):
832                 assert_coordinate_consistent(value, obj.coords.variables)
833             # DataArray key -> Variable key
834             key = {
835                 k: v.variable if isinstance(v, DataArray) else v
836                 for k, v in self._item_key_to_dict(key).items()
837             }
838             self.variable[key] = value
839 
840     def __delitem__(self, key: Any) -> None:
841         del self.coords[key]
842 
843     @property
844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
845         """Places to look-up items for attribute-style access"""
846         yield from self._item_sources
847         yield self.attrs
848 
849     @property
850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
851         """Places to look-up items for key-completion"""
852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
853 
854         # virtual coordinates
855         # uses empty dict -- everything here can already be found in self.coords.
856         yield HybridMappingProxy(keys=self.dims, mapping={})
857 
858     def __contains__(self, key: Any) -> bool:
859         return key in self.data
860 
861     @property
862     def loc(self) -> _LocIndexer:
863         """Attribute for location based indexing like pandas."""
864         return _LocIndexer(self)
865 
866     @property
867     def attrs(self) -> dict[Any, Any]:
868         """Dictionary storing arbitrary metadata with this array."""
869         return self.variable.attrs
870 
871     @attrs.setter
872     def attrs(self, value: Mapping[Any, Any]) -> None:
873         self.variable.attrs = dict(value)
874 
875     @property
876     def encoding(self) -> dict[Any, Any]:
877         """Dictionary of format-specific settings for how this array should be
878         serialized."""
879         return self.variable.encoding
880 
881     @encoding.setter
882     def encoding(self, value: Mapping[Any, Any]) -> None:
883         self.variable.encoding = dict(value)
884 
885     def reset_encoding(self: T_DataArray) -> T_DataArray:
886         """Return a new DataArray without encoding on the array or any attached
887         coords."""
888         ds = self._to_temp_dataset().reset_encoding()
889         return self._from_temp_dataset(ds)
890 
891     @property
892     def indexes(self) -> Indexes:
893         """Mapping of pandas.Index objects used for label based indexing.
894 
895         Raises an error if this Dataset has indexes that cannot be coerced
896         to pandas.Index objects.
897 
898         See Also
899         --------
900         DataArray.xindexes
901 
902         """
903         return self.xindexes.to_pandas_indexes()
904 
905     @property
906     def xindexes(self) -> Indexes:
907         """Mapping of xarray Index objects used for label based indexing."""
908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
909 
910     @property
911     def coords(self) -> DataArrayCoordinates:
912         """Dictionary-like container of coordinate arrays."""
913         return DataArrayCoordinates(self)
914 
915     @overload
916     def reset_coords(
917         self: T_DataArray,
918         names: Dims = None,
919         drop: Literal[False] = False,
920     ) -> Dataset:
921         ...
922 
923     @overload
924     def reset_coords(
925         self: T_DataArray,
926         names: Dims = None,
927         *,
928         drop: Literal[True],
929     ) -> T_DataArray:
930         ...
931 
932     def reset_coords(
933         self: T_DataArray,
934         names: Dims = None,
935         drop: bool = False,
936     ) -> T_DataArray | Dataset:
937         """Given names of coordinates, reset them to become variables.
938 
939         Parameters
940         ----------
941         names : str, Iterable of Hashable or None, optional
942             Name(s) of non-index coordinates in this dataset to reset into
943             variables. By default, all non-index coordinates are reset.
944         drop : bool, default: False
945             If True, remove coordinates instead of converting them into
946             variables.
947 
948         Returns
949         -------
950         Dataset, or DataArray if ``drop == True``
951 
952         Examples
953         --------
954         >>> temperature = np.arange(25).reshape(5, 5)
955         >>> pressure = np.arange(50, 75).reshape(5, 5)
956         >>> da = xr.DataArray(
957         ...     data=temperature,
958         ...     dims=["x", "y"],
959         ...     coords=dict(
960         ...         lon=("x", np.arange(10, 15)),
961         ...         lat=("y", np.arange(20, 25)),
962         ...         Pressure=(["x", "y"], pressure),
963         ...     ),
964         ...     name="Temperature",
965         ... )
966         >>> da
967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
968         array([[ 0,  1,  2,  3,  4],
969                [ 5,  6,  7,  8,  9],
970                [10, 11, 12, 13, 14],
971                [15, 16, 17, 18, 19],
972                [20, 21, 22, 23, 24]])
973         Coordinates:
974             lon       (x) int64 10 11 12 13 14
975             lat       (y) int64 20 21 22 23 24
976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
977         Dimensions without coordinates: x, y
978 
979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
980 
981         >>> da.reset_coords(names="Pressure")
982         <xarray.Dataset>
983         Dimensions:      (x: 5, y: 5)
984         Coordinates:
985             lon          (x) int64 10 11 12 13 14
986             lat          (y) int64 20 21 22 23 24
987         Dimensions without coordinates: x, y
988         Data variables:
989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
991 
992         Return DataArray without targeted coordinate:
993 
994         >>> da.reset_coords(names="Pressure", drop=True)
995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
996         array([[ 0,  1,  2,  3,  4],
997                [ 5,  6,  7,  8,  9],
998                [10, 11, 12, 13, 14],
999                [15, 16, 17, 18, 19],
1000                [20, 21, 22, 23, 24]])
1001         Coordinates:
1002             lon      (x) int64 10 11 12 13 14
1003             lat      (y) int64 20 21 22 23 24
1004         Dimensions without coordinates: x, y
1005         """
1006         if names is None:
1007             names = set(self.coords) - set(self._indexes)
1008         dataset = self.coords.to_dataset().reset_coords(names, drop)
1009         if drop:
1010             return self._replace(coords=dataset._variables)
1011         if self.name is None:
1012             raise ValueError(
1013                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1014             )
1015         dataset[self.name] = self.variable
1016         return dataset
1017 
1018     def __dask_tokenize__(self):
1019         from dask.base import normalize_token
1020 
1021         return normalize_token((type(self), self._variable, self._coords, self._name))
1022 
1023     def __dask_graph__(self):
1024         return self._to_temp_dataset().__dask_graph__()
1025 
1026     def __dask_keys__(self):
1027         return self._to_temp_dataset().__dask_keys__()
1028 
1029     def __dask_layers__(self):
1030         return self._to_temp_dataset().__dask_layers__()
1031 
1032     @property
1033     def __dask_optimize__(self):
1034         return self._to_temp_dataset().__dask_optimize__
1035 
1036     @property
1037     def __dask_scheduler__(self):
1038         return self._to_temp_dataset().__dask_scheduler__
1039 
1040     def __dask_postcompute__(self):
1041         func, args = self._to_temp_dataset().__dask_postcompute__()
1042         return self._dask_finalize, (self.name, func) + args
1043 
1044     def __dask_postpersist__(self):
1045         func, args = self._to_temp_dataset().__dask_postpersist__()
1046         return self._dask_finalize, (self.name, func) + args
1047 
1048     @staticmethod
1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1050         ds = func(results, *args, **kwargs)
1051         variable = ds._variables.pop(_THIS_ARRAY)
1052         coords = ds._variables
1053         indexes = ds._indexes
1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1055 
1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1057         """Manually trigger loading of this array's data from disk or a
1058         remote source into memory and return this array.
1059 
1060         Normally, it should not be necessary to call this method in user code,
1061         because all xarray functions should either work on deferred data or
1062         load data automatically. However, this method can be necessary when
1063         working with many file objects on disk.
1064 
1065         Parameters
1066         ----------
1067         **kwargs : dict
1068             Additional keyword arguments passed on to ``dask.compute``.
1069 
1070         See Also
1071         --------
1072         dask.compute
1073         """
1074         ds = self._to_temp_dataset().load(**kwargs)
1075         new = self._from_temp_dataset(ds)
1076         self._variable = new._variable
1077         self._coords = new._coords
1078         return self
1079 
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)
1101 
1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1103         """Trigger computation in constituent dask arrays
1104 
1105         This keeps them as dask arrays but encourages them to keep data in
1106         memory.  This is particularly useful when on a distributed machine.
1107         When on a single machine consider using ``.compute()`` instead.
1108 
1109         Parameters
1110         ----------
1111         **kwargs : dict
1112             Additional keyword arguments passed on to ``dask.persist``.
1113 
1114         See Also
1115         --------
1116         dask.persist
1117         """
1118         ds = self._to_temp_dataset().persist(**kwargs)
1119         return self._from_temp_dataset(ds)
1120 
1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1122         """Returns a copy of this array.
1123 
1124         If `deep=True`, a deep copy is made of the data array.
1125         Otherwise, a shallow copy is made, and the returned data array's
1126         values are a new view of this data array's values.
1127 
1128         Use `data` to create a new object with the same structure as
1129         original but entirely new data.
1130 
1131         Parameters
1132         ----------
1133         deep : bool, optional
1134             Whether the data array and its coordinates are loaded into memory
1135             and copied onto the new object. Default is True.
1136         data : array_like, optional
1137             Data to use in the new object. Must have same shape as original.
1138             When `data` is used, `deep` is ignored for all data variables,
1139             and only used for coords.
1140 
1141         Returns
1142         -------
1143         copy : DataArray
1144             New object with dimensions, attributes, coordinates, name,
1145             encoding, and optionally data copied from original.
1146 
1147         Examples
1148         --------
1149         Shallow versus deep copy
1150 
1151         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1152         >>> array.copy()
1153         <xarray.DataArray (x: 3)>
1154         array([1, 2, 3])
1155         Coordinates:
1156           * x        (x) <U1 'a' 'b' 'c'
1157         >>> array_0 = array.copy(deep=False)
1158         >>> array_0[0] = 7
1159         >>> array_0
1160         <xarray.DataArray (x: 3)>
1161         array([7, 2, 3])
1162         Coordinates:
1163           * x        (x) <U1 'a' 'b' 'c'
1164         >>> array
1165         <xarray.DataArray (x: 3)>
1166         array([7, 2, 3])
1167         Coordinates:
1168           * x        (x) <U1 'a' 'b' 'c'
1169 
1170         Changing the data using the ``data`` argument maintains the
1171         structure of the original object, but with the new data. Original
1172         object is unaffected.
1173 
1174         >>> array.copy(data=[0.1, 0.2, 0.3])
1175         <xarray.DataArray (x: 3)>
1176         array([0.1, 0.2, 0.3])
1177         Coordinates:
1178           * x        (x) <U1 'a' 'b' 'c'
1179         >>> array
1180         <xarray.DataArray (x: 3)>
1181         array([7, 2, 3])
1182         Coordinates:
1183           * x        (x) <U1 'a' 'b' 'c'
1184 
1185         See Also
1186         --------
1187         pandas.DataFrame.copy
1188         """
1189         return self._copy(deep=deep, data=data)
1190 
1191     def _copy(
1192         self: T_DataArray,
1193         deep: bool = True,
1194         data: Any = None,
1195         memo: dict[int, Any] | None = None,
1196     ) -> T_DataArray:
1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1199 
1200         coords = {}
1201         for k, v in self._coords.items():
1202             if k in index_vars:
1203                 coords[k] = index_vars[k]
1204             else:
1205                 coords[k] = v._copy(deep=deep, memo=memo)
1206 
1207         return self._replace(variable, coords, indexes=indexes)
1208 
1209     def __copy__(self: T_DataArray) -> T_DataArray:
1210         return self._copy(deep=False)
1211 
1212     def __deepcopy__(
1213         self: T_DataArray, memo: dict[int, Any] | None = None
1214     ) -> T_DataArray:
1215         return self._copy(deep=True, memo=memo)
1216 
1217     # mutable objects should not be Hashable
1218     # https://github.com/python/mypy/issues/4266
1219     __hash__ = None  # type: ignore[assignment]
1220 
1221     @property
1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1223         """
1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1225         the underlying data is not a dask array.
1226 
1227         See Also
1228         --------
1229         DataArray.chunk
1230         DataArray.chunksizes
1231         xarray.unify_chunks
1232         """
1233         return self.variable.chunks
1234 
1235     @property
1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1237         """
1238         Mapping from dimension names to block lengths for this dataarray's data, or None if
1239         the underlying data is not a dask array.
1240         Cannot be modified directly, but can be modified by calling .chunk().
1241 
1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1243         instead of a tuple of chunk shapes.
1244 
1245         See Also
1246         --------
1247         DataArray.chunk
1248         DataArray.chunks
1249         xarray.unify_chunks
1250         """
1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1252         return get_chunksizes(all_variables)
1253 
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)
1333 
1334     def isel(
1335         self: T_DataArray,
1336         indexers: Mapping[Any, Any] | None = None,
1337         drop: bool = False,
1338         missing_dims: ErrorOptionsWithWarn = "raise",
1339         **indexers_kwargs: Any,
1340     ) -> T_DataArray:
1341         """Return a new DataArray whose data is given by selecting indexes
1342         along the specified dimension(s).
1343 
1344         Parameters
1345         ----------
1346         indexers : dict, optional
1347             A dict with keys matching dimensions and values given
1348             by integers, slice objects or arrays.
1349             indexer can be a integer, slice, array-like or DataArray.
1350             If DataArrays are passed as indexers, xarray-style indexing will be
1351             carried out. See :ref:`indexing` for the details.
1352             One of indexers or indexers_kwargs must be provided.
1353         drop : bool, default: False
1354             If ``drop=True``, drop coordinates variables indexed by integers
1355             instead of making them scalar.
1356         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1357             What to do if dimensions that should be selected from are not present in the
1358             DataArray:
1359             - "raise": raise an exception
1360             - "warn": raise a warning, and ignore the missing dimensions
1361             - "ignore": ignore the missing dimensions
1362         **indexers_kwargs : {dim: indexer, ...}, optional
1363             The keyword arguments form of ``indexers``.
1364 
1365         Returns
1366         -------
1367         indexed : xarray.DataArray
1368 
1369         See Also
1370         --------
1371         Dataset.isel
1372         DataArray.sel
1373 
1374         Examples
1375         --------
1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1377         >>> da
1378         <xarray.DataArray (x: 5, y: 5)>
1379         array([[ 0,  1,  2,  3,  4],
1380                [ 5,  6,  7,  8,  9],
1381                [10, 11, 12, 13, 14],
1382                [15, 16, 17, 18, 19],
1383                [20, 21, 22, 23, 24]])
1384         Dimensions without coordinates: x, y
1385 
1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> da = da.isel(x=tgt_x, y=tgt_y)
1389         >>> da
1390         <xarray.DataArray (points: 5)>
1391         array([ 0,  6, 12, 18, 24])
1392         Dimensions without coordinates: points
1393         """
1394 
1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1396 
1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1398             ds = self._to_temp_dataset()._isel_fancy(
1399                 indexers, drop=drop, missing_dims=missing_dims
1400             )
1401             return self._from_temp_dataset(ds)
1402 
1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1404         # lists, or zero or one-dimensional np.ndarray's
1405 
1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1408 
1409         coords = {}
1410         for coord_name, coord_value in self._coords.items():
1411             if coord_name in index_variables:
1412                 coord_value = index_variables[coord_name]
1413             else:
1414                 coord_indexers = {
1415                     k: v for k, v in indexers.items() if k in coord_value.dims
1416                 }
1417                 if coord_indexers:
1418                     coord_value = coord_value.isel(coord_indexers)
1419                     if drop and coord_value.ndim == 0:
1420                         continue
1421             coords[coord_name] = coord_value
1422 
1423         return self._replace(variable=variable, coords=coords, indexes=indexes)
1424 
1425     def sel(
1426         self: T_DataArray,
1427         indexers: Mapping[Any, Any] | None = None,
1428         method: str | None = None,
1429         tolerance=None,
1430         drop: bool = False,
1431         **indexers_kwargs: Any,
1432     ) -> T_DataArray:
1433         """Return a new DataArray whose data is given by selecting index
1434         labels along the specified dimension(s).
1435 
1436         In contrast to `DataArray.isel`, indexers for this method should use
1437         labels instead of integers.
1438 
1439         Under the hood, this method is powered by using pandas's powerful Index
1440         objects. This makes label based indexing essentially just as fast as
1441         using integer indexing.
1442 
1443         It also means this method uses pandas's (well documented) logic for
1444         indexing. This means you can use string shortcuts for datetime indexes
1445         (e.g., '2000-01' to select all values in January 2000). It also means
1446         that slices are treated as inclusive of both the start and stop values,
1447         unlike normal Python indexing.
1448 
1449         .. warning::
1450 
1451           Do not try to assign values when using any of the indexing methods
1452           ``isel`` or ``sel``::
1453 
1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1455             # DO NOT do this
1456             da.isel(x=[0, 1, 2])[1] = -1
1457 
1458           Assigning values with the chained indexing using ``.sel`` or
1459           ``.isel`` fails silently.
1460 
1461         Parameters
1462         ----------
1463         indexers : dict, optional
1464             A dict with keys matching dimensions and values given
1465             by scalars, slices or arrays of tick labels. For dimensions with
1466             multi-index, the indexer may also be a dict-like object with keys
1467             matching index level names.
1468             If DataArrays are passed as indexers, xarray-style indexing will be
1469             carried out. See :ref:`indexing` for the details.
1470             One of indexers or indexers_kwargs must be provided.
1471         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1472             Method to use for inexact matches:
1473 
1474             - None (default): only exact matches
1475             - pad / ffill: propagate last valid index value forward
1476             - backfill / bfill: propagate next valid index value backward
1477             - nearest: use nearest valid index value
1478 
1479         tolerance : optional
1480             Maximum distance between original and new labels for inexact
1481             matches. The values of the index at the matching locations must
1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1483         drop : bool, optional
1484             If ``drop=True``, drop coordinates variables in `indexers` instead
1485             of making them scalar.
1486         **indexers_kwargs : {dim: indexer, ...}, optional
1487             The keyword arguments form of ``indexers``.
1488             One of indexers or indexers_kwargs must be provided.
1489 
1490         Returns
1491         -------
1492         obj : DataArray
1493             A new DataArray with the same contents as this DataArray, except the
1494             data and each dimension is indexed by the appropriate indexers.
1495             If indexer DataArrays have coordinates that do not conflict with
1496             this object, then these coordinates will be attached.
1497             In general, each array's data will be a view of the array's data
1498             in this DataArray, unless vectorized indexing was triggered by using
1499             an array indexer, in which case the data will be a copy.
1500 
1501         See Also
1502         --------
1503         Dataset.sel
1504         DataArray.isel
1505 
1506         Examples
1507         --------
1508         >>> da = xr.DataArray(
1509         ...     np.arange(25).reshape(5, 5),
1510         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1511         ...     dims=("x", "y"),
1512         ... )
1513         >>> da
1514         <xarray.DataArray (x: 5, y: 5)>
1515         array([[ 0,  1,  2,  3,  4],
1516                [ 5,  6,  7,  8,  9],
1517                [10, 11, 12, 13, 14],
1518                [15, 16, 17, 18, 19],
1519                [20, 21, 22, 23, 24]])
1520         Coordinates:
1521           * x        (x) int64 0 1 2 3 4
1522           * y        (y) int64 0 1 2 3 4
1523 
1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1527         >>> da
1528         <xarray.DataArray (points: 5)>
1529         array([ 0,  6, 12, 18, 24])
1530         Coordinates:
1531             x        (points) int64 0 1 2 3 4
1532             y        (points) int64 0 1 2 3 4
1533         Dimensions without coordinates: points
1534         """
1535         ds = self._to_temp_dataset().sel(
1536             indexers=indexers,
1537             drop=drop,
1538             method=method,
1539             tolerance=tolerance,
1540             **indexers_kwargs,
1541         )
1542         return self._from_temp_dataset(ds)
1543 
1544     def head(
1545         self: T_DataArray,
1546         indexers: Mapping[Any, int] | int | None = None,
1547         **indexers_kwargs: Any,
1548     ) -> T_DataArray:
1549         """Return a new DataArray whose data is given by the the first `n`
1550         values along the specified dimension(s). Default `n` = 5
1551 
1552         See Also
1553         --------
1554         Dataset.head
1555         DataArray.tail
1556         DataArray.thin
1557 
1558         Examples
1559         --------
1560         >>> da = xr.DataArray(
1561         ...     np.arange(25).reshape(5, 5),
1562         ...     dims=("x", "y"),
1563         ... )
1564         >>> da
1565         <xarray.DataArray (x: 5, y: 5)>
1566         array([[ 0,  1,  2,  3,  4],
1567                [ 5,  6,  7,  8,  9],
1568                [10, 11, 12, 13, 14],
1569                [15, 16, 17, 18, 19],
1570                [20, 21, 22, 23, 24]])
1571         Dimensions without coordinates: x, y
1572 
1573         >>> da.head(x=1)
1574         <xarray.DataArray (x: 1, y: 5)>
1575         array([[0, 1, 2, 3, 4]])
1576         Dimensions without coordinates: x, y
1577 
1578         >>> da.head({"x": 2, "y": 2})
1579         <xarray.DataArray (x: 2, y: 2)>
1580         array([[0, 1],
1581                [5, 6]])
1582         Dimensions without coordinates: x, y
1583         """
1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1585         return self._from_temp_dataset(ds)
1586 
1587     def tail(
1588         self: T_DataArray,
1589         indexers: Mapping[Any, int] | int | None = None,
1590         **indexers_kwargs: Any,
1591     ) -> T_DataArray:
1592         """Return a new DataArray whose data is given by the the last `n`
1593         values along the specified dimension(s). Default `n` = 5
1594 
1595         See Also
1596         --------
1597         Dataset.tail
1598         DataArray.head
1599         DataArray.thin
1600 
1601         Examples
1602         --------
1603         >>> da = xr.DataArray(
1604         ...     np.arange(25).reshape(5, 5),
1605         ...     dims=("x", "y"),
1606         ... )
1607         >>> da
1608         <xarray.DataArray (x: 5, y: 5)>
1609         array([[ 0,  1,  2,  3,  4],
1610                [ 5,  6,  7,  8,  9],
1611                [10, 11, 12, 13, 14],
1612                [15, 16, 17, 18, 19],
1613                [20, 21, 22, 23, 24]])
1614         Dimensions without coordinates: x, y
1615 
1616         >>> da.tail(y=1)
1617         <xarray.DataArray (x: 5, y: 1)>
1618         array([[ 4],
1619                [ 9],
1620                [14],
1621                [19],
1622                [24]])
1623         Dimensions without coordinates: x, y
1624 
1625         >>> da.tail({"x": 2, "y": 2})
1626         <xarray.DataArray (x: 2, y: 2)>
1627         array([[18, 19],
1628                [23, 24]])
1629         Dimensions without coordinates: x, y
1630         """
1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1632         return self._from_temp_dataset(ds)
1633 
1634     def thin(
1635         self: T_DataArray,
1636         indexers: Mapping[Any, int] | int | None = None,
1637         **indexers_kwargs: Any,
1638     ) -> T_DataArray:
1639         """Return a new DataArray whose data is given by each `n` value
1640         along the specified dimension(s).
1641 
1642         Examples
1643         --------
1644         >>> x_arr = np.arange(0, 26)
1645         >>> x_arr
1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1647                17, 18, 19, 20, 21, 22, 23, 24, 25])
1648         >>> x = xr.DataArray(
1649         ...     np.reshape(x_arr, (2, 13)),
1650         ...     dims=("x", "y"),
1651         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1652         ... )
1653         >>> x
1654         <xarray.DataArray (x: 2, y: 13)>
1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1657         Coordinates:
1658           * x        (x) int64 0 1
1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1660 
1661         >>>
1662         >>> x.thin(3)
1663         <xarray.DataArray (x: 1, y: 5)>
1664         array([[ 0,  3,  6,  9, 12]])
1665         Coordinates:
1666           * x        (x) int64 0
1667           * y        (y) int64 0 3 6 9 12
1668         >>> x.thin({"x": 2, "y": 5})
1669         <xarray.DataArray (x: 1, y: 3)>
1670         array([[ 0,  5, 10]])
1671         Coordinates:
1672           * x        (x) int64 0
1673           * y        (y) int64 0 5 10
1674 
1675         See Also
1676         --------
1677         Dataset.thin
1678         DataArray.head
1679         DataArray.tail
1680         """
1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1682         return self._from_temp_dataset(ds)
1683 
1684     def broadcast_like(
1685         self: T_DataArray,
1686         other: DataArray | Dataset,
1687         exclude: Iterable[Hashable] | None = None,
1688     ) -> T_DataArray:
1689         """Broadcast this DataArray against another Dataset or DataArray.
1690 
1691         This is equivalent to xr.broadcast(other, self)[1]
1692 
1693         xarray objects are broadcast against each other in arithmetic
1694         operations, so this method is not be necessary for most uses.
1695 
1696         If no change is needed, the input data is returned to the output
1697         without being copied.
1698 
1699         If new coords are added by the broadcast, their values are
1700         NaN filled.
1701 
1702         Parameters
1703         ----------
1704         other : Dataset or DataArray
1705             Object against which to broadcast this array.
1706         exclude : iterable of Hashable, optional
1707             Dimensions that must not be broadcasted
1708 
1709         Returns
1710         -------
1711         new_da : DataArray
1712             The caller broadcasted against ``other``.
1713 
1714         Examples
1715         --------
1716         >>> arr1 = xr.DataArray(
1717         ...     np.random.randn(2, 3),
1718         ...     dims=("x", "y"),
1719         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1720         ... )
1721         >>> arr2 = xr.DataArray(
1722         ...     np.random.randn(3, 2),
1723         ...     dims=("x", "y"),
1724         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1725         ... )
1726         >>> arr1
1727         <xarray.DataArray (x: 2, y: 3)>
1728         array([[ 1.76405235,  0.40015721,  0.97873798],
1729                [ 2.2408932 ,  1.86755799, -0.97727788]])
1730         Coordinates:
1731           * x        (x) <U1 'a' 'b'
1732           * y        (y) <U1 'a' 'b' 'c'
1733         >>> arr2
1734         <xarray.DataArray (x: 3, y: 2)>
1735         array([[ 0.95008842, -0.15135721],
1736                [-0.10321885,  0.4105985 ],
1737                [ 0.14404357,  1.45427351]])
1738         Coordinates:
1739           * x        (x) <U1 'a' 'b' 'c'
1740           * y        (y) <U1 'a' 'b'
1741         >>> arr1.broadcast_like(arr2)
1742         <xarray.DataArray (x: 3, y: 3)>
1743         array([[ 1.76405235,  0.40015721,  0.97873798],
1744                [ 2.2408932 ,  1.86755799, -0.97727788],
1745                [        nan,         nan,         nan]])
1746         Coordinates:
1747           * x        (x) <U1 'a' 'b' 'c'
1748           * y        (y) <U1 'a' 'b' 'c'
1749         """
1750         if exclude is None:
1751             exclude = set()
1752         else:
1753             exclude = set(exclude)
1754         args = align(other, self, join="outer", copy=False, exclude=exclude)
1755 
1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1757 
1758         return _broadcast_helper(
1759             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1760         )
1761 
1762     def _reindex_callback(
1763         self: T_DataArray,
1764         aligner: alignment.Aligner,
1765         dim_pos_indexers: dict[Hashable, Any],
1766         variables: dict[Hashable, Variable],
1767         indexes: dict[Hashable, Index],
1768         fill_value: Any,
1769         exclude_dims: frozenset[Hashable],
1770         exclude_vars: frozenset[Hashable],
1771     ) -> T_DataArray:
1772         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1773 
1774         if isinstance(fill_value, dict):
1775             fill_value = fill_value.copy()
1776             sentinel = object()
1777             value = fill_value.pop(self.name, sentinel)
1778             if value is not sentinel:
1779                 fill_value[_THIS_ARRAY] = value
1780 
1781         ds = self._to_temp_dataset()
1782         reindexed = ds._reindex_callback(
1783             aligner,
1784             dim_pos_indexers,
1785             variables,
1786             indexes,
1787             fill_value,
1788             exclude_dims,
1789             exclude_vars,
1790         )
1791         return self._from_temp_dataset(reindexed)
1792 
1793     def reindex_like(
1794         self: T_DataArray,
1795         other: DataArray | Dataset,
1796         method: ReindexMethodOptions = None,
1797         tolerance: int | float | Iterable[int | float] | None = None,
1798         copy: bool = True,
1799         fill_value=dtypes.NA,
1800     ) -> T_DataArray:
1801         """Conform this object onto the indexes of another object, filling in
1802         missing values with ``fill_value``. The default fill value is NaN.
1803 
1804         Parameters
1805         ----------
1806         other : Dataset or DataArray
1807             Object with an 'indexes' attribute giving a mapping from dimension
1808             names to pandas.Index objects, which provides coordinates upon
1809             which to index the variables in this dataset. The indexes on this
1810             other object need not be the same as the indexes on this
1811             dataset. Any mis-matched index values will be filled in with
1812             NaN, and any mis-matched dimension names will simply be ignored.
1813         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1814             Method to use for filling index values from other not found on this
1815             data array:
1816 
1817             - None (default): don't fill gaps
1818             - pad / ffill: propagate last valid index value forward
1819             - backfill / bfill: propagate next valid index value backward
1820             - nearest: use nearest valid index value
1821 
1822         tolerance : optional
1823             Maximum distance between original and new labels for inexact
1824             matches. The values of the index at the matching locations must
1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1826             Tolerance may be a scalar value, which applies the same tolerance
1827             to all values, or list-like, which applies variable tolerance per
1828             element. List-like must be the same size as the index and its dtype
1829             must exactly match the index’s type.
1830         copy : bool, default: True
1831             If ``copy=True``, data in the return value is always copied. If
1832             ``copy=False`` and reindexing is unnecessary, or can be performed
1833             with only slice operations, then the output may share memory with
1834             the input. In either case, a new xarray object is always returned.
1835         fill_value : scalar or dict-like, optional
1836             Value to use for newly missing values. If a dict-like, maps
1837             variable names (including coordinates) to fill values. Use this
1838             data array's name to refer to the data array's values.
1839 
1840         Returns
1841         -------
1842         reindexed : DataArray
1843             Another dataset array, with this array's data but coordinates from
1844             the other object.
1845 
1846         Examples
1847         --------
1848         >>> data = np.arange(12).reshape(4, 3)
1849         >>> da1 = xr.DataArray(
1850         ...     data=data,
1851         ...     dims=["x", "y"],
1852         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1853         ... )
1854         >>> da1
1855         <xarray.DataArray (x: 4, y: 3)>
1856         array([[ 0,  1,  2],
1857                [ 3,  4,  5],
1858                [ 6,  7,  8],
1859                [ 9, 10, 11]])
1860         Coordinates:
1861           * x        (x) int64 10 20 30 40
1862           * y        (y) int64 70 80 90
1863         >>> da2 = xr.DataArray(
1864         ...     data=data,
1865         ...     dims=["x", "y"],
1866         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1867         ... )
1868         >>> da2
1869         <xarray.DataArray (x: 4, y: 3)>
1870         array([[ 0,  1,  2],
1871                [ 3,  4,  5],
1872                [ 6,  7,  8],
1873                [ 9, 10, 11]])
1874         Coordinates:
1875           * x        (x) int64 40 30 20 10
1876           * y        (y) int64 90 80 70
1877 
1878         Reindexing with both DataArrays having the same coordinates set, but in different order:
1879 
1880         >>> da1.reindex_like(da2)
1881         <xarray.DataArray (x: 4, y: 3)>
1882         array([[11, 10,  9],
1883                [ 8,  7,  6],
1884                [ 5,  4,  3],
1885                [ 2,  1,  0]])
1886         Coordinates:
1887           * x        (x) int64 40 30 20 10
1888           * y        (y) int64 90 80 70
1889 
1890         Reindexing with the other array having coordinates which the source array doesn't have:
1891 
1892         >>> data = np.arange(12).reshape(4, 3)
1893         >>> da1 = xr.DataArray(
1894         ...     data=data,
1895         ...     dims=["x", "y"],
1896         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1897         ... )
1898         >>> da2 = xr.DataArray(
1899         ...     data=data,
1900         ...     dims=["x", "y"],
1901         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1902         ... )
1903         >>> da1.reindex_like(da2)
1904         <xarray.DataArray (x: 4, y: 3)>
1905         array([[ 3.,  4.,  5.],
1906                [ 0.,  1.,  2.],
1907                [nan, nan, nan],
1908                [nan, nan, nan]])
1909         Coordinates:
1910           * x        (x) int64 20 10 29 39
1911           * y        (y) int64 70 80 90
1912 
1913         Filling missing values with the previous valid index with respect to the coordinates' value:
1914 
1915         >>> da1.reindex_like(da2, method="ffill")
1916         <xarray.DataArray (x: 4, y: 3)>
1917         array([[3, 4, 5],
1918                [0, 1, 2],
1919                [3, 4, 5],
1920                [6, 7, 8]])
1921         Coordinates:
1922           * x        (x) int64 20 10 29 39
1923           * y        (y) int64 70 80 90
1924 
1925         Filling missing values while tolerating specified error for inexact matches:
1926 
1927         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1928         <xarray.DataArray (x: 4, y: 3)>
1929         array([[ 3.,  4.,  5.],
1930                [ 0.,  1.,  2.],
1931                [nan, nan, nan],
1932                [nan, nan, nan]])
1933         Coordinates:
1934           * x        (x) int64 20 10 29 39
1935           * y        (y) int64 70 80 90
1936 
1937         Filling missing values with manually specified values:
1938 
1939         >>> da1.reindex_like(da2, fill_value=19)
1940         <xarray.DataArray (x: 4, y: 3)>
1941         array([[ 3,  4,  5],
1942                [ 0,  1,  2],
1943                [19, 19, 19],
1944                [19, 19, 19]])
1945         Coordinates:
1946           * x        (x) int64 20 10 29 39
1947           * y        (y) int64 70 80 90
1948 
1949         See Also
1950         --------
1951         DataArray.reindex
1952         align
1953         """
1954         return alignment.reindex_like(
1955             self,
1956             other=other,
1957             method=method,
1958             tolerance=tolerance,
1959             copy=copy,
1960             fill_value=fill_value,
1961         )
1962 
1963     def reindex(
1964         self: T_DataArray,
1965         indexers: Mapping[Any, Any] | None = None,
1966         method: ReindexMethodOptions = None,
1967         tolerance: float | Iterable[float] | None = None,
1968         copy: bool = True,
1969         fill_value=dtypes.NA,
1970         **indexers_kwargs: Any,
1971     ) -> T_DataArray:
1972         """Conform this object onto the indexes of another object, filling in
1973         missing values with ``fill_value``. The default fill value is NaN.
1974 
1975         Parameters
1976         ----------
1977         indexers : dict, optional
1978             Dictionary with keys given by dimension names and values given by
1979             arrays of coordinates tick labels. Any mis-matched coordinate
1980             values will be filled in with NaN, and any mis-matched dimension
1981             names will simply be ignored.
1982             One of indexers or indexers_kwargs must be provided.
1983         copy : bool, optional
1984             If ``copy=True``, data in the return value is always copied. If
1985             ``copy=False`` and reindexing is unnecessary, or can be performed
1986             with only slice operations, then the output may share memory with
1987             the input. In either case, a new xarray object is always returned.
1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1989             Method to use for filling index values in ``indexers`` not found on
1990             this data array:
1991 
1992             - None (default): don't fill gaps
1993             - pad / ffill: propagate last valid index value forward
1994             - backfill / bfill: propagate next valid index value backward
1995             - nearest: use nearest valid index value
1996 
1997         tolerance : float | Iterable[float] | None, default: None
1998             Maximum distance between original and new labels for inexact
1999             matches. The values of the index at the matching locations must
2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2001             Tolerance may be a scalar value, which applies the same tolerance
2002             to all values, or list-like, which applies variable tolerance per
2003             element. List-like must be the same size as the index and its dtype
2004             must exactly match the index’s type.
2005         fill_value : scalar or dict-like, optional
2006             Value to use for newly missing values. If a dict-like, maps
2007             variable names (including coordinates) to fill values. Use this
2008             data array's name to refer to the data array's values.
2009         **indexers_kwargs : {dim: indexer, ...}, optional
2010             The keyword arguments form of ``indexers``.
2011             One of indexers or indexers_kwargs must be provided.
2012 
2013         Returns
2014         -------
2015         reindexed : DataArray
2016             Another dataset array, with this array's data but replaced
2017             coordinates.
2018 
2019         Examples
2020         --------
2021         Reverse latitude:
2022 
2023         >>> da = xr.DataArray(
2024         ...     np.arange(4),
2025         ...     coords=[np.array([90, 89, 88, 87])],
2026         ...     dims="lat",
2027         ... )
2028         >>> da
2029         <xarray.DataArray (lat: 4)>
2030         array([0, 1, 2, 3])
2031         Coordinates:
2032           * lat      (lat) int64 90 89 88 87
2033         >>> da.reindex(lat=da.lat[::-1])
2034         <xarray.DataArray (lat: 4)>
2035         array([3, 2, 1, 0])
2036         Coordinates:
2037           * lat      (lat) int64 87 88 89 90
2038 
2039         See Also
2040         --------
2041         DataArray.reindex_like
2042         align
2043         """
2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2045         return alignment.reindex(
2046             self,
2047             indexers=indexers,
2048             method=method,
2049             tolerance=tolerance,
2050             copy=copy,
2051             fill_value=fill_value,
2052         )
2053 
2054     def interp(
2055         self: T_DataArray,
2056         coords: Mapping[Any, Any] | None = None,
2057         method: InterpOptions = "linear",
2058         assume_sorted: bool = False,
2059         kwargs: Mapping[str, Any] | None = None,
2060         **coords_kwargs: Any,
2061     ) -> T_DataArray:
2062         """Interpolate a DataArray onto new coordinates
2063 
2064         Performs univariate or multivariate interpolation of a DataArray onto
2065         new coordinates using scipy's interpolation routines. If interpolating
2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2067         called. When interpolating along multiple existing dimensions, an
2068         attempt is made to decompose the interpolation into multiple
2069         1-dimensional interpolations. If this is possible,
2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2071         :py:func:`scipy.interpolate.interpn` is called.
2072 
2073         Parameters
2074         ----------
2075         coords : dict, optional
2076             Mapping from dimension names to the new coordinates.
2077             New coordinate can be a scalar, array-like or DataArray.
2078             If DataArrays are passed as new coordinates, their dimensions are
2079             used for the broadcasting. Missing values are skipped.
2080         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2081             The method used to interpolate. The method should be supported by
2082             the scipy interpolator:
2083 
2084             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2085               "quadratic", "cubic", "polynomial"}
2086             - ``interpn``: {"linear", "nearest"}
2087 
2088             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2089             also be provided.
2090         assume_sorted : bool, default: False
2091             If False, values of x can be in any order and they are sorted
2092             first. If True, x has to be an array of monotonically increasing
2093             values.
2094         kwargs : dict-like or None, default: None
2095             Additional keyword arguments passed to scipy's interpolator. Valid
2096             options and their behavior depend whether ``interp1d`` or
2097             ``interpn`` is used.
2098         **coords_kwargs : {dim: coordinate, ...}, optional
2099             The keyword arguments form of ``coords``.
2100             One of coords or coords_kwargs must be provided.
2101 
2102         Returns
2103         -------
2104         interpolated : DataArray
2105             New dataarray on the new coordinates.
2106 
2107         Notes
2108         -----
2109         scipy is required.
2110 
2111         See Also
2112         --------
2113         scipy.interpolate.interp1d
2114         scipy.interpolate.interpn
2115 
2116         Examples
2117         --------
2118         >>> da = xr.DataArray(
2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2120         ...     dims=("x", "y"),
2121         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2122         ... )
2123         >>> da
2124         <xarray.DataArray (x: 3, y: 4)>
2125         array([[ 1.,  4.,  2.,  9.],
2126                [ 2.,  7.,  6., nan],
2127                [ 6., nan,  5.,  8.]])
2128         Coordinates:
2129           * x        (x) int64 0 1 2
2130           * y        (y) int64 10 12 14 16
2131 
2132         1D linear interpolation (the default):
2133 
2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2135         <xarray.DataArray (x: 4, y: 4)>
2136         array([[1.  , 4.  , 2.  ,  nan],
2137                [1.75, 6.25, 5.  ,  nan],
2138                [3.  ,  nan, 5.75,  nan],
2139                [5.  ,  nan, 5.25,  nan]])
2140         Coordinates:
2141           * y        (y) int64 10 12 14 16
2142           * x        (x) float64 0.0 0.75 1.25 1.75
2143 
2144         1D nearest interpolation:
2145 
2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2147         <xarray.DataArray (x: 4, y: 4)>
2148         array([[ 1.,  4.,  2.,  9.],
2149                [ 2.,  7.,  6., nan],
2150                [ 2.,  7.,  6., nan],
2151                [ 6., nan,  5.,  8.]])
2152         Coordinates:
2153           * y        (y) int64 10 12 14 16
2154           * x        (x) float64 0.0 0.75 1.25 1.75
2155 
2156         1D linear extrapolation:
2157 
2158         >>> da.interp(
2159         ...     x=[1, 1.5, 2.5, 3.5],
2160         ...     method="linear",
2161         ...     kwargs={"fill_value": "extrapolate"},
2162         ... )
2163         <xarray.DataArray (x: 4, y: 4)>
2164         array([[ 2. ,  7. ,  6. ,  nan],
2165                [ 4. ,  nan,  5.5,  nan],
2166                [ 8. ,  nan,  4.5,  nan],
2167                [12. ,  nan,  3.5,  nan]])
2168         Coordinates:
2169           * y        (y) int64 10 12 14 16
2170           * x        (x) float64 1.0 1.5 2.5 3.5
2171 
2172         2D linear interpolation:
2173 
2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2175         <xarray.DataArray (x: 4, y: 3)>
2176         array([[2.5  , 3.   ,   nan],
2177                [4.   , 5.625,   nan],
2178                [  nan,   nan,   nan],
2179                [  nan,   nan,   nan]])
2180         Coordinates:
2181           * x        (x) float64 0.0 0.75 1.25 1.75
2182           * y        (y) int64 11 13 15
2183         """
2184         if self.dtype.kind not in "uifc":
2185             raise TypeError(
2186                 "interp only works for a numeric type array. "
2187                 "Given {}.".format(self.dtype)
2188             )
2189         ds = self._to_temp_dataset().interp(
2190             coords,
2191             method=method,
2192             kwargs=kwargs,
2193             assume_sorted=assume_sorted,
2194             **coords_kwargs,
2195         )
2196         return self._from_temp_dataset(ds)
2197 
2198     def interp_like(
2199         self: T_DataArray,
2200         other: DataArray | Dataset,
2201         method: InterpOptions = "linear",
2202         assume_sorted: bool = False,
2203         kwargs: Mapping[str, Any] | None = None,
2204     ) -> T_DataArray:
2205         """Interpolate this object onto the coordinates of another object,
2206         filling out of range values with NaN.
2207 
2208         If interpolating along a single existing dimension,
2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2210         along multiple existing dimensions, an attempt is made to decompose the
2211         interpolation into multiple 1-dimensional interpolations. If this is
2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2213         :py:func:`scipy.interpolate.interpn` is called.
2214 
2215         Parameters
2216         ----------
2217         other : Dataset or DataArray
2218             Object with an 'indexes' attribute giving a mapping from dimension
2219             names to an 1d array-like, which provides coordinates upon
2220             which to index the variables in this dataset. Missing values are skipped.
2221         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2222             The method used to interpolate. The method should be supported by
2223             the scipy interpolator:
2224 
2225             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2226               "polynomial"} when ``interp1d`` is called.
2227             - {"linear", "nearest"} when ``interpn`` is called.
2228 
2229             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2230             also be provided.
2231         assume_sorted : bool, default: False
2232             If False, values of coordinates that are interpolated over can be
2233             in any order and they are sorted first. If True, interpolated
2234             coordinates are assumed to be an array of monotonically increasing
2235             values.
2236         kwargs : dict, optional
2237             Additional keyword passed to scipy's interpolator.
2238 
2239         Returns
2240         -------
2241         interpolated : DataArray
2242             Another dataarray by interpolating this dataarray's data along the
2243             coordinates of the other object.
2244 
2245         Examples
2246         --------
2247         >>> data = np.arange(12).reshape(4, 3)
2248         >>> da1 = xr.DataArray(
2249         ...     data=data,
2250         ...     dims=["x", "y"],
2251         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2252         ... )
2253         >>> da1
2254         <xarray.DataArray (x: 4, y: 3)>
2255         array([[ 0,  1,  2],
2256                [ 3,  4,  5],
2257                [ 6,  7,  8],
2258                [ 9, 10, 11]])
2259         Coordinates:
2260           * x        (x) int64 10 20 30 40
2261           * y        (y) int64 70 80 90
2262         >>> da2 = xr.DataArray(
2263         ...     data=data,
2264         ...     dims=["x", "y"],
2265         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2266         ... )
2267         >>> da2
2268         <xarray.DataArray (x: 4, y: 3)>
2269         array([[ 0,  1,  2],
2270                [ 3,  4,  5],
2271                [ 6,  7,  8],
2272                [ 9, 10, 11]])
2273         Coordinates:
2274           * x        (x) int64 10 20 29 39
2275           * y        (y) int64 70 80 90
2276 
2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2278 
2279         >>> da2.interp_like(da1)
2280         <xarray.DataArray (x: 4, y: 3)>
2281         array([[0. , 1. , 2. ],
2282                [3. , 4. , 5. ],
2283                [6.3, 7.3, 8.3],
2284                [nan, nan, nan]])
2285         Coordinates:
2286           * x        (x) int64 10 20 30 40
2287           * y        (y) int64 70 80 90
2288 
2289         Could also extrapolate missing values:
2290 
2291         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2292         <xarray.DataArray (x: 4, y: 3)>
2293         array([[ 0. ,  1. ,  2. ],
2294                [ 3. ,  4. ,  5. ],
2295                [ 6.3,  7.3,  8.3],
2296                [ 9.3, 10.3, 11.3]])
2297         Coordinates:
2298           * x        (x) int64 10 20 30 40
2299           * y        (y) int64 70 80 90
2300 
2301         Notes
2302         -----
2303         scipy is required.
2304         If the dataarray has object-type coordinates, reindex is used for these
2305         coordinates instead of the interpolation.
2306 
2307         See Also
2308         --------
2309         DataArray.interp
2310         DataArray.reindex_like
2311         """
2312         if self.dtype.kind not in "uifc":
2313             raise TypeError(
2314                 "interp only works for a numeric type array. "
2315                 "Given {}.".format(self.dtype)
2316             )
2317         ds = self._to_temp_dataset().interp_like(
2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2319         )
2320         return self._from_temp_dataset(ds)
2321 
2322     # change type of self and return to T_DataArray once
2323     # https://github.com/python/mypy/issues/12846 is resolved
2324     def rename(
2325         self,
2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2327         **names: Hashable,
2328     ) -> DataArray:
2329         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2330 
2331         Parameters
2332         ----------
2333         new_name_or_name_dict : str or dict-like, optional
2334             If the argument is dict-like, it used as a mapping from old
2335             names to new names for coordinates or dimensions. Otherwise,
2336             use the argument as the new name for this array.
2337         **names : Hashable, optional
2338             The keyword arguments form of a mapping from old names to
2339             new names for coordinates or dimensions.
2340             One of new_name_or_name_dict or names must be provided.
2341 
2342         Returns
2343         -------
2344         renamed : DataArray
2345             Renamed array or array with renamed coordinates.
2346 
2347         See Also
2348         --------
2349         Dataset.rename
2350         DataArray.swap_dims
2351         """
2352         if new_name_or_name_dict is None and not names:
2353             # change name to None?
2354             return self._replace(name=None)
2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2356             # change dims/coords
2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2358             dataset = self._to_temp_dataset()._rename(name_dict)
2359             return self._from_temp_dataset(dataset)
2360         if utils.hashable(new_name_or_name_dict) and names:
2361             # change name + dims/coords
2362             dataset = self._to_temp_dataset()._rename(names)
2363             dataarray = self._from_temp_dataset(dataset)
2364             return dataarray._replace(name=new_name_or_name_dict)
2365         # only change name
2366         return self._replace(name=new_name_or_name_dict)
2367 
2368     def swap_dims(
2369         self: T_DataArray,
2370         dims_dict: Mapping[Any, Hashable] | None = None,
2371         **dims_kwargs,
2372     ) -> T_DataArray:
2373         """Returns a new DataArray with swapped dimensions.
2374 
2375         Parameters
2376         ----------
2377         dims_dict : dict-like
2378             Dictionary whose keys are current dimension names and whose values
2379             are new names.
2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2381             The keyword arguments form of ``dims_dict``.
2382             One of dims_dict or dims_kwargs must be provided.
2383 
2384         Returns
2385         -------
2386         swapped : DataArray
2387             DataArray with swapped dimensions.
2388 
2389         Examples
2390         --------
2391         >>> arr = xr.DataArray(
2392         ...     data=[0, 1],
2393         ...     dims="x",
2394         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2395         ... )
2396         >>> arr
2397         <xarray.DataArray (x: 2)>
2398         array([0, 1])
2399         Coordinates:
2400           * x        (x) <U1 'a' 'b'
2401             y        (x) int64 0 1
2402 
2403         >>> arr.swap_dims({"x": "y"})
2404         <xarray.DataArray (y: 2)>
2405         array([0, 1])
2406         Coordinates:
2407             x        (y) <U1 'a' 'b'
2408           * y        (y) int64 0 1
2409 
2410         >>> arr.swap_dims({"x": "z"})
2411         <xarray.DataArray (z: 2)>
2412         array([0, 1])
2413         Coordinates:
2414             x        (z) <U1 'a' 'b'
2415             y        (z) int64 0 1
2416         Dimensions without coordinates: z
2417 
2418         See Also
2419         --------
2420         DataArray.rename
2421         Dataset.swap_dims
2422         """
2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2424         ds = self._to_temp_dataset().swap_dims(dims_dict)
2425         return self._from_temp_dataset(ds)
2426 
2427     # change type of self and return to T_DataArray once
2428     # https://github.com/python/mypy/issues/12846 is resolved
2429     def expand_dims(
2430         self,
2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2432         axis: None | int | Sequence[int] = None,
2433         **dim_kwargs: Any,
2434     ) -> DataArray:
2435         """Return a new object with an additional axis (or axes) inserted at
2436         the corresponding position in the array shape. The new object is a
2437         view into the underlying array, not a copy.
2438 
2439         If dim is already a scalar coordinate, it will be promoted to a 1D
2440         coordinate consisting of a single value.
2441 
2442         Parameters
2443         ----------
2444         dim : Hashable, sequence of Hashable, dict, or None, optional
2445             Dimensions to include on the new variable.
2446             If provided as str or sequence of str, then dimensions are inserted
2447             with length 1. If provided as a dict, then the keys are the new
2448             dimensions and the values are either integers (giving the length of
2449             the new dimensions) or sequence/ndarray (giving the coordinates of
2450             the new dimensions).
2451         axis : int, sequence of int, or None, default: None
2452             Axis position(s) where new axis is to be inserted (position(s) on
2453             the result array). If a sequence of integers is passed,
2454             multiple axes are inserted. In this case, dim arguments should be
2455             same length list. If axis=None is passed, all the axes will be
2456             inserted to the start of the result array.
2457         **dim_kwargs : int or sequence or ndarray
2458             The keywords are arbitrary dimensions being inserted and the values
2459             are either the lengths of the new dims (if int is given), or their
2460             coordinates. Note, this is an alternative to passing a dict to the
2461             dim kwarg and will only be used if dim is None.
2462 
2463         Returns
2464         -------
2465         expanded : DataArray
2466             This object, but with additional dimension(s).
2467 
2468         See Also
2469         --------
2470         Dataset.expand_dims
2471 
2472         Examples
2473         --------
2474         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2475         >>> da
2476         <xarray.DataArray (x: 5)>
2477         array([0, 1, 2, 3, 4])
2478         Dimensions without coordinates: x
2479 
2480         Add new dimension of length 2:
2481 
2482         >>> da.expand_dims(dim={"y": 2})
2483         <xarray.DataArray (y: 2, x: 5)>
2484         array([[0, 1, 2, 3, 4],
2485                [0, 1, 2, 3, 4]])
2486         Dimensions without coordinates: y, x
2487 
2488         >>> da.expand_dims(dim={"y": 2}, axis=1)
2489         <xarray.DataArray (x: 5, y: 2)>
2490         array([[0, 0],
2491                [1, 1],
2492                [2, 2],
2493                [3, 3],
2494                [4, 4]])
2495         Dimensions without coordinates: x, y
2496 
2497         Add a new dimension with coordinates from array:
2498 
2499         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2500         <xarray.DataArray (y: 5, x: 5)>
2501         array([[0, 1, 2, 3, 4],
2502                [0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4]])
2506         Coordinates:
2507           * y        (y) int64 0 1 2 3 4
2508         Dimensions without coordinates: x
2509         """
2510         if isinstance(dim, int):
2511             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2513             if len(dim) != len(set(dim)):
2514                 raise ValueError("dims should not contain duplicate values.")
2515             dim = dict.fromkeys(dim, 1)
2516         elif dim is not None and not isinstance(dim, Mapping):
2517             dim = {cast(Hashable, dim): 1}
2518 
2519         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2520         ds = self._to_temp_dataset().expand_dims(dim, axis)
2521         return self._from_temp_dataset(ds)
2522 
2523     # change type of self and return to T_DataArray once
2524     # https://github.com/python/mypy/issues/12846 is resolved
2525     def set_index(
2526         self,
2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2528         append: bool = False,
2529         **indexes_kwargs: Hashable | Sequence[Hashable],
2530     ) -> DataArray:
2531         """Set DataArray (multi-)indexes using one or more existing
2532         coordinates.
2533 
2534         This legacy method is limited to pandas (multi-)indexes and
2535         1-dimensional "dimension" coordinates. See
2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2537         Xarray-compatible index from one or more arbitrary coordinates.
2538 
2539         Parameters
2540         ----------
2541         indexes : {dim: index, ...}
2542             Mapping from names matching dimensions and values given
2543             by (lists of) the names of existing coordinates or variables to set
2544             as new (multi-)index.
2545         append : bool, default: False
2546             If True, append the supplied index(es) to the existing index(es).
2547             Otherwise replace the existing index(es).
2548         **indexes_kwargs : optional
2549             The keyword arguments form of ``indexes``.
2550             One of indexes or indexes_kwargs must be provided.
2551 
2552         Returns
2553         -------
2554         obj : DataArray
2555             Another DataArray, with this data but replaced coordinates.
2556 
2557         Examples
2558         --------
2559         >>> arr = xr.DataArray(
2560         ...     data=np.ones((2, 3)),
2561         ...     dims=["x", "y"],
2562         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2563         ... )
2564         >>> arr
2565         <xarray.DataArray (x: 2, y: 3)>
2566         array([[1., 1., 1.],
2567                [1., 1., 1.]])
2568         Coordinates:
2569           * x        (x) int64 0 1
2570           * y        (y) int64 0 1 2
2571             a        (x) int64 3 4
2572         >>> arr.set_index(x="a")
2573         <xarray.DataArray (x: 2, y: 3)>
2574         array([[1., 1., 1.],
2575                [1., 1., 1.]])
2576         Coordinates:
2577           * x        (x) int64 3 4
2578           * y        (y) int64 0 1 2
2579 
2580         See Also
2581         --------
2582         DataArray.reset_index
2583         DataArray.set_xindex
2584         """
2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2586         return self._from_temp_dataset(ds)
2587 
2588     # change type of self and return to T_DataArray once
2589     # https://github.com/python/mypy/issues/12846 is resolved
2590     def reset_index(
2591         self,
2592         dims_or_levels: Hashable | Sequence[Hashable],
2593         drop: bool = False,
2594     ) -> DataArray:
2595         """Reset the specified index(es) or multi-index level(s).
2596 
2597         This legacy method is specific to pandas (multi-)indexes and
2598         1-dimensional "dimension" coordinates. See the more generic
2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2600         method to respectively drop and set pandas or custom indexes for
2601         arbitrary coordinates.
2602 
2603         Parameters
2604         ----------
2605         dims_or_levels : Hashable or sequence of Hashable
2606             Name(s) of the dimension(s) and/or multi-index level(s) that will
2607             be reset.
2608         drop : bool, default: False
2609             If True, remove the specified indexes and/or multi-index levels
2610             instead of extracting them as new coordinates (default: False).
2611 
2612         Returns
2613         -------
2614         obj : DataArray
2615             Another dataarray, with this dataarray's data but replaced
2616             coordinates.
2617 
2618         See Also
2619         --------
2620         DataArray.set_index
2621         DataArray.set_xindex
2622         DataArray.drop_indexes
2623         """
2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2625         return self._from_temp_dataset(ds)
2626 
2627     def set_xindex(
2628         self: T_DataArray,
2629         coord_names: str | Sequence[Hashable],
2630         index_cls: type[Index] | None = None,
2631         **options,
2632     ) -> T_DataArray:
2633         """Set a new, Xarray-compatible index from one or more existing
2634         coordinate(s).
2635 
2636         Parameters
2637         ----------
2638         coord_names : str or list
2639             Name(s) of the coordinate(s) used to build the index.
2640             If several names are given, their order matters.
2641         index_cls : subclass of :class:`~xarray.indexes.Index`
2642             The type of index to create. By default, try setting
2643             a pandas (multi-)index from the supplied coordinates.
2644         **options
2645             Options passed to the index constructor.
2646 
2647         Returns
2648         -------
2649         obj : DataArray
2650             Another dataarray, with this dataarray's data and with a new index.
2651 
2652         """
2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2654         return self._from_temp_dataset(ds)
2655 
2656     def reorder_levels(
2657         self: T_DataArray,
2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2659         **dim_order_kwargs: Sequence[int | Hashable],
2660     ) -> T_DataArray:
2661         """Rearrange index levels using input order.
2662 
2663         Parameters
2664         ----------
2665         dim_order dict-like of Hashable to int or Hashable: optional
2666             Mapping from names matching dimensions and values given
2667             by lists representing new level orders. Every given dimension
2668             must have a multi-index.
2669         **dim_order_kwargs : optional
2670             The keyword arguments form of ``dim_order``.
2671             One of dim_order or dim_order_kwargs must be provided.
2672 
2673         Returns
2674         -------
2675         obj : DataArray
2676             Another dataarray, with this dataarray's data but replaced
2677             coordinates.
2678         """
2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2680         return self._from_temp_dataset(ds)
2681 
2682     def stack(
2683         self: T_DataArray,
2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2685         create_index: bool | None = True,
2686         index_cls: type[Index] = PandasMultiIndex,
2687         **dimensions_kwargs: Sequence[Hashable],
2688     ) -> T_DataArray:
2689         """
2690         Stack any number of existing dimensions into a single new dimension.
2691 
2692         New dimensions will be added at the end, and the corresponding
2693         coordinate variables will be combined into a MultiIndex.
2694 
2695         Parameters
2696         ----------
2697         dimensions : mapping of Hashable to sequence of Hashable
2698             Mapping of the form `new_name=(dim1, dim2, ...)`.
2699             Names of new dimensions, and the existing dimensions that they
2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2702             all dimensions.
2703         create_index : bool or None, default: True
2704             If True, create a multi-index for each of the stacked dimensions.
2705             If False, don't create any index.
2706             If None, create a multi-index only if exactly one single (1-d) coordinate
2707             index is found for every dimension to stack.
2708         index_cls: class, optional
2709             Can be used to pass a custom multi-index type. Must be an Xarray index that
2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2711         **dimensions_kwargs
2712             The keyword arguments form of ``dimensions``.
2713             One of dimensions or dimensions_kwargs must be provided.
2714 
2715         Returns
2716         -------
2717         stacked : DataArray
2718             DataArray with stacked data.
2719 
2720         Examples
2721         --------
2722         >>> arr = xr.DataArray(
2723         ...     np.arange(6).reshape(2, 3),
2724         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2725         ... )
2726         >>> arr
2727         <xarray.DataArray (x: 2, y: 3)>
2728         array([[0, 1, 2],
2729                [3, 4, 5]])
2730         Coordinates:
2731           * x        (x) <U1 'a' 'b'
2732           * y        (y) int64 0 1 2
2733         >>> stacked = arr.stack(z=("x", "y"))
2734         >>> stacked.indexes["z"]
2735         MultiIndex([('a', 0),
2736                     ('a', 1),
2737                     ('a', 2),
2738                     ('b', 0),
2739                     ('b', 1),
2740                     ('b', 2)],
2741                    name='z')
2742 
2743         See Also
2744         --------
2745         DataArray.unstack
2746         """
2747         ds = self._to_temp_dataset().stack(
2748             dimensions,
2749             create_index=create_index,
2750             index_cls=index_cls,
2751             **dimensions_kwargs,
2752         )
2753         return self._from_temp_dataset(ds)
2754 
2755     # change type of self and return to T_DataArray once
2756     # https://github.com/python/mypy/issues/12846 is resolved
2757     def unstack(
2758         self,
2759         dim: Dims = None,
2760         fill_value: Any = dtypes.NA,
2761         sparse: bool = False,
2762     ) -> DataArray:
2763         """
2764         Unstack existing dimensions corresponding to MultiIndexes into
2765         multiple new dimensions.
2766 
2767         New dimensions will be added at the end.
2768 
2769         Parameters
2770         ----------
2771         dim : str, Iterable of Hashable or None, optional
2772             Dimension(s) over which to unstack. By default unstacks all
2773             MultiIndexes.
2774         fill_value : scalar or dict-like, default: nan
2775             Value to be filled. If a dict-like, maps variable names to
2776             fill values. Use the data array's name to refer to its
2777             name. If not provided or if the dict-like does not contain
2778             all variables, the dtype's NA value will be used.
2779         sparse : bool, default: False
2780             Use sparse-array if True
2781 
2782         Returns
2783         -------
2784         unstacked : DataArray
2785             Array with unstacked data.
2786 
2787         Examples
2788         --------
2789         >>> arr = xr.DataArray(
2790         ...     np.arange(6).reshape(2, 3),
2791         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2792         ... )
2793         >>> arr
2794         <xarray.DataArray (x: 2, y: 3)>
2795         array([[0, 1, 2],
2796                [3, 4, 5]])
2797         Coordinates:
2798           * x        (x) <U1 'a' 'b'
2799           * y        (y) int64 0 1 2
2800         >>> stacked = arr.stack(z=("x", "y"))
2801         >>> stacked.indexes["z"]
2802         MultiIndex([('a', 0),
2803                     ('a', 1),
2804                     ('a', 2),
2805                     ('b', 0),
2806                     ('b', 1),
2807                     ('b', 2)],
2808                    name='z')
2809         >>> roundtripped = stacked.unstack()
2810         >>> arr.identical(roundtripped)
2811         True
2812 
2813         See Also
2814         --------
2815         DataArray.stack
2816         """
2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2818         return self._from_temp_dataset(ds)
2819 
2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2821         """Unstack DataArray expanding to Dataset along a given level of a
2822         stacked coordinate.
2823 
2824         This is the inverse operation of Dataset.to_stacked_array.
2825 
2826         Parameters
2827         ----------
2828         dim : Hashable
2829             Name of existing dimension to unstack
2830         level : int or Hashable, default: 0
2831             The MultiIndex level to expand to a dataset along. Can either be
2832             the integer index of the level or its name.
2833 
2834         Returns
2835         -------
2836         unstacked: Dataset
2837 
2838         Examples
2839         --------
2840         >>> arr = xr.DataArray(
2841         ...     np.arange(6).reshape(2, 3),
2842         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2843         ... )
2844         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2845         >>> data
2846         <xarray.Dataset>
2847         Dimensions:  (x: 2, y: 3)
2848         Coordinates:
2849           * x        (x) <U1 'a' 'b'
2850           * y        (y) int64 0 1 2
2851         Data variables:
2852             a        (x, y) int64 0 1 2 3 4 5
2853             b        (x) int64 0 3
2854         >>> stacked = data.to_stacked_array("z", ["x"])
2855         >>> stacked.indexes["z"]
2856         MultiIndex([('a', 0.0),
2857                     ('a', 1.0),
2858                     ('a', 2.0),
2859                     ('b', nan)],
2860                    name='z')
2861         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2862         >>> data.identical(roundtripped)
2863         True
2864 
2865         See Also
2866         --------
2867         Dataset.to_stacked_array
2868         """
2869         idx = self._indexes[dim].to_pandas_index()
2870         if not isinstance(idx, pd.MultiIndex):
2871             raise ValueError(f"'{dim}' is not a stacked coordinate")
2872 
2873         level_number = idx._get_level_number(level)
2874         variables = idx.levels[level_number]
2875         variable_dim = idx.names[level_number]
2876 
2877         # pull variables out of datarray
2878         data_dict = {}
2879         for k in variables:
2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2881 
2882         # unstacked dataset
2883         return Dataset(data_dict)
2884 
2885     def transpose(
2886         self: T_DataArray,
2887         *dims: Hashable,
2888         transpose_coords: bool = True,
2889         missing_dims: ErrorOptionsWithWarn = "raise",
2890     ) -> T_DataArray:
2891         """Return a new DataArray object with transposed dimensions.
2892 
2893         Parameters
2894         ----------
2895         *dims : Hashable, optional
2896             By default, reverse the dimensions. Otherwise, reorder the
2897             dimensions to this order.
2898         transpose_coords : bool, default: True
2899             If True, also transpose the coordinates of this DataArray.
2900         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2901             What to do if dimensions that should be selected from are not present in the
2902             DataArray:
2903             - "raise": raise an exception
2904             - "warn": raise a warning, and ignore the missing dimensions
2905             - "ignore": ignore the missing dimensions
2906 
2907         Returns
2908         -------
2909         transposed : DataArray
2910             The returned DataArray's array is transposed.
2911 
2912         Notes
2913         -----
2914         This operation returns a view of this array's data. It is
2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2916         -- the data will be fully loaded.
2917 
2918         See Also
2919         --------
2920         numpy.transpose
2921         Dataset.transpose
2922         """
2923         if dims:
2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2925         variable = self.variable.transpose(*dims)
2926         if transpose_coords:
2927             coords: dict[Hashable, Variable] = {}
2928             for name, coord in self.coords.items():
2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2930                 coords[name] = coord.variable.transpose(*coord_dims)
2931             return self._replace(variable, coords)
2932         else:
2933             return self._replace(variable)
2934 
2935     @property
2936     def T(self: T_DataArray) -> T_DataArray:
2937         return self.transpose()
2938 
2939     # change type of self and return to T_DataArray once
2940     # https://github.com/python/mypy/issues/12846 is resolved
2941     def drop_vars(
2942         self,
2943         names: Hashable | Iterable[Hashable],
2944         *,
2945         errors: ErrorOptions = "raise",
2946     ) -> DataArray:
2947         """Returns an array with dropped variables.
2948 
2949         Parameters
2950         ----------
2951         names : Hashable or iterable of Hashable
2952             Name(s) of variables to drop.
2953         errors : {"raise", "ignore"}, default: "raise"
2954             If 'raise', raises a ValueError error if any of the variable
2955             passed are not in the dataset. If 'ignore', any given names that are in the
2956             DataArray are dropped and no error is raised.
2957 
2958         Returns
2959         -------
2960         dropped : Dataset
2961             New Dataset copied from `self` with variables removed.
2962 
2963         Examples
2964         -------
2965         >>> data = np.arange(12).reshape(4, 3)
2966         >>> da = xr.DataArray(
2967         ...     data=data,
2968         ...     dims=["x", "y"],
2969         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2970         ... )
2971         >>> da
2972         <xarray.DataArray (x: 4, y: 3)>
2973         array([[ 0,  1,  2],
2974                [ 3,  4,  5],
2975                [ 6,  7,  8],
2976                [ 9, 10, 11]])
2977         Coordinates:
2978           * x        (x) int64 10 20 30 40
2979           * y        (y) int64 70 80 90
2980 
2981         Removing a single variable:
2982 
2983         >>> da.drop_vars("x")
2984         <xarray.DataArray (x: 4, y: 3)>
2985         array([[ 0,  1,  2],
2986                [ 3,  4,  5],
2987                [ 6,  7,  8],
2988                [ 9, 10, 11]])
2989         Coordinates:
2990           * y        (y) int64 70 80 90
2991         Dimensions without coordinates: x
2992 
2993         Removing a list of variables:
2994 
2995         >>> da.drop_vars(["x", "y"])
2996         <xarray.DataArray (x: 4, y: 3)>
2997         array([[ 0,  1,  2],
2998                [ 3,  4,  5],
2999                [ 6,  7,  8],
3000                [ 9, 10, 11]])
3001         Dimensions without coordinates: x, y
3002         """
3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3004         return self._from_temp_dataset(ds)
3005 
3006     def drop_indexes(
3007         self: T_DataArray,
3008         coord_names: Hashable | Iterable[Hashable],
3009         *,
3010         errors: ErrorOptions = "raise",
3011     ) -> T_DataArray:
3012         """Drop the indexes assigned to the given coordinates.
3013 
3014         Parameters
3015         ----------
3016         coord_names : hashable or iterable of hashable
3017             Name(s) of the coordinate(s) for which to drop the index.
3018         errors : {"raise", "ignore"}, default: "raise"
3019             If 'raise', raises a ValueError error if any of the coordinates
3020             passed have no index or are not in the dataset.
3021             If 'ignore', no error is raised.
3022 
3023         Returns
3024         -------
3025         dropped : DataArray
3026             A new dataarray with dropped indexes.
3027         """
3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3029         return self._from_temp_dataset(ds)
3030 
3031     def drop(
3032         self: T_DataArray,
3033         labels: Mapping[Any, Any] | None = None,
3034         dim: Hashable | None = None,
3035         *,
3036         errors: ErrorOptions = "raise",
3037         **labels_kwargs,
3038     ) -> T_DataArray:
3039         """Backward compatible method based on `drop_vars` and `drop_sel`
3040 
3041         Using either `drop_vars` or `drop_sel` is encouraged
3042 
3043         See Also
3044         --------
3045         DataArray.drop_vars
3046         DataArray.drop_sel
3047         """
3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3049         return self._from_temp_dataset(ds)
3050 
3051     def drop_sel(
3052         self: T_DataArray,
3053         labels: Mapping[Any, Any] | None = None,
3054         *,
3055         errors: ErrorOptions = "raise",
3056         **labels_kwargs,
3057     ) -> T_DataArray:
3058         """Drop index labels from this DataArray.
3059 
3060         Parameters
3061         ----------
3062         labels : mapping of Hashable to Any
3063             Index labels to drop
3064         errors : {"raise", "ignore"}, default: "raise"
3065             If 'raise', raises a ValueError error if
3066             any of the index labels passed are not
3067             in the dataset. If 'ignore', any given labels that are in the
3068             dataset are dropped and no error is raised.
3069         **labels_kwargs : {dim: label, ...}, optional
3070             The keyword arguments form of ``dim`` and ``labels``
3071 
3072         Returns
3073         -------
3074         dropped : DataArray
3075 
3076         Examples
3077         --------
3078         >>> da = xr.DataArray(
3079         ...     np.arange(25).reshape(5, 5),
3080         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3081         ...     dims=("x", "y"),
3082         ... )
3083         >>> da
3084         <xarray.DataArray (x: 5, y: 5)>
3085         array([[ 0,  1,  2,  3,  4],
3086                [ 5,  6,  7,  8,  9],
3087                [10, 11, 12, 13, 14],
3088                [15, 16, 17, 18, 19],
3089                [20, 21, 22, 23, 24]])
3090         Coordinates:
3091           * x        (x) int64 0 2 4 6 8
3092           * y        (y) int64 0 3 6 9 12
3093 
3094         >>> da.drop_sel(x=[0, 2], y=9)
3095         <xarray.DataArray (x: 3, y: 4)>
3096         array([[10, 11, 12, 14],
3097                [15, 16, 17, 19],
3098                [20, 21, 22, 24]])
3099         Coordinates:
3100           * x        (x) int64 4 6 8
3101           * y        (y) int64 0 3 6 12
3102 
3103         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3104         <xarray.DataArray (x: 4, y: 3)>
3105         array([[ 2,  3,  4],
3106                [ 7,  8,  9],
3107                [12, 13, 14],
3108                [22, 23, 24]])
3109         Coordinates:
3110           * x        (x) int64 0 2 4 8
3111           * y        (y) int64 6 9 12
3112         """
3113         if labels_kwargs or isinstance(labels, dict):
3114             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3115 
3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3117         return self._from_temp_dataset(ds)
3118 
3119     def drop_isel(
3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3121     ) -> T_DataArray:
3122         """Drop index positions from this DataArray.
3123 
3124         Parameters
3125         ----------
3126         indexers : mapping of Hashable to Any or None, default: None
3127             Index locations to drop
3128         **indexers_kwargs : {dim: position, ...}, optional
3129             The keyword arguments form of ``dim`` and ``positions``
3130 
3131         Returns
3132         -------
3133         dropped : DataArray
3134 
3135         Raises
3136         ------
3137         IndexError
3138 
3139         Examples
3140         --------
3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3142         >>> da
3143         <xarray.DataArray (X: 5, Y: 5)>
3144         array([[ 0,  1,  2,  3,  4],
3145                [ 5,  6,  7,  8,  9],
3146                [10, 11, 12, 13, 14],
3147                [15, 16, 17, 18, 19],
3148                [20, 21, 22, 23, 24]])
3149         Dimensions without coordinates: X, Y
3150 
3151         >>> da.drop_isel(X=[0, 4], Y=2)
3152         <xarray.DataArray (X: 3, Y: 4)>
3153         array([[ 5,  6,  8,  9],
3154                [10, 11, 13, 14],
3155                [15, 16, 18, 19]])
3156         Dimensions without coordinates: X, Y
3157 
3158         >>> da.drop_isel({"X": 3, "Y": 3})
3159         <xarray.DataArray (X: 4, Y: 4)>
3160         array([[ 0,  1,  2,  4],
3161                [ 5,  6,  7,  9],
3162                [10, 11, 12, 14],
3163                [20, 21, 22, 24]])
3164         Dimensions without coordinates: X, Y
3165         """
3166         dataset = self._to_temp_dataset()
3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3168         return self._from_temp_dataset(dataset)
3169 
3170     def dropna(
3171         self: T_DataArray,
3172         dim: Hashable,
3173         how: Literal["any", "all"] = "any",
3174         thresh: int | None = None,
3175     ) -> T_DataArray:
3176         """Returns a new array with dropped labels for missing values along
3177         the provided dimension.
3178 
3179         Parameters
3180         ----------
3181         dim : Hashable
3182             Dimension along which to drop missing values. Dropping along
3183             multiple dimensions simultaneously is not yet supported.
3184         how : {"any", "all"}, default: "any"
3185             - any : if any NA values are present, drop that label
3186             - all : if all values are NA, drop that label
3187 
3188         thresh : int or None, default: None
3189             If supplied, require this many non-NA values.
3190 
3191         Returns
3192         -------
3193         dropped : DataArray
3194 
3195         Examples
3196         --------
3197         >>> temperature = [
3198         ...     [0, 4, 2, 9],
3199         ...     [np.nan, np.nan, np.nan, np.nan],
3200         ...     [np.nan, 4, 2, 0],
3201         ...     [3, 1, 0, 0],
3202         ... ]
3203         >>> da = xr.DataArray(
3204         ...     data=temperature,
3205         ...     dims=["Y", "X"],
3206         ...     coords=dict(
3207         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3208         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3209         ...     ),
3210         ... )
3211         >>> da
3212         <xarray.DataArray (Y: 4, X: 4)>
3213         array([[ 0.,  4.,  2.,  9.],
3214                [nan, nan, nan, nan],
3215                [nan,  4.,  2.,  0.],
3216                [ 3.,  1.,  0.,  0.]])
3217         Coordinates:
3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3219             lon      (X) float64 10.0 10.25 10.5 10.75
3220         Dimensions without coordinates: Y, X
3221 
3222         >>> da.dropna(dim="Y", how="any")
3223         <xarray.DataArray (Y: 2, X: 4)>
3224         array([[0., 4., 2., 9.],
3225                [3., 1., 0., 0.]])
3226         Coordinates:
3227             lat      (Y) float64 -20.0 -20.75
3228             lon      (X) float64 10.0 10.25 10.5 10.75
3229         Dimensions without coordinates: Y, X
3230 
3231         Drop values only if all values along the dimension are NaN:
3232 
3233         >>> da.dropna(dim="Y", how="all")
3234         <xarray.DataArray (Y: 3, X: 4)>
3235         array([[ 0.,  4.,  2.,  9.],
3236                [nan,  4.,  2.,  0.],
3237                [ 3.,  1.,  0.,  0.]])
3238         Coordinates:
3239             lat      (Y) float64 -20.0 -20.5 -20.75
3240             lon      (X) float64 10.0 10.25 10.5 10.75
3241         Dimensions without coordinates: Y, X
3242         """
3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3244         return self._from_temp_dataset(ds)
3245 
3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3247         """Fill missing values in this object.
3248 
3249         This operation follows the normal broadcasting and alignment rules that
3250         xarray uses for binary arithmetic, except the result is aligned to this
3251         object (``join='left'``) instead of aligned to the intersection of
3252         index coordinates (``join='inner'``).
3253 
3254         Parameters
3255         ----------
3256         value : scalar, ndarray or DataArray
3257             Used to fill all matching missing values in this array. If the
3258             argument is a DataArray, it is first aligned with (reindexed to)
3259             this array.
3260 
3261         Returns
3262         -------
3263         filled : DataArray
3264 
3265         Examples
3266         --------
3267         >>> da = xr.DataArray(
3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3269         ...     dims="Z",
3270         ...     coords=dict(
3271         ...         Z=("Z", np.arange(6)),
3272         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3273         ...     ),
3274         ... )
3275         >>> da
3276         <xarray.DataArray (Z: 6)>
3277         array([ 1.,  4., nan,  0.,  3., nan])
3278         Coordinates:
3279           * Z        (Z) int64 0 1 2 3 4 5
3280             height   (Z) int64 0 10 20 30 40 50
3281 
3282         Fill all NaN values with 0:
3283 
3284         >>> da.fillna(0)
3285         <xarray.DataArray (Z: 6)>
3286         array([1., 4., 0., 0., 3., 0.])
3287         Coordinates:
3288           * Z        (Z) int64 0 1 2 3 4 5
3289             height   (Z) int64 0 10 20 30 40 50
3290 
3291         Fill NaN values with corresponding values in array:
3292 
3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3294         <xarray.DataArray (Z: 6)>
3295         array([1., 4., 4., 0., 3., 9.])
3296         Coordinates:
3297           * Z        (Z) int64 0 1 2 3 4 5
3298             height   (Z) int64 0 10 20 30 40 50
3299         """
3300         if utils.is_dict_like(value):
3301             raise TypeError(
3302                 "cannot provide fill value as a dictionary with "
3303                 "fillna on a DataArray"
3304             )
3305         out = ops.fillna(self, value)
3306         return out
3307 
3308     def interpolate_na(
3309         self: T_DataArray,
3310         dim: Hashable | None = None,
3311         method: InterpOptions = "linear",
3312         limit: int | None = None,
3313         use_coordinate: bool | str = True,
3314         max_gap: (
3315             None
3316             | int
3317             | float
3318             | str
3319             | pd.Timedelta
3320             | np.timedelta64
3321             | datetime.timedelta
3322         ) = None,
3323         keep_attrs: bool | None = None,
3324         **kwargs: Any,
3325     ) -> T_DataArray:
3326         """Fill in NaNs by interpolating according to different methods.
3327 
3328         Parameters
3329         ----------
3330         dim : Hashable or None, optional
3331             Specifies the dimension along which to interpolate.
3332         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3333             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3334             String indicating which method to use for interpolation:
3335 
3336             - 'linear': linear interpolation. Additional keyword
3337               arguments are passed to :py:func:`numpy.interp`
3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3339               are passed to :py:func:`scipy.interpolate.interp1d`. If
3340               ``method='polynomial'``, the ``order`` keyword argument must also be
3341               provided.
3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3343               respective :py:class:`scipy.interpolate` classes.
3344 
3345         use_coordinate : bool or str, default: True
3346             Specifies which index to use as the x values in the interpolation
3347             formulated as `y = f(x)`. If False, values are treated as if
3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
3349             used. If ``use_coordinate`` is a string, it specifies the name of a
3350             coordinate variable to use as the index.
3351         limit : int or None, default: None
3352             Maximum number of consecutive NaNs to fill. Must be greater than 0
3353             or None for no limit. This filling is done regardless of the size of
3354             the gap in the data. To only interpolate over gaps less than a given length,
3355             see ``max_gap``.
3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3358             Use None for no limit. When interpolating along a datetime64 dimension
3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3360 
3361             - a string that is valid input for pandas.to_timedelta
3362             - a :py:class:`numpy.timedelta64` object
3363             - a :py:class:`pandas.Timedelta` object
3364             - a :py:class:`datetime.timedelta` object
3365 
3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3367             dimensions has not been implemented yet. Gap length is defined as the difference
3368             between coordinate values at the first data point after a gap and the last value
3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3370             between coordinate values at the first (last) valid data point and the first (last) NaN.
3371             For example, consider::
3372 
3373                 <xarray.DataArray (x: 9)>
3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3375                 Coordinates:
3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3377 
3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3379         keep_attrs : bool or None, default: None
3380             If True, the dataarray's attributes (`attrs`) will be copied from
3381             the original object to the new one.  If False, the new
3382             object will be returned without attributes.
3383         **kwargs : dict, optional
3384             parameters passed verbatim to the underlying interpolation function
3385 
3386         Returns
3387         -------
3388         interpolated: DataArray
3389             Filled in DataArray.
3390 
3391         See Also
3392         --------
3393         numpy.interp
3394         scipy.interpolate
3395 
3396         Examples
3397         --------
3398         >>> da = xr.DataArray(
3399         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3400         ... )
3401         >>> da
3402         <xarray.DataArray (x: 5)>
3403         array([nan,  2.,  3., nan,  0.])
3404         Coordinates:
3405           * x        (x) int64 0 1 2 3 4
3406 
3407         >>> da.interpolate_na(dim="x", method="linear")
3408         <xarray.DataArray (x: 5)>
3409         array([nan, 2. , 3. , 1.5, 0. ])
3410         Coordinates:
3411           * x        (x) int64 0 1 2 3 4
3412 
3413         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3414         <xarray.DataArray (x: 5)>
3415         array([1. , 2. , 3. , 1.5, 0. ])
3416         Coordinates:
3417           * x        (x) int64 0 1 2 3 4
3418         """
3419         from xarray.core.missing import interp_na
3420 
3421         return interp_na(
3422             self,
3423             dim=dim,
3424             method=method,
3425             limit=limit,
3426             use_coordinate=use_coordinate,
3427             max_gap=max_gap,
3428             keep_attrs=keep_attrs,
3429             **kwargs,
3430         )
3431 
3432     def ffill(
3433         self: T_DataArray, dim: Hashable, limit: int | None = None
3434     ) -> T_DataArray:
3435         """Fill NaN values by propagating values forward
3436 
3437         *Requires bottleneck.*
3438 
3439         Parameters
3440         ----------
3441         dim : Hashable
3442             Specifies the dimension along which to propagate values when
3443             filling.
3444         limit : int or None, default: None
3445             The maximum number of consecutive NaN values to forward fill. In
3446             other words, if there is a gap with more than this number of
3447             consecutive NaNs, it will only be partially filled. Must be greater
3448             than 0 or None for no limit. Must be None or greater than or equal
3449             to axis length if filling along chunked axes (dimensions).
3450 
3451         Returns
3452         -------
3453         filled : DataArray
3454 
3455         Examples
3456         --------
3457         >>> temperature = np.array(
3458         ...     [
3459         ...         [np.nan, 1, 3],
3460         ...         [0, np.nan, 5],
3461         ...         [5, np.nan, np.nan],
3462         ...         [3, np.nan, np.nan],
3463         ...         [0, 2, 0],
3464         ...     ]
3465         ... )
3466         >>> da = xr.DataArray(
3467         ...     data=temperature,
3468         ...     dims=["Y", "X"],
3469         ...     coords=dict(
3470         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3471         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3472         ...     ),
3473         ... )
3474         >>> da
3475         <xarray.DataArray (Y: 5, X: 3)>
3476         array([[nan,  1.,  3.],
3477                [ 0., nan,  5.],
3478                [ 5., nan, nan],
3479                [ 3., nan, nan],
3480                [ 0.,  2.,  0.]])
3481         Coordinates:
3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3483             lon      (X) float64 10.0 10.25 10.5
3484         Dimensions without coordinates: Y, X
3485 
3486         Fill all NaN values:
3487 
3488         >>> da.ffill(dim="Y", limit=None)
3489         <xarray.DataArray (Y: 5, X: 3)>
3490         array([[nan,  1.,  3.],
3491                [ 0.,  1.,  5.],
3492                [ 5.,  1.,  5.],
3493                [ 3.,  1.,  5.],
3494                [ 0.,  2.,  0.]])
3495         Coordinates:
3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3497             lon      (X) float64 10.0 10.25 10.5
3498         Dimensions without coordinates: Y, X
3499 
3500         Fill only the first of consecutive NaN values:
3501 
3502         >>> da.ffill(dim="Y", limit=1)
3503         <xarray.DataArray (Y: 5, X: 3)>
3504         array([[nan,  1.,  3.],
3505                [ 0.,  1.,  5.],
3506                [ 5., nan,  5.],
3507                [ 3., nan, nan],
3508                [ 0.,  2.,  0.]])
3509         Coordinates:
3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3511             lon      (X) float64 10.0 10.25 10.5
3512         Dimensions without coordinates: Y, X
3513         """
3514         from xarray.core.missing import ffill
3515 
3516         return ffill(self, dim, limit=limit)
3517 
3518     def bfill(
3519         self: T_DataArray, dim: Hashable, limit: int | None = None
3520     ) -> T_DataArray:
3521         """Fill NaN values by propagating values backward
3522 
3523         *Requires bottleneck.*
3524 
3525         Parameters
3526         ----------
3527         dim : str
3528             Specifies the dimension along which to propagate values when
3529             filling.
3530         limit : int or None, default: None
3531             The maximum number of consecutive NaN values to backward fill. In
3532             other words, if there is a gap with more than this number of
3533             consecutive NaNs, it will only be partially filled. Must be greater
3534             than 0 or None for no limit. Must be None or greater than or equal
3535             to axis length if filling along chunked axes (dimensions).
3536 
3537         Returns
3538         -------
3539         filled : DataArray
3540 
3541         Examples
3542         --------
3543         >>> temperature = np.array(
3544         ...     [
3545         ...         [0, 1, 3],
3546         ...         [0, np.nan, 5],
3547         ...         [5, np.nan, np.nan],
3548         ...         [3, np.nan, np.nan],
3549         ...         [np.nan, 2, 0],
3550         ...     ]
3551         ... )
3552         >>> da = xr.DataArray(
3553         ...     data=temperature,
3554         ...     dims=["Y", "X"],
3555         ...     coords=dict(
3556         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3557         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3558         ...     ),
3559         ... )
3560         >>> da
3561         <xarray.DataArray (Y: 5, X: 3)>
3562         array([[ 0.,  1.,  3.],
3563                [ 0., nan,  5.],
3564                [ 5., nan, nan],
3565                [ 3., nan, nan],
3566                [nan,  2.,  0.]])
3567         Coordinates:
3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3569             lon      (X) float64 10.0 10.25 10.5
3570         Dimensions without coordinates: Y, X
3571 
3572         Fill all NaN values:
3573 
3574         >>> da.bfill(dim="Y", limit=None)
3575         <xarray.DataArray (Y: 5, X: 3)>
3576         array([[ 0.,  1.,  3.],
3577                [ 0.,  2.,  5.],
3578                [ 5.,  2.,  0.],
3579                [ 3.,  2.,  0.],
3580                [nan,  2.,  0.]])
3581         Coordinates:
3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3583             lon      (X) float64 10.0 10.25 10.5
3584         Dimensions without coordinates: Y, X
3585 
3586         Fill only the first of consecutive NaN values:
3587 
3588         >>> da.bfill(dim="Y", limit=1)
3589         <xarray.DataArray (Y: 5, X: 3)>
3590         array([[ 0.,  1.,  3.],
3591                [ 0., nan,  5.],
3592                [ 5., nan, nan],
3593                [ 3.,  2.,  0.],
3594                [nan,  2.,  0.]])
3595         Coordinates:
3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3597             lon      (X) float64 10.0 10.25 10.5
3598         Dimensions without coordinates: Y, X
3599         """
3600         from xarray.core.missing import bfill
3601 
3602         return bfill(self, dim, limit=limit)
3603 
3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3605         """Combine two DataArray objects, with union of coordinates.
3606 
3607         This operation follows the normal broadcasting and alignment rules of
3608         ``join='outer'``.  Default to non-null values of array calling the
3609         method.  Use np.nan to fill in vacant cells after alignment.
3610 
3611         Parameters
3612         ----------
3613         other : DataArray
3614             Used to fill all matching missing values in this array.
3615 
3616         Returns
3617         -------
3618         DataArray
3619         """
3620         return ops.fillna(self, other, join="outer")
3621 
3622     def reduce(
3623         self: T_DataArray,
3624         func: Callable[..., Any],
3625         dim: Dims = None,
3626         *,
3627         axis: int | Sequence[int] | None = None,
3628         keep_attrs: bool | None = None,
3629         keepdims: bool = False,
3630         **kwargs: Any,
3631     ) -> T_DataArray:
3632         """Reduce this array by applying `func` along some dimension(s).
3633 
3634         Parameters
3635         ----------
3636         func : callable
3637             Function which can be called in the form
3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3639             np.ndarray over an integer valued axis.
3640         dim : "...", str, Iterable of Hashable or None, optional
3641             Dimension(s) over which to apply `func`. By default `func` is
3642             applied over all dimensions.
3643         axis : int or sequence of int, optional
3644             Axis(es) over which to repeatedly apply `func`. Only one of the
3645             'dim' and 'axis' arguments can be supplied. If neither are
3646             supplied, then the reduction is calculated over the flattened array
3647             (by calling `f(x)` without an axis argument).
3648         keep_attrs : bool or None, optional
3649             If True, the variable's attributes (`attrs`) will be copied from
3650             the original object to the new one.  If False (default), the new
3651             object will be returned without attributes.
3652         keepdims : bool, default: False
3653             If True, the dimensions which are reduced are left in the result
3654             as dimensions of size one. Coordinates that use these dimensions
3655             are removed.
3656         **kwargs : dict
3657             Additional keyword arguments passed on to `func`.
3658 
3659         Returns
3660         -------
3661         reduced : DataArray
3662             DataArray with this object's array replaced with an array with
3663             summarized data and the indicated dimension(s) removed.
3664         """
3665 
3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3667         return self._replace_maybe_drop_dims(var)
3668 
3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3670         """Convert this array into a pandas object with the same shape.
3671 
3672         The type of the returned object depends on the number of DataArray
3673         dimensions:
3674 
3675         * 0D -> `xarray.DataArray`
3676         * 1D -> `pandas.Series`
3677         * 2D -> `pandas.DataFrame`
3678 
3679         Only works for arrays with 2 or fewer dimensions.
3680 
3681         The DataArray constructor performs the inverse transformation.
3682 
3683         Returns
3684         -------
3685         result : DataArray | Series | DataFrame
3686             DataArray, pandas Series or pandas DataFrame.
3687         """
3688         # TODO: consolidate the info about pandas constructors and the
3689         # attributes that correspond to their indexes into a separate module?
3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3691         try:
3692             constructor = constructors[self.ndim]
3693         except KeyError:
3694             raise ValueError(
3695                 f"Cannot convert arrays with {self.ndim} dimensions into "
3696                 "pandas objects. Requires 2 or fewer dimensions."
3697             )
3698         indexes = [self.get_index(dim) for dim in self.dims]
3699         return constructor(self.values, *indexes)
3700 
3701     def to_dataframe(
3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3703     ) -> pd.DataFrame:
3704         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3705 
3706         The DataFrame is indexed by the Cartesian product of index coordinates
3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3708         included as columns in the DataFrame.
3709 
3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3711         doesn't rely on a MultiIndex to build the DataFrame.
3712 
3713         Parameters
3714         ----------
3715         name: Hashable or None, optional
3716             Name to give to this array (required if unnamed).
3717         dim_order: Sequence of Hashable or None, optional
3718             Hierarchical dimension order for the resulting dataframe.
3719             Array content is transposed to this order and then written out as flat
3720             vectors in contiguous order, so the last dimension in this list
3721             will be contiguous in the resulting DataFrame. This has a major
3722             influence on which operations are efficient on the resulting
3723             dataframe.
3724 
3725             If provided, must include all dimensions of this DataArray. By default,
3726             dimensions are sorted according to the DataArray dimensions order.
3727 
3728         Returns
3729         -------
3730         result: DataFrame
3731             DataArray as a pandas DataFrame.
3732 
3733         See also
3734         --------
3735         DataArray.to_pandas
3736         DataArray.to_series
3737         """
3738         if name is None:
3739             name = self.name
3740         if name is None:
3741             raise ValueError(
3742                 "cannot convert an unnamed DataArray to a "
3743                 "DataFrame: use the ``name`` parameter"
3744             )
3745         if self.ndim == 0:
3746             raise ValueError("cannot convert a scalar to a DataFrame")
3747 
3748         # By using a unique name, we can convert a DataArray into a DataFrame
3749         # even if it shares a name with one of its coordinates.
3750         # I would normally use unique_name = object() but that results in a
3751         # dataframe with columns in the wrong order, for reasons I have not
3752         # been able to debug (possibly a pandas bug?).
3753         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3754         ds = self._to_dataset_whole(name=unique_name)
3755 
3756         if dim_order is None:
3757             ordered_dims = dict(zip(self.dims, self.shape))
3758         else:
3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3760 
3761         df = ds._to_dataframe(ordered_dims)
3762         df.columns = [name if c == unique_name else c for c in df.columns]
3763         return df
3764 
3765     def to_series(self) -> pd.Series:
3766         """Convert this array into a pandas.Series.
3767 
3768         The Series is indexed by the Cartesian product of index coordinates
3769         (in the form of a :py:class:`pandas.MultiIndex`).
3770 
3771         Returns
3772         -------
3773         result : Series
3774             DataArray as a pandas Series.
3775 
3776         See also
3777         --------
3778         DataArray.to_pandas
3779         DataArray.to_dataframe
3780         """
3781         index = self.coords.to_index()
3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3783 
3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3785         """Convert this array into a numpy.ma.MaskedArray
3786 
3787         Parameters
3788         ----------
3789         copy : bool, default: True
3790             If True make a copy of the array in the result. If False,
3791             a MaskedArray view of DataArray.values is returned.
3792 
3793         Returns
3794         -------
3795         result : MaskedArray
3796             Masked where invalid values (nan or inf) occur.
3797         """
3798         values = self.to_numpy()  # only compute lazy arrays once
3799         isnull = pd.isnull(values)
3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3801 
3802     # path=None writes to bytes
3803     @overload
3804     def to_netcdf(
3805         self,
3806         path: None = None,
3807         mode: Literal["w", "a"] = "w",
3808         format: T_NetcdfTypes | None = None,
3809         group: str | None = None,
3810         engine: T_NetcdfEngine | None = None,
3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3812         unlimited_dims: Iterable[Hashable] | None = None,
3813         compute: bool = True,
3814         invalid_netcdf: bool = False,
3815     ) -> bytes:
3816         ...
3817 
3818     # default return None
3819     @overload
3820     def to_netcdf(
3821         self,
3822         path: str | PathLike,
3823         mode: Literal["w", "a"] = "w",
3824         format: T_NetcdfTypes | None = None,
3825         group: str | None = None,
3826         engine: T_NetcdfEngine | None = None,
3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3828         unlimited_dims: Iterable[Hashable] | None = None,
3829         compute: Literal[True] = True,
3830         invalid_netcdf: bool = False,
3831     ) -> None:
3832         ...
3833 
3834     # compute=False returns dask.Delayed
3835     @overload
3836     def to_netcdf(
3837         self,
3838         path: str | PathLike,
3839         mode: Literal["w", "a"] = "w",
3840         format: T_NetcdfTypes | None = None,
3841         group: str | None = None,
3842         engine: T_NetcdfEngine | None = None,
3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3844         unlimited_dims: Iterable[Hashable] | None = None,
3845         *,
3846         compute: Literal[False],
3847         invalid_netcdf: bool = False,
3848     ) -> Delayed:
3849         ...
3850 
3851     def to_netcdf(
3852         self,
3853         path: str | PathLike | None = None,
3854         mode: Literal["w", "a"] = "w",
3855         format: T_NetcdfTypes | None = None,
3856         group: str | None = None,
3857         engine: T_NetcdfEngine | None = None,
3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3859         unlimited_dims: Iterable[Hashable] | None = None,
3860         compute: bool = True,
3861         invalid_netcdf: bool = False,
3862     ) -> bytes | Delayed | None:
3863         """Write DataArray contents to a netCDF file.
3864 
3865         Parameters
3866         ----------
3867         path : str, path-like or None, optional
3868             Path to which to save this dataset. File-like objects are only
3869             supported by the scipy engine. If no path is provided, this
3870             function returns the resulting netCDF file as bytes; in this case,
3871             we need to use scipy, which does not support netCDF version 4 (the
3872             default format becomes NETCDF3_64BIT).
3873         mode : {"w", "a"}, default: "w"
3874             Write ('w') or append ('a') mode. If mode='w', any existing file at
3875             this location will be overwritten. If mode='a', existing variables
3876             will be overwritten.
3877         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3878                   "NETCDF3_CLASSIC"}, optional
3879             File format for the resulting netCDF file:
3880 
3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3882               features.
3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3884               netCDF 3 compatible API features.
3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3886               which fully supports 2+ GB files, but is only compatible with
3887               clients linked against netCDF version 3.6.0 or later.
3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3889               handle 2+ GB files very well.
3890 
3891             All formats are supported by the netCDF4-python library.
3892             scipy.io.netcdf only supports the last two formats.
3893 
3894             The default format is NETCDF4 if you are saving a file to disk and
3895             have the netCDF4-python library available. Otherwise, xarray falls
3896             back to using scipy to write netCDF files and defaults to the
3897             NETCDF3_64BIT format (scipy does not support netCDF4).
3898         group : str, optional
3899             Path to the netCDF4 group in the given file to open (only works for
3900             format='NETCDF4'). The group(s) will be created if necessary.
3901         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3902             Engine to use when writing netCDF files. If not provided, the
3903             default engine is chosen based on available dependencies, with a
3904             preference for 'netcdf4' if writing to a file on disk.
3905         encoding : dict, optional
3906             Nested dictionary with variable names as keys and dictionaries of
3907             variable specific encodings as values, e.g.,
3908             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3909             "zlib": True}, ...}``
3910 
3911             The `h5netcdf` engine supports both the NetCDF4-style compression
3912             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3913             ones ``{"compression": "gzip", "compression_opts": 9}``.
3914             This allows using any compression plugin installed in the HDF5
3915             library, e.g. LZF.
3916 
3917         unlimited_dims : iterable of Hashable, optional
3918             Dimension(s) that should be serialized as unlimited dimensions.
3919             By default, no dimensions are treated as unlimited dimensions.
3920             Note that unlimited_dims may also be set via
3921             ``dataset.encoding["unlimited_dims"]``.
3922         compute: bool, default: True
3923             If true compute immediately, otherwise return a
3924             ``dask.delayed.Delayed`` object that can be computed later.
3925         invalid_netcdf: bool, default: False
3926             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3927             hdf5 files which are invalid netcdf as described in
3928             https://github.com/h5netcdf/h5netcdf.
3929 
3930         Returns
3931         -------
3932         store: bytes or Delayed or None
3933             * ``bytes`` if path is None
3934             * ``dask.delayed.Delayed`` if compute is False
3935             * None otherwise
3936 
3937         Notes
3938         -----
3939         Only xarray.Dataset objects can be written to netCDF files, so
3940         the xarray.DataArray is converted to a xarray.Dataset object
3941         containing a single variable. If the DataArray has no name, or if the
3942         name is the same as a coordinate name, then it is given the name
3943         ``"__xarray_dataarray_variable__"``.
3944 
3945         See Also
3946         --------
3947         Dataset.to_netcdf
3948         """
3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3950 
3951         if self.name is None:
3952             # If no name is set then use a generic xarray name
3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3954         elif self.name in self.coords or self.name in self.dims:
3955             # The name is the same as one of the coords names, which netCDF
3956             # doesn't support, so rename it but keep track of the old name
3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3958             dataset.attrs[DATAARRAY_NAME] = self.name
3959         else:
3960             # No problems with the name - so we're fine!
3961             dataset = self.to_dataset()
3962 
3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3964             dataset,
3965             path,
3966             mode=mode,
3967             format=format,
3968             group=group,
3969             engine=engine,
3970             encoding=encoding,
3971             unlimited_dims=unlimited_dims,
3972             compute=compute,
3973             multifile=False,
3974             invalid_netcdf=invalid_netcdf,
3975         )
3976 
3977     # compute=True (default) returns ZarrStore
3978     @overload
3979     def to_zarr(
3980         self,
3981         store: MutableMapping | str | PathLike[str] | None = None,
3982         chunk_store: MutableMapping | str | PathLike | None = None,
3983         mode: Literal["w", "w-", "a", "r+", None] = None,
3984         synchronizer=None,
3985         group: str | None = None,
3986         encoding: Mapping | None = None,
3987         compute: Literal[True] = True,
3988         consolidated: bool | None = None,
3989         append_dim: Hashable | None = None,
3990         region: Mapping[str, slice] | None = None,
3991         safe_chunks: bool = True,
3992         storage_options: dict[str, str] | None = None,
3993         zarr_version: int | None = None,
3994     ) -> ZarrStore:
3995         ...
3996 
3997     # compute=False returns dask.Delayed
3998     @overload
3999     def to_zarr(
4000         self,
4001         store: MutableMapping | str | PathLike[str] | None = None,
4002         chunk_store: MutableMapping | str | PathLike | None = None,
4003         mode: Literal["w", "w-", "a", "r+", None] = None,
4004         synchronizer=None,
4005         group: str | None = None,
4006         encoding: Mapping | None = None,
4007         *,
4008         compute: Literal[False],
4009         consolidated: bool | None = None,
4010         append_dim: Hashable | None = None,
4011         region: Mapping[str, slice] | None = None,
4012         safe_chunks: bool = True,
4013         storage_options: dict[str, str] | None = None,
4014         zarr_version: int | None = None,
4015     ) -> Delayed:
4016         ...
4017 
4018     def to_zarr(
4019         self,
4020         store: MutableMapping | str | PathLike[str] | None = None,
4021         chunk_store: MutableMapping | str | PathLike | None = None,
4022         mode: Literal["w", "w-", "a", "r+", None] = None,
4023         synchronizer=None,
4024         group: str | None = None,
4025         encoding: Mapping | None = None,
4026         compute: bool = True,
4027         consolidated: bool | None = None,
4028         append_dim: Hashable | None = None,
4029         region: Mapping[str, slice] | None = None,
4030         safe_chunks: bool = True,
4031         storage_options: dict[str, str] | None = None,
4032         zarr_version: int | None = None,
4033     ) -> ZarrStore | Delayed:
4034         """Write DataArray contents to a Zarr store
4035 
4036         Zarr chunks are determined in the following way:
4037 
4038         - From the ``chunks`` attribute in each variable's ``encoding``
4039           (can be set via `DataArray.chunk`).
4040         - If the variable is a Dask array, from the dask chunks
4041         - If neither Dask chunks nor encoding chunks are present, chunks will
4042           be determined automatically by Zarr
4043         - If both Dask chunks and encoding chunks are present, encoding chunks
4044           will be used, provided that there is a many-to-one relationship between
4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.
4047           This restriction ensures that no synchronization / locks are required
4048           when writing. To disable this restriction, use ``safe_chunks=False``.
4049 
4050         Parameters
4051         ----------
4052         store : MutableMapping, str or path-like, optional
4053             Store or path to directory in local or remote file system.
4054         chunk_store : MutableMapping, str or path-like, optional
4055             Store or path to directory in local or remote file system only for Zarr
4056             array chunks. Requires zarr-python v2.4.0 or later.
4057         mode : {"w", "w-", "a", "r+", None}, optional
4058             Persistence mode: "w" means create (overwrite if exists);
4059             "w-" means create (fail if exists);
4060             "a" means override existing variables (create if does not exist);
4061             "r+" means modify existing array *values* only (raise an error if
4062             any metadata or shapes would change).
4063             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
4064             "r+" if ``region`` is set and ``w-`` otherwise.
4065         synchronizer : object, optional
4066             Zarr array synchronizer.
4067         group : str, optional
4068             Group path. (a.k.a. `path` in zarr terminology.)
4069         encoding : dict, optional
4070             Nested dictionary with variable names as keys and dictionaries of
4071             variable specific encodings as values, e.g.,
4072             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
4073         compute : bool, default: True
4074             If True write array data immediately, otherwise return a
4075             ``dask.delayed.Delayed`` object that can be computed to write
4076             array data later. Metadata is always updated eagerly.
4077         consolidated : bool, optional
4078             If True, apply zarr's `consolidate_metadata` function to the store
4079             after writing metadata and read existing stores with consolidated
4080             metadata; if False, do not. The default (`consolidated=None`) means
4081             write consolidated metadata and attempt to read consolidated
4082             metadata for existing stores (falling back to non-consolidated).
4083 
4084             When the experimental ``zarr_version=3``, ``consolidated`` must be
4085             either be ``None`` or ``False``.
4086         append_dim : hashable, optional
4087             If set, the dimension along which the data will be appended. All
4088             other dimensions on overridden variables must remain the same size.
4089         region : dict, optional
4090             Optional mapping from dimension names to integer slices along
4091             dataarray dimensions to indicate the region of existing zarr array(s)
4092             in which to write this datarray's data. For example,
4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
4094             that values should be written to the region ``0:1000`` along ``x``
4095             and ``10000:11000`` along ``y``.
4096 
4097             Two restrictions apply to the use of ``region``:
4098 
4099             - If ``region`` is set, _all_ variables in a dataarray must have at
4100               least one dimension in common with the region. Other variables
4101               should be written in a separate call to ``to_zarr()``.
4102             - Dimensions cannot be included in both ``region`` and
4103               ``append_dim`` at the same time. To create empty arrays to fill
4104               in with ``region``, use a separate call to ``to_zarr()`` with
4105               ``compute=False``. See "Appending to existing Zarr stores" in
4106               the reference documentation for full details.
4107         safe_chunks : bool, default: True
4108             If True, only allow writes to when there is a many-to-one relationship
4109             between Zarr chunks (specified in encoding) and Dask chunks.
4110             Set False to override this restriction; however, data may become corrupted
4111             if Zarr arrays are written in parallel. This option may be useful in combination
4112             with ``compute=False`` to initialize a Zarr store from an existing
4113             DataArray with arbitrary chunk structure.
4114         storage_options : dict, optional
4115             Any additional parameters for the storage backend (ignored for local
4116             paths).
4117         zarr_version : int or None, optional
4118             The desired zarr spec version to target (currently 2 or 3). The
4119             default of None will attempt to determine the zarr version from
4120             ``store`` when possible, otherwise defaulting to 2.
4121 
4122         Returns
4123         -------
4124             * ``dask.delayed.Delayed`` if compute is False
4125             * ZarrStore otherwise
4126 
4127         References
4128         ----------
4129         https://zarr.readthedocs.io/
4130 
4131         Notes
4132         -----
4133         Zarr chunking behavior:
4134             If chunks are found in the encoding argument or attribute
4135             corresponding to any DataArray, those chunks are used.
4136             If a DataArray is a dask array, it is written with those chunks.
4137             If not other chunks are found, Zarr uses its own heuristics to
4138             choose automatic chunk sizes.
4139 
4140         encoding:
4141             The encoding attribute (if exists) of the DataArray(s) will be
4142             used. Override any existing encodings by providing the ``encoding`` kwarg.
4143 
4144         See Also
4145         --------
4146         Dataset.to_zarr
4147         :ref:`io.zarr`
4148             The I/O user guide, with more details and examples.
4149         """
4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr
4151 
4152         if self.name is None:
4153             # If no name is set then use a generic xarray name
4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4155         elif self.name in self.coords or self.name in self.dims:
4156             # The name is the same as one of the coords names, which the netCDF data model
4157             # does not support, so rename it but keep track of the old name
4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4159             dataset.attrs[DATAARRAY_NAME] = self.name
4160         else:
4161             # No problems with the name - so we're fine!
4162             dataset = self.to_dataset()
4163 
4164         return to_zarr(  # type: ignore[call-overload,misc]
4165             dataset,
4166             store=store,
4167             chunk_store=chunk_store,
4168             mode=mode,
4169             synchronizer=synchronizer,
4170             group=group,
4171             encoding=encoding,
4172             compute=compute,
4173             consolidated=consolidated,
4174             append_dim=append_dim,
4175             region=region,
4176             safe_chunks=safe_chunks,
4177             storage_options=storage_options,
4178             zarr_version=zarr_version,
4179         )
4180 
4181     def to_dict(
4182         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
4183     ) -> dict[str, Any]:
4184         """
4185         Convert this xarray.DataArray into a dictionary following xarray
4186         naming conventions.
4187 
4188         Converts all variables and attributes to native Python objects.
4189         Useful for converting to json. To avoid datetime incompatibility
4190         use decode_times=False kwarg in xarray.open_dataset.
4191 
4192         Parameters
4193         ----------
4194         data : bool or {"list", "array"}, default: "list"
4195             Whether to include the actual data in the dictionary. When set to
4196             False, returns just the schema. If set to "array", returns data as
4197             underlying array type. If set to "list" (or True for backwards
4198             compatibility), returns data in lists of Python data types. Note
4199             that for obtaining the "list" output efficiently, use
4200             `da.compute().to_dict(data="list")`.
4201 
4202         encoding : bool, default: False
4203             Whether to include the Dataset's encoding in the dictionary.
4204 
4205         Returns
4206         -------
4207         dict: dict
4208 
4209         See Also
4210         --------
4211         DataArray.from_dict
4212         Dataset.to_dict
4213         """
4214         d = self.variable.to_dict(data=data)
4215         d.update({"coords": {}, "name": self.name})
4216         for k, coord in self.coords.items():
4217             d["coords"][k] = coord.variable.to_dict(data=data)
4218         if encoding:
4219             d["encoding"] = dict(self.encoding)
4220         return d
4221 
4222     @classmethod
4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4224         """Convert a dictionary into an xarray.DataArray
4225 
4226         Parameters
4227         ----------
4228         d : dict
4229             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4230 
4231         Returns
4232         -------
4233         obj : xarray.DataArray
4234 
4235         See Also
4236         --------
4237         DataArray.to_dict
4238         Dataset.from_dict
4239 
4240         Examples
4241         --------
4242         >>> d = {"dims": "t", "data": [1, 2, 3]}
4243         >>> da = xr.DataArray.from_dict(d)
4244         >>> da
4245         <xarray.DataArray (t: 3)>
4246         array([1, 2, 3])
4247         Dimensions without coordinates: t
4248 
4249         >>> d = {
4250         ...     "coords": {
4251         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4252         ...     },
4253         ...     "attrs": {"title": "air temperature"},
4254         ...     "dims": "t",
4255         ...     "data": [10, 20, 30],
4256         ...     "name": "a",
4257         ... }
4258         >>> da = xr.DataArray.from_dict(d)
4259         >>> da
4260         <xarray.DataArray 'a' (t: 3)>
4261         array([10, 20, 30])
4262         Coordinates:
4263           * t        (t) int64 0 1 2
4264         Attributes:
4265             title:    air temperature
4266         """
4267         coords = None
4268         if "coords" in d:
4269             try:
4270                 coords = {
4271                     k: (v["dims"], v["data"], v.get("attrs"))
4272                     for k, v in d["coords"].items()
4273                 }
4274             except KeyError as e:
4275                 raise ValueError(
4276                     "cannot convert dict when coords are missing the key "
4277                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4278                 )
4279         try:
4280             data = d["data"]
4281         except KeyError:
4282             raise ValueError("cannot convert dict without the key 'data''")
4283         else:
4284             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4285 
4286         obj.encoding.update(d.get("encoding", {}))
4287 
4288         return obj
4289 
4290     @classmethod
4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4292         """Convert a pandas.Series into an xarray.DataArray.
4293 
4294         If the series's index is a MultiIndex, it will be expanded into a
4295         tensor product of one-dimensional coordinates (filling in missing
4296         values with NaN). Thus this operation should be the inverse of the
4297         `to_series` method.
4298 
4299         Parameters
4300         ----------
4301         series : Series
4302             Pandas Series object to convert.
4303         sparse : bool, default: False
4304             If sparse=True, creates a sparse array instead of a dense NumPy array.
4305             Requires the pydata/sparse package.
4306 
4307         See Also
4308         --------
4309         DataArray.to_series
4310         Dataset.from_dataframe
4311         """
4312         temp_name = "__temporary_name"
4313         df = pd.DataFrame({temp_name: series})
4314         ds = Dataset.from_dataframe(df, sparse=sparse)
4315         result = cast(DataArray, ds[temp_name])
4316         result.name = series.name
4317         return result
4318 
4319     def to_cdms2(self) -> cdms2_Variable:
4320         """Convert this array into a cdms2.Variable"""
4321         from xarray.convert import to_cdms2
4322 
4323         return to_cdms2(self)
4324 
4325     @classmethod
4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4327         """Convert a cdms2.Variable into an xarray.DataArray"""
4328         from xarray.convert import from_cdms2
4329 
4330         return from_cdms2(variable)
4331 
4332     def to_iris(self) -> iris_Cube:
4333         """Convert this array into a iris.cube.Cube"""
4334         from xarray.convert import to_iris
4335 
4336         return to_iris(self)
4337 
4338     @classmethod
4339     def from_iris(cls, cube: iris_Cube) -> DataArray:
4340         """Convert a iris.cube.Cube into an xarray.DataArray"""
4341         from xarray.convert import from_iris
4342 
4343         return from_iris(cube)
4344 
4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4346         """Helper function for equals, broadcast_equals, and identical"""
4347 
4348         def compat(x, y):
4349             return getattr(x.variable, compat_str)(y.variable)
4350 
4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4352             self, other
4353         )
4354 
4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4356         """Two DataArrays are broadcast equal if they are equal after
4357         broadcasting them against each other such that they have the same
4358         dimensions.
4359 
4360         Parameters
4361         ----------
4362         other : DataArray
4363             DataArray to compare to.
4364 
4365         Returns
4366         ----------
4367         equal : bool
4368             True if the two DataArrays are broadcast equal.
4369 
4370         See Also
4371         --------
4372         DataArray.equals
4373         DataArray.identical
4374 
4375         Examples
4376         --------
4377         >>> a = xr.DataArray([1, 2], dims="X")
4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4379         >>> a
4380         <xarray.DataArray (X: 2)>
4381         array([1, 2])
4382         Dimensions without coordinates: X
4383         >>> b
4384         <xarray.DataArray (X: 2, Y: 2)>
4385         array([[1, 1],
4386                [2, 2]])
4387         Dimensions without coordinates: X, Y
4388 
4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4390 
4391         >>> a.equals(b)
4392         False
4393         >>> a2, b2 = xr.broadcast(a, b)
4394         >>> a2.equals(b2)
4395         True
4396         >>> a.broadcast_equals(b)
4397         True
4398         """
4399         try:
4400             return self._all_compat(other, "broadcast_equals")
4401         except (TypeError, AttributeError):
4402             return False
4403 
4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4405         """True if two DataArrays have the same dimensions, coordinates and
4406         values; otherwise False.
4407 
4408         DataArrays can still be equal (like pandas objects) if they have NaN
4409         values in the same locations.
4410 
4411         This method is necessary because `v1 == v2` for ``DataArray``
4412         does element-wise comparisons (like numpy.ndarrays).
4413 
4414         Parameters
4415         ----------
4416         other : DataArray
4417             DataArray to compare to.
4418 
4419         Returns
4420         ----------
4421         equal : bool
4422             True if the two DataArrays are equal.
4423 
4424         See Also
4425         --------
4426         DataArray.broadcast_equals
4427         DataArray.identical
4428 
4429         Examples
4430         --------
4431         >>> a = xr.DataArray([1, 2, 3], dims="X")
4432         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4433         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4434         >>> d = xr.DataArray([3, 2, 1], dims="X")
4435         >>> a
4436         <xarray.DataArray (X: 3)>
4437         array([1, 2, 3])
4438         Dimensions without coordinates: X
4439         >>> b
4440         <xarray.DataArray (X: 3)>
4441         array([1, 2, 3])
4442         Dimensions without coordinates: X
4443         Attributes:
4444             units:    m
4445         >>> c
4446         <xarray.DataArray (Y: 3)>
4447         array([1, 2, 3])
4448         Dimensions without coordinates: Y
4449         >>> d
4450         <xarray.DataArray (X: 3)>
4451         array([3, 2, 1])
4452         Dimensions without coordinates: X
4453 
4454         >>> a.equals(b)
4455         True
4456         >>> a.equals(c)
4457         False
4458         >>> a.equals(d)
4459         False
4460         """
4461         try:
4462             return self._all_compat(other, "equals")
4463         except (TypeError, AttributeError):
4464             return False
4465 
4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4467         """Like equals, but also checks the array name and attributes, and
4468         attributes on all coordinates.
4469 
4470         Parameters
4471         ----------
4472         other : DataArray
4473             DataArray to compare to.
4474 
4475         Returns
4476         ----------
4477         equal : bool
4478             True if the two DataArrays are identical.
4479 
4480         See Also
4481         --------
4482         DataArray.broadcast_equals
4483         DataArray.equals
4484 
4485         Examples
4486         --------
4487         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4488         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4489         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4490         >>> a
4491         <xarray.DataArray 'Width' (X: 3)>
4492         array([1, 2, 3])
4493         Dimensions without coordinates: X
4494         Attributes:
4495             units:    m
4496         >>> b
4497         <xarray.DataArray 'Width' (X: 3)>
4498         array([1, 2, 3])
4499         Dimensions without coordinates: X
4500         Attributes:
4501             units:    m
4502         >>> c
4503         <xarray.DataArray 'Width' (X: 3)>
4504         array([1, 2, 3])
4505         Dimensions without coordinates: X
4506         Attributes:
4507             units:    ft
4508 
4509         >>> a.equals(b)
4510         True
4511         >>> a.identical(b)
4512         True
4513 
4514         >>> a.equals(c)
4515         True
4516         >>> a.identical(c)
4517         False
4518         """
4519         try:
4520             return self.name == other.name and self._all_compat(other, "identical")
4521         except (TypeError, AttributeError):
4522             return False
4523 
4524     def _result_name(self, other: Any = None) -> Hashable | None:
4525         # use the same naming heuristics as pandas:
4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4527         other_name = getattr(other, "name", _default)
4528         if other_name is _default or other_name == self.name:
4529             return self.name
4530         else:
4531             return None
4532 
4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4534         new_var = self.variable.__array_wrap__(obj, context)
4535         return self._replace(new_var)
4536 
4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4538         return self.dot(obj)
4539 
4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4541         # currently somewhat duplicative, as only other DataArrays are
4542         # compatible with matmul
4543         return computation.dot(other, self)
4544 
4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4546         keep_attrs = kwargs.pop("keep_attrs", None)
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=True)
4549         with warnings.catch_warnings():
4550             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4551             warnings.filterwarnings(
4552                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4553             )
4554             with np.errstate(all="ignore"):
4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4556             if keep_attrs:
4557                 da.attrs = self.attrs
4558             return da
4559 
4560     def _binary_op(
4561         self: T_DataArray,
4562         other: Any,
4563         f: Callable,
4564         reflexive: bool = False,
4565     ) -> T_DataArray:
4566         from xarray.core.groupby import GroupBy
4567 
4568         if isinstance(other, (Dataset, GroupBy)):
4569             return NotImplemented
4570         if isinstance(other, DataArray):
4571             align_type = OPTIONS["arithmetic_join"]
4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4573         other_variable = getattr(other, "variable", other)
4574         other_coords = getattr(other, "coords", None)
4575 
4576         variable = (
4577             f(self.variable, other_variable)
4578             if not reflexive
4579             else f(other_variable, self.variable)
4580         )
4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4582         name = self._result_name(other)
4583 
4584         return self._replace(variable, coords, name, indexes=indexes)
4585 
4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4587         from xarray.core.groupby import GroupBy
4588 
4589         if isinstance(other, GroupBy):
4590             raise TypeError(
4591                 "in-place operations between a DataArray and "
4592                 "a grouped object are not permitted"
4593             )
4594         # n.b. we can't align other to self (with other.reindex_like(self))
4595         # because `other` may be converted into floats, which would cause
4596         # in-place arithmetic to fail unpredictably. Instead, we simply
4597         # don't support automatic alignment with in-place arithmetic.
4598         other_coords = getattr(other, "coords", None)
4599         other_variable = getattr(other, "variable", other)
4600         try:
4601             with self.coords._merge_inplace(other_coords):
4602                 f(self.variable, other_variable)
4603         except MergeError as exc:
4604             raise MergeError(
4605                 "Automatic alignment is not supported for in-place operations.\n"
4606                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4607                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4608             ) from exc
4609         return self
4610 
4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4612         self.attrs = other.attrs
4613 
4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4615 
4616     def _title_for_slice(self, truncate: int = 50) -> str:
4617         """
4618         If the dataarray has 1 dimensional coordinates or comes from a slice
4619         we can show that info in the title
4620 
4621         Parameters
4622         ----------
4623         truncate : int, default: 50
4624             maximum number of characters for title
4625 
4626         Returns
4627         -------
4628         title : string
4629             Can be used for plot titles
4630 
4631         """
4632         one_dims = []
4633         for dim, coord in self.coords.items():
4634             if coord.size == 1:
4635                 one_dims.append(
4636                     "{dim} = {v}{unit}".format(
4637                         dim=dim,
4638                         v=format_item(coord.values),
4639                         unit=_get_units_from_attrs(coord),
4640                     )
4641                 )
4642 
4643         title = ", ".join(one_dims)
4644         if len(title) > truncate:
4645             title = title[: (truncate - 3)] + "..."
4646 
4647         return title
4648 
4649     def diff(
4650         self: T_DataArray,
4651         dim: Hashable,
4652         n: int = 1,
4653         label: Literal["upper", "lower"] = "upper",
4654     ) -> T_DataArray:
4655         """Calculate the n-th order discrete difference along given axis.
4656 
4657         Parameters
4658         ----------
4659         dim : Hashable
4660             Dimension over which to calculate the finite difference.
4661         n : int, default: 1
4662             The number of times values are differenced.
4663         label : {"upper", "lower"}, default: "upper"
4664             The new coordinate in dimension ``dim`` will have the
4665             values of either the minuend's or subtrahend's coordinate
4666             for values 'upper' and 'lower', respectively.
4667 
4668         Returns
4669         -------
4670         difference : DataArray
4671             The n-th order finite difference of this object.
4672 
4673         Notes
4674         -----
4675         `n` matches numpy's behavior and is different from pandas' first argument named
4676         `periods`.
4677 
4678         Examples
4679         --------
4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4681         >>> arr.diff("x")
4682         <xarray.DataArray (x: 3)>
4683         array([0, 1, 0])
4684         Coordinates:
4685           * x        (x) int64 2 3 4
4686         >>> arr.diff("x", 2)
4687         <xarray.DataArray (x: 2)>
4688         array([ 1, -1])
4689         Coordinates:
4690           * x        (x) int64 3 4
4691 
4692         See Also
4693         --------
4694         DataArray.differentiate
4695         """
4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4697         return self._from_temp_dataset(ds)
4698 
4699     def shift(
4700         self: T_DataArray,
4701         shifts: Mapping[Any, int] | None = None,
4702         fill_value: Any = dtypes.NA,
4703         **shifts_kwargs: int,
4704     ) -> T_DataArray:
4705         """Shift this DataArray by an offset along one or more dimensions.
4706 
4707         Only the data is moved; coordinates stay in place. This is consistent
4708         with the behavior of ``shift`` in pandas.
4709 
4710         Values shifted from beyond array bounds will appear at one end of
4711         each dimension, which are filled according to `fill_value`. For periodic
4712         offsets instead see `roll`.
4713 
4714         Parameters
4715         ----------
4716         shifts : mapping of Hashable to int or None, optional
4717             Integer offset to shift along each of the given dimensions.
4718             Positive offsets shift to the right; negative offsets shift to the
4719             left.
4720         fill_value : scalar, optional
4721             Value to use for newly missing values
4722         **shifts_kwargs
4723             The keyword arguments form of ``shifts``.
4724             One of shifts or shifts_kwargs must be provided.
4725 
4726         Returns
4727         -------
4728         shifted : DataArray
4729             DataArray with the same coordinates and attributes but shifted
4730             data.
4731 
4732         See Also
4733         --------
4734         roll
4735 
4736         Examples
4737         --------
4738         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4739         >>> arr.shift(x=1)
4740         <xarray.DataArray (x: 3)>
4741         array([nan,  5.,  6.])
4742         Dimensions without coordinates: x
4743         """
4744         variable = self.variable.shift(
4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4746         )
4747         return self._replace(variable=variable)
4748 
4749     def roll(
4750         self: T_DataArray,
4751         shifts: Mapping[Hashable, int] | None = None,
4752         roll_coords: bool = False,
4753         **shifts_kwargs: int,
4754     ) -> T_DataArray:
4755         """Roll this array by an offset along one or more dimensions.
4756 
4757         Unlike shift, roll treats the given dimensions as periodic, so will not
4758         create any missing values to be filled.
4759 
4760         Unlike shift, roll may rotate all variables, including coordinates
4761         if specified. The direction of rotation is consistent with
4762         :py:func:`numpy.roll`.
4763 
4764         Parameters
4765         ----------
4766         shifts : mapping of Hashable to int, optional
4767             Integer offset to rotate each of the given dimensions.
4768             Positive offsets roll to the right; negative offsets roll to the
4769             left.
4770         roll_coords : bool, default: False
4771             Indicates whether to roll the coordinates by the offset too.
4772         **shifts_kwargs : {dim: offset, ...}, optional
4773             The keyword arguments form of ``shifts``.
4774             One of shifts or shifts_kwargs must be provided.
4775 
4776         Returns
4777         -------
4778         rolled : DataArray
4779             DataArray with the same attributes but rolled data and coordinates.
4780 
4781         See Also
4782         --------
4783         shift
4784 
4785         Examples
4786         --------
4787         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4788         >>> arr.roll(x=1)
4789         <xarray.DataArray (x: 3)>
4790         array([7, 5, 6])
4791         Dimensions without coordinates: x
4792         """
4793         ds = self._to_temp_dataset().roll(
4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4795         )
4796         return self._from_temp_dataset(ds)
4797 
4798     @property
4799     def real(self: T_DataArray) -> T_DataArray:
4800         """
4801         The real part of the array.
4802 
4803         See Also
4804         --------
4805         numpy.ndarray.real
4806         """
4807         return self._replace(self.variable.real)
4808 
4809     @property
4810     def imag(self: T_DataArray) -> T_DataArray:
4811         """
4812         The imaginary part of the array.
4813 
4814         See Also
4815         --------
4816         numpy.ndarray.imag
4817         """
4818         return self._replace(self.variable.imag)
4819 
4820     def dot(
4821         self: T_DataArray,
4822         other: T_DataArray,
4823         dims: Dims = None,
4824     ) -> T_DataArray:
4825         """Perform dot product of two DataArrays along their shared dims.
4826 
4827         Equivalent to taking taking tensordot over all shared dims.
4828 
4829         Parameters
4830         ----------
4831         other : DataArray
4832             The other array with which the dot product is performed.
4833         dims : ..., str, Iterable of Hashable or None, optional
4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4835             If not specified, then all the common dimensions are summed over.
4836 
4837         Returns
4838         -------
4839         result : DataArray
4840             Array resulting from the dot product over all shared dimensions.
4841 
4842         See Also
4843         --------
4844         dot
4845         numpy.tensordot
4846 
4847         Examples
4848         --------
4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4850         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4851         >>> dm_vals = np.arange(4)
4852         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4853 
4854         >>> dm.dims
4855         ('z',)
4856 
4857         >>> da.dims
4858         ('x', 'y', 'z')
4859 
4860         >>> dot_result = da.dot(dm)
4861         >>> dot_result.dims
4862         ('x', 'y')
4863 
4864         """
4865         if isinstance(other, Dataset):
4866             raise NotImplementedError(
4867                 "dot products are not yet supported with Dataset objects."
4868             )
4869         if not isinstance(other, DataArray):
4870             raise TypeError("dot only operates on DataArrays.")
4871 
4872         return computation.dot(self, other, dims=dims)
4873 
4874     # change type of self and return to T_DataArray once
4875     # https://github.com/python/mypy/issues/12846 is resolved
4876     def sortby(
4877         self,
4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4879         ascending: bool = True,
4880     ) -> DataArray:
4881         """Sort object by labels or values (along an axis).
4882 
4883         Sorts the dataarray, either along specified dimensions,
4884         or according to values of 1-D dataarrays that share dimension
4885         with calling object.
4886 
4887         If the input variables are dataarrays, then the dataarrays are aligned
4888         (via left-join) to the calling object prior to sorting by cell values.
4889         NaNs are sorted to the end, following Numpy convention.
4890 
4891         If multiple sorts along the same dimension is
4892         given, numpy's lexsort is performed along that dimension:
4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4894         and the FIRST key in the sequence is used as the primary sort key,
4895         followed by the 2nd key, etc.
4896 
4897         Parameters
4898         ----------
4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4900             1D DataArray objects or name(s) of 1D variable(s) in
4901             coords whose values are used to sort this array.
4902         ascending : bool, default: True
4903             Whether to sort by ascending or descending order.
4904 
4905         Returns
4906         -------
4907         sorted : DataArray
4908             A new dataarray where all the specified dims are sorted by dim
4909             labels.
4910 
4911         See Also
4912         --------
4913         Dataset.sortby
4914         numpy.sort
4915         pandas.sort_values
4916         pandas.sort_index
4917 
4918         Examples
4919         --------
4920         >>> da = xr.DataArray(
4921         ...     np.random.rand(5),
4922         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4923         ...     dims="time",
4924         ... )
4925         >>> da
4926         <xarray.DataArray (time: 5)>
4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4928         Coordinates:
4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4930 
4931         >>> da.sortby(da)
4932         <xarray.DataArray (time: 5)>
4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4934         Coordinates:
4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4936         """
4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4938         return self._from_temp_dataset(ds)
4939 
4940     def quantile(
4941         self: T_DataArray,
4942         q: ArrayLike,
4943         dim: Dims = None,
4944         method: QuantileMethods = "linear",
4945         keep_attrs: bool | None = None,
4946         skipna: bool | None = None,
4947         interpolation: QuantileMethods | None = None,
4948     ) -> T_DataArray:
4949         """Compute the qth quantile of the data along the specified dimension.
4950 
4951         Returns the qth quantiles(s) of the array elements.
4952 
4953         Parameters
4954         ----------
4955         q : float or array-like of float
4956             Quantile to compute, which must be between 0 and 1 inclusive.
4957         dim : str or Iterable of Hashable, optional
4958             Dimension(s) over which to apply quantile.
4959         method : str, default: "linear"
4960             This optional parameter specifies the interpolation method to use when the
4961             desired quantile lies between two data points. The options sorted by their R
4962             type as summarized in the H&F paper [1]_ are:
4963 
4964                 1. "inverted_cdf" (*)
4965                 2. "averaged_inverted_cdf" (*)
4966                 3. "closest_observation" (*)
4967                 4. "interpolated_inverted_cdf" (*)
4968                 5. "hazen" (*)
4969                 6. "weibull" (*)
4970                 7. "linear"  (default)
4971                 8. "median_unbiased" (*)
4972                 9. "normal_unbiased" (*)
4973 
4974             The first three methods are discontiuous. The following discontinuous
4975             variations of the default "linear" (7.) option are also available:
4976 
4977                 * "lower"
4978                 * "higher"
4979                 * "midpoint"
4980                 * "nearest"
4981 
4982             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4983             was previously called "interpolation", renamed in accordance with numpy
4984             version 1.22.0.
4985 
4986             (*) These methods require numpy version 1.22 or newer.
4987 
4988         keep_attrs : bool or None, optional
4989             If True, the dataset's attributes (`attrs`) will be copied from
4990             the original object to the new one.  If False (default), the new
4991             object will be returned without attributes.
4992         skipna : bool or None, optional
4993             If True, skip missing values (as marked by NaN). By default, only
4994             skips missing values for float dtypes; other dtypes either do not
4995             have a sentinel missing value (int) or skipna=True has not been
4996             implemented (object, datetime64 or timedelta64).
4997 
4998         Returns
4999         -------
5000         quantiles : DataArray
5001             If `q` is a single quantile, then the result
5002             is a scalar. If multiple percentiles are given, first axis of
5003             the result corresponds to the quantile and a quantile dimension
5004             is added to the return array. The other dimensions are the
5005             dimensions that remain after the reduction of the array.
5006 
5007         See Also
5008         --------
5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
5010 
5011         Examples
5012         --------
5013         >>> da = xr.DataArray(
5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
5015         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5016         ...     dims=("x", "y"),
5017         ... )
5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)
5019         <xarray.DataArray ()>
5020         array(0.7)
5021         Coordinates:
5022             quantile  float64 0.0
5023         >>> da.quantile(0, dim="x")
5024         <xarray.DataArray (y: 4)>
5025         array([0.7, 4.2, 2.6, 1.5])
5026         Coordinates:
5027           * y         (y) float64 1.0 1.5 2.0 2.5
5028             quantile  float64 0.0
5029         >>> da.quantile([0, 0.5, 1])
5030         <xarray.DataArray (quantile: 3)>
5031         array([0.7, 3.4, 9.4])
5032         Coordinates:
5033           * quantile  (quantile) float64 0.0 0.5 1.0
5034         >>> da.quantile([0, 0.5, 1], dim="x")
5035         <xarray.DataArray (quantile: 3, y: 4)>
5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],
5037                [3.6 , 5.75, 6.  , 1.7 ],
5038                [6.5 , 7.3 , 9.4 , 1.9 ]])
5039         Coordinates:
5040           * y         (y) float64 1.0 1.5 2.0 2.5
5041           * quantile  (quantile) float64 0.0 0.5 1.0
5042 
5043         References
5044         ----------
5045         .. [1] R. J. Hyndman and Y. Fan,
5046            "Sample quantiles in statistical packages,"
5047            The American Statistician, 50(4), pp. 361-365, 1996
5048         """
5049 
5050         ds = self._to_temp_dataset().quantile(
5051             q,
5052             dim=dim,
5053             keep_attrs=keep_attrs,
5054             method=method,
5055             skipna=skipna,
5056             interpolation=interpolation,
5057         )
5058         return self._from_temp_dataset(ds)
5059 
5060     def rank(
5061         self: T_DataArray,
5062         dim: Hashable,
5063         pct: bool = False,
5064         keep_attrs: bool | None = None,
5065     ) -> T_DataArray:
5066         """Ranks the data.
5067 
5068         Equal values are assigned a rank that is the average of the ranks that
5069         would have been otherwise assigned to all of the values within that
5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
5071 
5072         NaNs in the input array are returned as NaNs.
5073 
5074         The `bottleneck` library is required.
5075 
5076         Parameters
5077         ----------
5078         dim : Hashable
5079             Dimension over which to compute rank.
5080         pct : bool, default: False
5081             If True, compute percentage ranks, otherwise compute integer ranks.
5082         keep_attrs : bool or None, optional
5083             If True, the dataset's attributes (`attrs`) will be copied from
5084             the original object to the new one.  If False (default), the new
5085             object will be returned without attributes.
5086 
5087         Returns
5088         -------
5089         ranked : DataArray
5090             DataArray with the same coordinates and dtype 'float64'.
5091 
5092         Examples
5093         --------
5094         >>> arr = xr.DataArray([5, 6, 7], dims="x")
5095         >>> arr.rank("x")
5096         <xarray.DataArray (x: 3)>
5097         array([1., 2., 3.])
5098         Dimensions without coordinates: x
5099         """
5100 
5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
5102         return self._from_temp_dataset(ds)
5103 
5104     def differentiate(
5105         self: T_DataArray,
5106         coord: Hashable,
5107         edge_order: Literal[1, 2] = 1,
5108         datetime_unit: DatetimeUnitOptions = None,
5109     ) -> T_DataArray:
5110         """ Differentiate the array with the second order accurate central
5111         differences.
5112 
5113         .. note::
5114             This feature is limited to simple cartesian geometry, i.e. coord
5115             must be one dimensional.
5116 
5117         Parameters
5118         ----------
5119         coord : Hashable
5120             The coordinate to be used to compute the gradient.
5121         edge_order : {1, 2}, default: 1
5122             N-th order accurate differences at the boundaries.
5123         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5124                          "us", "ns", "ps", "fs", "as", None}, optional
5125             Unit to compute gradient. Only valid for datetime coordinate.
5126 
5127         Returns
5128         -------
5129         differentiated: DataArray
5130 
5131         See also
5132         --------
5133         numpy.gradient: corresponding numpy function
5134 
5135         Examples
5136         --------
5137 
5138         >>> da = xr.DataArray(
5139         ...     np.arange(12).reshape(4, 3),
5140         ...     dims=["x", "y"],
5141         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5142         ... )
5143         >>> da
5144         <xarray.DataArray (x: 4, y: 3)>
5145         array([[ 0,  1,  2],
5146                [ 3,  4,  5],
5147                [ 6,  7,  8],
5148                [ 9, 10, 11]])
5149         Coordinates:
5150           * x        (x) float64 0.0 0.1 1.1 1.2
5151         Dimensions without coordinates: y
5152         >>>
5153         >>> da.differentiate("x")
5154         <xarray.DataArray (x: 4, y: 3)>
5155         array([[30.        , 30.        , 30.        ],
5156                [27.54545455, 27.54545455, 27.54545455],
5157                [27.54545455, 27.54545455, 27.54545455],
5158                [30.        , 30.        , 30.        ]])
5159         Coordinates:
5160           * x        (x) float64 0.0 0.1 1.1 1.2
5161         Dimensions without coordinates: y
5162         """
5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
5164         return self._from_temp_dataset(ds)
5165 
5166     # change type of self and return to T_DataArray once
5167     # https://github.com/python/mypy/issues/12846 is resolved
5168     def integrate(
5169         self,
5170         coord: Hashable | Sequence[Hashable] = None,
5171         datetime_unit: DatetimeUnitOptions = None,
5172     ) -> DataArray:
5173         """Integrate along the given coordinate using the trapezoidal rule.
5174 
5175         .. note::
5176             This feature is limited to simple cartesian geometry, i.e. coord
5177             must be one dimensional.
5178 
5179         Parameters
5180         ----------
5181         coord : Hashable, or sequence of Hashable
5182             Coordinate(s) used for the integration.
5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5184                         'ps', 'fs', 'as', None}, optional
5185             Specify the unit if a datetime coordinate is used.
5186 
5187         Returns
5188         -------
5189         integrated : DataArray
5190 
5191         See also
5192         --------
5193         Dataset.integrate
5194         numpy.trapz : corresponding numpy function
5195 
5196         Examples
5197         --------
5198 
5199         >>> da = xr.DataArray(
5200         ...     np.arange(12).reshape(4, 3),
5201         ...     dims=["x", "y"],
5202         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5203         ... )
5204         >>> da
5205         <xarray.DataArray (x: 4, y: 3)>
5206         array([[ 0,  1,  2],
5207                [ 3,  4,  5],
5208                [ 6,  7,  8],
5209                [ 9, 10, 11]])
5210         Coordinates:
5211           * x        (x) float64 0.0 0.1 1.1 1.2
5212         Dimensions without coordinates: y
5213         >>>
5214         >>> da.integrate("x")
5215         <xarray.DataArray (y: 3)>
5216         array([5.4, 6.6, 7.8])
5217         Dimensions without coordinates: y
5218         """
5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5220         return self._from_temp_dataset(ds)
5221 
5222     # change type of self and return to T_DataArray once
5223     # https://github.com/python/mypy/issues/12846 is resolved
5224     def cumulative_integrate(
5225         self,
5226         coord: Hashable | Sequence[Hashable] = None,
5227         datetime_unit: DatetimeUnitOptions = None,
5228     ) -> DataArray:
5229         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5230 
5231         .. note::
5232             This feature is limited to simple cartesian geometry, i.e. coord
5233             must be one dimensional.
5234 
5235             The first entry of the cumulative integral is always 0, in order to keep the
5236             length of the dimension unchanged between input and output.
5237 
5238         Parameters
5239         ----------
5240         coord : Hashable, or sequence of Hashable
5241             Coordinate(s) used for the integration.
5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5243                         'ps', 'fs', 'as', None}, optional
5244             Specify the unit if a datetime coordinate is used.
5245 
5246         Returns
5247         -------
5248         integrated : DataArray
5249 
5250         See also
5251         --------
5252         Dataset.cumulative_integrate
5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5254 
5255         Examples
5256         --------
5257 
5258         >>> da = xr.DataArray(
5259         ...     np.arange(12).reshape(4, 3),
5260         ...     dims=["x", "y"],
5261         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5262         ... )
5263         >>> da
5264         <xarray.DataArray (x: 4, y: 3)>
5265         array([[ 0,  1,  2],
5266                [ 3,  4,  5],
5267                [ 6,  7,  8],
5268                [ 9, 10, 11]])
5269         Coordinates:
5270           * x        (x) float64 0.0 0.1 1.1 1.2
5271         Dimensions without coordinates: y
5272         >>>
5273         >>> da.cumulative_integrate("x")
5274         <xarray.DataArray (x: 4, y: 3)>
5275         array([[0.  , 0.  , 0.  ],
5276                [0.15, 0.25, 0.35],
5277                [4.65, 5.75, 6.85],
5278                [5.4 , 6.6 , 7.8 ]])
5279         Coordinates:
5280           * x        (x) float64 0.0 0.1 1.1 1.2
5281         Dimensions without coordinates: y
5282         """
5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5284         return self._from_temp_dataset(ds)
5285 
5286     def unify_chunks(self) -> DataArray:
5287         """Unify chunk size along all chunked dimensions of this DataArray.
5288 
5289         Returns
5290         -------
5291         DataArray with consistent chunk sizes for all dask-array variables
5292 
5293         See Also
5294         --------
5295         dask.array.core.unify_chunks
5296         """
5297 
5298         return unify_chunks(self)[0]
5299 
5300     def map_blocks(
5301         self,
5302         func: Callable[..., T_Xarray],
5303         args: Sequence[Any] = (),
5304         kwargs: Mapping[str, Any] | None = None,
5305         template: DataArray | Dataset | None = None,
5306     ) -> T_Xarray:
5307         """
5308         Apply a function to each block of this DataArray.
5309 
5310         .. warning::
5311             This method is experimental and its signature may change.
5312 
5313         Parameters
5314         ----------
5315         func : callable
5316             User-provided function that accepts a DataArray as its first
5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5318             corresponding to one chunk along each chunked dimension. ``func`` will be
5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5320 
5321             This function must return either a single DataArray or a single Dataset.
5322 
5323             This function cannot add a new chunked dimension.
5324         args : sequence
5325             Passed to func after unpacking and subsetting any xarray objects by blocks.
5326             xarray objects in args must be aligned with this object, otherwise an error is raised.
5327         kwargs : mapping
5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5329             subset to blocks. Passing dask collections in kwargs is not allowed.
5330         template : DataArray or Dataset, optional
5331             xarray object representing the final result after compute is called. If not provided,
5332             the function will be first run on mocked-up data, that looks like this object but
5333             has sizes 0, to determine properties of the returned object such as dtype,
5334             variable names, attributes, new dimensions and new indexes (if any).
5335             ``template`` must be provided if the function changes the size of existing dimensions.
5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5337             ``attrs`` set by ``func`` will be ignored.
5338 
5339         Returns
5340         -------
5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5342         function.
5343 
5344         Notes
5345         -----
5346         This function is designed for when ``func`` needs to manipulate a whole xarray object
5347         subset to each block. Each block is loaded into memory. In the more common case where
5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5349 
5350         If none of the variables in this object is backed by dask arrays, calling this function is
5351         equivalent to calling ``func(obj, *args, **kwargs)``.
5352 
5353         See Also
5354         --------
5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5356         xarray.DataArray.map_blocks
5357 
5358         Examples
5359         --------
5360         Calculate an anomaly from climatology using ``.groupby()``. Using
5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5362         its indices, and its methods like ``.groupby()``.
5363 
5364         >>> def calculate_anomaly(da, groupby_type="time.month"):
5365         ...     gb = da.groupby(groupby_type)
5366         ...     clim = gb.mean(dim="time")
5367         ...     return gb - clim
5368         ...
5369         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5370         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5371         >>> np.random.seed(123)
5372         >>> array = xr.DataArray(
5373         ...     np.random.rand(len(time)),
5374         ...     dims=["time"],
5375         ...     coords={"time": time, "month": month},
5376         ... ).chunk()
5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5378         <xarray.DataArray (time: 24)>
5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5384         Coordinates:
5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5387 
5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5389         to the function being applied in ``xr.map_blocks()``:
5390 
5391         >>> array.map_blocks(
5392         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5393         ... )  # doctest: +ELLIPSIS
5394         <xarray.DataArray (time: 24)>
5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5396         Coordinates:
5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5399         """
5400         from xarray.core.parallel import map_blocks
5401 
5402         return map_blocks(func, self, args, kwargs, template)
5403 
5404     def polyfit(
5405         self,
5406         dim: Hashable,
5407         deg: int,
5408         skipna: bool | None = None,
5409         rcond: float | None = None,
5410         w: Hashable | Any | None = None,
5411         full: bool = False,
5412         cov: bool | Literal["unscaled"] = False,
5413     ) -> Dataset:
5414         """
5415         Least squares polynomial fit.
5416 
5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5418         invalid values when `skipna = True`.
5419 
5420         Parameters
5421         ----------
5422         dim : Hashable
5423             Coordinate along which to fit the polynomials.
5424         deg : int
5425             Degree of the fitting polynomial.
5426         skipna : bool or None, optional
5427             If True, removes all invalid values before fitting each 1D slices of the array.
5428             Default is True if data is stored in a dask.array or if there is any
5429             invalid values, False otherwise.
5430         rcond : float or None, optional
5431             Relative condition number to the fit.
5432         w : Hashable, array-like or None, optional
5433             Weights to apply to the y-coordinate of the sample points.
5434             Can be an array-like object or the name of a coordinate in the dataset.
5435         full : bool, default: False
5436             Whether to return the residuals, matrix rank and singular values in addition
5437             to the coefficients.
5438         cov : bool or "unscaled", default: False
5439             Whether to return to the covariance matrix in addition to the coefficients.
5440             The matrix is not scaled if `cov='unscaled'`.
5441 
5442         Returns
5443         -------
5444         polyfit_results : Dataset
5445             A single dataset which contains:
5446 
5447             polyfit_coefficients
5448                 The coefficients of the best fit.
5449             polyfit_residuals
5450                 The residuals of the least-square computation (only included if `full=True`).
5451                 When the matrix rank is deficient, np.nan is returned.
5452             [dim]_matrix_rank
5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5454             [dim]_singular_value
5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5456             polyfit_covariance
5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5458 
5459         See Also
5460         --------
5461         numpy.polyfit
5462         numpy.polyval
5463         xarray.polyval
5464         """
5465         return self._to_temp_dataset().polyfit(
5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5467         )
5468 
5469     def pad(
5470         self: T_DataArray,
5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5472         mode: PadModeOptions = "constant",
5473         stat_length: int
5474         | tuple[int, int]
5475         | Mapping[Any, tuple[int, int]]
5476         | None = None,
5477         constant_values: float
5478         | tuple[float, float]
5479         | Mapping[Any, tuple[float, float]]
5480         | None = None,
5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5482         reflect_type: PadReflectOptions = None,
5483         keep_attrs: bool | None = None,
5484         **pad_width_kwargs: Any,
5485     ) -> T_DataArray:
5486         """Pad this array along one or more dimensions.
5487 
5488         .. warning::
5489             This function is experimental and its behaviour is likely to change
5490             especially regarding padding of dimension coordinates (or IndexVariables).
5491 
5492         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5493         coordinates will be padded with the same mode, otherwise coordinates
5494         are padded using the "constant" mode with fill_value dtypes.NA.
5495 
5496         Parameters
5497         ----------
5498         pad_width : mapping of Hashable to tuple of int
5499             Mapping with the form of {dim: (pad_before, pad_after)}
5500             describing the number of values padded along each dimension.
5501             {dim: pad} is a shortcut for pad_before = pad_after = pad
5502         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5503             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5504             How to pad the DataArray (taken from numpy docs):
5505 
5506             - "constant": Pads with a constant value.
5507             - "edge": Pads with the edge values of array.
5508             - "linear_ramp": Pads with the linear ramp between end_value and the
5509               array edge value.
5510             - "maximum": Pads with the maximum value of all or part of the
5511               vector along each axis.
5512             - "mean": Pads with the mean value of all or part of the
5513               vector along each axis.
5514             - "median": Pads with the median value of all or part of the
5515               vector along each axis.
5516             - "minimum": Pads with the minimum value of all or part of the
5517               vector along each axis.
5518             - "reflect": Pads with the reflection of the vector mirrored on
5519               the first and last values of the vector along each axis.
5520             - "symmetric": Pads with the reflection of the vector mirrored
5521               along the edge of the array.
5522             - "wrap": Pads with the wrap of the vector along the axis.
5523               The first values are used to pad the end and the
5524               end values are used to pad the beginning.
5525 
5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5528             values at edge of each axis used to calculate the statistic value.
5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5530             statistic lengths along each dimension.
5531             ((before, after),) yields same before and after statistic lengths
5532             for each dimension.
5533             (stat_length,) or int is a shortcut for before = after = statistic
5534             length for all axes.
5535             Default is ``None``, to use the entire axis.
5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5537             Used in 'constant'.  The values to set the padded values for each
5538             axis.
5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5540             pad constants along each dimension.
5541             ``((before, after),)`` yields same before and after constants for each
5542             dimension.
5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5544             all dimensions.
5545             Default is 0.
5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5547             Used in 'linear_ramp'.  The values used for the ending value of the
5548             linear_ramp and that will form the edge of the padded array.
5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5550             end values along each dimension.
5551             ``((before, after),)`` yields same before and after end values for each
5552             axis.
5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5554             all axes.
5555             Default is 0.
5556         reflect_type : {"even", "odd", None}, optional
5557             Used in "reflect", and "symmetric". The "even" style is the
5558             default with an unaltered reflection around the edge value. For
5559             the "odd" style, the extended part of the array is created by
5560             subtracting the reflected values from two times the edge value.
5561         keep_attrs : bool or None, optional
5562             If True, the attributes (``attrs``) will be copied from the
5563             original object to the new one. If False, the new object
5564             will be returned without attributes.
5565         **pad_width_kwargs
5566             The keyword arguments form of ``pad_width``.
5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5568 
5569         Returns
5570         -------
5571         padded : DataArray
5572             DataArray with the padded coordinates and data.
5573 
5574         See Also
5575         --------
5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5577 
5578         Notes
5579         -----
5580         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5581         promoted to ``float`` and padded with ``np.nan``.
5582 
5583         Padding coordinates will drop their corresponding index (if any) and will reset default
5584         indexes for dimension coordinates.
5585 
5586         Examples
5587         --------
5588         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5589         >>> arr.pad(x=(1, 2), constant_values=0)
5590         <xarray.DataArray (x: 6)>
5591         array([0, 5, 6, 7, 0, 0])
5592         Coordinates:
5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5594 
5595         >>> da = xr.DataArray(
5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5597         ...     dims=["x", "y"],
5598         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5599         ... )
5600         >>> da.pad(x=1)
5601         <xarray.DataArray (x: 4, y: 4)>
5602         array([[nan, nan, nan, nan],
5603                [ 0.,  1.,  2.,  3.],
5604                [10., 11., 12., 13.],
5605                [nan, nan, nan, nan]])
5606         Coordinates:
5607           * x        (x) float64 nan 0.0 1.0 nan
5608           * y        (y) int64 10 20 30 40
5609             z        (x) float64 nan 100.0 200.0 nan
5610 
5611         Careful, ``constant_values`` are coerced to the data type of the array which may
5612         lead to a loss of precision:
5613 
5614         >>> da.pad(x=1, constant_values=1.23456789)
5615         <xarray.DataArray (x: 4, y: 4)>
5616         array([[ 1,  1,  1,  1],
5617                [ 0,  1,  2,  3],
5618                [10, 11, 12, 13],
5619                [ 1,  1,  1,  1]])
5620         Coordinates:
5621           * x        (x) float64 nan 0.0 1.0 nan
5622           * y        (y) int64 10 20 30 40
5623             z        (x) float64 nan 100.0 200.0 nan
5624         """
5625         ds = self._to_temp_dataset().pad(
5626             pad_width=pad_width,
5627             mode=mode,
5628             stat_length=stat_length,
5629             constant_values=constant_values,
5630             end_values=end_values,
5631             reflect_type=reflect_type,
5632             keep_attrs=keep_attrs,
5633             **pad_width_kwargs,
5634         )
5635         return self._from_temp_dataset(ds)
5636 
5637     def idxmin(
5638         self,
5639         dim: Hashable | None = None,
5640         skipna: bool | None = None,
5641         fill_value: Any = dtypes.NA,
5642         keep_attrs: bool | None = None,
5643     ) -> DataArray:
5644         """Return the coordinate label of the minimum value along a dimension.
5645 
5646         Returns a new `DataArray` named after the dimension with the values of
5647         the coordinate labels along that dimension corresponding to minimum
5648         values along that dimension.
5649 
5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5652 
5653         Parameters
5654         ----------
5655         dim : str, optional
5656             Dimension over which to apply `idxmin`.  This is optional for 1D
5657             arrays, but required for arrays with 2 or more dimensions.
5658         skipna : bool or None, default: None
5659             If True, skip missing values (as marked by NaN). By default, only
5660             skips missing values for ``float``, ``complex``, and ``object``
5661             dtypes; other dtypes either do not have a sentinel missing value
5662             (``int``) or ``skipna=True`` has not been implemented
5663             (``datetime64`` or ``timedelta64``).
5664         fill_value : Any, default: NaN
5665             Value to be filled in case all of the values along a dimension are
5666             null.  By default this is NaN.  The fill value and result are
5667             automatically converted to a compatible dtype if possible.
5668             Ignored if ``skipna`` is False.
5669         keep_attrs : bool or None, optional
5670             If True, the attributes (``attrs``) will be copied from the
5671             original object to the new one. If False, the new object
5672             will be returned without attributes.
5673 
5674         Returns
5675         -------
5676         reduced : DataArray
5677             New `DataArray` object with `idxmin` applied to its data and the
5678             indicated dimension removed.
5679 
5680         See Also
5681         --------
5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5683 
5684         Examples
5685         --------
5686         >>> array = xr.DataArray(
5687         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5688         ... )
5689         >>> array.min()
5690         <xarray.DataArray ()>
5691         array(-2)
5692         >>> array.argmin(...)
5693         {'x': <xarray.DataArray ()>
5694         array(4)}
5695         >>> array.idxmin()
5696         <xarray.DataArray 'x' ()>
5697         array('e', dtype='<U1')
5698 
5699         >>> array = xr.DataArray(
5700         ...     [
5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5704         ...     ],
5705         ...     dims=["y", "x"],
5706         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5707         ... )
5708         >>> array.min(dim="x")
5709         <xarray.DataArray (y: 3)>
5710         array([-2., -4.,  1.])
5711         Coordinates:
5712           * y        (y) int64 -1 0 1
5713         >>> array.argmin(dim="x")
5714         <xarray.DataArray (y: 3)>
5715         array([4, 0, 2])
5716         Coordinates:
5717           * y        (y) int64 -1 0 1
5718         >>> array.idxmin(dim="x")
5719         <xarray.DataArray 'x' (y: 3)>
5720         array([16.,  0.,  4.])
5721         Coordinates:
5722           * y        (y) int64 -1 0 1
5723         """
5724         return computation._calc_idxminmax(
5725             array=self,
5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5727             dim=dim,
5728             skipna=skipna,
5729             fill_value=fill_value,
5730             keep_attrs=keep_attrs,
5731         )
5732 
5733     def idxmax(
5734         self,
5735         dim: Hashable = None,
5736         skipna: bool | None = None,
5737         fill_value: Any = dtypes.NA,
5738         keep_attrs: bool | None = None,
5739     ) -> DataArray:
5740         """Return the coordinate label of the maximum value along a dimension.
5741 
5742         Returns a new `DataArray` named after the dimension with the values of
5743         the coordinate labels along that dimension corresponding to maximum
5744         values along that dimension.
5745 
5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5748 
5749         Parameters
5750         ----------
5751         dim : Hashable, optional
5752             Dimension over which to apply `idxmax`.  This is optional for 1D
5753             arrays, but required for arrays with 2 or more dimensions.
5754         skipna : bool or None, default: None
5755             If True, skip missing values (as marked by NaN). By default, only
5756             skips missing values for ``float``, ``complex``, and ``object``
5757             dtypes; other dtypes either do not have a sentinel missing value
5758             (``int``) or ``skipna=True`` has not been implemented
5759             (``datetime64`` or ``timedelta64``).
5760         fill_value : Any, default: NaN
5761             Value to be filled in case all of the values along a dimension are
5762             null.  By default this is NaN.  The fill value and result are
5763             automatically converted to a compatible dtype if possible.
5764             Ignored if ``skipna`` is False.
5765         keep_attrs : bool or None, optional
5766             If True, the attributes (``attrs``) will be copied from the
5767             original object to the new one. If False, the new object
5768             will be returned without attributes.
5769 
5770         Returns
5771         -------
5772         reduced : DataArray
5773             New `DataArray` object with `idxmax` applied to its data and the
5774             indicated dimension removed.
5775 
5776         See Also
5777         --------
5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5779 
5780         Examples
5781         --------
5782         >>> array = xr.DataArray(
5783         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5784         ... )
5785         >>> array.max()
5786         <xarray.DataArray ()>
5787         array(2)
5788         >>> array.argmax(...)
5789         {'x': <xarray.DataArray ()>
5790         array(1)}
5791         >>> array.idxmax()
5792         <xarray.DataArray 'x' ()>
5793         array('b', dtype='<U1')
5794 
5795         >>> array = xr.DataArray(
5796         ...     [
5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5800         ...     ],
5801         ...     dims=["y", "x"],
5802         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5803         ... )
5804         >>> array.max(dim="x")
5805         <xarray.DataArray (y: 3)>
5806         array([2., 2., 1.])
5807         Coordinates:
5808           * y        (y) int64 -1 0 1
5809         >>> array.argmax(dim="x")
5810         <xarray.DataArray (y: 3)>
5811         array([0, 2, 2])
5812         Coordinates:
5813           * y        (y) int64 -1 0 1
5814         >>> array.idxmax(dim="x")
5815         <xarray.DataArray 'x' (y: 3)>
5816         array([0., 4., 4.])
5817         Coordinates:
5818           * y        (y) int64 -1 0 1
5819         """
5820         return computation._calc_idxminmax(
5821             array=self,
5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5823             dim=dim,
5824             skipna=skipna,
5825             fill_value=fill_value,
5826             keep_attrs=keep_attrs,
5827         )
5828 
5829     # change type of self and return to T_DataArray once
5830     # https://github.com/python/mypy/issues/12846 is resolved
5831     def argmin(
5832         self,
5833         dim: Dims = None,
5834         axis: int | None = None,
5835         keep_attrs: bool | None = None,
5836         skipna: bool | None = None,
5837     ) -> DataArray | dict[Hashable, DataArray]:
5838         """Index or indices of the minimum of the DataArray over one or more dimensions.
5839 
5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5841         which can be passed directly to isel(). If a single str is passed to 'dim' then
5842         returns a DataArray with dtype int.
5843 
5844         If there are multiple minima, the indices of the first one found will be
5845         returned.
5846 
5847         Parameters
5848         ----------
5849         dim : "...", str, Iterable of Hashable or None, optional
5850             The dimensions over which to find the minimum. By default, finds minimum over
5851             all dimensions - for now returning an int for backward compatibility, but
5852             this is deprecated, in future will return a dict with indices for all
5853             dimensions; to return a dict with all dimensions now, pass '...'.
5854         axis : int or None, optional
5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5856             can be supplied.
5857         keep_attrs : bool or None, optional
5858             If True, the attributes (`attrs`) will be copied from the original
5859             object to the new one. If False, the new object will be
5860             returned without attributes.
5861         skipna : bool or None, optional
5862             If True, skip missing values (as marked by NaN). By default, only
5863             skips missing values for float dtypes; other dtypes either do not
5864             have a sentinel missing value (int) or skipna=True has not been
5865             implemented (object, datetime64 or timedelta64).
5866 
5867         Returns
5868         -------
5869         result : DataArray or dict of DataArray
5870 
5871         See Also
5872         --------
5873         Variable.argmin, DataArray.idxmin
5874 
5875         Examples
5876         --------
5877         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5878         >>> array.min()
5879         <xarray.DataArray ()>
5880         array(-1)
5881         >>> array.argmin(...)
5882         {'x': <xarray.DataArray ()>
5883         array(2)}
5884         >>> array.isel(array.argmin(...))
5885         <xarray.DataArray ()>
5886         array(-1)
5887 
5888         >>> array = xr.DataArray(
5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5890         ...     dims=("x", "y", "z"),
5891         ... )
5892         >>> array.min(dim="x")
5893         <xarray.DataArray (y: 3, z: 3)>
5894         array([[ 1,  2,  1],
5895                [ 2, -5,  1],
5896                [ 2,  1,  1]])
5897         Dimensions without coordinates: y, z
5898         >>> array.argmin(dim="x")
5899         <xarray.DataArray (y: 3, z: 3)>
5900         array([[1, 0, 0],
5901                [1, 1, 1],
5902                [0, 0, 1]])
5903         Dimensions without coordinates: y, z
5904         >>> array.argmin(dim=["x"])
5905         {'x': <xarray.DataArray (y: 3, z: 3)>
5906         array([[1, 0, 0],
5907                [1, 1, 1],
5908                [0, 0, 1]])
5909         Dimensions without coordinates: y, z}
5910         >>> array.min(dim=("x", "z"))
5911         <xarray.DataArray (y: 3)>
5912         array([ 1, -5,  1])
5913         Dimensions without coordinates: y
5914         >>> array.argmin(dim=["x", "z"])
5915         {'x': <xarray.DataArray (y: 3)>
5916         array([0, 1, 0])
5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5918         array([2, 1, 1])
5919         Dimensions without coordinates: y}
5920         >>> array.isel(array.argmin(dim=["x", "z"]))
5921         <xarray.DataArray (y: 3)>
5922         array([ 1, -5,  1])
5923         Dimensions without coordinates: y
5924         """
5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5926         if isinstance(result, dict):
5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5928         else:
5929             return self._replace_maybe_drop_dims(result)
5930 
5931     # change type of self and return to T_DataArray once
5932     # https://github.com/python/mypy/issues/12846 is resolved
5933     def argmax(
5934         self,
5935         dim: Dims = None,
5936         axis: int | None = None,
5937         keep_attrs: bool | None = None,
5938         skipna: bool | None = None,
5939     ) -> DataArray | dict[Hashable, DataArray]:
5940         """Index or indices of the maximum of the DataArray over one or more dimensions.
5941 
5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5943         which can be passed directly to isel(). If a single str is passed to 'dim' then
5944         returns a DataArray with dtype int.
5945 
5946         If there are multiple maxima, the indices of the first one found will be
5947         returned.
5948 
5949         Parameters
5950         ----------
5951         dim : "...", str, Iterable of Hashable or None, optional
5952             The dimensions over which to find the maximum. By default, finds maximum over
5953             all dimensions - for now returning an int for backward compatibility, but
5954             this is deprecated, in future will return a dict with indices for all
5955             dimensions; to return a dict with all dimensions now, pass '...'.
5956         axis : int or None, optional
5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5958             can be supplied.
5959         keep_attrs : bool or None, optional
5960             If True, the attributes (`attrs`) will be copied from the original
5961             object to the new one. If False, the new object will be
5962             returned without attributes.
5963         skipna : bool or None, optional
5964             If True, skip missing values (as marked by NaN). By default, only
5965             skips missing values for float dtypes; other dtypes either do not
5966             have a sentinel missing value (int) or skipna=True has not been
5967             implemented (object, datetime64 or timedelta64).
5968 
5969         Returns
5970         -------
5971         result : DataArray or dict of DataArray
5972 
5973         See Also
5974         --------
5975         Variable.argmax, DataArray.idxmax
5976 
5977         Examples
5978         --------
5979         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5980         >>> array.max()
5981         <xarray.DataArray ()>
5982         array(3)
5983         >>> array.argmax(...)
5984         {'x': <xarray.DataArray ()>
5985         array(3)}
5986         >>> array.isel(array.argmax(...))
5987         <xarray.DataArray ()>
5988         array(3)
5989 
5990         >>> array = xr.DataArray(
5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5992         ...     dims=("x", "y", "z"),
5993         ... )
5994         >>> array.max(dim="x")
5995         <xarray.DataArray (y: 3, z: 3)>
5996         array([[3, 3, 2],
5997                [3, 5, 2],
5998                [2, 3, 3]])
5999         Dimensions without coordinates: y, z
6000         >>> array.argmax(dim="x")
6001         <xarray.DataArray (y: 3, z: 3)>
6002         array([[0, 1, 1],
6003                [0, 1, 0],
6004                [0, 1, 0]])
6005         Dimensions without coordinates: y, z
6006         >>> array.argmax(dim=["x"])
6007         {'x': <xarray.DataArray (y: 3, z: 3)>
6008         array([[0, 1, 1],
6009                [0, 1, 0],
6010                [0, 1, 0]])
6011         Dimensions without coordinates: y, z}
6012         >>> array.max(dim=("x", "z"))
6013         <xarray.DataArray (y: 3)>
6014         array([3, 5, 3])
6015         Dimensions without coordinates: y
6016         >>> array.argmax(dim=["x", "z"])
6017         {'x': <xarray.DataArray (y: 3)>
6018         array([0, 1, 0])
6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
6020         array([0, 1, 2])
6021         Dimensions without coordinates: y}
6022         >>> array.isel(array.argmax(dim=["x", "z"]))
6023         <xarray.DataArray (y: 3)>
6024         array([3, 5, 3])
6025         Dimensions without coordinates: y
6026         """
6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
6028         if isinstance(result, dict):
6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
6030         else:
6031             return self._replace_maybe_drop_dims(result)
6032 
6033     def query(
6034         self,
6035         queries: Mapping[Any, Any] | None = None,
6036         parser: QueryParserOptions = "pandas",
6037         engine: QueryEngineOptions = None,
6038         missing_dims: ErrorOptionsWithWarn = "raise",
6039         **queries_kwargs: Any,
6040     ) -> DataArray:
6041         """Return a new data array indexed along the specified
6042         dimension(s), where the indexers are given as strings containing
6043         Python expressions to be evaluated against the values in the array.
6044 
6045         Parameters
6046         ----------
6047         queries : dict-like or None, optional
6048             A dict-like with keys matching dimensions and values given by strings
6049             containing Python expressions to be evaluated against the data variables
6050             in the dataset. The expressions will be evaluated using the pandas
6051             eval() function, and can contain any valid Python expressions but cannot
6052             contain any Python statements.
6053         parser : {"pandas", "python"}, default: "pandas"
6054             The parser to use to construct the syntax tree from the expression.
6055             The default of 'pandas' parses code slightly different than standard
6056             Python. Alternatively, you can parse an expression using the 'python'
6057             parser to retain strict Python semantics.
6058         engine : {"python", "numexpr", None}, default: None
6059             The engine used to evaluate the expression. Supported engines are:
6060 
6061             - None: tries to use numexpr, falls back to python
6062             - "numexpr": evaluates expressions using numexpr
6063             - "python": performs operations as if you had eval’d in top level python
6064 
6065         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
6066             What to do if dimensions that should be selected from are not present in the
6067             DataArray:
6068 
6069             - "raise": raise an exception
6070             - "warn": raise a warning, and ignore the missing dimensions
6071             - "ignore": ignore the missing dimensions
6072 
6073         **queries_kwargs : {dim: query, ...}, optional
6074             The keyword arguments form of ``queries``.
6075             One of queries or queries_kwargs must be provided.
6076 
6077         Returns
6078         -------
6079         obj : DataArray
6080             A new DataArray with the same contents as this dataset, indexed by
6081             the results of the appropriate queries.
6082 
6083         See Also
6084         --------
6085         DataArray.isel
6086         Dataset.query
6087         pandas.eval
6088 
6089         Examples
6090         --------
6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
6092         >>> da
6093         <xarray.DataArray 'a' (x: 5)>
6094         array([0, 1, 2, 3, 4])
6095         Dimensions without coordinates: x
6096         >>> da.query(x="a > 2")
6097         <xarray.DataArray 'a' (x: 2)>
6098         array([3, 4])
6099         Dimensions without coordinates: x
6100         """
6101 
6102         ds = self._to_dataset_whole(shallow_copy=True)
6103         ds = ds.query(
6104             queries=queries,
6105             parser=parser,
6106             engine=engine,
6107             missing_dims=missing_dims,
6108             **queries_kwargs,
6109         )
6110         return ds[self.name]
6111 
6112     def curvefit(
6113         self,
6114         coords: str | DataArray | Iterable[str | DataArray],
6115         func: Callable[..., Any],
6116         reduce_dims: Dims = None,
6117         skipna: bool = True,
6118         p0: dict[str, Any] | None = None,
6119         bounds: dict[str, Any] | None = None,
6120         param_names: Sequence[str] | None = None,
6121         kwargs: dict[str, Any] | None = None,
6122     ) -> Dataset:
6123         """
6124         Curve fitting optimization for arbitrary functions.
6125 
6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
6127 
6128         Parameters
6129         ----------
6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable
6131             Independent coordinate(s) over which to perform the curve fitting. Must share
6132             at least one dimension with the calling object. When fitting multi-dimensional
6133             functions, supply `coords` as a sequence in the same order as arguments in
6134             `func`. To fit along existing dimensions of the calling object, `coords` can
6135             also be specified as a str or sequence of strs.
6136         func : callable
6137             User specified function in the form `f(x, *params)` which returns a numpy
6138             array of length `len(x)`. `params` are the fittable parameters which are optimized
6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
6140             coordinates, e.g. `f((x0, x1), *params)`.
6141         reduce_dims : str, Iterable of Hashable or None, optional
6142             Additional dimension(s) over which to aggregate while fitting. For example,
6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
6144             aggregate all lat and lon points and fit the specified function along the
6145             time dimension.
6146         skipna : bool, default: True
6147             Whether to skip missing values when fitting. Default is True.
6148         p0 : dict-like or None, optional
6149             Optional dictionary of parameter names to initial guesses passed to the
6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
6151             be assigned initial values following the default scipy behavior.
6152         bounds : dict-like or None, optional
6153             Optional dictionary of parameter names to bounding values passed to the
6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
6155             will be unbounded following the default scipy behavior.
6156         param_names : sequence of Hashable or None, optional
6157             Sequence of names for the fittable parameters of `func`. If not supplied,
6158             this will be automatically determined by arguments of `func`. `param_names`
6159             should be manually supplied when fitting a function that takes a variable
6160             number of parameters.
6161         **kwargs : optional
6162             Additional keyword arguments to passed to scipy curve_fit.
6163 
6164         Returns
6165         -------
6166         curvefit_results : Dataset
6167             A single dataset which contains:
6168 
6169             [var]_curvefit_coefficients
6170                 The coefficients of the best fit.
6171             [var]_curvefit_covariance
6172                 The covariance matrix of the coefficient estimates.
6173 
6174         See Also
6175         --------
6176         DataArray.polyfit
6177         scipy.optimize.curve_fit
6178         """
6179         return self._to_temp_dataset().curvefit(
6180             coords,
6181             func,
6182             reduce_dims=reduce_dims,
6183             skipna=skipna,
6184             p0=p0,
6185             bounds=bounds,
6186             param_names=param_names,
6187             kwargs=kwargs,
6188         )
6189 
6190     def drop_duplicates(
6191         self: T_DataArray,
6192         dim: Hashable | Iterable[Hashable],
6193         keep: Literal["first", "last", False] = "first",
6194     ) -> T_DataArray:
6195         """Returns a new DataArray with duplicate dimension values removed.
6196 
6197         Parameters
6198         ----------
6199         dim : dimension label or labels
6200             Pass `...` to drop duplicates along all dimensions.
6201         keep : {"first", "last", False}, default: "first"
6202             Determines which duplicates (if any) to keep.
6203 
6204             - ``"first"`` : Drop duplicates except for the first occurrence.
6205             - ``"last"`` : Drop duplicates except for the last occurrence.
6206             - False : Drop all duplicates.
6207 
6208         Returns
6209         -------
6210         DataArray
6211 
6212         See Also
6213         --------
6214         Dataset.drop_duplicates
6215 
6216         Examples
6217         --------
6218         >>> da = xr.DataArray(
6219         ...     np.arange(25).reshape(5, 5),
6220         ...     dims=("x", "y"),
6221         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6222         ... )
6223         >>> da
6224         <xarray.DataArray (x: 5, y: 5)>
6225         array([[ 0,  1,  2,  3,  4],
6226                [ 5,  6,  7,  8,  9],
6227                [10, 11, 12, 13, 14],
6228                [15, 16, 17, 18, 19],
6229                [20, 21, 22, 23, 24]])
6230         Coordinates:
6231           * x        (x) int64 0 0 1 2 3
6232           * y        (y) int64 0 1 2 3 3
6233 
6234         >>> da.drop_duplicates(dim="x")
6235         <xarray.DataArray (x: 4, y: 5)>
6236         array([[ 0,  1,  2,  3,  4],
6237                [10, 11, 12, 13, 14],
6238                [15, 16, 17, 18, 19],
6239                [20, 21, 22, 23, 24]])
6240         Coordinates:
6241           * x        (x) int64 0 1 2 3
6242           * y        (y) int64 0 1 2 3 3
6243 
6244         >>> da.drop_duplicates(dim="x", keep="last")
6245         <xarray.DataArray (x: 4, y: 5)>
6246         array([[ 5,  6,  7,  8,  9],
6247                [10, 11, 12, 13, 14],
6248                [15, 16, 17, 18, 19],
6249                [20, 21, 22, 23, 24]])
6250         Coordinates:
6251           * x        (x) int64 0 1 2 3
6252           * y        (y) int64 0 1 2 3 3
6253 
6254         Drop all duplicate dimension values:
6255 
6256         >>> da.drop_duplicates(dim=...)
6257         <xarray.DataArray (x: 4, y: 4)>
6258         array([[ 0,  1,  2,  3],
6259                [10, 11, 12, 13],
6260                [15, 16, 17, 18],
6261                [20, 21, 22, 23]])
6262         Coordinates:
6263           * x        (x) int64 0 1 2 3
6264           * y        (y) int64 0 1 2 3
6265         """
6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6267         return self._from_temp_dataset(deduplicated)
6268 
6269     def convert_calendar(
6270         self,
6271         calendar: str,
6272         dim: str = "time",
6273         align_on: str | None = None,
6274         missing: Any | None = None,
6275         use_cftime: bool | None = None,
6276     ) -> DataArray:
6277         """Convert the DataArray to another calendar.
6278 
6279         Only converts the individual timestamps, does not modify any data except
6280         in dropping invalid/surplus dates or inserting missing dates.
6281 
6282         If the source and target calendars are either no_leap, all_leap or a
6283         standard type, only the type of the time array is modified.
6284         When converting to a leap year from a non-leap year, the 29th of February
6285         is removed from the array. In the other direction the 29th of February
6286         will be missing in the output, unless `missing` is specified,
6287         in which case that value is inserted.
6288 
6289         For conversions involving `360_day` calendars, see Notes.
6290 
6291         This method is safe to use with sub-daily data as it doesn't touch the
6292         time part of the timestamps.
6293 
6294         Parameters
6295         ---------
6296         calendar : str
6297             The target calendar name.
6298         dim : str
6299             Name of the time coordinate.
6300         align_on : {None, 'date', 'year'}
6301             Must be specified when either source or target is a `360_day` calendar,
6302            ignored otherwise. See Notes.
6303         missing : Optional[any]
6304             By default, i.e. if the value is None, this method will simply attempt
6305             to convert the dates in the source calendar to the same dates in the
6306             target calendar, and drop any of those that are not possible to
6307             represent.  If a value is provided, a new time coordinate will be
6308             created in the target calendar with the same frequency as the original
6309             time coordinate; for any dates that are not present in the source, the
6310             data will be filled with this value.  Note that using this mode requires
6311             that the source data have an inferable frequency; for more information
6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6313             target calendar combinations, this could result in many missing values, see notes.
6314         use_cftime : boolean, optional
6315             Whether to use cftime objects in the output, only used if `calendar`
6316             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6317             If True, the new time axis uses cftime objects.
6318             If None (default), it uses :py:class:`numpy.datetime64` values if the
6319             date range permits it, and :py:class:`cftime.datetime` objects if not.
6320             If False, it uses :py:class:`numpy.datetime64`  or fails.
6321 
6322         Returns
6323         -------
6324         DataArray
6325             Copy of the dataarray with the time coordinate converted to the
6326             target calendar. If 'missing' was None (default), invalid dates in
6327             the new calendar are dropped, but missing dates are not inserted.
6328             If `missing` was given, the new data is reindexed to have a time axis
6329             with the same frequency as the source, but in the new calendar; any
6330             missing datapoints are filled with `missing`.
6331 
6332         Notes
6333         -----
6334         Passing a value to `missing` is only usable if the source's time coordinate as an
6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6336         if the target coordinate, generated from this frequency, has dates equivalent to the
6337         source. It is usually **not** appropriate to use this mode with:
6338 
6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6341             or 'mH' where 24 % m != 0).
6342 
6343         If one of the source or target calendars is `"360_day"`, `align_on` must
6344         be specified and two options are offered.
6345 
6346         - "year"
6347             The dates are translated according to their relative position in the year,
6348             ignoring their original month and day information, meaning that the
6349             missing/surplus days are added/removed at regular intervals.
6350 
6351             From a `360_day` to a standard calendar, the output will be missing the
6352             following dates (day of year in parentheses):
6353 
6354             To a leap year:
6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6356                 September 31st (275) and November 30th (335).
6357             To a non-leap year:
6358                 February 6th (36), April 19th (109), July 2nd (183),
6359                 September 12th (255), November 25th (329).
6360 
6361             From a standard calendar to a `"360_day"`, the following dates in the
6362             source array will be dropped:
6363 
6364             From a leap year:
6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6366                 September 31st (275), December 1st (336)
6367             From a non-leap year:
6368                 February 6th (37), April 20th (110), July 2nd (183),
6369                 September 13th (256), November 25th (329)
6370 
6371             This option is best used on daily and subdaily data.
6372 
6373         - "date"
6374             The month/day information is conserved and invalid dates are dropped
6375             from the output. This means that when converting from a `"360_day"` to a
6376             standard calendar, all 31st (Jan, March, May, July, August, October and
6377             December) will be missing as there is no equivalent dates in the
6378             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6379             will be dropped as there are no equivalent dates in a standard calendar.
6380 
6381             This option is best used with data on a frequency coarser than daily.
6382         """
6383         return convert_calendar(
6384             self,
6385             calendar,
6386             dim=dim,
6387             align_on=align_on,
6388             missing=missing,
6389             use_cftime=use_cftime,
6390         )
6391 
6392     def interp_calendar(
6393         self,
6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6395         dim: str = "time",
6396     ) -> DataArray:
6397         """Interpolates the DataArray to another calendar based on decimal year measure.
6398 
6399         Each timestamp in `source` and `target` are first converted to their decimal
6400         year equivalent then `source` is interpolated on the target coordinate.
6401         The decimal year of a timestamp is its year plus its sub-year component
6402         converted to the fraction of its year. For example "2000-03-01 12:00" is
6403         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6404 
6405         This method should only be used when the time (HH:MM:SS) information of
6406         time coordinate is not important.
6407 
6408         Parameters
6409         ----------
6410         target: DataArray or DatetimeIndex or CFTimeIndex
6411             The target time coordinate of a valid dtype
6412             (np.datetime64 or cftime objects)
6413         dim : str
6414             The time coordinate name.
6415 
6416         Return
6417         ------
6418         DataArray
6419             The source interpolated on the decimal years of target,
6420         """
6421         return interp_calendar(self, target, dim=dim)
6422 
6423     def groupby(
6424         self,
6425         group: Hashable | DataArray | IndexVariable,
6426         squeeze: bool = True,
6427         restore_coord_dims: bool = False,
6428     ) -> DataArrayGroupBy:
6429         """Returns a DataArrayGroupBy object for performing grouped operations.
6430 
6431         Parameters
6432         ----------
6433         group : Hashable, DataArray or IndexVariable
6434             Array whose unique values should be used to group this array. If a
6435             Hashable, must be the name of a coordinate contained in this dataarray.
6436         squeeze : bool, default: True
6437             If "group" is a dimension of any arrays in this dataset, `squeeze`
6438             controls whether the subarrays have a dimension of length 1 along
6439             that dimension or if the dimension is squeezed out.
6440         restore_coord_dims : bool, default: False
6441             If True, also restore the dimension order of multi-dimensional
6442             coordinates.
6443 
6444         Returns
6445         -------
6446         grouped : DataArrayGroupBy
6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6448             iterated over in the form of `(unique_value, grouped_array)` pairs.
6449 
6450         Examples
6451         --------
6452         Calculate daily anomalies for daily data:
6453 
6454         >>> da = xr.DataArray(
6455         ...     np.linspace(0, 1826, num=1827),
6456         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6457         ...     dims="time",
6458         ... )
6459         >>> da
6460         <xarray.DataArray (time: 1827)>
6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6462                1.826e+03])
6463         Coordinates:
6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6465         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6466         <xarray.DataArray (time: 1827)>
6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6468         Coordinates:
6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6471 
6472         See Also
6473         --------
6474         :ref:`groupby`
6475             Users guide explanation of how to group and bin data.
6476         DataArray.groupby_bins
6477         Dataset.groupby
6478         core.groupby.DataArrayGroupBy
6479         pandas.DataFrame.groupby
6480         """
6481         from xarray.core.groupby import (
6482             DataArrayGroupBy,
6483             ResolvedUniqueGrouper,
6484             UniqueGrouper,
6485             _validate_groupby_squeeze,
6486         )
6487 
6488         _validate_groupby_squeeze(squeeze)
6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
6490         return DataArrayGroupBy(
6491             self,
6492             (rgrouper,),
6493             squeeze=squeeze,
6494             restore_coord_dims=restore_coord_dims,
6495         )
6496 
6497     def groupby_bins(
6498         self,
6499         group: Hashable | DataArray | IndexVariable,
6500         bins: ArrayLike,
6501         right: bool = True,
6502         labels: ArrayLike | Literal[False] | None = None,
6503         precision: int = 3,
6504         include_lowest: bool = False,
6505         squeeze: bool = True,
6506         restore_coord_dims: bool = False,
6507     ) -> DataArrayGroupBy:
6508         """Returns a DataArrayGroupBy object for performing grouped operations.
6509 
6510         Rather than using all unique values of `group`, the values are discretized
6511         first by applying `pandas.cut` [1]_ to `group`.
6512 
6513         Parameters
6514         ----------
6515         group : Hashable, DataArray or IndexVariable
6516             Array whose binned values should be used to group this array. If a
6517             Hashable, must be the name of a coordinate contained in this dataarray.
6518         bins : int or array-like
6519             If bins is an int, it defines the number of equal-width bins in the
6520             range of x. However, in this case, the range of x is extended by .1%
6521             on each side to include the min or max values of x. If bins is a
6522             sequence it defines the bin edges allowing for non-uniform bin
6523             width. No extension of the range of x is done in this case.
6524         right : bool, default: True
6525             Indicates whether the bins include the rightmost edge or not. If
6526             right == True (the default), then the bins [1,2,3,4] indicate
6527             (1,2], (2,3], (3,4].
6528         labels : array-like, False or None, default: None
6529             Used as labels for the resulting bins. Must be of the same length as
6530             the resulting bins. If False, string bin labels are assigned by
6531             `pandas.cut`.
6532         precision : int, default: 3
6533             The precision at which to store and display the bins labels.
6534         include_lowest : bool, default: False
6535             Whether the first interval should be left-inclusive or not.
6536         squeeze : bool, default: True
6537             If "group" is a dimension of any arrays in this dataset, `squeeze`
6538             controls whether the subarrays have a dimension of length 1 along
6539             that dimension or if the dimension is squeezed out.
6540         restore_coord_dims : bool, default: False
6541             If True, also restore the dimension order of multi-dimensional
6542             coordinates.
6543 
6544         Returns
6545         -------
6546         grouped : DataArrayGroupBy
6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6548             iterated over in the form of `(unique_value, grouped_array)` pairs.
6549             The name of the group has the added suffix `_bins` in order to
6550             distinguish it from the original variable.
6551 
6552         See Also
6553         --------
6554         :ref:`groupby`
6555             Users guide explanation of how to group and bin data.
6556         DataArray.groupby
6557         Dataset.groupby_bins
6558         core.groupby.DataArrayGroupBy
6559         pandas.DataFrame.groupby
6560 
6561         References
6562         ----------
6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6564         """
6565         from xarray.core.groupby import (
6566             BinGrouper,
6567             DataArrayGroupBy,
6568             ResolvedBinGrouper,
6569             _validate_groupby_squeeze,
6570         )
6571 
6572         _validate_groupby_squeeze(squeeze)
6573         grouper = BinGrouper(
6574             bins=bins,
6575             cut_kwargs={
6576                 "right": right,
6577                 "labels": labels,
6578                 "precision": precision,
6579                 "include_lowest": include_lowest,
6580             },
6581         )
6582         rgrouper = ResolvedBinGrouper(grouper, group, self)
6583 
6584         return DataArrayGroupBy(
6585             self,
6586             (rgrouper,),
6587             squeeze=squeeze,
6588             restore_coord_dims=restore_coord_dims,
6589         )
6590 
6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6592         """
6593         Weighted DataArray operations.
6594 
6595         Parameters
6596         ----------
6597         weights : DataArray
6598             An array of weights associated with the values in this Dataset.
6599             Each value in the data contributes to the reduction operation
6600             according to its associated weight.
6601 
6602         Notes
6603         -----
6604         ``weights`` must be a DataArray and cannot contain missing values.
6605         Missing values can be replaced by ``weights.fillna(0)``.
6606 
6607         Returns
6608         -------
6609         core.weighted.DataArrayWeighted
6610 
6611         See Also
6612         --------
6613         Dataset.weighted
6614         """
6615         from xarray.core.weighted import DataArrayWeighted
6616 
6617         return DataArrayWeighted(self, weights)
6618 
6619     def rolling(
6620         self,
6621         dim: Mapping[Any, int] | None = None,
6622         min_periods: int | None = None,
6623         center: bool | Mapping[Any, bool] = False,
6624         **window_kwargs: int,
6625     ) -> DataArrayRolling:
6626         """
6627         Rolling window object for DataArrays.
6628 
6629         Parameters
6630         ----------
6631         dim : dict, optional
6632             Mapping from the dimension name to create the rolling iterator
6633             along (e.g. `time`) to its moving window size.
6634         min_periods : int or None, default: None
6635             Minimum number of observations in window required to have a value
6636             (otherwise result is NA). The default, None, is equivalent to
6637             setting min_periods equal to the size of the window.
6638         center : bool or Mapping to int, default: False
6639             Set the labels at the center of the window.
6640         **window_kwargs : optional
6641             The keyword arguments form of ``dim``.
6642             One of dim or window_kwargs must be provided.
6643 
6644         Returns
6645         -------
6646         core.rolling.DataArrayRolling
6647 
6648         Examples
6649         --------
6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6651 
6652         >>> da = xr.DataArray(
6653         ...     np.linspace(0, 11, num=12),
6654         ...     coords=[
6655         ...         pd.date_range(
6656         ...             "1999-12-15",
6657         ...             periods=12,
6658         ...             freq=pd.DateOffset(months=1),
6659         ...         )
6660         ...     ],
6661         ...     dims="time",
6662         ... )
6663         >>> da
6664         <xarray.DataArray (time: 12)>
6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6666         Coordinates:
6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6668         >>> da.rolling(time=3, center=True).mean()
6669         <xarray.DataArray (time: 12)>
6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6671         Coordinates:
6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6673 
6674         Remove the NaNs using ``dropna()``:
6675 
6676         >>> da.rolling(time=3, center=True).mean().dropna("time")
6677         <xarray.DataArray (time: 10)>
6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6679         Coordinates:
6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6681 
6682         See Also
6683         --------
6684         core.rolling.DataArrayRolling
6685         Dataset.rolling
6686         """
6687         from xarray.core.rolling import DataArrayRolling
6688 
6689         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6691 
6692     def coarsen(
6693         self,
6694         dim: Mapping[Any, int] | None = None,
6695         boundary: CoarsenBoundaryOptions = "exact",
6696         side: SideOptions | Mapping[Any, SideOptions] = "left",
6697         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6698         **window_kwargs: int,
6699     ) -> DataArrayCoarsen:
6700         """
6701         Coarsen object for DataArrays.
6702 
6703         Parameters
6704         ----------
6705         dim : mapping of hashable to int, optional
6706             Mapping from the dimension name to the window size.
6707         boundary : {"exact", "trim", "pad"}, default: "exact"
6708             If 'exact', a ValueError will be raised if dimension size is not a
6709             multiple of the window size. If 'trim', the excess entries are
6710             dropped. If 'pad', NA will be padded.
6711         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6712         coord_func : str or mapping of hashable to str, default: "mean"
6713             function (name) that is applied to the coordinates,
6714             or a mapping from coordinate name to function (name).
6715 
6716         Returns
6717         -------
6718         core.rolling.DataArrayCoarsen
6719 
6720         Examples
6721         --------
6722         Coarsen the long time series by averaging over every three days.
6723 
6724         >>> da = xr.DataArray(
6725         ...     np.linspace(0, 364, num=364),
6726         ...     dims="time",
6727         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6728         ... )
6729         >>> da  # +doctest: ELLIPSIS
6730         <xarray.DataArray (time: 364)>
6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6734         ...
6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6736                360.99173554, 361.99449036, 362.99724518, 364.        ])
6737         Coordinates:
6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6739         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6740         <xarray.DataArray (time: 121)>
6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6744         ...
6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6746                361.99449036])
6747         Coordinates:
6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6749         >>>
6750 
6751         See Also
6752         --------
6753         core.rolling.DataArrayCoarsen
6754         Dataset.coarsen
6755         """
6756         from xarray.core.rolling import DataArrayCoarsen
6757 
6758         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6759         return DataArrayCoarsen(
6760             self,
6761             dim,
6762             boundary=boundary,
6763             side=side,
6764             coord_func=coord_func,
6765         )
6766 
6767     def resample(
6768         self,
6769         indexer: Mapping[Any, str] | None = None,
6770         skipna: bool | None = None,
6771         closed: SideOptions | None = None,
6772         label: SideOptions | None = None,
6773         base: int | None = None,
6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6775         origin: str | DatetimeLike = "start_day",
6776         keep_attrs: bool | None = None,
6777         loffset: datetime.timedelta | str | None = None,
6778         restore_coord_dims: bool | None = None,
6779         **indexer_kwargs: str,
6780     ) -> DataArrayResample:
6781         """Returns a Resample object for performing resampling operations.
6782 
6783         Handles both downsampling and upsampling. The resampled
6784         dimension must be a datetime-like coordinate. If any intervals
6785         contain no values from the original object, they will be given
6786         the value ``NaN``.
6787 
6788         Parameters
6789         ----------
6790         indexer : Mapping of Hashable to str, optional
6791             Mapping from the dimension name to resample frequency [1]_. The
6792             dimension must be datetime-like.
6793         skipna : bool, optional
6794             Whether to skip missing values when aggregating in downsampling.
6795         closed : {"left", "right"}, optional
6796             Side of each interval to treat as closed.
6797         label : {"left", "right"}, optional
6798             Side of each interval to use for labeling.
6799         base : int, optional
6800             For frequencies that evenly subdivide 1 day, the "origin" of the
6801             aggregated intervals. For example, for "24H" frequency, base could
6802             range from 0 through 23.
6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6804             The datetime on which to adjust the grouping. The timezone of origin
6805             must match the timezone of the index.
6806 
6807             If a datetime is not used, these values are also supported:
6808             - 'epoch': `origin` is 1970-01-01
6809             - 'start': `origin` is the first value of the timeseries
6810             - 'start_day': `origin` is the first day at midnight of the timeseries
6811             - 'end': `origin` is the last value of the timeseries
6812             - 'end_day': `origin` is the ceiling midnight of the last day
6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6814             An offset timedelta added to the origin.
6815         loffset : timedelta or str, optional
6816             Offset used to adjust the resampled time labels. Some pandas date
6817             offset strings are supported.
6818         restore_coord_dims : bool, optional
6819             If True, also restore the dimension order of multi-dimensional
6820             coordinates.
6821         **indexer_kwargs : str
6822             The keyword arguments form of ``indexer``.
6823             One of indexer or indexer_kwargs must be provided.
6824 
6825         Returns
6826         -------
6827         resampled : core.resample.DataArrayResample
6828             This object resampled.
6829 
6830         Examples
6831         --------
6832         Downsample monthly time-series data to seasonal data:
6833 
6834         >>> da = xr.DataArray(
6835         ...     np.linspace(0, 11, num=12),
6836         ...     coords=[
6837         ...         pd.date_range(
6838         ...             "1999-12-15",
6839         ...             periods=12,
6840         ...             freq=pd.DateOffset(months=1),
6841         ...         )
6842         ...     ],
6843         ...     dims="time",
6844         ... )
6845         >>> da
6846         <xarray.DataArray (time: 12)>
6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6848         Coordinates:
6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6850         >>> da.resample(time="QS-DEC").mean()
6851         <xarray.DataArray (time: 4)>
6852         array([ 1.,  4.,  7., 10.])
6853         Coordinates:
6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6855 
6856         Upsample monthly time-series data to daily data:
6857 
6858         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6859         <xarray.DataArray (time: 337)>
6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6863         ...
6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6865                10.96774194, 11.        ])
6866         Coordinates:
6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6868 
6869         Limit scope of upsampling method
6870 
6871         >>> da.resample(time="1D").nearest(tolerance="1D")
6872         <xarray.DataArray (time: 337)>
6873         array([ 0.,  0., nan, ..., nan, 11., 11.])
6874         Coordinates:
6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6876 
6877         See Also
6878         --------
6879         Dataset.resample
6880         pandas.Series.resample
6881         pandas.DataFrame.resample
6882 
6883         References
6884         ----------
6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6886         """
6887         from xarray.core.resample import DataArrayResample
6888 
6889         return self._resample(
6890             resample_cls=DataArrayResample,
6891             indexer=indexer,
6892             skipna=skipna,
6893             closed=closed,
6894             label=label,
6895             base=base,
6896             offset=offset,
6897             origin=origin,
6898             keep_attrs=keep_attrs,
6899             loffset=loffset,
6900             restore_coord_dims=restore_coord_dims,
6901             **indexer_kwargs,
6902         )
6903 
6904     def to_dask_dataframe(
6905         self,
6906         dim_order: Sequence[Hashable] | None = None,
6907         set_index: bool = False,
6908     ) -> DaskDataFrame:
6909         """Convert this array into a dask.dataframe.DataFrame.
6910 
6911         Parameters
6912         ----------
6913         dim_order : Sequence of Hashable or None , optional
6914             Hierarchical dimension order for the resulting dataframe.
6915             Array content is transposed to this order and then written out as flat
6916             vectors in contiguous order, so the last dimension in this list
6917             will be contiguous in the resulting DataFrame. This has a major influence
6918             on which operations are efficient on the resulting dask dataframe.
6919         set_index : bool, default: False
6920             If set_index=True, the dask DataFrame is indexed by this dataset's
6921             coordinate. Since dask DataFrames do not support multi-indexes,
6922             set_index only works if the dataset only contains one dimension.
6923 
6924         Returns
6925         -------
6926         dask.dataframe.DataFrame
6927 
6928         Examples
6929         --------
6930         >>> da = xr.DataArray(
6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),
6932         ...     dims=("time", "lat", "lon"),
6933         ...     coords={
6934         ...         "time": np.arange(4),
6935         ...         "lat": [-30, -20],
6936         ...         "lon": [120, 130],
6937         ...     },
6938         ...     name="eg_dataarray",
6939         ...     attrs={"units": "Celsius", "description": "Random temperature data"},
6940         ... )
6941         >>> da.to_dask_dataframe(["lat", "lon", "time"]).compute()
6942             lat  lon  time  eg_dataarray
6943         0   -30  120     0             0
6944         1   -30  120     1             4
6945         2   -30  120     2             8
6946         3   -30  120     3            12
6947         4   -30  130     0             1
6948         5   -30  130     1             5
6949         6   -30  130     2             9
6950         7   -30  130     3            13
6951         8   -20  120     0             2
6952         9   -20  120     1             6
6953         10  -20  120     2            10
6954         11  -20  120     3            14
6955         12  -20  130     0             3
6956         13  -20  130     1             7
6957         14  -20  130     2            11
6958         15  -20  130     3            15
6959         """
6960         if self.name is None:
6961             raise ValueError(
6962                 "Cannot convert an unnamed DataArray to a "
6963                 "dask dataframe : use the ``.rename`` method to assign a name."
6964             )
6965         name = self.name
6966         ds = self._to_dataset_whole(name, shallow_copy=False)
6967         return ds.to_dask_dataframe(dim_order, set_index)
6968 
6969     # this needs to be at the end, or mypy will confuse with `str`
6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6971     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 22:04:43.250 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>chunk</method>
<code>
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)

</code><intended_behavior>The method should include support for a more general chunking mechanism that is not tightly coupled with dask. This entails creating an abstraction layer that can interpret chunks arguments for different frameworks like dask or cubed. The method should dynamically determine the chunking strategy based on the specified backend engine.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
229 class DataArray(
230     AbstractArray,
231     DataWithCoords,
232     DataArrayArithmetic,
233     DataArrayAggregations,
234 ):
235     """N-dimensional array with labeled coordinates and dimensions.
236 
237     DataArray provides a wrapper around numpy ndarrays that uses
238     labeled dimensions and coordinates to support metadata aware
239     operations. The API is similar to that for the pandas Series or
240     DataFrame, but DataArray objects can have any number of dimensions,
241     and their contents have fixed data types.
242 
243     Additional features over raw numpy arrays:
244 
245     - Apply operations over dimensions by name: ``x.sum('time')``.
246     - Select or assign values by integer location (like numpy):
247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
248       ``x.sel(time='2014-01-01')``.
249     - Mathematical operations (e.g., ``x - y``) vectorize across
250       multiple dimensions (known in numpy as "broadcasting") based on
251       dimension names, regardless of their original order.
252     - Keep track of arbitrary metadata in the form of a Python
253       dictionary: ``x.attrs``
254     - Convert to a pandas Series: ``x.to_series()``.
255 
256     Getting items from or doing mathematical operations with a
257     DataArray always returns another DataArray.
258 
259     Parameters
260     ----------
261     data : array_like
262         Values for this array. Must be an ``numpy.ndarray``, ndarray
263         like, or castable to an ``ndarray``. If a self-described xarray
264         or pandas object, attempts are made to use this array's
265         metadata to fill in other unspecified arguments. A view of the
266         array's data is used instead of a copy if possible.
267     coords : sequence or dict of array_like, optional
268         Coordinates (tick labels) to use for indexing along each
269         dimension. The following notations are accepted:
270 
271         - mapping {dimension name: array-like}
272         - sequence of tuples that are valid arguments for
273           ``xarray.Variable()``
274           - (dims, data)
275           - (dims, data, attrs)
276           - (dims, data, attrs, encoding)
277 
278         Additionally, it is possible to define a coord whose name
279         does not match the dimension name, or a coord based on multiple
280         dimensions, with one of the following notations:
281 
282         - mapping {coord name: DataArray}
283         - mapping {coord name: Variable}
284         - mapping {coord name: (dimension name, array-like)}
285         - mapping {coord name: (tuple of dimension names, array-like)}
286 
287     dims : Hashable or sequence of Hashable, optional
288         Name(s) of the data dimension(s). Must be either a Hashable
289         (only for 1D data) or a sequence of Hashables with length equal
290         to the number of dimensions. If this argument is omitted,
291         dimension names are taken from ``coords`` (if possible) and
292         otherwise default to ``['dim_0', ... 'dim_n']``.
293     name : str or None, optional
294         Name of this array.
295     attrs : dict_like or None, optional
296         Attributes to assign to the new instance. By default, an empty
297         attribute dictionary is initialized.
298 
299     Examples
300     --------
301     Create data:
302 
303     >>> np.random.seed(0)
304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
307     >>> time = pd.date_range("2014-09-06", periods=3)
308     >>> reference_time = pd.Timestamp("2014-09-05")
309 
310     Initialize a dataarray with multiple dimensions:
311 
312     >>> da = xr.DataArray(
313     ...     data=temperature,
314     ...     dims=["x", "y", "time"],
315     ...     coords=dict(
316     ...         lon=(["x", "y"], lon),
317     ...         lat=(["x", "y"], lat),
318     ...         time=time,
319     ...         reference_time=reference_time,
320     ...     ),
321     ...     attrs=dict(
322     ...         description="Ambient temperature.",
323     ...         units="degC",
324     ...     ),
325     ... )
326     >>> da
327     <xarray.DataArray (x: 2, y: 2, time: 3)>
328     array([[[29.11241877, 18.20125767, 22.82990387],
329             [32.92714559, 29.94046392,  7.18177696]],
330     <BLANKLINE>
331            [[22.60070734, 13.78914233, 14.17424919],
332             [18.28478802, 16.15234857, 26.63418806]]])
333     Coordinates:
334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
335         lat             (x, y) float64 42.25 42.21 42.63 42.59
336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
337         reference_time  datetime64[ns] 2014-09-05
338     Dimensions without coordinates: x, y
339     Attributes:
340         description:  Ambient temperature.
341         units:        degC
342 
343     Find out where the coldest temperature was:
344 
345     >>> da.isel(da.argmin(...))
346     <xarray.DataArray ()>
347     array(7.18177696)
348     Coordinates:
349         lon             float64 -99.32
350         lat             float64 42.21
351         time            datetime64[ns] 2014-09-08
352         reference_time  datetime64[ns] 2014-09-05
353     Attributes:
354         description:  Ambient temperature.
355         units:        degC
356     """
357 
358     _cache: dict[str, Any]
359     _coords: dict[Any, Variable]
360     _close: Callable[[], None] | None
361     _indexes: dict[Hashable, Index]
362     _name: Hashable | None
363     _variable: Variable
364 
365     __slots__ = (
366         "_cache",
367         "_coords",
368         "_close",
369         "_indexes",
370         "_name",
371         "_variable",
372         "__weakref__",
373     )
374 
375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
376 
377     def __init__(
378         self,
379         data: Any = dtypes.NA,
380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
381         | Mapping[Any, Any]
382         | None = None,
383         dims: Hashable | Sequence[Hashable] | None = None,
384         name: Hashable | None = None,
385         attrs: Mapping | None = None,
386         # internal parameters
387         indexes: dict[Hashable, Index] | None = None,
388         fastpath: bool = False,
389     ) -> None:
390         if fastpath:
391             variable = data
392             assert dims is None
393             assert attrs is None
394             assert indexes is not None
395         else:
396             # TODO: (benbovy - explicit indexes) remove
397             # once it becomes part of the public interface
398             if indexes is not None:
399                 raise ValueError("Providing explicit indexes is not supported yet")
400 
401             # try to fill in arguments from data if they weren't supplied
402             if coords is None:
403                 if isinstance(data, DataArray):
404                     coords = data.coords
405                 elif isinstance(data, pd.Series):
406                     coords = [data.index]
407                 elif isinstance(data, pd.DataFrame):
408                     coords = [data.index, data.columns]
409                 elif isinstance(data, (pd.Index, IndexVariable)):
410                     coords = [data]
411 
412             if dims is None:
413                 dims = getattr(data, "dims", getattr(coords, "dims", None))
414             if name is None:
415                 name = getattr(data, "name", None)
416             if attrs is None and not isinstance(data, PANDAS_TYPES):
417                 attrs = getattr(data, "attrs", None)
418 
419             data = _check_data_shape(data, coords, dims)
420             data = as_compatible_data(data)
421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
422             variable = Variable(dims, data, attrs, fastpath=True)
423             indexes, coords = _create_indexes_from_coords(coords)
424 
425         # These fully describe a DataArray
426         self._variable = variable
427         assert isinstance(coords, dict)
428         self._coords = coords
429         self._name = name
430 
431         # TODO(shoyer): document this argument, once it becomes part of the
432         # public interface.
433         self._indexes = indexes
434 
435         self._close = None
436 
437     @classmethod
438     def _construct_direct(
439         cls: type[T_DataArray],
440         variable: Variable,
441         coords: dict[Any, Variable],
442         name: Hashable,
443         indexes: dict[Hashable, Index],
444     ) -> T_DataArray:
445         """Shortcut around __init__ for internal use when we want to skip
446         costly validation
447         """
448         obj = object.__new__(cls)
449         obj._variable = variable
450         obj._coords = coords
451         obj._name = name
452         obj._indexes = indexes
453         obj._close = None
454         return obj
455 
456     def _replace(
457         self: T_DataArray,
458         variable: Variable | None = None,
459         coords=None,
460         name: Hashable | None | Default = _default,
461         indexes=None,
462     ) -> T_DataArray:
463         if variable is None:
464             variable = self.variable
465         if coords is None:
466             coords = self._coords
467         if indexes is None:
468             indexes = self._indexes
469         if name is _default:
470             name = self.name
471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
472 
473     def _replace_maybe_drop_dims(
474         self: T_DataArray,
475         variable: Variable,
476         name: Hashable | None | Default = _default,
477     ) -> T_DataArray:
478         if variable.dims == self.dims and variable.shape == self.shape:
479             coords = self._coords.copy()
480             indexes = self._indexes
481         elif variable.dims == self.dims:
482             # Shape has changed (e.g. from reduce(..., keepdims=True)
483             new_sizes = dict(zip(self.dims, variable.shape))
484             coords = {
485                 k: v
486                 for k, v in self._coords.items()
487                 if v.shape == tuple(new_sizes[d] for d in v.dims)
488             }
489             indexes = filter_indexes_from_coords(self._indexes, set(coords))
490         else:
491             allowed_dims = set(variable.dims)
492             coords = {
493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
494             }
495             indexes = filter_indexes_from_coords(self._indexes, set(coords))
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _overwrite_indexes(
499         self: T_DataArray,
500         indexes: Mapping[Any, Index],
501         coords: Mapping[Any, Variable] | None = None,
502         drop_coords: list[Hashable] | None = None,
503         rename_dims: Mapping[Any, Any] | None = None,
504     ) -> T_DataArray:
505         """Maybe replace indexes and their corresponding coordinates."""
506         if not indexes:
507             return self
508 
509         if coords is None:
510             coords = {}
511         if drop_coords is None:
512             drop_coords = []
513 
514         new_variable = self.variable.copy()
515         new_coords = self._coords.copy()
516         new_indexes = dict(self._indexes)
517 
518         for name in indexes:
519             new_coords[name] = coords[name]
520             new_indexes[name] = indexes[name]
521 
522         for name in drop_coords:
523             new_coords.pop(name)
524             new_indexes.pop(name)
525 
526         if rename_dims:
527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
528 
529         return self._replace(
530             variable=new_variable, coords=new_coords, indexes=new_indexes
531         )
532 
533     def _to_temp_dataset(self) -> Dataset:
534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
535 
536     def _from_temp_dataset(
537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
538     ) -> T_DataArray:
539         variable = dataset._variables.pop(_THIS_ARRAY)
540         coords = dataset._variables
541         indexes = dataset._indexes
542         return self._replace(variable, coords, name, indexes=indexes)
543 
544     def _to_dataset_split(self, dim: Hashable) -> Dataset:
545         """splits dataarray along dimension 'dim'"""
546 
547         def subset(dim, label):
548             array = self.loc[{dim: label}]
549             array.attrs = {}
550             return as_variable(array)
551 
552         variables = {label: subset(dim, label) for label in self.get_index(dim)}
553         variables.update({k: v for k, v in self._coords.items() if k != dim})
554         coord_names = set(self._coords) - {dim}
555         indexes = filter_indexes_from_coords(self._indexes, coord_names)
556         dataset = Dataset._construct_direct(
557             variables, coord_names, indexes=indexes, attrs=self.attrs
558         )
559         return dataset
560 
561     def _to_dataset_whole(
562         self, name: Hashable = None, shallow_copy: bool = True
563     ) -> Dataset:
564         if name is None:
565             name = self.name
566         if name is None:
567             raise ValueError(
568                 "unable to convert unnamed DataArray to a "
569                 "Dataset without providing an explicit name"
570             )
571         if name in self.coords:
572             raise ValueError(
573                 "cannot create a Dataset from a DataArray with "
574                 "the same name as one of its coordinates"
575             )
576         # use private APIs for speed: this is called by _to_temp_dataset(),
577         # which is used in the guts of a lot of operations (e.g., reindex)
578         variables = self._coords.copy()
579         variables[name] = self.variable
580         if shallow_copy:
581             for k in variables:
582                 variables[k] = variables[k].copy(deep=False)
583         indexes = self._indexes
584 
585         coord_names = set(self._coords)
586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
587 
588     def to_dataset(
589         self,
590         dim: Hashable = None,
591         *,
592         name: Hashable = None,
593         promote_attrs: bool = False,
594     ) -> Dataset:
595         """Convert a DataArray to a Dataset.
596 
597         Parameters
598         ----------
599         dim : Hashable, optional
600             Name of the dimension on this array along which to split this array
601             into separate variables. If not provided, this array is converted
602             into a Dataset of one variable.
603         name : Hashable, optional
604             Name to substitute for this array's name. Only valid if ``dim`` is
605             not provided.
606         promote_attrs : bool, default: False
607             Set to True to shallow copy attrs of DataArray to returned Dataset.
608 
609         Returns
610         -------
611         dataset : Dataset
612         """
613         if dim is not None and dim not in self.dims:
614             raise TypeError(
615                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
616             )
617 
618         if dim is not None:
619             if name is not None:
620                 raise TypeError("cannot supply both dim and name arguments")
621             result = self._to_dataset_split(dim)
622         else:
623             result = self._to_dataset_whole(name)
624 
625         if promote_attrs:
626             result.attrs = dict(self.attrs)
627 
628         return result
629 
630     @property
631     def name(self) -> Hashable | None:
632         """The name of this array."""
633         return self._name
634 
635     @name.setter
636     def name(self, value: Hashable | None) -> None:
637         self._name = value
638 
639     @property
640     def variable(self) -> Variable:
641         """Low level interface to the Variable object for this DataArray."""
642         return self._variable
643 
644     @property
645     def dtype(self) -> np.dtype:
646         """
647         Data-type of the array’s elements.
648 
649         See Also
650         --------
651         ndarray.dtype
652         numpy.dtype
653         """
654         return self.variable.dtype
655 
656     @property
657     def shape(self) -> tuple[int, ...]:
658         """
659         Tuple of array dimensions.
660 
661         See Also
662         --------
663         numpy.ndarray.shape
664         """
665         return self.variable.shape
666 
667     @property
668     def size(self) -> int:
669         """
670         Number of elements in the array.
671 
672         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
673 
674         See Also
675         --------
676         numpy.ndarray.size
677         """
678         return self.variable.size
679 
680     @property
681     def nbytes(self) -> int:
682         """
683         Total bytes consumed by the elements of this DataArray's data.
684 
685         If the underlying data array does not include ``nbytes``, estimates
686         the bytes consumed based on the ``size`` and ``dtype``.
687         """
688         return self.variable.nbytes
689 
690     @property
691     def ndim(self) -> int:
692         """
693         Number of array dimensions.
694 
695         See Also
696         --------
697         numpy.ndarray.ndim
698         """
699         return self.variable.ndim
700 
701     def __len__(self) -> int:
702         return len(self.variable)
703 
704     @property
705     def data(self) -> Any:
706         """
707         The DataArray's data as an array. The underlying array type
708         (e.g. dask, sparse, pint) is preserved.
709 
710         See Also
711         --------
712         DataArray.to_numpy
713         DataArray.as_numpy
714         DataArray.values
715         """
716         return self.variable.data
717 
718     @data.setter
719     def data(self, value: Any) -> None:
720         self.variable.data = value
721 
722     @property
723     def values(self) -> np.ndarray:
724         """
725         The array's data as a numpy.ndarray.
726 
727         If the array's data is not a numpy.ndarray this will attempt to convert
728         it naively using np.array(), which will raise an error if the array
729         type does not support coercion like this (e.g. cupy).
730         """
731         return self.variable.values
732 
733     @values.setter
734     def values(self, value: Any) -> None:
735         self.variable.values = value
736 
737     def to_numpy(self) -> np.ndarray:
738         """
739         Coerces wrapped data to numpy and returns a numpy.ndarray.
740 
741         See Also
742         --------
743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
744         Dataset.as_numpy
745         DataArray.values
746         DataArray.data
747         """
748         return self.variable.to_numpy()
749 
750     def as_numpy(self: T_DataArray) -> T_DataArray:
751         """
752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
753 
754         See Also
755         --------
756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
757         Dataset.as_numpy : Converts all variables in a Dataset.
758         DataArray.values
759         DataArray.data
760         """
761         coords = {k: v.as_numpy() for k, v in self._coords.items()}
762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
763 
764     @property
765     def _in_memory(self) -> bool:
766         return self.variable._in_memory
767 
768     def _to_index(self) -> pd.Index:
769         return self.variable._to_index()
770 
771     def to_index(self) -> pd.Index:
772         """Convert this variable to a pandas.Index. Only possible for 1D
773         arrays.
774         """
775         return self.variable.to_index()
776 
777     @property
778     def dims(self) -> tuple[Hashable, ...]:
779         """Tuple of dimension names associated with this array.
780 
781         Note that the type of this property is inconsistent with
782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
783         consistently named properties.
784 
785         See Also
786         --------
787         DataArray.sizes
788         Dataset.dims
789         """
790         return self.variable.dims
791 
792     @dims.setter
793     def dims(self, value: Any) -> NoReturn:
794         raise AttributeError(
795             "you cannot assign dims on a DataArray. Use "
796             ".rename() or .swap_dims() instead."
797         )
798 
799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
800         if utils.is_dict_like(key):
801             return key
802         key = indexing.expanded_indexer(key, self.ndim)
803         return dict(zip(self.dims, key))
804 
805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
806         from xarray.core.dataset import _get_virtual_variable
807 
808         try:
809             var = self._coords[key]
810         except KeyError:
811             dim_sizes = dict(zip(self.dims, self.shape))
812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
813 
814         return self._replace_maybe_drop_dims(var, name=key)
815 
816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
817         if isinstance(key, str):
818             return self._getitem_coord(key)
819         else:
820             # xarray-style array indexing
821             return self.isel(indexers=self._item_key_to_dict(key))
822 
823     def __setitem__(self, key: Any, value: Any) -> None:
824         if isinstance(key, str):
825             self.coords[key] = value
826         else:
827             # Coordinates in key, value and self[key] should be consistent.
828             # TODO Coordinate consistency in key is checked here, but it
829             # causes unnecessary indexing. It should be optimized.
830             obj = self[key]
831             if isinstance(value, DataArray):
832                 assert_coordinate_consistent(value, obj.coords.variables)
833             # DataArray key -> Variable key
834             key = {
835                 k: v.variable if isinstance(v, DataArray) else v
836                 for k, v in self._item_key_to_dict(key).items()
837             }
838             self.variable[key] = value
839 
840     def __delitem__(self, key: Any) -> None:
841         del self.coords[key]
842 
843     @property
844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
845         """Places to look-up items for attribute-style access"""
846         yield from self._item_sources
847         yield self.attrs
848 
849     @property
850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
851         """Places to look-up items for key-completion"""
852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
853 
854         # virtual coordinates
855         # uses empty dict -- everything here can already be found in self.coords.
856         yield HybridMappingProxy(keys=self.dims, mapping={})
857 
858     def __contains__(self, key: Any) -> bool:
859         return key in self.data
860 
861     @property
862     def loc(self) -> _LocIndexer:
863         """Attribute for location based indexing like pandas."""
864         return _LocIndexer(self)
865 
866     @property
867     def attrs(self) -> dict[Any, Any]:
868         """Dictionary storing arbitrary metadata with this array."""
869         return self.variable.attrs
870 
871     @attrs.setter
872     def attrs(self, value: Mapping[Any, Any]) -> None:
873         self.variable.attrs = dict(value)
874 
875     @property
876     def encoding(self) -> dict[Any, Any]:
877         """Dictionary of format-specific settings for how this array should be
878         serialized."""
879         return self.variable.encoding
880 
881     @encoding.setter
882     def encoding(self, value: Mapping[Any, Any]) -> None:
883         self.variable.encoding = dict(value)
884 
885     def reset_encoding(self: T_DataArray) -> T_DataArray:
886         """Return a new DataArray without encoding on the array or any attached
887         coords."""
888         ds = self._to_temp_dataset().reset_encoding()
889         return self._from_temp_dataset(ds)
890 
891     @property
892     def indexes(self) -> Indexes:
893         """Mapping of pandas.Index objects used for label based indexing.
894 
895         Raises an error if this Dataset has indexes that cannot be coerced
896         to pandas.Index objects.
897 
898         See Also
899         --------
900         DataArray.xindexes
901 
902         """
903         return self.xindexes.to_pandas_indexes()
904 
905     @property
906     def xindexes(self) -> Indexes:
907         """Mapping of xarray Index objects used for label based indexing."""
908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
909 
910     @property
911     def coords(self) -> DataArrayCoordinates:
912         """Dictionary-like container of coordinate arrays."""
913         return DataArrayCoordinates(self)
914 
915     @overload
916     def reset_coords(
917         self: T_DataArray,
918         names: Dims = None,
919         drop: Literal[False] = False,
920     ) -> Dataset:
921         ...
922 
923     @overload
924     def reset_coords(
925         self: T_DataArray,
926         names: Dims = None,
927         *,
928         drop: Literal[True],
929     ) -> T_DataArray:
930         ...
931 
932     def reset_coords(
933         self: T_DataArray,
934         names: Dims = None,
935         drop: bool = False,
936     ) -> T_DataArray | Dataset:
937         """Given names of coordinates, reset them to become variables.
938 
939         Parameters
940         ----------
941         names : str, Iterable of Hashable or None, optional
942             Name(s) of non-index coordinates in this dataset to reset into
943             variables. By default, all non-index coordinates are reset.
944         drop : bool, default: False
945             If True, remove coordinates instead of converting them into
946             variables.
947 
948         Returns
949         -------
950         Dataset, or DataArray if ``drop == True``
951 
952         Examples
953         --------
954         >>> temperature = np.arange(25).reshape(5, 5)
955         >>> pressure = np.arange(50, 75).reshape(5, 5)
956         >>> da = xr.DataArray(
957         ...     data=temperature,
958         ...     dims=["x", "y"],
959         ...     coords=dict(
960         ...         lon=("x", np.arange(10, 15)),
961         ...         lat=("y", np.arange(20, 25)),
962         ...         Pressure=(["x", "y"], pressure),
963         ...     ),
964         ...     name="Temperature",
965         ... )
966         >>> da
967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
968         array([[ 0,  1,  2,  3,  4],
969                [ 5,  6,  7,  8,  9],
970                [10, 11, 12, 13, 14],
971                [15, 16, 17, 18, 19],
972                [20, 21, 22, 23, 24]])
973         Coordinates:
974             lon       (x) int64 10 11 12 13 14
975             lat       (y) int64 20 21 22 23 24
976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
977         Dimensions without coordinates: x, y
978 
979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
980 
981         >>> da.reset_coords(names="Pressure")
982         <xarray.Dataset>
983         Dimensions:      (x: 5, y: 5)
984         Coordinates:
985             lon          (x) int64 10 11 12 13 14
986             lat          (y) int64 20 21 22 23 24
987         Dimensions without coordinates: x, y
988         Data variables:
989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
991 
992         Return DataArray without targeted coordinate:
993 
994         >>> da.reset_coords(names="Pressure", drop=True)
995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
996         array([[ 0,  1,  2,  3,  4],
997                [ 5,  6,  7,  8,  9],
998                [10, 11, 12, 13, 14],
999                [15, 16, 17, 18, 19],
1000                [20, 21, 22, 23, 24]])
1001         Coordinates:
1002             lon      (x) int64 10 11 12 13 14
1003             lat      (y) int64 20 21 22 23 24
1004         Dimensions without coordinates: x, y
1005         """
1006         if names is None:
1007             names = set(self.coords) - set(self._indexes)
1008         dataset = self.coords.to_dataset().reset_coords(names, drop)
1009         if drop:
1010             return self._replace(coords=dataset._variables)
1011         if self.name is None:
1012             raise ValueError(
1013                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1014             )
1015         dataset[self.name] = self.variable
1016         return dataset
1017 
1018     def __dask_tokenize__(self):
1019         from dask.base import normalize_token
1020 
1021         return normalize_token((type(self), self._variable, self._coords, self._name))
1022 
1023     def __dask_graph__(self):
1024         return self._to_temp_dataset().__dask_graph__()
1025 
1026     def __dask_keys__(self):
1027         return self._to_temp_dataset().__dask_keys__()
1028 
1029     def __dask_layers__(self):
1030         return self._to_temp_dataset().__dask_layers__()
1031 
1032     @property
1033     def __dask_optimize__(self):
1034         return self._to_temp_dataset().__dask_optimize__
1035 
1036     @property
1037     def __dask_scheduler__(self):
1038         return self._to_temp_dataset().__dask_scheduler__
1039 
1040     def __dask_postcompute__(self):
1041         func, args = self._to_temp_dataset().__dask_postcompute__()
1042         return self._dask_finalize, (self.name, func) + args
1043 
1044     def __dask_postpersist__(self):
1045         func, args = self._to_temp_dataset().__dask_postpersist__()
1046         return self._dask_finalize, (self.name, func) + args
1047 
1048     @staticmethod
1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1050         ds = func(results, *args, **kwargs)
1051         variable = ds._variables.pop(_THIS_ARRAY)
1052         coords = ds._variables
1053         indexes = ds._indexes
1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1055 
1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1057         """Manually trigger loading of this array's data from disk or a
1058         remote source into memory and return this array.
1059 
1060         Normally, it should not be necessary to call this method in user code,
1061         because all xarray functions should either work on deferred data or
1062         load data automatically. However, this method can be necessary when
1063         working with many file objects on disk.
1064 
1065         Parameters
1066         ----------
1067         **kwargs : dict
1068             Additional keyword arguments passed on to ``dask.compute``.
1069 
1070         See Also
1071         --------
1072         dask.compute
1073         """
1074         ds = self._to_temp_dataset().load(**kwargs)
1075         new = self._from_temp_dataset(ds)
1076         self._variable = new._variable
1077         self._coords = new._coords
1078         return self
1079 
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)
1101 
1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1103         """Trigger computation in constituent dask arrays
1104 
1105         This keeps them as dask arrays but encourages them to keep data in
1106         memory.  This is particularly useful when on a distributed machine.
1107         When on a single machine consider using ``.compute()`` instead.
1108 
1109         Parameters
1110         ----------
1111         **kwargs : dict
1112             Additional keyword arguments passed on to ``dask.persist``.
1113 
1114         See Also
1115         --------
1116         dask.persist
1117         """
1118         ds = self._to_temp_dataset().persist(**kwargs)
1119         return self._from_temp_dataset(ds)
1120 
1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1122         """Returns a copy of this array.
1123 
1124         If `deep=True`, a deep copy is made of the data array.
1125         Otherwise, a shallow copy is made, and the returned data array's
1126         values are a new view of this data array's values.
1127 
1128         Use `data` to create a new object with the same structure as
1129         original but entirely new data.
1130 
1131         Parameters
1132         ----------
1133         deep : bool, optional
1134             Whether the data array and its coordinates are loaded into memory
1135             and copied onto the new object. Default is True.
1136         data : array_like, optional
1137             Data to use in the new object. Must have same shape as original.
1138             When `data` is used, `deep` is ignored for all data variables,
1139             and only used for coords.
1140 
1141         Returns
1142         -------
1143         copy : DataArray
1144             New object with dimensions, attributes, coordinates, name,
1145             encoding, and optionally data copied from original.
1146 
1147         Examples
1148         --------
1149         Shallow versus deep copy
1150 
1151         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1152         >>> array.copy()
1153         <xarray.DataArray (x: 3)>
1154         array([1, 2, 3])
1155         Coordinates:
1156           * x        (x) <U1 'a' 'b' 'c'
1157         >>> array_0 = array.copy(deep=False)
1158         >>> array_0[0] = 7
1159         >>> array_0
1160         <xarray.DataArray (x: 3)>
1161         array([7, 2, 3])
1162         Coordinates:
1163           * x        (x) <U1 'a' 'b' 'c'
1164         >>> array
1165         <xarray.DataArray (x: 3)>
1166         array([7, 2, 3])
1167         Coordinates:
1168           * x        (x) <U1 'a' 'b' 'c'
1169 
1170         Changing the data using the ``data`` argument maintains the
1171         structure of the original object, but with the new data. Original
1172         object is unaffected.
1173 
1174         >>> array.copy(data=[0.1, 0.2, 0.3])
1175         <xarray.DataArray (x: 3)>
1176         array([0.1, 0.2, 0.3])
1177         Coordinates:
1178           * x        (x) <U1 'a' 'b' 'c'
1179         >>> array
1180         <xarray.DataArray (x: 3)>
1181         array([7, 2, 3])
1182         Coordinates:
1183           * x        (x) <U1 'a' 'b' 'c'
1184 
1185         See Also
1186         --------
1187         pandas.DataFrame.copy
1188         """
1189         return self._copy(deep=deep, data=data)
1190 
1191     def _copy(
1192         self: T_DataArray,
1193         deep: bool = True,
1194         data: Any = None,
1195         memo: dict[int, Any] | None = None,
1196     ) -> T_DataArray:
1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1199 
1200         coords = {}
1201         for k, v in self._coords.items():
1202             if k in index_vars:
1203                 coords[k] = index_vars[k]
1204             else:
1205                 coords[k] = v._copy(deep=deep, memo=memo)
1206 
1207         return self._replace(variable, coords, indexes=indexes)
1208 
1209     def __copy__(self: T_DataArray) -> T_DataArray:
1210         return self._copy(deep=False)
1211 
1212     def __deepcopy__(
1213         self: T_DataArray, memo: dict[int, Any] | None = None
1214     ) -> T_DataArray:
1215         return self._copy(deep=True, memo=memo)
1216 
1217     # mutable objects should not be Hashable
1218     # https://github.com/python/mypy/issues/4266
1219     __hash__ = None  # type: ignore[assignment]
1220 
1221     @property
1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1223         """
1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1225         the underlying data is not a dask array.
1226 
1227         See Also
1228         --------
1229         DataArray.chunk
1230         DataArray.chunksizes
1231         xarray.unify_chunks
1232         """
1233         return self.variable.chunks
1234 
1235     @property
1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1237         """
1238         Mapping from dimension names to block lengths for this dataarray's data, or None if
1239         the underlying data is not a dask array.
1240         Cannot be modified directly, but can be modified by calling .chunk().
1241 
1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1243         instead of a tuple of chunk shapes.
1244 
1245         See Also
1246         --------
1247         DataArray.chunk
1248         DataArray.chunks
1249         xarray.unify_chunks
1250         """
1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1252         return get_chunksizes(all_variables)
1253 
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)
1333 
1334     def isel(
1335         self: T_DataArray,
1336         indexers: Mapping[Any, Any] | None = None,
1337         drop: bool = False,
1338         missing_dims: ErrorOptionsWithWarn = "raise",
1339         **indexers_kwargs: Any,
1340     ) -> T_DataArray:
1341         """Return a new DataArray whose data is given by selecting indexes
1342         along the specified dimension(s).
1343 
1344         Parameters
1345         ----------
1346         indexers : dict, optional
1347             A dict with keys matching dimensions and values given
1348             by integers, slice objects or arrays.
1349             indexer can be a integer, slice, array-like or DataArray.
1350             If DataArrays are passed as indexers, xarray-style indexing will be
1351             carried out. See :ref:`indexing` for the details.
1352             One of indexers or indexers_kwargs must be provided.
1353         drop : bool, default: False
1354             If ``drop=True``, drop coordinates variables indexed by integers
1355             instead of making them scalar.
1356         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1357             What to do if dimensions that should be selected from are not present in the
1358             DataArray:
1359             - "raise": raise an exception
1360             - "warn": raise a warning, and ignore the missing dimensions
1361             - "ignore": ignore the missing dimensions
1362         **indexers_kwargs : {dim: indexer, ...}, optional
1363             The keyword arguments form of ``indexers``.
1364 
1365         Returns
1366         -------
1367         indexed : xarray.DataArray
1368 
1369         See Also
1370         --------
1371         Dataset.isel
1372         DataArray.sel
1373 
1374         Examples
1375         --------
1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1377         >>> da
1378         <xarray.DataArray (x: 5, y: 5)>
1379         array([[ 0,  1,  2,  3,  4],
1380                [ 5,  6,  7,  8,  9],
1381                [10, 11, 12, 13, 14],
1382                [15, 16, 17, 18, 19],
1383                [20, 21, 22, 23, 24]])
1384         Dimensions without coordinates: x, y
1385 
1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> da = da.isel(x=tgt_x, y=tgt_y)
1389         >>> da
1390         <xarray.DataArray (points: 5)>
1391         array([ 0,  6, 12, 18, 24])
1392         Dimensions without coordinates: points
1393         """
1394 
1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1396 
1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1398             ds = self._to_temp_dataset()._isel_fancy(
1399                 indexers, drop=drop, missing_dims=missing_dims
1400             )
1401             return self._from_temp_dataset(ds)
1402 
1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1404         # lists, or zero or one-dimensional np.ndarray's
1405 
1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1408 
1409         coords = {}
1410         for coord_name, coord_value in self._coords.items():
1411             if coord_name in index_variables:
1412                 coord_value = index_variables[coord_name]
1413             else:
1414                 coord_indexers = {
1415                     k: v for k, v in indexers.items() if k in coord_value.dims
1416                 }
1417                 if coord_indexers:
1418                     coord_value = coord_value.isel(coord_indexers)
1419                     if drop and coord_value.ndim == 0:
1420                         continue
1421             coords[coord_name] = coord_value
1422 
1423         return self._replace(variable=variable, coords=coords, indexes=indexes)
1424 
1425     def sel(
1426         self: T_DataArray,
1427         indexers: Mapping[Any, Any] | None = None,
1428         method: str | None = None,
1429         tolerance=None,
1430         drop: bool = False,
1431         **indexers_kwargs: Any,
1432     ) -> T_DataArray:
1433         """Return a new DataArray whose data is given by selecting index
1434         labels along the specified dimension(s).
1435 
1436         In contrast to `DataArray.isel`, indexers for this method should use
1437         labels instead of integers.
1438 
1439         Under the hood, this method is powered by using pandas's powerful Index
1440         objects. This makes label based indexing essentially just as fast as
1441         using integer indexing.
1442 
1443         It also means this method uses pandas's (well documented) logic for
1444         indexing. This means you can use string shortcuts for datetime indexes
1445         (e.g., '2000-01' to select all values in January 2000). It also means
1446         that slices are treated as inclusive of both the start and stop values,
1447         unlike normal Python indexing.
1448 
1449         .. warning::
1450 
1451           Do not try to assign values when using any of the indexing methods
1452           ``isel`` or ``sel``::
1453 
1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1455             # DO NOT do this
1456             da.isel(x=[0, 1, 2])[1] = -1
1457 
1458           Assigning values with the chained indexing using ``.sel`` or
1459           ``.isel`` fails silently.
1460 
1461         Parameters
1462         ----------
1463         indexers : dict, optional
1464             A dict with keys matching dimensions and values given
1465             by scalars, slices or arrays of tick labels. For dimensions with
1466             multi-index, the indexer may also be a dict-like object with keys
1467             matching index level names.
1468             If DataArrays are passed as indexers, xarray-style indexing will be
1469             carried out. See :ref:`indexing` for the details.
1470             One of indexers or indexers_kwargs must be provided.
1471         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1472             Method to use for inexact matches:
1473 
1474             - None (default): only exact matches
1475             - pad / ffill: propagate last valid index value forward
1476             - backfill / bfill: propagate next valid index value backward
1477             - nearest: use nearest valid index value
1478 
1479         tolerance : optional
1480             Maximum distance between original and new labels for inexact
1481             matches. The values of the index at the matching locations must
1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1483         drop : bool, optional
1484             If ``drop=True``, drop coordinates variables in `indexers` instead
1485             of making them scalar.
1486         **indexers_kwargs : {dim: indexer, ...}, optional
1487             The keyword arguments form of ``indexers``.
1488             One of indexers or indexers_kwargs must be provided.
1489 
1490         Returns
1491         -------
1492         obj : DataArray
1493             A new DataArray with the same contents as this DataArray, except the
1494             data and each dimension is indexed by the appropriate indexers.
1495             If indexer DataArrays have coordinates that do not conflict with
1496             this object, then these coordinates will be attached.
1497             In general, each array's data will be a view of the array's data
1498             in this DataArray, unless vectorized indexing was triggered by using
1499             an array indexer, in which case the data will be a copy.
1500 
1501         See Also
1502         --------
1503         Dataset.sel
1504         DataArray.isel
1505 
1506         Examples
1507         --------
1508         >>> da = xr.DataArray(
1509         ...     np.arange(25).reshape(5, 5),
1510         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1511         ...     dims=("x", "y"),
1512         ... )
1513         >>> da
1514         <xarray.DataArray (x: 5, y: 5)>
1515         array([[ 0,  1,  2,  3,  4],
1516                [ 5,  6,  7,  8,  9],
1517                [10, 11, 12, 13, 14],
1518                [15, 16, 17, 18, 19],
1519                [20, 21, 22, 23, 24]])
1520         Coordinates:
1521           * x        (x) int64 0 1 2 3 4
1522           * y        (y) int64 0 1 2 3 4
1523 
1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1527         >>> da
1528         <xarray.DataArray (points: 5)>
1529         array([ 0,  6, 12, 18, 24])
1530         Coordinates:
1531             x        (points) int64 0 1 2 3 4
1532             y        (points) int64 0 1 2 3 4
1533         Dimensions without coordinates: points
1534         """
1535         ds = self._to_temp_dataset().sel(
1536             indexers=indexers,
1537             drop=drop,
1538             method=method,
1539             tolerance=tolerance,
1540             **indexers_kwargs,
1541         )
1542         return self._from_temp_dataset(ds)
1543 
1544     def head(
1545         self: T_DataArray,
1546         indexers: Mapping[Any, int] | int | None = None,
1547         **indexers_kwargs: Any,
1548     ) -> T_DataArray:
1549         """Return a new DataArray whose data is given by the the first `n`
1550         values along the specified dimension(s). Default `n` = 5
1551 
1552         See Also
1553         --------
1554         Dataset.head
1555         DataArray.tail
1556         DataArray.thin
1557 
1558         Examples
1559         --------
1560         >>> da = xr.DataArray(
1561         ...     np.arange(25).reshape(5, 5),
1562         ...     dims=("x", "y"),
1563         ... )
1564         >>> da
1565         <xarray.DataArray (x: 5, y: 5)>
1566         array([[ 0,  1,  2,  3,  4],
1567                [ 5,  6,  7,  8,  9],
1568                [10, 11, 12, 13, 14],
1569                [15, 16, 17, 18, 19],
1570                [20, 21, 22, 23, 24]])
1571         Dimensions without coordinates: x, y
1572 
1573         >>> da.head(x=1)
1574         <xarray.DataArray (x: 1, y: 5)>
1575         array([[0, 1, 2, 3, 4]])
1576         Dimensions without coordinates: x, y
1577 
1578         >>> da.head({"x": 2, "y": 2})
1579         <xarray.DataArray (x: 2, y: 2)>
1580         array([[0, 1],
1581                [5, 6]])
1582         Dimensions without coordinates: x, y
1583         """
1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1585         return self._from_temp_dataset(ds)
1586 
1587     def tail(
1588         self: T_DataArray,
1589         indexers: Mapping[Any, int] | int | None = None,
1590         **indexers_kwargs: Any,
1591     ) -> T_DataArray:
1592         """Return a new DataArray whose data is given by the the last `n`
1593         values along the specified dimension(s). Default `n` = 5
1594 
1595         See Also
1596         --------
1597         Dataset.tail
1598         DataArray.head
1599         DataArray.thin
1600 
1601         Examples
1602         --------
1603         >>> da = xr.DataArray(
1604         ...     np.arange(25).reshape(5, 5),
1605         ...     dims=("x", "y"),
1606         ... )
1607         >>> da
1608         <xarray.DataArray (x: 5, y: 5)>
1609         array([[ 0,  1,  2,  3,  4],
1610                [ 5,  6,  7,  8,  9],
1611                [10, 11, 12, 13, 14],
1612                [15, 16, 17, 18, 19],
1613                [20, 21, 22, 23, 24]])
1614         Dimensions without coordinates: x, y
1615 
1616         >>> da.tail(y=1)
1617         <xarray.DataArray (x: 5, y: 1)>
1618         array([[ 4],
1619                [ 9],
1620                [14],
1621                [19],
1622                [24]])
1623         Dimensions without coordinates: x, y
1624 
1625         >>> da.tail({"x": 2, "y": 2})
1626         <xarray.DataArray (x: 2, y: 2)>
1627         array([[18, 19],
1628                [23, 24]])
1629         Dimensions without coordinates: x, y
1630         """
1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1632         return self._from_temp_dataset(ds)
1633 
1634     def thin(
1635         self: T_DataArray,
1636         indexers: Mapping[Any, int] | int | None = None,
1637         **indexers_kwargs: Any,
1638     ) -> T_DataArray:
1639         """Return a new DataArray whose data is given by each `n` value
1640         along the specified dimension(s).
1641 
1642         Examples
1643         --------
1644         >>> x_arr = np.arange(0, 26)
1645         >>> x_arr
1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1647                17, 18, 19, 20, 21, 22, 23, 24, 25])
1648         >>> x = xr.DataArray(
1649         ...     np.reshape(x_arr, (2, 13)),
1650         ...     dims=("x", "y"),
1651         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1652         ... )
1653         >>> x
1654         <xarray.DataArray (x: 2, y: 13)>
1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1657         Coordinates:
1658           * x        (x) int64 0 1
1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1660 
1661         >>>
1662         >>> x.thin(3)
1663         <xarray.DataArray (x: 1, y: 5)>
1664         array([[ 0,  3,  6,  9, 12]])
1665         Coordinates:
1666           * x        (x) int64 0
1667           * y        (y) int64 0 3 6 9 12
1668         >>> x.thin({"x": 2, "y": 5})
1669         <xarray.DataArray (x: 1, y: 3)>
1670         array([[ 0,  5, 10]])
1671         Coordinates:
1672           * x        (x) int64 0
1673           * y        (y) int64 0 5 10
1674 
1675         See Also
1676         --------
1677         Dataset.thin
1678         DataArray.head
1679         DataArray.tail
1680         """
1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1682         return self._from_temp_dataset(ds)
1683 
1684     def broadcast_like(
1685         self: T_DataArray,
1686         other: DataArray | Dataset,
1687         exclude: Iterable[Hashable] | None = None,
1688     ) -> T_DataArray:
1689         """Broadcast this DataArray against another Dataset or DataArray.
1690 
1691         This is equivalent to xr.broadcast(other, self)[1]
1692 
1693         xarray objects are broadcast against each other in arithmetic
1694         operations, so this method is not be necessary for most uses.
1695 
1696         If no change is needed, the input data is returned to the output
1697         without being copied.
1698 
1699         If new coords are added by the broadcast, their values are
1700         NaN filled.
1701 
1702         Parameters
1703         ----------
1704         other : Dataset or DataArray
1705             Object against which to broadcast this array.
1706         exclude : iterable of Hashable, optional
1707             Dimensions that must not be broadcasted
1708 
1709         Returns
1710         -------
1711         new_da : DataArray
1712             The caller broadcasted against ``other``.
1713 
1714         Examples
1715         --------
1716         >>> arr1 = xr.DataArray(
1717         ...     np.random.randn(2, 3),
1718         ...     dims=("x", "y"),
1719         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1720         ... )
1721         >>> arr2 = xr.DataArray(
1722         ...     np.random.randn(3, 2),
1723         ...     dims=("x", "y"),
1724         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1725         ... )
1726         >>> arr1
1727         <xarray.DataArray (x: 2, y: 3)>
1728         array([[ 1.76405235,  0.40015721,  0.97873798],
1729                [ 2.2408932 ,  1.86755799, -0.97727788]])
1730         Coordinates:
1731           * x        (x) <U1 'a' 'b'
1732           * y        (y) <U1 'a' 'b' 'c'
1733         >>> arr2
1734         <xarray.DataArray (x: 3, y: 2)>
1735         array([[ 0.95008842, -0.15135721],
1736                [-0.10321885,  0.4105985 ],
1737                [ 0.14404357,  1.45427351]])
1738         Coordinates:
1739           * x        (x) <U1 'a' 'b' 'c'
1740           * y        (y) <U1 'a' 'b'
1741         >>> arr1.broadcast_like(arr2)
1742         <xarray.DataArray (x: 3, y: 3)>
1743         array([[ 1.76405235,  0.40015721,  0.97873798],
1744                [ 2.2408932 ,  1.86755799, -0.97727788],
1745                [        nan,         nan,         nan]])
1746         Coordinates:
1747           * x        (x) <U1 'a' 'b' 'c'
1748           * y        (y) <U1 'a' 'b' 'c'
1749         """
1750         if exclude is None:
1751             exclude = set()
1752         else:
1753             exclude = set(exclude)
1754         args = align(other, self, join="outer", copy=False, exclude=exclude)
1755 
1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1757 
1758         return _broadcast_helper(
1759             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1760         )
1761 
1762     def _reindex_callback(
1763         self: T_DataArray,
1764         aligner: alignment.Aligner,
1765         dim_pos_indexers: dict[Hashable, Any],
1766         variables: dict[Hashable, Variable],
1767         indexes: dict[Hashable, Index],
1768         fill_value: Any,
1769         exclude_dims: frozenset[Hashable],
1770         exclude_vars: frozenset[Hashable],
1771     ) -> T_DataArray:
1772         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1773 
1774         if isinstance(fill_value, dict):
1775             fill_value = fill_value.copy()
1776             sentinel = object()
1777             value = fill_value.pop(self.name, sentinel)
1778             if value is not sentinel:
1779                 fill_value[_THIS_ARRAY] = value
1780 
1781         ds = self._to_temp_dataset()
1782         reindexed = ds._reindex_callback(
1783             aligner,
1784             dim_pos_indexers,
1785             variables,
1786             indexes,
1787             fill_value,
1788             exclude_dims,
1789             exclude_vars,
1790         )
1791         return self._from_temp_dataset(reindexed)
1792 
1793     def reindex_like(
1794         self: T_DataArray,
1795         other: DataArray | Dataset,
1796         method: ReindexMethodOptions = None,
1797         tolerance: int | float | Iterable[int | float] | None = None,
1798         copy: bool = True,
1799         fill_value=dtypes.NA,
1800     ) -> T_DataArray:
1801         """Conform this object onto the indexes of another object, filling in
1802         missing values with ``fill_value``. The default fill value is NaN.
1803 
1804         Parameters
1805         ----------
1806         other : Dataset or DataArray
1807             Object with an 'indexes' attribute giving a mapping from dimension
1808             names to pandas.Index objects, which provides coordinates upon
1809             which to index the variables in this dataset. The indexes on this
1810             other object need not be the same as the indexes on this
1811             dataset. Any mis-matched index values will be filled in with
1812             NaN, and any mis-matched dimension names will simply be ignored.
1813         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1814             Method to use for filling index values from other not found on this
1815             data array:
1816 
1817             - None (default): don't fill gaps
1818             - pad / ffill: propagate last valid index value forward
1819             - backfill / bfill: propagate next valid index value backward
1820             - nearest: use nearest valid index value
1821 
1822         tolerance : optional
1823             Maximum distance between original and new labels for inexact
1824             matches. The values of the index at the matching locations must
1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1826             Tolerance may be a scalar value, which applies the same tolerance
1827             to all values, or list-like, which applies variable tolerance per
1828             element. List-like must be the same size as the index and its dtype
1829             must exactly match the index’s type.
1830         copy : bool, default: True
1831             If ``copy=True``, data in the return value is always copied. If
1832             ``copy=False`` and reindexing is unnecessary, or can be performed
1833             with only slice operations, then the output may share memory with
1834             the input. In either case, a new xarray object is always returned.
1835         fill_value : scalar or dict-like, optional
1836             Value to use for newly missing values. If a dict-like, maps
1837             variable names (including coordinates) to fill values. Use this
1838             data array's name to refer to the data array's values.
1839 
1840         Returns
1841         -------
1842         reindexed : DataArray
1843             Another dataset array, with this array's data but coordinates from
1844             the other object.
1845 
1846         Examples
1847         --------
1848         >>> data = np.arange(12).reshape(4, 3)
1849         >>> da1 = xr.DataArray(
1850         ...     data=data,
1851         ...     dims=["x", "y"],
1852         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1853         ... )
1854         >>> da1
1855         <xarray.DataArray (x: 4, y: 3)>
1856         array([[ 0,  1,  2],
1857                [ 3,  4,  5],
1858                [ 6,  7,  8],
1859                [ 9, 10, 11]])
1860         Coordinates:
1861           * x        (x) int64 10 20 30 40
1862           * y        (y) int64 70 80 90
1863         >>> da2 = xr.DataArray(
1864         ...     data=data,
1865         ...     dims=["x", "y"],
1866         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1867         ... )
1868         >>> da2
1869         <xarray.DataArray (x: 4, y: 3)>
1870         array([[ 0,  1,  2],
1871                [ 3,  4,  5],
1872                [ 6,  7,  8],
1873                [ 9, 10, 11]])
1874         Coordinates:
1875           * x        (x) int64 40 30 20 10
1876           * y        (y) int64 90 80 70
1877 
1878         Reindexing with both DataArrays having the same coordinates set, but in different order:
1879 
1880         >>> da1.reindex_like(da2)
1881         <xarray.DataArray (x: 4, y: 3)>
1882         array([[11, 10,  9],
1883                [ 8,  7,  6],
1884                [ 5,  4,  3],
1885                [ 2,  1,  0]])
1886         Coordinates:
1887           * x        (x) int64 40 30 20 10
1888           * y        (y) int64 90 80 70
1889 
1890         Reindexing with the other array having coordinates which the source array doesn't have:
1891 
1892         >>> data = np.arange(12).reshape(4, 3)
1893         >>> da1 = xr.DataArray(
1894         ...     data=data,
1895         ...     dims=["x", "y"],
1896         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1897         ... )
1898         >>> da2 = xr.DataArray(
1899         ...     data=data,
1900         ...     dims=["x", "y"],
1901         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1902         ... )
1903         >>> da1.reindex_like(da2)
1904         <xarray.DataArray (x: 4, y: 3)>
1905         array([[ 3.,  4.,  5.],
1906                [ 0.,  1.,  2.],
1907                [nan, nan, nan],
1908                [nan, nan, nan]])
1909         Coordinates:
1910           * x        (x) int64 20 10 29 39
1911           * y        (y) int64 70 80 90
1912 
1913         Filling missing values with the previous valid index with respect to the coordinates' value:
1914 
1915         >>> da1.reindex_like(da2, method="ffill")
1916         <xarray.DataArray (x: 4, y: 3)>
1917         array([[3, 4, 5],
1918                [0, 1, 2],
1919                [3, 4, 5],
1920                [6, 7, 8]])
1921         Coordinates:
1922           * x        (x) int64 20 10 29 39
1923           * y        (y) int64 70 80 90
1924 
1925         Filling missing values while tolerating specified error for inexact matches:
1926 
1927         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1928         <xarray.DataArray (x: 4, y: 3)>
1929         array([[ 3.,  4.,  5.],
1930                [ 0.,  1.,  2.],
1931                [nan, nan, nan],
1932                [nan, nan, nan]])
1933         Coordinates:
1934           * x        (x) int64 20 10 29 39
1935           * y        (y) int64 70 80 90
1936 
1937         Filling missing values with manually specified values:
1938 
1939         >>> da1.reindex_like(da2, fill_value=19)
1940         <xarray.DataArray (x: 4, y: 3)>
1941         array([[ 3,  4,  5],
1942                [ 0,  1,  2],
1943                [19, 19, 19],
1944                [19, 19, 19]])
1945         Coordinates:
1946           * x        (x) int64 20 10 29 39
1947           * y        (y) int64 70 80 90
1948 
1949         See Also
1950         --------
1951         DataArray.reindex
1952         align
1953         """
1954         return alignment.reindex_like(
1955             self,
1956             other=other,
1957             method=method,
1958             tolerance=tolerance,
1959             copy=copy,
1960             fill_value=fill_value,
1961         )
1962 
1963     def reindex(
1964         self: T_DataArray,
1965         indexers: Mapping[Any, Any] | None = None,
1966         method: ReindexMethodOptions = None,
1967         tolerance: float | Iterable[float] | None = None,
1968         copy: bool = True,
1969         fill_value=dtypes.NA,
1970         **indexers_kwargs: Any,
1971     ) -> T_DataArray:
1972         """Conform this object onto the indexes of another object, filling in
1973         missing values with ``fill_value``. The default fill value is NaN.
1974 
1975         Parameters
1976         ----------
1977         indexers : dict, optional
1978             Dictionary with keys given by dimension names and values given by
1979             arrays of coordinates tick labels. Any mis-matched coordinate
1980             values will be filled in with NaN, and any mis-matched dimension
1981             names will simply be ignored.
1982             One of indexers or indexers_kwargs must be provided.
1983         copy : bool, optional
1984             If ``copy=True``, data in the return value is always copied. If
1985             ``copy=False`` and reindexing is unnecessary, or can be performed
1986             with only slice operations, then the output may share memory with
1987             the input. In either case, a new xarray object is always returned.
1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1989             Method to use for filling index values in ``indexers`` not found on
1990             this data array:
1991 
1992             - None (default): don't fill gaps
1993             - pad / ffill: propagate last valid index value forward
1994             - backfill / bfill: propagate next valid index value backward
1995             - nearest: use nearest valid index value
1996 
1997         tolerance : float | Iterable[float] | None, default: None
1998             Maximum distance between original and new labels for inexact
1999             matches. The values of the index at the matching locations must
2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2001             Tolerance may be a scalar value, which applies the same tolerance
2002             to all values, or list-like, which applies variable tolerance per
2003             element. List-like must be the same size as the index and its dtype
2004             must exactly match the index’s type.
2005         fill_value : scalar or dict-like, optional
2006             Value to use for newly missing values. If a dict-like, maps
2007             variable names (including coordinates) to fill values. Use this
2008             data array's name to refer to the data array's values.
2009         **indexers_kwargs : {dim: indexer, ...}, optional
2010             The keyword arguments form of ``indexers``.
2011             One of indexers or indexers_kwargs must be provided.
2012 
2013         Returns
2014         -------
2015         reindexed : DataArray
2016             Another dataset array, with this array's data but replaced
2017             coordinates.
2018 
2019         Examples
2020         --------
2021         Reverse latitude:
2022 
2023         >>> da = xr.DataArray(
2024         ...     np.arange(4),
2025         ...     coords=[np.array([90, 89, 88, 87])],
2026         ...     dims="lat",
2027         ... )
2028         >>> da
2029         <xarray.DataArray (lat: 4)>
2030         array([0, 1, 2, 3])
2031         Coordinates:
2032           * lat      (lat) int64 90 89 88 87
2033         >>> da.reindex(lat=da.lat[::-1])
2034         <xarray.DataArray (lat: 4)>
2035         array([3, 2, 1, 0])
2036         Coordinates:
2037           * lat      (lat) int64 87 88 89 90
2038 
2039         See Also
2040         --------
2041         DataArray.reindex_like
2042         align
2043         """
2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2045         return alignment.reindex(
2046             self,
2047             indexers=indexers,
2048             method=method,
2049             tolerance=tolerance,
2050             copy=copy,
2051             fill_value=fill_value,
2052         )
2053 
2054     def interp(
2055         self: T_DataArray,
2056         coords: Mapping[Any, Any] | None = None,
2057         method: InterpOptions = "linear",
2058         assume_sorted: bool = False,
2059         kwargs: Mapping[str, Any] | None = None,
2060         **coords_kwargs: Any,
2061     ) -> T_DataArray:
2062         """Interpolate a DataArray onto new coordinates
2063 
2064         Performs univariate or multivariate interpolation of a DataArray onto
2065         new coordinates using scipy's interpolation routines. If interpolating
2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2067         called. When interpolating along multiple existing dimensions, an
2068         attempt is made to decompose the interpolation into multiple
2069         1-dimensional interpolations. If this is possible,
2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2071         :py:func:`scipy.interpolate.interpn` is called.
2072 
2073         Parameters
2074         ----------
2075         coords : dict, optional
2076             Mapping from dimension names to the new coordinates.
2077             New coordinate can be a scalar, array-like or DataArray.
2078             If DataArrays are passed as new coordinates, their dimensions are
2079             used for the broadcasting. Missing values are skipped.
2080         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2081             The method used to interpolate. The method should be supported by
2082             the scipy interpolator:
2083 
2084             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2085               "quadratic", "cubic", "polynomial"}
2086             - ``interpn``: {"linear", "nearest"}
2087 
2088             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2089             also be provided.
2090         assume_sorted : bool, default: False
2091             If False, values of x can be in any order and they are sorted
2092             first. If True, x has to be an array of monotonically increasing
2093             values.
2094         kwargs : dict-like or None, default: None
2095             Additional keyword arguments passed to scipy's interpolator. Valid
2096             options and their behavior depend whether ``interp1d`` or
2097             ``interpn`` is used.
2098         **coords_kwargs : {dim: coordinate, ...}, optional
2099             The keyword arguments form of ``coords``.
2100             One of coords or coords_kwargs must be provided.
2101 
2102         Returns
2103         -------
2104         interpolated : DataArray
2105             New dataarray on the new coordinates.
2106 
2107         Notes
2108         -----
2109         scipy is required.
2110 
2111         See Also
2112         --------
2113         scipy.interpolate.interp1d
2114         scipy.interpolate.interpn
2115 
2116         Examples
2117         --------
2118         >>> da = xr.DataArray(
2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2120         ...     dims=("x", "y"),
2121         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2122         ... )
2123         >>> da
2124         <xarray.DataArray (x: 3, y: 4)>
2125         array([[ 1.,  4.,  2.,  9.],
2126                [ 2.,  7.,  6., nan],
2127                [ 6., nan,  5.,  8.]])
2128         Coordinates:
2129           * x        (x) int64 0 1 2
2130           * y        (y) int64 10 12 14 16
2131 
2132         1D linear interpolation (the default):
2133 
2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2135         <xarray.DataArray (x: 4, y: 4)>
2136         array([[1.  , 4.  , 2.  ,  nan],
2137                [1.75, 6.25, 5.  ,  nan],
2138                [3.  ,  nan, 5.75,  nan],
2139                [5.  ,  nan, 5.25,  nan]])
2140         Coordinates:
2141           * y        (y) int64 10 12 14 16
2142           * x        (x) float64 0.0 0.75 1.25 1.75
2143 
2144         1D nearest interpolation:
2145 
2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2147         <xarray.DataArray (x: 4, y: 4)>
2148         array([[ 1.,  4.,  2.,  9.],
2149                [ 2.,  7.,  6., nan],
2150                [ 2.,  7.,  6., nan],
2151                [ 6., nan,  5.,  8.]])
2152         Coordinates:
2153           * y        (y) int64 10 12 14 16
2154           * x        (x) float64 0.0 0.75 1.25 1.75
2155 
2156         1D linear extrapolation:
2157 
2158         >>> da.interp(
2159         ...     x=[1, 1.5, 2.5, 3.5],
2160         ...     method="linear",
2161         ...     kwargs={"fill_value": "extrapolate"},
2162         ... )
2163         <xarray.DataArray (x: 4, y: 4)>
2164         array([[ 2. ,  7. ,  6. ,  nan],
2165                [ 4. ,  nan,  5.5,  nan],
2166                [ 8. ,  nan,  4.5,  nan],
2167                [12. ,  nan,  3.5,  nan]])
2168         Coordinates:
2169           * y        (y) int64 10 12 14 16
2170           * x        (x) float64 1.0 1.5 2.5 3.5
2171 
2172         2D linear interpolation:
2173 
2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2175         <xarray.DataArray (x: 4, y: 3)>
2176         array([[2.5  , 3.   ,   nan],
2177                [4.   , 5.625,   nan],
2178                [  nan,   nan,   nan],
2179                [  nan,   nan,   nan]])
2180         Coordinates:
2181           * x        (x) float64 0.0 0.75 1.25 1.75
2182           * y        (y) int64 11 13 15
2183         """
2184         if self.dtype.kind not in "uifc":
2185             raise TypeError(
2186                 "interp only works for a numeric type array. "
2187                 "Given {}.".format(self.dtype)
2188             )
2189         ds = self._to_temp_dataset().interp(
2190             coords,
2191             method=method,
2192             kwargs=kwargs,
2193             assume_sorted=assume_sorted,
2194             **coords_kwargs,
2195         )
2196         return self._from_temp_dataset(ds)
2197 
2198     def interp_like(
2199         self: T_DataArray,
2200         other: DataArray | Dataset,
2201         method: InterpOptions = "linear",
2202         assume_sorted: bool = False,
2203         kwargs: Mapping[str, Any] | None = None,
2204     ) -> T_DataArray:
2205         """Interpolate this object onto the coordinates of another object,
2206         filling out of range values with NaN.
2207 
2208         If interpolating along a single existing dimension,
2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2210         along multiple existing dimensions, an attempt is made to decompose the
2211         interpolation into multiple 1-dimensional interpolations. If this is
2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2213         :py:func:`scipy.interpolate.interpn` is called.
2214 
2215         Parameters
2216         ----------
2217         other : Dataset or DataArray
2218             Object with an 'indexes' attribute giving a mapping from dimension
2219             names to an 1d array-like, which provides coordinates upon
2220             which to index the variables in this dataset. Missing values are skipped.
2221         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2222             The method used to interpolate. The method should be supported by
2223             the scipy interpolator:
2224 
2225             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2226               "polynomial"} when ``interp1d`` is called.
2227             - {"linear", "nearest"} when ``interpn`` is called.
2228 
2229             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2230             also be provided.
2231         assume_sorted : bool, default: False
2232             If False, values of coordinates that are interpolated over can be
2233             in any order and they are sorted first. If True, interpolated
2234             coordinates are assumed to be an array of monotonically increasing
2235             values.
2236         kwargs : dict, optional
2237             Additional keyword passed to scipy's interpolator.
2238 
2239         Returns
2240         -------
2241         interpolated : DataArray
2242             Another dataarray by interpolating this dataarray's data along the
2243             coordinates of the other object.
2244 
2245         Examples
2246         --------
2247         >>> data = np.arange(12).reshape(4, 3)
2248         >>> da1 = xr.DataArray(
2249         ...     data=data,
2250         ...     dims=["x", "y"],
2251         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2252         ... )
2253         >>> da1
2254         <xarray.DataArray (x: 4, y: 3)>
2255         array([[ 0,  1,  2],
2256                [ 3,  4,  5],
2257                [ 6,  7,  8],
2258                [ 9, 10, 11]])
2259         Coordinates:
2260           * x        (x) int64 10 20 30 40
2261           * y        (y) int64 70 80 90
2262         >>> da2 = xr.DataArray(
2263         ...     data=data,
2264         ...     dims=["x", "y"],
2265         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2266         ... )
2267         >>> da2
2268         <xarray.DataArray (x: 4, y: 3)>
2269         array([[ 0,  1,  2],
2270                [ 3,  4,  5],
2271                [ 6,  7,  8],
2272                [ 9, 10, 11]])
2273         Coordinates:
2274           * x        (x) int64 10 20 29 39
2275           * y        (y) int64 70 80 90
2276 
2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2278 
2279         >>> da2.interp_like(da1)
2280         <xarray.DataArray (x: 4, y: 3)>
2281         array([[0. , 1. , 2. ],
2282                [3. , 4. , 5. ],
2283                [6.3, 7.3, 8.3],
2284                [nan, nan, nan]])
2285         Coordinates:
2286           * x        (x) int64 10 20 30 40
2287           * y        (y) int64 70 80 90
2288 
2289         Could also extrapolate missing values:
2290 
2291         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2292         <xarray.DataArray (x: 4, y: 3)>
2293         array([[ 0. ,  1. ,  2. ],
2294                [ 3. ,  4. ,  5. ],
2295                [ 6.3,  7.3,  8.3],
2296                [ 9.3, 10.3, 11.3]])
2297         Coordinates:
2298           * x        (x) int64 10 20 30 40
2299           * y        (y) int64 70 80 90
2300 
2301         Notes
2302         -----
2303         scipy is required.
2304         If the dataarray has object-type coordinates, reindex is used for these
2305         coordinates instead of the interpolation.
2306 
2307         See Also
2308         --------
2309         DataArray.interp
2310         DataArray.reindex_like
2311         """
2312         if self.dtype.kind not in "uifc":
2313             raise TypeError(
2314                 "interp only works for a numeric type array. "
2315                 "Given {}.".format(self.dtype)
2316             )
2317         ds = self._to_temp_dataset().interp_like(
2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2319         )
2320         return self._from_temp_dataset(ds)
2321 
2322     # change type of self and return to T_DataArray once
2323     # https://github.com/python/mypy/issues/12846 is resolved
2324     def rename(
2325         self,
2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2327         **names: Hashable,
2328     ) -> DataArray:
2329         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2330 
2331         Parameters
2332         ----------
2333         new_name_or_name_dict : str or dict-like, optional
2334             If the argument is dict-like, it used as a mapping from old
2335             names to new names for coordinates or dimensions. Otherwise,
2336             use the argument as the new name for this array.
2337         **names : Hashable, optional
2338             The keyword arguments form of a mapping from old names to
2339             new names for coordinates or dimensions.
2340             One of new_name_or_name_dict or names must be provided.
2341 
2342         Returns
2343         -------
2344         renamed : DataArray
2345             Renamed array or array with renamed coordinates.
2346 
2347         See Also
2348         --------
2349         Dataset.rename
2350         DataArray.swap_dims
2351         """
2352         if new_name_or_name_dict is None and not names:
2353             # change name to None?
2354             return self._replace(name=None)
2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2356             # change dims/coords
2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2358             dataset = self._to_temp_dataset()._rename(name_dict)
2359             return self._from_temp_dataset(dataset)
2360         if utils.hashable(new_name_or_name_dict) and names:
2361             # change name + dims/coords
2362             dataset = self._to_temp_dataset()._rename(names)
2363             dataarray = self._from_temp_dataset(dataset)
2364             return dataarray._replace(name=new_name_or_name_dict)
2365         # only change name
2366         return self._replace(name=new_name_or_name_dict)
2367 
2368     def swap_dims(
2369         self: T_DataArray,
2370         dims_dict: Mapping[Any, Hashable] | None = None,
2371         **dims_kwargs,
2372     ) -> T_DataArray:
2373         """Returns a new DataArray with swapped dimensions.
2374 
2375         Parameters
2376         ----------
2377         dims_dict : dict-like
2378             Dictionary whose keys are current dimension names and whose values
2379             are new names.
2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2381             The keyword arguments form of ``dims_dict``.
2382             One of dims_dict or dims_kwargs must be provided.
2383 
2384         Returns
2385         -------
2386         swapped : DataArray
2387             DataArray with swapped dimensions.
2388 
2389         Examples
2390         --------
2391         >>> arr = xr.DataArray(
2392         ...     data=[0, 1],
2393         ...     dims="x",
2394         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2395         ... )
2396         >>> arr
2397         <xarray.DataArray (x: 2)>
2398         array([0, 1])
2399         Coordinates:
2400           * x        (x) <U1 'a' 'b'
2401             y        (x) int64 0 1
2402 
2403         >>> arr.swap_dims({"x": "y"})
2404         <xarray.DataArray (y: 2)>
2405         array([0, 1])
2406         Coordinates:
2407             x        (y) <U1 'a' 'b'
2408           * y        (y) int64 0 1
2409 
2410         >>> arr.swap_dims({"x": "z"})
2411         <xarray.DataArray (z: 2)>
2412         array([0, 1])
2413         Coordinates:
2414             x        (z) <U1 'a' 'b'
2415             y        (z) int64 0 1
2416         Dimensions without coordinates: z
2417 
2418         See Also
2419         --------
2420         DataArray.rename
2421         Dataset.swap_dims
2422         """
2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2424         ds = self._to_temp_dataset().swap_dims(dims_dict)
2425         return self._from_temp_dataset(ds)
2426 
2427     # change type of self and return to T_DataArray once
2428     # https://github.com/python/mypy/issues/12846 is resolved
2429     def expand_dims(
2430         self,
2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2432         axis: None | int | Sequence[int] = None,
2433         **dim_kwargs: Any,
2434     ) -> DataArray:
2435         """Return a new object with an additional axis (or axes) inserted at
2436         the corresponding position in the array shape. The new object is a
2437         view into the underlying array, not a copy.
2438 
2439         If dim is already a scalar coordinate, it will be promoted to a 1D
2440         coordinate consisting of a single value.
2441 
2442         Parameters
2443         ----------
2444         dim : Hashable, sequence of Hashable, dict, or None, optional
2445             Dimensions to include on the new variable.
2446             If provided as str or sequence of str, then dimensions are inserted
2447             with length 1. If provided as a dict, then the keys are the new
2448             dimensions and the values are either integers (giving the length of
2449             the new dimensions) or sequence/ndarray (giving the coordinates of
2450             the new dimensions).
2451         axis : int, sequence of int, or None, default: None
2452             Axis position(s) where new axis is to be inserted (position(s) on
2453             the result array). If a sequence of integers is passed,
2454             multiple axes are inserted. In this case, dim arguments should be
2455             same length list. If axis=None is passed, all the axes will be
2456             inserted to the start of the result array.
2457         **dim_kwargs : int or sequence or ndarray
2458             The keywords are arbitrary dimensions being inserted and the values
2459             are either the lengths of the new dims (if int is given), or their
2460             coordinates. Note, this is an alternative to passing a dict to the
2461             dim kwarg and will only be used if dim is None.
2462 
2463         Returns
2464         -------
2465         expanded : DataArray
2466             This object, but with additional dimension(s).
2467 
2468         See Also
2469         --------
2470         Dataset.expand_dims
2471 
2472         Examples
2473         --------
2474         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2475         >>> da
2476         <xarray.DataArray (x: 5)>
2477         array([0, 1, 2, 3, 4])
2478         Dimensions without coordinates: x
2479 
2480         Add new dimension of length 2:
2481 
2482         >>> da.expand_dims(dim={"y": 2})
2483         <xarray.DataArray (y: 2, x: 5)>
2484         array([[0, 1, 2, 3, 4],
2485                [0, 1, 2, 3, 4]])
2486         Dimensions without coordinates: y, x
2487 
2488         >>> da.expand_dims(dim={"y": 2}, axis=1)
2489         <xarray.DataArray (x: 5, y: 2)>
2490         array([[0, 0],
2491                [1, 1],
2492                [2, 2],
2493                [3, 3],
2494                [4, 4]])
2495         Dimensions without coordinates: x, y
2496 
2497         Add a new dimension with coordinates from array:
2498 
2499         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2500         <xarray.DataArray (y: 5, x: 5)>
2501         array([[0, 1, 2, 3, 4],
2502                [0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4]])
2506         Coordinates:
2507           * y        (y) int64 0 1 2 3 4
2508         Dimensions without coordinates: x
2509         """
2510         if isinstance(dim, int):
2511             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2513             if len(dim) != len(set(dim)):
2514                 raise ValueError("dims should not contain duplicate values.")
2515             dim = dict.fromkeys(dim, 1)
2516         elif dim is not None and not isinstance(dim, Mapping):
2517             dim = {cast(Hashable, dim): 1}
2518 
2519         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2520         ds = self._to_temp_dataset().expand_dims(dim, axis)
2521         return self._from_temp_dataset(ds)
2522 
2523     # change type of self and return to T_DataArray once
2524     # https://github.com/python/mypy/issues/12846 is resolved
2525     def set_index(
2526         self,
2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2528         append: bool = False,
2529         **indexes_kwargs: Hashable | Sequence[Hashable],
2530     ) -> DataArray:
2531         """Set DataArray (multi-)indexes using one or more existing
2532         coordinates.
2533 
2534         This legacy method is limited to pandas (multi-)indexes and
2535         1-dimensional "dimension" coordinates. See
2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2537         Xarray-compatible index from one or more arbitrary coordinates.
2538 
2539         Parameters
2540         ----------
2541         indexes : {dim: index, ...}
2542             Mapping from names matching dimensions and values given
2543             by (lists of) the names of existing coordinates or variables to set
2544             as new (multi-)index.
2545         append : bool, default: False
2546             If True, append the supplied index(es) to the existing index(es).
2547             Otherwise replace the existing index(es).
2548         **indexes_kwargs : optional
2549             The keyword arguments form of ``indexes``.
2550             One of indexes or indexes_kwargs must be provided.
2551 
2552         Returns
2553         -------
2554         obj : DataArray
2555             Another DataArray, with this data but replaced coordinates.
2556 
2557         Examples
2558         --------
2559         >>> arr = xr.DataArray(
2560         ...     data=np.ones((2, 3)),
2561         ...     dims=["x", "y"],
2562         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2563         ... )
2564         >>> arr
2565         <xarray.DataArray (x: 2, y: 3)>
2566         array([[1., 1., 1.],
2567                [1., 1., 1.]])
2568         Coordinates:
2569           * x        (x) int64 0 1
2570           * y        (y) int64 0 1 2
2571             a        (x) int64 3 4
2572         >>> arr.set_index(x="a")
2573         <xarray.DataArray (x: 2, y: 3)>
2574         array([[1., 1., 1.],
2575                [1., 1., 1.]])
2576         Coordinates:
2577           * x        (x) int64 3 4
2578           * y        (y) int64 0 1 2
2579 
2580         See Also
2581         --------
2582         DataArray.reset_index
2583         DataArray.set_xindex
2584         """
2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2586         return self._from_temp_dataset(ds)
2587 
2588     # change type of self and return to T_DataArray once
2589     # https://github.com/python/mypy/issues/12846 is resolved
2590     def reset_index(
2591         self,
2592         dims_or_levels: Hashable | Sequence[Hashable],
2593         drop: bool = False,
2594     ) -> DataArray:
2595         """Reset the specified index(es) or multi-index level(s).
2596 
2597         This legacy method is specific to pandas (multi-)indexes and
2598         1-dimensional "dimension" coordinates. See the more generic
2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2600         method to respectively drop and set pandas or custom indexes for
2601         arbitrary coordinates.
2602 
2603         Parameters
2604         ----------
2605         dims_or_levels : Hashable or sequence of Hashable
2606             Name(s) of the dimension(s) and/or multi-index level(s) that will
2607             be reset.
2608         drop : bool, default: False
2609             If True, remove the specified indexes and/or multi-index levels
2610             instead of extracting them as new coordinates (default: False).
2611 
2612         Returns
2613         -------
2614         obj : DataArray
2615             Another dataarray, with this dataarray's data but replaced
2616             coordinates.
2617 
2618         See Also
2619         --------
2620         DataArray.set_index
2621         DataArray.set_xindex
2622         DataArray.drop_indexes
2623         """
2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2625         return self._from_temp_dataset(ds)
2626 
2627     def set_xindex(
2628         self: T_DataArray,
2629         coord_names: str | Sequence[Hashable],
2630         index_cls: type[Index] | None = None,
2631         **options,
2632     ) -> T_DataArray:
2633         """Set a new, Xarray-compatible index from one or more existing
2634         coordinate(s).
2635 
2636         Parameters
2637         ----------
2638         coord_names : str or list
2639             Name(s) of the coordinate(s) used to build the index.
2640             If several names are given, their order matters.
2641         index_cls : subclass of :class:`~xarray.indexes.Index`
2642             The type of index to create. By default, try setting
2643             a pandas (multi-)index from the supplied coordinates.
2644         **options
2645             Options passed to the index constructor.
2646 
2647         Returns
2648         -------
2649         obj : DataArray
2650             Another dataarray, with this dataarray's data and with a new index.
2651 
2652         """
2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2654         return self._from_temp_dataset(ds)
2655 
2656     def reorder_levels(
2657         self: T_DataArray,
2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2659         **dim_order_kwargs: Sequence[int | Hashable],
2660     ) -> T_DataArray:
2661         """Rearrange index levels using input order.
2662 
2663         Parameters
2664         ----------
2665         dim_order dict-like of Hashable to int or Hashable: optional
2666             Mapping from names matching dimensions and values given
2667             by lists representing new level orders. Every given dimension
2668             must have a multi-index.
2669         **dim_order_kwargs : optional
2670             The keyword arguments form of ``dim_order``.
2671             One of dim_order or dim_order_kwargs must be provided.
2672 
2673         Returns
2674         -------
2675         obj : DataArray
2676             Another dataarray, with this dataarray's data but replaced
2677             coordinates.
2678         """
2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2680         return self._from_temp_dataset(ds)
2681 
2682     def stack(
2683         self: T_DataArray,
2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2685         create_index: bool | None = True,
2686         index_cls: type[Index] = PandasMultiIndex,
2687         **dimensions_kwargs: Sequence[Hashable],
2688     ) -> T_DataArray:
2689         """
2690         Stack any number of existing dimensions into a single new dimension.
2691 
2692         New dimensions will be added at the end, and the corresponding
2693         coordinate variables will be combined into a MultiIndex.
2694 
2695         Parameters
2696         ----------
2697         dimensions : mapping of Hashable to sequence of Hashable
2698             Mapping of the form `new_name=(dim1, dim2, ...)`.
2699             Names of new dimensions, and the existing dimensions that they
2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2702             all dimensions.
2703         create_index : bool or None, default: True
2704             If True, create a multi-index for each of the stacked dimensions.
2705             If False, don't create any index.
2706             If None, create a multi-index only if exactly one single (1-d) coordinate
2707             index is found for every dimension to stack.
2708         index_cls: class, optional
2709             Can be used to pass a custom multi-index type. Must be an Xarray index that
2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2711         **dimensions_kwargs
2712             The keyword arguments form of ``dimensions``.
2713             One of dimensions or dimensions_kwargs must be provided.
2714 
2715         Returns
2716         -------
2717         stacked : DataArray
2718             DataArray with stacked data.
2719 
2720         Examples
2721         --------
2722         >>> arr = xr.DataArray(
2723         ...     np.arange(6).reshape(2, 3),
2724         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2725         ... )
2726         >>> arr
2727         <xarray.DataArray (x: 2, y: 3)>
2728         array([[0, 1, 2],
2729                [3, 4, 5]])
2730         Coordinates:
2731           * x        (x) <U1 'a' 'b'
2732           * y        (y) int64 0 1 2
2733         >>> stacked = arr.stack(z=("x", "y"))
2734         >>> stacked.indexes["z"]
2735         MultiIndex([('a', 0),
2736                     ('a', 1),
2737                     ('a', 2),
2738                     ('b', 0),
2739                     ('b', 1),
2740                     ('b', 2)],
2741                    name='z')
2742 
2743         See Also
2744         --------
2745         DataArray.unstack
2746         """
2747         ds = self._to_temp_dataset().stack(
2748             dimensions,
2749             create_index=create_index,
2750             index_cls=index_cls,
2751             **dimensions_kwargs,
2752         )
2753         return self._from_temp_dataset(ds)
2754 
2755     # change type of self and return to T_DataArray once
2756     # https://github.com/python/mypy/issues/12846 is resolved
2757     def unstack(
2758         self,
2759         dim: Dims = None,
2760         fill_value: Any = dtypes.NA,
2761         sparse: bool = False,
2762     ) -> DataArray:
2763         """
2764         Unstack existing dimensions corresponding to MultiIndexes into
2765         multiple new dimensions.
2766 
2767         New dimensions will be added at the end.
2768 
2769         Parameters
2770         ----------
2771         dim : str, Iterable of Hashable or None, optional
2772             Dimension(s) over which to unstack. By default unstacks all
2773             MultiIndexes.
2774         fill_value : scalar or dict-like, default: nan
2775             Value to be filled. If a dict-like, maps variable names to
2776             fill values. Use the data array's name to refer to its
2777             name. If not provided or if the dict-like does not contain
2778             all variables, the dtype's NA value will be used.
2779         sparse : bool, default: False
2780             Use sparse-array if True
2781 
2782         Returns
2783         -------
2784         unstacked : DataArray
2785             Array with unstacked data.
2786 
2787         Examples
2788         --------
2789         >>> arr = xr.DataArray(
2790         ...     np.arange(6).reshape(2, 3),
2791         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2792         ... )
2793         >>> arr
2794         <xarray.DataArray (x: 2, y: 3)>
2795         array([[0, 1, 2],
2796                [3, 4, 5]])
2797         Coordinates:
2798           * x        (x) <U1 'a' 'b'
2799           * y        (y) int64 0 1 2
2800         >>> stacked = arr.stack(z=("x", "y"))
2801         >>> stacked.indexes["z"]
2802         MultiIndex([('a', 0),
2803                     ('a', 1),
2804                     ('a', 2),
2805                     ('b', 0),
2806                     ('b', 1),
2807                     ('b', 2)],
2808                    name='z')
2809         >>> roundtripped = stacked.unstack()
2810         >>> arr.identical(roundtripped)
2811         True
2812 
2813         See Also
2814         --------
2815         DataArray.stack
2816         """
2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2818         return self._from_temp_dataset(ds)
2819 
2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2821         """Unstack DataArray expanding to Dataset along a given level of a
2822         stacked coordinate.
2823 
2824         This is the inverse operation of Dataset.to_stacked_array.
2825 
2826         Parameters
2827         ----------
2828         dim : Hashable
2829             Name of existing dimension to unstack
2830         level : int or Hashable, default: 0
2831             The MultiIndex level to expand to a dataset along. Can either be
2832             the integer index of the level or its name.
2833 
2834         Returns
2835         -------
2836         unstacked: Dataset
2837 
2838         Examples
2839         --------
2840         >>> arr = xr.DataArray(
2841         ...     np.arange(6).reshape(2, 3),
2842         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2843         ... )
2844         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2845         >>> data
2846         <xarray.Dataset>
2847         Dimensions:  (x: 2, y: 3)
2848         Coordinates:
2849           * x        (x) <U1 'a' 'b'
2850           * y        (y) int64 0 1 2
2851         Data variables:
2852             a        (x, y) int64 0 1 2 3 4 5
2853             b        (x) int64 0 3
2854         >>> stacked = data.to_stacked_array("z", ["x"])
2855         >>> stacked.indexes["z"]
2856         MultiIndex([('a', 0.0),
2857                     ('a', 1.0),
2858                     ('a', 2.0),
2859                     ('b', nan)],
2860                    name='z')
2861         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2862         >>> data.identical(roundtripped)
2863         True
2864 
2865         See Also
2866         --------
2867         Dataset.to_stacked_array
2868         """
2869         idx = self._indexes[dim].to_pandas_index()
2870         if not isinstance(idx, pd.MultiIndex):
2871             raise ValueError(f"'{dim}' is not a stacked coordinate")
2872 
2873         level_number = idx._get_level_number(level)
2874         variables = idx.levels[level_number]
2875         variable_dim = idx.names[level_number]
2876 
2877         # pull variables out of datarray
2878         data_dict = {}
2879         for k in variables:
2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2881 
2882         # unstacked dataset
2883         return Dataset(data_dict)
2884 
2885     def transpose(
2886         self: T_DataArray,
2887         *dims: Hashable,
2888         transpose_coords: bool = True,
2889         missing_dims: ErrorOptionsWithWarn = "raise",
2890     ) -> T_DataArray:
2891         """Return a new DataArray object with transposed dimensions.
2892 
2893         Parameters
2894         ----------
2895         *dims : Hashable, optional
2896             By default, reverse the dimensions. Otherwise, reorder the
2897             dimensions to this order.
2898         transpose_coords : bool, default: True
2899             If True, also transpose the coordinates of this DataArray.
2900         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2901             What to do if dimensions that should be selected from are not present in the
2902             DataArray:
2903             - "raise": raise an exception
2904             - "warn": raise a warning, and ignore the missing dimensions
2905             - "ignore": ignore the missing dimensions
2906 
2907         Returns
2908         -------
2909         transposed : DataArray
2910             The returned DataArray's array is transposed.
2911 
2912         Notes
2913         -----
2914         This operation returns a view of this array's data. It is
2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2916         -- the data will be fully loaded.
2917 
2918         See Also
2919         --------
2920         numpy.transpose
2921         Dataset.transpose
2922         """
2923         if dims:
2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2925         variable = self.variable.transpose(*dims)
2926         if transpose_coords:
2927             coords: dict[Hashable, Variable] = {}
2928             for name, coord in self.coords.items():
2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2930                 coords[name] = coord.variable.transpose(*coord_dims)
2931             return self._replace(variable, coords)
2932         else:
2933             return self._replace(variable)
2934 
2935     @property
2936     def T(self: T_DataArray) -> T_DataArray:
2937         return self.transpose()
2938 
2939     # change type of self and return to T_DataArray once
2940     # https://github.com/python/mypy/issues/12846 is resolved
2941     def drop_vars(
2942         self,
2943         names: Hashable | Iterable[Hashable],
2944         *,
2945         errors: ErrorOptions = "raise",
2946     ) -> DataArray:
2947         """Returns an array with dropped variables.
2948 
2949         Parameters
2950         ----------
2951         names : Hashable or iterable of Hashable
2952             Name(s) of variables to drop.
2953         errors : {"raise", "ignore"}, default: "raise"
2954             If 'raise', raises a ValueError error if any of the variable
2955             passed are not in the dataset. If 'ignore', any given names that are in the
2956             DataArray are dropped and no error is raised.
2957 
2958         Returns
2959         -------
2960         dropped : Dataset
2961             New Dataset copied from `self` with variables removed.
2962 
2963         Examples
2964         -------
2965         >>> data = np.arange(12).reshape(4, 3)
2966         >>> da = xr.DataArray(
2967         ...     data=data,
2968         ...     dims=["x", "y"],
2969         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2970         ... )
2971         >>> da
2972         <xarray.DataArray (x: 4, y: 3)>
2973         array([[ 0,  1,  2],
2974                [ 3,  4,  5],
2975                [ 6,  7,  8],
2976                [ 9, 10, 11]])
2977         Coordinates:
2978           * x        (x) int64 10 20 30 40
2979           * y        (y) int64 70 80 90
2980 
2981         Removing a single variable:
2982 
2983         >>> da.drop_vars("x")
2984         <xarray.DataArray (x: 4, y: 3)>
2985         array([[ 0,  1,  2],
2986                [ 3,  4,  5],
2987                [ 6,  7,  8],
2988                [ 9, 10, 11]])
2989         Coordinates:
2990           * y        (y) int64 70 80 90
2991         Dimensions without coordinates: x
2992 
2993         Removing a list of variables:
2994 
2995         >>> da.drop_vars(["x", "y"])
2996         <xarray.DataArray (x: 4, y: 3)>
2997         array([[ 0,  1,  2],
2998                [ 3,  4,  5],
2999                [ 6,  7,  8],
3000                [ 9, 10, 11]])
3001         Dimensions without coordinates: x, y
3002         """
3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3004         return self._from_temp_dataset(ds)
3005 
3006     def drop_indexes(
3007         self: T_DataArray,
3008         coord_names: Hashable | Iterable[Hashable],
3009         *,
3010         errors: ErrorOptions = "raise",
3011     ) -> T_DataArray:
3012         """Drop the indexes assigned to the given coordinates.
3013 
3014         Parameters
3015         ----------
3016         coord_names : hashable or iterable of hashable
3017             Name(s) of the coordinate(s) for which to drop the index.
3018         errors : {"raise", "ignore"}, default: "raise"
3019             If 'raise', raises a ValueError error if any of the coordinates
3020             passed have no index or are not in the dataset.
3021             If 'ignore', no error is raised.
3022 
3023         Returns
3024         -------
3025         dropped : DataArray
3026             A new dataarray with dropped indexes.
3027         """
3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3029         return self._from_temp_dataset(ds)
3030 
3031     def drop(
3032         self: T_DataArray,
3033         labels: Mapping[Any, Any] | None = None,
3034         dim: Hashable | None = None,
3035         *,
3036         errors: ErrorOptions = "raise",
3037         **labels_kwargs,
3038     ) -> T_DataArray:
3039         """Backward compatible method based on `drop_vars` and `drop_sel`
3040 
3041         Using either `drop_vars` or `drop_sel` is encouraged
3042 
3043         See Also
3044         --------
3045         DataArray.drop_vars
3046         DataArray.drop_sel
3047         """
3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3049         return self._from_temp_dataset(ds)
3050 
3051     def drop_sel(
3052         self: T_DataArray,
3053         labels: Mapping[Any, Any] | None = None,
3054         *,
3055         errors: ErrorOptions = "raise",
3056         **labels_kwargs,
3057     ) -> T_DataArray:
3058         """Drop index labels from this DataArray.
3059 
3060         Parameters
3061         ----------
3062         labels : mapping of Hashable to Any
3063             Index labels to drop
3064         errors : {"raise", "ignore"}, default: "raise"
3065             If 'raise', raises a ValueError error if
3066             any of the index labels passed are not
3067             in the dataset. If 'ignore', any given labels that are in the
3068             dataset are dropped and no error is raised.
3069         **labels_kwargs : {dim: label, ...}, optional
3070             The keyword arguments form of ``dim`` and ``labels``
3071 
3072         Returns
3073         -------
3074         dropped : DataArray
3075 
3076         Examples
3077         --------
3078         >>> da = xr.DataArray(
3079         ...     np.arange(25).reshape(5, 5),
3080         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3081         ...     dims=("x", "y"),
3082         ... )
3083         >>> da
3084         <xarray.DataArray (x: 5, y: 5)>
3085         array([[ 0,  1,  2,  3,  4],
3086                [ 5,  6,  7,  8,  9],
3087                [10, 11, 12, 13, 14],
3088                [15, 16, 17, 18, 19],
3089                [20, 21, 22, 23, 24]])
3090         Coordinates:
3091           * x        (x) int64 0 2 4 6 8
3092           * y        (y) int64 0 3 6 9 12
3093 
3094         >>> da.drop_sel(x=[0, 2], y=9)
3095         <xarray.DataArray (x: 3, y: 4)>
3096         array([[10, 11, 12, 14],
3097                [15, 16, 17, 19],
3098                [20, 21, 22, 24]])
3099         Coordinates:
3100           * x        (x) int64 4 6 8
3101           * y        (y) int64 0 3 6 12
3102 
3103         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3104         <xarray.DataArray (x: 4, y: 3)>
3105         array([[ 2,  3,  4],
3106                [ 7,  8,  9],
3107                [12, 13, 14],
3108                [22, 23, 24]])
3109         Coordinates:
3110           * x        (x) int64 0 2 4 8
3111           * y        (y) int64 6 9 12
3112         """
3113         if labels_kwargs or isinstance(labels, dict):
3114             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3115 
3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3117         return self._from_temp_dataset(ds)
3118 
3119     def drop_isel(
3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3121     ) -> T_DataArray:
3122         """Drop index positions from this DataArray.
3123 
3124         Parameters
3125         ----------
3126         indexers : mapping of Hashable to Any or None, default: None
3127             Index locations to drop
3128         **indexers_kwargs : {dim: position, ...}, optional
3129             The keyword arguments form of ``dim`` and ``positions``
3130 
3131         Returns
3132         -------
3133         dropped : DataArray
3134 
3135         Raises
3136         ------
3137         IndexError
3138 
3139         Examples
3140         --------
3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3142         >>> da
3143         <xarray.DataArray (X: 5, Y: 5)>
3144         array([[ 0,  1,  2,  3,  4],
3145                [ 5,  6,  7,  8,  9],
3146                [10, 11, 12, 13, 14],
3147                [15, 16, 17, 18, 19],
3148                [20, 21, 22, 23, 24]])
3149         Dimensions without coordinates: X, Y
3150 
3151         >>> da.drop_isel(X=[0, 4], Y=2)
3152         <xarray.DataArray (X: 3, Y: 4)>
3153         array([[ 5,  6,  8,  9],
3154                [10, 11, 13, 14],
3155                [15, 16, 18, 19]])
3156         Dimensions without coordinates: X, Y
3157 
3158         >>> da.drop_isel({"X": 3, "Y": 3})
3159         <xarray.DataArray (X: 4, Y: 4)>
3160         array([[ 0,  1,  2,  4],
3161                [ 5,  6,  7,  9],
3162                [10, 11, 12, 14],
3163                [20, 21, 22, 24]])
3164         Dimensions without coordinates: X, Y
3165         """
3166         dataset = self._to_temp_dataset()
3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3168         return self._from_temp_dataset(dataset)
3169 
3170     def dropna(
3171         self: T_DataArray,
3172         dim: Hashable,
3173         how: Literal["any", "all"] = "any",
3174         thresh: int | None = None,
3175     ) -> T_DataArray:
3176         """Returns a new array with dropped labels for missing values along
3177         the provided dimension.
3178 
3179         Parameters
3180         ----------
3181         dim : Hashable
3182             Dimension along which to drop missing values. Dropping along
3183             multiple dimensions simultaneously is not yet supported.
3184         how : {"any", "all"}, default: "any"
3185             - any : if any NA values are present, drop that label
3186             - all : if all values are NA, drop that label
3187 
3188         thresh : int or None, default: None
3189             If supplied, require this many non-NA values.
3190 
3191         Returns
3192         -------
3193         dropped : DataArray
3194 
3195         Examples
3196         --------
3197         >>> temperature = [
3198         ...     [0, 4, 2, 9],
3199         ...     [np.nan, np.nan, np.nan, np.nan],
3200         ...     [np.nan, 4, 2, 0],
3201         ...     [3, 1, 0, 0],
3202         ... ]
3203         >>> da = xr.DataArray(
3204         ...     data=temperature,
3205         ...     dims=["Y", "X"],
3206         ...     coords=dict(
3207         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3208         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3209         ...     ),
3210         ... )
3211         >>> da
3212         <xarray.DataArray (Y: 4, X: 4)>
3213         array([[ 0.,  4.,  2.,  9.],
3214                [nan, nan, nan, nan],
3215                [nan,  4.,  2.,  0.],
3216                [ 3.,  1.,  0.,  0.]])
3217         Coordinates:
3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3219             lon      (X) float64 10.0 10.25 10.5 10.75
3220         Dimensions without coordinates: Y, X
3221 
3222         >>> da.dropna(dim="Y", how="any")
3223         <xarray.DataArray (Y: 2, X: 4)>
3224         array([[0., 4., 2., 9.],
3225                [3., 1., 0., 0.]])
3226         Coordinates:
3227             lat      (Y) float64 -20.0 -20.75
3228             lon      (X) float64 10.0 10.25 10.5 10.75
3229         Dimensions without coordinates: Y, X
3230 
3231         Drop values only if all values along the dimension are NaN:
3232 
3233         >>> da.dropna(dim="Y", how="all")
3234         <xarray.DataArray (Y: 3, X: 4)>
3235         array([[ 0.,  4.,  2.,  9.],
3236                [nan,  4.,  2.,  0.],
3237                [ 3.,  1.,  0.,  0.]])
3238         Coordinates:
3239             lat      (Y) float64 -20.0 -20.5 -20.75
3240             lon      (X) float64 10.0 10.25 10.5 10.75
3241         Dimensions without coordinates: Y, X
3242         """
3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3244         return self._from_temp_dataset(ds)
3245 
3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3247         """Fill missing values in this object.
3248 
3249         This operation follows the normal broadcasting and alignment rules that
3250         xarray uses for binary arithmetic, except the result is aligned to this
3251         object (``join='left'``) instead of aligned to the intersection of
3252         index coordinates (``join='inner'``).
3253 
3254         Parameters
3255         ----------
3256         value : scalar, ndarray or DataArray
3257             Used to fill all matching missing values in this array. If the
3258             argument is a DataArray, it is first aligned with (reindexed to)
3259             this array.
3260 
3261         Returns
3262         -------
3263         filled : DataArray
3264 
3265         Examples
3266         --------
3267         >>> da = xr.DataArray(
3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3269         ...     dims="Z",
3270         ...     coords=dict(
3271         ...         Z=("Z", np.arange(6)),
3272         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3273         ...     ),
3274         ... )
3275         >>> da
3276         <xarray.DataArray (Z: 6)>
3277         array([ 1.,  4., nan,  0.,  3., nan])
3278         Coordinates:
3279           * Z        (Z) int64 0 1 2 3 4 5
3280             height   (Z) int64 0 10 20 30 40 50
3281 
3282         Fill all NaN values with 0:
3283 
3284         >>> da.fillna(0)
3285         <xarray.DataArray (Z: 6)>
3286         array([1., 4., 0., 0., 3., 0.])
3287         Coordinates:
3288           * Z        (Z) int64 0 1 2 3 4 5
3289             height   (Z) int64 0 10 20 30 40 50
3290 
3291         Fill NaN values with corresponding values in array:
3292 
3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3294         <xarray.DataArray (Z: 6)>
3295         array([1., 4., 4., 0., 3., 9.])
3296         Coordinates:
3297           * Z        (Z) int64 0 1 2 3 4 5
3298             height   (Z) int64 0 10 20 30 40 50
3299         """
3300         if utils.is_dict_like(value):
3301             raise TypeError(
3302                 "cannot provide fill value as a dictionary with "
3303                 "fillna on a DataArray"
3304             )
3305         out = ops.fillna(self, value)
3306         return out
3307 
3308     def interpolate_na(
3309         self: T_DataArray,
3310         dim: Hashable | None = None,
3311         method: InterpOptions = "linear",
3312         limit: int | None = None,
3313         use_coordinate: bool | str = True,
3314         max_gap: (
3315             None
3316             | int
3317             | float
3318             | str
3319             | pd.Timedelta
3320             | np.timedelta64
3321             | datetime.timedelta
3322         ) = None,
3323         keep_attrs: bool | None = None,
3324         **kwargs: Any,
3325     ) -> T_DataArray:
3326         """Fill in NaNs by interpolating according to different methods.
3327 
3328         Parameters
3329         ----------
3330         dim : Hashable or None, optional
3331             Specifies the dimension along which to interpolate.
3332         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3333             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3334             String indicating which method to use for interpolation:
3335 
3336             - 'linear': linear interpolation. Additional keyword
3337               arguments are passed to :py:func:`numpy.interp`
3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3339               are passed to :py:func:`scipy.interpolate.interp1d`. If
3340               ``method='polynomial'``, the ``order`` keyword argument must also be
3341               provided.
3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3343               respective :py:class:`scipy.interpolate` classes.
3344 
3345         use_coordinate : bool or str, default: True
3346             Specifies which index to use as the x values in the interpolation
3347             formulated as `y = f(x)`. If False, values are treated as if
3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
3349             used. If ``use_coordinate`` is a string, it specifies the name of a
3350             coordinate variable to use as the index.
3351         limit : int or None, default: None
3352             Maximum number of consecutive NaNs to fill. Must be greater than 0
3353             or None for no limit. This filling is done regardless of the size of
3354             the gap in the data. To only interpolate over gaps less than a given length,
3355             see ``max_gap``.
3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3358             Use None for no limit. When interpolating along a datetime64 dimension
3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3360 
3361             - a string that is valid input for pandas.to_timedelta
3362             - a :py:class:`numpy.timedelta64` object
3363             - a :py:class:`pandas.Timedelta` object
3364             - a :py:class:`datetime.timedelta` object
3365 
3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3367             dimensions has not been implemented yet. Gap length is defined as the difference
3368             between coordinate values at the first data point after a gap and the last value
3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3370             between coordinate values at the first (last) valid data point and the first (last) NaN.
3371             For example, consider::
3372 
3373                 <xarray.DataArray (x: 9)>
3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3375                 Coordinates:
3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3377 
3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3379         keep_attrs : bool or None, default: None
3380             If True, the dataarray's attributes (`attrs`) will be copied from
3381             the original object to the new one.  If False, the new
3382             object will be returned without attributes.
3383         **kwargs : dict, optional
3384             parameters passed verbatim to the underlying interpolation function
3385 
3386         Returns
3387         -------
3388         interpolated: DataArray
3389             Filled in DataArray.
3390 
3391         See Also
3392         --------
3393         numpy.interp
3394         scipy.interpolate
3395 
3396         Examples
3397         --------
3398         >>> da = xr.DataArray(
3399         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3400         ... )
3401         >>> da
3402         <xarray.DataArray (x: 5)>
3403         array([nan,  2.,  3., nan,  0.])
3404         Coordinates:
3405           * x        (x) int64 0 1 2 3 4
3406 
3407         >>> da.interpolate_na(dim="x", method="linear")
3408         <xarray.DataArray (x: 5)>
3409         array([nan, 2. , 3. , 1.5, 0. ])
3410         Coordinates:
3411           * x        (x) int64 0 1 2 3 4
3412 
3413         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3414         <xarray.DataArray (x: 5)>
3415         array([1. , 2. , 3. , 1.5, 0. ])
3416         Coordinates:
3417           * x        (x) int64 0 1 2 3 4
3418         """
3419         from xarray.core.missing import interp_na
3420 
3421         return interp_na(
3422             self,
3423             dim=dim,
3424             method=method,
3425             limit=limit,
3426             use_coordinate=use_coordinate,
3427             max_gap=max_gap,
3428             keep_attrs=keep_attrs,
3429             **kwargs,
3430         )
3431 
3432     def ffill(
3433         self: T_DataArray, dim: Hashable, limit: int | None = None
3434     ) -> T_DataArray:
3435         """Fill NaN values by propagating values forward
3436 
3437         *Requires bottleneck.*
3438 
3439         Parameters
3440         ----------
3441         dim : Hashable
3442             Specifies the dimension along which to propagate values when
3443             filling.
3444         limit : int or None, default: None
3445             The maximum number of consecutive NaN values to forward fill. In
3446             other words, if there is a gap with more than this number of
3447             consecutive NaNs, it will only be partially filled. Must be greater
3448             than 0 or None for no limit. Must be None or greater than or equal
3449             to axis length if filling along chunked axes (dimensions).
3450 
3451         Returns
3452         -------
3453         filled : DataArray
3454 
3455         Examples
3456         --------
3457         >>> temperature = np.array(
3458         ...     [
3459         ...         [np.nan, 1, 3],
3460         ...         [0, np.nan, 5],
3461         ...         [5, np.nan, np.nan],
3462         ...         [3, np.nan, np.nan],
3463         ...         [0, 2, 0],
3464         ...     ]
3465         ... )
3466         >>> da = xr.DataArray(
3467         ...     data=temperature,
3468         ...     dims=["Y", "X"],
3469         ...     coords=dict(
3470         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3471         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3472         ...     ),
3473         ... )
3474         >>> da
3475         <xarray.DataArray (Y: 5, X: 3)>
3476         array([[nan,  1.,  3.],
3477                [ 0., nan,  5.],
3478                [ 5., nan, nan],
3479                [ 3., nan, nan],
3480                [ 0.,  2.,  0.]])
3481         Coordinates:
3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3483             lon      (X) float64 10.0 10.25 10.5
3484         Dimensions without coordinates: Y, X
3485 
3486         Fill all NaN values:
3487 
3488         >>> da.ffill(dim="Y", limit=None)
3489         <xarray.DataArray (Y: 5, X: 3)>
3490         array([[nan,  1.,  3.],
3491                [ 0.,  1.,  5.],
3492                [ 5.,  1.,  5.],
3493                [ 3.,  1.,  5.],
3494                [ 0.,  2.,  0.]])
3495         Coordinates:
3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3497             lon      (X) float64 10.0 10.25 10.5
3498         Dimensions without coordinates: Y, X
3499 
3500         Fill only the first of consecutive NaN values:
3501 
3502         >>> da.ffill(dim="Y", limit=1)
3503         <xarray.DataArray (Y: 5, X: 3)>
3504         array([[nan,  1.,  3.],
3505                [ 0.,  1.,  5.],
3506                [ 5., nan,  5.],
3507                [ 3., nan, nan],
3508                [ 0.,  2.,  0.]])
3509         Coordinates:
3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3511             lon      (X) float64 10.0 10.25 10.5
3512         Dimensions without coordinates: Y, X
3513         """
3514         from xarray.core.missing import ffill
3515 
3516         return ffill(self, dim, limit=limit)
3517 
3518     def bfill(
3519         self: T_DataArray, dim: Hashable, limit: int | None = None
3520     ) -> T_DataArray:
3521         """Fill NaN values by propagating values backward
3522 
3523         *Requires bottleneck.*
3524 
3525         Parameters
3526         ----------
3527         dim : str
3528             Specifies the dimension along which to propagate values when
3529             filling.
3530         limit : int or None, default: None
3531             The maximum number of consecutive NaN values to backward fill. In
3532             other words, if there is a gap with more than this number of
3533             consecutive NaNs, it will only be partially filled. Must be greater
3534             than 0 or None for no limit. Must be None or greater than or equal
3535             to axis length if filling along chunked axes (dimensions).
3536 
3537         Returns
3538         -------
3539         filled : DataArray
3540 
3541         Examples
3542         --------
3543         >>> temperature = np.array(
3544         ...     [
3545         ...         [0, 1, 3],
3546         ...         [0, np.nan, 5],
3547         ...         [5, np.nan, np.nan],
3548         ...         [3, np.nan, np.nan],
3549         ...         [np.nan, 2, 0],
3550         ...     ]
3551         ... )
3552         >>> da = xr.DataArray(
3553         ...     data=temperature,
3554         ...     dims=["Y", "X"],
3555         ...     coords=dict(
3556         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3557         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3558         ...     ),
3559         ... )
3560         >>> da
3561         <xarray.DataArray (Y: 5, X: 3)>
3562         array([[ 0.,  1.,  3.],
3563                [ 0., nan,  5.],
3564                [ 5., nan, nan],
3565                [ 3., nan, nan],
3566                [nan,  2.,  0.]])
3567         Coordinates:
3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3569             lon      (X) float64 10.0 10.25 10.5
3570         Dimensions without coordinates: Y, X
3571 
3572         Fill all NaN values:
3573 
3574         >>> da.bfill(dim="Y", limit=None)
3575         <xarray.DataArray (Y: 5, X: 3)>
3576         array([[ 0.,  1.,  3.],
3577                [ 0.,  2.,  5.],
3578                [ 5.,  2.,  0.],
3579                [ 3.,  2.,  0.],
3580                [nan,  2.,  0.]])
3581         Coordinates:
3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3583             lon      (X) float64 10.0 10.25 10.5
3584         Dimensions without coordinates: Y, X
3585 
3586         Fill only the first of consecutive NaN values:
3587 
3588         >>> da.bfill(dim="Y", limit=1)
3589         <xarray.DataArray (Y: 5, X: 3)>
3590         array([[ 0.,  1.,  3.],
3591                [ 0., nan,  5.],
3592                [ 5., nan, nan],
3593                [ 3.,  2.,  0.],
3594                [nan,  2.,  0.]])
3595         Coordinates:
3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3597             lon      (X) float64 10.0 10.25 10.5
3598         Dimensions without coordinates: Y, X
3599         """
3600         from xarray.core.missing import bfill
3601 
3602         return bfill(self, dim, limit=limit)
3603 
3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3605         """Combine two DataArray objects, with union of coordinates.
3606 
3607         This operation follows the normal broadcasting and alignment rules of
3608         ``join='outer'``.  Default to non-null values of array calling the
3609         method.  Use np.nan to fill in vacant cells after alignment.
3610 
3611         Parameters
3612         ----------
3613         other : DataArray
3614             Used to fill all matching missing values in this array.
3615 
3616         Returns
3617         -------
3618         DataArray
3619         """
3620         return ops.fillna(self, other, join="outer")
3621 
3622     def reduce(
3623         self: T_DataArray,
3624         func: Callable[..., Any],
3625         dim: Dims = None,
3626         *,
3627         axis: int | Sequence[int] | None = None,
3628         keep_attrs: bool | None = None,
3629         keepdims: bool = False,
3630         **kwargs: Any,
3631     ) -> T_DataArray:
3632         """Reduce this array by applying `func` along some dimension(s).
3633 
3634         Parameters
3635         ----------
3636         func : callable
3637             Function which can be called in the form
3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3639             np.ndarray over an integer valued axis.
3640         dim : "...", str, Iterable of Hashable or None, optional
3641             Dimension(s) over which to apply `func`. By default `func` is
3642             applied over all dimensions.
3643         axis : int or sequence of int, optional
3644             Axis(es) over which to repeatedly apply `func`. Only one of the
3645             'dim' and 'axis' arguments can be supplied. If neither are
3646             supplied, then the reduction is calculated over the flattened array
3647             (by calling `f(x)` without an axis argument).
3648         keep_attrs : bool or None, optional
3649             If True, the variable's attributes (`attrs`) will be copied from
3650             the original object to the new one.  If False (default), the new
3651             object will be returned without attributes.
3652         keepdims : bool, default: False
3653             If True, the dimensions which are reduced are left in the result
3654             as dimensions of size one. Coordinates that use these dimensions
3655             are removed.
3656         **kwargs : dict
3657             Additional keyword arguments passed on to `func`.
3658 
3659         Returns
3660         -------
3661         reduced : DataArray
3662             DataArray with this object's array replaced with an array with
3663             summarized data and the indicated dimension(s) removed.
3664         """
3665 
3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3667         return self._replace_maybe_drop_dims(var)
3668 
3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3670         """Convert this array into a pandas object with the same shape.
3671 
3672         The type of the returned object depends on the number of DataArray
3673         dimensions:
3674 
3675         * 0D -> `xarray.DataArray`
3676         * 1D -> `pandas.Series`
3677         * 2D -> `pandas.DataFrame`
3678 
3679         Only works for arrays with 2 or fewer dimensions.
3680 
3681         The DataArray constructor performs the inverse transformation.
3682 
3683         Returns
3684         -------
3685         result : DataArray | Series | DataFrame
3686             DataArray, pandas Series or pandas DataFrame.
3687         """
3688         # TODO: consolidate the info about pandas constructors and the
3689         # attributes that correspond to their indexes into a separate module?
3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3691         try:
3692             constructor = constructors[self.ndim]
3693         except KeyError:
3694             raise ValueError(
3695                 f"Cannot convert arrays with {self.ndim} dimensions into "
3696                 "pandas objects. Requires 2 or fewer dimensions."
3697             )
3698         indexes = [self.get_index(dim) for dim in self.dims]
3699         return constructor(self.values, *indexes)
3700 
3701     def to_dataframe(
3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3703     ) -> pd.DataFrame:
3704         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3705 
3706         The DataFrame is indexed by the Cartesian product of index coordinates
3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3708         included as columns in the DataFrame.
3709 
3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3711         doesn't rely on a MultiIndex to build the DataFrame.
3712 
3713         Parameters
3714         ----------
3715         name: Hashable or None, optional
3716             Name to give to this array (required if unnamed).
3717         dim_order: Sequence of Hashable or None, optional
3718             Hierarchical dimension order for the resulting dataframe.
3719             Array content is transposed to this order and then written out as flat
3720             vectors in contiguous order, so the last dimension in this list
3721             will be contiguous in the resulting DataFrame. This has a major
3722             influence on which operations are efficient on the resulting
3723             dataframe.
3724 
3725             If provided, must include all dimensions of this DataArray. By default,
3726             dimensions are sorted according to the DataArray dimensions order.
3727 
3728         Returns
3729         -------
3730         result: DataFrame
3731             DataArray as a pandas DataFrame.
3732 
3733         See also
3734         --------
3735         DataArray.to_pandas
3736         DataArray.to_series
3737         """
3738         if name is None:
3739             name = self.name
3740         if name is None:
3741             raise ValueError(
3742                 "cannot convert an unnamed DataArray to a "
3743                 "DataFrame: use the ``name`` parameter"
3744             )
3745         if self.ndim == 0:
3746             raise ValueError("cannot convert a scalar to a DataFrame")
3747 
3748         # By using a unique name, we can convert a DataArray into a DataFrame
3749         # even if it shares a name with one of its coordinates.
3750         # I would normally use unique_name = object() but that results in a
3751         # dataframe with columns in the wrong order, for reasons I have not
3752         # been able to debug (possibly a pandas bug?).
3753         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3754         ds = self._to_dataset_whole(name=unique_name)
3755 
3756         if dim_order is None:
3757             ordered_dims = dict(zip(self.dims, self.shape))
3758         else:
3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3760 
3761         df = ds._to_dataframe(ordered_dims)
3762         df.columns = [name if c == unique_name else c for c in df.columns]
3763         return df
3764 
3765     def to_series(self) -> pd.Series:
3766         """Convert this array into a pandas.Series.
3767 
3768         The Series is indexed by the Cartesian product of index coordinates
3769         (in the form of a :py:class:`pandas.MultiIndex`).
3770 
3771         Returns
3772         -------
3773         result : Series
3774             DataArray as a pandas Series.
3775 
3776         See also
3777         --------
3778         DataArray.to_pandas
3779         DataArray.to_dataframe
3780         """
3781         index = self.coords.to_index()
3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3783 
3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3785         """Convert this array into a numpy.ma.MaskedArray
3786 
3787         Parameters
3788         ----------
3789         copy : bool, default: True
3790             If True make a copy of the array in the result. If False,
3791             a MaskedArray view of DataArray.values is returned.
3792 
3793         Returns
3794         -------
3795         result : MaskedArray
3796             Masked where invalid values (nan or inf) occur.
3797         """
3798         values = self.to_numpy()  # only compute lazy arrays once
3799         isnull = pd.isnull(values)
3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3801 
3802     # path=None writes to bytes
3803     @overload
3804     def to_netcdf(
3805         self,
3806         path: None = None,
3807         mode: Literal["w", "a"] = "w",
3808         format: T_NetcdfTypes | None = None,
3809         group: str | None = None,
3810         engine: T_NetcdfEngine | None = None,
3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3812         unlimited_dims: Iterable[Hashable] | None = None,
3813         compute: bool = True,
3814         invalid_netcdf: bool = False,
3815     ) -> bytes:
3816         ...
3817 
3818     # default return None
3819     @overload
3820     def to_netcdf(
3821         self,
3822         path: str | PathLike,
3823         mode: Literal["w", "a"] = "w",
3824         format: T_NetcdfTypes | None = None,
3825         group: str | None = None,
3826         engine: T_NetcdfEngine | None = None,
3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3828         unlimited_dims: Iterable[Hashable] | None = None,
3829         compute: Literal[True] = True,
3830         invalid_netcdf: bool = False,
3831     ) -> None:
3832         ...
3833 
3834     # compute=False returns dask.Delayed
3835     @overload
3836     def to_netcdf(
3837         self,
3838         path: str | PathLike,
3839         mode: Literal["w", "a"] = "w",
3840         format: T_NetcdfTypes | None = None,
3841         group: str | None = None,
3842         engine: T_NetcdfEngine | None = None,
3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3844         unlimited_dims: Iterable[Hashable] | None = None,
3845         *,
3846         compute: Literal[False],
3847         invalid_netcdf: bool = False,
3848     ) -> Delayed:
3849         ...
3850 
3851     def to_netcdf(
3852         self,
3853         path: str | PathLike | None = None,
3854         mode: Literal["w", "a"] = "w",
3855         format: T_NetcdfTypes | None = None,
3856         group: str | None = None,
3857         engine: T_NetcdfEngine | None = None,
3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3859         unlimited_dims: Iterable[Hashable] | None = None,
3860         compute: bool = True,
3861         invalid_netcdf: bool = False,
3862     ) -> bytes | Delayed | None:
3863         """Write DataArray contents to a netCDF file.
3864 
3865         Parameters
3866         ----------
3867         path : str, path-like or None, optional
3868             Path to which to save this dataset. File-like objects are only
3869             supported by the scipy engine. If no path is provided, this
3870             function returns the resulting netCDF file as bytes; in this case,
3871             we need to use scipy, which does not support netCDF version 4 (the
3872             default format becomes NETCDF3_64BIT).
3873         mode : {"w", "a"}, default: "w"
3874             Write ('w') or append ('a') mode. If mode='w', any existing file at
3875             this location will be overwritten. If mode='a', existing variables
3876             will be overwritten.
3877         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3878                   "NETCDF3_CLASSIC"}, optional
3879             File format for the resulting netCDF file:
3880 
3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3882               features.
3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3884               netCDF 3 compatible API features.
3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3886               which fully supports 2+ GB files, but is only compatible with
3887               clients linked against netCDF version 3.6.0 or later.
3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3889               handle 2+ GB files very well.
3890 
3891             All formats are supported by the netCDF4-python library.
3892             scipy.io.netcdf only supports the last two formats.
3893 
3894             The default format is NETCDF4 if you are saving a file to disk and
3895             have the netCDF4-python library available. Otherwise, xarray falls
3896             back to using scipy to write netCDF files and defaults to the
3897             NETCDF3_64BIT format (scipy does not support netCDF4).
3898         group : str, optional
3899             Path to the netCDF4 group in the given file to open (only works for
3900             format='NETCDF4'). The group(s) will be created if necessary.
3901         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3902             Engine to use when writing netCDF files. If not provided, the
3903             default engine is chosen based on available dependencies, with a
3904             preference for 'netcdf4' if writing to a file on disk.
3905         encoding : dict, optional
3906             Nested dictionary with variable names as keys and dictionaries of
3907             variable specific encodings as values, e.g.,
3908             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3909             "zlib": True}, ...}``
3910 
3911             The `h5netcdf` engine supports both the NetCDF4-style compression
3912             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3913             ones ``{"compression": "gzip", "compression_opts": 9}``.
3914             This allows using any compression plugin installed in the HDF5
3915             library, e.g. LZF.
3916 
3917         unlimited_dims : iterable of Hashable, optional
3918             Dimension(s) that should be serialized as unlimited dimensions.
3919             By default, no dimensions are treated as unlimited dimensions.
3920             Note that unlimited_dims may also be set via
3921             ``dataset.encoding["unlimited_dims"]``.
3922         compute: bool, default: True
3923             If true compute immediately, otherwise return a
3924             ``dask.delayed.Delayed`` object that can be computed later.
3925         invalid_netcdf: bool, default: False
3926             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3927             hdf5 files which are invalid netcdf as described in
3928             https://github.com/h5netcdf/h5netcdf.
3929 
3930         Returns
3931         -------
3932         store: bytes or Delayed or None
3933             * ``bytes`` if path is None
3934             * ``dask.delayed.Delayed`` if compute is False
3935             * None otherwise
3936 
3937         Notes
3938         -----
3939         Only xarray.Dataset objects can be written to netCDF files, so
3940         the xarray.DataArray is converted to a xarray.Dataset object
3941         containing a single variable. If the DataArray has no name, or if the
3942         name is the same as a coordinate name, then it is given the name
3943         ``"__xarray_dataarray_variable__"``.
3944 
3945         See Also
3946         --------
3947         Dataset.to_netcdf
3948         """
3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3950 
3951         if self.name is None:
3952             # If no name is set then use a generic xarray name
3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3954         elif self.name in self.coords or self.name in self.dims:
3955             # The name is the same as one of the coords names, which netCDF
3956             # doesn't support, so rename it but keep track of the old name
3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3958             dataset.attrs[DATAARRAY_NAME] = self.name
3959         else:
3960             # No problems with the name - so we're fine!
3961             dataset = self.to_dataset()
3962 
3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3964             dataset,
3965             path,
3966             mode=mode,
3967             format=format,
3968             group=group,
3969             engine=engine,
3970             encoding=encoding,
3971             unlimited_dims=unlimited_dims,
3972             compute=compute,
3973             multifile=False,
3974             invalid_netcdf=invalid_netcdf,
3975         )
3976 
3977     # compute=True (default) returns ZarrStore
3978     @overload
3979     def to_zarr(
3980         self,
3981         store: MutableMapping | str | PathLike[str] | None = None,
3982         chunk_store: MutableMapping | str | PathLike | None = None,
3983         mode: Literal["w", "w-", "a", "r+", None] = None,
3984         synchronizer=None,
3985         group: str | None = None,
3986         encoding: Mapping | None = None,
3987         compute: Literal[True] = True,
3988         consolidated: bool | None = None,
3989         append_dim: Hashable | None = None,
3990         region: Mapping[str, slice] | None = None,
3991         safe_chunks: bool = True,
3992         storage_options: dict[str, str] | None = None,
3993         zarr_version: int | None = None,
3994     ) -> ZarrStore:
3995         ...
3996 
3997     # compute=False returns dask.Delayed
3998     @overload
3999     def to_zarr(
4000         self,
4001         store: MutableMapping | str | PathLike[str] | None = None,
4002         chunk_store: MutableMapping | str | PathLike | None = None,
4003         mode: Literal["w", "w-", "a", "r+", None] = None,
4004         synchronizer=None,
4005         group: str | None = None,
4006         encoding: Mapping | None = None,
4007         *,
4008         compute: Literal[False],
4009         consolidated: bool | None = None,
4010         append_dim: Hashable | None = None,
4011         region: Mapping[str, slice] | None = None,
4012         safe_chunks: bool = True,
4013         storage_options: dict[str, str] | None = None,
4014         zarr_version: int | None = None,
4015     ) -> Delayed:
4016         ...
4017 
4018     def to_zarr(
4019         self,
4020         store: MutableMapping | str | PathLike[str] | None = None,
4021         chunk_store: MutableMapping | str | PathLike | None = None,
4022         mode: Literal["w", "w-", "a", "r+", None] = None,
4023         synchronizer=None,
4024         group: str | None = None,
4025         encoding: Mapping | None = None,
4026         compute: bool = True,
4027         consolidated: bool | None = None,
4028         append_dim: Hashable | None = None,
4029         region: Mapping[str, slice] | None = None,
4030         safe_chunks: bool = True,
4031         storage_options: dict[str, str] | None = None,
4032         zarr_version: int | None = None,
4033     ) -> ZarrStore | Delayed:
4034         """Write DataArray contents to a Zarr store
4035 
4036         Zarr chunks are determined in the following way:
4037 
4038         - From the ``chunks`` attribute in each variable's ``encoding``
4039           (can be set via `DataArray.chunk`).
4040         - If the variable is a Dask array, from the dask chunks
4041         - If neither Dask chunks nor encoding chunks are present, chunks will
4042           be determined automatically by Zarr
4043         - If both Dask chunks and encoding chunks are present, encoding chunks
4044           will be used, provided that there is a many-to-one relationship between
4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.
4047           This restriction ensures that no synchronization / locks are required
4048           when writing. To disable this restriction, use ``safe_chunks=False``.
4049 
4050         Parameters
4051         ----------
4052         store : MutableMapping, str or path-like, optional
4053             Store or path to directory in local or remote file system.
4054         chunk_store : MutableMapping, str or path-like, optional
4055             Store or path to directory in local or remote file system only for Zarr
4056             array chunks. Requires zarr-python v2.4.0 or later.
4057         mode : {"w", "w-", "a", "r+", None}, optional
4058             Persistence mode: "w" means create (overwrite if exists);
4059             "w-" means create (fail if exists);
4060             "a" means override existing variables (create if does not exist);
4061             "r+" means modify existing array *values* only (raise an error if
4062             any metadata or shapes would change).
4063             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
4064             "r+" if ``region`` is set and ``w-`` otherwise.
4065         synchronizer : object, optional
4066             Zarr array synchronizer.
4067         group : str, optional
4068             Group path. (a.k.a. `path` in zarr terminology.)
4069         encoding : dict, optional
4070             Nested dictionary with variable names as keys and dictionaries of
4071             variable specific encodings as values, e.g.,
4072             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
4073         compute : bool, default: True
4074             If True write array data immediately, otherwise return a
4075             ``dask.delayed.Delayed`` object that can be computed to write
4076             array data later. Metadata is always updated eagerly.
4077         consolidated : bool, optional
4078             If True, apply zarr's `consolidate_metadata` function to the store
4079             after writing metadata and read existing stores with consolidated
4080             metadata; if False, do not. The default (`consolidated=None`) means
4081             write consolidated metadata and attempt to read consolidated
4082             metadata for existing stores (falling back to non-consolidated).
4083 
4084             When the experimental ``zarr_version=3``, ``consolidated`` must be
4085             either be ``None`` or ``False``.
4086         append_dim : hashable, optional
4087             If set, the dimension along which the data will be appended. All
4088             other dimensions on overridden variables must remain the same size.
4089         region : dict, optional
4090             Optional mapping from dimension names to integer slices along
4091             dataarray dimensions to indicate the region of existing zarr array(s)
4092             in which to write this datarray's data. For example,
4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
4094             that values should be written to the region ``0:1000`` along ``x``
4095             and ``10000:11000`` along ``y``.
4096 
4097             Two restrictions apply to the use of ``region``:
4098 
4099             - If ``region`` is set, _all_ variables in a dataarray must have at
4100               least one dimension in common with the region. Other variables
4101               should be written in a separate call to ``to_zarr()``.
4102             - Dimensions cannot be included in both ``region`` and
4103               ``append_dim`` at the same time. To create empty arrays to fill
4104               in with ``region``, use a separate call to ``to_zarr()`` with
4105               ``compute=False``. See "Appending to existing Zarr stores" in
4106               the reference documentation for full details.
4107         safe_chunks : bool, default: True
4108             If True, only allow writes to when there is a many-to-one relationship
4109             between Zarr chunks (specified in encoding) and Dask chunks.
4110             Set False to override this restriction; however, data may become corrupted
4111             if Zarr arrays are written in parallel. This option may be useful in combination
4112             with ``compute=False`` to initialize a Zarr store from an existing
4113             DataArray with arbitrary chunk structure.
4114         storage_options : dict, optional
4115             Any additional parameters for the storage backend (ignored for local
4116             paths).
4117         zarr_version : int or None, optional
4118             The desired zarr spec version to target (currently 2 or 3). The
4119             default of None will attempt to determine the zarr version from
4120             ``store`` when possible, otherwise defaulting to 2.
4121 
4122         Returns
4123         -------
4124             * ``dask.delayed.Delayed`` if compute is False
4125             * ZarrStore otherwise
4126 
4127         References
4128         ----------
4129         https://zarr.readthedocs.io/
4130 
4131         Notes
4132         -----
4133         Zarr chunking behavior:
4134             If chunks are found in the encoding argument or attribute
4135             corresponding to any DataArray, those chunks are used.
4136             If a DataArray is a dask array, it is written with those chunks.
4137             If not other chunks are found, Zarr uses its own heuristics to
4138             choose automatic chunk sizes.
4139 
4140         encoding:
4141             The encoding attribute (if exists) of the DataArray(s) will be
4142             used. Override any existing encodings by providing the ``encoding`` kwarg.
4143 
4144         See Also
4145         --------
4146         Dataset.to_zarr
4147         :ref:`io.zarr`
4148             The I/O user guide, with more details and examples.
4149         """
4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr
4151 
4152         if self.name is None:
4153             # If no name is set then use a generic xarray name
4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4155         elif self.name in self.coords or self.name in self.dims:
4156             # The name is the same as one of the coords names, which the netCDF data model
4157             # does not support, so rename it but keep track of the old name
4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4159             dataset.attrs[DATAARRAY_NAME] = self.name
4160         else:
4161             # No problems with the name - so we're fine!
4162             dataset = self.to_dataset()
4163 
4164         return to_zarr(  # type: ignore[call-overload,misc]
4165             dataset,
4166             store=store,
4167             chunk_store=chunk_store,
4168             mode=mode,
4169             synchronizer=synchronizer,
4170             group=group,
4171             encoding=encoding,
4172             compute=compute,
4173             consolidated=consolidated,
4174             append_dim=append_dim,
4175             region=region,
4176             safe_chunks=safe_chunks,
4177             storage_options=storage_options,
4178             zarr_version=zarr_version,
4179         )
4180 
4181     def to_dict(
4182         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
4183     ) -> dict[str, Any]:
4184         """
4185         Convert this xarray.DataArray into a dictionary following xarray
4186         naming conventions.
4187 
4188         Converts all variables and attributes to native Python objects.
4189         Useful for converting to json. To avoid datetime incompatibility
4190         use decode_times=False kwarg in xarray.open_dataset.
4191 
4192         Parameters
4193         ----------
4194         data : bool or {"list", "array"}, default: "list"
4195             Whether to include the actual data in the dictionary. When set to
4196             False, returns just the schema. If set to "array", returns data as
4197             underlying array type. If set to "list" (or True for backwards
4198             compatibility), returns data in lists of Python data types. Note
4199             that for obtaining the "list" output efficiently, use
4200             `da.compute().to_dict(data="list")`.
4201 
4202         encoding : bool, default: False
4203             Whether to include the Dataset's encoding in the dictionary.
4204 
4205         Returns
4206         -------
4207         dict: dict
4208 
4209         See Also
4210         --------
4211         DataArray.from_dict
4212         Dataset.to_dict
4213         """
4214         d = self.variable.to_dict(data=data)
4215         d.update({"coords": {}, "name": self.name})
4216         for k, coord in self.coords.items():
4217             d["coords"][k] = coord.variable.to_dict(data=data)
4218         if encoding:
4219             d["encoding"] = dict(self.encoding)
4220         return d
4221 
4222     @classmethod
4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4224         """Convert a dictionary into an xarray.DataArray
4225 
4226         Parameters
4227         ----------
4228         d : dict
4229             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4230 
4231         Returns
4232         -------
4233         obj : xarray.DataArray
4234 
4235         See Also
4236         --------
4237         DataArray.to_dict
4238         Dataset.from_dict
4239 
4240         Examples
4241         --------
4242         >>> d = {"dims": "t", "data": [1, 2, 3]}
4243         >>> da = xr.DataArray.from_dict(d)
4244         >>> da
4245         <xarray.DataArray (t: 3)>
4246         array([1, 2, 3])
4247         Dimensions without coordinates: t
4248 
4249         >>> d = {
4250         ...     "coords": {
4251         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4252         ...     },
4253         ...     "attrs": {"title": "air temperature"},
4254         ...     "dims": "t",
4255         ...     "data": [10, 20, 30],
4256         ...     "name": "a",
4257         ... }
4258         >>> da = xr.DataArray.from_dict(d)
4259         >>> da
4260         <xarray.DataArray 'a' (t: 3)>
4261         array([10, 20, 30])
4262         Coordinates:
4263           * t        (t) int64 0 1 2
4264         Attributes:
4265             title:    air temperature
4266         """
4267         coords = None
4268         if "coords" in d:
4269             try:
4270                 coords = {
4271                     k: (v["dims"], v["data"], v.get("attrs"))
4272                     for k, v in d["coords"].items()
4273                 }
4274             except KeyError as e:
4275                 raise ValueError(
4276                     "cannot convert dict when coords are missing the key "
4277                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4278                 )
4279         try:
4280             data = d["data"]
4281         except KeyError:
4282             raise ValueError("cannot convert dict without the key 'data''")
4283         else:
4284             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4285 
4286         obj.encoding.update(d.get("encoding", {}))
4287 
4288         return obj
4289 
4290     @classmethod
4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4292         """Convert a pandas.Series into an xarray.DataArray.
4293 
4294         If the series's index is a MultiIndex, it will be expanded into a
4295         tensor product of one-dimensional coordinates (filling in missing
4296         values with NaN). Thus this operation should be the inverse of the
4297         `to_series` method.
4298 
4299         Parameters
4300         ----------
4301         series : Series
4302             Pandas Series object to convert.
4303         sparse : bool, default: False
4304             If sparse=True, creates a sparse array instead of a dense NumPy array.
4305             Requires the pydata/sparse package.
4306 
4307         See Also
4308         --------
4309         DataArray.to_series
4310         Dataset.from_dataframe
4311         """
4312         temp_name = "__temporary_name"
4313         df = pd.DataFrame({temp_name: series})
4314         ds = Dataset.from_dataframe(df, sparse=sparse)
4315         result = cast(DataArray, ds[temp_name])
4316         result.name = series.name
4317         return result
4318 
4319     def to_cdms2(self) -> cdms2_Variable:
4320         """Convert this array into a cdms2.Variable"""
4321         from xarray.convert import to_cdms2
4322 
4323         return to_cdms2(self)
4324 
4325     @classmethod
4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4327         """Convert a cdms2.Variable into an xarray.DataArray"""
4328         from xarray.convert import from_cdms2
4329 
4330         return from_cdms2(variable)
4331 
4332     def to_iris(self) -> iris_Cube:
4333         """Convert this array into a iris.cube.Cube"""
4334         from xarray.convert import to_iris
4335 
4336         return to_iris(self)
4337 
4338     @classmethod
4339     def from_iris(cls, cube: iris_Cube) -> DataArray:
4340         """Convert a iris.cube.Cube into an xarray.DataArray"""
4341         from xarray.convert import from_iris
4342 
4343         return from_iris(cube)
4344 
4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4346         """Helper function for equals, broadcast_equals, and identical"""
4347 
4348         def compat(x, y):
4349             return getattr(x.variable, compat_str)(y.variable)
4350 
4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4352             self, other
4353         )
4354 
4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4356         """Two DataArrays are broadcast equal if they are equal after
4357         broadcasting them against each other such that they have the same
4358         dimensions.
4359 
4360         Parameters
4361         ----------
4362         other : DataArray
4363             DataArray to compare to.
4364 
4365         Returns
4366         ----------
4367         equal : bool
4368             True if the two DataArrays are broadcast equal.
4369 
4370         See Also
4371         --------
4372         DataArray.equals
4373         DataArray.identical
4374 
4375         Examples
4376         --------
4377         >>> a = xr.DataArray([1, 2], dims="X")
4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4379         >>> a
4380         <xarray.DataArray (X: 2)>
4381         array([1, 2])
4382         Dimensions without coordinates: X
4383         >>> b
4384         <xarray.DataArray (X: 2, Y: 2)>
4385         array([[1, 1],
4386                [2, 2]])
4387         Dimensions without coordinates: X, Y
4388 
4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4390 
4391         >>> a.equals(b)
4392         False
4393         >>> a2, b2 = xr.broadcast(a, b)
4394         >>> a2.equals(b2)
4395         True
4396         >>> a.broadcast_equals(b)
4397         True
4398         """
4399         try:
4400             return self._all_compat(other, "broadcast_equals")
4401         except (TypeError, AttributeError):
4402             return False
4403 
4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4405         """True if two DataArrays have the same dimensions, coordinates and
4406         values; otherwise False.
4407 
4408         DataArrays can still be equal (like pandas objects) if they have NaN
4409         values in the same locations.
4410 
4411         This method is necessary because `v1 == v2` for ``DataArray``
4412         does element-wise comparisons (like numpy.ndarrays).
4413 
4414         Parameters
4415         ----------
4416         other : DataArray
4417             DataArray to compare to.
4418 
4419         Returns
4420         ----------
4421         equal : bool
4422             True if the two DataArrays are equal.
4423 
4424         See Also
4425         --------
4426         DataArray.broadcast_equals
4427         DataArray.identical
4428 
4429         Examples
4430         --------
4431         >>> a = xr.DataArray([1, 2, 3], dims="X")
4432         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4433         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4434         >>> d = xr.DataArray([3, 2, 1], dims="X")
4435         >>> a
4436         <xarray.DataArray (X: 3)>
4437         array([1, 2, 3])
4438         Dimensions without coordinates: X
4439         >>> b
4440         <xarray.DataArray (X: 3)>
4441         array([1, 2, 3])
4442         Dimensions without coordinates: X
4443         Attributes:
4444             units:    m
4445         >>> c
4446         <xarray.DataArray (Y: 3)>
4447         array([1, 2, 3])
4448         Dimensions without coordinates: Y
4449         >>> d
4450         <xarray.DataArray (X: 3)>
4451         array([3, 2, 1])
4452         Dimensions without coordinates: X
4453 
4454         >>> a.equals(b)
4455         True
4456         >>> a.equals(c)
4457         False
4458         >>> a.equals(d)
4459         False
4460         """
4461         try:
4462             return self._all_compat(other, "equals")
4463         except (TypeError, AttributeError):
4464             return False
4465 
4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4467         """Like equals, but also checks the array name and attributes, and
4468         attributes on all coordinates.
4469 
4470         Parameters
4471         ----------
4472         other : DataArray
4473             DataArray to compare to.
4474 
4475         Returns
4476         ----------
4477         equal : bool
4478             True if the two DataArrays are identical.
4479 
4480         See Also
4481         --------
4482         DataArray.broadcast_equals
4483         DataArray.equals
4484 
4485         Examples
4486         --------
4487         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4488         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4489         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4490         >>> a
4491         <xarray.DataArray 'Width' (X: 3)>
4492         array([1, 2, 3])
4493         Dimensions without coordinates: X
4494         Attributes:
4495             units:    m
4496         >>> b
4497         <xarray.DataArray 'Width' (X: 3)>
4498         array([1, 2, 3])
4499         Dimensions without coordinates: X
4500         Attributes:
4501             units:    m
4502         >>> c
4503         <xarray.DataArray 'Width' (X: 3)>
4504         array([1, 2, 3])
4505         Dimensions without coordinates: X
4506         Attributes:
4507             units:    ft
4508 
4509         >>> a.equals(b)
4510         True
4511         >>> a.identical(b)
4512         True
4513 
4514         >>> a.equals(c)
4515         True
4516         >>> a.identical(c)
4517         False
4518         """
4519         try:
4520             return self.name == other.name and self._all_compat(other, "identical")
4521         except (TypeError, AttributeError):
4522             return False
4523 
4524     def _result_name(self, other: Any = None) -> Hashable | None:
4525         # use the same naming heuristics as pandas:
4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4527         other_name = getattr(other, "name", _default)
4528         if other_name is _default or other_name == self.name:
4529             return self.name
4530         else:
4531             return None
4532 
4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4534         new_var = self.variable.__array_wrap__(obj, context)
4535         return self._replace(new_var)
4536 
4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4538         return self.dot(obj)
4539 
4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4541         # currently somewhat duplicative, as only other DataArrays are
4542         # compatible with matmul
4543         return computation.dot(other, self)
4544 
4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4546         keep_attrs = kwargs.pop("keep_attrs", None)
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=True)
4549         with warnings.catch_warnings():
4550             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4551             warnings.filterwarnings(
4552                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4553             )
4554             with np.errstate(all="ignore"):
4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4556             if keep_attrs:
4557                 da.attrs = self.attrs
4558             return da
4559 
4560     def _binary_op(
4561         self: T_DataArray,
4562         other: Any,
4563         f: Callable,
4564         reflexive: bool = False,
4565     ) -> T_DataArray:
4566         from xarray.core.groupby import GroupBy
4567 
4568         if isinstance(other, (Dataset, GroupBy)):
4569             return NotImplemented
4570         if isinstance(other, DataArray):
4571             align_type = OPTIONS["arithmetic_join"]
4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4573         other_variable = getattr(other, "variable", other)
4574         other_coords = getattr(other, "coords", None)
4575 
4576         variable = (
4577             f(self.variable, other_variable)
4578             if not reflexive
4579             else f(other_variable, self.variable)
4580         )
4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4582         name = self._result_name(other)
4583 
4584         return self._replace(variable, coords, name, indexes=indexes)
4585 
4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4587         from xarray.core.groupby import GroupBy
4588 
4589         if isinstance(other, GroupBy):
4590             raise TypeError(
4591                 "in-place operations between a DataArray and "
4592                 "a grouped object are not permitted"
4593             )
4594         # n.b. we can't align other to self (with other.reindex_like(self))
4595         # because `other` may be converted into floats, which would cause
4596         # in-place arithmetic to fail unpredictably. Instead, we simply
4597         # don't support automatic alignment with in-place arithmetic.
4598         other_coords = getattr(other, "coords", None)
4599         other_variable = getattr(other, "variable", other)
4600         try:
4601             with self.coords._merge_inplace(other_coords):
4602                 f(self.variable, other_variable)
4603         except MergeError as exc:
4604             raise MergeError(
4605                 "Automatic alignment is not supported for in-place operations.\n"
4606                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4607                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4608             ) from exc
4609         return self
4610 
4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4612         self.attrs = other.attrs
4613 
4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4615 
4616     def _title_for_slice(self, truncate: int = 50) -> str:
4617         """
4618         If the dataarray has 1 dimensional coordinates or comes from a slice
4619         we can show that info in the title
4620 
4621         Parameters
4622         ----------
4623         truncate : int, default: 50
4624             maximum number of characters for title
4625 
4626         Returns
4627         -------
4628         title : string
4629             Can be used for plot titles
4630 
4631         """
4632         one_dims = []
4633         for dim, coord in self.coords.items():
4634             if coord.size == 1:
4635                 one_dims.append(
4636                     "{dim} = {v}{unit}".format(
4637                         dim=dim,
4638                         v=format_item(coord.values),
4639                         unit=_get_units_from_attrs(coord),
4640                     )
4641                 )
4642 
4643         title = ", ".join(one_dims)
4644         if len(title) > truncate:
4645             title = title[: (truncate - 3)] + "..."
4646 
4647         return title
4648 
4649     def diff(
4650         self: T_DataArray,
4651         dim: Hashable,
4652         n: int = 1,
4653         label: Literal["upper", "lower"] = "upper",
4654     ) -> T_DataArray:
4655         """Calculate the n-th order discrete difference along given axis.
4656 
4657         Parameters
4658         ----------
4659         dim : Hashable
4660             Dimension over which to calculate the finite difference.
4661         n : int, default: 1
4662             The number of times values are differenced.
4663         label : {"upper", "lower"}, default: "upper"
4664             The new coordinate in dimension ``dim`` will have the
4665             values of either the minuend's or subtrahend's coordinate
4666             for values 'upper' and 'lower', respectively.
4667 
4668         Returns
4669         -------
4670         difference : DataArray
4671             The n-th order finite difference of this object.
4672 
4673         Notes
4674         -----
4675         `n` matches numpy's behavior and is different from pandas' first argument named
4676         `periods`.
4677 
4678         Examples
4679         --------
4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4681         >>> arr.diff("x")
4682         <xarray.DataArray (x: 3)>
4683         array([0, 1, 0])
4684         Coordinates:
4685           * x        (x) int64 2 3 4
4686         >>> arr.diff("x", 2)
4687         <xarray.DataArray (x: 2)>
4688         array([ 1, -1])
4689         Coordinates:
4690           * x        (x) int64 3 4
4691 
4692         See Also
4693         --------
4694         DataArray.differentiate
4695         """
4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4697         return self._from_temp_dataset(ds)
4698 
4699     def shift(
4700         self: T_DataArray,
4701         shifts: Mapping[Any, int] | None = None,
4702         fill_value: Any = dtypes.NA,
4703         **shifts_kwargs: int,
4704     ) -> T_DataArray:
4705         """Shift this DataArray by an offset along one or more dimensions.
4706 
4707         Only the data is moved; coordinates stay in place. This is consistent
4708         with the behavior of ``shift`` in pandas.
4709 
4710         Values shifted from beyond array bounds will appear at one end of
4711         each dimension, which are filled according to `fill_value`. For periodic
4712         offsets instead see `roll`.
4713 
4714         Parameters
4715         ----------
4716         shifts : mapping of Hashable to int or None, optional
4717             Integer offset to shift along each of the given dimensions.
4718             Positive offsets shift to the right; negative offsets shift to the
4719             left.
4720         fill_value : scalar, optional
4721             Value to use for newly missing values
4722         **shifts_kwargs
4723             The keyword arguments form of ``shifts``.
4724             One of shifts or shifts_kwargs must be provided.
4725 
4726         Returns
4727         -------
4728         shifted : DataArray
4729             DataArray with the same coordinates and attributes but shifted
4730             data.
4731 
4732         See Also
4733         --------
4734         roll
4735 
4736         Examples
4737         --------
4738         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4739         >>> arr.shift(x=1)
4740         <xarray.DataArray (x: 3)>
4741         array([nan,  5.,  6.])
4742         Dimensions without coordinates: x
4743         """
4744         variable = self.variable.shift(
4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4746         )
4747         return self._replace(variable=variable)
4748 
4749     def roll(
4750         self: T_DataArray,
4751         shifts: Mapping[Hashable, int] | None = None,
4752         roll_coords: bool = False,
4753         **shifts_kwargs: int,
4754     ) -> T_DataArray:
4755         """Roll this array by an offset along one or more dimensions.
4756 
4757         Unlike shift, roll treats the given dimensions as periodic, so will not
4758         create any missing values to be filled.
4759 
4760         Unlike shift, roll may rotate all variables, including coordinates
4761         if specified. The direction of rotation is consistent with
4762         :py:func:`numpy.roll`.
4763 
4764         Parameters
4765         ----------
4766         shifts : mapping of Hashable to int, optional
4767             Integer offset to rotate each of the given dimensions.
4768             Positive offsets roll to the right; negative offsets roll to the
4769             left.
4770         roll_coords : bool, default: False
4771             Indicates whether to roll the coordinates by the offset too.
4772         **shifts_kwargs : {dim: offset, ...}, optional
4773             The keyword arguments form of ``shifts``.
4774             One of shifts or shifts_kwargs must be provided.
4775 
4776         Returns
4777         -------
4778         rolled : DataArray
4779             DataArray with the same attributes but rolled data and coordinates.
4780 
4781         See Also
4782         --------
4783         shift
4784 
4785         Examples
4786         --------
4787         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4788         >>> arr.roll(x=1)
4789         <xarray.DataArray (x: 3)>
4790         array([7, 5, 6])
4791         Dimensions without coordinates: x
4792         """
4793         ds = self._to_temp_dataset().roll(
4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4795         )
4796         return self._from_temp_dataset(ds)
4797 
4798     @property
4799     def real(self: T_DataArray) -> T_DataArray:
4800         """
4801         The real part of the array.
4802 
4803         See Also
4804         --------
4805         numpy.ndarray.real
4806         """
4807         return self._replace(self.variable.real)
4808 
4809     @property
4810     def imag(self: T_DataArray) -> T_DataArray:
4811         """
4812         The imaginary part of the array.
4813 
4814         See Also
4815         --------
4816         numpy.ndarray.imag
4817         """
4818         return self._replace(self.variable.imag)
4819 
4820     def dot(
4821         self: T_DataArray,
4822         other: T_DataArray,
4823         dims: Dims = None,
4824     ) -> T_DataArray:
4825         """Perform dot product of two DataArrays along their shared dims.
4826 
4827         Equivalent to taking taking tensordot over all shared dims.
4828 
4829         Parameters
4830         ----------
4831         other : DataArray
4832             The other array with which the dot product is performed.
4833         dims : ..., str, Iterable of Hashable or None, optional
4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4835             If not specified, then all the common dimensions are summed over.
4836 
4837         Returns
4838         -------
4839         result : DataArray
4840             Array resulting from the dot product over all shared dimensions.
4841 
4842         See Also
4843         --------
4844         dot
4845         numpy.tensordot
4846 
4847         Examples
4848         --------
4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4850         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4851         >>> dm_vals = np.arange(4)
4852         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4853 
4854         >>> dm.dims
4855         ('z',)
4856 
4857         >>> da.dims
4858         ('x', 'y', 'z')
4859 
4860         >>> dot_result = da.dot(dm)
4861         >>> dot_result.dims
4862         ('x', 'y')
4863 
4864         """
4865         if isinstance(other, Dataset):
4866             raise NotImplementedError(
4867                 "dot products are not yet supported with Dataset objects."
4868             )
4869         if not isinstance(other, DataArray):
4870             raise TypeError("dot only operates on DataArrays.")
4871 
4872         return computation.dot(self, other, dims=dims)
4873 
4874     # change type of self and return to T_DataArray once
4875     # https://github.com/python/mypy/issues/12846 is resolved
4876     def sortby(
4877         self,
4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4879         ascending: bool = True,
4880     ) -> DataArray:
4881         """Sort object by labels or values (along an axis).
4882 
4883         Sorts the dataarray, either along specified dimensions,
4884         or according to values of 1-D dataarrays that share dimension
4885         with calling object.
4886 
4887         If the input variables are dataarrays, then the dataarrays are aligned
4888         (via left-join) to the calling object prior to sorting by cell values.
4889         NaNs are sorted to the end, following Numpy convention.
4890 
4891         If multiple sorts along the same dimension is
4892         given, numpy's lexsort is performed along that dimension:
4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4894         and the FIRST key in the sequence is used as the primary sort key,
4895         followed by the 2nd key, etc.
4896 
4897         Parameters
4898         ----------
4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4900             1D DataArray objects or name(s) of 1D variable(s) in
4901             coords whose values are used to sort this array.
4902         ascending : bool, default: True
4903             Whether to sort by ascending or descending order.
4904 
4905         Returns
4906         -------
4907         sorted : DataArray
4908             A new dataarray where all the specified dims are sorted by dim
4909             labels.
4910 
4911         See Also
4912         --------
4913         Dataset.sortby
4914         numpy.sort
4915         pandas.sort_values
4916         pandas.sort_index
4917 
4918         Examples
4919         --------
4920         >>> da = xr.DataArray(
4921         ...     np.random.rand(5),
4922         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4923         ...     dims="time",
4924         ... )
4925         >>> da
4926         <xarray.DataArray (time: 5)>
4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4928         Coordinates:
4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4930 
4931         >>> da.sortby(da)
4932         <xarray.DataArray (time: 5)>
4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4934         Coordinates:
4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4936         """
4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4938         return self._from_temp_dataset(ds)
4939 
4940     def quantile(
4941         self: T_DataArray,
4942         q: ArrayLike,
4943         dim: Dims = None,
4944         method: QuantileMethods = "linear",
4945         keep_attrs: bool | None = None,
4946         skipna: bool | None = None,
4947         interpolation: QuantileMethods | None = None,
4948     ) -> T_DataArray:
4949         """Compute the qth quantile of the data along the specified dimension.
4950 
4951         Returns the qth quantiles(s) of the array elements.
4952 
4953         Parameters
4954         ----------
4955         q : float or array-like of float
4956             Quantile to compute, which must be between 0 and 1 inclusive.
4957         dim : str or Iterable of Hashable, optional
4958             Dimension(s) over which to apply quantile.
4959         method : str, default: "linear"
4960             This optional parameter specifies the interpolation method to use when the
4961             desired quantile lies between two data points. The options sorted by their R
4962             type as summarized in the H&F paper [1]_ are:
4963 
4964                 1. "inverted_cdf" (*)
4965                 2. "averaged_inverted_cdf" (*)
4966                 3. "closest_observation" (*)
4967                 4. "interpolated_inverted_cdf" (*)
4968                 5. "hazen" (*)
4969                 6. "weibull" (*)
4970                 7. "linear"  (default)
4971                 8. "median_unbiased" (*)
4972                 9. "normal_unbiased" (*)
4973 
4974             The first three methods are discontiuous. The following discontinuous
4975             variations of the default "linear" (7.) option are also available:
4976 
4977                 * "lower"
4978                 * "higher"
4979                 * "midpoint"
4980                 * "nearest"
4981 
4982             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4983             was previously called "interpolation", renamed in accordance with numpy
4984             version 1.22.0.
4985 
4986             (*) These methods require numpy version 1.22 or newer.
4987 
4988         keep_attrs : bool or None, optional
4989             If True, the dataset's attributes (`attrs`) will be copied from
4990             the original object to the new one.  If False (default), the new
4991             object will be returned without attributes.
4992         skipna : bool or None, optional
4993             If True, skip missing values (as marked by NaN). By default, only
4994             skips missing values for float dtypes; other dtypes either do not
4995             have a sentinel missing value (int) or skipna=True has not been
4996             implemented (object, datetime64 or timedelta64).
4997 
4998         Returns
4999         -------
5000         quantiles : DataArray
5001             If `q` is a single quantile, then the result
5002             is a scalar. If multiple percentiles are given, first axis of
5003             the result corresponds to the quantile and a quantile dimension
5004             is added to the return array. The other dimensions are the
5005             dimensions that remain after the reduction of the array.
5006 
5007         See Also
5008         --------
5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
5010 
5011         Examples
5012         --------
5013         >>> da = xr.DataArray(
5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
5015         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5016         ...     dims=("x", "y"),
5017         ... )
5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)
5019         <xarray.DataArray ()>
5020         array(0.7)
5021         Coordinates:
5022             quantile  float64 0.0
5023         >>> da.quantile(0, dim="x")
5024         <xarray.DataArray (y: 4)>
5025         array([0.7, 4.2, 2.6, 1.5])
5026         Coordinates:
5027           * y         (y) float64 1.0 1.5 2.0 2.5
5028             quantile  float64 0.0
5029         >>> da.quantile([0, 0.5, 1])
5030         <xarray.DataArray (quantile: 3)>
5031         array([0.7, 3.4, 9.4])
5032         Coordinates:
5033           * quantile  (quantile) float64 0.0 0.5 1.0
5034         >>> da.quantile([0, 0.5, 1], dim="x")
5035         <xarray.DataArray (quantile: 3, y: 4)>
5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],
5037                [3.6 , 5.75, 6.  , 1.7 ],
5038                [6.5 , 7.3 , 9.4 , 1.9 ]])
5039         Coordinates:
5040           * y         (y) float64 1.0 1.5 2.0 2.5
5041           * quantile  (quantile) float64 0.0 0.5 1.0
5042 
5043         References
5044         ----------
5045         .. [1] R. J. Hyndman and Y. Fan,
5046            "Sample quantiles in statistical packages,"
5047            The American Statistician, 50(4), pp. 361-365, 1996
5048         """
5049 
5050         ds = self._to_temp_dataset().quantile(
5051             q,
5052             dim=dim,
5053             keep_attrs=keep_attrs,
5054             method=method,
5055             skipna=skipna,
5056             interpolation=interpolation,
5057         )
5058         return self._from_temp_dataset(ds)
5059 
5060     def rank(
5061         self: T_DataArray,
5062         dim: Hashable,
5063         pct: bool = False,
5064         keep_attrs: bool | None = None,
5065     ) -> T_DataArray:
5066         """Ranks the data.
5067 
5068         Equal values are assigned a rank that is the average of the ranks that
5069         would have been otherwise assigned to all of the values within that
5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
5071 
5072         NaNs in the input array are returned as NaNs.
5073 
5074         The `bottleneck` library is required.
5075 
5076         Parameters
5077         ----------
5078         dim : Hashable
5079             Dimension over which to compute rank.
5080         pct : bool, default: False
5081             If True, compute percentage ranks, otherwise compute integer ranks.
5082         keep_attrs : bool or None, optional
5083             If True, the dataset's attributes (`attrs`) will be copied from
5084             the original object to the new one.  If False (default), the new
5085             object will be returned without attributes.
5086 
5087         Returns
5088         -------
5089         ranked : DataArray
5090             DataArray with the same coordinates and dtype 'float64'.
5091 
5092         Examples
5093         --------
5094         >>> arr = xr.DataArray([5, 6, 7], dims="x")
5095         >>> arr.rank("x")
5096         <xarray.DataArray (x: 3)>
5097         array([1., 2., 3.])
5098         Dimensions without coordinates: x
5099         """
5100 
5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
5102         return self._from_temp_dataset(ds)
5103 
5104     def differentiate(
5105         self: T_DataArray,
5106         coord: Hashable,
5107         edge_order: Literal[1, 2] = 1,
5108         datetime_unit: DatetimeUnitOptions = None,
5109     ) -> T_DataArray:
5110         """ Differentiate the array with the second order accurate central
5111         differences.
5112 
5113         .. note::
5114             This feature is limited to simple cartesian geometry, i.e. coord
5115             must be one dimensional.
5116 
5117         Parameters
5118         ----------
5119         coord : Hashable
5120             The coordinate to be used to compute the gradient.
5121         edge_order : {1, 2}, default: 1
5122             N-th order accurate differences at the boundaries.
5123         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5124                          "us", "ns", "ps", "fs", "as", None}, optional
5125             Unit to compute gradient. Only valid for datetime coordinate.
5126 
5127         Returns
5128         -------
5129         differentiated: DataArray
5130 
5131         See also
5132         --------
5133         numpy.gradient: corresponding numpy function
5134 
5135         Examples
5136         --------
5137 
5138         >>> da = xr.DataArray(
5139         ...     np.arange(12).reshape(4, 3),
5140         ...     dims=["x", "y"],
5141         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5142         ... )
5143         >>> da
5144         <xarray.DataArray (x: 4, y: 3)>
5145         array([[ 0,  1,  2],
5146                [ 3,  4,  5],
5147                [ 6,  7,  8],
5148                [ 9, 10, 11]])
5149         Coordinates:
5150           * x        (x) float64 0.0 0.1 1.1 1.2
5151         Dimensions without coordinates: y
5152         >>>
5153         >>> da.differentiate("x")
5154         <xarray.DataArray (x: 4, y: 3)>
5155         array([[30.        , 30.        , 30.        ],
5156                [27.54545455, 27.54545455, 27.54545455],
5157                [27.54545455, 27.54545455, 27.54545455],
5158                [30.        , 30.        , 30.        ]])
5159         Coordinates:
5160           * x        (x) float64 0.0 0.1 1.1 1.2
5161         Dimensions without coordinates: y
5162         """
5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
5164         return self._from_temp_dataset(ds)
5165 
5166     # change type of self and return to T_DataArray once
5167     # https://github.com/python/mypy/issues/12846 is resolved
5168     def integrate(
5169         self,
5170         coord: Hashable | Sequence[Hashable] = None,
5171         datetime_unit: DatetimeUnitOptions = None,
5172     ) -> DataArray:
5173         """Integrate along the given coordinate using the trapezoidal rule.
5174 
5175         .. note::
5176             This feature is limited to simple cartesian geometry, i.e. coord
5177             must be one dimensional.
5178 
5179         Parameters
5180         ----------
5181         coord : Hashable, or sequence of Hashable
5182             Coordinate(s) used for the integration.
5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5184                         'ps', 'fs', 'as', None}, optional
5185             Specify the unit if a datetime coordinate is used.
5186 
5187         Returns
5188         -------
5189         integrated : DataArray
5190 
5191         See also
5192         --------
5193         Dataset.integrate
5194         numpy.trapz : corresponding numpy function
5195 
5196         Examples
5197         --------
5198 
5199         >>> da = xr.DataArray(
5200         ...     np.arange(12).reshape(4, 3),
5201         ...     dims=["x", "y"],
5202         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5203         ... )
5204         >>> da
5205         <xarray.DataArray (x: 4, y: 3)>
5206         array([[ 0,  1,  2],
5207                [ 3,  4,  5],
5208                [ 6,  7,  8],
5209                [ 9, 10, 11]])
5210         Coordinates:
5211           * x        (x) float64 0.0 0.1 1.1 1.2
5212         Dimensions without coordinates: y
5213         >>>
5214         >>> da.integrate("x")
5215         <xarray.DataArray (y: 3)>
5216         array([5.4, 6.6, 7.8])
5217         Dimensions without coordinates: y
5218         """
5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5220         return self._from_temp_dataset(ds)
5221 
5222     # change type of self and return to T_DataArray once
5223     # https://github.com/python/mypy/issues/12846 is resolved
5224     def cumulative_integrate(
5225         self,
5226         coord: Hashable | Sequence[Hashable] = None,
5227         datetime_unit: DatetimeUnitOptions = None,
5228     ) -> DataArray:
5229         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5230 
5231         .. note::
5232             This feature is limited to simple cartesian geometry, i.e. coord
5233             must be one dimensional.
5234 
5235             The first entry of the cumulative integral is always 0, in order to keep the
5236             length of the dimension unchanged between input and output.
5237 
5238         Parameters
5239         ----------
5240         coord : Hashable, or sequence of Hashable
5241             Coordinate(s) used for the integration.
5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5243                         'ps', 'fs', 'as', None}, optional
5244             Specify the unit if a datetime coordinate is used.
5245 
5246         Returns
5247         -------
5248         integrated : DataArray
5249 
5250         See also
5251         --------
5252         Dataset.cumulative_integrate
5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5254 
5255         Examples
5256         --------
5257 
5258         >>> da = xr.DataArray(
5259         ...     np.arange(12).reshape(4, 3),
5260         ...     dims=["x", "y"],
5261         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5262         ... )
5263         >>> da
5264         <xarray.DataArray (x: 4, y: 3)>
5265         array([[ 0,  1,  2],
5266                [ 3,  4,  5],
5267                [ 6,  7,  8],
5268                [ 9, 10, 11]])
5269         Coordinates:
5270           * x        (x) float64 0.0 0.1 1.1 1.2
5271         Dimensions without coordinates: y
5272         >>>
5273         >>> da.cumulative_integrate("x")
5274         <xarray.DataArray (x: 4, y: 3)>
5275         array([[0.  , 0.  , 0.  ],
5276                [0.15, 0.25, 0.35],
5277                [4.65, 5.75, 6.85],
5278                [5.4 , 6.6 , 7.8 ]])
5279         Coordinates:
5280           * x        (x) float64 0.0 0.1 1.1 1.2
5281         Dimensions without coordinates: y
5282         """
5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5284         return self._from_temp_dataset(ds)
5285 
5286     def unify_chunks(self) -> DataArray:
5287         """Unify chunk size along all chunked dimensions of this DataArray.
5288 
5289         Returns
5290         -------
5291         DataArray with consistent chunk sizes for all dask-array variables
5292 
5293         See Also
5294         --------
5295         dask.array.core.unify_chunks
5296         """
5297 
5298         return unify_chunks(self)[0]
5299 
5300     def map_blocks(
5301         self,
5302         func: Callable[..., T_Xarray],
5303         args: Sequence[Any] = (),
5304         kwargs: Mapping[str, Any] | None = None,
5305         template: DataArray | Dataset | None = None,
5306     ) -> T_Xarray:
5307         """
5308         Apply a function to each block of this DataArray.
5309 
5310         .. warning::
5311             This method is experimental and its signature may change.
5312 
5313         Parameters
5314         ----------
5315         func : callable
5316             User-provided function that accepts a DataArray as its first
5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5318             corresponding to one chunk along each chunked dimension. ``func`` will be
5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5320 
5321             This function must return either a single DataArray or a single Dataset.
5322 
5323             This function cannot add a new chunked dimension.
5324         args : sequence
5325             Passed to func after unpacking and subsetting any xarray objects by blocks.
5326             xarray objects in args must be aligned with this object, otherwise an error is raised.
5327         kwargs : mapping
5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5329             subset to blocks. Passing dask collections in kwargs is not allowed.
5330         template : DataArray or Dataset, optional
5331             xarray object representing the final result after compute is called. If not provided,
5332             the function will be first run on mocked-up data, that looks like this object but
5333             has sizes 0, to determine properties of the returned object such as dtype,
5334             variable names, attributes, new dimensions and new indexes (if any).
5335             ``template`` must be provided if the function changes the size of existing dimensions.
5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5337             ``attrs`` set by ``func`` will be ignored.
5338 
5339         Returns
5340         -------
5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5342         function.
5343 
5344         Notes
5345         -----
5346         This function is designed for when ``func`` needs to manipulate a whole xarray object
5347         subset to each block. Each block is loaded into memory. In the more common case where
5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5349 
5350         If none of the variables in this object is backed by dask arrays, calling this function is
5351         equivalent to calling ``func(obj, *args, **kwargs)``.
5352 
5353         See Also
5354         --------
5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5356         xarray.DataArray.map_blocks
5357 
5358         Examples
5359         --------
5360         Calculate an anomaly from climatology using ``.groupby()``. Using
5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5362         its indices, and its methods like ``.groupby()``.
5363 
5364         >>> def calculate_anomaly(da, groupby_type="time.month"):
5365         ...     gb = da.groupby(groupby_type)
5366         ...     clim = gb.mean(dim="time")
5367         ...     return gb - clim
5368         ...
5369         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5370         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5371         >>> np.random.seed(123)
5372         >>> array = xr.DataArray(
5373         ...     np.random.rand(len(time)),
5374         ...     dims=["time"],
5375         ...     coords={"time": time, "month": month},
5376         ... ).chunk()
5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5378         <xarray.DataArray (time: 24)>
5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5384         Coordinates:
5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5387 
5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5389         to the function being applied in ``xr.map_blocks()``:
5390 
5391         >>> array.map_blocks(
5392         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5393         ... )  # doctest: +ELLIPSIS
5394         <xarray.DataArray (time: 24)>
5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5396         Coordinates:
5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5399         """
5400         from xarray.core.parallel import map_blocks
5401 
5402         return map_blocks(func, self, args, kwargs, template)
5403 
5404     def polyfit(
5405         self,
5406         dim: Hashable,
5407         deg: int,
5408         skipna: bool | None = None,
5409         rcond: float | None = None,
5410         w: Hashable | Any | None = None,
5411         full: bool = False,
5412         cov: bool | Literal["unscaled"] = False,
5413     ) -> Dataset:
5414         """
5415         Least squares polynomial fit.
5416 
5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5418         invalid values when `skipna = True`.
5419 
5420         Parameters
5421         ----------
5422         dim : Hashable
5423             Coordinate along which to fit the polynomials.
5424         deg : int
5425             Degree of the fitting polynomial.
5426         skipna : bool or None, optional
5427             If True, removes all invalid values before fitting each 1D slices of the array.
5428             Default is True if data is stored in a dask.array or if there is any
5429             invalid values, False otherwise.
5430         rcond : float or None, optional
5431             Relative condition number to the fit.
5432         w : Hashable, array-like or None, optional
5433             Weights to apply to the y-coordinate of the sample points.
5434             Can be an array-like object or the name of a coordinate in the dataset.
5435         full : bool, default: False
5436             Whether to return the residuals, matrix rank and singular values in addition
5437             to the coefficients.
5438         cov : bool or "unscaled", default: False
5439             Whether to return to the covariance matrix in addition to the coefficients.
5440             The matrix is not scaled if `cov='unscaled'`.
5441 
5442         Returns
5443         -------
5444         polyfit_results : Dataset
5445             A single dataset which contains:
5446 
5447             polyfit_coefficients
5448                 The coefficients of the best fit.
5449             polyfit_residuals
5450                 The residuals of the least-square computation (only included if `full=True`).
5451                 When the matrix rank is deficient, np.nan is returned.
5452             [dim]_matrix_rank
5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5454             [dim]_singular_value
5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5456             polyfit_covariance
5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5458 
5459         See Also
5460         --------
5461         numpy.polyfit
5462         numpy.polyval
5463         xarray.polyval
5464         """
5465         return self._to_temp_dataset().polyfit(
5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5467         )
5468 
5469     def pad(
5470         self: T_DataArray,
5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5472         mode: PadModeOptions = "constant",
5473         stat_length: int
5474         | tuple[int, int]
5475         | Mapping[Any, tuple[int, int]]
5476         | None = None,
5477         constant_values: float
5478         | tuple[float, float]
5479         | Mapping[Any, tuple[float, float]]
5480         | None = None,
5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5482         reflect_type: PadReflectOptions = None,
5483         keep_attrs: bool | None = None,
5484         **pad_width_kwargs: Any,
5485     ) -> T_DataArray:
5486         """Pad this array along one or more dimensions.
5487 
5488         .. warning::
5489             This function is experimental and its behaviour is likely to change
5490             especially regarding padding of dimension coordinates (or IndexVariables).
5491 
5492         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5493         coordinates will be padded with the same mode, otherwise coordinates
5494         are padded using the "constant" mode with fill_value dtypes.NA.
5495 
5496         Parameters
5497         ----------
5498         pad_width : mapping of Hashable to tuple of int
5499             Mapping with the form of {dim: (pad_before, pad_after)}
5500             describing the number of values padded along each dimension.
5501             {dim: pad} is a shortcut for pad_before = pad_after = pad
5502         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5503             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5504             How to pad the DataArray (taken from numpy docs):
5505 
5506             - "constant": Pads with a constant value.
5507             - "edge": Pads with the edge values of array.
5508             - "linear_ramp": Pads with the linear ramp between end_value and the
5509               array edge value.
5510             - "maximum": Pads with the maximum value of all or part of the
5511               vector along each axis.
5512             - "mean": Pads with the mean value of all or part of the
5513               vector along each axis.
5514             - "median": Pads with the median value of all or part of the
5515               vector along each axis.
5516             - "minimum": Pads with the minimum value of all or part of the
5517               vector along each axis.
5518             - "reflect": Pads with the reflection of the vector mirrored on
5519               the first and last values of the vector along each axis.
5520             - "symmetric": Pads with the reflection of the vector mirrored
5521               along the edge of the array.
5522             - "wrap": Pads with the wrap of the vector along the axis.
5523               The first values are used to pad the end and the
5524               end values are used to pad the beginning.
5525 
5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5528             values at edge of each axis used to calculate the statistic value.
5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5530             statistic lengths along each dimension.
5531             ((before, after),) yields same before and after statistic lengths
5532             for each dimension.
5533             (stat_length,) or int is a shortcut for before = after = statistic
5534             length for all axes.
5535             Default is ``None``, to use the entire axis.
5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5537             Used in 'constant'.  The values to set the padded values for each
5538             axis.
5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5540             pad constants along each dimension.
5541             ``((before, after),)`` yields same before and after constants for each
5542             dimension.
5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5544             all dimensions.
5545             Default is 0.
5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5547             Used in 'linear_ramp'.  The values used for the ending value of the
5548             linear_ramp and that will form the edge of the padded array.
5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5550             end values along each dimension.
5551             ``((before, after),)`` yields same before and after end values for each
5552             axis.
5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5554             all axes.
5555             Default is 0.
5556         reflect_type : {"even", "odd", None}, optional
5557             Used in "reflect", and "symmetric". The "even" style is the
5558             default with an unaltered reflection around the edge value. For
5559             the "odd" style, the extended part of the array is created by
5560             subtracting the reflected values from two times the edge value.
5561         keep_attrs : bool or None, optional
5562             If True, the attributes (``attrs``) will be copied from the
5563             original object to the new one. If False, the new object
5564             will be returned without attributes.
5565         **pad_width_kwargs
5566             The keyword arguments form of ``pad_width``.
5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5568 
5569         Returns
5570         -------
5571         padded : DataArray
5572             DataArray with the padded coordinates and data.
5573 
5574         See Also
5575         --------
5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5577 
5578         Notes
5579         -----
5580         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5581         promoted to ``float`` and padded with ``np.nan``.
5582 
5583         Padding coordinates will drop their corresponding index (if any) and will reset default
5584         indexes for dimension coordinates.
5585 
5586         Examples
5587         --------
5588         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5589         >>> arr.pad(x=(1, 2), constant_values=0)
5590         <xarray.DataArray (x: 6)>
5591         array([0, 5, 6, 7, 0, 0])
5592         Coordinates:
5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5594 
5595         >>> da = xr.DataArray(
5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5597         ...     dims=["x", "y"],
5598         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5599         ... )
5600         >>> da.pad(x=1)
5601         <xarray.DataArray (x: 4, y: 4)>
5602         array([[nan, nan, nan, nan],
5603                [ 0.,  1.,  2.,  3.],
5604                [10., 11., 12., 13.],
5605                [nan, nan, nan, nan]])
5606         Coordinates:
5607           * x        (x) float64 nan 0.0 1.0 nan
5608           * y        (y) int64 10 20 30 40
5609             z        (x) float64 nan 100.0 200.0 nan
5610 
5611         Careful, ``constant_values`` are coerced to the data type of the array which may
5612         lead to a loss of precision:
5613 
5614         >>> da.pad(x=1, constant_values=1.23456789)
5615         <xarray.DataArray (x: 4, y: 4)>
5616         array([[ 1,  1,  1,  1],
5617                [ 0,  1,  2,  3],
5618                [10, 11, 12, 13],
5619                [ 1,  1,  1,  1]])
5620         Coordinates:
5621           * x        (x) float64 nan 0.0 1.0 nan
5622           * y        (y) int64 10 20 30 40
5623             z        (x) float64 nan 100.0 200.0 nan
5624         """
5625         ds = self._to_temp_dataset().pad(
5626             pad_width=pad_width,
5627             mode=mode,
5628             stat_length=stat_length,
5629             constant_values=constant_values,
5630             end_values=end_values,
5631             reflect_type=reflect_type,
5632             keep_attrs=keep_attrs,
5633             **pad_width_kwargs,
5634         )
5635         return self._from_temp_dataset(ds)
5636 
5637     def idxmin(
5638         self,
5639         dim: Hashable | None = None,
5640         skipna: bool | None = None,
5641         fill_value: Any = dtypes.NA,
5642         keep_attrs: bool | None = None,
5643     ) -> DataArray:
5644         """Return the coordinate label of the minimum value along a dimension.
5645 
5646         Returns a new `DataArray` named after the dimension with the values of
5647         the coordinate labels along that dimension corresponding to minimum
5648         values along that dimension.
5649 
5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5652 
5653         Parameters
5654         ----------
5655         dim : str, optional
5656             Dimension over which to apply `idxmin`.  This is optional for 1D
5657             arrays, but required for arrays with 2 or more dimensions.
5658         skipna : bool or None, default: None
5659             If True, skip missing values (as marked by NaN). By default, only
5660             skips missing values for ``float``, ``complex``, and ``object``
5661             dtypes; other dtypes either do not have a sentinel missing value
5662             (``int``) or ``skipna=True`` has not been implemented
5663             (``datetime64`` or ``timedelta64``).
5664         fill_value : Any, default: NaN
5665             Value to be filled in case all of the values along a dimension are
5666             null.  By default this is NaN.  The fill value and result are
5667             automatically converted to a compatible dtype if possible.
5668             Ignored if ``skipna`` is False.
5669         keep_attrs : bool or None, optional
5670             If True, the attributes (``attrs``) will be copied from the
5671             original object to the new one. If False, the new object
5672             will be returned without attributes.
5673 
5674         Returns
5675         -------
5676         reduced : DataArray
5677             New `DataArray` object with `idxmin` applied to its data and the
5678             indicated dimension removed.
5679 
5680         See Also
5681         --------
5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5683 
5684         Examples
5685         --------
5686         >>> array = xr.DataArray(
5687         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5688         ... )
5689         >>> array.min()
5690         <xarray.DataArray ()>
5691         array(-2)
5692         >>> array.argmin(...)
5693         {'x': <xarray.DataArray ()>
5694         array(4)}
5695         >>> array.idxmin()
5696         <xarray.DataArray 'x' ()>
5697         array('e', dtype='<U1')
5698 
5699         >>> array = xr.DataArray(
5700         ...     [
5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5704         ...     ],
5705         ...     dims=["y", "x"],
5706         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5707         ... )
5708         >>> array.min(dim="x")
5709         <xarray.DataArray (y: 3)>
5710         array([-2., -4.,  1.])
5711         Coordinates:
5712           * y        (y) int64 -1 0 1
5713         >>> array.argmin(dim="x")
5714         <xarray.DataArray (y: 3)>
5715         array([4, 0, 2])
5716         Coordinates:
5717           * y        (y) int64 -1 0 1
5718         >>> array.idxmin(dim="x")
5719         <xarray.DataArray 'x' (y: 3)>
5720         array([16.,  0.,  4.])
5721         Coordinates:
5722           * y        (y) int64 -1 0 1
5723         """
5724         return computation._calc_idxminmax(
5725             array=self,
5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5727             dim=dim,
5728             skipna=skipna,
5729             fill_value=fill_value,
5730             keep_attrs=keep_attrs,
5731         )
5732 
5733     def idxmax(
5734         self,
5735         dim: Hashable = None,
5736         skipna: bool | None = None,
5737         fill_value: Any = dtypes.NA,
5738         keep_attrs: bool | None = None,
5739     ) -> DataArray:
5740         """Return the coordinate label of the maximum value along a dimension.
5741 
5742         Returns a new `DataArray` named after the dimension with the values of
5743         the coordinate labels along that dimension corresponding to maximum
5744         values along that dimension.
5745 
5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5748 
5749         Parameters
5750         ----------
5751         dim : Hashable, optional
5752             Dimension over which to apply `idxmax`.  This is optional for 1D
5753             arrays, but required for arrays with 2 or more dimensions.
5754         skipna : bool or None, default: None
5755             If True, skip missing values (as marked by NaN). By default, only
5756             skips missing values for ``float``, ``complex``, and ``object``
5757             dtypes; other dtypes either do not have a sentinel missing value
5758             (``int``) or ``skipna=True`` has not been implemented
5759             (``datetime64`` or ``timedelta64``).
5760         fill_value : Any, default: NaN
5761             Value to be filled in case all of the values along a dimension are
5762             null.  By default this is NaN.  The fill value and result are
5763             automatically converted to a compatible dtype if possible.
5764             Ignored if ``skipna`` is False.
5765         keep_attrs : bool or None, optional
5766             If True, the attributes (``attrs``) will be copied from the
5767             original object to the new one. If False, the new object
5768             will be returned without attributes.
5769 
5770         Returns
5771         -------
5772         reduced : DataArray
5773             New `DataArray` object with `idxmax` applied to its data and the
5774             indicated dimension removed.
5775 
5776         See Also
5777         --------
5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5779 
5780         Examples
5781         --------
5782         >>> array = xr.DataArray(
5783         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5784         ... )
5785         >>> array.max()
5786         <xarray.DataArray ()>
5787         array(2)
5788         >>> array.argmax(...)
5789         {'x': <xarray.DataArray ()>
5790         array(1)}
5791         >>> array.idxmax()
5792         <xarray.DataArray 'x' ()>
5793         array('b', dtype='<U1')
5794 
5795         >>> array = xr.DataArray(
5796         ...     [
5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5800         ...     ],
5801         ...     dims=["y", "x"],
5802         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5803         ... )
5804         >>> array.max(dim="x")
5805         <xarray.DataArray (y: 3)>
5806         array([2., 2., 1.])
5807         Coordinates:
5808           * y        (y) int64 -1 0 1
5809         >>> array.argmax(dim="x")
5810         <xarray.DataArray (y: 3)>
5811         array([0, 2, 2])
5812         Coordinates:
5813           * y        (y) int64 -1 0 1
5814         >>> array.idxmax(dim="x")
5815         <xarray.DataArray 'x' (y: 3)>
5816         array([0., 4., 4.])
5817         Coordinates:
5818           * y        (y) int64 -1 0 1
5819         """
5820         return computation._calc_idxminmax(
5821             array=self,
5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5823             dim=dim,
5824             skipna=skipna,
5825             fill_value=fill_value,
5826             keep_attrs=keep_attrs,
5827         )
5828 
5829     # change type of self and return to T_DataArray once
5830     # https://github.com/python/mypy/issues/12846 is resolved
5831     def argmin(
5832         self,
5833         dim: Dims = None,
5834         axis: int | None = None,
5835         keep_attrs: bool | None = None,
5836         skipna: bool | None = None,
5837     ) -> DataArray | dict[Hashable, DataArray]:
5838         """Index or indices of the minimum of the DataArray over one or more dimensions.
5839 
5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5841         which can be passed directly to isel(). If a single str is passed to 'dim' then
5842         returns a DataArray with dtype int.
5843 
5844         If there are multiple minima, the indices of the first one found will be
5845         returned.
5846 
5847         Parameters
5848         ----------
5849         dim : "...", str, Iterable of Hashable or None, optional
5850             The dimensions over which to find the minimum. By default, finds minimum over
5851             all dimensions - for now returning an int for backward compatibility, but
5852             this is deprecated, in future will return a dict with indices for all
5853             dimensions; to return a dict with all dimensions now, pass '...'.
5854         axis : int or None, optional
5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5856             can be supplied.
5857         keep_attrs : bool or None, optional
5858             If True, the attributes (`attrs`) will be copied from the original
5859             object to the new one. If False, the new object will be
5860             returned without attributes.
5861         skipna : bool or None, optional
5862             If True, skip missing values (as marked by NaN). By default, only
5863             skips missing values for float dtypes; other dtypes either do not
5864             have a sentinel missing value (int) or skipna=True has not been
5865             implemented (object, datetime64 or timedelta64).
5866 
5867         Returns
5868         -------
5869         result : DataArray or dict of DataArray
5870 
5871         See Also
5872         --------
5873         Variable.argmin, DataArray.idxmin
5874 
5875         Examples
5876         --------
5877         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5878         >>> array.min()
5879         <xarray.DataArray ()>
5880         array(-1)
5881         >>> array.argmin(...)
5882         {'x': <xarray.DataArray ()>
5883         array(2)}
5884         >>> array.isel(array.argmin(...))
5885         <xarray.DataArray ()>
5886         array(-1)
5887 
5888         >>> array = xr.DataArray(
5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5890         ...     dims=("x", "y", "z"),
5891         ... )
5892         >>> array.min(dim="x")
5893         <xarray.DataArray (y: 3, z: 3)>
5894         array([[ 1,  2,  1],
5895                [ 2, -5,  1],
5896                [ 2,  1,  1]])
5897         Dimensions without coordinates: y, z
5898         >>> array.argmin(dim="x")
5899         <xarray.DataArray (y: 3, z: 3)>
5900         array([[1, 0, 0],
5901                [1, 1, 1],
5902                [0, 0, 1]])
5903         Dimensions without coordinates: y, z
5904         >>> array.argmin(dim=["x"])
5905         {'x': <xarray.DataArray (y: 3, z: 3)>
5906         array([[1, 0, 0],
5907                [1, 1, 1],
5908                [0, 0, 1]])
5909         Dimensions without coordinates: y, z}
5910         >>> array.min(dim=("x", "z"))
5911         <xarray.DataArray (y: 3)>
5912         array([ 1, -5,  1])
5913         Dimensions without coordinates: y
5914         >>> array.argmin(dim=["x", "z"])
5915         {'x': <xarray.DataArray (y: 3)>
5916         array([0, 1, 0])
5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5918         array([2, 1, 1])
5919         Dimensions without coordinates: y}
5920         >>> array.isel(array.argmin(dim=["x", "z"]))
5921         <xarray.DataArray (y: 3)>
5922         array([ 1, -5,  1])
5923         Dimensions without coordinates: y
5924         """
5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5926         if isinstance(result, dict):
5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5928         else:
5929             return self._replace_maybe_drop_dims(result)
5930 
5931     # change type of self and return to T_DataArray once
5932     # https://github.com/python/mypy/issues/12846 is resolved
5933     def argmax(
5934         self,
5935         dim: Dims = None,
5936         axis: int | None = None,
5937         keep_attrs: bool | None = None,
5938         skipna: bool | None = None,
5939     ) -> DataArray | dict[Hashable, DataArray]:
5940         """Index or indices of the maximum of the DataArray over one or more dimensions.
5941 
5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5943         which can be passed directly to isel(). If a single str is passed to 'dim' then
5944         returns a DataArray with dtype int.
5945 
5946         If there are multiple maxima, the indices of the first one found will be
5947         returned.
5948 
5949         Parameters
5950         ----------
5951         dim : "...", str, Iterable of Hashable or None, optional
5952             The dimensions over which to find the maximum. By default, finds maximum over
5953             all dimensions - for now returning an int for backward compatibility, but
5954             this is deprecated, in future will return a dict with indices for all
5955             dimensions; to return a dict with all dimensions now, pass '...'.
5956         axis : int or None, optional
5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5958             can be supplied.
5959         keep_attrs : bool or None, optional
5960             If True, the attributes (`attrs`) will be copied from the original
5961             object to the new one. If False, the new object will be
5962             returned without attributes.
5963         skipna : bool or None, optional
5964             If True, skip missing values (as marked by NaN). By default, only
5965             skips missing values for float dtypes; other dtypes either do not
5966             have a sentinel missing value (int) or skipna=True has not been
5967             implemented (object, datetime64 or timedelta64).
5968 
5969         Returns
5970         -------
5971         result : DataArray or dict of DataArray
5972 
5973         See Also
5974         --------
5975         Variable.argmax, DataArray.idxmax
5976 
5977         Examples
5978         --------
5979         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5980         >>> array.max()
5981         <xarray.DataArray ()>
5982         array(3)
5983         >>> array.argmax(...)
5984         {'x': <xarray.DataArray ()>
5985         array(3)}
5986         >>> array.isel(array.argmax(...))
5987         <xarray.DataArray ()>
5988         array(3)
5989 
5990         >>> array = xr.DataArray(
5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5992         ...     dims=("x", "y", "z"),
5993         ... )
5994         >>> array.max(dim="x")
5995         <xarray.DataArray (y: 3, z: 3)>
5996         array([[3, 3, 2],
5997                [3, 5, 2],
5998                [2, 3, 3]])
5999         Dimensions without coordinates: y, z
6000         >>> array.argmax(dim="x")
6001         <xarray.DataArray (y: 3, z: 3)>
6002         array([[0, 1, 1],
6003                [0, 1, 0],
6004                [0, 1, 0]])
6005         Dimensions without coordinates: y, z
6006         >>> array.argmax(dim=["x"])
6007         {'x': <xarray.DataArray (y: 3, z: 3)>
6008         array([[0, 1, 1],
6009                [0, 1, 0],
6010                [0, 1, 0]])
6011         Dimensions without coordinates: y, z}
6012         >>> array.max(dim=("x", "z"))
6013         <xarray.DataArray (y: 3)>
6014         array([3, 5, 3])
6015         Dimensions without coordinates: y
6016         >>> array.argmax(dim=["x", "z"])
6017         {'x': <xarray.DataArray (y: 3)>
6018         array([0, 1, 0])
6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
6020         array([0, 1, 2])
6021         Dimensions without coordinates: y}
6022         >>> array.isel(array.argmax(dim=["x", "z"]))
6023         <xarray.DataArray (y: 3)>
6024         array([3, 5, 3])
6025         Dimensions without coordinates: y
6026         """
6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
6028         if isinstance(result, dict):
6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
6030         else:
6031             return self._replace_maybe_drop_dims(result)
6032 
6033     def query(
6034         self,
6035         queries: Mapping[Any, Any] | None = None,
6036         parser: QueryParserOptions = "pandas",
6037         engine: QueryEngineOptions = None,
6038         missing_dims: ErrorOptionsWithWarn = "raise",
6039         **queries_kwargs: Any,
6040     ) -> DataArray:
6041         """Return a new data array indexed along the specified
6042         dimension(s), where the indexers are given as strings containing
6043         Python expressions to be evaluated against the values in the array.
6044 
6045         Parameters
6046         ----------
6047         queries : dict-like or None, optional
6048             A dict-like with keys matching dimensions and values given by strings
6049             containing Python expressions to be evaluated against the data variables
6050             in the dataset. The expressions will be evaluated using the pandas
6051             eval() function, and can contain any valid Python expressions but cannot
6052             contain any Python statements.
6053         parser : {"pandas", "python"}, default: "pandas"
6054             The parser to use to construct the syntax tree from the expression.
6055             The default of 'pandas' parses code slightly different than standard
6056             Python. Alternatively, you can parse an expression using the 'python'
6057             parser to retain strict Python semantics.
6058         engine : {"python", "numexpr", None}, default: None
6059             The engine used to evaluate the expression. Supported engines are:
6060 
6061             - None: tries to use numexpr, falls back to python
6062             - "numexpr": evaluates expressions using numexpr
6063             - "python": performs operations as if you had eval’d in top level python
6064 
6065         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
6066             What to do if dimensions that should be selected from are not present in the
6067             DataArray:
6068 
6069             - "raise": raise an exception
6070             - "warn": raise a warning, and ignore the missing dimensions
6071             - "ignore": ignore the missing dimensions
6072 
6073         **queries_kwargs : {dim: query, ...}, optional
6074             The keyword arguments form of ``queries``.
6075             One of queries or queries_kwargs must be provided.
6076 
6077         Returns
6078         -------
6079         obj : DataArray
6080             A new DataArray with the same contents as this dataset, indexed by
6081             the results of the appropriate queries.
6082 
6083         See Also
6084         --------
6085         DataArray.isel
6086         Dataset.query
6087         pandas.eval
6088 
6089         Examples
6090         --------
6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
6092         >>> da
6093         <xarray.DataArray 'a' (x: 5)>
6094         array([0, 1, 2, 3, 4])
6095         Dimensions without coordinates: x
6096         >>> da.query(x="a > 2")
6097         <xarray.DataArray 'a' (x: 2)>
6098         array([3, 4])
6099         Dimensions without coordinates: x
6100         """
6101 
6102         ds = self._to_dataset_whole(shallow_copy=True)
6103         ds = ds.query(
6104             queries=queries,
6105             parser=parser,
6106             engine=engine,
6107             missing_dims=missing_dims,
6108             **queries_kwargs,
6109         )
6110         return ds[self.name]
6111 
6112     def curvefit(
6113         self,
6114         coords: str | DataArray | Iterable[str | DataArray],
6115         func: Callable[..., Any],
6116         reduce_dims: Dims = None,
6117         skipna: bool = True,
6118         p0: dict[str, Any] | None = None,
6119         bounds: dict[str, Any] | None = None,
6120         param_names: Sequence[str] | None = None,
6121         kwargs: dict[str, Any] | None = None,
6122     ) -> Dataset:
6123         """
6124         Curve fitting optimization for arbitrary functions.
6125 
6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
6127 
6128         Parameters
6129         ----------
6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable
6131             Independent coordinate(s) over which to perform the curve fitting. Must share
6132             at least one dimension with the calling object. When fitting multi-dimensional
6133             functions, supply `coords` as a sequence in the same order as arguments in
6134             `func`. To fit along existing dimensions of the calling object, `coords` can
6135             also be specified as a str or sequence of strs.
6136         func : callable
6137             User specified function in the form `f(x, *params)` which returns a numpy
6138             array of length `len(x)`. `params` are the fittable parameters which are optimized
6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
6140             coordinates, e.g. `f((x0, x1), *params)`.
6141         reduce_dims : str, Iterable of Hashable or None, optional
6142             Additional dimension(s) over which to aggregate while fitting. For example,
6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
6144             aggregate all lat and lon points and fit the specified function along the
6145             time dimension.
6146         skipna : bool, default: True
6147             Whether to skip missing values when fitting. Default is True.
6148         p0 : dict-like or None, optional
6149             Optional dictionary of parameter names to initial guesses passed to the
6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
6151             be assigned initial values following the default scipy behavior.
6152         bounds : dict-like or None, optional
6153             Optional dictionary of parameter names to bounding values passed to the
6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
6155             will be unbounded following the default scipy behavior.
6156         param_names : sequence of Hashable or None, optional
6157             Sequence of names for the fittable parameters of `func`. If not supplied,
6158             this will be automatically determined by arguments of `func`. `param_names`
6159             should be manually supplied when fitting a function that takes a variable
6160             number of parameters.
6161         **kwargs : optional
6162             Additional keyword arguments to passed to scipy curve_fit.
6163 
6164         Returns
6165         -------
6166         curvefit_results : Dataset
6167             A single dataset which contains:
6168 
6169             [var]_curvefit_coefficients
6170                 The coefficients of the best fit.
6171             [var]_curvefit_covariance
6172                 The covariance matrix of the coefficient estimates.
6173 
6174         See Also
6175         --------
6176         DataArray.polyfit
6177         scipy.optimize.curve_fit
6178         """
6179         return self._to_temp_dataset().curvefit(
6180             coords,
6181             func,
6182             reduce_dims=reduce_dims,
6183             skipna=skipna,
6184             p0=p0,
6185             bounds=bounds,
6186             param_names=param_names,
6187             kwargs=kwargs,
6188         )
6189 
6190     def drop_duplicates(
6191         self: T_DataArray,
6192         dim: Hashable | Iterable[Hashable],
6193         keep: Literal["first", "last", False] = "first",
6194     ) -> T_DataArray:
6195         """Returns a new DataArray with duplicate dimension values removed.
6196 
6197         Parameters
6198         ----------
6199         dim : dimension label or labels
6200             Pass `...` to drop duplicates along all dimensions.
6201         keep : {"first", "last", False}, default: "first"
6202             Determines which duplicates (if any) to keep.
6203 
6204             - ``"first"`` : Drop duplicates except for the first occurrence.
6205             - ``"last"`` : Drop duplicates except for the last occurrence.
6206             - False : Drop all duplicates.
6207 
6208         Returns
6209         -------
6210         DataArray
6211 
6212         See Also
6213         --------
6214         Dataset.drop_duplicates
6215 
6216         Examples
6217         --------
6218         >>> da = xr.DataArray(
6219         ...     np.arange(25).reshape(5, 5),
6220         ...     dims=("x", "y"),
6221         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6222         ... )
6223         >>> da
6224         <xarray.DataArray (x: 5, y: 5)>
6225         array([[ 0,  1,  2,  3,  4],
6226                [ 5,  6,  7,  8,  9],
6227                [10, 11, 12, 13, 14],
6228                [15, 16, 17, 18, 19],
6229                [20, 21, 22, 23, 24]])
6230         Coordinates:
6231           * x        (x) int64 0 0 1 2 3
6232           * y        (y) int64 0 1 2 3 3
6233 
6234         >>> da.drop_duplicates(dim="x")
6235         <xarray.DataArray (x: 4, y: 5)>
6236         array([[ 0,  1,  2,  3,  4],
6237                [10, 11, 12, 13, 14],
6238                [15, 16, 17, 18, 19],
6239                [20, 21, 22, 23, 24]])
6240         Coordinates:
6241           * x        (x) int64 0 1 2 3
6242           * y        (y) int64 0 1 2 3 3
6243 
6244         >>> da.drop_duplicates(dim="x", keep="last")
6245         <xarray.DataArray (x: 4, y: 5)>
6246         array([[ 5,  6,  7,  8,  9],
6247                [10, 11, 12, 13, 14],
6248                [15, 16, 17, 18, 19],
6249                [20, 21, 22, 23, 24]])
6250         Coordinates:
6251           * x        (x) int64 0 1 2 3
6252           * y        (y) int64 0 1 2 3 3
6253 
6254         Drop all duplicate dimension values:
6255 
6256         >>> da.drop_duplicates(dim=...)
6257         <xarray.DataArray (x: 4, y: 4)>
6258         array([[ 0,  1,  2,  3],
6259                [10, 11, 12, 13],
6260                [15, 16, 17, 18],
6261                [20, 21, 22, 23]])
6262         Coordinates:
6263           * x        (x) int64 0 1 2 3
6264           * y        (y) int64 0 1 2 3
6265         """
6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6267         return self._from_temp_dataset(deduplicated)
6268 
6269     def convert_calendar(
6270         self,
6271         calendar: str,
6272         dim: str = "time",
6273         align_on: str | None = None,
6274         missing: Any | None = None,
6275         use_cftime: bool | None = None,
6276     ) -> DataArray:
6277         """Convert the DataArray to another calendar.
6278 
6279         Only converts the individual timestamps, does not modify any data except
6280         in dropping invalid/surplus dates or inserting missing dates.
6281 
6282         If the source and target calendars are either no_leap, all_leap or a
6283         standard type, only the type of the time array is modified.
6284         When converting to a leap year from a non-leap year, the 29th of February
6285         is removed from the array. In the other direction the 29th of February
6286         will be missing in the output, unless `missing` is specified,
6287         in which case that value is inserted.
6288 
6289         For conversions involving `360_day` calendars, see Notes.
6290 
6291         This method is safe to use with sub-daily data as it doesn't touch the
6292         time part of the timestamps.
6293 
6294         Parameters
6295         ---------
6296         calendar : str
6297             The target calendar name.
6298         dim : str
6299             Name of the time coordinate.
6300         align_on : {None, 'date', 'year'}
6301             Must be specified when either source or target is a `360_day` calendar,
6302            ignored otherwise. See Notes.
6303         missing : Optional[any]
6304             By default, i.e. if the value is None, this method will simply attempt
6305             to convert the dates in the source calendar to the same dates in the
6306             target calendar, and drop any of those that are not possible to
6307             represent.  If a value is provided, a new time coordinate will be
6308             created in the target calendar with the same frequency as the original
6309             time coordinate; for any dates that are not present in the source, the
6310             data will be filled with this value.  Note that using this mode requires
6311             that the source data have an inferable frequency; for more information
6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6313             target calendar combinations, this could result in many missing values, see notes.
6314         use_cftime : boolean, optional
6315             Whether to use cftime objects in the output, only used if `calendar`
6316             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6317             If True, the new time axis uses cftime objects.
6318             If None (default), it uses :py:class:`numpy.datetime64` values if the
6319             date range permits it, and :py:class:`cftime.datetime` objects if not.
6320             If False, it uses :py:class:`numpy.datetime64`  or fails.
6321 
6322         Returns
6323         -------
6324         DataArray
6325             Copy of the dataarray with the time coordinate converted to the
6326             target calendar. If 'missing' was None (default), invalid dates in
6327             the new calendar are dropped, but missing dates are not inserted.
6328             If `missing` was given, the new data is reindexed to have a time axis
6329             with the same frequency as the source, but in the new calendar; any
6330             missing datapoints are filled with `missing`.
6331 
6332         Notes
6333         -----
6334         Passing a value to `missing` is only usable if the source's time coordinate as an
6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6336         if the target coordinate, generated from this frequency, has dates equivalent to the
6337         source. It is usually **not** appropriate to use this mode with:
6338 
6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6341             or 'mH' where 24 % m != 0).
6342 
6343         If one of the source or target calendars is `"360_day"`, `align_on` must
6344         be specified and two options are offered.
6345 
6346         - "year"
6347             The dates are translated according to their relative position in the year,
6348             ignoring their original month and day information, meaning that the
6349             missing/surplus days are added/removed at regular intervals.
6350 
6351             From a `360_day` to a standard calendar, the output will be missing the
6352             following dates (day of year in parentheses):
6353 
6354             To a leap year:
6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6356                 September 31st (275) and November 30th (335).
6357             To a non-leap year:
6358                 February 6th (36), April 19th (109), July 2nd (183),
6359                 September 12th (255), November 25th (329).
6360 
6361             From a standard calendar to a `"360_day"`, the following dates in the
6362             source array will be dropped:
6363 
6364             From a leap year:
6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6366                 September 31st (275), December 1st (336)
6367             From a non-leap year:
6368                 February 6th (37), April 20th (110), July 2nd (183),
6369                 September 13th (256), November 25th (329)
6370 
6371             This option is best used on daily and subdaily data.
6372 
6373         - "date"
6374             The month/day information is conserved and invalid dates are dropped
6375             from the output. This means that when converting from a `"360_day"` to a
6376             standard calendar, all 31st (Jan, March, May, July, August, October and
6377             December) will be missing as there is no equivalent dates in the
6378             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6379             will be dropped as there are no equivalent dates in a standard calendar.
6380 
6381             This option is best used with data on a frequency coarser than daily.
6382         """
6383         return convert_calendar(
6384             self,
6385             calendar,
6386             dim=dim,
6387             align_on=align_on,
6388             missing=missing,
6389             use_cftime=use_cftime,
6390         )
6391 
6392     def interp_calendar(
6393         self,
6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6395         dim: str = "time",
6396     ) -> DataArray:
6397         """Interpolates the DataArray to another calendar based on decimal year measure.
6398 
6399         Each timestamp in `source` and `target` are first converted to their decimal
6400         year equivalent then `source` is interpolated on the target coordinate.
6401         The decimal year of a timestamp is its year plus its sub-year component
6402         converted to the fraction of its year. For example "2000-03-01 12:00" is
6403         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6404 
6405         This method should only be used when the time (HH:MM:SS) information of
6406         time coordinate is not important.
6407 
6408         Parameters
6409         ----------
6410         target: DataArray or DatetimeIndex or CFTimeIndex
6411             The target time coordinate of a valid dtype
6412             (np.datetime64 or cftime objects)
6413         dim : str
6414             The time coordinate name.
6415 
6416         Return
6417         ------
6418         DataArray
6419             The source interpolated on the decimal years of target,
6420         """
6421         return interp_calendar(self, target, dim=dim)
6422 
6423     def groupby(
6424         self,
6425         group: Hashable | DataArray | IndexVariable,
6426         squeeze: bool = True,
6427         restore_coord_dims: bool = False,
6428     ) -> DataArrayGroupBy:
6429         """Returns a DataArrayGroupBy object for performing grouped operations.
6430 
6431         Parameters
6432         ----------
6433         group : Hashable, DataArray or IndexVariable
6434             Array whose unique values should be used to group this array. If a
6435             Hashable, must be the name of a coordinate contained in this dataarray.
6436         squeeze : bool, default: True
6437             If "group" is a dimension of any arrays in this dataset, `squeeze`
6438             controls whether the subarrays have a dimension of length 1 along
6439             that dimension or if the dimension is squeezed out.
6440         restore_coord_dims : bool, default: False
6441             If True, also restore the dimension order of multi-dimensional
6442             coordinates.
6443 
6444         Returns
6445         -------
6446         grouped : DataArrayGroupBy
6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6448             iterated over in the form of `(unique_value, grouped_array)` pairs.
6449 
6450         Examples
6451         --------
6452         Calculate daily anomalies for daily data:
6453 
6454         >>> da = xr.DataArray(
6455         ...     np.linspace(0, 1826, num=1827),
6456         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6457         ...     dims="time",
6458         ... )
6459         >>> da
6460         <xarray.DataArray (time: 1827)>
6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6462                1.826e+03])
6463         Coordinates:
6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6465         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6466         <xarray.DataArray (time: 1827)>
6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6468         Coordinates:
6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6471 
6472         See Also
6473         --------
6474         :ref:`groupby`
6475             Users guide explanation of how to group and bin data.
6476         DataArray.groupby_bins
6477         Dataset.groupby
6478         core.groupby.DataArrayGroupBy
6479         pandas.DataFrame.groupby
6480         """
6481         from xarray.core.groupby import (
6482             DataArrayGroupBy,
6483             ResolvedUniqueGrouper,
6484             UniqueGrouper,
6485             _validate_groupby_squeeze,
6486         )
6487 
6488         _validate_groupby_squeeze(squeeze)
6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
6490         return DataArrayGroupBy(
6491             self,
6492             (rgrouper,),
6493             squeeze=squeeze,
6494             restore_coord_dims=restore_coord_dims,
6495         )
6496 
6497     def groupby_bins(
6498         self,
6499         group: Hashable | DataArray | IndexVariable,
6500         bins: ArrayLike,
6501         right: bool = True,
6502         labels: ArrayLike | Literal[False] | None = None,
6503         precision: int = 3,
6504         include_lowest: bool = False,
6505         squeeze: bool = True,
6506         restore_coord_dims: bool = False,
6507     ) -> DataArrayGroupBy:
6508         """Returns a DataArrayGroupBy object for performing grouped operations.
6509 
6510         Rather than using all unique values of `group`, the values are discretized
6511         first by applying `pandas.cut` [1]_ to `group`.
6512 
6513         Parameters
6514         ----------
6515         group : Hashable, DataArray or IndexVariable
6516             Array whose binned values should be used to group this array. If a
6517             Hashable, must be the name of a coordinate contained in this dataarray.
6518         bins : int or array-like
6519             If bins is an int, it defines the number of equal-width bins in the
6520             range of x. However, in this case, the range of x is extended by .1%
6521             on each side to include the min or max values of x. If bins is a
6522             sequence it defines the bin edges allowing for non-uniform bin
6523             width. No extension of the range of x is done in this case.
6524         right : bool, default: True
6525             Indicates whether the bins include the rightmost edge or not. If
6526             right == True (the default), then the bins [1,2,3,4] indicate
6527             (1,2], (2,3], (3,4].
6528         labels : array-like, False or None, default: None
6529             Used as labels for the resulting bins. Must be of the same length as
6530             the resulting bins. If False, string bin labels are assigned by
6531             `pandas.cut`.
6532         precision : int, default: 3
6533             The precision at which to store and display the bins labels.
6534         include_lowest : bool, default: False
6535             Whether the first interval should be left-inclusive or not.
6536         squeeze : bool, default: True
6537             If "group" is a dimension of any arrays in this dataset, `squeeze`
6538             controls whether the subarrays have a dimension of length 1 along
6539             that dimension or if the dimension is squeezed out.
6540         restore_coord_dims : bool, default: False
6541             If True, also restore the dimension order of multi-dimensional
6542             coordinates.
6543 
6544         Returns
6545         -------
6546         grouped : DataArrayGroupBy
6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6548             iterated over in the form of `(unique_value, grouped_array)` pairs.
6549             The name of the group has the added suffix `_bins` in order to
6550             distinguish it from the original variable.
6551 
6552         See Also
6553         --------
6554         :ref:`groupby`
6555             Users guide explanation of how to group and bin data.
6556         DataArray.groupby
6557         Dataset.groupby_bins
6558         core.groupby.DataArrayGroupBy
6559         pandas.DataFrame.groupby
6560 
6561         References
6562         ----------
6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6564         """
6565         from xarray.core.groupby import (
6566             BinGrouper,
6567             DataArrayGroupBy,
6568             ResolvedBinGrouper,
6569             _validate_groupby_squeeze,
6570         )
6571 
6572         _validate_groupby_squeeze(squeeze)
6573         grouper = BinGrouper(
6574             bins=bins,
6575             cut_kwargs={
6576                 "right": right,
6577                 "labels": labels,
6578                 "precision": precision,
6579                 "include_lowest": include_lowest,
6580             },
6581         )
6582         rgrouper = ResolvedBinGrouper(grouper, group, self)
6583 
6584         return DataArrayGroupBy(
6585             self,
6586             (rgrouper,),
6587             squeeze=squeeze,
6588             restore_coord_dims=restore_coord_dims,
6589         )
6590 
6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6592         """
6593         Weighted DataArray operations.
6594 
6595         Parameters
6596         ----------
6597         weights : DataArray
6598             An array of weights associated with the values in this Dataset.
6599             Each value in the data contributes to the reduction operation
6600             according to its associated weight.
6601 
6602         Notes
6603         -----
6604         ``weights`` must be a DataArray and cannot contain missing values.
6605         Missing values can be replaced by ``weights.fillna(0)``.
6606 
6607         Returns
6608         -------
6609         core.weighted.DataArrayWeighted
6610 
6611         See Also
6612         --------
6613         Dataset.weighted
6614         """
6615         from xarray.core.weighted import DataArrayWeighted
6616 
6617         return DataArrayWeighted(self, weights)
6618 
6619     def rolling(
6620         self,
6621         dim: Mapping[Any, int] | None = None,
6622         min_periods: int | None = None,
6623         center: bool | Mapping[Any, bool] = False,
6624         **window_kwargs: int,
6625     ) -> DataArrayRolling:
6626         """
6627         Rolling window object for DataArrays.
6628 
6629         Parameters
6630         ----------
6631         dim : dict, optional
6632             Mapping from the dimension name to create the rolling iterator
6633             along (e.g. `time`) to its moving window size.
6634         min_periods : int or None, default: None
6635             Minimum number of observations in window required to have a value
6636             (otherwise result is NA). The default, None, is equivalent to
6637             setting min_periods equal to the size of the window.
6638         center : bool or Mapping to int, default: False
6639             Set the labels at the center of the window.
6640         **window_kwargs : optional
6641             The keyword arguments form of ``dim``.
6642             One of dim or window_kwargs must be provided.
6643 
6644         Returns
6645         -------
6646         core.rolling.DataArrayRolling
6647 
6648         Examples
6649         --------
6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6651 
6652         >>> da = xr.DataArray(
6653         ...     np.linspace(0, 11, num=12),
6654         ...     coords=[
6655         ...         pd.date_range(
6656         ...             "1999-12-15",
6657         ...             periods=12,
6658         ...             freq=pd.DateOffset(months=1),
6659         ...         )
6660         ...     ],
6661         ...     dims="time",
6662         ... )
6663         >>> da
6664         <xarray.DataArray (time: 12)>
6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6666         Coordinates:
6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6668         >>> da.rolling(time=3, center=True).mean()
6669         <xarray.DataArray (time: 12)>
6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6671         Coordinates:
6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6673 
6674         Remove the NaNs using ``dropna()``:
6675 
6676         >>> da.rolling(time=3, center=True).mean().dropna("time")
6677         <xarray.DataArray (time: 10)>
6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6679         Coordinates:
6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6681 
6682         See Also
6683         --------
6684         core.rolling.DataArrayRolling
6685         Dataset.rolling
6686         """
6687         from xarray.core.rolling import DataArrayRolling
6688 
6689         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6691 
6692     def coarsen(
6693         self,
6694         dim: Mapping[Any, int] | None = None,
6695         boundary: CoarsenBoundaryOptions = "exact",
6696         side: SideOptions | Mapping[Any, SideOptions] = "left",
6697         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6698         **window_kwargs: int,
6699     ) -> DataArrayCoarsen:
6700         """
6701         Coarsen object for DataArrays.
6702 
6703         Parameters
6704         ----------
6705         dim : mapping of hashable to int, optional
6706             Mapping from the dimension name to the window size.
6707         boundary : {"exact", "trim", "pad"}, default: "exact"
6708             If 'exact', a ValueError will be raised if dimension size is not a
6709             multiple of the window size. If 'trim', the excess entries are
6710             dropped. If 'pad', NA will be padded.
6711         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6712         coord_func : str or mapping of hashable to str, default: "mean"
6713             function (name) that is applied to the coordinates,
6714             or a mapping from coordinate name to function (name).
6715 
6716         Returns
6717         -------
6718         core.rolling.DataArrayCoarsen
6719 
6720         Examples
6721         --------
6722         Coarsen the long time series by averaging over every three days.
6723 
6724         >>> da = xr.DataArray(
6725         ...     np.linspace(0, 364, num=364),
6726         ...     dims="time",
6727         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6728         ... )
6729         >>> da  # +doctest: ELLIPSIS
6730         <xarray.DataArray (time: 364)>
6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6734         ...
6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6736                360.99173554, 361.99449036, 362.99724518, 364.        ])
6737         Coordinates:
6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6739         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6740         <xarray.DataArray (time: 121)>
6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6744         ...
6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6746                361.99449036])
6747         Coordinates:
6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6749         >>>
6750 
6751         See Also
6752         --------
6753         core.rolling.DataArrayCoarsen
6754         Dataset.coarsen
6755         """
6756         from xarray.core.rolling import DataArrayCoarsen
6757 
6758         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6759         return DataArrayCoarsen(
6760             self,
6761             dim,
6762             boundary=boundary,
6763             side=side,
6764             coord_func=coord_func,
6765         )
6766 
6767     def resample(
6768         self,
6769         indexer: Mapping[Any, str] | None = None,
6770         skipna: bool | None = None,
6771         closed: SideOptions | None = None,
6772         label: SideOptions | None = None,
6773         base: int | None = None,
6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6775         origin: str | DatetimeLike = "start_day",
6776         keep_attrs: bool | None = None,
6777         loffset: datetime.timedelta | str | None = None,
6778         restore_coord_dims: bool | None = None,
6779         **indexer_kwargs: str,
6780     ) -> DataArrayResample:
6781         """Returns a Resample object for performing resampling operations.
6782 
6783         Handles both downsampling and upsampling. The resampled
6784         dimension must be a datetime-like coordinate. If any intervals
6785         contain no values from the original object, they will be given
6786         the value ``NaN``.
6787 
6788         Parameters
6789         ----------
6790         indexer : Mapping of Hashable to str, optional
6791             Mapping from the dimension name to resample frequency [1]_. The
6792             dimension must be datetime-like.
6793         skipna : bool, optional
6794             Whether to skip missing values when aggregating in downsampling.
6795         closed : {"left", "right"}, optional
6796             Side of each interval to treat as closed.
6797         label : {"left", "right"}, optional
6798             Side of each interval to use for labeling.
6799         base : int, optional
6800             For frequencies that evenly subdivide 1 day, the "origin" of the
6801             aggregated intervals. For example, for "24H" frequency, base could
6802             range from 0 through 23.
6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6804             The datetime on which to adjust the grouping. The timezone of origin
6805             must match the timezone of the index.
6806 
6807             If a datetime is not used, these values are also supported:
6808             - 'epoch': `origin` is 1970-01-01
6809             - 'start': `origin` is the first value of the timeseries
6810             - 'start_day': `origin` is the first day at midnight of the timeseries
6811             - 'end': `origin` is the last value of the timeseries
6812             - 'end_day': `origin` is the ceiling midnight of the last day
6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6814             An offset timedelta added to the origin.
6815         loffset : timedelta or str, optional
6816             Offset used to adjust the resampled time labels. Some pandas date
6817             offset strings are supported.
6818         restore_coord_dims : bool, optional
6819             If True, also restore the dimension order of multi-dimensional
6820             coordinates.
6821         **indexer_kwargs : str
6822             The keyword arguments form of ``indexer``.
6823             One of indexer or indexer_kwargs must be provided.
6824 
6825         Returns
6826         -------
6827         resampled : core.resample.DataArrayResample
6828             This object resampled.
6829 
6830         Examples
6831         --------
6832         Downsample monthly time-series data to seasonal data:
6833 
6834         >>> da = xr.DataArray(
6835         ...     np.linspace(0, 11, num=12),
6836         ...     coords=[
6837         ...         pd.date_range(
6838         ...             "1999-12-15",
6839         ...             periods=12,
6840         ...             freq=pd.DateOffset(months=1),
6841         ...         )
6842         ...     ],
6843         ...     dims="time",
6844         ... )
6845         >>> da
6846         <xarray.DataArray (time: 12)>
6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6848         Coordinates:
6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6850         >>> da.resample(time="QS-DEC").mean()
6851         <xarray.DataArray (time: 4)>
6852         array([ 1.,  4.,  7., 10.])
6853         Coordinates:
6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6855 
6856         Upsample monthly time-series data to daily data:
6857 
6858         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6859         <xarray.DataArray (time: 337)>
6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6863         ...
6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6865                10.96774194, 11.        ])
6866         Coordinates:
6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6868 
6869         Limit scope of upsampling method
6870 
6871         >>> da.resample(time="1D").nearest(tolerance="1D")
6872         <xarray.DataArray (time: 337)>
6873         array([ 0.,  0., nan, ..., nan, 11., 11.])
6874         Coordinates:
6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6876 
6877         See Also
6878         --------
6879         Dataset.resample
6880         pandas.Series.resample
6881         pandas.DataFrame.resample
6882 
6883         References
6884         ----------
6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6886         """
6887         from xarray.core.resample import DataArrayResample
6888 
6889         return self._resample(
6890             resample_cls=DataArrayResample,
6891             indexer=indexer,
6892             skipna=skipna,
6893             closed=closed,
6894             label=label,
6895             base=base,
6896             offset=offset,
6897             origin=origin,
6898             keep_attrs=keep_attrs,
6899             loffset=loffset,
6900             restore_coord_dims=restore_coord_dims,
6901             **indexer_kwargs,
6902         )
6903 
6904     def to_dask_dataframe(
6905         self,
6906         dim_order: Sequence[Hashable] | None = None,
6907         set_index: bool = False,
6908     ) -> DaskDataFrame:
6909         """Convert this array into a dask.dataframe.DataFrame.
6910 
6911         Parameters
6912         ----------
6913         dim_order : Sequence of Hashable or None , optional
6914             Hierarchical dimension order for the resulting dataframe.
6915             Array content is transposed to this order and then written out as flat
6916             vectors in contiguous order, so the last dimension in this list
6917             will be contiguous in the resulting DataFrame. This has a major influence
6918             on which operations are efficient on the resulting dask dataframe.
6919         set_index : bool, default: False
6920             If set_index=True, the dask DataFrame is indexed by this dataset's
6921             coordinate. Since dask DataFrames do not support multi-indexes,
6922             set_index only works if the dataset only contains one dimension.
6923 
6924         Returns
6925         -------
6926         dask.dataframe.DataFrame
6927 
6928         Examples
6929         --------
6930         >>> da = xr.DataArray(
6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),
6932         ...     dims=("time", "lat", "lon"),
6933         ...     coords={
6934         ...         "time": np.arange(4),
6935         ...         "lat": [-30, -20],
6936         ...         "lon": [120, 130],
6937         ...     },
6938         ...     name="eg_dataarray",
6939         ...     attrs={"units": "Celsius", "description": "Random temperature data"},
6940         ... )
6941         >>> da.to_dask_dataframe(["lat", "lon", "time"]).compute()
6942             lat  lon  time  eg_dataarray
6943         0   -30  120     0             0
6944         1   -30  120     1             4
6945         2   -30  120     2             8
6946         3   -30  120     3            12
6947         4   -30  130     0             1
6948         5   -30  130     1             5
6949         6   -30  130     2             9
6950         7   -30  130     3            13
6951         8   -20  120     0             2
6952         9   -20  120     1             6
6953         10  -20  120     2            10
6954         11  -20  120     3            14
6955         12  -20  130     0             3
6956         13  -20  130     1             7
6957         14  -20  130     2            11
6958         15  -20  130     3            15
6959         """
6960         if self.name is None:
6961             raise ValueError(
6962                 "Cannot convert an unnamed DataArray to a "
6963                 "dask dataframe : use the ``.rename`` method to assign a name."
6964             )
6965         name = self.name
6966         ds = self._to_dataset_whole(name, shallow_copy=False)
6967         return ds.to_dask_dataframe(dim_order, set_index)
6968 
6969     # this needs to be at the end, or mypy will confuse with `str`
6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6971     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>compute</method>
<code>
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)

</code><intended_behavior>Similar to chunk, the compute method should be refactored to support computations through alternative backends. This requires abstracting away dask-specific call to a compute action that’s applicable to the selected backend, including a backend management system that handles the method calls to the appropriate library.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
229 class DataArray(
230     AbstractArray,
231     DataWithCoords,
232     DataArrayArithmetic,
233     DataArrayAggregations,
234 ):
235     """N-dimensional array with labeled coordinates and dimensions.
236 
237     DataArray provides a wrapper around numpy ndarrays that uses
238     labeled dimensions and coordinates to support metadata aware
239     operations. The API is similar to that for the pandas Series or
240     DataFrame, but DataArray objects can have any number of dimensions,
241     and their contents have fixed data types.
242 
243     Additional features over raw numpy arrays:
244 
245     - Apply operations over dimensions by name: ``x.sum('time')``.
246     - Select or assign values by integer location (like numpy):
247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
248       ``x.sel(time='2014-01-01')``.
249     - Mathematical operations (e.g., ``x - y``) vectorize across
250       multiple dimensions (known in numpy as "broadcasting") based on
251       dimension names, regardless of their original order.
252     - Keep track of arbitrary metadata in the form of a Python
253       dictionary: ``x.attrs``
254     - Convert to a pandas Series: ``x.to_series()``.
255 
256     Getting items from or doing mathematical operations with a
257     DataArray always returns another DataArray.
258 
259     Parameters
260     ----------
261     data : array_like
262         Values for this array. Must be an ``numpy.ndarray``, ndarray
263         like, or castable to an ``ndarray``. If a self-described xarray
264         or pandas object, attempts are made to use this array's
265         metadata to fill in other unspecified arguments. A view of the
266         array's data is used instead of a copy if possible.
267     coords : sequence or dict of array_like, optional
268         Coordinates (tick labels) to use for indexing along each
269         dimension. The following notations are accepted:
270 
271         - mapping {dimension name: array-like}
272         - sequence of tuples that are valid arguments for
273           ``xarray.Variable()``
274           - (dims, data)
275           - (dims, data, attrs)
276           - (dims, data, attrs, encoding)
277 
278         Additionally, it is possible to define a coord whose name
279         does not match the dimension name, or a coord based on multiple
280         dimensions, with one of the following notations:
281 
282         - mapping {coord name: DataArray}
283         - mapping {coord name: Variable}
284         - mapping {coord name: (dimension name, array-like)}
285         - mapping {coord name: (tuple of dimension names, array-like)}
286 
287     dims : Hashable or sequence of Hashable, optional
288         Name(s) of the data dimension(s). Must be either a Hashable
289         (only for 1D data) or a sequence of Hashables with length equal
290         to the number of dimensions. If this argument is omitted,
291         dimension names are taken from ``coords`` (if possible) and
292         otherwise default to ``['dim_0', ... 'dim_n']``.
293     name : str or None, optional
294         Name of this array.
295     attrs : dict_like or None, optional
296         Attributes to assign to the new instance. By default, an empty
297         attribute dictionary is initialized.
298 
299     Examples
300     --------
301     Create data:
302 
303     >>> np.random.seed(0)
304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
307     >>> time = pd.date_range("2014-09-06", periods=3)
308     >>> reference_time = pd.Timestamp("2014-09-05")
309 
310     Initialize a dataarray with multiple dimensions:
311 
312     >>> da = xr.DataArray(
313     ...     data=temperature,
314     ...     dims=["x", "y", "time"],
315     ...     coords=dict(
316     ...         lon=(["x", "y"], lon),
317     ...         lat=(["x", "y"], lat),
318     ...         time=time,
319     ...         reference_time=reference_time,
320     ...     ),
321     ...     attrs=dict(
322     ...         description="Ambient temperature.",
323     ...         units="degC",
324     ...     ),
325     ... )
326     >>> da
327     <xarray.DataArray (x: 2, y: 2, time: 3)>
328     array([[[29.11241877, 18.20125767, 22.82990387],
329             [32.92714559, 29.94046392,  7.18177696]],
330     <BLANKLINE>
331            [[22.60070734, 13.78914233, 14.17424919],
332             [18.28478802, 16.15234857, 26.63418806]]])
333     Coordinates:
334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
335         lat             (x, y) float64 42.25 42.21 42.63 42.59
336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
337         reference_time  datetime64[ns] 2014-09-05
338     Dimensions without coordinates: x, y
339     Attributes:
340         description:  Ambient temperature.
341         units:        degC
342 
343     Find out where the coldest temperature was:
344 
345     >>> da.isel(da.argmin(...))
346     <xarray.DataArray ()>
347     array(7.18177696)
348     Coordinates:
349         lon             float64 -99.32
350         lat             float64 42.21
351         time            datetime64[ns] 2014-09-08
352         reference_time  datetime64[ns] 2014-09-05
353     Attributes:
354         description:  Ambient temperature.
355         units:        degC
356     """
357 
358     _cache: dict[str, Any]
359     _coords: dict[Any, Variable]
360     _close: Callable[[], None] | None
361     _indexes: dict[Hashable, Index]
362     _name: Hashable | None
363     _variable: Variable
364 
365     __slots__ = (
366         "_cache",
367         "_coords",
368         "_close",
369         "_indexes",
370         "_name",
371         "_variable",
372         "__weakref__",
373     )
374 
375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
376 
377     def __init__(
378         self,
379         data: Any = dtypes.NA,
380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
381         | Mapping[Any, Any]
382         | None = None,
383         dims: Hashable | Sequence[Hashable] | None = None,
384         name: Hashable | None = None,
385         attrs: Mapping | None = None,
386         # internal parameters
387         indexes: dict[Hashable, Index] | None = None,
388         fastpath: bool = False,
389     ) -> None:
390         if fastpath:
391             variable = data
392             assert dims is None
393             assert attrs is None
394             assert indexes is not None
395         else:
396             # TODO: (benbovy - explicit indexes) remove
397             # once it becomes part of the public interface
398             if indexes is not None:
399                 raise ValueError("Providing explicit indexes is not supported yet")
400 
401             # try to fill in arguments from data if they weren't supplied
402             if coords is None:
403                 if isinstance(data, DataArray):
404                     coords = data.coords
405                 elif isinstance(data, pd.Series):
406                     coords = [data.index]
407                 elif isinstance(data, pd.DataFrame):
408                     coords = [data.index, data.columns]
409                 elif isinstance(data, (pd.Index, IndexVariable)):
410                     coords = [data]
411 
412             if dims is None:
413                 dims = getattr(data, "dims", getattr(coords, "dims", None))
414             if name is None:
415                 name = getattr(data, "name", None)
416             if attrs is None and not isinstance(data, PANDAS_TYPES):
417                 attrs = getattr(data, "attrs", None)
418 
419             data = _check_data_shape(data, coords, dims)
420             data = as_compatible_data(data)
421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
422             variable = Variable(dims, data, attrs, fastpath=True)
423             indexes, coords = _create_indexes_from_coords(coords)
424 
425         # These fully describe a DataArray
426         self._variable = variable
427         assert isinstance(coords, dict)
428         self._coords = coords
429         self._name = name
430 
431         # TODO(shoyer): document this argument, once it becomes part of the
432         # public interface.
433         self._indexes = indexes
434 
435         self._close = None
436 
437     @classmethod
438     def _construct_direct(
439         cls: type[T_DataArray],
440         variable: Variable,
441         coords: dict[Any, Variable],
442         name: Hashable,
443         indexes: dict[Hashable, Index],
444     ) -> T_DataArray:
445         """Shortcut around __init__ for internal use when we want to skip
446         costly validation
447         """
448         obj = object.__new__(cls)
449         obj._variable = variable
450         obj._coords = coords
451         obj._name = name
452         obj._indexes = indexes
453         obj._close = None
454         return obj
455 
456     def _replace(
457         self: T_DataArray,
458         variable: Variable | None = None,
459         coords=None,
460         name: Hashable | None | Default = _default,
461         indexes=None,
462     ) -> T_DataArray:
463         if variable is None:
464             variable = self.variable
465         if coords is None:
466             coords = self._coords
467         if indexes is None:
468             indexes = self._indexes
469         if name is _default:
470             name = self.name
471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
472 
473     def _replace_maybe_drop_dims(
474         self: T_DataArray,
475         variable: Variable,
476         name: Hashable | None | Default = _default,
477     ) -> T_DataArray:
478         if variable.dims == self.dims and variable.shape == self.shape:
479             coords = self._coords.copy()
480             indexes = self._indexes
481         elif variable.dims == self.dims:
482             # Shape has changed (e.g. from reduce(..., keepdims=True)
483             new_sizes = dict(zip(self.dims, variable.shape))
484             coords = {
485                 k: v
486                 for k, v in self._coords.items()
487                 if v.shape == tuple(new_sizes[d] for d in v.dims)
488             }
489             indexes = filter_indexes_from_coords(self._indexes, set(coords))
490         else:
491             allowed_dims = set(variable.dims)
492             coords = {
493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
494             }
495             indexes = filter_indexes_from_coords(self._indexes, set(coords))
496         return self._replace(variable, coords, name, indexes=indexes)
497 
498     def _overwrite_indexes(
499         self: T_DataArray,
500         indexes: Mapping[Any, Index],
501         coords: Mapping[Any, Variable] | None = None,
502         drop_coords: list[Hashable] | None = None,
503         rename_dims: Mapping[Any, Any] | None = None,
504     ) -> T_DataArray:
505         """Maybe replace indexes and their corresponding coordinates."""
506         if not indexes:
507             return self
508 
509         if coords is None:
510             coords = {}
511         if drop_coords is None:
512             drop_coords = []
513 
514         new_variable = self.variable.copy()
515         new_coords = self._coords.copy()
516         new_indexes = dict(self._indexes)
517 
518         for name in indexes:
519             new_coords[name] = coords[name]
520             new_indexes[name] = indexes[name]
521 
522         for name in drop_coords:
523             new_coords.pop(name)
524             new_indexes.pop(name)
525 
526         if rename_dims:
527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)
528 
529         return self._replace(
530             variable=new_variable, coords=new_coords, indexes=new_indexes
531         )
532 
533     def _to_temp_dataset(self) -> Dataset:
534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
535 
536     def _from_temp_dataset(
537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
538     ) -> T_DataArray:
539         variable = dataset._variables.pop(_THIS_ARRAY)
540         coords = dataset._variables
541         indexes = dataset._indexes
542         return self._replace(variable, coords, name, indexes=indexes)
543 
544     def _to_dataset_split(self, dim: Hashable) -> Dataset:
545         """splits dataarray along dimension 'dim'"""
546 
547         def subset(dim, label):
548             array = self.loc[{dim: label}]
549             array.attrs = {}
550             return as_variable(array)
551 
552         variables = {label: subset(dim, label) for label in self.get_index(dim)}
553         variables.update({k: v for k, v in self._coords.items() if k != dim})
554         coord_names = set(self._coords) - {dim}
555         indexes = filter_indexes_from_coords(self._indexes, coord_names)
556         dataset = Dataset._construct_direct(
557             variables, coord_names, indexes=indexes, attrs=self.attrs
558         )
559         return dataset
560 
561     def _to_dataset_whole(
562         self, name: Hashable = None, shallow_copy: bool = True
563     ) -> Dataset:
564         if name is None:
565             name = self.name
566         if name is None:
567             raise ValueError(
568                 "unable to convert unnamed DataArray to a "
569                 "Dataset without providing an explicit name"
570             )
571         if name in self.coords:
572             raise ValueError(
573                 "cannot create a Dataset from a DataArray with "
574                 "the same name as one of its coordinates"
575             )
576         # use private APIs for speed: this is called by _to_temp_dataset(),
577         # which is used in the guts of a lot of operations (e.g., reindex)
578         variables = self._coords.copy()
579         variables[name] = self.variable
580         if shallow_copy:
581             for k in variables:
582                 variables[k] = variables[k].copy(deep=False)
583         indexes = self._indexes
584 
585         coord_names = set(self._coords)
586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
587 
588     def to_dataset(
589         self,
590         dim: Hashable = None,
591         *,
592         name: Hashable = None,
593         promote_attrs: bool = False,
594     ) -> Dataset:
595         """Convert a DataArray to a Dataset.
596 
597         Parameters
598         ----------
599         dim : Hashable, optional
600             Name of the dimension on this array along which to split this array
601             into separate variables. If not provided, this array is converted
602             into a Dataset of one variable.
603         name : Hashable, optional
604             Name to substitute for this array's name. Only valid if ``dim`` is
605             not provided.
606         promote_attrs : bool, default: False
607             Set to True to shallow copy attrs of DataArray to returned Dataset.
608 
609         Returns
610         -------
611         dataset : Dataset
612         """
613         if dim is not None and dim not in self.dims:
614             raise TypeError(
615                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
616             )
617 
618         if dim is not None:
619             if name is not None:
620                 raise TypeError("cannot supply both dim and name arguments")
621             result = self._to_dataset_split(dim)
622         else:
623             result = self._to_dataset_whole(name)
624 
625         if promote_attrs:
626             result.attrs = dict(self.attrs)
627 
628         return result
629 
630     @property
631     def name(self) -> Hashable | None:
632         """The name of this array."""
633         return self._name
634 
635     @name.setter
636     def name(self, value: Hashable | None) -> None:
637         self._name = value
638 
639     @property
640     def variable(self) -> Variable:
641         """Low level interface to the Variable object for this DataArray."""
642         return self._variable
643 
644     @property
645     def dtype(self) -> np.dtype:
646         """
647         Data-type of the array’s elements.
648 
649         See Also
650         --------
651         ndarray.dtype
652         numpy.dtype
653         """
654         return self.variable.dtype
655 
656     @property
657     def shape(self) -> tuple[int, ...]:
658         """
659         Tuple of array dimensions.
660 
661         See Also
662         --------
663         numpy.ndarray.shape
664         """
665         return self.variable.shape
666 
667     @property
668     def size(self) -> int:
669         """
670         Number of elements in the array.
671 
672         Equal to ``np.prod(a.shape)``, i.e., the product of the array’s dimensions.
673 
674         See Also
675         --------
676         numpy.ndarray.size
677         """
678         return self.variable.size
679 
680     @property
681     def nbytes(self) -> int:
682         """
683         Total bytes consumed by the elements of this DataArray's data.
684 
685         If the underlying data array does not include ``nbytes``, estimates
686         the bytes consumed based on the ``size`` and ``dtype``.
687         """
688         return self.variable.nbytes
689 
690     @property
691     def ndim(self) -> int:
692         """
693         Number of array dimensions.
694 
695         See Also
696         --------
697         numpy.ndarray.ndim
698         """
699         return self.variable.ndim
700 
701     def __len__(self) -> int:
702         return len(self.variable)
703 
704     @property
705     def data(self) -> Any:
706         """
707         The DataArray's data as an array. The underlying array type
708         (e.g. dask, sparse, pint) is preserved.
709 
710         See Also
711         --------
712         DataArray.to_numpy
713         DataArray.as_numpy
714         DataArray.values
715         """
716         return self.variable.data
717 
718     @data.setter
719     def data(self, value: Any) -> None:
720         self.variable.data = value
721 
722     @property
723     def values(self) -> np.ndarray:
724         """
725         The array's data as a numpy.ndarray.
726 
727         If the array's data is not a numpy.ndarray this will attempt to convert
728         it naively using np.array(), which will raise an error if the array
729         type does not support coercion like this (e.g. cupy).
730         """
731         return self.variable.values
732 
733     @values.setter
734     def values(self, value: Any) -> None:
735         self.variable.values = value
736 
737     def to_numpy(self) -> np.ndarray:
738         """
739         Coerces wrapped data to numpy and returns a numpy.ndarray.
740 
741         See Also
742         --------
743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
744         Dataset.as_numpy
745         DataArray.values
746         DataArray.data
747         """
748         return self.variable.to_numpy()
749 
750     def as_numpy(self: T_DataArray) -> T_DataArray:
751         """
752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
753 
754         See Also
755         --------
756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
757         Dataset.as_numpy : Converts all variables in a Dataset.
758         DataArray.values
759         DataArray.data
760         """
761         coords = {k: v.as_numpy() for k, v in self._coords.items()}
762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
763 
764     @property
765     def _in_memory(self) -> bool:
766         return self.variable._in_memory
767 
768     def _to_index(self) -> pd.Index:
769         return self.variable._to_index()
770 
771     def to_index(self) -> pd.Index:
772         """Convert this variable to a pandas.Index. Only possible for 1D
773         arrays.
774         """
775         return self.variable.to_index()
776 
777     @property
778     def dims(self) -> tuple[Hashable, ...]:
779         """Tuple of dimension names associated with this array.
780 
781         Note that the type of this property is inconsistent with
782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
783         consistently named properties.
784 
785         See Also
786         --------
787         DataArray.sizes
788         Dataset.dims
789         """
790         return self.variable.dims
791 
792     @dims.setter
793     def dims(self, value: Any) -> NoReturn:
794         raise AttributeError(
795             "you cannot assign dims on a DataArray. Use "
796             ".rename() or .swap_dims() instead."
797         )
798 
799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
800         if utils.is_dict_like(key):
801             return key
802         key = indexing.expanded_indexer(key, self.ndim)
803         return dict(zip(self.dims, key))
804 
805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
806         from xarray.core.dataset import _get_virtual_variable
807 
808         try:
809             var = self._coords[key]
810         except KeyError:
811             dim_sizes = dict(zip(self.dims, self.shape))
812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
813 
814         return self._replace_maybe_drop_dims(var, name=key)
815 
816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
817         if isinstance(key, str):
818             return self._getitem_coord(key)
819         else:
820             # xarray-style array indexing
821             return self.isel(indexers=self._item_key_to_dict(key))
822 
823     def __setitem__(self, key: Any, value: Any) -> None:
824         if isinstance(key, str):
825             self.coords[key] = value
826         else:
827             # Coordinates in key, value and self[key] should be consistent.
828             # TODO Coordinate consistency in key is checked here, but it
829             # causes unnecessary indexing. It should be optimized.
830             obj = self[key]
831             if isinstance(value, DataArray):
832                 assert_coordinate_consistent(value, obj.coords.variables)
833             # DataArray key -> Variable key
834             key = {
835                 k: v.variable if isinstance(v, DataArray) else v
836                 for k, v in self._item_key_to_dict(key).items()
837             }
838             self.variable[key] = value
839 
840     def __delitem__(self, key: Any) -> None:
841         del self.coords[key]
842 
843     @property
844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
845         """Places to look-up items for attribute-style access"""
846         yield from self._item_sources
847         yield self.attrs
848 
849     @property
850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
851         """Places to look-up items for key-completion"""
852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
853 
854         # virtual coordinates
855         # uses empty dict -- everything here can already be found in self.coords.
856         yield HybridMappingProxy(keys=self.dims, mapping={})
857 
858     def __contains__(self, key: Any) -> bool:
859         return key in self.data
860 
861     @property
862     def loc(self) -> _LocIndexer:
863         """Attribute for location based indexing like pandas."""
864         return _LocIndexer(self)
865 
866     @property
867     def attrs(self) -> dict[Any, Any]:
868         """Dictionary storing arbitrary metadata with this array."""
869         return self.variable.attrs
870 
871     @attrs.setter
872     def attrs(self, value: Mapping[Any, Any]) -> None:
873         self.variable.attrs = dict(value)
874 
875     @property
876     def encoding(self) -> dict[Any, Any]:
877         """Dictionary of format-specific settings for how this array should be
878         serialized."""
879         return self.variable.encoding
880 
881     @encoding.setter
882     def encoding(self, value: Mapping[Any, Any]) -> None:
883         self.variable.encoding = dict(value)
884 
885     def reset_encoding(self: T_DataArray) -> T_DataArray:
886         """Return a new DataArray without encoding on the array or any attached
887         coords."""
888         ds = self._to_temp_dataset().reset_encoding()
889         return self._from_temp_dataset(ds)
890 
891     @property
892     def indexes(self) -> Indexes:
893         """Mapping of pandas.Index objects used for label based indexing.
894 
895         Raises an error if this Dataset has indexes that cannot be coerced
896         to pandas.Index objects.
897 
898         See Also
899         --------
900         DataArray.xindexes
901 
902         """
903         return self.xindexes.to_pandas_indexes()
904 
905     @property
906     def xindexes(self) -> Indexes:
907         """Mapping of xarray Index objects used for label based indexing."""
908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
909 
910     @property
911     def coords(self) -> DataArrayCoordinates:
912         """Dictionary-like container of coordinate arrays."""
913         return DataArrayCoordinates(self)
914 
915     @overload
916     def reset_coords(
917         self: T_DataArray,
918         names: Dims = None,
919         drop: Literal[False] = False,
920     ) -> Dataset:
921         ...
922 
923     @overload
924     def reset_coords(
925         self: T_DataArray,
926         names: Dims = None,
927         *,
928         drop: Literal[True],
929     ) -> T_DataArray:
930         ...
931 
932     def reset_coords(
933         self: T_DataArray,
934         names: Dims = None,
935         drop: bool = False,
936     ) -> T_DataArray | Dataset:
937         """Given names of coordinates, reset them to become variables.
938 
939         Parameters
940         ----------
941         names : str, Iterable of Hashable or None, optional
942             Name(s) of non-index coordinates in this dataset to reset into
943             variables. By default, all non-index coordinates are reset.
944         drop : bool, default: False
945             If True, remove coordinates instead of converting them into
946             variables.
947 
948         Returns
949         -------
950         Dataset, or DataArray if ``drop == True``
951 
952         Examples
953         --------
954         >>> temperature = np.arange(25).reshape(5, 5)
955         >>> pressure = np.arange(50, 75).reshape(5, 5)
956         >>> da = xr.DataArray(
957         ...     data=temperature,
958         ...     dims=["x", "y"],
959         ...     coords=dict(
960         ...         lon=("x", np.arange(10, 15)),
961         ...         lat=("y", np.arange(20, 25)),
962         ...         Pressure=(["x", "y"], pressure),
963         ...     ),
964         ...     name="Temperature",
965         ... )
966         >>> da
967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
968         array([[ 0,  1,  2,  3,  4],
969                [ 5,  6,  7,  8,  9],
970                [10, 11, 12, 13, 14],
971                [15, 16, 17, 18, 19],
972                [20, 21, 22, 23, 24]])
973         Coordinates:
974             lon       (x) int64 10 11 12 13 14
975             lat       (y) int64 20 21 22 23 24
976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74
977         Dimensions without coordinates: x, y
978 
979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:
980 
981         >>> da.reset_coords(names="Pressure")
982         <xarray.Dataset>
983         Dimensions:      (x: 5, y: 5)
984         Coordinates:
985             lon          (x) int64 10 11 12 13 14
986             lat          (y) int64 20 21 22 23 24
987         Dimensions without coordinates: x, y
988         Data variables:
989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74
990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24
991 
992         Return DataArray without targeted coordinate:
993 
994         >>> da.reset_coords(names="Pressure", drop=True)
995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>
996         array([[ 0,  1,  2,  3,  4],
997                [ 5,  6,  7,  8,  9],
998                [10, 11, 12, 13, 14],
999                [15, 16, 17, 18, 19],
1000                [20, 21, 22, 23, 24]])
1001         Coordinates:
1002             lon      (x) int64 10 11 12 13 14
1003             lat      (y) int64 20 21 22 23 24
1004         Dimensions without coordinates: x, y
1005         """
1006         if names is None:
1007             names = set(self.coords) - set(self._indexes)
1008         dataset = self.coords.to_dataset().reset_coords(names, drop)
1009         if drop:
1010             return self._replace(coords=dataset._variables)
1011         if self.name is None:
1012             raise ValueError(
1013                 "cannot reset_coords with drop=False on an unnamed DataArrray"
1014             )
1015         dataset[self.name] = self.variable
1016         return dataset
1017 
1018     def __dask_tokenize__(self):
1019         from dask.base import normalize_token
1020 
1021         return normalize_token((type(self), self._variable, self._coords, self._name))
1022 
1023     def __dask_graph__(self):
1024         return self._to_temp_dataset().__dask_graph__()
1025 
1026     def __dask_keys__(self):
1027         return self._to_temp_dataset().__dask_keys__()
1028 
1029     def __dask_layers__(self):
1030         return self._to_temp_dataset().__dask_layers__()
1031 
1032     @property
1033     def __dask_optimize__(self):
1034         return self._to_temp_dataset().__dask_optimize__
1035 
1036     @property
1037     def __dask_scheduler__(self):
1038         return self._to_temp_dataset().__dask_scheduler__
1039 
1040     def __dask_postcompute__(self):
1041         func, args = self._to_temp_dataset().__dask_postcompute__()
1042         return self._dask_finalize, (self.name, func) + args
1043 
1044     def __dask_postpersist__(self):
1045         func, args = self._to_temp_dataset().__dask_postpersist__()
1046         return self._dask_finalize, (self.name, func) + args
1047 
1048     @staticmethod
1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
1050         ds = func(results, *args, **kwargs)
1051         variable = ds._variables.pop(_THIS_ARRAY)
1052         coords = ds._variables
1053         indexes = ds._indexes
1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1055 
1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:
1057         """Manually trigger loading of this array's data from disk or a
1058         remote source into memory and return this array.
1059 
1060         Normally, it should not be necessary to call this method in user code,
1061         because all xarray functions should either work on deferred data or
1062         load data automatically. However, this method can be necessary when
1063         working with many file objects on disk.
1064 
1065         Parameters
1066         ----------
1067         **kwargs : dict
1068             Additional keyword arguments passed on to ``dask.compute``.
1069 
1070         See Also
1071         --------
1072         dask.compute
1073         """
1074         ds = self._to_temp_dataset().load(**kwargs)
1075         new = self._from_temp_dataset(ds)
1076         self._variable = new._variable
1077         self._coords = new._coords
1078         return self
1079 
1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
1081         """Manually trigger loading of this array's data from disk or a
1082         remote source into memory and return a new array. The original is
1083         left unaltered.
1084 
1085         Normally, it should not be necessary to call this method in user code,
1086         because all xarray functions should either work on deferred data or
1087         load data automatically. However, this method can be necessary when
1088         working with many file objects on disk.
1089 
1090         Parameters
1091         ----------
1092         **kwargs : dict
1093             Additional keyword arguments passed on to ``dask.compute``.
1094 
1095         See Also
1096         --------
1097         dask.compute
1098         """
1099         new = self.copy(deep=False)
1100         return new.load(**kwargs)
1101 
1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1103         """Trigger computation in constituent dask arrays
1104 
1105         This keeps them as dask arrays but encourages them to keep data in
1106         memory.  This is particularly useful when on a distributed machine.
1107         When on a single machine consider using ``.compute()`` instead.
1108 
1109         Parameters
1110         ----------
1111         **kwargs : dict
1112             Additional keyword arguments passed on to ``dask.persist``.
1113 
1114         See Also
1115         --------
1116         dask.persist
1117         """
1118         ds = self._to_temp_dataset().persist(**kwargs)
1119         return self._from_temp_dataset(ds)
1120 
1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1122         """Returns a copy of this array.
1123 
1124         If `deep=True`, a deep copy is made of the data array.
1125         Otherwise, a shallow copy is made, and the returned data array's
1126         values are a new view of this data array's values.
1127 
1128         Use `data` to create a new object with the same structure as
1129         original but entirely new data.
1130 
1131         Parameters
1132         ----------
1133         deep : bool, optional
1134             Whether the data array and its coordinates are loaded into memory
1135             and copied onto the new object. Default is True.
1136         data : array_like, optional
1137             Data to use in the new object. Must have same shape as original.
1138             When `data` is used, `deep` is ignored for all data variables,
1139             and only used for coords.
1140 
1141         Returns
1142         -------
1143         copy : DataArray
1144             New object with dimensions, attributes, coordinates, name,
1145             encoding, and optionally data copied from original.
1146 
1147         Examples
1148         --------
1149         Shallow versus deep copy
1150 
1151         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1152         >>> array.copy()
1153         <xarray.DataArray (x: 3)>
1154         array([1, 2, 3])
1155         Coordinates:
1156           * x        (x) <U1 'a' 'b' 'c'
1157         >>> array_0 = array.copy(deep=False)
1158         >>> array_0[0] = 7
1159         >>> array_0
1160         <xarray.DataArray (x: 3)>
1161         array([7, 2, 3])
1162         Coordinates:
1163           * x        (x) <U1 'a' 'b' 'c'
1164         >>> array
1165         <xarray.DataArray (x: 3)>
1166         array([7, 2, 3])
1167         Coordinates:
1168           * x        (x) <U1 'a' 'b' 'c'
1169 
1170         Changing the data using the ``data`` argument maintains the
1171         structure of the original object, but with the new data. Original
1172         object is unaffected.
1173 
1174         >>> array.copy(data=[0.1, 0.2, 0.3])
1175         <xarray.DataArray (x: 3)>
1176         array([0.1, 0.2, 0.3])
1177         Coordinates:
1178           * x        (x) <U1 'a' 'b' 'c'
1179         >>> array
1180         <xarray.DataArray (x: 3)>
1181         array([7, 2, 3])
1182         Coordinates:
1183           * x        (x) <U1 'a' 'b' 'c'
1184 
1185         See Also
1186         --------
1187         pandas.DataFrame.copy
1188         """
1189         return self._copy(deep=deep, data=data)
1190 
1191     def _copy(
1192         self: T_DataArray,
1193         deep: bool = True,
1194         data: Any = None,
1195         memo: dict[int, Any] | None = None,
1196     ) -> T_DataArray:
1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)
1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1199 
1200         coords = {}
1201         for k, v in self._coords.items():
1202             if k in index_vars:
1203                 coords[k] = index_vars[k]
1204             else:
1205                 coords[k] = v._copy(deep=deep, memo=memo)
1206 
1207         return self._replace(variable, coords, indexes=indexes)
1208 
1209     def __copy__(self: T_DataArray) -> T_DataArray:
1210         return self._copy(deep=False)
1211 
1212     def __deepcopy__(
1213         self: T_DataArray, memo: dict[int, Any] | None = None
1214     ) -> T_DataArray:
1215         return self._copy(deep=True, memo=memo)
1216 
1217     # mutable objects should not be Hashable
1218     # https://github.com/python/mypy/issues/4266
1219     __hash__ = None  # type: ignore[assignment]
1220 
1221     @property
1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1223         """
1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1225         the underlying data is not a dask array.
1226 
1227         See Also
1228         --------
1229         DataArray.chunk
1230         DataArray.chunksizes
1231         xarray.unify_chunks
1232         """
1233         return self.variable.chunks
1234 
1235     @property
1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1237         """
1238         Mapping from dimension names to block lengths for this dataarray's data, or None if
1239         the underlying data is not a dask array.
1240         Cannot be modified directly, but can be modified by calling .chunk().
1241 
1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1243         instead of a tuple of chunk shapes.
1244 
1245         See Also
1246         --------
1247         DataArray.chunk
1248         DataArray.chunks
1249         xarray.unify_chunks
1250         """
1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1252         return get_chunksizes(all_variables)
1253 
1254     def chunk(
1255         self: T_DataArray,
1256         chunks: (
1257             int
1258             | Literal["auto"]
1259             | tuple[int, ...]
1260             | tuple[tuple[int, ...], ...]
1261             | Mapping[Any, None | int | tuple[int, ...]]
1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1263         name_prefix: str = "xarray-",
1264         token: str | None = None,
1265         lock: bool = False,
1266         inline_array: bool = False,
1267         **chunks_kwargs: Any,
1268     ) -> T_DataArray:
1269         """Coerce this array's data into a dask arrays with the given chunks.
1270 
1271         If this variable is a non-dask array, it will be converted to dask
1272         array. If it's a dask array, it will be rechunked to the given chunk
1273         sizes.
1274 
1275         If neither chunks is not provided for one or more dimensions, chunk
1276         sizes along that dimension will not be updated; non-dask arrays will be
1277         converted into dask arrays with a single block.
1278 
1279         Parameters
1280         ----------
1281         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1282             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1283             ``{"x": 5, "y": 5}``.
1284         name_prefix : str, optional
1285             Prefix for the name of the new dask array.
1286         token : str, optional
1287             Token uniquely identifying this array.
1288         lock : optional
1289             Passed on to :py:func:`dask.array.from_array`, if the array is not
1290             already as dask array.
1291         inline_array: optional
1292             Passed on to :py:func:`dask.array.from_array`, if the array is not
1293             already as dask array.
1294         **chunks_kwargs : {dim: chunks, ...}, optional
1295             The keyword arguments form of ``chunks``.
1296             One of chunks or chunks_kwargs must be provided.
1297 
1298         Returns
1299         -------
1300         chunked : xarray.DataArray
1301 
1302         See Also
1303         --------
1304         DataArray.chunks
1305         DataArray.chunksizes
1306         xarray.unify_chunks
1307         dask.array.from_array
1308         """
1309         if chunks is None:
1310             warnings.warn(
1311                 "None value for 'chunks' is deprecated. "
1312                 "It will raise an error in the future. Use instead '{}'",
1313                 category=FutureWarning,
1314             )
1315             chunks = {}
1316 
1317         if isinstance(chunks, (float, str, int)):
1318             # ignoring type; unclear why it won't accept a Literal into the value.
1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1320         elif isinstance(chunks, (tuple, list)):
1321             chunks = dict(zip(self.dims, chunks))
1322         else:
1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1324 
1325         ds = self._to_temp_dataset().chunk(
1326             chunks,
1327             name_prefix=name_prefix,
1328             token=token,
1329             lock=lock,
1330             inline_array=inline_array,
1331         )
1332         return self._from_temp_dataset(ds)
1333 
1334     def isel(
1335         self: T_DataArray,
1336         indexers: Mapping[Any, Any] | None = None,
1337         drop: bool = False,
1338         missing_dims: ErrorOptionsWithWarn = "raise",
1339         **indexers_kwargs: Any,
1340     ) -> T_DataArray:
1341         """Return a new DataArray whose data is given by selecting indexes
1342         along the specified dimension(s).
1343 
1344         Parameters
1345         ----------
1346         indexers : dict, optional
1347             A dict with keys matching dimensions and values given
1348             by integers, slice objects or arrays.
1349             indexer can be a integer, slice, array-like or DataArray.
1350             If DataArrays are passed as indexers, xarray-style indexing will be
1351             carried out. See :ref:`indexing` for the details.
1352             One of indexers or indexers_kwargs must be provided.
1353         drop : bool, default: False
1354             If ``drop=True``, drop coordinates variables indexed by integers
1355             instead of making them scalar.
1356         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1357             What to do if dimensions that should be selected from are not present in the
1358             DataArray:
1359             - "raise": raise an exception
1360             - "warn": raise a warning, and ignore the missing dimensions
1361             - "ignore": ignore the missing dimensions
1362         **indexers_kwargs : {dim: indexer, ...}, optional
1363             The keyword arguments form of ``indexers``.
1364 
1365         Returns
1366         -------
1367         indexed : xarray.DataArray
1368 
1369         See Also
1370         --------
1371         Dataset.isel
1372         DataArray.sel
1373 
1374         Examples
1375         --------
1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1377         >>> da
1378         <xarray.DataArray (x: 5, y: 5)>
1379         array([[ 0,  1,  2,  3,  4],
1380                [ 5,  6,  7,  8,  9],
1381                [10, 11, 12, 13, 14],
1382                [15, 16, 17, 18, 19],
1383                [20, 21, 22, 23, 24]])
1384         Dimensions without coordinates: x, y
1385 
1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1388         >>> da = da.isel(x=tgt_x, y=tgt_y)
1389         >>> da
1390         <xarray.DataArray (points: 5)>
1391         array([ 0,  6, 12, 18, 24])
1392         Dimensions without coordinates: points
1393         """
1394 
1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1396 
1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1398             ds = self._to_temp_dataset()._isel_fancy(
1399                 indexers, drop=drop, missing_dims=missing_dims
1400             )
1401             return self._from_temp_dataset(ds)
1402 
1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1404         # lists, or zero or one-dimensional np.ndarray's
1405 
1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1408 
1409         coords = {}
1410         for coord_name, coord_value in self._coords.items():
1411             if coord_name in index_variables:
1412                 coord_value = index_variables[coord_name]
1413             else:
1414                 coord_indexers = {
1415                     k: v for k, v in indexers.items() if k in coord_value.dims
1416                 }
1417                 if coord_indexers:
1418                     coord_value = coord_value.isel(coord_indexers)
1419                     if drop and coord_value.ndim == 0:
1420                         continue
1421             coords[coord_name] = coord_value
1422 
1423         return self._replace(variable=variable, coords=coords, indexes=indexes)
1424 
1425     def sel(
1426         self: T_DataArray,
1427         indexers: Mapping[Any, Any] | None = None,
1428         method: str | None = None,
1429         tolerance=None,
1430         drop: bool = False,
1431         **indexers_kwargs: Any,
1432     ) -> T_DataArray:
1433         """Return a new DataArray whose data is given by selecting index
1434         labels along the specified dimension(s).
1435 
1436         In contrast to `DataArray.isel`, indexers for this method should use
1437         labels instead of integers.
1438 
1439         Under the hood, this method is powered by using pandas's powerful Index
1440         objects. This makes label based indexing essentially just as fast as
1441         using integer indexing.
1442 
1443         It also means this method uses pandas's (well documented) logic for
1444         indexing. This means you can use string shortcuts for datetime indexes
1445         (e.g., '2000-01' to select all values in January 2000). It also means
1446         that slices are treated as inclusive of both the start and stop values,
1447         unlike normal Python indexing.
1448 
1449         .. warning::
1450 
1451           Do not try to assign values when using any of the indexing methods
1452           ``isel`` or ``sel``::
1453 
1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1455             # DO NOT do this
1456             da.isel(x=[0, 1, 2])[1] = -1
1457 
1458           Assigning values with the chained indexing using ``.sel`` or
1459           ``.isel`` fails silently.
1460 
1461         Parameters
1462         ----------
1463         indexers : dict, optional
1464             A dict with keys matching dimensions and values given
1465             by scalars, slices or arrays of tick labels. For dimensions with
1466             multi-index, the indexer may also be a dict-like object with keys
1467             matching index level names.
1468             If DataArrays are passed as indexers, xarray-style indexing will be
1469             carried out. See :ref:`indexing` for the details.
1470             One of indexers or indexers_kwargs must be provided.
1471         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1472             Method to use for inexact matches:
1473 
1474             - None (default): only exact matches
1475             - pad / ffill: propagate last valid index value forward
1476             - backfill / bfill: propagate next valid index value backward
1477             - nearest: use nearest valid index value
1478 
1479         tolerance : optional
1480             Maximum distance between original and new labels for inexact
1481             matches. The values of the index at the matching locations must
1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1483         drop : bool, optional
1484             If ``drop=True``, drop coordinates variables in `indexers` instead
1485             of making them scalar.
1486         **indexers_kwargs : {dim: indexer, ...}, optional
1487             The keyword arguments form of ``indexers``.
1488             One of indexers or indexers_kwargs must be provided.
1489 
1490         Returns
1491         -------
1492         obj : DataArray
1493             A new DataArray with the same contents as this DataArray, except the
1494             data and each dimension is indexed by the appropriate indexers.
1495             If indexer DataArrays have coordinates that do not conflict with
1496             this object, then these coordinates will be attached.
1497             In general, each array's data will be a view of the array's data
1498             in this DataArray, unless vectorized indexing was triggered by using
1499             an array indexer, in which case the data will be a copy.
1500 
1501         See Also
1502         --------
1503         Dataset.sel
1504         DataArray.isel
1505 
1506         Examples
1507         --------
1508         >>> da = xr.DataArray(
1509         ...     np.arange(25).reshape(5, 5),
1510         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1511         ...     dims=("x", "y"),
1512         ... )
1513         >>> da
1514         <xarray.DataArray (x: 5, y: 5)>
1515         array([[ 0,  1,  2,  3,  4],
1516                [ 5,  6,  7,  8,  9],
1517                [10, 11, 12, 13, 14],
1518                [15, 16, 17, 18, 19],
1519                [20, 21, 22, 23, 24]])
1520         Coordinates:
1521           * x        (x) int64 0 1 2 3 4
1522           * y        (y) int64 0 1 2 3 4
1523 
1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1527         >>> da
1528         <xarray.DataArray (points: 5)>
1529         array([ 0,  6, 12, 18, 24])
1530         Coordinates:
1531             x        (points) int64 0 1 2 3 4
1532             y        (points) int64 0 1 2 3 4
1533         Dimensions without coordinates: points
1534         """
1535         ds = self._to_temp_dataset().sel(
1536             indexers=indexers,
1537             drop=drop,
1538             method=method,
1539             tolerance=tolerance,
1540             **indexers_kwargs,
1541         )
1542         return self._from_temp_dataset(ds)
1543 
1544     def head(
1545         self: T_DataArray,
1546         indexers: Mapping[Any, int] | int | None = None,
1547         **indexers_kwargs: Any,
1548     ) -> T_DataArray:
1549         """Return a new DataArray whose data is given by the the first `n`
1550         values along the specified dimension(s). Default `n` = 5
1551 
1552         See Also
1553         --------
1554         Dataset.head
1555         DataArray.tail
1556         DataArray.thin
1557 
1558         Examples
1559         --------
1560         >>> da = xr.DataArray(
1561         ...     np.arange(25).reshape(5, 5),
1562         ...     dims=("x", "y"),
1563         ... )
1564         >>> da
1565         <xarray.DataArray (x: 5, y: 5)>
1566         array([[ 0,  1,  2,  3,  4],
1567                [ 5,  6,  7,  8,  9],
1568                [10, 11, 12, 13, 14],
1569                [15, 16, 17, 18, 19],
1570                [20, 21, 22, 23, 24]])
1571         Dimensions without coordinates: x, y
1572 
1573         >>> da.head(x=1)
1574         <xarray.DataArray (x: 1, y: 5)>
1575         array([[0, 1, 2, 3, 4]])
1576         Dimensions without coordinates: x, y
1577 
1578         >>> da.head({"x": 2, "y": 2})
1579         <xarray.DataArray (x: 2, y: 2)>
1580         array([[0, 1],
1581                [5, 6]])
1582         Dimensions without coordinates: x, y
1583         """
1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1585         return self._from_temp_dataset(ds)
1586 
1587     def tail(
1588         self: T_DataArray,
1589         indexers: Mapping[Any, int] | int | None = None,
1590         **indexers_kwargs: Any,
1591     ) -> T_DataArray:
1592         """Return a new DataArray whose data is given by the the last `n`
1593         values along the specified dimension(s). Default `n` = 5
1594 
1595         See Also
1596         --------
1597         Dataset.tail
1598         DataArray.head
1599         DataArray.thin
1600 
1601         Examples
1602         --------
1603         >>> da = xr.DataArray(
1604         ...     np.arange(25).reshape(5, 5),
1605         ...     dims=("x", "y"),
1606         ... )
1607         >>> da
1608         <xarray.DataArray (x: 5, y: 5)>
1609         array([[ 0,  1,  2,  3,  4],
1610                [ 5,  6,  7,  8,  9],
1611                [10, 11, 12, 13, 14],
1612                [15, 16, 17, 18, 19],
1613                [20, 21, 22, 23, 24]])
1614         Dimensions without coordinates: x, y
1615 
1616         >>> da.tail(y=1)
1617         <xarray.DataArray (x: 5, y: 1)>
1618         array([[ 4],
1619                [ 9],
1620                [14],
1621                [19],
1622                [24]])
1623         Dimensions without coordinates: x, y
1624 
1625         >>> da.tail({"x": 2, "y": 2})
1626         <xarray.DataArray (x: 2, y: 2)>
1627         array([[18, 19],
1628                [23, 24]])
1629         Dimensions without coordinates: x, y
1630         """
1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1632         return self._from_temp_dataset(ds)
1633 
1634     def thin(
1635         self: T_DataArray,
1636         indexers: Mapping[Any, int] | int | None = None,
1637         **indexers_kwargs: Any,
1638     ) -> T_DataArray:
1639         """Return a new DataArray whose data is given by each `n` value
1640         along the specified dimension(s).
1641 
1642         Examples
1643         --------
1644         >>> x_arr = np.arange(0, 26)
1645         >>> x_arr
1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1647                17, 18, 19, 20, 21, 22, 23, 24, 25])
1648         >>> x = xr.DataArray(
1649         ...     np.reshape(x_arr, (2, 13)),
1650         ...     dims=("x", "y"),
1651         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1652         ... )
1653         >>> x
1654         <xarray.DataArray (x: 2, y: 13)>
1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1657         Coordinates:
1658           * x        (x) int64 0 1
1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1660 
1661         >>>
1662         >>> x.thin(3)
1663         <xarray.DataArray (x: 1, y: 5)>
1664         array([[ 0,  3,  6,  9, 12]])
1665         Coordinates:
1666           * x        (x) int64 0
1667           * y        (y) int64 0 3 6 9 12
1668         >>> x.thin({"x": 2, "y": 5})
1669         <xarray.DataArray (x: 1, y: 3)>
1670         array([[ 0,  5, 10]])
1671         Coordinates:
1672           * x        (x) int64 0
1673           * y        (y) int64 0 5 10
1674 
1675         See Also
1676         --------
1677         Dataset.thin
1678         DataArray.head
1679         DataArray.tail
1680         """
1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1682         return self._from_temp_dataset(ds)
1683 
1684     def broadcast_like(
1685         self: T_DataArray,
1686         other: DataArray | Dataset,
1687         exclude: Iterable[Hashable] | None = None,
1688     ) -> T_DataArray:
1689         """Broadcast this DataArray against another Dataset or DataArray.
1690 
1691         This is equivalent to xr.broadcast(other, self)[1]
1692 
1693         xarray objects are broadcast against each other in arithmetic
1694         operations, so this method is not be necessary for most uses.
1695 
1696         If no change is needed, the input data is returned to the output
1697         without being copied.
1698 
1699         If new coords are added by the broadcast, their values are
1700         NaN filled.
1701 
1702         Parameters
1703         ----------
1704         other : Dataset or DataArray
1705             Object against which to broadcast this array.
1706         exclude : iterable of Hashable, optional
1707             Dimensions that must not be broadcasted
1708 
1709         Returns
1710         -------
1711         new_da : DataArray
1712             The caller broadcasted against ``other``.
1713 
1714         Examples
1715         --------
1716         >>> arr1 = xr.DataArray(
1717         ...     np.random.randn(2, 3),
1718         ...     dims=("x", "y"),
1719         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1720         ... )
1721         >>> arr2 = xr.DataArray(
1722         ...     np.random.randn(3, 2),
1723         ...     dims=("x", "y"),
1724         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1725         ... )
1726         >>> arr1
1727         <xarray.DataArray (x: 2, y: 3)>
1728         array([[ 1.76405235,  0.40015721,  0.97873798],
1729                [ 2.2408932 ,  1.86755799, -0.97727788]])
1730         Coordinates:
1731           * x        (x) <U1 'a' 'b'
1732           * y        (y) <U1 'a' 'b' 'c'
1733         >>> arr2
1734         <xarray.DataArray (x: 3, y: 2)>
1735         array([[ 0.95008842, -0.15135721],
1736                [-0.10321885,  0.4105985 ],
1737                [ 0.14404357,  1.45427351]])
1738         Coordinates:
1739           * x        (x) <U1 'a' 'b' 'c'
1740           * y        (y) <U1 'a' 'b'
1741         >>> arr1.broadcast_like(arr2)
1742         <xarray.DataArray (x: 3, y: 3)>
1743         array([[ 1.76405235,  0.40015721,  0.97873798],
1744                [ 2.2408932 ,  1.86755799, -0.97727788],
1745                [        nan,         nan,         nan]])
1746         Coordinates:
1747           * x        (x) <U1 'a' 'b' 'c'
1748           * y        (y) <U1 'a' 'b' 'c'
1749         """
1750         if exclude is None:
1751             exclude = set()
1752         else:
1753             exclude = set(exclude)
1754         args = align(other, self, join="outer", copy=False, exclude=exclude)
1755 
1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1757 
1758         return _broadcast_helper(
1759             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1760         )
1761 
1762     def _reindex_callback(
1763         self: T_DataArray,
1764         aligner: alignment.Aligner,
1765         dim_pos_indexers: dict[Hashable, Any],
1766         variables: dict[Hashable, Variable],
1767         indexes: dict[Hashable, Index],
1768         fill_value: Any,
1769         exclude_dims: frozenset[Hashable],
1770         exclude_vars: frozenset[Hashable],
1771     ) -> T_DataArray:
1772         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1773 
1774         if isinstance(fill_value, dict):
1775             fill_value = fill_value.copy()
1776             sentinel = object()
1777             value = fill_value.pop(self.name, sentinel)
1778             if value is not sentinel:
1779                 fill_value[_THIS_ARRAY] = value
1780 
1781         ds = self._to_temp_dataset()
1782         reindexed = ds._reindex_callback(
1783             aligner,
1784             dim_pos_indexers,
1785             variables,
1786             indexes,
1787             fill_value,
1788             exclude_dims,
1789             exclude_vars,
1790         )
1791         return self._from_temp_dataset(reindexed)
1792 
1793     def reindex_like(
1794         self: T_DataArray,
1795         other: DataArray | Dataset,
1796         method: ReindexMethodOptions = None,
1797         tolerance: int | float | Iterable[int | float] | None = None,
1798         copy: bool = True,
1799         fill_value=dtypes.NA,
1800     ) -> T_DataArray:
1801         """Conform this object onto the indexes of another object, filling in
1802         missing values with ``fill_value``. The default fill value is NaN.
1803 
1804         Parameters
1805         ----------
1806         other : Dataset or DataArray
1807             Object with an 'indexes' attribute giving a mapping from dimension
1808             names to pandas.Index objects, which provides coordinates upon
1809             which to index the variables in this dataset. The indexes on this
1810             other object need not be the same as the indexes on this
1811             dataset. Any mis-matched index values will be filled in with
1812             NaN, and any mis-matched dimension names will simply be ignored.
1813         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1814             Method to use for filling index values from other not found on this
1815             data array:
1816 
1817             - None (default): don't fill gaps
1818             - pad / ffill: propagate last valid index value forward
1819             - backfill / bfill: propagate next valid index value backward
1820             - nearest: use nearest valid index value
1821 
1822         tolerance : optional
1823             Maximum distance between original and new labels for inexact
1824             matches. The values of the index at the matching locations must
1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1826             Tolerance may be a scalar value, which applies the same tolerance
1827             to all values, or list-like, which applies variable tolerance per
1828             element. List-like must be the same size as the index and its dtype
1829             must exactly match the index’s type.
1830         copy : bool, default: True
1831             If ``copy=True``, data in the return value is always copied. If
1832             ``copy=False`` and reindexing is unnecessary, or can be performed
1833             with only slice operations, then the output may share memory with
1834             the input. In either case, a new xarray object is always returned.
1835         fill_value : scalar or dict-like, optional
1836             Value to use for newly missing values. If a dict-like, maps
1837             variable names (including coordinates) to fill values. Use this
1838             data array's name to refer to the data array's values.
1839 
1840         Returns
1841         -------
1842         reindexed : DataArray
1843             Another dataset array, with this array's data but coordinates from
1844             the other object.
1845 
1846         Examples
1847         --------
1848         >>> data = np.arange(12).reshape(4, 3)
1849         >>> da1 = xr.DataArray(
1850         ...     data=data,
1851         ...     dims=["x", "y"],
1852         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1853         ... )
1854         >>> da1
1855         <xarray.DataArray (x: 4, y: 3)>
1856         array([[ 0,  1,  2],
1857                [ 3,  4,  5],
1858                [ 6,  7,  8],
1859                [ 9, 10, 11]])
1860         Coordinates:
1861           * x        (x) int64 10 20 30 40
1862           * y        (y) int64 70 80 90
1863         >>> da2 = xr.DataArray(
1864         ...     data=data,
1865         ...     dims=["x", "y"],
1866         ...     coords={"x": [40, 30, 20, 10], "y": [90, 80, 70]},
1867         ... )
1868         >>> da2
1869         <xarray.DataArray (x: 4, y: 3)>
1870         array([[ 0,  1,  2],
1871                [ 3,  4,  5],
1872                [ 6,  7,  8],
1873                [ 9, 10, 11]])
1874         Coordinates:
1875           * x        (x) int64 40 30 20 10
1876           * y        (y) int64 90 80 70
1877 
1878         Reindexing with both DataArrays having the same coordinates set, but in different order:
1879 
1880         >>> da1.reindex_like(da2)
1881         <xarray.DataArray (x: 4, y: 3)>
1882         array([[11, 10,  9],
1883                [ 8,  7,  6],
1884                [ 5,  4,  3],
1885                [ 2,  1,  0]])
1886         Coordinates:
1887           * x        (x) int64 40 30 20 10
1888           * y        (y) int64 90 80 70
1889 
1890         Reindexing with the other array having coordinates which the source array doesn't have:
1891 
1892         >>> data = np.arange(12).reshape(4, 3)
1893         >>> da1 = xr.DataArray(
1894         ...     data=data,
1895         ...     dims=["x", "y"],
1896         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
1897         ... )
1898         >>> da2 = xr.DataArray(
1899         ...     data=data,
1900         ...     dims=["x", "y"],
1901         ...     coords={"x": [20, 10, 29, 39], "y": [70, 80, 90]},
1902         ... )
1903         >>> da1.reindex_like(da2)
1904         <xarray.DataArray (x: 4, y: 3)>
1905         array([[ 3.,  4.,  5.],
1906                [ 0.,  1.,  2.],
1907                [nan, nan, nan],
1908                [nan, nan, nan]])
1909         Coordinates:
1910           * x        (x) int64 20 10 29 39
1911           * y        (y) int64 70 80 90
1912 
1913         Filling missing values with the previous valid index with respect to the coordinates' value:
1914 
1915         >>> da1.reindex_like(da2, method="ffill")
1916         <xarray.DataArray (x: 4, y: 3)>
1917         array([[3, 4, 5],
1918                [0, 1, 2],
1919                [3, 4, 5],
1920                [6, 7, 8]])
1921         Coordinates:
1922           * x        (x) int64 20 10 29 39
1923           * y        (y) int64 70 80 90
1924 
1925         Filling missing values while tolerating specified error for inexact matches:
1926 
1927         >>> da1.reindex_like(da2, method="ffill", tolerance=5)
1928         <xarray.DataArray (x: 4, y: 3)>
1929         array([[ 3.,  4.,  5.],
1930                [ 0.,  1.,  2.],
1931                [nan, nan, nan],
1932                [nan, nan, nan]])
1933         Coordinates:
1934           * x        (x) int64 20 10 29 39
1935           * y        (y) int64 70 80 90
1936 
1937         Filling missing values with manually specified values:
1938 
1939         >>> da1.reindex_like(da2, fill_value=19)
1940         <xarray.DataArray (x: 4, y: 3)>
1941         array([[ 3,  4,  5],
1942                [ 0,  1,  2],
1943                [19, 19, 19],
1944                [19, 19, 19]])
1945         Coordinates:
1946           * x        (x) int64 20 10 29 39
1947           * y        (y) int64 70 80 90
1948 
1949         See Also
1950         --------
1951         DataArray.reindex
1952         align
1953         """
1954         return alignment.reindex_like(
1955             self,
1956             other=other,
1957             method=method,
1958             tolerance=tolerance,
1959             copy=copy,
1960             fill_value=fill_value,
1961         )
1962 
1963     def reindex(
1964         self: T_DataArray,
1965         indexers: Mapping[Any, Any] | None = None,
1966         method: ReindexMethodOptions = None,
1967         tolerance: float | Iterable[float] | None = None,
1968         copy: bool = True,
1969         fill_value=dtypes.NA,
1970         **indexers_kwargs: Any,
1971     ) -> T_DataArray:
1972         """Conform this object onto the indexes of another object, filling in
1973         missing values with ``fill_value``. The default fill value is NaN.
1974 
1975         Parameters
1976         ----------
1977         indexers : dict, optional
1978             Dictionary with keys given by dimension names and values given by
1979             arrays of coordinates tick labels. Any mis-matched coordinate
1980             values will be filled in with NaN, and any mis-matched dimension
1981             names will simply be ignored.
1982             One of indexers or indexers_kwargs must be provided.
1983         copy : bool, optional
1984             If ``copy=True``, data in the return value is always copied. If
1985             ``copy=False`` and reindexing is unnecessary, or can be performed
1986             with only slice operations, then the output may share memory with
1987             the input. In either case, a new xarray object is always returned.
1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1989             Method to use for filling index values in ``indexers`` not found on
1990             this data array:
1991 
1992             - None (default): don't fill gaps
1993             - pad / ffill: propagate last valid index value forward
1994             - backfill / bfill: propagate next valid index value backward
1995             - nearest: use nearest valid index value
1996 
1997         tolerance : float | Iterable[float] | None, default: None
1998             Maximum distance between original and new labels for inexact
1999             matches. The values of the index at the matching locations must
2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2001             Tolerance may be a scalar value, which applies the same tolerance
2002             to all values, or list-like, which applies variable tolerance per
2003             element. List-like must be the same size as the index and its dtype
2004             must exactly match the index’s type.
2005         fill_value : scalar or dict-like, optional
2006             Value to use for newly missing values. If a dict-like, maps
2007             variable names (including coordinates) to fill values. Use this
2008             data array's name to refer to the data array's values.
2009         **indexers_kwargs : {dim: indexer, ...}, optional
2010             The keyword arguments form of ``indexers``.
2011             One of indexers or indexers_kwargs must be provided.
2012 
2013         Returns
2014         -------
2015         reindexed : DataArray
2016             Another dataset array, with this array's data but replaced
2017             coordinates.
2018 
2019         Examples
2020         --------
2021         Reverse latitude:
2022 
2023         >>> da = xr.DataArray(
2024         ...     np.arange(4),
2025         ...     coords=[np.array([90, 89, 88, 87])],
2026         ...     dims="lat",
2027         ... )
2028         >>> da
2029         <xarray.DataArray (lat: 4)>
2030         array([0, 1, 2, 3])
2031         Coordinates:
2032           * lat      (lat) int64 90 89 88 87
2033         >>> da.reindex(lat=da.lat[::-1])
2034         <xarray.DataArray (lat: 4)>
2035         array([3, 2, 1, 0])
2036         Coordinates:
2037           * lat      (lat) int64 87 88 89 90
2038 
2039         See Also
2040         --------
2041         DataArray.reindex_like
2042         align
2043         """
2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2045         return alignment.reindex(
2046             self,
2047             indexers=indexers,
2048             method=method,
2049             tolerance=tolerance,
2050             copy=copy,
2051             fill_value=fill_value,
2052         )
2053 
2054     def interp(
2055         self: T_DataArray,
2056         coords: Mapping[Any, Any] | None = None,
2057         method: InterpOptions = "linear",
2058         assume_sorted: bool = False,
2059         kwargs: Mapping[str, Any] | None = None,
2060         **coords_kwargs: Any,
2061     ) -> T_DataArray:
2062         """Interpolate a DataArray onto new coordinates
2063 
2064         Performs univariate or multivariate interpolation of a DataArray onto
2065         new coordinates using scipy's interpolation routines. If interpolating
2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
2067         called. When interpolating along multiple existing dimensions, an
2068         attempt is made to decompose the interpolation into multiple
2069         1-dimensional interpolations. If this is possible,
2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2071         :py:func:`scipy.interpolate.interpn` is called.
2072 
2073         Parameters
2074         ----------
2075         coords : dict, optional
2076             Mapping from dimension names to the new coordinates.
2077             New coordinate can be a scalar, array-like or DataArray.
2078             If DataArrays are passed as new coordinates, their dimensions are
2079             used for the broadcasting. Missing values are skipped.
2080         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2081             The method used to interpolate. The method should be supported by
2082             the scipy interpolator:
2083 
2084             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
2085               "quadratic", "cubic", "polynomial"}
2086             - ``interpn``: {"linear", "nearest"}
2087 
2088             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2089             also be provided.
2090         assume_sorted : bool, default: False
2091             If False, values of x can be in any order and they are sorted
2092             first. If True, x has to be an array of monotonically increasing
2093             values.
2094         kwargs : dict-like or None, default: None
2095             Additional keyword arguments passed to scipy's interpolator. Valid
2096             options and their behavior depend whether ``interp1d`` or
2097             ``interpn`` is used.
2098         **coords_kwargs : {dim: coordinate, ...}, optional
2099             The keyword arguments form of ``coords``.
2100             One of coords or coords_kwargs must be provided.
2101 
2102         Returns
2103         -------
2104         interpolated : DataArray
2105             New dataarray on the new coordinates.
2106 
2107         Notes
2108         -----
2109         scipy is required.
2110 
2111         See Also
2112         --------
2113         scipy.interpolate.interp1d
2114         scipy.interpolate.interpn
2115 
2116         Examples
2117         --------
2118         >>> da = xr.DataArray(
2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2120         ...     dims=("x", "y"),
2121         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2122         ... )
2123         >>> da
2124         <xarray.DataArray (x: 3, y: 4)>
2125         array([[ 1.,  4.,  2.,  9.],
2126                [ 2.,  7.,  6., nan],
2127                [ 6., nan,  5.,  8.]])
2128         Coordinates:
2129           * x        (x) int64 0 1 2
2130           * y        (y) int64 10 12 14 16
2131 
2132         1D linear interpolation (the default):
2133 
2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
2135         <xarray.DataArray (x: 4, y: 4)>
2136         array([[1.  , 4.  , 2.  ,  nan],
2137                [1.75, 6.25, 5.  ,  nan],
2138                [3.  ,  nan, 5.75,  nan],
2139                [5.  ,  nan, 5.25,  nan]])
2140         Coordinates:
2141           * y        (y) int64 10 12 14 16
2142           * x        (x) float64 0.0 0.75 1.25 1.75
2143 
2144         1D nearest interpolation:
2145 
2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2147         <xarray.DataArray (x: 4, y: 4)>
2148         array([[ 1.,  4.,  2.,  9.],
2149                [ 2.,  7.,  6., nan],
2150                [ 2.,  7.,  6., nan],
2151                [ 6., nan,  5.,  8.]])
2152         Coordinates:
2153           * y        (y) int64 10 12 14 16
2154           * x        (x) float64 0.0 0.75 1.25 1.75
2155 
2156         1D linear extrapolation:
2157 
2158         >>> da.interp(
2159         ...     x=[1, 1.5, 2.5, 3.5],
2160         ...     method="linear",
2161         ...     kwargs={"fill_value": "extrapolate"},
2162         ... )
2163         <xarray.DataArray (x: 4, y: 4)>
2164         array([[ 2. ,  7. ,  6. ,  nan],
2165                [ 4. ,  nan,  5.5,  nan],
2166                [ 8. ,  nan,  4.5,  nan],
2167                [12. ,  nan,  3.5,  nan]])
2168         Coordinates:
2169           * y        (y) int64 10 12 14 16
2170           * x        (x) float64 1.0 1.5 2.5 3.5
2171 
2172         2D linear interpolation:
2173 
2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2175         <xarray.DataArray (x: 4, y: 3)>
2176         array([[2.5  , 3.   ,   nan],
2177                [4.   , 5.625,   nan],
2178                [  nan,   nan,   nan],
2179                [  nan,   nan,   nan]])
2180         Coordinates:
2181           * x        (x) float64 0.0 0.75 1.25 1.75
2182           * y        (y) int64 11 13 15
2183         """
2184         if self.dtype.kind not in "uifc":
2185             raise TypeError(
2186                 "interp only works for a numeric type array. "
2187                 "Given {}.".format(self.dtype)
2188             )
2189         ds = self._to_temp_dataset().interp(
2190             coords,
2191             method=method,
2192             kwargs=kwargs,
2193             assume_sorted=assume_sorted,
2194             **coords_kwargs,
2195         )
2196         return self._from_temp_dataset(ds)
2197 
2198     def interp_like(
2199         self: T_DataArray,
2200         other: DataArray | Dataset,
2201         method: InterpOptions = "linear",
2202         assume_sorted: bool = False,
2203         kwargs: Mapping[str, Any] | None = None,
2204     ) -> T_DataArray:
2205         """Interpolate this object onto the coordinates of another object,
2206         filling out of range values with NaN.
2207 
2208         If interpolating along a single existing dimension,
2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
2210         along multiple existing dimensions, an attempt is made to decompose the
2211         interpolation into multiple 1-dimensional interpolations. If this is
2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
2213         :py:func:`scipy.interpolate.interpn` is called.
2214 
2215         Parameters
2216         ----------
2217         other : Dataset or DataArray
2218             Object with an 'indexes' attribute giving a mapping from dimension
2219             names to an 1d array-like, which provides coordinates upon
2220             which to index the variables in this dataset. Missing values are skipped.
2221         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
2222             The method used to interpolate. The method should be supported by
2223             the scipy interpolator:
2224 
2225             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
2226               "polynomial"} when ``interp1d`` is called.
2227             - {"linear", "nearest"} when ``interpn`` is called.
2228 
2229             If ``"polynomial"`` is passed, the ``order`` keyword argument must
2230             also be provided.
2231         assume_sorted : bool, default: False
2232             If False, values of coordinates that are interpolated over can be
2233             in any order and they are sorted first. If True, interpolated
2234             coordinates are assumed to be an array of monotonically increasing
2235             values.
2236         kwargs : dict, optional
2237             Additional keyword passed to scipy's interpolator.
2238 
2239         Returns
2240         -------
2241         interpolated : DataArray
2242             Another dataarray by interpolating this dataarray's data along the
2243             coordinates of the other object.
2244 
2245         Examples
2246         --------
2247         >>> data = np.arange(12).reshape(4, 3)
2248         >>> da1 = xr.DataArray(
2249         ...     data=data,
2250         ...     dims=["x", "y"],
2251         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2252         ... )
2253         >>> da1
2254         <xarray.DataArray (x: 4, y: 3)>
2255         array([[ 0,  1,  2],
2256                [ 3,  4,  5],
2257                [ 6,  7,  8],
2258                [ 9, 10, 11]])
2259         Coordinates:
2260           * x        (x) int64 10 20 30 40
2261           * y        (y) int64 70 80 90
2262         >>> da2 = xr.DataArray(
2263         ...     data=data,
2264         ...     dims=["x", "y"],
2265         ...     coords={"x": [10, 20, 29, 39], "y": [70, 80, 90]},
2266         ... )
2267         >>> da2
2268         <xarray.DataArray (x: 4, y: 3)>
2269         array([[ 0,  1,  2],
2270                [ 3,  4,  5],
2271                [ 6,  7,  8],
2272                [ 9, 10, 11]])
2273         Coordinates:
2274           * x        (x) int64 10 20 29 39
2275           * y        (y) int64 70 80 90
2276 
2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:
2278 
2279         >>> da2.interp_like(da1)
2280         <xarray.DataArray (x: 4, y: 3)>
2281         array([[0. , 1. , 2. ],
2282                [3. , 4. , 5. ],
2283                [6.3, 7.3, 8.3],
2284                [nan, nan, nan]])
2285         Coordinates:
2286           * x        (x) int64 10 20 30 40
2287           * y        (y) int64 70 80 90
2288 
2289         Could also extrapolate missing values:
2290 
2291         >>> da2.interp_like(da1, kwargs={"fill_value": "extrapolate"})
2292         <xarray.DataArray (x: 4, y: 3)>
2293         array([[ 0. ,  1. ,  2. ],
2294                [ 3. ,  4. ,  5. ],
2295                [ 6.3,  7.3,  8.3],
2296                [ 9.3, 10.3, 11.3]])
2297         Coordinates:
2298           * x        (x) int64 10 20 30 40
2299           * y        (y) int64 70 80 90
2300 
2301         Notes
2302         -----
2303         scipy is required.
2304         If the dataarray has object-type coordinates, reindex is used for these
2305         coordinates instead of the interpolation.
2306 
2307         See Also
2308         --------
2309         DataArray.interp
2310         DataArray.reindex_like
2311         """
2312         if self.dtype.kind not in "uifc":
2313             raise TypeError(
2314                 "interp only works for a numeric type array. "
2315                 "Given {}.".format(self.dtype)
2316             )
2317         ds = self._to_temp_dataset().interp_like(
2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
2319         )
2320         return self._from_temp_dataset(ds)
2321 
2322     # change type of self and return to T_DataArray once
2323     # https://github.com/python/mypy/issues/12846 is resolved
2324     def rename(
2325         self,
2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2327         **names: Hashable,
2328     ) -> DataArray:
2329         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2330 
2331         Parameters
2332         ----------
2333         new_name_or_name_dict : str or dict-like, optional
2334             If the argument is dict-like, it used as a mapping from old
2335             names to new names for coordinates or dimensions. Otherwise,
2336             use the argument as the new name for this array.
2337         **names : Hashable, optional
2338             The keyword arguments form of a mapping from old names to
2339             new names for coordinates or dimensions.
2340             One of new_name_or_name_dict or names must be provided.
2341 
2342         Returns
2343         -------
2344         renamed : DataArray
2345             Renamed array or array with renamed coordinates.
2346 
2347         See Also
2348         --------
2349         Dataset.rename
2350         DataArray.swap_dims
2351         """
2352         if new_name_or_name_dict is None and not names:
2353             # change name to None?
2354             return self._replace(name=None)
2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2356             # change dims/coords
2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2358             dataset = self._to_temp_dataset()._rename(name_dict)
2359             return self._from_temp_dataset(dataset)
2360         if utils.hashable(new_name_or_name_dict) and names:
2361             # change name + dims/coords
2362             dataset = self._to_temp_dataset()._rename(names)
2363             dataarray = self._from_temp_dataset(dataset)
2364             return dataarray._replace(name=new_name_or_name_dict)
2365         # only change name
2366         return self._replace(name=new_name_or_name_dict)
2367 
2368     def swap_dims(
2369         self: T_DataArray,
2370         dims_dict: Mapping[Any, Hashable] | None = None,
2371         **dims_kwargs,
2372     ) -> T_DataArray:
2373         """Returns a new DataArray with swapped dimensions.
2374 
2375         Parameters
2376         ----------
2377         dims_dict : dict-like
2378             Dictionary whose keys are current dimension names and whose values
2379             are new names.
2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2381             The keyword arguments form of ``dims_dict``.
2382             One of dims_dict or dims_kwargs must be provided.
2383 
2384         Returns
2385         -------
2386         swapped : DataArray
2387             DataArray with swapped dimensions.
2388 
2389         Examples
2390         --------
2391         >>> arr = xr.DataArray(
2392         ...     data=[0, 1],
2393         ...     dims="x",
2394         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2395         ... )
2396         >>> arr
2397         <xarray.DataArray (x: 2)>
2398         array([0, 1])
2399         Coordinates:
2400           * x        (x) <U1 'a' 'b'
2401             y        (x) int64 0 1
2402 
2403         >>> arr.swap_dims({"x": "y"})
2404         <xarray.DataArray (y: 2)>
2405         array([0, 1])
2406         Coordinates:
2407             x        (y) <U1 'a' 'b'
2408           * y        (y) int64 0 1
2409 
2410         >>> arr.swap_dims({"x": "z"})
2411         <xarray.DataArray (z: 2)>
2412         array([0, 1])
2413         Coordinates:
2414             x        (z) <U1 'a' 'b'
2415             y        (z) int64 0 1
2416         Dimensions without coordinates: z
2417 
2418         See Also
2419         --------
2420         DataArray.rename
2421         Dataset.swap_dims
2422         """
2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2424         ds = self._to_temp_dataset().swap_dims(dims_dict)
2425         return self._from_temp_dataset(ds)
2426 
2427     # change type of self and return to T_DataArray once
2428     # https://github.com/python/mypy/issues/12846 is resolved
2429     def expand_dims(
2430         self,
2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2432         axis: None | int | Sequence[int] = None,
2433         **dim_kwargs: Any,
2434     ) -> DataArray:
2435         """Return a new object with an additional axis (or axes) inserted at
2436         the corresponding position in the array shape. The new object is a
2437         view into the underlying array, not a copy.
2438 
2439         If dim is already a scalar coordinate, it will be promoted to a 1D
2440         coordinate consisting of a single value.
2441 
2442         Parameters
2443         ----------
2444         dim : Hashable, sequence of Hashable, dict, or None, optional
2445             Dimensions to include on the new variable.
2446             If provided as str or sequence of str, then dimensions are inserted
2447             with length 1. If provided as a dict, then the keys are the new
2448             dimensions and the values are either integers (giving the length of
2449             the new dimensions) or sequence/ndarray (giving the coordinates of
2450             the new dimensions).
2451         axis : int, sequence of int, or None, default: None
2452             Axis position(s) where new axis is to be inserted (position(s) on
2453             the result array). If a sequence of integers is passed,
2454             multiple axes are inserted. In this case, dim arguments should be
2455             same length list. If axis=None is passed, all the axes will be
2456             inserted to the start of the result array.
2457         **dim_kwargs : int or sequence or ndarray
2458             The keywords are arbitrary dimensions being inserted and the values
2459             are either the lengths of the new dims (if int is given), or their
2460             coordinates. Note, this is an alternative to passing a dict to the
2461             dim kwarg and will only be used if dim is None.
2462 
2463         Returns
2464         -------
2465         expanded : DataArray
2466             This object, but with additional dimension(s).
2467 
2468         See Also
2469         --------
2470         Dataset.expand_dims
2471 
2472         Examples
2473         --------
2474         >>> da = xr.DataArray(np.arange(5), dims=("x"))
2475         >>> da
2476         <xarray.DataArray (x: 5)>
2477         array([0, 1, 2, 3, 4])
2478         Dimensions without coordinates: x
2479 
2480         Add new dimension of length 2:
2481 
2482         >>> da.expand_dims(dim={"y": 2})
2483         <xarray.DataArray (y: 2, x: 5)>
2484         array([[0, 1, 2, 3, 4],
2485                [0, 1, 2, 3, 4]])
2486         Dimensions without coordinates: y, x
2487 
2488         >>> da.expand_dims(dim={"y": 2}, axis=1)
2489         <xarray.DataArray (x: 5, y: 2)>
2490         array([[0, 0],
2491                [1, 1],
2492                [2, 2],
2493                [3, 3],
2494                [4, 4]])
2495         Dimensions without coordinates: x, y
2496 
2497         Add a new dimension with coordinates from array:
2498 
2499         >>> da.expand_dims(dim={"y": np.arange(5)}, axis=0)
2500         <xarray.DataArray (y: 5, x: 5)>
2501         array([[0, 1, 2, 3, 4],
2502                [0, 1, 2, 3, 4],
2503                [0, 1, 2, 3, 4],
2504                [0, 1, 2, 3, 4],
2505                [0, 1, 2, 3, 4]])
2506         Coordinates:
2507           * y        (y) int64 0 1 2 3 4
2508         Dimensions without coordinates: x
2509         """
2510         if isinstance(dim, int):
2511             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2513             if len(dim) != len(set(dim)):
2514                 raise ValueError("dims should not contain duplicate values.")
2515             dim = dict.fromkeys(dim, 1)
2516         elif dim is not None and not isinstance(dim, Mapping):
2517             dim = {cast(Hashable, dim): 1}
2518 
2519         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2520         ds = self._to_temp_dataset().expand_dims(dim, axis)
2521         return self._from_temp_dataset(ds)
2522 
2523     # change type of self and return to T_DataArray once
2524     # https://github.com/python/mypy/issues/12846 is resolved
2525     def set_index(
2526         self,
2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
2528         append: bool = False,
2529         **indexes_kwargs: Hashable | Sequence[Hashable],
2530     ) -> DataArray:
2531         """Set DataArray (multi-)indexes using one or more existing
2532         coordinates.
2533 
2534         This legacy method is limited to pandas (multi-)indexes and
2535         1-dimensional "dimension" coordinates. See
2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom
2537         Xarray-compatible index from one or more arbitrary coordinates.
2538 
2539         Parameters
2540         ----------
2541         indexes : {dim: index, ...}
2542             Mapping from names matching dimensions and values given
2543             by (lists of) the names of existing coordinates or variables to set
2544             as new (multi-)index.
2545         append : bool, default: False
2546             If True, append the supplied index(es) to the existing index(es).
2547             Otherwise replace the existing index(es).
2548         **indexes_kwargs : optional
2549             The keyword arguments form of ``indexes``.
2550             One of indexes or indexes_kwargs must be provided.
2551 
2552         Returns
2553         -------
2554         obj : DataArray
2555             Another DataArray, with this data but replaced coordinates.
2556 
2557         Examples
2558         --------
2559         >>> arr = xr.DataArray(
2560         ...     data=np.ones((2, 3)),
2561         ...     dims=["x", "y"],
2562         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2563         ... )
2564         >>> arr
2565         <xarray.DataArray (x: 2, y: 3)>
2566         array([[1., 1., 1.],
2567                [1., 1., 1.]])
2568         Coordinates:
2569           * x        (x) int64 0 1
2570           * y        (y) int64 0 1 2
2571             a        (x) int64 3 4
2572         >>> arr.set_index(x="a")
2573         <xarray.DataArray (x: 2, y: 3)>
2574         array([[1., 1., 1.],
2575                [1., 1., 1.]])
2576         Coordinates:
2577           * x        (x) int64 3 4
2578           * y        (y) int64 0 1 2
2579 
2580         See Also
2581         --------
2582         DataArray.reset_index
2583         DataArray.set_xindex
2584         """
2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2586         return self._from_temp_dataset(ds)
2587 
2588     # change type of self and return to T_DataArray once
2589     # https://github.com/python/mypy/issues/12846 is resolved
2590     def reset_index(
2591         self,
2592         dims_or_levels: Hashable | Sequence[Hashable],
2593         drop: bool = False,
2594     ) -> DataArray:
2595         """Reset the specified index(es) or multi-index level(s).
2596 
2597         This legacy method is specific to pandas (multi-)indexes and
2598         1-dimensional "dimension" coordinates. See the more generic
2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`
2600         method to respectively drop and set pandas or custom indexes for
2601         arbitrary coordinates.
2602 
2603         Parameters
2604         ----------
2605         dims_or_levels : Hashable or sequence of Hashable
2606             Name(s) of the dimension(s) and/or multi-index level(s) that will
2607             be reset.
2608         drop : bool, default: False
2609             If True, remove the specified indexes and/or multi-index levels
2610             instead of extracting them as new coordinates (default: False).
2611 
2612         Returns
2613         -------
2614         obj : DataArray
2615             Another dataarray, with this dataarray's data but replaced
2616             coordinates.
2617 
2618         See Also
2619         --------
2620         DataArray.set_index
2621         DataArray.set_xindex
2622         DataArray.drop_indexes
2623         """
2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2625         return self._from_temp_dataset(ds)
2626 
2627     def set_xindex(
2628         self: T_DataArray,
2629         coord_names: str | Sequence[Hashable],
2630         index_cls: type[Index] | None = None,
2631         **options,
2632     ) -> T_DataArray:
2633         """Set a new, Xarray-compatible index from one or more existing
2634         coordinate(s).
2635 
2636         Parameters
2637         ----------
2638         coord_names : str or list
2639             Name(s) of the coordinate(s) used to build the index.
2640             If several names are given, their order matters.
2641         index_cls : subclass of :class:`~xarray.indexes.Index`
2642             The type of index to create. By default, try setting
2643             a pandas (multi-)index from the supplied coordinates.
2644         **options
2645             Options passed to the index constructor.
2646 
2647         Returns
2648         -------
2649         obj : DataArray
2650             Another dataarray, with this dataarray's data and with a new index.
2651 
2652         """
2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)
2654         return self._from_temp_dataset(ds)
2655 
2656     def reorder_levels(
2657         self: T_DataArray,
2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2659         **dim_order_kwargs: Sequence[int | Hashable],
2660     ) -> T_DataArray:
2661         """Rearrange index levels using input order.
2662 
2663         Parameters
2664         ----------
2665         dim_order dict-like of Hashable to int or Hashable: optional
2666             Mapping from names matching dimensions and values given
2667             by lists representing new level orders. Every given dimension
2668             must have a multi-index.
2669         **dim_order_kwargs : optional
2670             The keyword arguments form of ``dim_order``.
2671             One of dim_order or dim_order_kwargs must be provided.
2672 
2673         Returns
2674         -------
2675         obj : DataArray
2676             Another dataarray, with this dataarray's data but replaced
2677             coordinates.
2678         """
2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2680         return self._from_temp_dataset(ds)
2681 
2682     def stack(
2683         self: T_DataArray,
2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2685         create_index: bool | None = True,
2686         index_cls: type[Index] = PandasMultiIndex,
2687         **dimensions_kwargs: Sequence[Hashable],
2688     ) -> T_DataArray:
2689         """
2690         Stack any number of existing dimensions into a single new dimension.
2691 
2692         New dimensions will be added at the end, and the corresponding
2693         coordinate variables will be combined into a MultiIndex.
2694 
2695         Parameters
2696         ----------
2697         dimensions : mapping of Hashable to sequence of Hashable
2698             Mapping of the form `new_name=(dim1, dim2, ...)`.
2699             Names of new dimensions, and the existing dimensions that they
2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2702             all dimensions.
2703         create_index : bool or None, default: True
2704             If True, create a multi-index for each of the stacked dimensions.
2705             If False, don't create any index.
2706             If None, create a multi-index only if exactly one single (1-d) coordinate
2707             index is found for every dimension to stack.
2708         index_cls: class, optional
2709             Can be used to pass a custom multi-index type. Must be an Xarray index that
2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2711         **dimensions_kwargs
2712             The keyword arguments form of ``dimensions``.
2713             One of dimensions or dimensions_kwargs must be provided.
2714 
2715         Returns
2716         -------
2717         stacked : DataArray
2718             DataArray with stacked data.
2719 
2720         Examples
2721         --------
2722         >>> arr = xr.DataArray(
2723         ...     np.arange(6).reshape(2, 3),
2724         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2725         ... )
2726         >>> arr
2727         <xarray.DataArray (x: 2, y: 3)>
2728         array([[0, 1, 2],
2729                [3, 4, 5]])
2730         Coordinates:
2731           * x        (x) <U1 'a' 'b'
2732           * y        (y) int64 0 1 2
2733         >>> stacked = arr.stack(z=("x", "y"))
2734         >>> stacked.indexes["z"]
2735         MultiIndex([('a', 0),
2736                     ('a', 1),
2737                     ('a', 2),
2738                     ('b', 0),
2739                     ('b', 1),
2740                     ('b', 2)],
2741                    name='z')
2742 
2743         See Also
2744         --------
2745         DataArray.unstack
2746         """
2747         ds = self._to_temp_dataset().stack(
2748             dimensions,
2749             create_index=create_index,
2750             index_cls=index_cls,
2751             **dimensions_kwargs,
2752         )
2753         return self._from_temp_dataset(ds)
2754 
2755     # change type of self and return to T_DataArray once
2756     # https://github.com/python/mypy/issues/12846 is resolved
2757     def unstack(
2758         self,
2759         dim: Dims = None,
2760         fill_value: Any = dtypes.NA,
2761         sparse: bool = False,
2762     ) -> DataArray:
2763         """
2764         Unstack existing dimensions corresponding to MultiIndexes into
2765         multiple new dimensions.
2766 
2767         New dimensions will be added at the end.
2768 
2769         Parameters
2770         ----------
2771         dim : str, Iterable of Hashable or None, optional
2772             Dimension(s) over which to unstack. By default unstacks all
2773             MultiIndexes.
2774         fill_value : scalar or dict-like, default: nan
2775             Value to be filled. If a dict-like, maps variable names to
2776             fill values. Use the data array's name to refer to its
2777             name. If not provided or if the dict-like does not contain
2778             all variables, the dtype's NA value will be used.
2779         sparse : bool, default: False
2780             Use sparse-array if True
2781 
2782         Returns
2783         -------
2784         unstacked : DataArray
2785             Array with unstacked data.
2786 
2787         Examples
2788         --------
2789         >>> arr = xr.DataArray(
2790         ...     np.arange(6).reshape(2, 3),
2791         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2792         ... )
2793         >>> arr
2794         <xarray.DataArray (x: 2, y: 3)>
2795         array([[0, 1, 2],
2796                [3, 4, 5]])
2797         Coordinates:
2798           * x        (x) <U1 'a' 'b'
2799           * y        (y) int64 0 1 2
2800         >>> stacked = arr.stack(z=("x", "y"))
2801         >>> stacked.indexes["z"]
2802         MultiIndex([('a', 0),
2803                     ('a', 1),
2804                     ('a', 2),
2805                     ('b', 0),
2806                     ('b', 1),
2807                     ('b', 2)],
2808                    name='z')
2809         >>> roundtripped = stacked.unstack()
2810         >>> arr.identical(roundtripped)
2811         True
2812 
2813         See Also
2814         --------
2815         DataArray.stack
2816         """
2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2818         return self._from_temp_dataset(ds)
2819 
2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2821         """Unstack DataArray expanding to Dataset along a given level of a
2822         stacked coordinate.
2823 
2824         This is the inverse operation of Dataset.to_stacked_array.
2825 
2826         Parameters
2827         ----------
2828         dim : Hashable
2829             Name of existing dimension to unstack
2830         level : int or Hashable, default: 0
2831             The MultiIndex level to expand to a dataset along. Can either be
2832             the integer index of the level or its name.
2833 
2834         Returns
2835         -------
2836         unstacked: Dataset
2837 
2838         Examples
2839         --------
2840         >>> arr = xr.DataArray(
2841         ...     np.arange(6).reshape(2, 3),
2842         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2843         ... )
2844         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2845         >>> data
2846         <xarray.Dataset>
2847         Dimensions:  (x: 2, y: 3)
2848         Coordinates:
2849           * x        (x) <U1 'a' 'b'
2850           * y        (y) int64 0 1 2
2851         Data variables:
2852             a        (x, y) int64 0 1 2 3 4 5
2853             b        (x) int64 0 3
2854         >>> stacked = data.to_stacked_array("z", ["x"])
2855         >>> stacked.indexes["z"]
2856         MultiIndex([('a', 0.0),
2857                     ('a', 1.0),
2858                     ('a', 2.0),
2859                     ('b', nan)],
2860                    name='z')
2861         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2862         >>> data.identical(roundtripped)
2863         True
2864 
2865         See Also
2866         --------
2867         Dataset.to_stacked_array
2868         """
2869         idx = self._indexes[dim].to_pandas_index()
2870         if not isinstance(idx, pd.MultiIndex):
2871             raise ValueError(f"'{dim}' is not a stacked coordinate")
2872 
2873         level_number = idx._get_level_number(level)
2874         variables = idx.levels[level_number]
2875         variable_dim = idx.names[level_number]
2876 
2877         # pull variables out of datarray
2878         data_dict = {}
2879         for k in variables:
2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2881 
2882         # unstacked dataset
2883         return Dataset(data_dict)
2884 
2885     def transpose(
2886         self: T_DataArray,
2887         *dims: Hashable,
2888         transpose_coords: bool = True,
2889         missing_dims: ErrorOptionsWithWarn = "raise",
2890     ) -> T_DataArray:
2891         """Return a new DataArray object with transposed dimensions.
2892 
2893         Parameters
2894         ----------
2895         *dims : Hashable, optional
2896             By default, reverse the dimensions. Otherwise, reorder the
2897             dimensions to this order.
2898         transpose_coords : bool, default: True
2899             If True, also transpose the coordinates of this DataArray.
2900         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2901             What to do if dimensions that should be selected from are not present in the
2902             DataArray:
2903             - "raise": raise an exception
2904             - "warn": raise a warning, and ignore the missing dimensions
2905             - "ignore": ignore the missing dimensions
2906 
2907         Returns
2908         -------
2909         transposed : DataArray
2910             The returned DataArray's array is transposed.
2911 
2912         Notes
2913         -----
2914         This operation returns a view of this array's data. It is
2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2916         -- the data will be fully loaded.
2917 
2918         See Also
2919         --------
2920         numpy.transpose
2921         Dataset.transpose
2922         """
2923         if dims:
2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2925         variable = self.variable.transpose(*dims)
2926         if transpose_coords:
2927             coords: dict[Hashable, Variable] = {}
2928             for name, coord in self.coords.items():
2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2930                 coords[name] = coord.variable.transpose(*coord_dims)
2931             return self._replace(variable, coords)
2932         else:
2933             return self._replace(variable)
2934 
2935     @property
2936     def T(self: T_DataArray) -> T_DataArray:
2937         return self.transpose()
2938 
2939     # change type of self and return to T_DataArray once
2940     # https://github.com/python/mypy/issues/12846 is resolved
2941     def drop_vars(
2942         self,
2943         names: Hashable | Iterable[Hashable],
2944         *,
2945         errors: ErrorOptions = "raise",
2946     ) -> DataArray:
2947         """Returns an array with dropped variables.
2948 
2949         Parameters
2950         ----------
2951         names : Hashable or iterable of Hashable
2952             Name(s) of variables to drop.
2953         errors : {"raise", "ignore"}, default: "raise"
2954             If 'raise', raises a ValueError error if any of the variable
2955             passed are not in the dataset. If 'ignore', any given names that are in the
2956             DataArray are dropped and no error is raised.
2957 
2958         Returns
2959         -------
2960         dropped : Dataset
2961             New Dataset copied from `self` with variables removed.
2962 
2963         Examples
2964         -------
2965         >>> data = np.arange(12).reshape(4, 3)
2966         >>> da = xr.DataArray(
2967         ...     data=data,
2968         ...     dims=["x", "y"],
2969         ...     coords={"x": [10, 20, 30, 40], "y": [70, 80, 90]},
2970         ... )
2971         >>> da
2972         <xarray.DataArray (x: 4, y: 3)>
2973         array([[ 0,  1,  2],
2974                [ 3,  4,  5],
2975                [ 6,  7,  8],
2976                [ 9, 10, 11]])
2977         Coordinates:
2978           * x        (x) int64 10 20 30 40
2979           * y        (y) int64 70 80 90
2980 
2981         Removing a single variable:
2982 
2983         >>> da.drop_vars("x")
2984         <xarray.DataArray (x: 4, y: 3)>
2985         array([[ 0,  1,  2],
2986                [ 3,  4,  5],
2987                [ 6,  7,  8],
2988                [ 9, 10, 11]])
2989         Coordinates:
2990           * y        (y) int64 70 80 90
2991         Dimensions without coordinates: x
2992 
2993         Removing a list of variables:
2994 
2995         >>> da.drop_vars(["x", "y"])
2996         <xarray.DataArray (x: 4, y: 3)>
2997         array([[ 0,  1,  2],
2998                [ 3,  4,  5],
2999                [ 6,  7,  8],
3000                [ 9, 10, 11]])
3001         Dimensions without coordinates: x, y
3002         """
3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
3004         return self._from_temp_dataset(ds)
3005 
3006     def drop_indexes(
3007         self: T_DataArray,
3008         coord_names: Hashable | Iterable[Hashable],
3009         *,
3010         errors: ErrorOptions = "raise",
3011     ) -> T_DataArray:
3012         """Drop the indexes assigned to the given coordinates.
3013 
3014         Parameters
3015         ----------
3016         coord_names : hashable or iterable of hashable
3017             Name(s) of the coordinate(s) for which to drop the index.
3018         errors : {"raise", "ignore"}, default: "raise"
3019             If 'raise', raises a ValueError error if any of the coordinates
3020             passed have no index or are not in the dataset.
3021             If 'ignore', no error is raised.
3022 
3023         Returns
3024         -------
3025         dropped : DataArray
3026             A new dataarray with dropped indexes.
3027         """
3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)
3029         return self._from_temp_dataset(ds)
3030 
3031     def drop(
3032         self: T_DataArray,
3033         labels: Mapping[Any, Any] | None = None,
3034         dim: Hashable | None = None,
3035         *,
3036         errors: ErrorOptions = "raise",
3037         **labels_kwargs,
3038     ) -> T_DataArray:
3039         """Backward compatible method based on `drop_vars` and `drop_sel`
3040 
3041         Using either `drop_vars` or `drop_sel` is encouraged
3042 
3043         See Also
3044         --------
3045         DataArray.drop_vars
3046         DataArray.drop_sel
3047         """
3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
3049         return self._from_temp_dataset(ds)
3050 
3051     def drop_sel(
3052         self: T_DataArray,
3053         labels: Mapping[Any, Any] | None = None,
3054         *,
3055         errors: ErrorOptions = "raise",
3056         **labels_kwargs,
3057     ) -> T_DataArray:
3058         """Drop index labels from this DataArray.
3059 
3060         Parameters
3061         ----------
3062         labels : mapping of Hashable to Any
3063             Index labels to drop
3064         errors : {"raise", "ignore"}, default: "raise"
3065             If 'raise', raises a ValueError error if
3066             any of the index labels passed are not
3067             in the dataset. If 'ignore', any given labels that are in the
3068             dataset are dropped and no error is raised.
3069         **labels_kwargs : {dim: label, ...}, optional
3070             The keyword arguments form of ``dim`` and ``labels``
3071 
3072         Returns
3073         -------
3074         dropped : DataArray
3075 
3076         Examples
3077         --------
3078         >>> da = xr.DataArray(
3079         ...     np.arange(25).reshape(5, 5),
3080         ...     coords={"x": np.arange(0, 9, 2), "y": np.arange(0, 13, 3)},
3081         ...     dims=("x", "y"),
3082         ... )
3083         >>> da
3084         <xarray.DataArray (x: 5, y: 5)>
3085         array([[ 0,  1,  2,  3,  4],
3086                [ 5,  6,  7,  8,  9],
3087                [10, 11, 12, 13, 14],
3088                [15, 16, 17, 18, 19],
3089                [20, 21, 22, 23, 24]])
3090         Coordinates:
3091           * x        (x) int64 0 2 4 6 8
3092           * y        (y) int64 0 3 6 9 12
3093 
3094         >>> da.drop_sel(x=[0, 2], y=9)
3095         <xarray.DataArray (x: 3, y: 4)>
3096         array([[10, 11, 12, 14],
3097                [15, 16, 17, 19],
3098                [20, 21, 22, 24]])
3099         Coordinates:
3100           * x        (x) int64 4 6 8
3101           * y        (y) int64 0 3 6 12
3102 
3103         >>> da.drop_sel({"x": 6, "y": [0, 3]})
3104         <xarray.DataArray (x: 4, y: 3)>
3105         array([[ 2,  3,  4],
3106                [ 7,  8,  9],
3107                [12, 13, 14],
3108                [22, 23, 24]])
3109         Coordinates:
3110           * x        (x) int64 0 2 4 8
3111           * y        (y) int64 6 9 12
3112         """
3113         if labels_kwargs or isinstance(labels, dict):
3114             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3115 
3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
3117         return self._from_temp_dataset(ds)
3118 
3119     def drop_isel(
3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
3121     ) -> T_DataArray:
3122         """Drop index positions from this DataArray.
3123 
3124         Parameters
3125         ----------
3126         indexers : mapping of Hashable to Any or None, default: None
3127             Index locations to drop
3128         **indexers_kwargs : {dim: position, ...}, optional
3129             The keyword arguments form of ``dim`` and ``positions``
3130 
3131         Returns
3132         -------
3133         dropped : DataArray
3134 
3135         Raises
3136         ------
3137         IndexError
3138 
3139         Examples
3140         --------
3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("X", "Y"))
3142         >>> da
3143         <xarray.DataArray (X: 5, Y: 5)>
3144         array([[ 0,  1,  2,  3,  4],
3145                [ 5,  6,  7,  8,  9],
3146                [10, 11, 12, 13, 14],
3147                [15, 16, 17, 18, 19],
3148                [20, 21, 22, 23, 24]])
3149         Dimensions without coordinates: X, Y
3150 
3151         >>> da.drop_isel(X=[0, 4], Y=2)
3152         <xarray.DataArray (X: 3, Y: 4)>
3153         array([[ 5,  6,  8,  9],
3154                [10, 11, 13, 14],
3155                [15, 16, 18, 19]])
3156         Dimensions without coordinates: X, Y
3157 
3158         >>> da.drop_isel({"X": 3, "Y": 3})
3159         <xarray.DataArray (X: 4, Y: 4)>
3160         array([[ 0,  1,  2,  4],
3161                [ 5,  6,  7,  9],
3162                [10, 11, 12, 14],
3163                [20, 21, 22, 24]])
3164         Dimensions without coordinates: X, Y
3165         """
3166         dataset = self._to_temp_dataset()
3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
3168         return self._from_temp_dataset(dataset)
3169 
3170     def dropna(
3171         self: T_DataArray,
3172         dim: Hashable,
3173         how: Literal["any", "all"] = "any",
3174         thresh: int | None = None,
3175     ) -> T_DataArray:
3176         """Returns a new array with dropped labels for missing values along
3177         the provided dimension.
3178 
3179         Parameters
3180         ----------
3181         dim : Hashable
3182             Dimension along which to drop missing values. Dropping along
3183             multiple dimensions simultaneously is not yet supported.
3184         how : {"any", "all"}, default: "any"
3185             - any : if any NA values are present, drop that label
3186             - all : if all values are NA, drop that label
3187 
3188         thresh : int or None, default: None
3189             If supplied, require this many non-NA values.
3190 
3191         Returns
3192         -------
3193         dropped : DataArray
3194 
3195         Examples
3196         --------
3197         >>> temperature = [
3198         ...     [0, 4, 2, 9],
3199         ...     [np.nan, np.nan, np.nan, np.nan],
3200         ...     [np.nan, 4, 2, 0],
3201         ...     [3, 1, 0, 0],
3202         ... ]
3203         >>> da = xr.DataArray(
3204         ...     data=temperature,
3205         ...     dims=["Y", "X"],
3206         ...     coords=dict(
3207         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75])),
3208         ...         lon=("X", np.array([10.0, 10.25, 10.5, 10.75])),
3209         ...     ),
3210         ... )
3211         >>> da
3212         <xarray.DataArray (Y: 4, X: 4)>
3213         array([[ 0.,  4.,  2.,  9.],
3214                [nan, nan, nan, nan],
3215                [nan,  4.,  2.,  0.],
3216                [ 3.,  1.,  0.,  0.]])
3217         Coordinates:
3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75
3219             lon      (X) float64 10.0 10.25 10.5 10.75
3220         Dimensions without coordinates: Y, X
3221 
3222         >>> da.dropna(dim="Y", how="any")
3223         <xarray.DataArray (Y: 2, X: 4)>
3224         array([[0., 4., 2., 9.],
3225                [3., 1., 0., 0.]])
3226         Coordinates:
3227             lat      (Y) float64 -20.0 -20.75
3228             lon      (X) float64 10.0 10.25 10.5 10.75
3229         Dimensions without coordinates: Y, X
3230 
3231         Drop values only if all values along the dimension are NaN:
3232 
3233         >>> da.dropna(dim="Y", how="all")
3234         <xarray.DataArray (Y: 3, X: 4)>
3235         array([[ 0.,  4.,  2.,  9.],
3236                [nan,  4.,  2.,  0.],
3237                [ 3.,  1.,  0.,  0.]])
3238         Coordinates:
3239             lat      (Y) float64 -20.0 -20.5 -20.75
3240             lon      (X) float64 10.0 10.25 10.5 10.75
3241         Dimensions without coordinates: Y, X
3242         """
3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
3244         return self._from_temp_dataset(ds)
3245 
3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
3247         """Fill missing values in this object.
3248 
3249         This operation follows the normal broadcasting and alignment rules that
3250         xarray uses for binary arithmetic, except the result is aligned to this
3251         object (``join='left'``) instead of aligned to the intersection of
3252         index coordinates (``join='inner'``).
3253 
3254         Parameters
3255         ----------
3256         value : scalar, ndarray or DataArray
3257             Used to fill all matching missing values in this array. If the
3258             argument is a DataArray, it is first aligned with (reindexed to)
3259             this array.
3260 
3261         Returns
3262         -------
3263         filled : DataArray
3264 
3265         Examples
3266         --------
3267         >>> da = xr.DataArray(
3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),
3269         ...     dims="Z",
3270         ...     coords=dict(
3271         ...         Z=("Z", np.arange(6)),
3272         ...         height=("Z", np.array([0, 10, 20, 30, 40, 50])),
3273         ...     ),
3274         ... )
3275         >>> da
3276         <xarray.DataArray (Z: 6)>
3277         array([ 1.,  4., nan,  0.,  3., nan])
3278         Coordinates:
3279           * Z        (Z) int64 0 1 2 3 4 5
3280             height   (Z) int64 0 10 20 30 40 50
3281 
3282         Fill all NaN values with 0:
3283 
3284         >>> da.fillna(0)
3285         <xarray.DataArray (Z: 6)>
3286         array([1., 4., 0., 0., 3., 0.])
3287         Coordinates:
3288           * Z        (Z) int64 0 1 2 3 4 5
3289             height   (Z) int64 0 10 20 30 40 50
3290 
3291         Fill NaN values with corresponding values in array:
3292 
3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))
3294         <xarray.DataArray (Z: 6)>
3295         array([1., 4., 4., 0., 3., 9.])
3296         Coordinates:
3297           * Z        (Z) int64 0 1 2 3 4 5
3298             height   (Z) int64 0 10 20 30 40 50
3299         """
3300         if utils.is_dict_like(value):
3301             raise TypeError(
3302                 "cannot provide fill value as a dictionary with "
3303                 "fillna on a DataArray"
3304             )
3305         out = ops.fillna(self, value)
3306         return out
3307 
3308     def interpolate_na(
3309         self: T_DataArray,
3310         dim: Hashable | None = None,
3311         method: InterpOptions = "linear",
3312         limit: int | None = None,
3313         use_coordinate: bool | str = True,
3314         max_gap: (
3315             None
3316             | int
3317             | float
3318             | str
3319             | pd.Timedelta
3320             | np.timedelta64
3321             | datetime.timedelta
3322         ) = None,
3323         keep_attrs: bool | None = None,
3324         **kwargs: Any,
3325     ) -> T_DataArray:
3326         """Fill in NaNs by interpolating according to different methods.
3327 
3328         Parameters
3329         ----------
3330         dim : Hashable or None, optional
3331             Specifies the dimension along which to interpolate.
3332         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3333             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3334             String indicating which method to use for interpolation:
3335 
3336             - 'linear': linear interpolation. Additional keyword
3337               arguments are passed to :py:func:`numpy.interp`
3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3339               are passed to :py:func:`scipy.interpolate.interp1d`. If
3340               ``method='polynomial'``, the ``order`` keyword argument must also be
3341               provided.
3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3343               respective :py:class:`scipy.interpolate` classes.
3344 
3345         use_coordinate : bool or str, default: True
3346             Specifies which index to use as the x values in the interpolation
3347             formulated as `y = f(x)`. If False, values are treated as if
3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is
3349             used. If ``use_coordinate`` is a string, it specifies the name of a
3350             coordinate variable to use as the index.
3351         limit : int or None, default: None
3352             Maximum number of consecutive NaNs to fill. Must be greater than 0
3353             or None for no limit. This filling is done regardless of the size of
3354             the gap in the data. To only interpolate over gaps less than a given length,
3355             see ``max_gap``.
3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
3358             Use None for no limit. When interpolating along a datetime64 dimension
3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
3360 
3361             - a string that is valid input for pandas.to_timedelta
3362             - a :py:class:`numpy.timedelta64` object
3363             - a :py:class:`pandas.Timedelta` object
3364             - a :py:class:`datetime.timedelta` object
3365 
3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
3367             dimensions has not been implemented yet. Gap length is defined as the difference
3368             between coordinate values at the first data point after a gap and the last value
3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference
3370             between coordinate values at the first (last) valid data point and the first (last) NaN.
3371             For example, consider::
3372 
3373                 <xarray.DataArray (x: 9)>
3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
3375                 Coordinates:
3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8
3377 
3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
3379         keep_attrs : bool or None, default: None
3380             If True, the dataarray's attributes (`attrs`) will be copied from
3381             the original object to the new one.  If False, the new
3382             object will be returned without attributes.
3383         **kwargs : dict, optional
3384             parameters passed verbatim to the underlying interpolation function
3385 
3386         Returns
3387         -------
3388         interpolated: DataArray
3389             Filled in DataArray.
3390 
3391         See Also
3392         --------
3393         numpy.interp
3394         scipy.interpolate
3395 
3396         Examples
3397         --------
3398         >>> da = xr.DataArray(
3399         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
3400         ... )
3401         >>> da
3402         <xarray.DataArray (x: 5)>
3403         array([nan,  2.,  3., nan,  0.])
3404         Coordinates:
3405           * x        (x) int64 0 1 2 3 4
3406 
3407         >>> da.interpolate_na(dim="x", method="linear")
3408         <xarray.DataArray (x: 5)>
3409         array([nan, 2. , 3. , 1.5, 0. ])
3410         Coordinates:
3411           * x        (x) int64 0 1 2 3 4
3412 
3413         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
3414         <xarray.DataArray (x: 5)>
3415         array([1. , 2. , 3. , 1.5, 0. ])
3416         Coordinates:
3417           * x        (x) int64 0 1 2 3 4
3418         """
3419         from xarray.core.missing import interp_na
3420 
3421         return interp_na(
3422             self,
3423             dim=dim,
3424             method=method,
3425             limit=limit,
3426             use_coordinate=use_coordinate,
3427             max_gap=max_gap,
3428             keep_attrs=keep_attrs,
3429             **kwargs,
3430         )
3431 
3432     def ffill(
3433         self: T_DataArray, dim: Hashable, limit: int | None = None
3434     ) -> T_DataArray:
3435         """Fill NaN values by propagating values forward
3436 
3437         *Requires bottleneck.*
3438 
3439         Parameters
3440         ----------
3441         dim : Hashable
3442             Specifies the dimension along which to propagate values when
3443             filling.
3444         limit : int or None, default: None
3445             The maximum number of consecutive NaN values to forward fill. In
3446             other words, if there is a gap with more than this number of
3447             consecutive NaNs, it will only be partially filled. Must be greater
3448             than 0 or None for no limit. Must be None or greater than or equal
3449             to axis length if filling along chunked axes (dimensions).
3450 
3451         Returns
3452         -------
3453         filled : DataArray
3454 
3455         Examples
3456         --------
3457         >>> temperature = np.array(
3458         ...     [
3459         ...         [np.nan, 1, 3],
3460         ...         [0, np.nan, 5],
3461         ...         [5, np.nan, np.nan],
3462         ...         [3, np.nan, np.nan],
3463         ...         [0, 2, 0],
3464         ...     ]
3465         ... )
3466         >>> da = xr.DataArray(
3467         ...     data=temperature,
3468         ...     dims=["Y", "X"],
3469         ...     coords=dict(
3470         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3471         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3472         ...     ),
3473         ... )
3474         >>> da
3475         <xarray.DataArray (Y: 5, X: 3)>
3476         array([[nan,  1.,  3.],
3477                [ 0., nan,  5.],
3478                [ 5., nan, nan],
3479                [ 3., nan, nan],
3480                [ 0.,  2.,  0.]])
3481         Coordinates:
3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3483             lon      (X) float64 10.0 10.25 10.5
3484         Dimensions without coordinates: Y, X
3485 
3486         Fill all NaN values:
3487 
3488         >>> da.ffill(dim="Y", limit=None)
3489         <xarray.DataArray (Y: 5, X: 3)>
3490         array([[nan,  1.,  3.],
3491                [ 0.,  1.,  5.],
3492                [ 5.,  1.,  5.],
3493                [ 3.,  1.,  5.],
3494                [ 0.,  2.,  0.]])
3495         Coordinates:
3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3497             lon      (X) float64 10.0 10.25 10.5
3498         Dimensions without coordinates: Y, X
3499 
3500         Fill only the first of consecutive NaN values:
3501 
3502         >>> da.ffill(dim="Y", limit=1)
3503         <xarray.DataArray (Y: 5, X: 3)>
3504         array([[nan,  1.,  3.],
3505                [ 0.,  1.,  5.],
3506                [ 5., nan,  5.],
3507                [ 3., nan, nan],
3508                [ 0.,  2.,  0.]])
3509         Coordinates:
3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3511             lon      (X) float64 10.0 10.25 10.5
3512         Dimensions without coordinates: Y, X
3513         """
3514         from xarray.core.missing import ffill
3515 
3516         return ffill(self, dim, limit=limit)
3517 
3518     def bfill(
3519         self: T_DataArray, dim: Hashable, limit: int | None = None
3520     ) -> T_DataArray:
3521         """Fill NaN values by propagating values backward
3522 
3523         *Requires bottleneck.*
3524 
3525         Parameters
3526         ----------
3527         dim : str
3528             Specifies the dimension along which to propagate values when
3529             filling.
3530         limit : int or None, default: None
3531             The maximum number of consecutive NaN values to backward fill. In
3532             other words, if there is a gap with more than this number of
3533             consecutive NaNs, it will only be partially filled. Must be greater
3534             than 0 or None for no limit. Must be None or greater than or equal
3535             to axis length if filling along chunked axes (dimensions).
3536 
3537         Returns
3538         -------
3539         filled : DataArray
3540 
3541         Examples
3542         --------
3543         >>> temperature = np.array(
3544         ...     [
3545         ...         [0, 1, 3],
3546         ...         [0, np.nan, 5],
3547         ...         [5, np.nan, np.nan],
3548         ...         [3, np.nan, np.nan],
3549         ...         [np.nan, 2, 0],
3550         ...     ]
3551         ... )
3552         >>> da = xr.DataArray(
3553         ...     data=temperature,
3554         ...     dims=["Y", "X"],
3555         ...     coords=dict(
3556         ...         lat=("Y", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),
3557         ...         lon=("X", np.array([10.0, 10.25, 10.5])),
3558         ...     ),
3559         ... )
3560         >>> da
3561         <xarray.DataArray (Y: 5, X: 3)>
3562         array([[ 0.,  1.,  3.],
3563                [ 0., nan,  5.],
3564                [ 5., nan, nan],
3565                [ 3., nan, nan],
3566                [nan,  2.,  0.]])
3567         Coordinates:
3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3569             lon      (X) float64 10.0 10.25 10.5
3570         Dimensions without coordinates: Y, X
3571 
3572         Fill all NaN values:
3573 
3574         >>> da.bfill(dim="Y", limit=None)
3575         <xarray.DataArray (Y: 5, X: 3)>
3576         array([[ 0.,  1.,  3.],
3577                [ 0.,  2.,  5.],
3578                [ 5.,  2.,  0.],
3579                [ 3.,  2.,  0.],
3580                [nan,  2.,  0.]])
3581         Coordinates:
3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3583             lon      (X) float64 10.0 10.25 10.5
3584         Dimensions without coordinates: Y, X
3585 
3586         Fill only the first of consecutive NaN values:
3587 
3588         >>> da.bfill(dim="Y", limit=1)
3589         <xarray.DataArray (Y: 5, X: 3)>
3590         array([[ 0.,  1.,  3.],
3591                [ 0., nan,  5.],
3592                [ 5., nan, nan],
3593                [ 3.,  2.,  0.],
3594                [nan,  2.,  0.]])
3595         Coordinates:
3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0
3597             lon      (X) float64 10.0 10.25 10.5
3598         Dimensions without coordinates: Y, X
3599         """
3600         from xarray.core.missing import bfill
3601 
3602         return bfill(self, dim, limit=limit)
3603 
3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3605         """Combine two DataArray objects, with union of coordinates.
3606 
3607         This operation follows the normal broadcasting and alignment rules of
3608         ``join='outer'``.  Default to non-null values of array calling the
3609         method.  Use np.nan to fill in vacant cells after alignment.
3610 
3611         Parameters
3612         ----------
3613         other : DataArray
3614             Used to fill all matching missing values in this array.
3615 
3616         Returns
3617         -------
3618         DataArray
3619         """
3620         return ops.fillna(self, other, join="outer")
3621 
3622     def reduce(
3623         self: T_DataArray,
3624         func: Callable[..., Any],
3625         dim: Dims = None,
3626         *,
3627         axis: int | Sequence[int] | None = None,
3628         keep_attrs: bool | None = None,
3629         keepdims: bool = False,
3630         **kwargs: Any,
3631     ) -> T_DataArray:
3632         """Reduce this array by applying `func` along some dimension(s).
3633 
3634         Parameters
3635         ----------
3636         func : callable
3637             Function which can be called in the form
3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an
3639             np.ndarray over an integer valued axis.
3640         dim : "...", str, Iterable of Hashable or None, optional
3641             Dimension(s) over which to apply `func`. By default `func` is
3642             applied over all dimensions.
3643         axis : int or sequence of int, optional
3644             Axis(es) over which to repeatedly apply `func`. Only one of the
3645             'dim' and 'axis' arguments can be supplied. If neither are
3646             supplied, then the reduction is calculated over the flattened array
3647             (by calling `f(x)` without an axis argument).
3648         keep_attrs : bool or None, optional
3649             If True, the variable's attributes (`attrs`) will be copied from
3650             the original object to the new one.  If False (default), the new
3651             object will be returned without attributes.
3652         keepdims : bool, default: False
3653             If True, the dimensions which are reduced are left in the result
3654             as dimensions of size one. Coordinates that use these dimensions
3655             are removed.
3656         **kwargs : dict
3657             Additional keyword arguments passed on to `func`.
3658 
3659         Returns
3660         -------
3661         reduced : DataArray
3662             DataArray with this object's array replaced with an array with
3663             summarized data and the indicated dimension(s) removed.
3664         """
3665 
3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
3667         return self._replace_maybe_drop_dims(var)
3668 
3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
3670         """Convert this array into a pandas object with the same shape.
3671 
3672         The type of the returned object depends on the number of DataArray
3673         dimensions:
3674 
3675         * 0D -> `xarray.DataArray`
3676         * 1D -> `pandas.Series`
3677         * 2D -> `pandas.DataFrame`
3678 
3679         Only works for arrays with 2 or fewer dimensions.
3680 
3681         The DataArray constructor performs the inverse transformation.
3682 
3683         Returns
3684         -------
3685         result : DataArray | Series | DataFrame
3686             DataArray, pandas Series or pandas DataFrame.
3687         """
3688         # TODO: consolidate the info about pandas constructors and the
3689         # attributes that correspond to their indexes into a separate module?
3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
3691         try:
3692             constructor = constructors[self.ndim]
3693         except KeyError:
3694             raise ValueError(
3695                 f"Cannot convert arrays with {self.ndim} dimensions into "
3696                 "pandas objects. Requires 2 or fewer dimensions."
3697             )
3698         indexes = [self.get_index(dim) for dim in self.dims]
3699         return constructor(self.values, *indexes)
3700 
3701     def to_dataframe(
3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
3703     ) -> pd.DataFrame:
3704         """Convert this array and its coordinates into a tidy pandas.DataFrame.
3705 
3706         The DataFrame is indexed by the Cartesian product of index coordinates
3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
3708         included as columns in the DataFrame.
3709 
3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
3711         doesn't rely on a MultiIndex to build the DataFrame.
3712 
3713         Parameters
3714         ----------
3715         name: Hashable or None, optional
3716             Name to give to this array (required if unnamed).
3717         dim_order: Sequence of Hashable or None, optional
3718             Hierarchical dimension order for the resulting dataframe.
3719             Array content is transposed to this order and then written out as flat
3720             vectors in contiguous order, so the last dimension in this list
3721             will be contiguous in the resulting DataFrame. This has a major
3722             influence on which operations are efficient on the resulting
3723             dataframe.
3724 
3725             If provided, must include all dimensions of this DataArray. By default,
3726             dimensions are sorted according to the DataArray dimensions order.
3727 
3728         Returns
3729         -------
3730         result: DataFrame
3731             DataArray as a pandas DataFrame.
3732 
3733         See also
3734         --------
3735         DataArray.to_pandas
3736         DataArray.to_series
3737         """
3738         if name is None:
3739             name = self.name
3740         if name is None:
3741             raise ValueError(
3742                 "cannot convert an unnamed DataArray to a "
3743                 "DataFrame: use the ``name`` parameter"
3744             )
3745         if self.ndim == 0:
3746             raise ValueError("cannot convert a scalar to a DataFrame")
3747 
3748         # By using a unique name, we can convert a DataArray into a DataFrame
3749         # even if it shares a name with one of its coordinates.
3750         # I would normally use unique_name = object() but that results in a
3751         # dataframe with columns in the wrong order, for reasons I have not
3752         # been able to debug (possibly a pandas bug?).
3753         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3754         ds = self._to_dataset_whole(name=unique_name)
3755 
3756         if dim_order is None:
3757             ordered_dims = dict(zip(self.dims, self.shape))
3758         else:
3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3760 
3761         df = ds._to_dataframe(ordered_dims)
3762         df.columns = [name if c == unique_name else c for c in df.columns]
3763         return df
3764 
3765     def to_series(self) -> pd.Series:
3766         """Convert this array into a pandas.Series.
3767 
3768         The Series is indexed by the Cartesian product of index coordinates
3769         (in the form of a :py:class:`pandas.MultiIndex`).
3770 
3771         Returns
3772         -------
3773         result : Series
3774             DataArray as a pandas Series.
3775 
3776         See also
3777         --------
3778         DataArray.to_pandas
3779         DataArray.to_dataframe
3780         """
3781         index = self.coords.to_index()
3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3783 
3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3785         """Convert this array into a numpy.ma.MaskedArray
3786 
3787         Parameters
3788         ----------
3789         copy : bool, default: True
3790             If True make a copy of the array in the result. If False,
3791             a MaskedArray view of DataArray.values is returned.
3792 
3793         Returns
3794         -------
3795         result : MaskedArray
3796             Masked where invalid values (nan or inf) occur.
3797         """
3798         values = self.to_numpy()  # only compute lazy arrays once
3799         isnull = pd.isnull(values)
3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3801 
3802     # path=None writes to bytes
3803     @overload
3804     def to_netcdf(
3805         self,
3806         path: None = None,
3807         mode: Literal["w", "a"] = "w",
3808         format: T_NetcdfTypes | None = None,
3809         group: str | None = None,
3810         engine: T_NetcdfEngine | None = None,
3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3812         unlimited_dims: Iterable[Hashable] | None = None,
3813         compute: bool = True,
3814         invalid_netcdf: bool = False,
3815     ) -> bytes:
3816         ...
3817 
3818     # default return None
3819     @overload
3820     def to_netcdf(
3821         self,
3822         path: str | PathLike,
3823         mode: Literal["w", "a"] = "w",
3824         format: T_NetcdfTypes | None = None,
3825         group: str | None = None,
3826         engine: T_NetcdfEngine | None = None,
3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3828         unlimited_dims: Iterable[Hashable] | None = None,
3829         compute: Literal[True] = True,
3830         invalid_netcdf: bool = False,
3831     ) -> None:
3832         ...
3833 
3834     # compute=False returns dask.Delayed
3835     @overload
3836     def to_netcdf(
3837         self,
3838         path: str | PathLike,
3839         mode: Literal["w", "a"] = "w",
3840         format: T_NetcdfTypes | None = None,
3841         group: str | None = None,
3842         engine: T_NetcdfEngine | None = None,
3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3844         unlimited_dims: Iterable[Hashable] | None = None,
3845         *,
3846         compute: Literal[False],
3847         invalid_netcdf: bool = False,
3848     ) -> Delayed:
3849         ...
3850 
3851     def to_netcdf(
3852         self,
3853         path: str | PathLike | None = None,
3854         mode: Literal["w", "a"] = "w",
3855         format: T_NetcdfTypes | None = None,
3856         group: str | None = None,
3857         engine: T_NetcdfEngine | None = None,
3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3859         unlimited_dims: Iterable[Hashable] | None = None,
3860         compute: bool = True,
3861         invalid_netcdf: bool = False,
3862     ) -> bytes | Delayed | None:
3863         """Write DataArray contents to a netCDF file.
3864 
3865         Parameters
3866         ----------
3867         path : str, path-like or None, optional
3868             Path to which to save this dataset. File-like objects are only
3869             supported by the scipy engine. If no path is provided, this
3870             function returns the resulting netCDF file as bytes; in this case,
3871             we need to use scipy, which does not support netCDF version 4 (the
3872             default format becomes NETCDF3_64BIT).
3873         mode : {"w", "a"}, default: "w"
3874             Write ('w') or append ('a') mode. If mode='w', any existing file at
3875             this location will be overwritten. If mode='a', existing variables
3876             will be overwritten.
3877         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3878                   "NETCDF3_CLASSIC"}, optional
3879             File format for the resulting netCDF file:
3880 
3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3882               features.
3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3884               netCDF 3 compatible API features.
3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3886               which fully supports 2+ GB files, but is only compatible with
3887               clients linked against netCDF version 3.6.0 or later.
3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3889               handle 2+ GB files very well.
3890 
3891             All formats are supported by the netCDF4-python library.
3892             scipy.io.netcdf only supports the last two formats.
3893 
3894             The default format is NETCDF4 if you are saving a file to disk and
3895             have the netCDF4-python library available. Otherwise, xarray falls
3896             back to using scipy to write netCDF files and defaults to the
3897             NETCDF3_64BIT format (scipy does not support netCDF4).
3898         group : str, optional
3899             Path to the netCDF4 group in the given file to open (only works for
3900             format='NETCDF4'). The group(s) will be created if necessary.
3901         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3902             Engine to use when writing netCDF files. If not provided, the
3903             default engine is chosen based on available dependencies, with a
3904             preference for 'netcdf4' if writing to a file on disk.
3905         encoding : dict, optional
3906             Nested dictionary with variable names as keys and dictionaries of
3907             variable specific encodings as values, e.g.,
3908             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3909             "zlib": True}, ...}``
3910 
3911             The `h5netcdf` engine supports both the NetCDF4-style compression
3912             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3913             ones ``{"compression": "gzip", "compression_opts": 9}``.
3914             This allows using any compression plugin installed in the HDF5
3915             library, e.g. LZF.
3916 
3917         unlimited_dims : iterable of Hashable, optional
3918             Dimension(s) that should be serialized as unlimited dimensions.
3919             By default, no dimensions are treated as unlimited dimensions.
3920             Note that unlimited_dims may also be set via
3921             ``dataset.encoding["unlimited_dims"]``.
3922         compute: bool, default: True
3923             If true compute immediately, otherwise return a
3924             ``dask.delayed.Delayed`` object that can be computed later.
3925         invalid_netcdf: bool, default: False
3926             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3927             hdf5 files which are invalid netcdf as described in
3928             https://github.com/h5netcdf/h5netcdf.
3929 
3930         Returns
3931         -------
3932         store: bytes or Delayed or None
3933             * ``bytes`` if path is None
3934             * ``dask.delayed.Delayed`` if compute is False
3935             * None otherwise
3936 
3937         Notes
3938         -----
3939         Only xarray.Dataset objects can be written to netCDF files, so
3940         the xarray.DataArray is converted to a xarray.Dataset object
3941         containing a single variable. If the DataArray has no name, or if the
3942         name is the same as a coordinate name, then it is given the name
3943         ``"__xarray_dataarray_variable__"``.
3944 
3945         See Also
3946         --------
3947         Dataset.to_netcdf
3948         """
3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3950 
3951         if self.name is None:
3952             # If no name is set then use a generic xarray name
3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3954         elif self.name in self.coords or self.name in self.dims:
3955             # The name is the same as one of the coords names, which netCDF
3956             # doesn't support, so rename it but keep track of the old name
3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3958             dataset.attrs[DATAARRAY_NAME] = self.name
3959         else:
3960             # No problems with the name - so we're fine!
3961             dataset = self.to_dataset()
3962 
3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3964             dataset,
3965             path,
3966             mode=mode,
3967             format=format,
3968             group=group,
3969             engine=engine,
3970             encoding=encoding,
3971             unlimited_dims=unlimited_dims,
3972             compute=compute,
3973             multifile=False,
3974             invalid_netcdf=invalid_netcdf,
3975         )
3976 
3977     # compute=True (default) returns ZarrStore
3978     @overload
3979     def to_zarr(
3980         self,
3981         store: MutableMapping | str | PathLike[str] | None = None,
3982         chunk_store: MutableMapping | str | PathLike | None = None,
3983         mode: Literal["w", "w-", "a", "r+", None] = None,
3984         synchronizer=None,
3985         group: str | None = None,
3986         encoding: Mapping | None = None,
3987         compute: Literal[True] = True,
3988         consolidated: bool | None = None,
3989         append_dim: Hashable | None = None,
3990         region: Mapping[str, slice] | None = None,
3991         safe_chunks: bool = True,
3992         storage_options: dict[str, str] | None = None,
3993         zarr_version: int | None = None,
3994     ) -> ZarrStore:
3995         ...
3996 
3997     # compute=False returns dask.Delayed
3998     @overload
3999     def to_zarr(
4000         self,
4001         store: MutableMapping | str | PathLike[str] | None = None,
4002         chunk_store: MutableMapping | str | PathLike | None = None,
4003         mode: Literal["w", "w-", "a", "r+", None] = None,
4004         synchronizer=None,
4005         group: str | None = None,
4006         encoding: Mapping | None = None,
4007         *,
4008         compute: Literal[False],
4009         consolidated: bool | None = None,
4010         append_dim: Hashable | None = None,
4011         region: Mapping[str, slice] | None = None,
4012         safe_chunks: bool = True,
4013         storage_options: dict[str, str] | None = None,
4014         zarr_version: int | None = None,
4015     ) -> Delayed:
4016         ...
4017 
4018     def to_zarr(
4019         self,
4020         store: MutableMapping | str | PathLike[str] | None = None,
4021         chunk_store: MutableMapping | str | PathLike | None = None,
4022         mode: Literal["w", "w-", "a", "r+", None] = None,
4023         synchronizer=None,
4024         group: str | None = None,
4025         encoding: Mapping | None = None,
4026         compute: bool = True,
4027         consolidated: bool | None = None,
4028         append_dim: Hashable | None = None,
4029         region: Mapping[str, slice] | None = None,
4030         safe_chunks: bool = True,
4031         storage_options: dict[str, str] | None = None,
4032         zarr_version: int | None = None,
4033     ) -> ZarrStore | Delayed:
4034         """Write DataArray contents to a Zarr store
4035 
4036         Zarr chunks are determined in the following way:
4037 
4038         - From the ``chunks`` attribute in each variable's ``encoding``
4039           (can be set via `DataArray.chunk`).
4040         - If the variable is a Dask array, from the dask chunks
4041         - If neither Dask chunks nor encoding chunks are present, chunks will
4042           be determined automatically by Zarr
4043         - If both Dask chunks and encoding chunks are present, encoding chunks
4044           will be used, provided that there is a many-to-one relationship between
4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.
4047           This restriction ensures that no synchronization / locks are required
4048           when writing. To disable this restriction, use ``safe_chunks=False``.
4049 
4050         Parameters
4051         ----------
4052         store : MutableMapping, str or path-like, optional
4053             Store or path to directory in local or remote file system.
4054         chunk_store : MutableMapping, str or path-like, optional
4055             Store or path to directory in local or remote file system only for Zarr
4056             array chunks. Requires zarr-python v2.4.0 or later.
4057         mode : {"w", "w-", "a", "r+", None}, optional
4058             Persistence mode: "w" means create (overwrite if exists);
4059             "w-" means create (fail if exists);
4060             "a" means override existing variables (create if does not exist);
4061             "r+" means modify existing array *values* only (raise an error if
4062             any metadata or shapes would change).
4063             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
4064             "r+" if ``region`` is set and ``w-`` otherwise.
4065         synchronizer : object, optional
4066             Zarr array synchronizer.
4067         group : str, optional
4068             Group path. (a.k.a. `path` in zarr terminology.)
4069         encoding : dict, optional
4070             Nested dictionary with variable names as keys and dictionaries of
4071             variable specific encodings as values, e.g.,
4072             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
4073         compute : bool, default: True
4074             If True write array data immediately, otherwise return a
4075             ``dask.delayed.Delayed`` object that can be computed to write
4076             array data later. Metadata is always updated eagerly.
4077         consolidated : bool, optional
4078             If True, apply zarr's `consolidate_metadata` function to the store
4079             after writing metadata and read existing stores with consolidated
4080             metadata; if False, do not. The default (`consolidated=None`) means
4081             write consolidated metadata and attempt to read consolidated
4082             metadata for existing stores (falling back to non-consolidated).
4083 
4084             When the experimental ``zarr_version=3``, ``consolidated`` must be
4085             either be ``None`` or ``False``.
4086         append_dim : hashable, optional
4087             If set, the dimension along which the data will be appended. All
4088             other dimensions on overridden variables must remain the same size.
4089         region : dict, optional
4090             Optional mapping from dimension names to integer slices along
4091             dataarray dimensions to indicate the region of existing zarr array(s)
4092             in which to write this datarray's data. For example,
4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
4094             that values should be written to the region ``0:1000`` along ``x``
4095             and ``10000:11000`` along ``y``.
4096 
4097             Two restrictions apply to the use of ``region``:
4098 
4099             - If ``region`` is set, _all_ variables in a dataarray must have at
4100               least one dimension in common with the region. Other variables
4101               should be written in a separate call to ``to_zarr()``.
4102             - Dimensions cannot be included in both ``region`` and
4103               ``append_dim`` at the same time. To create empty arrays to fill
4104               in with ``region``, use a separate call to ``to_zarr()`` with
4105               ``compute=False``. See "Appending to existing Zarr stores" in
4106               the reference documentation for full details.
4107         safe_chunks : bool, default: True
4108             If True, only allow writes to when there is a many-to-one relationship
4109             between Zarr chunks (specified in encoding) and Dask chunks.
4110             Set False to override this restriction; however, data may become corrupted
4111             if Zarr arrays are written in parallel. This option may be useful in combination
4112             with ``compute=False`` to initialize a Zarr store from an existing
4113             DataArray with arbitrary chunk structure.
4114         storage_options : dict, optional
4115             Any additional parameters for the storage backend (ignored for local
4116             paths).
4117         zarr_version : int or None, optional
4118             The desired zarr spec version to target (currently 2 or 3). The
4119             default of None will attempt to determine the zarr version from
4120             ``store`` when possible, otherwise defaulting to 2.
4121 
4122         Returns
4123         -------
4124             * ``dask.delayed.Delayed`` if compute is False
4125             * ZarrStore otherwise
4126 
4127         References
4128         ----------
4129         https://zarr.readthedocs.io/
4130 
4131         Notes
4132         -----
4133         Zarr chunking behavior:
4134             If chunks are found in the encoding argument or attribute
4135             corresponding to any DataArray, those chunks are used.
4136             If a DataArray is a dask array, it is written with those chunks.
4137             If not other chunks are found, Zarr uses its own heuristics to
4138             choose automatic chunk sizes.
4139 
4140         encoding:
4141             The encoding attribute (if exists) of the DataArray(s) will be
4142             used. Override any existing encodings by providing the ``encoding`` kwarg.
4143 
4144         See Also
4145         --------
4146         Dataset.to_zarr
4147         :ref:`io.zarr`
4148             The I/O user guide, with more details and examples.
4149         """
4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr
4151 
4152         if self.name is None:
4153             # If no name is set then use a generic xarray name
4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4155         elif self.name in self.coords or self.name in self.dims:
4156             # The name is the same as one of the coords names, which the netCDF data model
4157             # does not support, so rename it but keep track of the old name
4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
4159             dataset.attrs[DATAARRAY_NAME] = self.name
4160         else:
4161             # No problems with the name - so we're fine!
4162             dataset = self.to_dataset()
4163 
4164         return to_zarr(  # type: ignore[call-overload,misc]
4165             dataset,
4166             store=store,
4167             chunk_store=chunk_store,
4168             mode=mode,
4169             synchronizer=synchronizer,
4170             group=group,
4171             encoding=encoding,
4172             compute=compute,
4173             consolidated=consolidated,
4174             append_dim=append_dim,
4175             region=region,
4176             safe_chunks=safe_chunks,
4177             storage_options=storage_options,
4178             zarr_version=zarr_version,
4179         )
4180 
4181     def to_dict(
4182         self, data: bool | Literal["list", "array"] = "list", encoding: bool = False
4183     ) -> dict[str, Any]:
4184         """
4185         Convert this xarray.DataArray into a dictionary following xarray
4186         naming conventions.
4187 
4188         Converts all variables and attributes to native Python objects.
4189         Useful for converting to json. To avoid datetime incompatibility
4190         use decode_times=False kwarg in xarray.open_dataset.
4191 
4192         Parameters
4193         ----------
4194         data : bool or {"list", "array"}, default: "list"
4195             Whether to include the actual data in the dictionary. When set to
4196             False, returns just the schema. If set to "array", returns data as
4197             underlying array type. If set to "list" (or True for backwards
4198             compatibility), returns data in lists of Python data types. Note
4199             that for obtaining the "list" output efficiently, use
4200             `da.compute().to_dict(data="list")`.
4201 
4202         encoding : bool, default: False
4203             Whether to include the Dataset's encoding in the dictionary.
4204 
4205         Returns
4206         -------
4207         dict: dict
4208 
4209         See Also
4210         --------
4211         DataArray.from_dict
4212         Dataset.to_dict
4213         """
4214         d = self.variable.to_dict(data=data)
4215         d.update({"coords": {}, "name": self.name})
4216         for k, coord in self.coords.items():
4217             d["coords"][k] = coord.variable.to_dict(data=data)
4218         if encoding:
4219             d["encoding"] = dict(self.encoding)
4220         return d
4221 
4222     @classmethod
4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
4224         """Convert a dictionary into an xarray.DataArray
4225 
4226         Parameters
4227         ----------
4228         d : dict
4229             Mapping with a minimum structure of {"dims": [...], "data": [...]}
4230 
4231         Returns
4232         -------
4233         obj : xarray.DataArray
4234 
4235         See Also
4236         --------
4237         DataArray.to_dict
4238         Dataset.from_dict
4239 
4240         Examples
4241         --------
4242         >>> d = {"dims": "t", "data": [1, 2, 3]}
4243         >>> da = xr.DataArray.from_dict(d)
4244         >>> da
4245         <xarray.DataArray (t: 3)>
4246         array([1, 2, 3])
4247         Dimensions without coordinates: t
4248 
4249         >>> d = {
4250         ...     "coords": {
4251         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
4252         ...     },
4253         ...     "attrs": {"title": "air temperature"},
4254         ...     "dims": "t",
4255         ...     "data": [10, 20, 30],
4256         ...     "name": "a",
4257         ... }
4258         >>> da = xr.DataArray.from_dict(d)
4259         >>> da
4260         <xarray.DataArray 'a' (t: 3)>
4261         array([10, 20, 30])
4262         Coordinates:
4263           * t        (t) int64 0 1 2
4264         Attributes:
4265             title:    air temperature
4266         """
4267         coords = None
4268         if "coords" in d:
4269             try:
4270                 coords = {
4271                     k: (v["dims"], v["data"], v.get("attrs"))
4272                     for k, v in d["coords"].items()
4273                 }
4274             except KeyError as e:
4275                 raise ValueError(
4276                     "cannot convert dict when coords are missing the key "
4277                     "'{dims_data}'".format(dims_data=str(e.args[0]))
4278                 )
4279         try:
4280             data = d["data"]
4281         except KeyError:
4282             raise ValueError("cannot convert dict without the key 'data''")
4283         else:
4284             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
4285 
4286         obj.encoding.update(d.get("encoding", {}))
4287 
4288         return obj
4289 
4290     @classmethod
4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
4292         """Convert a pandas.Series into an xarray.DataArray.
4293 
4294         If the series's index is a MultiIndex, it will be expanded into a
4295         tensor product of one-dimensional coordinates (filling in missing
4296         values with NaN). Thus this operation should be the inverse of the
4297         `to_series` method.
4298 
4299         Parameters
4300         ----------
4301         series : Series
4302             Pandas Series object to convert.
4303         sparse : bool, default: False
4304             If sparse=True, creates a sparse array instead of a dense NumPy array.
4305             Requires the pydata/sparse package.
4306 
4307         See Also
4308         --------
4309         DataArray.to_series
4310         Dataset.from_dataframe
4311         """
4312         temp_name = "__temporary_name"
4313         df = pd.DataFrame({temp_name: series})
4314         ds = Dataset.from_dataframe(df, sparse=sparse)
4315         result = cast(DataArray, ds[temp_name])
4316         result.name = series.name
4317         return result
4318 
4319     def to_cdms2(self) -> cdms2_Variable:
4320         """Convert this array into a cdms2.Variable"""
4321         from xarray.convert import to_cdms2
4322 
4323         return to_cdms2(self)
4324 
4325     @classmethod
4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
4327         """Convert a cdms2.Variable into an xarray.DataArray"""
4328         from xarray.convert import from_cdms2
4329 
4330         return from_cdms2(variable)
4331 
4332     def to_iris(self) -> iris_Cube:
4333         """Convert this array into a iris.cube.Cube"""
4334         from xarray.convert import to_iris
4335 
4336         return to_iris(self)
4337 
4338     @classmethod
4339     def from_iris(cls, cube: iris_Cube) -> DataArray:
4340         """Convert a iris.cube.Cube into an xarray.DataArray"""
4341         from xarray.convert import from_iris
4342 
4343         return from_iris(cube)
4344 
4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
4346         """Helper function for equals, broadcast_equals, and identical"""
4347 
4348         def compat(x, y):
4349             return getattr(x.variable, compat_str)(y.variable)
4350 
4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
4352             self, other
4353         )
4354 
4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
4356         """Two DataArrays are broadcast equal if they are equal after
4357         broadcasting them against each other such that they have the same
4358         dimensions.
4359 
4360         Parameters
4361         ----------
4362         other : DataArray
4363             DataArray to compare to.
4364 
4365         Returns
4366         ----------
4367         equal : bool
4368             True if the two DataArrays are broadcast equal.
4369 
4370         See Also
4371         --------
4372         DataArray.equals
4373         DataArray.identical
4374 
4375         Examples
4376         --------
4377         >>> a = xr.DataArray([1, 2], dims="X")
4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=["X", "Y"])
4379         >>> a
4380         <xarray.DataArray (X: 2)>
4381         array([1, 2])
4382         Dimensions without coordinates: X
4383         >>> b
4384         <xarray.DataArray (X: 2, Y: 2)>
4385         array([[1, 1],
4386                [2, 2]])
4387         Dimensions without coordinates: X, Y
4388 
4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.
4390 
4391         >>> a.equals(b)
4392         False
4393         >>> a2, b2 = xr.broadcast(a, b)
4394         >>> a2.equals(b2)
4395         True
4396         >>> a.broadcast_equals(b)
4397         True
4398         """
4399         try:
4400             return self._all_compat(other, "broadcast_equals")
4401         except (TypeError, AttributeError):
4402             return False
4403 
4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:
4405         """True if two DataArrays have the same dimensions, coordinates and
4406         values; otherwise False.
4407 
4408         DataArrays can still be equal (like pandas objects) if they have NaN
4409         values in the same locations.
4410 
4411         This method is necessary because `v1 == v2` for ``DataArray``
4412         does element-wise comparisons (like numpy.ndarrays).
4413 
4414         Parameters
4415         ----------
4416         other : DataArray
4417             DataArray to compare to.
4418 
4419         Returns
4420         ----------
4421         equal : bool
4422             True if the two DataArrays are equal.
4423 
4424         See Also
4425         --------
4426         DataArray.broadcast_equals
4427         DataArray.identical
4428 
4429         Examples
4430         --------
4431         >>> a = xr.DataArray([1, 2, 3], dims="X")
4432         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"))
4433         >>> c = xr.DataArray([1, 2, 3], dims="Y")
4434         >>> d = xr.DataArray([3, 2, 1], dims="X")
4435         >>> a
4436         <xarray.DataArray (X: 3)>
4437         array([1, 2, 3])
4438         Dimensions without coordinates: X
4439         >>> b
4440         <xarray.DataArray (X: 3)>
4441         array([1, 2, 3])
4442         Dimensions without coordinates: X
4443         Attributes:
4444             units:    m
4445         >>> c
4446         <xarray.DataArray (Y: 3)>
4447         array([1, 2, 3])
4448         Dimensions without coordinates: Y
4449         >>> d
4450         <xarray.DataArray (X: 3)>
4451         array([3, 2, 1])
4452         Dimensions without coordinates: X
4453 
4454         >>> a.equals(b)
4455         True
4456         >>> a.equals(c)
4457         False
4458         >>> a.equals(d)
4459         False
4460         """
4461         try:
4462             return self._all_compat(other, "equals")
4463         except (TypeError, AttributeError):
4464             return False
4465 
4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:
4467         """Like equals, but also checks the array name and attributes, and
4468         attributes on all coordinates.
4469 
4470         Parameters
4471         ----------
4472         other : DataArray
4473             DataArray to compare to.
4474 
4475         Returns
4476         ----------
4477         equal : bool
4478             True if the two DataArrays are identical.
4479 
4480         See Also
4481         --------
4482         DataArray.broadcast_equals
4483         DataArray.equals
4484 
4485         Examples
4486         --------
4487         >>> a = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4488         >>> b = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="m"), name="Width")
4489         >>> c = xr.DataArray([1, 2, 3], dims="X", attrs=dict(units="ft"), name="Width")
4490         >>> a
4491         <xarray.DataArray 'Width' (X: 3)>
4492         array([1, 2, 3])
4493         Dimensions without coordinates: X
4494         Attributes:
4495             units:    m
4496         >>> b
4497         <xarray.DataArray 'Width' (X: 3)>
4498         array([1, 2, 3])
4499         Dimensions without coordinates: X
4500         Attributes:
4501             units:    m
4502         >>> c
4503         <xarray.DataArray 'Width' (X: 3)>
4504         array([1, 2, 3])
4505         Dimensions without coordinates: X
4506         Attributes:
4507             units:    ft
4508 
4509         >>> a.equals(b)
4510         True
4511         >>> a.identical(b)
4512         True
4513 
4514         >>> a.equals(c)
4515         True
4516         >>> a.identical(c)
4517         False
4518         """
4519         try:
4520             return self.name == other.name and self._all_compat(other, "identical")
4521         except (TypeError, AttributeError):
4522             return False
4523 
4524     def _result_name(self, other: Any = None) -> Hashable | None:
4525         # use the same naming heuristics as pandas:
4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
4527         other_name = getattr(other, "name", _default)
4528         if other_name is _default or other_name == self.name:
4529             return self.name
4530         else:
4531             return None
4532 
4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
4534         new_var = self.variable.__array_wrap__(obj, context)
4535         return self._replace(new_var)
4536 
4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
4538         return self.dot(obj)
4539 
4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
4541         # currently somewhat duplicative, as only other DataArrays are
4542         # compatible with matmul
4543         return computation.dot(other, self)
4544 
4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
4546         keep_attrs = kwargs.pop("keep_attrs", None)
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=True)
4549         with warnings.catch_warnings():
4550             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
4551             warnings.filterwarnings(
4552                 "ignore", r"Mean of empty slice", category=RuntimeWarning
4553             )
4554             with np.errstate(all="ignore"):
4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
4556             if keep_attrs:
4557                 da.attrs = self.attrs
4558             return da
4559 
4560     def _binary_op(
4561         self: T_DataArray,
4562         other: Any,
4563         f: Callable,
4564         reflexive: bool = False,
4565     ) -> T_DataArray:
4566         from xarray.core.groupby import GroupBy
4567 
4568         if isinstance(other, (Dataset, GroupBy)):
4569             return NotImplemented
4570         if isinstance(other, DataArray):
4571             align_type = OPTIONS["arithmetic_join"]
4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
4573         other_variable = getattr(other, "variable", other)
4574         other_coords = getattr(other, "coords", None)
4575 
4576         variable = (
4577             f(self.variable, other_variable)
4578             if not reflexive
4579             else f(other_variable, self.variable)
4580         )
4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
4582         name = self._result_name(other)
4583 
4584         return self._replace(variable, coords, name, indexes=indexes)
4585 
4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
4587         from xarray.core.groupby import GroupBy
4588 
4589         if isinstance(other, GroupBy):
4590             raise TypeError(
4591                 "in-place operations between a DataArray and "
4592                 "a grouped object are not permitted"
4593             )
4594         # n.b. we can't align other to self (with other.reindex_like(self))
4595         # because `other` may be converted into floats, which would cause
4596         # in-place arithmetic to fail unpredictably. Instead, we simply
4597         # don't support automatic alignment with in-place arithmetic.
4598         other_coords = getattr(other, "coords", None)
4599         other_variable = getattr(other, "variable", other)
4600         try:
4601             with self.coords._merge_inplace(other_coords):
4602                 f(self.variable, other_variable)
4603         except MergeError as exc:
4604             raise MergeError(
4605                 "Automatic alignment is not supported for in-place operations.\n"
4606                 "Consider aligning the indices manually or using a not-in-place operation.\n"
4607                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
4608             ) from exc
4609         return self
4610 
4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
4612         self.attrs = other.attrs
4613 
4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)
4615 
4616     def _title_for_slice(self, truncate: int = 50) -> str:
4617         """
4618         If the dataarray has 1 dimensional coordinates or comes from a slice
4619         we can show that info in the title
4620 
4621         Parameters
4622         ----------
4623         truncate : int, default: 50
4624             maximum number of characters for title
4625 
4626         Returns
4627         -------
4628         title : string
4629             Can be used for plot titles
4630 
4631         """
4632         one_dims = []
4633         for dim, coord in self.coords.items():
4634             if coord.size == 1:
4635                 one_dims.append(
4636                     "{dim} = {v}{unit}".format(
4637                         dim=dim,
4638                         v=format_item(coord.values),
4639                         unit=_get_units_from_attrs(coord),
4640                     )
4641                 )
4642 
4643         title = ", ".join(one_dims)
4644         if len(title) > truncate:
4645             title = title[: (truncate - 3)] + "..."
4646 
4647         return title
4648 
4649     def diff(
4650         self: T_DataArray,
4651         dim: Hashable,
4652         n: int = 1,
4653         label: Literal["upper", "lower"] = "upper",
4654     ) -> T_DataArray:
4655         """Calculate the n-th order discrete difference along given axis.
4656 
4657         Parameters
4658         ----------
4659         dim : Hashable
4660             Dimension over which to calculate the finite difference.
4661         n : int, default: 1
4662             The number of times values are differenced.
4663         label : {"upper", "lower"}, default: "upper"
4664             The new coordinate in dimension ``dim`` will have the
4665             values of either the minuend's or subtrahend's coordinate
4666             for values 'upper' and 'lower', respectively.
4667 
4668         Returns
4669         -------
4670         difference : DataArray
4671             The n-th order finite difference of this object.
4672 
4673         Notes
4674         -----
4675         `n` matches numpy's behavior and is different from pandas' first argument named
4676         `periods`.
4677 
4678         Examples
4679         --------
4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
4681         >>> arr.diff("x")
4682         <xarray.DataArray (x: 3)>
4683         array([0, 1, 0])
4684         Coordinates:
4685           * x        (x) int64 2 3 4
4686         >>> arr.diff("x", 2)
4687         <xarray.DataArray (x: 2)>
4688         array([ 1, -1])
4689         Coordinates:
4690           * x        (x) int64 3 4
4691 
4692         See Also
4693         --------
4694         DataArray.differentiate
4695         """
4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
4697         return self._from_temp_dataset(ds)
4698 
4699     def shift(
4700         self: T_DataArray,
4701         shifts: Mapping[Any, int] | None = None,
4702         fill_value: Any = dtypes.NA,
4703         **shifts_kwargs: int,
4704     ) -> T_DataArray:
4705         """Shift this DataArray by an offset along one or more dimensions.
4706 
4707         Only the data is moved; coordinates stay in place. This is consistent
4708         with the behavior of ``shift`` in pandas.
4709 
4710         Values shifted from beyond array bounds will appear at one end of
4711         each dimension, which are filled according to `fill_value`. For periodic
4712         offsets instead see `roll`.
4713 
4714         Parameters
4715         ----------
4716         shifts : mapping of Hashable to int or None, optional
4717             Integer offset to shift along each of the given dimensions.
4718             Positive offsets shift to the right; negative offsets shift to the
4719             left.
4720         fill_value : scalar, optional
4721             Value to use for newly missing values
4722         **shifts_kwargs
4723             The keyword arguments form of ``shifts``.
4724             One of shifts or shifts_kwargs must be provided.
4725 
4726         Returns
4727         -------
4728         shifted : DataArray
4729             DataArray with the same coordinates and attributes but shifted
4730             data.
4731 
4732         See Also
4733         --------
4734         roll
4735 
4736         Examples
4737         --------
4738         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4739         >>> arr.shift(x=1)
4740         <xarray.DataArray (x: 3)>
4741         array([nan,  5.,  6.])
4742         Dimensions without coordinates: x
4743         """
4744         variable = self.variable.shift(
4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs
4746         )
4747         return self._replace(variable=variable)
4748 
4749     def roll(
4750         self: T_DataArray,
4751         shifts: Mapping[Hashable, int] | None = None,
4752         roll_coords: bool = False,
4753         **shifts_kwargs: int,
4754     ) -> T_DataArray:
4755         """Roll this array by an offset along one or more dimensions.
4756 
4757         Unlike shift, roll treats the given dimensions as periodic, so will not
4758         create any missing values to be filled.
4759 
4760         Unlike shift, roll may rotate all variables, including coordinates
4761         if specified. The direction of rotation is consistent with
4762         :py:func:`numpy.roll`.
4763 
4764         Parameters
4765         ----------
4766         shifts : mapping of Hashable to int, optional
4767             Integer offset to rotate each of the given dimensions.
4768             Positive offsets roll to the right; negative offsets roll to the
4769             left.
4770         roll_coords : bool, default: False
4771             Indicates whether to roll the coordinates by the offset too.
4772         **shifts_kwargs : {dim: offset, ...}, optional
4773             The keyword arguments form of ``shifts``.
4774             One of shifts or shifts_kwargs must be provided.
4775 
4776         Returns
4777         -------
4778         rolled : DataArray
4779             DataArray with the same attributes but rolled data and coordinates.
4780 
4781         See Also
4782         --------
4783         shift
4784 
4785         Examples
4786         --------
4787         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4788         >>> arr.roll(x=1)
4789         <xarray.DataArray (x: 3)>
4790         array([7, 5, 6])
4791         Dimensions without coordinates: x
4792         """
4793         ds = self._to_temp_dataset().roll(
4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
4795         )
4796         return self._from_temp_dataset(ds)
4797 
4798     @property
4799     def real(self: T_DataArray) -> T_DataArray:
4800         """
4801         The real part of the array.
4802 
4803         See Also
4804         --------
4805         numpy.ndarray.real
4806         """
4807         return self._replace(self.variable.real)
4808 
4809     @property
4810     def imag(self: T_DataArray) -> T_DataArray:
4811         """
4812         The imaginary part of the array.
4813 
4814         See Also
4815         --------
4816         numpy.ndarray.imag
4817         """
4818         return self._replace(self.variable.imag)
4819 
4820     def dot(
4821         self: T_DataArray,
4822         other: T_DataArray,
4823         dims: Dims = None,
4824     ) -> T_DataArray:
4825         """Perform dot product of two DataArrays along their shared dims.
4826 
4827         Equivalent to taking taking tensordot over all shared dims.
4828 
4829         Parameters
4830         ----------
4831         other : DataArray
4832             The other array with which the dot product is performed.
4833         dims : ..., str, Iterable of Hashable or None, optional
4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
4835             If not specified, then all the common dimensions are summed over.
4836 
4837         Returns
4838         -------
4839         result : DataArray
4840             Array resulting from the dot product over all shared dimensions.
4841 
4842         See Also
4843         --------
4844         dot
4845         numpy.tensordot
4846 
4847         Examples
4848         --------
4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
4850         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
4851         >>> dm_vals = np.arange(4)
4852         >>> dm = xr.DataArray(dm_vals, dims=["z"])
4853 
4854         >>> dm.dims
4855         ('z',)
4856 
4857         >>> da.dims
4858         ('x', 'y', 'z')
4859 
4860         >>> dot_result = da.dot(dm)
4861         >>> dot_result.dims
4862         ('x', 'y')
4863 
4864         """
4865         if isinstance(other, Dataset):
4866             raise NotImplementedError(
4867                 "dot products are not yet supported with Dataset objects."
4868             )
4869         if not isinstance(other, DataArray):
4870             raise TypeError("dot only operates on DataArrays.")
4871 
4872         return computation.dot(self, other, dims=dims)
4873 
4874     # change type of self and return to T_DataArray once
4875     # https://github.com/python/mypy/issues/12846 is resolved
4876     def sortby(
4877         self,
4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
4879         ascending: bool = True,
4880     ) -> DataArray:
4881         """Sort object by labels or values (along an axis).
4882 
4883         Sorts the dataarray, either along specified dimensions,
4884         or according to values of 1-D dataarrays that share dimension
4885         with calling object.
4886 
4887         If the input variables are dataarrays, then the dataarrays are aligned
4888         (via left-join) to the calling object prior to sorting by cell values.
4889         NaNs are sorted to the end, following Numpy convention.
4890 
4891         If multiple sorts along the same dimension is
4892         given, numpy's lexsort is performed along that dimension:
4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
4894         and the FIRST key in the sequence is used as the primary sort key,
4895         followed by the 2nd key, etc.
4896 
4897         Parameters
4898         ----------
4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray
4900             1D DataArray objects or name(s) of 1D variable(s) in
4901             coords whose values are used to sort this array.
4902         ascending : bool, default: True
4903             Whether to sort by ascending or descending order.
4904 
4905         Returns
4906         -------
4907         sorted : DataArray
4908             A new dataarray where all the specified dims are sorted by dim
4909             labels.
4910 
4911         See Also
4912         --------
4913         Dataset.sortby
4914         numpy.sort
4915         pandas.sort_values
4916         pandas.sort_index
4917 
4918         Examples
4919         --------
4920         >>> da = xr.DataArray(
4921         ...     np.random.rand(5),
4922         ...     coords=[pd.date_range("1/1/2000", periods=5)],
4923         ...     dims="time",
4924         ... )
4925         >>> da
4926         <xarray.DataArray (time: 5)>
4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
4928         Coordinates:
4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
4930 
4931         >>> da.sortby(da)
4932         <xarray.DataArray (time: 5)>
4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
4934         Coordinates:
4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
4936         """
4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
4938         return self._from_temp_dataset(ds)
4939 
4940     def quantile(
4941         self: T_DataArray,
4942         q: ArrayLike,
4943         dim: Dims = None,
4944         method: QuantileMethods = "linear",
4945         keep_attrs: bool | None = None,
4946         skipna: bool | None = None,
4947         interpolation: QuantileMethods | None = None,
4948     ) -> T_DataArray:
4949         """Compute the qth quantile of the data along the specified dimension.
4950 
4951         Returns the qth quantiles(s) of the array elements.
4952 
4953         Parameters
4954         ----------
4955         q : float or array-like of float
4956             Quantile to compute, which must be between 0 and 1 inclusive.
4957         dim : str or Iterable of Hashable, optional
4958             Dimension(s) over which to apply quantile.
4959         method : str, default: "linear"
4960             This optional parameter specifies the interpolation method to use when the
4961             desired quantile lies between two data points. The options sorted by their R
4962             type as summarized in the H&F paper [1]_ are:
4963 
4964                 1. "inverted_cdf" (*)
4965                 2. "averaged_inverted_cdf" (*)
4966                 3. "closest_observation" (*)
4967                 4. "interpolated_inverted_cdf" (*)
4968                 5. "hazen" (*)
4969                 6. "weibull" (*)
4970                 7. "linear"  (default)
4971                 8. "median_unbiased" (*)
4972                 9. "normal_unbiased" (*)
4973 
4974             The first three methods are discontiuous. The following discontinuous
4975             variations of the default "linear" (7.) option are also available:
4976 
4977                 * "lower"
4978                 * "higher"
4979                 * "midpoint"
4980                 * "nearest"
4981 
4982             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
4983             was previously called "interpolation", renamed in accordance with numpy
4984             version 1.22.0.
4985 
4986             (*) These methods require numpy version 1.22 or newer.
4987 
4988         keep_attrs : bool or None, optional
4989             If True, the dataset's attributes (`attrs`) will be copied from
4990             the original object to the new one.  If False (default), the new
4991             object will be returned without attributes.
4992         skipna : bool or None, optional
4993             If True, skip missing values (as marked by NaN). By default, only
4994             skips missing values for float dtypes; other dtypes either do not
4995             have a sentinel missing value (int) or skipna=True has not been
4996             implemented (object, datetime64 or timedelta64).
4997 
4998         Returns
4999         -------
5000         quantiles : DataArray
5001             If `q` is a single quantile, then the result
5002             is a scalar. If multiple percentiles are given, first axis of
5003             the result corresponds to the quantile and a quantile dimension
5004             is added to the return array. The other dimensions are the
5005             dimensions that remain after the reduction of the array.
5006 
5007         See Also
5008         --------
5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
5010 
5011         Examples
5012         --------
5013         >>> da = xr.DataArray(
5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
5015         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5016         ...     dims=("x", "y"),
5017         ... )
5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)
5019         <xarray.DataArray ()>
5020         array(0.7)
5021         Coordinates:
5022             quantile  float64 0.0
5023         >>> da.quantile(0, dim="x")
5024         <xarray.DataArray (y: 4)>
5025         array([0.7, 4.2, 2.6, 1.5])
5026         Coordinates:
5027           * y         (y) float64 1.0 1.5 2.0 2.5
5028             quantile  float64 0.0
5029         >>> da.quantile([0, 0.5, 1])
5030         <xarray.DataArray (quantile: 3)>
5031         array([0.7, 3.4, 9.4])
5032         Coordinates:
5033           * quantile  (quantile) float64 0.0 0.5 1.0
5034         >>> da.quantile([0, 0.5, 1], dim="x")
5035         <xarray.DataArray (quantile: 3, y: 4)>
5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],
5037                [3.6 , 5.75, 6.  , 1.7 ],
5038                [6.5 , 7.3 , 9.4 , 1.9 ]])
5039         Coordinates:
5040           * y         (y) float64 1.0 1.5 2.0 2.5
5041           * quantile  (quantile) float64 0.0 0.5 1.0
5042 
5043         References
5044         ----------
5045         .. [1] R. J. Hyndman and Y. Fan,
5046            "Sample quantiles in statistical packages,"
5047            The American Statistician, 50(4), pp. 361-365, 1996
5048         """
5049 
5050         ds = self._to_temp_dataset().quantile(
5051             q,
5052             dim=dim,
5053             keep_attrs=keep_attrs,
5054             method=method,
5055             skipna=skipna,
5056             interpolation=interpolation,
5057         )
5058         return self._from_temp_dataset(ds)
5059 
5060     def rank(
5061         self: T_DataArray,
5062         dim: Hashable,
5063         pct: bool = False,
5064         keep_attrs: bool | None = None,
5065     ) -> T_DataArray:
5066         """Ranks the data.
5067 
5068         Equal values are assigned a rank that is the average of the ranks that
5069         would have been otherwise assigned to all of the values within that
5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
5071 
5072         NaNs in the input array are returned as NaNs.
5073 
5074         The `bottleneck` library is required.
5075 
5076         Parameters
5077         ----------
5078         dim : Hashable
5079             Dimension over which to compute rank.
5080         pct : bool, default: False
5081             If True, compute percentage ranks, otherwise compute integer ranks.
5082         keep_attrs : bool or None, optional
5083             If True, the dataset's attributes (`attrs`) will be copied from
5084             the original object to the new one.  If False (default), the new
5085             object will be returned without attributes.
5086 
5087         Returns
5088         -------
5089         ranked : DataArray
5090             DataArray with the same coordinates and dtype 'float64'.
5091 
5092         Examples
5093         --------
5094         >>> arr = xr.DataArray([5, 6, 7], dims="x")
5095         >>> arr.rank("x")
5096         <xarray.DataArray (x: 3)>
5097         array([1., 2., 3.])
5098         Dimensions without coordinates: x
5099         """
5100 
5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
5102         return self._from_temp_dataset(ds)
5103 
5104     def differentiate(
5105         self: T_DataArray,
5106         coord: Hashable,
5107         edge_order: Literal[1, 2] = 1,
5108         datetime_unit: DatetimeUnitOptions = None,
5109     ) -> T_DataArray:
5110         """ Differentiate the array with the second order accurate central
5111         differences.
5112 
5113         .. note::
5114             This feature is limited to simple cartesian geometry, i.e. coord
5115             must be one dimensional.
5116 
5117         Parameters
5118         ----------
5119         coord : Hashable
5120             The coordinate to be used to compute the gradient.
5121         edge_order : {1, 2}, default: 1
5122             N-th order accurate differences at the boundaries.
5123         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5124                          "us", "ns", "ps", "fs", "as", None}, optional
5125             Unit to compute gradient. Only valid for datetime coordinate.
5126 
5127         Returns
5128         -------
5129         differentiated: DataArray
5130 
5131         See also
5132         --------
5133         numpy.gradient: corresponding numpy function
5134 
5135         Examples
5136         --------
5137 
5138         >>> da = xr.DataArray(
5139         ...     np.arange(12).reshape(4, 3),
5140         ...     dims=["x", "y"],
5141         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5142         ... )
5143         >>> da
5144         <xarray.DataArray (x: 4, y: 3)>
5145         array([[ 0,  1,  2],
5146                [ 3,  4,  5],
5147                [ 6,  7,  8],
5148                [ 9, 10, 11]])
5149         Coordinates:
5150           * x        (x) float64 0.0 0.1 1.1 1.2
5151         Dimensions without coordinates: y
5152         >>>
5153         >>> da.differentiate("x")
5154         <xarray.DataArray (x: 4, y: 3)>
5155         array([[30.        , 30.        , 30.        ],
5156                [27.54545455, 27.54545455, 27.54545455],
5157                [27.54545455, 27.54545455, 27.54545455],
5158                [30.        , 30.        , 30.        ]])
5159         Coordinates:
5160           * x        (x) float64 0.0 0.1 1.1 1.2
5161         Dimensions without coordinates: y
5162         """
5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
5164         return self._from_temp_dataset(ds)
5165 
5166     # change type of self and return to T_DataArray once
5167     # https://github.com/python/mypy/issues/12846 is resolved
5168     def integrate(
5169         self,
5170         coord: Hashable | Sequence[Hashable] = None,
5171         datetime_unit: DatetimeUnitOptions = None,
5172     ) -> DataArray:
5173         """Integrate along the given coordinate using the trapezoidal rule.
5174 
5175         .. note::
5176             This feature is limited to simple cartesian geometry, i.e. coord
5177             must be one dimensional.
5178 
5179         Parameters
5180         ----------
5181         coord : Hashable, or sequence of Hashable
5182             Coordinate(s) used for the integration.
5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5184                         'ps', 'fs', 'as', None}, optional
5185             Specify the unit if a datetime coordinate is used.
5186 
5187         Returns
5188         -------
5189         integrated : DataArray
5190 
5191         See also
5192         --------
5193         Dataset.integrate
5194         numpy.trapz : corresponding numpy function
5195 
5196         Examples
5197         --------
5198 
5199         >>> da = xr.DataArray(
5200         ...     np.arange(12).reshape(4, 3),
5201         ...     dims=["x", "y"],
5202         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5203         ... )
5204         >>> da
5205         <xarray.DataArray (x: 4, y: 3)>
5206         array([[ 0,  1,  2],
5207                [ 3,  4,  5],
5208                [ 6,  7,  8],
5209                [ 9, 10, 11]])
5210         Coordinates:
5211           * x        (x) float64 0.0 0.1 1.1 1.2
5212         Dimensions without coordinates: y
5213         >>>
5214         >>> da.integrate("x")
5215         <xarray.DataArray (y: 3)>
5216         array([5.4, 6.6, 7.8])
5217         Dimensions without coordinates: y
5218         """
5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
5220         return self._from_temp_dataset(ds)
5221 
5222     # change type of self and return to T_DataArray once
5223     # https://github.com/python/mypy/issues/12846 is resolved
5224     def cumulative_integrate(
5225         self,
5226         coord: Hashable | Sequence[Hashable] = None,
5227         datetime_unit: DatetimeUnitOptions = None,
5228     ) -> DataArray:
5229         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
5230 
5231         .. note::
5232             This feature is limited to simple cartesian geometry, i.e. coord
5233             must be one dimensional.
5234 
5235             The first entry of the cumulative integral is always 0, in order to keep the
5236             length of the dimension unchanged between input and output.
5237 
5238         Parameters
5239         ----------
5240         coord : Hashable, or sequence of Hashable
5241             Coordinate(s) used for the integration.
5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
5243                         'ps', 'fs', 'as', None}, optional
5244             Specify the unit if a datetime coordinate is used.
5245 
5246         Returns
5247         -------
5248         integrated : DataArray
5249 
5250         See also
5251         --------
5252         Dataset.cumulative_integrate
5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function
5254 
5255         Examples
5256         --------
5257 
5258         >>> da = xr.DataArray(
5259         ...     np.arange(12).reshape(4, 3),
5260         ...     dims=["x", "y"],
5261         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
5262         ... )
5263         >>> da
5264         <xarray.DataArray (x: 4, y: 3)>
5265         array([[ 0,  1,  2],
5266                [ 3,  4,  5],
5267                [ 6,  7,  8],
5268                [ 9, 10, 11]])
5269         Coordinates:
5270           * x        (x) float64 0.0 0.1 1.1 1.2
5271         Dimensions without coordinates: y
5272         >>>
5273         >>> da.cumulative_integrate("x")
5274         <xarray.DataArray (x: 4, y: 3)>
5275         array([[0.  , 0.  , 0.  ],
5276                [0.15, 0.25, 0.35],
5277                [4.65, 5.75, 6.85],
5278                [5.4 , 6.6 , 7.8 ]])
5279         Coordinates:
5280           * x        (x) float64 0.0 0.1 1.1 1.2
5281         Dimensions without coordinates: y
5282         """
5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
5284         return self._from_temp_dataset(ds)
5285 
5286     def unify_chunks(self) -> DataArray:
5287         """Unify chunk size along all chunked dimensions of this DataArray.
5288 
5289         Returns
5290         -------
5291         DataArray with consistent chunk sizes for all dask-array variables
5292 
5293         See Also
5294         --------
5295         dask.array.core.unify_chunks
5296         """
5297 
5298         return unify_chunks(self)[0]
5299 
5300     def map_blocks(
5301         self,
5302         func: Callable[..., T_Xarray],
5303         args: Sequence[Any] = (),
5304         kwargs: Mapping[str, Any] | None = None,
5305         template: DataArray | Dataset | None = None,
5306     ) -> T_Xarray:
5307         """
5308         Apply a function to each block of this DataArray.
5309 
5310         .. warning::
5311             This method is experimental and its signature may change.
5312 
5313         Parameters
5314         ----------
5315         func : callable
5316             User-provided function that accepts a DataArray as its first
5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),
5318             corresponding to one chunk along each chunked dimension. ``func`` will be
5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
5320 
5321             This function must return either a single DataArray or a single Dataset.
5322 
5323             This function cannot add a new chunked dimension.
5324         args : sequence
5325             Passed to func after unpacking and subsetting any xarray objects by blocks.
5326             xarray objects in args must be aligned with this object, otherwise an error is raised.
5327         kwargs : mapping
5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5329             subset to blocks. Passing dask collections in kwargs is not allowed.
5330         template : DataArray or Dataset, optional
5331             xarray object representing the final result after compute is called. If not provided,
5332             the function will be first run on mocked-up data, that looks like this object but
5333             has sizes 0, to determine properties of the returned object such as dtype,
5334             variable names, attributes, new dimensions and new indexes (if any).
5335             ``template`` must be provided if the function changes the size of existing dimensions.
5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
5337             ``attrs`` set by ``func`` will be ignored.
5338 
5339         Returns
5340         -------
5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
5342         function.
5343 
5344         Notes
5345         -----
5346         This function is designed for when ``func`` needs to manipulate a whole xarray object
5347         subset to each block. Each block is loaded into memory. In the more common case where
5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
5349 
5350         If none of the variables in this object is backed by dask arrays, calling this function is
5351         equivalent to calling ``func(obj, *args, **kwargs)``.
5352 
5353         See Also
5354         --------
5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
5356         xarray.DataArray.map_blocks
5357 
5358         Examples
5359         --------
5360         Calculate an anomaly from climatology using ``.groupby()``. Using
5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
5362         its indices, and its methods like ``.groupby()``.
5363 
5364         >>> def calculate_anomaly(da, groupby_type="time.month"):
5365         ...     gb = da.groupby(groupby_type)
5366         ...     clim = gb.mean(dim="time")
5367         ...     return gb - clim
5368         ...
5369         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
5370         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
5371         >>> np.random.seed(123)
5372         >>> array = xr.DataArray(
5373         ...     np.random.rand(len(time)),
5374         ...     dims=["time"],
5375         ...     coords={"time": time, "month": month},
5376         ... ).chunk()
5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()
5378         <xarray.DataArray (time: 24)>
5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
5384         Coordinates:
5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
5387 
5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
5389         to the function being applied in ``xr.map_blocks()``:
5390 
5391         >>> array.map_blocks(
5392         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
5393         ... )  # doctest: +ELLIPSIS
5394         <xarray.DataArray (time: 24)>
5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
5396         Coordinates:
5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
5399         """
5400         from xarray.core.parallel import map_blocks
5401 
5402         return map_blocks(func, self, args, kwargs, template)
5403 
5404     def polyfit(
5405         self,
5406         dim: Hashable,
5407         deg: int,
5408         skipna: bool | None = None,
5409         rcond: float | None = None,
5410         w: Hashable | Any | None = None,
5411         full: bool = False,
5412         cov: bool | Literal["unscaled"] = False,
5413     ) -> Dataset:
5414         """
5415         Least squares polynomial fit.
5416 
5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping
5418         invalid values when `skipna = True`.
5419 
5420         Parameters
5421         ----------
5422         dim : Hashable
5423             Coordinate along which to fit the polynomials.
5424         deg : int
5425             Degree of the fitting polynomial.
5426         skipna : bool or None, optional
5427             If True, removes all invalid values before fitting each 1D slices of the array.
5428             Default is True if data is stored in a dask.array or if there is any
5429             invalid values, False otherwise.
5430         rcond : float or None, optional
5431             Relative condition number to the fit.
5432         w : Hashable, array-like or None, optional
5433             Weights to apply to the y-coordinate of the sample points.
5434             Can be an array-like object or the name of a coordinate in the dataset.
5435         full : bool, default: False
5436             Whether to return the residuals, matrix rank and singular values in addition
5437             to the coefficients.
5438         cov : bool or "unscaled", default: False
5439             Whether to return to the covariance matrix in addition to the coefficients.
5440             The matrix is not scaled if `cov='unscaled'`.
5441 
5442         Returns
5443         -------
5444         polyfit_results : Dataset
5445             A single dataset which contains:
5446 
5447             polyfit_coefficients
5448                 The coefficients of the best fit.
5449             polyfit_residuals
5450                 The residuals of the least-square computation (only included if `full=True`).
5451                 When the matrix rank is deficient, np.nan is returned.
5452             [dim]_matrix_rank
5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5454             [dim]_singular_value
5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
5456             polyfit_covariance
5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
5458 
5459         See Also
5460         --------
5461         numpy.polyfit
5462         numpy.polyval
5463         xarray.polyval
5464         """
5465         return self._to_temp_dataset().polyfit(
5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
5467         )
5468 
5469     def pad(
5470         self: T_DataArray,
5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
5472         mode: PadModeOptions = "constant",
5473         stat_length: int
5474         | tuple[int, int]
5475         | Mapping[Any, tuple[int, int]]
5476         | None = None,
5477         constant_values: float
5478         | tuple[float, float]
5479         | Mapping[Any, tuple[float, float]]
5480         | None = None,
5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
5482         reflect_type: PadReflectOptions = None,
5483         keep_attrs: bool | None = None,
5484         **pad_width_kwargs: Any,
5485     ) -> T_DataArray:
5486         """Pad this array along one or more dimensions.
5487 
5488         .. warning::
5489             This function is experimental and its behaviour is likely to change
5490             especially regarding padding of dimension coordinates (or IndexVariables).
5491 
5492         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
5493         coordinates will be padded with the same mode, otherwise coordinates
5494         are padded using the "constant" mode with fill_value dtypes.NA.
5495 
5496         Parameters
5497         ----------
5498         pad_width : mapping of Hashable to tuple of int
5499             Mapping with the form of {dim: (pad_before, pad_after)}
5500             describing the number of values padded along each dimension.
5501             {dim: pad} is a shortcut for pad_before = pad_after = pad
5502         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
5503             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
5504             How to pad the DataArray (taken from numpy docs):
5505 
5506             - "constant": Pads with a constant value.
5507             - "edge": Pads with the edge values of array.
5508             - "linear_ramp": Pads with the linear ramp between end_value and the
5509               array edge value.
5510             - "maximum": Pads with the maximum value of all or part of the
5511               vector along each axis.
5512             - "mean": Pads with the mean value of all or part of the
5513               vector along each axis.
5514             - "median": Pads with the median value of all or part of the
5515               vector along each axis.
5516             - "minimum": Pads with the minimum value of all or part of the
5517               vector along each axis.
5518             - "reflect": Pads with the reflection of the vector mirrored on
5519               the first and last values of the vector along each axis.
5520             - "symmetric": Pads with the reflection of the vector mirrored
5521               along the edge of the array.
5522             - "wrap": Pads with the wrap of the vector along the axis.
5523               The first values are used to pad the end and the
5524               end values are used to pad the beginning.
5525 
5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None
5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
5528             values at edge of each axis used to calculate the statistic value.
5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
5530             statistic lengths along each dimension.
5531             ((before, after),) yields same before and after statistic lengths
5532             for each dimension.
5533             (stat_length,) or int is a shortcut for before = after = statistic
5534             length for all axes.
5535             Default is ``None``, to use the entire axis.
5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5537             Used in 'constant'.  The values to set the padded values for each
5538             axis.
5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5540             pad constants along each dimension.
5541             ``((before, after),)`` yields same before and after constants for each
5542             dimension.
5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5544             all dimensions.
5545             Default is 0.
5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
5547             Used in 'linear_ramp'.  The values used for the ending value of the
5548             linear_ramp and that will form the edge of the padded array.
5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
5550             end values along each dimension.
5551             ``((before, after),)`` yields same before and after end values for each
5552             axis.
5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
5554             all axes.
5555             Default is 0.
5556         reflect_type : {"even", "odd", None}, optional
5557             Used in "reflect", and "symmetric". The "even" style is the
5558             default with an unaltered reflection around the edge value. For
5559             the "odd" style, the extended part of the array is created by
5560             subtracting the reflected values from two times the edge value.
5561         keep_attrs : bool or None, optional
5562             If True, the attributes (``attrs``) will be copied from the
5563             original object to the new one. If False, the new object
5564             will be returned without attributes.
5565         **pad_width_kwargs
5566             The keyword arguments form of ``pad_width``.
5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
5568 
5569         Returns
5570         -------
5571         padded : DataArray
5572             DataArray with the padded coordinates and data.
5573 
5574         See Also
5575         --------
5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
5577 
5578         Notes
5579         -----
5580         For ``mode="constant"`` and ``constant_values=None``, integer types will be
5581         promoted to ``float`` and padded with ``np.nan``.
5582 
5583         Padding coordinates will drop their corresponding index (if any) and will reset default
5584         indexes for dimension coordinates.
5585 
5586         Examples
5587         --------
5588         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
5589         >>> arr.pad(x=(1, 2), constant_values=0)
5590         <xarray.DataArray (x: 6)>
5591         array([0, 5, 6, 7, 0, 0])
5592         Coordinates:
5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
5594 
5595         >>> da = xr.DataArray(
5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
5597         ...     dims=["x", "y"],
5598         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
5599         ... )
5600         >>> da.pad(x=1)
5601         <xarray.DataArray (x: 4, y: 4)>
5602         array([[nan, nan, nan, nan],
5603                [ 0.,  1.,  2.,  3.],
5604                [10., 11., 12., 13.],
5605                [nan, nan, nan, nan]])
5606         Coordinates:
5607           * x        (x) float64 nan 0.0 1.0 nan
5608           * y        (y) int64 10 20 30 40
5609             z        (x) float64 nan 100.0 200.0 nan
5610 
5611         Careful, ``constant_values`` are coerced to the data type of the array which may
5612         lead to a loss of precision:
5613 
5614         >>> da.pad(x=1, constant_values=1.23456789)
5615         <xarray.DataArray (x: 4, y: 4)>
5616         array([[ 1,  1,  1,  1],
5617                [ 0,  1,  2,  3],
5618                [10, 11, 12, 13],
5619                [ 1,  1,  1,  1]])
5620         Coordinates:
5621           * x        (x) float64 nan 0.0 1.0 nan
5622           * y        (y) int64 10 20 30 40
5623             z        (x) float64 nan 100.0 200.0 nan
5624         """
5625         ds = self._to_temp_dataset().pad(
5626             pad_width=pad_width,
5627             mode=mode,
5628             stat_length=stat_length,
5629             constant_values=constant_values,
5630             end_values=end_values,
5631             reflect_type=reflect_type,
5632             keep_attrs=keep_attrs,
5633             **pad_width_kwargs,
5634         )
5635         return self._from_temp_dataset(ds)
5636 
5637     def idxmin(
5638         self,
5639         dim: Hashable | None = None,
5640         skipna: bool | None = None,
5641         fill_value: Any = dtypes.NA,
5642         keep_attrs: bool | None = None,
5643     ) -> DataArray:
5644         """Return the coordinate label of the minimum value along a dimension.
5645 
5646         Returns a new `DataArray` named after the dimension with the values of
5647         the coordinate labels along that dimension corresponding to minimum
5648         values along that dimension.
5649 
5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the
5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
5652 
5653         Parameters
5654         ----------
5655         dim : str, optional
5656             Dimension over which to apply `idxmin`.  This is optional for 1D
5657             arrays, but required for arrays with 2 or more dimensions.
5658         skipna : bool or None, default: None
5659             If True, skip missing values (as marked by NaN). By default, only
5660             skips missing values for ``float``, ``complex``, and ``object``
5661             dtypes; other dtypes either do not have a sentinel missing value
5662             (``int``) or ``skipna=True`` has not been implemented
5663             (``datetime64`` or ``timedelta64``).
5664         fill_value : Any, default: NaN
5665             Value to be filled in case all of the values along a dimension are
5666             null.  By default this is NaN.  The fill value and result are
5667             automatically converted to a compatible dtype if possible.
5668             Ignored if ``skipna`` is False.
5669         keep_attrs : bool or None, optional
5670             If True, the attributes (``attrs``) will be copied from the
5671             original object to the new one. If False, the new object
5672             will be returned without attributes.
5673 
5674         Returns
5675         -------
5676         reduced : DataArray
5677             New `DataArray` object with `idxmin` applied to its data and the
5678             indicated dimension removed.
5679 
5680         See Also
5681         --------
5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
5683 
5684         Examples
5685         --------
5686         >>> array = xr.DataArray(
5687         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5688         ... )
5689         >>> array.min()
5690         <xarray.DataArray ()>
5691         array(-2)
5692         >>> array.argmin(...)
5693         {'x': <xarray.DataArray ()>
5694         array(4)}
5695         >>> array.idxmin()
5696         <xarray.DataArray 'x' ()>
5697         array('e', dtype='<U1')
5698 
5699         >>> array = xr.DataArray(
5700         ...     [
5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5704         ...     ],
5705         ...     dims=["y", "x"],
5706         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5707         ... )
5708         >>> array.min(dim="x")
5709         <xarray.DataArray (y: 3)>
5710         array([-2., -4.,  1.])
5711         Coordinates:
5712           * y        (y) int64 -1 0 1
5713         >>> array.argmin(dim="x")
5714         <xarray.DataArray (y: 3)>
5715         array([4, 0, 2])
5716         Coordinates:
5717           * y        (y) int64 -1 0 1
5718         >>> array.idxmin(dim="x")
5719         <xarray.DataArray 'x' (y: 3)>
5720         array([16.,  0.,  4.])
5721         Coordinates:
5722           * y        (y) int64 -1 0 1
5723         """
5724         return computation._calc_idxminmax(
5725             array=self,
5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
5727             dim=dim,
5728             skipna=skipna,
5729             fill_value=fill_value,
5730             keep_attrs=keep_attrs,
5731         )
5732 
5733     def idxmax(
5734         self,
5735         dim: Hashable = None,
5736         skipna: bool | None = None,
5737         fill_value: Any = dtypes.NA,
5738         keep_attrs: bool | None = None,
5739     ) -> DataArray:
5740         """Return the coordinate label of the maximum value along a dimension.
5741 
5742         Returns a new `DataArray` named after the dimension with the values of
5743         the coordinate labels along that dimension corresponding to maximum
5744         values along that dimension.
5745 
5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the
5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
5748 
5749         Parameters
5750         ----------
5751         dim : Hashable, optional
5752             Dimension over which to apply `idxmax`.  This is optional for 1D
5753             arrays, but required for arrays with 2 or more dimensions.
5754         skipna : bool or None, default: None
5755             If True, skip missing values (as marked by NaN). By default, only
5756             skips missing values for ``float``, ``complex``, and ``object``
5757             dtypes; other dtypes either do not have a sentinel missing value
5758             (``int``) or ``skipna=True`` has not been implemented
5759             (``datetime64`` or ``timedelta64``).
5760         fill_value : Any, default: NaN
5761             Value to be filled in case all of the values along a dimension are
5762             null.  By default this is NaN.  The fill value and result are
5763             automatically converted to a compatible dtype if possible.
5764             Ignored if ``skipna`` is False.
5765         keep_attrs : bool or None, optional
5766             If True, the attributes (``attrs``) will be copied from the
5767             original object to the new one. If False, the new object
5768             will be returned without attributes.
5769 
5770         Returns
5771         -------
5772         reduced : DataArray
5773             New `DataArray` object with `idxmax` applied to its data and the
5774             indicated dimension removed.
5775 
5776         See Also
5777         --------
5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
5779 
5780         Examples
5781         --------
5782         >>> array = xr.DataArray(
5783         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
5784         ... )
5785         >>> array.max()
5786         <xarray.DataArray ()>
5787         array(2)
5788         >>> array.argmax(...)
5789         {'x': <xarray.DataArray ()>
5790         array(1)}
5791         >>> array.idxmax()
5792         <xarray.DataArray 'x' ()>
5793         array('b', dtype='<U1')
5794 
5795         >>> array = xr.DataArray(
5796         ...     [
5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
5800         ...     ],
5801         ...     dims=["y", "x"],
5802         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
5803         ... )
5804         >>> array.max(dim="x")
5805         <xarray.DataArray (y: 3)>
5806         array([2., 2., 1.])
5807         Coordinates:
5808           * y        (y) int64 -1 0 1
5809         >>> array.argmax(dim="x")
5810         <xarray.DataArray (y: 3)>
5811         array([0, 2, 2])
5812         Coordinates:
5813           * y        (y) int64 -1 0 1
5814         >>> array.idxmax(dim="x")
5815         <xarray.DataArray 'x' (y: 3)>
5816         array([0., 4., 4.])
5817         Coordinates:
5818           * y        (y) int64 -1 0 1
5819         """
5820         return computation._calc_idxminmax(
5821             array=self,
5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
5823             dim=dim,
5824             skipna=skipna,
5825             fill_value=fill_value,
5826             keep_attrs=keep_attrs,
5827         )
5828 
5829     # change type of self and return to T_DataArray once
5830     # https://github.com/python/mypy/issues/12846 is resolved
5831     def argmin(
5832         self,
5833         dim: Dims = None,
5834         axis: int | None = None,
5835         keep_attrs: bool | None = None,
5836         skipna: bool | None = None,
5837     ) -> DataArray | dict[Hashable, DataArray]:
5838         """Index or indices of the minimum of the DataArray over one or more dimensions.
5839 
5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5841         which can be passed directly to isel(). If a single str is passed to 'dim' then
5842         returns a DataArray with dtype int.
5843 
5844         If there are multiple minima, the indices of the first one found will be
5845         returned.
5846 
5847         Parameters
5848         ----------
5849         dim : "...", str, Iterable of Hashable or None, optional
5850             The dimensions over which to find the minimum. By default, finds minimum over
5851             all dimensions - for now returning an int for backward compatibility, but
5852             this is deprecated, in future will return a dict with indices for all
5853             dimensions; to return a dict with all dimensions now, pass '...'.
5854         axis : int or None, optional
5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
5856             can be supplied.
5857         keep_attrs : bool or None, optional
5858             If True, the attributes (`attrs`) will be copied from the original
5859             object to the new one. If False, the new object will be
5860             returned without attributes.
5861         skipna : bool or None, optional
5862             If True, skip missing values (as marked by NaN). By default, only
5863             skips missing values for float dtypes; other dtypes either do not
5864             have a sentinel missing value (int) or skipna=True has not been
5865             implemented (object, datetime64 or timedelta64).
5866 
5867         Returns
5868         -------
5869         result : DataArray or dict of DataArray
5870 
5871         See Also
5872         --------
5873         Variable.argmin, DataArray.idxmin
5874 
5875         Examples
5876         --------
5877         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5878         >>> array.min()
5879         <xarray.DataArray ()>
5880         array(-1)
5881         >>> array.argmin(...)
5882         {'x': <xarray.DataArray ()>
5883         array(2)}
5884         >>> array.isel(array.argmin(...))
5885         <xarray.DataArray ()>
5886         array(-1)
5887 
5888         >>> array = xr.DataArray(
5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
5890         ...     dims=("x", "y", "z"),
5891         ... )
5892         >>> array.min(dim="x")
5893         <xarray.DataArray (y: 3, z: 3)>
5894         array([[ 1,  2,  1],
5895                [ 2, -5,  1],
5896                [ 2,  1,  1]])
5897         Dimensions without coordinates: y, z
5898         >>> array.argmin(dim="x")
5899         <xarray.DataArray (y: 3, z: 3)>
5900         array([[1, 0, 0],
5901                [1, 1, 1],
5902                [0, 0, 1]])
5903         Dimensions without coordinates: y, z
5904         >>> array.argmin(dim=["x"])
5905         {'x': <xarray.DataArray (y: 3, z: 3)>
5906         array([[1, 0, 0],
5907                [1, 1, 1],
5908                [0, 0, 1]])
5909         Dimensions without coordinates: y, z}
5910         >>> array.min(dim=("x", "z"))
5911         <xarray.DataArray (y: 3)>
5912         array([ 1, -5,  1])
5913         Dimensions without coordinates: y
5914         >>> array.argmin(dim=["x", "z"])
5915         {'x': <xarray.DataArray (y: 3)>
5916         array([0, 1, 0])
5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
5918         array([2, 1, 1])
5919         Dimensions without coordinates: y}
5920         >>> array.isel(array.argmin(dim=["x", "z"]))
5921         <xarray.DataArray (y: 3)>
5922         array([ 1, -5,  1])
5923         Dimensions without coordinates: y
5924         """
5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
5926         if isinstance(result, dict):
5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
5928         else:
5929             return self._replace_maybe_drop_dims(result)
5930 
5931     # change type of self and return to T_DataArray once
5932     # https://github.com/python/mypy/issues/12846 is resolved
5933     def argmax(
5934         self,
5935         dim: Dims = None,
5936         axis: int | None = None,
5937         keep_attrs: bool | None = None,
5938         skipna: bool | None = None,
5939     ) -> DataArray | dict[Hashable, DataArray]:
5940         """Index or indices of the maximum of the DataArray over one or more dimensions.
5941 
5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
5943         which can be passed directly to isel(). If a single str is passed to 'dim' then
5944         returns a DataArray with dtype int.
5945 
5946         If there are multiple maxima, the indices of the first one found will be
5947         returned.
5948 
5949         Parameters
5950         ----------
5951         dim : "...", str, Iterable of Hashable or None, optional
5952             The dimensions over which to find the maximum. By default, finds maximum over
5953             all dimensions - for now returning an int for backward compatibility, but
5954             this is deprecated, in future will return a dict with indices for all
5955             dimensions; to return a dict with all dimensions now, pass '...'.
5956         axis : int or None, optional
5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
5958             can be supplied.
5959         keep_attrs : bool or None, optional
5960             If True, the attributes (`attrs`) will be copied from the original
5961             object to the new one. If False, the new object will be
5962             returned without attributes.
5963         skipna : bool or None, optional
5964             If True, skip missing values (as marked by NaN). By default, only
5965             skips missing values for float dtypes; other dtypes either do not
5966             have a sentinel missing value (int) or skipna=True has not been
5967             implemented (object, datetime64 or timedelta64).
5968 
5969         Returns
5970         -------
5971         result : DataArray or dict of DataArray
5972 
5973         See Also
5974         --------
5975         Variable.argmax, DataArray.idxmax
5976 
5977         Examples
5978         --------
5979         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
5980         >>> array.max()
5981         <xarray.DataArray ()>
5982         array(3)
5983         >>> array.argmax(...)
5984         {'x': <xarray.DataArray ()>
5985         array(3)}
5986         >>> array.isel(array.argmax(...))
5987         <xarray.DataArray ()>
5988         array(3)
5989 
5990         >>> array = xr.DataArray(
5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
5992         ...     dims=("x", "y", "z"),
5993         ... )
5994         >>> array.max(dim="x")
5995         <xarray.DataArray (y: 3, z: 3)>
5996         array([[3, 3, 2],
5997                [3, 5, 2],
5998                [2, 3, 3]])
5999         Dimensions without coordinates: y, z
6000         >>> array.argmax(dim="x")
6001         <xarray.DataArray (y: 3, z: 3)>
6002         array([[0, 1, 1],
6003                [0, 1, 0],
6004                [0, 1, 0]])
6005         Dimensions without coordinates: y, z
6006         >>> array.argmax(dim=["x"])
6007         {'x': <xarray.DataArray (y: 3, z: 3)>
6008         array([[0, 1, 1],
6009                [0, 1, 0],
6010                [0, 1, 0]])
6011         Dimensions without coordinates: y, z}
6012         >>> array.max(dim=("x", "z"))
6013         <xarray.DataArray (y: 3)>
6014         array([3, 5, 3])
6015         Dimensions without coordinates: y
6016         >>> array.argmax(dim=["x", "z"])
6017         {'x': <xarray.DataArray (y: 3)>
6018         array([0, 1, 0])
6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
6020         array([0, 1, 2])
6021         Dimensions without coordinates: y}
6022         >>> array.isel(array.argmax(dim=["x", "z"]))
6023         <xarray.DataArray (y: 3)>
6024         array([3, 5, 3])
6025         Dimensions without coordinates: y
6026         """
6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
6028         if isinstance(result, dict):
6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
6030         else:
6031             return self._replace_maybe_drop_dims(result)
6032 
6033     def query(
6034         self,
6035         queries: Mapping[Any, Any] | None = None,
6036         parser: QueryParserOptions = "pandas",
6037         engine: QueryEngineOptions = None,
6038         missing_dims: ErrorOptionsWithWarn = "raise",
6039         **queries_kwargs: Any,
6040     ) -> DataArray:
6041         """Return a new data array indexed along the specified
6042         dimension(s), where the indexers are given as strings containing
6043         Python expressions to be evaluated against the values in the array.
6044 
6045         Parameters
6046         ----------
6047         queries : dict-like or None, optional
6048             A dict-like with keys matching dimensions and values given by strings
6049             containing Python expressions to be evaluated against the data variables
6050             in the dataset. The expressions will be evaluated using the pandas
6051             eval() function, and can contain any valid Python expressions but cannot
6052             contain any Python statements.
6053         parser : {"pandas", "python"}, default: "pandas"
6054             The parser to use to construct the syntax tree from the expression.
6055             The default of 'pandas' parses code slightly different than standard
6056             Python. Alternatively, you can parse an expression using the 'python'
6057             parser to retain strict Python semantics.
6058         engine : {"python", "numexpr", None}, default: None
6059             The engine used to evaluate the expression. Supported engines are:
6060 
6061             - None: tries to use numexpr, falls back to python
6062             - "numexpr": evaluates expressions using numexpr
6063             - "python": performs operations as if you had eval’d in top level python
6064 
6065         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
6066             What to do if dimensions that should be selected from are not present in the
6067             DataArray:
6068 
6069             - "raise": raise an exception
6070             - "warn": raise a warning, and ignore the missing dimensions
6071             - "ignore": ignore the missing dimensions
6072 
6073         **queries_kwargs : {dim: query, ...}, optional
6074             The keyword arguments form of ``queries``.
6075             One of queries or queries_kwargs must be provided.
6076 
6077         Returns
6078         -------
6079         obj : DataArray
6080             A new DataArray with the same contents as this dataset, indexed by
6081             the results of the appropriate queries.
6082 
6083         See Also
6084         --------
6085         DataArray.isel
6086         Dataset.query
6087         pandas.eval
6088 
6089         Examples
6090         --------
6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
6092         >>> da
6093         <xarray.DataArray 'a' (x: 5)>
6094         array([0, 1, 2, 3, 4])
6095         Dimensions without coordinates: x
6096         >>> da.query(x="a > 2")
6097         <xarray.DataArray 'a' (x: 2)>
6098         array([3, 4])
6099         Dimensions without coordinates: x
6100         """
6101 
6102         ds = self._to_dataset_whole(shallow_copy=True)
6103         ds = ds.query(
6104             queries=queries,
6105             parser=parser,
6106             engine=engine,
6107             missing_dims=missing_dims,
6108             **queries_kwargs,
6109         )
6110         return ds[self.name]
6111 
6112     def curvefit(
6113         self,
6114         coords: str | DataArray | Iterable[str | DataArray],
6115         func: Callable[..., Any],
6116         reduce_dims: Dims = None,
6117         skipna: bool = True,
6118         p0: dict[str, Any] | None = None,
6119         bounds: dict[str, Any] | None = None,
6120         param_names: Sequence[str] | None = None,
6121         kwargs: dict[str, Any] | None = None,
6122     ) -> Dataset:
6123         """
6124         Curve fitting optimization for arbitrary functions.
6125 
6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
6127 
6128         Parameters
6129         ----------
6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable
6131             Independent coordinate(s) over which to perform the curve fitting. Must share
6132             at least one dimension with the calling object. When fitting multi-dimensional
6133             functions, supply `coords` as a sequence in the same order as arguments in
6134             `func`. To fit along existing dimensions of the calling object, `coords` can
6135             also be specified as a str or sequence of strs.
6136         func : callable
6137             User specified function in the form `f(x, *params)` which returns a numpy
6138             array of length `len(x)`. `params` are the fittable parameters which are optimized
6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
6140             coordinates, e.g. `f((x0, x1), *params)`.
6141         reduce_dims : str, Iterable of Hashable or None, optional
6142             Additional dimension(s) over which to aggregate while fitting. For example,
6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
6144             aggregate all lat and lon points and fit the specified function along the
6145             time dimension.
6146         skipna : bool, default: True
6147             Whether to skip missing values when fitting. Default is True.
6148         p0 : dict-like or None, optional
6149             Optional dictionary of parameter names to initial guesses passed to the
6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
6151             be assigned initial values following the default scipy behavior.
6152         bounds : dict-like or None, optional
6153             Optional dictionary of parameter names to bounding values passed to the
6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
6155             will be unbounded following the default scipy behavior.
6156         param_names : sequence of Hashable or None, optional
6157             Sequence of names for the fittable parameters of `func`. If not supplied,
6158             this will be automatically determined by arguments of `func`. `param_names`
6159             should be manually supplied when fitting a function that takes a variable
6160             number of parameters.
6161         **kwargs : optional
6162             Additional keyword arguments to passed to scipy curve_fit.
6163 
6164         Returns
6165         -------
6166         curvefit_results : Dataset
6167             A single dataset which contains:
6168 
6169             [var]_curvefit_coefficients
6170                 The coefficients of the best fit.
6171             [var]_curvefit_covariance
6172                 The covariance matrix of the coefficient estimates.
6173 
6174         See Also
6175         --------
6176         DataArray.polyfit
6177         scipy.optimize.curve_fit
6178         """
6179         return self._to_temp_dataset().curvefit(
6180             coords,
6181             func,
6182             reduce_dims=reduce_dims,
6183             skipna=skipna,
6184             p0=p0,
6185             bounds=bounds,
6186             param_names=param_names,
6187             kwargs=kwargs,
6188         )
6189 
6190     def drop_duplicates(
6191         self: T_DataArray,
6192         dim: Hashable | Iterable[Hashable],
6193         keep: Literal["first", "last", False] = "first",
6194     ) -> T_DataArray:
6195         """Returns a new DataArray with duplicate dimension values removed.
6196 
6197         Parameters
6198         ----------
6199         dim : dimension label or labels
6200             Pass `...` to drop duplicates along all dimensions.
6201         keep : {"first", "last", False}, default: "first"
6202             Determines which duplicates (if any) to keep.
6203 
6204             - ``"first"`` : Drop duplicates except for the first occurrence.
6205             - ``"last"`` : Drop duplicates except for the last occurrence.
6206             - False : Drop all duplicates.
6207 
6208         Returns
6209         -------
6210         DataArray
6211 
6212         See Also
6213         --------
6214         Dataset.drop_duplicates
6215 
6216         Examples
6217         --------
6218         >>> da = xr.DataArray(
6219         ...     np.arange(25).reshape(5, 5),
6220         ...     dims=("x", "y"),
6221         ...     coords={"x": np.array([0, 0, 1, 2, 3]), "y": np.array([0, 1, 2, 3, 3])},
6222         ... )
6223         >>> da
6224         <xarray.DataArray (x: 5, y: 5)>
6225         array([[ 0,  1,  2,  3,  4],
6226                [ 5,  6,  7,  8,  9],
6227                [10, 11, 12, 13, 14],
6228                [15, 16, 17, 18, 19],
6229                [20, 21, 22, 23, 24]])
6230         Coordinates:
6231           * x        (x) int64 0 0 1 2 3
6232           * y        (y) int64 0 1 2 3 3
6233 
6234         >>> da.drop_duplicates(dim="x")
6235         <xarray.DataArray (x: 4, y: 5)>
6236         array([[ 0,  1,  2,  3,  4],
6237                [10, 11, 12, 13, 14],
6238                [15, 16, 17, 18, 19],
6239                [20, 21, 22, 23, 24]])
6240         Coordinates:
6241           * x        (x) int64 0 1 2 3
6242           * y        (y) int64 0 1 2 3 3
6243 
6244         >>> da.drop_duplicates(dim="x", keep="last")
6245         <xarray.DataArray (x: 4, y: 5)>
6246         array([[ 5,  6,  7,  8,  9],
6247                [10, 11, 12, 13, 14],
6248                [15, 16, 17, 18, 19],
6249                [20, 21, 22, 23, 24]])
6250         Coordinates:
6251           * x        (x) int64 0 1 2 3
6252           * y        (y) int64 0 1 2 3 3
6253 
6254         Drop all duplicate dimension values:
6255 
6256         >>> da.drop_duplicates(dim=...)
6257         <xarray.DataArray (x: 4, y: 4)>
6258         array([[ 0,  1,  2,  3],
6259                [10, 11, 12, 13],
6260                [15, 16, 17, 18],
6261                [20, 21, 22, 23]])
6262         Coordinates:
6263           * x        (x) int64 0 1 2 3
6264           * y        (y) int64 0 1 2 3
6265         """
6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
6267         return self._from_temp_dataset(deduplicated)
6268 
6269     def convert_calendar(
6270         self,
6271         calendar: str,
6272         dim: str = "time",
6273         align_on: str | None = None,
6274         missing: Any | None = None,
6275         use_cftime: bool | None = None,
6276     ) -> DataArray:
6277         """Convert the DataArray to another calendar.
6278 
6279         Only converts the individual timestamps, does not modify any data except
6280         in dropping invalid/surplus dates or inserting missing dates.
6281 
6282         If the source and target calendars are either no_leap, all_leap or a
6283         standard type, only the type of the time array is modified.
6284         When converting to a leap year from a non-leap year, the 29th of February
6285         is removed from the array. In the other direction the 29th of February
6286         will be missing in the output, unless `missing` is specified,
6287         in which case that value is inserted.
6288 
6289         For conversions involving `360_day` calendars, see Notes.
6290 
6291         This method is safe to use with sub-daily data as it doesn't touch the
6292         time part of the timestamps.
6293 
6294         Parameters
6295         ---------
6296         calendar : str
6297             The target calendar name.
6298         dim : str
6299             Name of the time coordinate.
6300         align_on : {None, 'date', 'year'}
6301             Must be specified when either source or target is a `360_day` calendar,
6302            ignored otherwise. See Notes.
6303         missing : Optional[any]
6304             By default, i.e. if the value is None, this method will simply attempt
6305             to convert the dates in the source calendar to the same dates in the
6306             target calendar, and drop any of those that are not possible to
6307             represent.  If a value is provided, a new time coordinate will be
6308             created in the target calendar with the same frequency as the original
6309             time coordinate; for any dates that are not present in the source, the
6310             data will be filled with this value.  Note that using this mode requires
6311             that the source data have an inferable frequency; for more information
6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
6313             target calendar combinations, this could result in many missing values, see notes.
6314         use_cftime : boolean, optional
6315             Whether to use cftime objects in the output, only used if `calendar`
6316             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
6317             If True, the new time axis uses cftime objects.
6318             If None (default), it uses :py:class:`numpy.datetime64` values if the
6319             date range permits it, and :py:class:`cftime.datetime` objects if not.
6320             If False, it uses :py:class:`numpy.datetime64`  or fails.
6321 
6322         Returns
6323         -------
6324         DataArray
6325             Copy of the dataarray with the time coordinate converted to the
6326             target calendar. If 'missing' was None (default), invalid dates in
6327             the new calendar are dropped, but missing dates are not inserted.
6328             If `missing` was given, the new data is reindexed to have a time axis
6329             with the same frequency as the source, but in the new calendar; any
6330             missing datapoints are filled with `missing`.
6331 
6332         Notes
6333         -----
6334         Passing a value to `missing` is only usable if the source's time coordinate as an
6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
6336         if the target coordinate, generated from this frequency, has dates equivalent to the
6337         source. It is usually **not** appropriate to use this mode with:
6338 
6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
6341             or 'mH' where 24 % m != 0).
6342 
6343         If one of the source or target calendars is `"360_day"`, `align_on` must
6344         be specified and two options are offered.
6345 
6346         - "year"
6347             The dates are translated according to their relative position in the year,
6348             ignoring their original month and day information, meaning that the
6349             missing/surplus days are added/removed at regular intervals.
6350 
6351             From a `360_day` to a standard calendar, the output will be missing the
6352             following dates (day of year in parentheses):
6353 
6354             To a leap year:
6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
6356                 September 31st (275) and November 30th (335).
6357             To a non-leap year:
6358                 February 6th (36), April 19th (109), July 2nd (183),
6359                 September 12th (255), November 25th (329).
6360 
6361             From a standard calendar to a `"360_day"`, the following dates in the
6362             source array will be dropped:
6363 
6364             From a leap year:
6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
6366                 September 31st (275), December 1st (336)
6367             From a non-leap year:
6368                 February 6th (37), April 20th (110), July 2nd (183),
6369                 September 13th (256), November 25th (329)
6370 
6371             This option is best used on daily and subdaily data.
6372 
6373         - "date"
6374             The month/day information is conserved and invalid dates are dropped
6375             from the output. This means that when converting from a `"360_day"` to a
6376             standard calendar, all 31st (Jan, March, May, July, August, October and
6377             December) will be missing as there is no equivalent dates in the
6378             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
6379             will be dropped as there are no equivalent dates in a standard calendar.
6380 
6381             This option is best used with data on a frequency coarser than daily.
6382         """
6383         return convert_calendar(
6384             self,
6385             calendar,
6386             dim=dim,
6387             align_on=align_on,
6388             missing=missing,
6389             use_cftime=use_cftime,
6390         )
6391 
6392     def interp_calendar(
6393         self,
6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
6395         dim: str = "time",
6396     ) -> DataArray:
6397         """Interpolates the DataArray to another calendar based on decimal year measure.
6398 
6399         Each timestamp in `source` and `target` are first converted to their decimal
6400         year equivalent then `source` is interpolated on the target coordinate.
6401         The decimal year of a timestamp is its year plus its sub-year component
6402         converted to the fraction of its year. For example "2000-03-01 12:00" is
6403         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
6404 
6405         This method should only be used when the time (HH:MM:SS) information of
6406         time coordinate is not important.
6407 
6408         Parameters
6409         ----------
6410         target: DataArray or DatetimeIndex or CFTimeIndex
6411             The target time coordinate of a valid dtype
6412             (np.datetime64 or cftime objects)
6413         dim : str
6414             The time coordinate name.
6415 
6416         Return
6417         ------
6418         DataArray
6419             The source interpolated on the decimal years of target,
6420         """
6421         return interp_calendar(self, target, dim=dim)
6422 
6423     def groupby(
6424         self,
6425         group: Hashable | DataArray | IndexVariable,
6426         squeeze: bool = True,
6427         restore_coord_dims: bool = False,
6428     ) -> DataArrayGroupBy:
6429         """Returns a DataArrayGroupBy object for performing grouped operations.
6430 
6431         Parameters
6432         ----------
6433         group : Hashable, DataArray or IndexVariable
6434             Array whose unique values should be used to group this array. If a
6435             Hashable, must be the name of a coordinate contained in this dataarray.
6436         squeeze : bool, default: True
6437             If "group" is a dimension of any arrays in this dataset, `squeeze`
6438             controls whether the subarrays have a dimension of length 1 along
6439             that dimension or if the dimension is squeezed out.
6440         restore_coord_dims : bool, default: False
6441             If True, also restore the dimension order of multi-dimensional
6442             coordinates.
6443 
6444         Returns
6445         -------
6446         grouped : DataArrayGroupBy
6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6448             iterated over in the form of `(unique_value, grouped_array)` pairs.
6449 
6450         Examples
6451         --------
6452         Calculate daily anomalies for daily data:
6453 
6454         >>> da = xr.DataArray(
6455         ...     np.linspace(0, 1826, num=1827),
6456         ...     coords=[pd.date_range("2000-01-01", "2004-12-31", freq="D")],
6457         ...     dims="time",
6458         ... )
6459         >>> da
6460         <xarray.DataArray (time: 1827)>
6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
6462                1.826e+03])
6463         Coordinates:
6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6465         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
6466         <xarray.DataArray (time: 1827)>
6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
6468         Coordinates:
6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
6471 
6472         See Also
6473         --------
6474         :ref:`groupby`
6475             Users guide explanation of how to group and bin data.
6476         DataArray.groupby_bins
6477         Dataset.groupby
6478         core.groupby.DataArrayGroupBy
6479         pandas.DataFrame.groupby
6480         """
6481         from xarray.core.groupby import (
6482             DataArrayGroupBy,
6483             ResolvedUniqueGrouper,
6484             UniqueGrouper,
6485             _validate_groupby_squeeze,
6486         )
6487 
6488         _validate_groupby_squeeze(squeeze)
6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)
6490         return DataArrayGroupBy(
6491             self,
6492             (rgrouper,),
6493             squeeze=squeeze,
6494             restore_coord_dims=restore_coord_dims,
6495         )
6496 
6497     def groupby_bins(
6498         self,
6499         group: Hashable | DataArray | IndexVariable,
6500         bins: ArrayLike,
6501         right: bool = True,
6502         labels: ArrayLike | Literal[False] | None = None,
6503         precision: int = 3,
6504         include_lowest: bool = False,
6505         squeeze: bool = True,
6506         restore_coord_dims: bool = False,
6507     ) -> DataArrayGroupBy:
6508         """Returns a DataArrayGroupBy object for performing grouped operations.
6509 
6510         Rather than using all unique values of `group`, the values are discretized
6511         first by applying `pandas.cut` [1]_ to `group`.
6512 
6513         Parameters
6514         ----------
6515         group : Hashable, DataArray or IndexVariable
6516             Array whose binned values should be used to group this array. If a
6517             Hashable, must be the name of a coordinate contained in this dataarray.
6518         bins : int or array-like
6519             If bins is an int, it defines the number of equal-width bins in the
6520             range of x. However, in this case, the range of x is extended by .1%
6521             on each side to include the min or max values of x. If bins is a
6522             sequence it defines the bin edges allowing for non-uniform bin
6523             width. No extension of the range of x is done in this case.
6524         right : bool, default: True
6525             Indicates whether the bins include the rightmost edge or not. If
6526             right == True (the default), then the bins [1,2,3,4] indicate
6527             (1,2], (2,3], (3,4].
6528         labels : array-like, False or None, default: None
6529             Used as labels for the resulting bins. Must be of the same length as
6530             the resulting bins. If False, string bin labels are assigned by
6531             `pandas.cut`.
6532         precision : int, default: 3
6533             The precision at which to store and display the bins labels.
6534         include_lowest : bool, default: False
6535             Whether the first interval should be left-inclusive or not.
6536         squeeze : bool, default: True
6537             If "group" is a dimension of any arrays in this dataset, `squeeze`
6538             controls whether the subarrays have a dimension of length 1 along
6539             that dimension or if the dimension is squeezed out.
6540         restore_coord_dims : bool, default: False
6541             If True, also restore the dimension order of multi-dimensional
6542             coordinates.
6543 
6544         Returns
6545         -------
6546         grouped : DataArrayGroupBy
6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
6548             iterated over in the form of `(unique_value, grouped_array)` pairs.
6549             The name of the group has the added suffix `_bins` in order to
6550             distinguish it from the original variable.
6551 
6552         See Also
6553         --------
6554         :ref:`groupby`
6555             Users guide explanation of how to group and bin data.
6556         DataArray.groupby
6557         Dataset.groupby_bins
6558         core.groupby.DataArrayGroupBy
6559         pandas.DataFrame.groupby
6560 
6561         References
6562         ----------
6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
6564         """
6565         from xarray.core.groupby import (
6566             BinGrouper,
6567             DataArrayGroupBy,
6568             ResolvedBinGrouper,
6569             _validate_groupby_squeeze,
6570         )
6571 
6572         _validate_groupby_squeeze(squeeze)
6573         grouper = BinGrouper(
6574             bins=bins,
6575             cut_kwargs={
6576                 "right": right,
6577                 "labels": labels,
6578                 "precision": precision,
6579                 "include_lowest": include_lowest,
6580             },
6581         )
6582         rgrouper = ResolvedBinGrouper(grouper, group, self)
6583 
6584         return DataArrayGroupBy(
6585             self,
6586             (rgrouper,),
6587             squeeze=squeeze,
6588             restore_coord_dims=restore_coord_dims,
6589         )
6590 
6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:
6592         """
6593         Weighted DataArray operations.
6594 
6595         Parameters
6596         ----------
6597         weights : DataArray
6598             An array of weights associated with the values in this Dataset.
6599             Each value in the data contributes to the reduction operation
6600             according to its associated weight.
6601 
6602         Notes
6603         -----
6604         ``weights`` must be a DataArray and cannot contain missing values.
6605         Missing values can be replaced by ``weights.fillna(0)``.
6606 
6607         Returns
6608         -------
6609         core.weighted.DataArrayWeighted
6610 
6611         See Also
6612         --------
6613         Dataset.weighted
6614         """
6615         from xarray.core.weighted import DataArrayWeighted
6616 
6617         return DataArrayWeighted(self, weights)
6618 
6619     def rolling(
6620         self,
6621         dim: Mapping[Any, int] | None = None,
6622         min_periods: int | None = None,
6623         center: bool | Mapping[Any, bool] = False,
6624         **window_kwargs: int,
6625     ) -> DataArrayRolling:
6626         """
6627         Rolling window object for DataArrays.
6628 
6629         Parameters
6630         ----------
6631         dim : dict, optional
6632             Mapping from the dimension name to create the rolling iterator
6633             along (e.g. `time`) to its moving window size.
6634         min_periods : int or None, default: None
6635             Minimum number of observations in window required to have a value
6636             (otherwise result is NA). The default, None, is equivalent to
6637             setting min_periods equal to the size of the window.
6638         center : bool or Mapping to int, default: False
6639             Set the labels at the center of the window.
6640         **window_kwargs : optional
6641             The keyword arguments form of ``dim``.
6642             One of dim or window_kwargs must be provided.
6643 
6644         Returns
6645         -------
6646         core.rolling.DataArrayRolling
6647 
6648         Examples
6649         --------
6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
6651 
6652         >>> da = xr.DataArray(
6653         ...     np.linspace(0, 11, num=12),
6654         ...     coords=[
6655         ...         pd.date_range(
6656         ...             "1999-12-15",
6657         ...             periods=12,
6658         ...             freq=pd.DateOffset(months=1),
6659         ...         )
6660         ...     ],
6661         ...     dims="time",
6662         ... )
6663         >>> da
6664         <xarray.DataArray (time: 12)>
6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6666         Coordinates:
6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6668         >>> da.rolling(time=3, center=True).mean()
6669         <xarray.DataArray (time: 12)>
6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
6671         Coordinates:
6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6673 
6674         Remove the NaNs using ``dropna()``:
6675 
6676         >>> da.rolling(time=3, center=True).mean().dropna("time")
6677         <xarray.DataArray (time: 10)>
6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
6679         Coordinates:
6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
6681 
6682         See Also
6683         --------
6684         core.rolling.DataArrayRolling
6685         Dataset.rolling
6686         """
6687         from xarray.core.rolling import DataArrayRolling
6688 
6689         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
6691 
6692     def coarsen(
6693         self,
6694         dim: Mapping[Any, int] | None = None,
6695         boundary: CoarsenBoundaryOptions = "exact",
6696         side: SideOptions | Mapping[Any, SideOptions] = "left",
6697         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
6698         **window_kwargs: int,
6699     ) -> DataArrayCoarsen:
6700         """
6701         Coarsen object for DataArrays.
6702 
6703         Parameters
6704         ----------
6705         dim : mapping of hashable to int, optional
6706             Mapping from the dimension name to the window size.
6707         boundary : {"exact", "trim", "pad"}, default: "exact"
6708             If 'exact', a ValueError will be raised if dimension size is not a
6709             multiple of the window size. If 'trim', the excess entries are
6710             dropped. If 'pad', NA will be padded.
6711         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
6712         coord_func : str or mapping of hashable to str, default: "mean"
6713             function (name) that is applied to the coordinates,
6714             or a mapping from coordinate name to function (name).
6715 
6716         Returns
6717         -------
6718         core.rolling.DataArrayCoarsen
6719 
6720         Examples
6721         --------
6722         Coarsen the long time series by averaging over every three days.
6723 
6724         >>> da = xr.DataArray(
6725         ...     np.linspace(0, 364, num=364),
6726         ...     dims="time",
6727         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
6728         ... )
6729         >>> da  # +doctest: ELLIPSIS
6730         <xarray.DataArray (time: 364)>
6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
6734         ...
6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
6736                360.99173554, 361.99449036, 362.99724518, 364.        ])
6737         Coordinates:
6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
6739         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
6740         <xarray.DataArray (time: 121)>
6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
6744         ...
6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
6746                361.99449036])
6747         Coordinates:
6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
6749         >>>
6750 
6751         See Also
6752         --------
6753         core.rolling.DataArrayCoarsen
6754         Dataset.coarsen
6755         """
6756         from xarray.core.rolling import DataArrayCoarsen
6757 
6758         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
6759         return DataArrayCoarsen(
6760             self,
6761             dim,
6762             boundary=boundary,
6763             side=side,
6764             coord_func=coord_func,
6765         )
6766 
6767     def resample(
6768         self,
6769         indexer: Mapping[Any, str] | None = None,
6770         skipna: bool | None = None,
6771         closed: SideOptions | None = None,
6772         label: SideOptions | None = None,
6773         base: int | None = None,
6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,
6775         origin: str | DatetimeLike = "start_day",
6776         keep_attrs: bool | None = None,
6777         loffset: datetime.timedelta | str | None = None,
6778         restore_coord_dims: bool | None = None,
6779         **indexer_kwargs: str,
6780     ) -> DataArrayResample:
6781         """Returns a Resample object for performing resampling operations.
6782 
6783         Handles both downsampling and upsampling. The resampled
6784         dimension must be a datetime-like coordinate. If any intervals
6785         contain no values from the original object, they will be given
6786         the value ``NaN``.
6787 
6788         Parameters
6789         ----------
6790         indexer : Mapping of Hashable to str, optional
6791             Mapping from the dimension name to resample frequency [1]_. The
6792             dimension must be datetime-like.
6793         skipna : bool, optional
6794             Whether to skip missing values when aggregating in downsampling.
6795         closed : {"left", "right"}, optional
6796             Side of each interval to treat as closed.
6797         label : {"left", "right"}, optional
6798             Side of each interval to use for labeling.
6799         base : int, optional
6800             For frequencies that evenly subdivide 1 day, the "origin" of the
6801             aggregated intervals. For example, for "24H" frequency, base could
6802             range from 0 through 23.
6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'
6804             The datetime on which to adjust the grouping. The timezone of origin
6805             must match the timezone of the index.
6806 
6807             If a datetime is not used, these values are also supported:
6808             - 'epoch': `origin` is 1970-01-01
6809             - 'start': `origin` is the first value of the timeseries
6810             - 'start_day': `origin` is the first day at midnight of the timeseries
6811             - 'end': `origin` is the last value of the timeseries
6812             - 'end_day': `origin` is the ceiling midnight of the last day
6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None
6814             An offset timedelta added to the origin.
6815         loffset : timedelta or str, optional
6816             Offset used to adjust the resampled time labels. Some pandas date
6817             offset strings are supported.
6818         restore_coord_dims : bool, optional
6819             If True, also restore the dimension order of multi-dimensional
6820             coordinates.
6821         **indexer_kwargs : str
6822             The keyword arguments form of ``indexer``.
6823             One of indexer or indexer_kwargs must be provided.
6824 
6825         Returns
6826         -------
6827         resampled : core.resample.DataArrayResample
6828             This object resampled.
6829 
6830         Examples
6831         --------
6832         Downsample monthly time-series data to seasonal data:
6833 
6834         >>> da = xr.DataArray(
6835         ...     np.linspace(0, 11, num=12),
6836         ...     coords=[
6837         ...         pd.date_range(
6838         ...             "1999-12-15",
6839         ...             periods=12,
6840         ...             freq=pd.DateOffset(months=1),
6841         ...         )
6842         ...     ],
6843         ...     dims="time",
6844         ... )
6845         >>> da
6846         <xarray.DataArray (time: 12)>
6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
6848         Coordinates:
6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
6850         >>> da.resample(time="QS-DEC").mean()
6851         <xarray.DataArray (time: 4)>
6852         array([ 1.,  4.,  7., 10.])
6853         Coordinates:
6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
6855 
6856         Upsample monthly time-series data to daily data:
6857 
6858         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
6859         <xarray.DataArray (time: 337)>
6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
6863         ...
6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
6865                10.96774194, 11.        ])
6866         Coordinates:
6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6868 
6869         Limit scope of upsampling method
6870 
6871         >>> da.resample(time="1D").nearest(tolerance="1D")
6872         <xarray.DataArray (time: 337)>
6873         array([ 0.,  0., nan, ..., nan, 11., 11.])
6874         Coordinates:
6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
6876 
6877         See Also
6878         --------
6879         Dataset.resample
6880         pandas.Series.resample
6881         pandas.DataFrame.resample
6882 
6883         References
6884         ----------
6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
6886         """
6887         from xarray.core.resample import DataArrayResample
6888 
6889         return self._resample(
6890             resample_cls=DataArrayResample,
6891             indexer=indexer,
6892             skipna=skipna,
6893             closed=closed,
6894             label=label,
6895             base=base,
6896             offset=offset,
6897             origin=origin,
6898             keep_attrs=keep_attrs,
6899             loffset=loffset,
6900             restore_coord_dims=restore_coord_dims,
6901             **indexer_kwargs,
6902         )
6903 
6904     def to_dask_dataframe(
6905         self,
6906         dim_order: Sequence[Hashable] | None = None,
6907         set_index: bool = False,
6908     ) -> DaskDataFrame:
6909         """Convert this array into a dask.dataframe.DataFrame.
6910 
6911         Parameters
6912         ----------
6913         dim_order : Sequence of Hashable or None , optional
6914             Hierarchical dimension order for the resulting dataframe.
6915             Array content is transposed to this order and then written out as flat
6916             vectors in contiguous order, so the last dimension in this list
6917             will be contiguous in the resulting DataFrame. This has a major influence
6918             on which operations are efficient on the resulting dask dataframe.
6919         set_index : bool, default: False
6920             If set_index=True, the dask DataFrame is indexed by this dataset's
6921             coordinate. Since dask DataFrames do not support multi-indexes,
6922             set_index only works if the dataset only contains one dimension.
6923 
6924         Returns
6925         -------
6926         dask.dataframe.DataFrame
6927 
6928         Examples
6929         --------
6930         >>> da = xr.DataArray(
6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),
6932         ...     dims=("time", "lat", "lon"),
6933         ...     coords={
6934         ...         "time": np.arange(4),
6935         ...         "lat": [-30, -20],
6936         ...         "lon": [120, 130],
6937         ...     },
6938         ...     name="eg_dataarray",
6939         ...     attrs={"units": "Celsius", "description": "Random temperature data"},
6940         ... )
6941         >>> da.to_dask_dataframe(["lat", "lon", "time"]).compute()
6942             lat  lon  time  eg_dataarray
6943         0   -30  120     0             0
6944         1   -30  120     1             4
6945         2   -30  120     2             8
6946         3   -30  120     3            12
6947         4   -30  130     0             1
6948         5   -30  130     1             5
6949         6   -30  130     2             9
6950         7   -30  130     3            13
6951         8   -20  120     0             2
6952         9   -20  120     1             6
6953         10  -20  120     2            10
6954         11  -20  120     3            14
6955         12  -20  130     0             3
6956         13  -20  130     1             7
6957         14  -20  130     2            11
6958         15  -20  130     3            15
6959         """
6960         if self.name is None:
6961             raise ValueError(
6962                 "Cannot convert an unnamed DataArray to a "
6963                 "dask dataframe : use the ``.rename`` method to assign a name."
6964             )
6965         name = self.name
6966         ds = self._to_dataset_whole(name, shallow_copy=False)
6967         return ds.to_dask_dataframe(dim_order, set_index)
6968 
6969     # this needs to be at the end, or mypy will confuse with `str`
6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
6971     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 22:04:43.252 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 22:04:43.252 | INFO     | Search-only: skipping patch selection
2025-10-15 22:04:43.357 | INFO     | Task pydata__xarray-7019 completed successfully.
2025-10-15 22:04:43.358 | INFO     | No patch generated. You can try running ACR again.
