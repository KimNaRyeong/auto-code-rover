[
    {
        "rel_file_path": "xarray/core/groupby.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/groupby.py",
        "start": 492,
        "end": 719,
        "class_name": "DataArrayGroupBy",
        "method_name": null,
        "code": "492 class DataArrayGroupBy(GroupBy, ImplementsArrayReduce):\n493     \"\"\"GroupBy object specialized to grouping DataArray objects\n494     \"\"\"\n495 \n496     def _iter_grouped_shortcut(self):\n497         \"\"\"Fast version of `_iter_grouped` that yields Variables without\n498         metadata\n499         \"\"\"\n500         var = self._obj.variable\n501         for indices in self._group_indices:\n502             yield var[{self._group_dim: indices}]\n503 \n504     def _concat_shortcut(self, applied, dim, positions=None):\n505         # nb. don't worry too much about maintaining this method -- it does\n506         # speed things up, but it's not very interpretable and there are much\n507         # faster alternatives (e.g., doing the grouped aggregation in a\n508         # compiled language)\n509         stacked = Variable.concat(applied, dim, shortcut=True)\n510         reordered = _maybe_reorder(stacked, dim, positions)\n511         result = self._obj._replace_maybe_drop_dims(reordered)\n512         return result\n513 \n514     def _restore_dim_order(self, stacked):\n515         def lookup_order(dimension):\n516             if dimension == self._group.name:\n517                 dimension, = self._group.dims\n518             if dimension in self._obj.dims:\n519                 axis = self._obj.get_axis_num(dimension)\n520             else:\n521                 axis = 1e6  # some arbitrarily high value\n522             return axis\n523 \n524         new_order = sorted(stacked.dims, key=lookup_order)\n525         return stacked.transpose(\n526             *new_order, transpose_coords=self._restore_coord_dims)\n527 \n528     def apply(self, func, shortcut=False, args=(), **kwargs):\n529         \"\"\"Apply a function over each array in the group and concatenate them\n530         together into a new array.\n531 \n532         `func` is called like `func(ar, *args, **kwargs)` for each array `ar`\n533         in this group.\n534 \n535         Apply uses heuristics (like `pandas.GroupBy.apply`) to figure out how\n536         to stack together the array. The rule is:\n537 \n538         1. If the dimension along which the group coordinate is defined is\n539            still in the first grouped array after applying `func`, then stack\n540            over this dimension.\n541         2. Otherwise, stack over the new dimension given by name of this\n542            grouping (the argument to the `groupby` function).\n543 \n544         Parameters\n545         ----------\n546         func : function\n547             Callable to apply to each array.\n548         shortcut : bool, optional\n549             Whether or not to shortcut evaluation under the assumptions that:\n550             (1) The action of `func` does not depend on any of the array\n551                 metadata (attributes or coordinates) but only on the data and\n552                 dimensions.\n553             (2) The action of `func` creates arrays with homogeneous metadata,\n554                 that is, with the same dimensions and attributes.\n555             If these conditions are satisfied `shortcut` provides significant\n556             speedup. This should be the case for many common groupby operations\n557             (e.g., applying numpy ufuncs).\n558         args : tuple, optional\n559             Positional arguments passed to `func`.\n560         **kwargs\n561             Used to call `func(ar, **kwargs)` for each array `ar`.\n562 \n563         Returns\n564         -------\n565         applied : DataArray or DataArray\n566             The result of splitting, applying and combining this array.\n567         \"\"\"\n568         if shortcut:\n569             grouped = self._iter_grouped_shortcut()\n570         else:\n571             grouped = self._iter_grouped()\n572         applied = (maybe_wrap_array(arr, func(arr, *args, **kwargs))\n573                    for arr in grouped)\n574         return self._combine(applied, shortcut=shortcut)\n575 \n576     def _combine(self, applied, restore_coord_dims=False, shortcut=False):\n577         \"\"\"Recombine the applied objects like the original.\"\"\"\n578         applied_example, applied = peek_at(applied)\n579         coord, dim, positions = self._infer_concat_args(applied_example)\n580         if shortcut:\n581             combined = self._concat_shortcut(applied, dim, positions)\n582         else:\n583             combined = concat(applied, dim)\n584             combined = _maybe_reorder(combined, dim, positions)\n585 \n586         if isinstance(combined, type(self._obj)):\n587             # only restore dimension order for arrays\n588             combined = self._restore_dim_order(combined)\n589         if coord is not None:\n590             if shortcut:\n591                 combined._coords[coord.name] = as_variable(coord)\n592             else:\n593                 combined.coords[coord.name] = coord\n594         combined = self._maybe_restore_empty_groups(combined)\n595         combined = self._maybe_unstack(combined)\n596         return combined\n597 \n598     def quantile(self, q, dim=None, interpolation='linear', keep_attrs=None):\n599         \"\"\"Compute the qth quantile over each array in the groups and\n600         concatenate them together into a new array.\n601 \n602         Parameters\n603         ----------\n604         q : float in range of [0,1] (or sequence of floats)\n605             Quantile to compute, which must be between 0 and 1\n606             inclusive.\n607         dim : str or sequence of str, optional\n608             Dimension(s) over which to apply quantile.\n609             Defaults to the grouped dimension.\n610         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n611             This optional parameter specifies the interpolation method to\n612             use when the desired quantile lies between two data points\n613             ``i < j``:\n614                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n615                   the fractional part of the index surrounded by ``i`` and\n616                   ``j``.\n617                 * lower: ``i``.\n618                 * higher: ``j``.\n619                 * nearest: ``i`` or ``j``, whichever is nearest.\n620                 * midpoint: ``(i + j) / 2``.\n621 \n622         Returns\n623         -------\n624         quantiles : Variable\n625             If `q` is a single quantile, then the result\n626             is a scalar. If multiple percentiles are given, first axis of\n627             the result corresponds to the quantile and a quantile dimension\n628             is added to the return array. The other dimensions are the\n629             dimensions that remain after the reduction of the array.\n630 \n631         See Also\n632         --------\n633         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,\n634         DataArray.quantile\n635         \"\"\"\n636         if dim == DEFAULT_DIMS:\n637             dim = ALL_DIMS\n638             # TODO change this to dim = self._group_dim after\n639             # the deprecation process\n640             if self._obj.ndim > 1:\n641                 warnings.warn(\n642                     \"Default reduction dimension will be changed to the \"\n643                     \"grouped dimension in a future version of xarray. To \"\n644                     \"silence this warning, pass dim=xarray.ALL_DIMS \"\n645                     \"explicitly.\",\n646                     FutureWarning, stacklevel=2)\n647 \n648         out = self.apply(self._obj.__class__.quantile, shortcut=False,\n649                          q=q, dim=dim, interpolation=interpolation,\n650                          keep_attrs=keep_attrs)\n651 \n652         if np.asarray(q, dtype=np.float64).ndim == 0:\n653             out = out.drop('quantile')\n654         return out\n655 \n656     def reduce(self, func, dim=None, axis=None, keep_attrs=None,\n657                shortcut=True, **kwargs):\n658         \"\"\"Reduce the items in this group by applying `func` along some\n659         dimension(s).\n660 \n661         Parameters\n662         ----------\n663         func : function\n664             Function which can be called in the form\n665             `func(x, axis=axis, **kwargs)` to return the result of collapsing\n666             an np.ndarray over an integer valued axis.\n667         dim : str or sequence of str, optional\n668             Dimension(s) over which to apply `func`.\n669         axis : int or sequence of int, optional\n670             Axis(es) over which to apply `func`. Only one of the 'dimension'\n671             and 'axis' arguments can be supplied. If neither are supplied, then\n672             `func` is calculated over all dimension for each group item.\n673         keep_attrs : bool, optional\n674             If True, the datasets's attributes (`attrs`) will be copied from\n675             the original object to the new one.  If False (default), the new\n676             object will be returned without attributes.\n677         **kwargs : dict\n678             Additional keyword arguments passed on to `func`.\n679 \n680         Returns\n681         -------\n682         reduced : Array\n683             Array with summarized data and the indicated dimension(s)\n684             removed.\n685         \"\"\"\n686         if dim == DEFAULT_DIMS:\n687             dim = ALL_DIMS\n688             # TODO change this to dim = self._group_dim after\n689             # the deprecation process\n690             if self._obj.ndim > 1:\n691                 warnings.warn(\n692                     \"Default reduction dimension will be changed to the \"\n693                     \"grouped dimension in a future version of xarray. To \"\n694                     \"silence this warning, pass dim=xarray.ALL_DIMS \"\n695                     \"explicitly.\",\n696                     FutureWarning, stacklevel=2)\n697 \n698         if keep_attrs is None:\n699             keep_attrs = _get_keep_attrs(default=False)\n700 \n701         def reduce_array(ar):\n702             return ar.reduce(func, dim, axis, keep_attrs=keep_attrs, **kwargs)\n703         return self.apply(reduce_array, shortcut=shortcut)\n704 \n705     # TODO remove the following class method and DEFAULT_DIMS after the\n706     # deprecation cycle\n707     @classmethod\n708     def _reduce_method(cls, func, include_skipna, numeric_only):\n709         if include_skipna:\n710             def wrapped_func(self, dim=DEFAULT_DIMS, axis=None, skipna=None,\n711                              keep_attrs=None, **kwargs):\n712                 return self.reduce(func, dim, axis, keep_attrs=keep_attrs,\n713                                    skipna=skipna, allow_lazy=True, **kwargs)\n714         else:\n715             def wrapped_func(self, dim=DEFAULT_DIMS, axis=None,  # type: ignore\n716                              keep_attrs=None, **kwargs):\n717                 return self.reduce(func, dim, axis, keep_attrs=keep_attrs,\n718                                    allow_lazy=True, **kwargs)\n719         return wrapped_func\n",
        "intended_behavior": "These methods should include a pre-check for empty `DataArray` instances before proceeding with any grouped operation. If the `DataArray` is empty, the method should either: Return an empty `DataArray` with the appropriate metadata preserved (e.g., dimensions and coordinates if applicable without the grouped dimension), or Raise a custom, informative error instead of `StopIteration`, suggesting that the operation cannot proceed due to the absence of data. This error message should guide the user toward understanding the operation failed due to empty input rather than a logic error within their code or the library."
    },
    {
        "rel_file_path": "xarray/core/common.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/common.py",
        "start": 450,
        "end": 499,
        "class_name": "DataWithCoords",
        "method_name": "groupby",
        "code": "450     def groupby(self, group, squeeze: bool = True,\n451                 restore_coord_dims: Optional[bool] = None):\n452         \"\"\"Returns a GroupBy object for performing grouped operations.\n453 \n454         Parameters\n455         ----------\n456         group : str, DataArray or IndexVariable\n457             Array whose unique values should be used to group this array. If a\n458             string, must be the name of a variable contained in this dataset.\n459         squeeze : boolean, optional\n460             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n461             controls whether the subarrays have a dimension of length 1 along\n462             that dimension or if the dimension is squeezed out.\n463         restore_coord_dims : bool, optional\n464             If True, also restore the dimension order of multi-dimensional\n465             coordinates.\n466 \n467         Returns\n468         -------\n469         grouped : GroupBy\n470             A `GroupBy` object patterned after `pandas.GroupBy` that can be\n471             iterated over in the form of `(unique_value, grouped_array)` pairs.\n472 \n473         Examples\n474         --------\n475         Calculate daily anomalies for daily data:\n476 \n477         >>> da = xr.DataArray(np.linspace(0, 1826, num=1827),\n478         ...                   coords=[pd.date_range('1/1/2000', '31/12/2004',\n479         ...                           freq='D')],\n480         ...                   dims='time')\n481         >>> da\n482         <xarray.DataArray (time: 1827)>\n483         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n484         Coordinates:\n485           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n486         >>> da.groupby('time.dayofyear') - da.groupby('time.dayofyear').mean('time')\n487         <xarray.DataArray (time: 1827)>\n488         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n489         Coordinates:\n490           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n491             dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n492 \n493         See Also\n494         --------\n495         core.groupby.DataArrayGroupBy\n496         core.groupby.DatasetGroupBy\n497         \"\"\"  # noqa\n498         return self._groupby_cls(self, group, squeeze=squeeze,\n499                                  restore_coord_dims=restore_coord_dims)\n",
        "intended_behavior": "While this method's primary role is to set up groupby operations, including a safety check or warning here about potential issues when working with empty `DataArray` or datasets could preempt confusion. Specifically, it could: Check if the `DataArray` is empty before attempting to group, issuing a warning that some operations (such as mean reduction) may not behave as expected on empty DataArrays. Though not directly preventing the `StopIteration`, it increases user awareness about potential pitfalls."
    },
    {
        "rel_file_path": "xarray/core/common.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/common.py",
        "start": 253,
        "end": 1003,
        "class_name": "DataWithCoords",
        "method_name": null,
        "code": "253 class DataWithCoords(SupportsArithmetic, AttrAccessMixin):\n254     \"\"\"Shared base class for Dataset and DataArray.\"\"\"\n255 \n256     _rolling_exp_cls = RollingExp\n257 \n258     def squeeze(self, dim: Union[Hashable, Iterable[Hashable], None] = None,\n259                 drop: bool = False,\n260                 axis: Union[int, Iterable[int], None] = None):\n261         \"\"\"Return a new object with squeezed data.\n262 \n263         Parameters\n264         ----------\n265         dim : None or Hashable or iterable of Hashable, optional\n266             Selects a subset of the length one dimensions. If a dimension is\n267             selected with length greater than one, an error is raised. If\n268             None, all length one dimensions are squeezed.\n269         drop : bool, optional\n270             If ``drop=True``, drop squeezed coordinates instead of making them\n271             scalar.\n272         axis : None or int or iterable of int, optional\n273             Like dim, but positional.\n274 \n275         Returns\n276         -------\n277         squeezed : same type as caller\n278             This object, but with with all or a subset of the dimensions of\n279             length 1 removed.\n280 \n281         See Also\n282         --------\n283         numpy.squeeze\n284         \"\"\"\n285         dims = get_squeeze_dims(self, dim, axis)\n286         return self.isel(drop=drop, **{d: 0 for d in dims})\n287 \n288     def get_index(self, key: Hashable) -> pd.Index:\n289         \"\"\"Get an index for a dimension, with fall-back to a default RangeIndex\n290         \"\"\"\n291         if key not in self.dims:\n292             raise KeyError(key)\n293 \n294         try:\n295             return self.indexes[key]\n296         except KeyError:\n297             # need to ensure dtype=int64 in case range is empty on Python 2\n298             return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)\n299 \n300     def _calc_assign_results(self, kwargs: Mapping[str, T]\n301                              ) -> MutableMapping[str, T]:\n302         results = SortedKeysDict()  # type: SortedKeysDict[str, T]\n303         for k, v in kwargs.items():\n304             if callable(v):\n305                 results[k] = v(self)\n306             else:\n307                 results[k] = v\n308         return results\n309 \n310     def assign_coords(self, **kwargs):\n311         \"\"\"Assign new coordinates to this object.\n312 \n313         Returns a new object with all the original data in addition to the new\n314         coordinates.\n315 \n316         Parameters\n317         ----------\n318         kwargs : keyword, value pairs\n319             keywords are the variables names. If the values are callable, they\n320             are computed on this object and assigned to new coordinate\n321             variables. If the values are not callable, (e.g. a DataArray,\n322             scalar, or array), they are simply assigned.\n323 \n324         Returns\n325         -------\n326         assigned : same type as caller\n327             A new object with the new coordinates in addition to the existing\n328             data.\n329 \n330         Examples\n331         --------\n332 \n333         Convert longitude coordinates from 0-359 to -180-179:\n334 \n335         >>> da = xr.DataArray(np.random.rand(4),\n336         ...                   coords=[np.array([358, 359, 0, 1])],\n337         ...                   dims='lon')\n338         >>> da\n339         <xarray.DataArray (lon: 4)>\n340         array([0.28298 , 0.667347, 0.657938, 0.177683])\n341         Coordinates:\n342           * lon      (lon) int64 358 359 0 1\n343         >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n344         <xarray.DataArray (lon: 4)>\n345         array([0.28298 , 0.667347, 0.657938, 0.177683])\n346         Coordinates:\n347           * lon      (lon) int64 -2 -1 0 1\n348 \n349         Notes\n350         -----\n351         Since ``kwargs`` is a dictionary, the order of your arguments may not\n352         be preserved, and so the order of the new variables is not well\n353         defined. Assigning multiple variables within the same ``assign_coords``\n354         is possible, but you cannot reference other variables created within\n355         the same ``assign_coords`` call.\n356 \n357         See also\n358         --------\n359         Dataset.assign\n360         Dataset.swap_dims\n361         \"\"\"\n362         data = self.copy(deep=False)\n363         results = self._calc_assign_results(kwargs)\n364         data.coords.update(results)\n365         return data\n366 \n367     def assign_attrs(self, *args, **kwargs):\n368         \"\"\"Assign new attrs to this object.\n369 \n370         Returns a new object equivalent to self.attrs.update(*args, **kwargs).\n371 \n372         Parameters\n373         ----------\n374         args : positional arguments passed into ``attrs.update``.\n375         kwargs : keyword arguments passed into ``attrs.update``.\n376 \n377         Returns\n378         -------\n379         assigned : same type as caller\n380             A new object with the new attrs in addition to the existing data.\n381 \n382         See also\n383         --------\n384         Dataset.assign\n385         \"\"\"\n386         out = self.copy(deep=False)\n387         out.attrs.update(*args, **kwargs)\n388         return out\n389 \n390     def pipe(self, func: Union[Callable[..., T], Tuple[Callable[..., T], str]],\n391              *args, **kwargs) -> T:\n392         \"\"\"\n393         Apply func(self, *args, **kwargs)\n394 \n395         This method replicates the pandas method of the same name.\n396 \n397         Parameters\n398         ----------\n399         func : function\n400             function to apply to this xarray object (Dataset/DataArray).\n401             ``args``, and ``kwargs`` are passed into ``func``.\n402             Alternatively a ``(callable, data_keyword)`` tuple where\n403             ``data_keyword`` is a string indicating the keyword of\n404             ``callable`` that expects the xarray object.\n405         args : positional arguments passed into ``func``.\n406         kwargs : a dictionary of keyword arguments passed into ``func``.\n407 \n408         Returns\n409         -------\n410         object : the return type of ``func``.\n411 \n412         Notes\n413         -----\n414 \n415         Use ``.pipe`` when chaining together functions that expect\n416         xarray or pandas objects, e.g., instead of writing\n417 \n418         >>> f(g(h(ds), arg1=a), arg2=b, arg3=c)\n419 \n420         You can write\n421 \n422         >>> (ds.pipe(h)\n423         ...    .pipe(g, arg1=a)\n424         ...    .pipe(f, arg2=b, arg3=c)\n425         ... )\n426 \n427         If you have a function that takes the data as (say) the second\n428         argument, pass a tuple indicating which keyword expects the\n429         data. For example, suppose ``f`` takes its data as ``arg2``:\n430 \n431         >>> (ds.pipe(h)\n432         ...    .pipe(g, arg1=a)\n433         ...    .pipe((f, 'arg2'), arg1=a, arg3=c)\n434         ...  )\n435 \n436         See Also\n437         --------\n438         pandas.DataFrame.pipe\n439         \"\"\"\n440         if isinstance(func, tuple):\n441             func, target = func\n442             if target in kwargs:\n443                 raise ValueError('%s is both the pipe target and a keyword '\n444                                  'argument' % target)\n445             kwargs[target] = self\n446             return func(*args, **kwargs)\n447         else:\n448             return func(self, *args, **kwargs)\n449 \n450     def groupby(self, group, squeeze: bool = True,\n451                 restore_coord_dims: Optional[bool] = None):\n452         \"\"\"Returns a GroupBy object for performing grouped operations.\n453 \n454         Parameters\n455         ----------\n456         group : str, DataArray or IndexVariable\n457             Array whose unique values should be used to group this array. If a\n458             string, must be the name of a variable contained in this dataset.\n459         squeeze : boolean, optional\n460             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n461             controls whether the subarrays have a dimension of length 1 along\n462             that dimension or if the dimension is squeezed out.\n463         restore_coord_dims : bool, optional\n464             If True, also restore the dimension order of multi-dimensional\n465             coordinates.\n466 \n467         Returns\n468         -------\n469         grouped : GroupBy\n470             A `GroupBy` object patterned after `pandas.GroupBy` that can be\n471             iterated over in the form of `(unique_value, grouped_array)` pairs.\n472 \n473         Examples\n474         --------\n475         Calculate daily anomalies for daily data:\n476 \n477         >>> da = xr.DataArray(np.linspace(0, 1826, num=1827),\n478         ...                   coords=[pd.date_range('1/1/2000', '31/12/2004',\n479         ...                           freq='D')],\n480         ...                   dims='time')\n481         >>> da\n482         <xarray.DataArray (time: 1827)>\n483         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03, 1.826e+03])\n484         Coordinates:\n485           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n486         >>> da.groupby('time.dayofyear') - da.groupby('time.dayofyear').mean('time')\n487         <xarray.DataArray (time: 1827)>\n488         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n489         Coordinates:\n490           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n491             dayofyear  (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...\n492 \n493         See Also\n494         --------\n495         core.groupby.DataArrayGroupBy\n496         core.groupby.DatasetGroupBy\n497         \"\"\"  # noqa\n498         return self._groupby_cls(self, group, squeeze=squeeze,\n499                                  restore_coord_dims=restore_coord_dims)\n500 \n501     def groupby_bins(self, group, bins, right: bool = True, labels=None,\n502                      precision: int = 3, include_lowest: bool = False,\n503                      squeeze: bool = True,\n504                      restore_coord_dims: Optional[bool] = None):\n505         \"\"\"Returns a GroupBy object for performing grouped operations.\n506 \n507         Rather than using all unique values of `group`, the values are discretized\n508         first by applying `pandas.cut` [1]_ to `group`.\n509 \n510         Parameters\n511         ----------\n512         group : str, DataArray or IndexVariable\n513             Array whose binned values should be used to group this array. If a\n514             string, must be the name of a variable contained in this dataset.\n515         bins : int or array of scalars\n516             If bins is an int, it defines the number of equal-width bins in the\n517             range of x. However, in this case, the range of x is extended by .1%\n518             on each side to include the min or max values of x. If bins is a\n519             sequence it defines the bin edges allowing for non-uniform bin\n520             width. No extension of the range of x is done in this case.\n521         right : boolean, optional\n522             Indicates whether the bins include the rightmost edge or not. If\n523             right == True (the default), then the bins [1,2,3,4] indicate\n524             (1,2], (2,3], (3,4].\n525         labels : array or boolean, default None\n526             Used as labels for the resulting bins. Must be of the same length as\n527             the resulting bins. If False, string bin labels are assigned by\n528             `pandas.cut`.\n529         precision : int\n530             The precision at which to store and display the bins labels.\n531         include_lowest : bool\n532             Whether the first interval should be left-inclusive or not.\n533         squeeze : boolean, optional\n534             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n535             controls whether the subarrays have a dimension of length 1 along\n536             that dimension or if the dimension is squeezed out.\n537         restore_coord_dims : bool, optional\n538             If True, also restore the dimension order of multi-dimensional\n539             coordinates.\n540 \n541         Returns\n542         -------\n543         grouped : GroupBy\n544             A `GroupBy` object patterned after `pandas.GroupBy` that can be\n545             iterated over in the form of `(unique_value, grouped_array)` pairs.\n546             The name of the group has the added suffix `_bins` in order to\n547             distinguish it from the original variable.\n548 \n549         References\n550         ----------\n551         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n552         \"\"\"  # noqa\n553         return self._groupby_cls(self, group, squeeze=squeeze, bins=bins,\n554                                  restore_coord_dims=restore_coord_dims,\n555                                  cut_kwargs={'right': right, 'labels': labels,\n556                                              'precision': precision,\n557                                              'include_lowest':\n558                                                  include_lowest})\n559 \n560     def rolling(self, dim: Optional[Mapping[Hashable, int]] = None,\n561                 min_periods: Optional[int] = None, center: bool = False,\n562                 **window_kwargs: int):\n563         \"\"\"\n564         Rolling window object.\n565 \n566         Parameters\n567         ----------\n568         dim: dict, optional\n569             Mapping from the dimension name to create the rolling iterator\n570             along (e.g. `time`) to its moving window size.\n571         min_periods : int, default None\n572             Minimum number of observations in window required to have a value\n573             (otherwise result is NA). The default, None, is equivalent to\n574             setting min_periods equal to the size of the window.\n575         center : boolean, default False\n576             Set the labels at the center of the window.\n577         **window_kwargs : optional\n578             The keyword arguments form of ``dim``.\n579             One of dim or window_kwargs must be provided.\n580 \n581         Returns\n582         -------\n583         Rolling object (core.rolling.DataArrayRolling for DataArray,\n584         core.rolling.DatasetRolling for Dataset.)\n585 \n586         Examples\n587         --------\n588         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n589 \n590         >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n591         ...                   coords=[pd.date_range('15/12/1999',\n592         ...                           periods=12, freq=pd.DateOffset(months=1))],\n593         ...                   dims='time')\n594         >>> da\n595         <xarray.DataArray (time: 12)>\n596         array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n597         Coordinates:\n598           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n599         >>> da.rolling(time=3, center=True).mean()\n600         <xarray.DataArray (time: 12)>\n601         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n602         Coordinates:\n603           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n604 \n605         Remove the NaNs using ``dropna()``:\n606 \n607         >>> da.rolling(time=3, center=True).mean().dropna('time')\n608         <xarray.DataArray (time: 10)>\n609         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n610         Coordinates:\n611           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 2000-03-15 ...\n612 \n613         See Also\n614         --------\n615         core.rolling.DataArrayRolling\n616         core.rolling.DatasetRolling\n617         \"\"\"  # noqa\n618         dim = either_dict_or_kwargs(dim, window_kwargs, 'rolling')\n619         return self._rolling_cls(self, dim, min_periods=min_periods,\n620                                  center=center)\n621 \n622     def rolling_exp(\n623         self,\n624         window: Optional[Mapping[Hashable, int]] = None,\n625         window_type: str = 'span',\n626         **window_kwargs\n627     ):\n628         \"\"\"\n629         Exponentially-weighted moving window.\n630         Similar to EWM in pandas\n631 \n632         Requires the optional Numbagg dependency.\n633 \n634         Parameters\n635         ----------\n636         window : A single mapping from a dimension name to window value,\n637                  optional\n638             dim : str\n639                 Name of the dimension to create the rolling exponential window\n640                 along (e.g., `time`).\n641             window : int\n642                 Size of the moving window. The type of this is specified in\n643                 `window_type`\n644         window_type : str, one of ['span', 'com', 'halflife', 'alpha'],\n645                       default 'span'\n646             The format of the previously supplied window. Each is a simple\n647             numerical transformation of the others. Described in detail:\n648             https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html\n649         **window_kwargs : optional\n650             The keyword arguments form of ``window``.\n651             One of window or window_kwargs must be provided.\n652 \n653         See Also\n654         --------\n655         core.rolling_exp.RollingExp\n656         \"\"\"\n657         window = either_dict_or_kwargs(window, window_kwargs, 'rolling_exp')\n658 \n659         return self._rolling_exp_cls(self, window, window_type)\n660 \n661     def coarsen(self, dim: Optional[Mapping[Hashable, int]] = None,\n662                 boundary: str = 'exact',\n663                 side: Union[str, Mapping[Hashable, str]] = 'left',\n664                 coord_func: str = 'mean',\n665                 **window_kwargs: int):\n666         \"\"\"\n667         Coarsen object.\n668 \n669         Parameters\n670         ----------\n671         dim: dict, optional\n672             Mapping from the dimension name to the window size.\n673             dim : str\n674                 Name of the dimension to create the rolling iterator\n675                 along (e.g., `time`).\n676             window : int\n677                 Size of the moving window.\n678         boundary : 'exact' | 'trim' | 'pad'\n679             If 'exact', a ValueError will be raised if dimension size is not a\n680             multiple of the window size. If 'trim', the excess entries are\n681             dropped. If 'pad', NA will be padded.\n682         side : 'left' or 'right' or mapping from dimension to 'left' or 'right'\n683         coord_func: function (name) that is applied to the coordintes,\n684             or a mapping from coordinate name to function (name).\n685 \n686         Returns\n687         -------\n688         Coarsen object (core.rolling.DataArrayCoarsen for DataArray,\n689         core.rolling.DatasetCoarsen for Dataset.)\n690 \n691         Examples\n692         --------\n693         Coarsen the long time series by averaging over every four days.\n694 \n695         >>> da = xr.DataArray(np.linspace(0, 364, num=364),\n696         ...                   dims='time',\n697         ...                   coords={'time': pd.date_range(\n698         ...                       '15/12/1999', periods=364)})\n699         >>> da\n700         <xarray.DataArray (time: 364)>\n701         array([  0.      ,   1.002755,   2.00551 , ..., 361.99449 , 362.997245,\n702                364.      ])\n703         Coordinates:\n704           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n705         >>>\n706         >>> da.coarsen(time=3, boundary='trim').mean()\n707         <xarray.DataArray (time: 121)>\n708         array([  1.002755,   4.011019,   7.019284,  ...,  358.986226,\n709                361.99449 ])\n710         Coordinates:\n711           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n712         >>>\n713 \n714         See Also\n715         --------\n716         core.rolling.DataArrayCoarsen\n717         core.rolling.DatasetCoarsen\n718         \"\"\"\n719         dim = either_dict_or_kwargs(dim, window_kwargs, 'coarsen')\n720         return self._coarsen_cls(\n721             self, dim, boundary=boundary, side=side,\n722             coord_func=coord_func)\n723 \n724     def resample(self, indexer: Optional[Mapping[Hashable, str]] = None,\n725                  skipna=None, closed: Optional[str] = None,\n726                  label: Optional[str] = None,\n727                  base: int = 0, keep_attrs: Optional[bool] = None,\n728                  loffset=None, restore_coord_dims: Optional[bool] = None,\n729                  **indexer_kwargs: str):\n730         \"\"\"Returns a Resample object for performing resampling operations.\n731 \n732         Handles both downsampling and upsampling. If any intervals contain no\n733         values from the original object, they will be given the value ``NaN``.\n734 \n735         Parameters\n736         ----------\n737         indexer : {dim: freq}, optional\n738             Mapping from the dimension name to resample frequency.\n739         skipna : bool, optional\n740             Whether to skip missing values when aggregating in downsampling.\n741         closed : 'left' or 'right', optional\n742             Side of each interval to treat as closed.\n743         label : 'left or 'right', optional\n744             Side of each interval to use for labeling.\n745         base : int, optional\n746             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n747             aggregated intervals. For example, for '24H' frequency, base could\n748             range from 0 through 23.\n749         loffset : timedelta or str, optional\n750             Offset used to adjust the resampled time labels. Some pandas date\n751             offset strings are supported.\n752         keep_attrs : bool, optional\n753             If True, the object's attributes (`attrs`) will be copied from\n754             the original object to the new one.  If False (default), the new\n755             object will be returned without attributes.\n756         restore_coord_dims : bool, optional\n757             If True, also restore the dimension order of multi-dimensional\n758             coordinates.\n759         **indexer_kwargs : {dim: freq}\n760             The keyword arguments form of ``indexer``.\n761             One of indexer or indexer_kwargs must be provided.\n762 \n763         Returns\n764         -------\n765         resampled : same type as caller\n766             This object resampled.\n767 \n768         Examples\n769         --------\n770         Downsample monthly time-series data to seasonal data:\n771 \n772         >>> da = xr.DataArray(np.linspace(0, 11, num=12),\n773         ...                   coords=[pd.date_range('15/12/1999',\n774         ...                           periods=12, freq=pd.DateOffset(months=1))],\n775         ...                   dims='time')\n776         >>> da\n777         <xarray.DataArray (time: 12)>\n778         array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7., 8.,   9.,  10.,  11.])\n779         Coordinates:\n780           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 2000-02-15 ...\n781         >>> da.resample(time=\"QS-DEC\").mean()\n782         <xarray.DataArray (time: 4)>\n783         array([ 1.,  4.,  7., 10.])\n784         Coordinates:\n785           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n786 \n787         Upsample monthly time-series data to daily data:\n788 \n789         >>> da.resample(time='1D').interpolate('linear')\n790         <xarray.DataArray (time: 337)>\n791         array([ 0.      ,  0.032258,  0.064516, ..., 10.935484, 10.967742, 11.      ])\n792         Coordinates:\n793           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 1999-12-17 ...\n794 \n795         Limit scope of upsampling method\n796         >>> da.resample(time='1D').nearest(tolerance='1D')\n797         <xarray.DataArray (time: 337)>\n798         array([ 0.,  0., nan, ..., nan, 11., 11.])\n799         Coordinates:\n800           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n801 \n802         References\n803         ----------\n804 \n805         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n806         \"\"\"  # noqa\n807         # TODO support non-string indexer after removing the old API.\n808 \n809         from .dataarray import DataArray\n810         from .resample import RESAMPLE_DIM\n811         from ..coding.cftimeindex import CFTimeIndex\n812 \n813         if keep_attrs is None:\n814             keep_attrs = _get_keep_attrs(default=False)\n815 \n816         # note: the second argument (now 'skipna') use to be 'dim'\n817         if ((skipna is not None and not isinstance(skipna, bool))\n818                 or ('how' in indexer_kwargs and 'how' not in self.dims)\n819                 or ('dim' in indexer_kwargs and 'dim' not in self.dims)):\n820             raise TypeError(\n821                 'resample() no longer supports the `how` or '\n822                 '`dim` arguments. Instead call methods on resample '\n823                 \"objects, e.g., data.resample(time='1D').mean()\")\n824 \n825         indexer = either_dict_or_kwargs(indexer, indexer_kwargs, 'resample')\n826         if len(indexer) != 1:\n827             raise ValueError(\n828                 \"Resampling only supported along single dimensions.\"\n829             )\n830         dim, freq = next(iter(indexer.items()))\n831 \n832         dim_name = dim\n833         dim_coord = self[dim]\n834 \n835         if isinstance(self.indexes[dim_name], CFTimeIndex):\n836             from .resample_cftime import CFTimeGrouper\n837             grouper = CFTimeGrouper(freq, closed, label, base, loffset)\n838         else:\n839             # TODO: to_offset() call required for pandas==0.19.2\n840             grouper = pd.Grouper(freq=freq, closed=closed, label=label,\n841                                  base=base,\n842                                  loffset=pd.tseries.frequencies.to_offset(\n843                                      loffset))\n844         group = DataArray(dim_coord, coords=dim_coord.coords,\n845                           dims=dim_coord.dims, name=RESAMPLE_DIM)\n846         resampler = self._resample_cls(self, group=group, dim=dim_name,\n847                                        grouper=grouper,\n848                                        resample_dim=RESAMPLE_DIM,\n849                                        restore_coord_dims=restore_coord_dims)\n850 \n851         return resampler\n852 \n853     def where(self, cond, other=dtypes.NA, drop: bool = False):\n854         \"\"\"Filter elements from this object according to a condition.\n855 \n856         This operation follows the normal broadcasting and alignment rules that\n857         xarray uses for binary arithmetic.\n858 \n859         Parameters\n860         ----------\n861         cond : DataArray or Dataset with boolean dtype\n862             Locations at which to preserve this object's values.\n863         other : scalar, DataArray or Dataset, optional\n864             Value to use for locations in this object where ``cond`` is False.\n865             By default, these locations filled with NA.\n866         drop : boolean, optional\n867             If True, coordinate labels that only correspond to False values of\n868             the condition are dropped from the result. Mutually exclusive with\n869             ``other``.\n870 \n871         Returns\n872         -------\n873         Same type as caller.\n874 \n875         Examples\n876         --------\n877 \n878         >>> import numpy as np\n879         >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=('x', 'y'))\n880         >>> a.where(a.x + a.y < 4)\n881         <xarray.DataArray (x: 5, y: 5)>\n882         array([[  0.,   1.,   2.,   3.,  nan],\n883                [  5.,   6.,   7.,  nan,  nan],\n884                [ 10.,  11.,  nan,  nan,  nan],\n885                [ 15.,  nan,  nan,  nan,  nan],\n886                [ nan,  nan,  nan,  nan,  nan]])\n887         Dimensions without coordinates: x, y\n888         >>> a.where(a.x + a.y < 5, -1)\n889         <xarray.DataArray (x: 5, y: 5)>\n890         array([[ 0,  1,  2,  3,  4],\n891                [ 5,  6,  7,  8, -1],\n892                [10, 11, 12, -1, -1],\n893                [15, 16, -1, -1, -1],\n894                [20, -1, -1, -1, -1]])\n895         Dimensions without coordinates: x, y\n896         >>> a.where(a.x + a.y < 4, drop=True)\n897         <xarray.DataArray (x: 4, y: 4)>\n898         array([[  0.,   1.,   2.,   3.],\n899                [  5.,   6.,   7.,  nan],\n900                [ 10.,  11.,  nan,  nan],\n901                [ 15.,  nan,  nan,  nan]])\n902         Dimensions without coordinates: x, y\n903 \n904         See also\n905         --------\n906         numpy.where : corresponding numpy function\n907         where : equivalent function\n908         \"\"\"\n909         from .alignment import align\n910         from .dataarray import DataArray\n911         from .dataset import Dataset\n912 \n913         if drop:\n914             if other is not dtypes.NA:\n915                 raise ValueError('cannot set `other` if drop=True')\n916 \n917             if not isinstance(cond, (Dataset, DataArray)):\n918                 raise TypeError(\"cond argument is %r but must be a %r or %r\" %\n919                                 (cond, Dataset, DataArray))\n920 \n921             # align so we can use integer indexing\n922             self, cond = align(self, cond)\n923 \n924             # get cond with the minimal size needed for the Dataset\n925             if isinstance(cond, Dataset):\n926                 clipcond = cond.to_array().any('variable')\n927             else:\n928                 clipcond = cond\n929 \n930             # clip the data corresponding to coordinate dims that are not used\n931             nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))\n932             indexers = {k: np.unique(v) for k, v in nonzeros}\n933 \n934             self = self.isel(**indexers)\n935             cond = cond.isel(**indexers)\n936 \n937         return ops.where_method(self, cond, other)\n938 \n939     def close(self: Any) -> None:\n940         \"\"\"Close any files linked to this object\n941         \"\"\"\n942         if self._file_obj is not None:\n943             self._file_obj.close()\n944         self._file_obj = None\n945 \n946     def isin(self, test_elements):\n947         \"\"\"Tests each value in the array for whether it is in test elements.\n948 \n949         Parameters\n950         ----------\n951         test_elements : array_like\n952             The values against which to test each value of `element`.\n953             This argument is flattened if an array or array_like.\n954             See numpy notes for behavior with non-array-like parameters.\n955 \n956         Returns\n957         -------\n958         isin : same as object, bool\n959             Has the same shape as this object.\n960 \n961         Examples\n962         --------\n963 \n964         >>> array = xr.DataArray([1, 2, 3], dims='x')\n965         >>> array.isin([1, 3])\n966         <xarray.DataArray (x: 3)>\n967         array([ True, False,  True])\n968         Dimensions without coordinates: x\n969 \n970         See also\n971         --------\n972         numpy.isin\n973         \"\"\"\n974         from .computation import apply_ufunc\n975         from .dataset import Dataset\n976         from .dataarray import DataArray\n977         from .variable import Variable\n978 \n979         if isinstance(test_elements, Dataset):\n980             raise TypeError(\n981                 'isin() argument must be convertible to an array: {}'\n982                 .format(test_elements))\n983         elif isinstance(test_elements, (Variable, DataArray)):\n984             # need to explicitly pull out data to support dask arrays as the\n985             # second argument\n986             test_elements = test_elements.data\n987 \n988         return apply_ufunc(\n989             duck_array_ops.isin,\n990             self,\n991             kwargs=dict(test_elements=test_elements),\n992             dask='allowed',\n993         )\n994 \n995     def __enter__(self: T) -> T:\n996         return self\n997 \n998     def __exit__(self, exc_type, exc_value, traceback) -> None:\n999         self.close()\n1000 \n1001     def __getitem__(self, value):\n1002         # implementations of this class should implement this method\n1003         raise NotImplementedError\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]