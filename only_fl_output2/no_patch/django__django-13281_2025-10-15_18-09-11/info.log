2025-10-15 18:09:12.155 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 18:09:14.992 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=557, output_tokens=12, cost=0.005930
2025-10-15 18:09:34.890 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=737, output_tokens=711, cost=0.028700
2025-10-15 18:09:36.160 | INFO     | registered non reproducing test 0
2025-10-15 18:10:04.858 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2945, output_tokens=1028, cost=0.060290
2025-10-15 18:10:06.048 | INFO     | registered non reproducing test 1
2025-10-15 18:10:26.594 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2086, output_tokens=873, cost=0.047050
2025-10-15 18:10:27.793 | INFO     | registered non reproducing test 2
2025-10-15 18:10:27.793 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 18:10:27.799 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 18:10:38.057 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=848, output_tokens=372, cost=0.019640
2025-10-15 18:10:38.065 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 18:10:41.826 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=726, output_tokens=105, cost=0.010410
2025-10-15 18:10:41.828 | DEBUG    | Extracted a valid json.
2025-10-15 18:10:41.828 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Bar\")",
        "search_class(\"Foo\")",
        "search_class(\"TestCase\")",
        "search_method_in_class(\"copy\", \"Bar\")",
        "search_method_in_class(\"copy\", \"Foo\")",
        "search_method_in_class(\"deepcopy\", \"Bar\")",
        "search_method_in_class(\"deepcopy\", \"Foo\")",
        "search_code(\"_state.fields_cache\")"
    ],
    "bug_locations": []
}
2025-10-15 18:10:41.874 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 18:10:41.875 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 18:11:02.413 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1547, output_tokens=744, cost=0.037790
2025-10-15 18:11:02.428 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 18:11:13.554 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2507, output_tokens=350, cost=0.035570
2025-10-15 18:11:13.561 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 18:11:22.793 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=704, output_tokens=316, cost=0.016520
2025-10-15 18:11:22.794 | DEBUG    | Extracted a valid json.
2025-10-15 18:11:22.794 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/fields/mixins.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file likely contains mixins used by Django model fields. The code handling the caching of field values in instances, especially regarding ForeignKey fields, needs to ensure that upon copying a model instance, the field cache (`_state.fields_cache`) does not cause unintended sharing of related objects between the original and the copied instance. Specifically, the mechanism in place for caching related field objects should be adjusted or extended to clear or re-initialize the cache for copied instances, ensuring that changes to related fields on the copied instance do not affect the original instance's cached relations. This might involve modifying the mix-in to intercept copy operations and appropriately handle the fields cache."
        },
        {
            "file": "django/db/models/query.py",
            "class": "",
            "method": "",
            "intended_behavior": "Considering this file deals with querying, it also might include logic for how model instances are handled post-retrieval from the database, including caching. The intended behavior here would be to audit and potentially adjust the code paths that manage `_state.fields_cache` during or after instantiation and retrieval operations to accommodate the nuances of model instance copying, ensuring that each instance maintains its own independent cache unless explicitly shared. This ensures that operations on copied model instances do not have unintended side effects on the caching of related objects of the original instances."
        }
    ]
}
2025-10-15 18:11:22.808 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/fields/mixins.py</file>
<class>None</class>
<method>None</method>
<code>
1 from django.core import checks
2 
3 NOT_PROVIDED = object()
4 
5 
6 class FieldCacheMixin:
7     """Provide an API for working with the model's fields value cache."""
8 
9     def get_cache_name(self):
10         raise NotImplementedError
11 
12     def get_cached_value(self, instance, default=NOT_PROVIDED):
13         cache_name = self.get_cache_name()
14         try:
15             return instance._state.fields_cache[cache_name]
16         except KeyError:
17             if default is NOT_PROVIDED:
18                 raise
19             return default
20 
21     def is_cached(self, instance):
22         return self.get_cache_name() in instance._state.fields_cache
23 
24     def set_cached_value(self, instance, value):
25         instance._state.fields_cache[self.get_cache_name()] = value
26 
27     def delete_cached_value(self, instance):
28         del instance._state.fields_cache[self.get_cache_name()]
29 
30 
31 class CheckFieldDefaultMixin:
32     _default_hint = ('<valid default>', '<invalid default>')
33 
34     def _check_default(self):
35         if self.has_default() and self.default is not None and not callable(self.default):
36             return [
37                 checks.Warning(
38                     "%s default should be a callable instead of an instance "
39                     "so that it's not shared between all field instances." % (
40                         self.__class__.__name__,
41                     ),
42                     hint=(
43                         'Use a callable instead, e.g., use `%s` instead of '
44                         '`%s`.' % self._default_hint
45                     ),
46                     obj=self,
47                     id='fields.E010',
48                 )
49             ]
50         else:
51             return []
52 
53     def check(self, **kwargs):
54         errors = super().check(**kwargs)
55         errors.extend(self._check_default())
56         return errors

</code><intended_behavior>This file likely contains mixins used by Django model fields. The code handling the caching of field values in instances, especially regarding ForeignKey fields, needs to ensure that upon copying a model instance, the field cache (`_state.fields_cache`) does not cause unintended sharing of related objects between the original and the copied instance. Specifically, the mechanism in place for caching related field objects should be adjusted or extended to clear or re-initialize the cache for copied instances, ensuring that changes to related fields on the copied instance do not affect the original instance's cached relations. This might involve modifying the mix-in to intercept copy operations and appropriately handle the fields cache.</intended_behavior>, <file>django/db/models/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The main QuerySet implementation. This provides the public API for the ORM.
3 """
4 
5 import copy
6 import operator
7 import warnings
8 from collections import namedtuple
9 from functools import lru_cache
10 from itertools import chain
11 
12 import django
13 from django.conf import settings
14 from django.core import exceptions
15 from django.db import (
16     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,
17     router, transaction,
18 )
19 from django.db.models import AutoField, DateField, DateTimeField, sql
20 from django.db.models.constants import LOOKUP_SEP
21 from django.db.models.deletion import Collector
22 from django.db.models.expressions import Case, Expression, F, Value, When
23 from django.db.models.functions import Cast, Trunc
24 from django.db.models.query_utils import FilteredRelation, Q
25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
26 from django.db.models.utils import resolve_callables
27 from django.utils import timezone
28 from django.utils.functional import cached_property, partition
29 
30 # The maximum number of results to fetch in a get() query.
31 MAX_GET_RESULTS = 21
32 
33 # The maximum number of items to display in a QuerySet.__repr__
34 REPR_OUTPUT_SIZE = 20
35 
36 
37 class BaseIterable:
38     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
39         self.queryset = queryset
40         self.chunked_fetch = chunked_fetch
41         self.chunk_size = chunk_size
42 
43 
44 class ModelIterable(BaseIterable):
45     """Iterable that yields a model instance for each row."""
46 
47     def __iter__(self):
48         queryset = self.queryset
49         db = queryset.db
50         compiler = queryset.query.get_compiler(using=db)
51         # Execute the query. This will also fill compiler.select, klass_info,
52         # and annotations.
53         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
54         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,
55                                                   compiler.annotation_col_map)
56         model_cls = klass_info['model']
57         select_fields = klass_info['select_fields']
58         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1
59         init_list = [f[0].target.attname
60                      for f in select[model_fields_start:model_fields_end]]
61         related_populators = get_related_populators(klass_info, select, db)
62         known_related_objects = [
63             (field, related_objs, operator.attrgetter(*[
64                 field.attname
65                 if from_field == 'self' else
66                 queryset.model._meta.get_field(from_field).attname
67                 for from_field in field.from_fields
68             ])) for field, related_objs in queryset._known_related_objects.items()
69         ]
70         for row in compiler.results_iter(results):
71             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])
72             for rel_populator in related_populators:
73                 rel_populator.populate(row, obj)
74             if annotation_col_map:
75                 for attr_name, col_pos in annotation_col_map.items():
76                     setattr(obj, attr_name, row[col_pos])
77 
78             # Add the known related objects to the model.
79             for field, rel_objs, rel_getter in known_related_objects:
80                 # Avoid overwriting objects loaded by, e.g., select_related().
81                 if field.is_cached(obj):
82                     continue
83                 rel_obj_id = rel_getter(obj)
84                 try:
85                     rel_obj = rel_objs[rel_obj_id]
86                 except KeyError:
87                     pass  # May happen in qs1 | qs2 scenarios.
88                 else:
89                     setattr(obj, field.name, rel_obj)
90 
91             yield obj
92 
93 
94 class ValuesIterable(BaseIterable):
95     """
96     Iterable returned by QuerySet.values() that yields a dict for each row.
97     """
98 
99     def __iter__(self):
100         queryset = self.queryset
101         query = queryset.query
102         compiler = query.get_compiler(queryset.db)
103 
104         # extra(select=...) cols are always at the start of the row.
105         names = [
106             *query.extra_select,
107             *query.values_select,
108             *query.annotation_select,
109         ]
110         indexes = range(len(names))
111         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
112             yield {names[i]: row[i] for i in indexes}
113 
114 
115 class ValuesListIterable(BaseIterable):
116     """
117     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple
118     for each row.
119     """
120 
121     def __iter__(self):
122         queryset = self.queryset
123         query = queryset.query
124         compiler = query.get_compiler(queryset.db)
125 
126         if queryset._fields:
127             # extra(select=...) cols are always at the start of the row.
128             names = [
129                 *query.extra_select,
130                 *query.values_select,
131                 *query.annotation_select,
132             ]
133             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]
134             if fields != names:
135                 # Reorder according to fields.
136                 index_map = {name: idx for idx, name in enumerate(names)}
137                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])
138                 return map(
139                     rowfactory,
140                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
141                 )
142         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
143 
144 
145 class NamedValuesListIterable(ValuesListIterable):
146     """
147     Iterable returned by QuerySet.values_list(named=True) that yields a
148     namedtuple for each row.
149     """
150 
151     @staticmethod
152     @lru_cache()
153     def create_namedtuple_class(*names):
154         # Cache namedtuple() with @lru_cache() since it's too slow to be
155         # called for every QuerySet evaluation.
156         return namedtuple('Row', names)
157 
158     def __iter__(self):
159         queryset = self.queryset
160         if queryset._fields:
161             names = queryset._fields
162         else:
163             query = queryset.query
164             names = [*query.extra_select, *query.values_select, *query.annotation_select]
165         tuple_class = self.create_namedtuple_class(*names)
166         new = tuple.__new__
167         for row in super().__iter__():
168             yield new(tuple_class, row)
169 
170 
171 class FlatValuesListIterable(BaseIterable):
172     """
173     Iterable returned by QuerySet.values_list(flat=True) that yields single
174     values.
175     """
176 
177     def __iter__(self):
178         queryset = self.queryset
179         compiler = queryset.query.get_compiler(queryset.db)
180         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
181             yield row[0]
182 
183 
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, args, kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, args, kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, args, kwargs)
951 
952     def _filter_or_exclude(self, negate, args, kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, args, kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, args, kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         return self._annotate(args, kwargs, select=True)
1089 
1090     def alias(self, *args, **kwargs):
1091         """
1092         Return a query set with added aliases for extra data or aggregations.
1093         """
1094         self._not_support_combined_queries('alias')
1095         return self._annotate(args, kwargs, select=False)
1096 
1097     def _annotate(self, args, kwargs, select=True):
1098         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1099         annotations = {}
1100         for arg in args:
1101             # The default_alias property may raise a TypeError.
1102             try:
1103                 if arg.default_alias in kwargs:
1104                     raise ValueError("The named annotation '%s' conflicts with the "
1105                                      "default name for another annotation."
1106                                      % arg.default_alias)
1107             except TypeError:
1108                 raise TypeError("Complex annotations require an alias")
1109             annotations[arg.default_alias] = arg
1110         annotations.update(kwargs)
1111 
1112         clone = self._chain()
1113         names = self._fields
1114         if names is None:
1115             names = set(chain.from_iterable(
1116                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1117                 for field in self.model._meta.get_fields()
1118             ))
1119 
1120         for alias, annotation in annotations.items():
1121             if alias in names:
1122                 raise ValueError("The annotation '%s' conflicts with a field on "
1123                                  "the model." % alias)
1124             if isinstance(annotation, FilteredRelation):
1125                 clone.query.add_filtered_relation(annotation, alias)
1126             else:
1127                 clone.query.add_annotation(
1128                     annotation, alias, is_summary=False, select=select,
1129                 )
1130         for alias, annotation in clone.query.annotations.items():
1131             if alias in annotations and annotation.contains_aggregate:
1132                 if clone._fields is None:
1133                     clone.query.group_by = True
1134                 else:
1135                     clone.query.set_group_by()
1136                 break
1137 
1138         return clone
1139 
1140     def order_by(self, *field_names):
1141         """Return a new QuerySet instance with the ordering changed."""
1142         assert not self.query.is_sliced, \
1143             "Cannot reorder a query once a slice has been taken."
1144         obj = self._chain()
1145         obj.query.clear_ordering(force_empty=False)
1146         obj.query.add_ordering(*field_names)
1147         return obj
1148 
1149     def distinct(self, *field_names):
1150         """
1151         Return a new QuerySet instance that will select only distinct results.
1152         """
1153         self._not_support_combined_queries('distinct')
1154         assert not self.query.is_sliced, \
1155             "Cannot create distinct fields once a slice has been taken."
1156         obj = self._chain()
1157         obj.query.add_distinct_fields(*field_names)
1158         return obj
1159 
1160     def extra(self, select=None, where=None, params=None, tables=None,
1161               order_by=None, select_params=None):
1162         """Add extra SQL fragments to the query."""
1163         self._not_support_combined_queries('extra')
1164         assert not self.query.is_sliced, \
1165             "Cannot change a query once a slice has been taken"
1166         clone = self._chain()
1167         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1168         return clone
1169 
1170     def reverse(self):
1171         """Reverse the ordering of the QuerySet."""
1172         if self.query.is_sliced:
1173             raise TypeError('Cannot reverse a query once a slice has been taken.')
1174         clone = self._chain()
1175         clone.query.standard_ordering = not clone.query.standard_ordering
1176         return clone
1177 
1178     def defer(self, *fields):
1179         """
1180         Defer the loading of data for certain fields until they are accessed.
1181         Add the set of deferred fields to any existing set of deferred fields.
1182         The only exception to this is if None is passed in as the only
1183         parameter, in which case removal all deferrals.
1184         """
1185         self._not_support_combined_queries('defer')
1186         if self._fields is not None:
1187             raise TypeError("Cannot call defer() after .values() or .values_list()")
1188         clone = self._chain()
1189         if fields == (None,):
1190             clone.query.clear_deferred_loading()
1191         else:
1192             clone.query.add_deferred_loading(fields)
1193         return clone
1194 
1195     def only(self, *fields):
1196         """
1197         Essentially, the opposite of defer(). Only the fields passed into this
1198         method and that are not already specified as deferred are loaded
1199         immediately when the queryset is evaluated.
1200         """
1201         self._not_support_combined_queries('only')
1202         if self._fields is not None:
1203             raise TypeError("Cannot call only() after .values() or .values_list()")
1204         if fields == (None,):
1205             # Can only pass None to defer(), not only(), as the rest option.
1206             # That won't stop people trying to do this, so let's be explicit.
1207             raise TypeError("Cannot pass None as an argument to only().")
1208         for field in fields:
1209             field = field.split(LOOKUP_SEP, 1)[0]
1210             if field in self.query._filtered_relations:
1211                 raise ValueError('only() is not supported with FilteredRelation.')
1212         clone = self._chain()
1213         clone.query.add_immediate_loading(fields)
1214         return clone
1215 
1216     def using(self, alias):
1217         """Select which database this QuerySet should execute against."""
1218         clone = self._chain()
1219         clone._db = alias
1220         return clone
1221 
1222     ###################################
1223     # PUBLIC INTROSPECTION ATTRIBUTES #
1224     ###################################
1225 
1226     @property
1227     def ordered(self):
1228         """
1229         Return True if the QuerySet is ordered -- i.e. has an order_by()
1230         clause or a default ordering on the model (or is empty).
1231         """
1232         if isinstance(self, EmptyQuerySet):
1233             return True
1234         if self.query.extra_order_by or self.query.order_by:
1235             return True
1236         elif self.query.default_ordering and self.query.get_meta().ordering:
1237             return True
1238         else:
1239             return False
1240 
1241     @property
1242     def db(self):
1243         """Return the database used if this query is executed now."""
1244         if self._for_write:
1245             return self._db or router.db_for_write(self.model, **self._hints)
1246         return self._db or router.db_for_read(self.model, **self._hints)
1247 
1248     ###################
1249     # PRIVATE METHODS #
1250     ###################
1251 
1252     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1253         """
1254         Insert a new record for the given model. This provides an interface to
1255         the InsertQuery class and is how Model.save() is implemented.
1256         """
1257         self._for_write = True
1258         if using is None:
1259             using = self.db
1260         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1261         query.insert_values(fields, objs, raw=raw)
1262         return query.get_compiler(using=using).execute_sql(returning_fields)
1263     _insert.alters_data = True
1264     _insert.queryset_only = False
1265 
1266     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1267         """
1268         Helper method for bulk_create() to insert objs one batch at a time.
1269         """
1270         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1271             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1272         ops = connections[self.db].ops
1273         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1274         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1275         inserted_rows = []
1276         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1277         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1278             if bulk_return and not ignore_conflicts:
1279                 inserted_rows.extend(self._insert(
1280                     item, fields=fields, using=self.db,
1281                     returning_fields=self.model._meta.db_returning_fields,
1282                     ignore_conflicts=ignore_conflicts,
1283                 ))
1284             else:
1285                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1286         return inserted_rows
1287 
1288     def _chain(self, **kwargs):
1289         """
1290         Return a copy of the current QuerySet that's ready for another
1291         operation.
1292         """
1293         obj = self._clone()
1294         if obj._sticky_filter:
1295             obj.query.filter_is_sticky = True
1296             obj._sticky_filter = False
1297         obj.__dict__.update(kwargs)
1298         return obj
1299 
1300     def _clone(self):
1301         """
1302         Return a copy of the current QuerySet. A lightweight alternative
1303         to deepcopy().
1304         """
1305         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1306         c._sticky_filter = self._sticky_filter
1307         c._for_write = self._for_write
1308         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1309         c._known_related_objects = self._known_related_objects
1310         c._iterable_class = self._iterable_class
1311         c._fields = self._fields
1312         return c
1313 
1314     def _fetch_all(self):
1315         if self._result_cache is None:
1316             self._result_cache = list(self._iterable_class(self))
1317         if self._prefetch_related_lookups and not self._prefetch_done:
1318             self._prefetch_related_objects()
1319 
1320     def _next_is_sticky(self):
1321         """
1322         Indicate that the next filter call and the one following that should
1323         be treated as a single filter. This is only important when it comes to
1324         determining when to reuse tables for many-to-many filters. Required so
1325         that we can filter naturally on the results of related managers.
1326 
1327         This doesn't return a clone of the current QuerySet (it returns
1328         "self"). The method is only used internally and should be immediately
1329         followed by a filter() that does create a clone.
1330         """
1331         self._sticky_filter = True
1332         return self
1333 
1334     def _merge_sanity_check(self, other):
1335         """Check that two QuerySet classes may be merged."""
1336         if self._fields is not None and (
1337                 set(self.query.values_select) != set(other.query.values_select) or
1338                 set(self.query.extra_select) != set(other.query.extra_select) or
1339                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1340             raise TypeError(
1341                 "Merging '%s' classes must involve the same values in each case."
1342                 % self.__class__.__name__
1343             )
1344 
1345     def _merge_known_related_objects(self, other):
1346         """
1347         Keep track of all known related objects from either QuerySet instance.
1348         """
1349         for field, objects in other._known_related_objects.items():
1350             self._known_related_objects.setdefault(field, {}).update(objects)
1351 
1352     def resolve_expression(self, *args, **kwargs):
1353         if self._fields and len(self._fields) > 1:
1354             # values() queryset can only be used as nested queries
1355             # if they are set up to select only a single field.
1356             raise TypeError('Cannot use multi-field values as a filter value.')
1357         query = self.query.resolve_expression(*args, **kwargs)
1358         query._db = self._db
1359         return query
1360     resolve_expression.queryset_only = True
1361 
1362     def _add_hints(self, **hints):
1363         """
1364         Update hinting information for use by routers. Add new key/values or
1365         overwrite existing key/values.
1366         """
1367         self._hints.update(hints)
1368 
1369     def _has_filters(self):
1370         """
1371         Check if this QuerySet has any filtering going on. This isn't
1372         equivalent with checking if all objects are present in results, for
1373         example, qs[1:]._has_filters() -> False.
1374         """
1375         return self.query.has_filters()
1376 
1377     @staticmethod
1378     def _validate_values_are_expressions(values, method_name):
1379         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1380         if invalid_args:
1381             raise TypeError(
1382                 'QuerySet.%s() received non-expression(s): %s.' % (
1383                     method_name,
1384                     ', '.join(invalid_args),
1385                 )
1386             )
1387 
1388     def _not_support_combined_queries(self, operation_name):
1389         if self.query.combinator:
1390             raise NotSupportedError(
1391                 'Calling QuerySet.%s() after %s() is not supported.'
1392                 % (operation_name, self.query.combinator)
1393             )
1394 
1395 
1396 class InstanceCheckMeta(type):
1397     def __instancecheck__(self, instance):
1398         return isinstance(instance, QuerySet) and instance.query.is_empty()
1399 
1400 
1401 class EmptyQuerySet(metaclass=InstanceCheckMeta):
1402     """
1403     Marker class to checking if a queryset is empty by .none():
1404         isinstance(qs.none(), EmptyQuerySet) -> True
1405     """
1406 
1407     def __init__(self, *args, **kwargs):
1408         raise TypeError("EmptyQuerySet can't be instantiated")
1409 
1410 
1411 class RawQuerySet:
1412     """
1413     Provide an iterator which converts the results of raw SQL queries into
1414     annotated model instances.
1415     """
1416     def __init__(self, raw_query, model=None, query=None, params=None,
1417                  translations=None, using=None, hints=None):
1418         self.raw_query = raw_query
1419         self.model = model
1420         self._db = using
1421         self._hints = hints or {}
1422         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
1423         self.params = params or ()
1424         self.translations = translations or {}
1425         self._result_cache = None
1426         self._prefetch_related_lookups = ()
1427         self._prefetch_done = False
1428 
1429     def resolve_model_init_order(self):
1430         """Resolve the init field names and value positions."""
1431         converter = connections[self.db].introspection.identifier_converter
1432         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]
1433         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)
1434                              if column not in self.model_fields]
1435         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]
1436         model_init_names = [f.attname for f in model_init_fields]
1437         return model_init_names, model_init_order, annotation_fields
1438 
1439     def prefetch_related(self, *lookups):
1440         """Same as QuerySet.prefetch_related()"""
1441         clone = self._clone()
1442         if lookups == (None,):
1443             clone._prefetch_related_lookups = ()
1444         else:
1445             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1446         return clone
1447 
1448     def _prefetch_related_objects(self):
1449         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1450         self._prefetch_done = True
1451 
1452     def _clone(self):
1453         """Same as QuerySet._clone()"""
1454         c = self.__class__(
1455             self.raw_query, model=self.model, query=self.query, params=self.params,
1456             translations=self.translations, using=self._db, hints=self._hints
1457         )
1458         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1459         return c
1460 
1461     def _fetch_all(self):
1462         if self._result_cache is None:
1463             self._result_cache = list(self.iterator())
1464         if self._prefetch_related_lookups and not self._prefetch_done:
1465             self._prefetch_related_objects()
1466 
1467     def __len__(self):
1468         self._fetch_all()
1469         return len(self._result_cache)
1470 
1471     def __bool__(self):
1472         self._fetch_all()
1473         return bool(self._result_cache)
1474 
1475     def __iter__(self):
1476         self._fetch_all()
1477         return iter(self._result_cache)
1478 
1479     def iterator(self):
1480         # Cache some things for performance reasons outside the loop.
1481         db = self.db
1482         compiler = connections[db].ops.compiler('SQLCompiler')(
1483             self.query, connections[db], db
1484         )
1485 
1486         query = iter(self.query)
1487 
1488         try:
1489             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
1490             if self.model._meta.pk.attname not in model_init_names:
1491                 raise exceptions.FieldDoesNotExist(
1492                     'Raw query must include the primary key'
1493                 )
1494             model_cls = self.model
1495             fields = [self.model_fields.get(c) for c in self.columns]
1496             converters = compiler.get_converters([
1497                 f.get_col(f.model._meta.db_table) if f else None for f in fields
1498             ])
1499             if converters:
1500                 query = compiler.apply_converters(query, converters)
1501             for values in query:
1502                 # Associate fields to values
1503                 model_init_values = [values[pos] for pos in model_init_pos]
1504                 instance = model_cls.from_db(db, model_init_names, model_init_values)
1505                 if annotation_fields:
1506                     for column, pos in annotation_fields:
1507                         setattr(instance, column, values[pos])
1508                 yield instance
1509         finally:
1510             # Done iterating the Query. If it has its own cursor, close it.
1511             if hasattr(self.query, 'cursor') and self.query.cursor:
1512                 self.query.cursor.close()
1513 
1514     def __repr__(self):
1515         return "<%s: %s>" % (self.__class__.__name__, self.query)
1516 
1517     def __getitem__(self, k):
1518         return list(self)[k]
1519 
1520     @property
1521     def db(self):
1522         """Return the database used if this query is executed now."""
1523         return self._db or router.db_for_read(self.model, **self._hints)
1524 
1525     def using(self, alias):
1526         """Select the database this RawQuerySet should execute against."""
1527         return RawQuerySet(
1528             self.raw_query, model=self.model,
1529             query=self.query.chain(using=alias),
1530             params=self.params, translations=self.translations,
1531             using=alias,
1532         )
1533 
1534     @cached_property
1535     def columns(self):
1536         """
1537         A list of model field names in the order they'll appear in the
1538         query results.
1539         """
1540         columns = self.query.get_columns()
1541         # Adjust any column names which don't match field names
1542         for (query_name, model_name) in self.translations.items():
1543             # Ignore translations for nonexistent column names
1544             try:
1545                 index = columns.index(query_name)
1546             except ValueError:
1547                 pass
1548             else:
1549                 columns[index] = model_name
1550         return columns
1551 
1552     @cached_property
1553     def model_fields(self):
1554         """A dict mapping column names to model field names."""
1555         converter = connections[self.db].introspection.identifier_converter
1556         model_fields = {}
1557         for field in self.model._meta.fields:
1558             name, column = field.get_attname_column()
1559             model_fields[converter(column)] = field
1560         return model_fields
1561 
1562 
1563 class Prefetch:
1564     def __init__(self, lookup, queryset=None, to_attr=None):
1565         # `prefetch_through` is the path we traverse to perform the prefetch.
1566         self.prefetch_through = lookup
1567         # `prefetch_to` is the path to the attribute that stores the result.
1568         self.prefetch_to = lookup
1569         if queryset is not None and (
1570             isinstance(queryset, RawQuerySet) or (
1571                 hasattr(queryset, '_iterable_class') and
1572                 not issubclass(queryset._iterable_class, ModelIterable)
1573             )
1574         ):
1575             raise ValueError(
1576                 'Prefetch querysets cannot use raw(), values(), and '
1577                 'values_list().'
1578             )
1579         if to_attr:
1580             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])
1581 
1582         self.queryset = queryset
1583         self.to_attr = to_attr
1584 
1585     def __getstate__(self):
1586         obj_dict = self.__dict__.copy()
1587         if self.queryset is not None:
1588             # Prevent the QuerySet from being evaluated
1589             obj_dict['queryset'] = self.queryset._chain(
1590                 _result_cache=[],
1591                 _prefetch_done=True,
1592             )
1593         return obj_dict
1594 
1595     def add_prefix(self, prefix):
1596         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through
1597         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to
1598 
1599     def get_current_prefetch_to(self, level):
1600         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])
1601 
1602     def get_current_to_attr(self, level):
1603         parts = self.prefetch_to.split(LOOKUP_SEP)
1604         to_attr = parts[level]
1605         as_attr = self.to_attr and level == len(parts) - 1
1606         return to_attr, as_attr
1607 
1608     def get_current_queryset(self, level):
1609         if self.get_current_prefetch_to(level) == self.prefetch_to:
1610             return self.queryset
1611         return None
1612 
1613     def __eq__(self, other):
1614         if not isinstance(other, Prefetch):
1615             return NotImplemented
1616         return self.prefetch_to == other.prefetch_to
1617 
1618     def __hash__(self):
1619         return hash((self.__class__, self.prefetch_to))
1620 
1621 
1622 def normalize_prefetch_lookups(lookups, prefix=None):
1623     """Normalize lookups into Prefetch objects."""
1624     ret = []
1625     for lookup in lookups:
1626         if not isinstance(lookup, Prefetch):
1627             lookup = Prefetch(lookup)
1628         if prefix:
1629             lookup.add_prefix(prefix)
1630         ret.append(lookup)
1631     return ret
1632 
1633 
1634 def prefetch_related_objects(model_instances, *related_lookups):
1635     """
1636     Populate prefetched object caches for a list of model instances based on
1637     the lookups/Prefetch instances given.
1638     """
1639     if not model_instances:
1640         return  # nothing to do
1641 
1642     # We need to be able to dynamically add to the list of prefetch_related
1643     # lookups that we look up (see below).  So we need some book keeping to
1644     # ensure we don't do duplicate work.
1645     done_queries = {}    # dictionary of things like 'foo__bar': [results]
1646 
1647     auto_lookups = set()  # we add to this as we go through.
1648     followed_descriptors = set()  # recursion protection
1649 
1650     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))
1651     while all_lookups:
1652         lookup = all_lookups.pop()
1653         if lookup.prefetch_to in done_queries:
1654             if lookup.queryset is not None:
1655                 raise ValueError("'%s' lookup was already seen with a different queryset. "
1656                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)
1657 
1658             continue
1659 
1660         # Top level, the list of objects to decorate is the result cache
1661         # from the primary QuerySet. It won't be for deeper levels.
1662         obj_list = model_instances
1663 
1664         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)
1665         for level, through_attr in enumerate(through_attrs):
1666             # Prepare main instances
1667             if not obj_list:
1668                 break
1669 
1670             prefetch_to = lookup.get_current_prefetch_to(level)
1671             if prefetch_to in done_queries:
1672                 # Skip any prefetching, and any object preparation
1673                 obj_list = done_queries[prefetch_to]
1674                 continue
1675 
1676             # Prepare objects:
1677             good_objects = True
1678             for obj in obj_list:
1679                 # Since prefetching can re-use instances, it is possible to have
1680                 # the same instance multiple times in obj_list, so obj might
1681                 # already be prepared.
1682                 if not hasattr(obj, '_prefetched_objects_cache'):
1683                     try:
1684                         obj._prefetched_objects_cache = {}
1685                     except (AttributeError, TypeError):
1686                         # Must be an immutable object from
1687                         # values_list(flat=True), for example (TypeError) or
1688                         # a QuerySet subclass that isn't returning Model
1689                         # instances (AttributeError), either in Django or a 3rd
1690                         # party. prefetch_related() doesn't make sense, so quit.
1691                         good_objects = False
1692                         break
1693             if not good_objects:
1694                 break
1695 
1696             # Descend down tree
1697 
1698             # We assume that objects retrieved are homogeneous (which is the premise
1699             # of prefetch_related), so what applies to first object applies to all.
1700             first_obj = obj_list[0]
1701             to_attr = lookup.get_current_to_attr(level)[0]
1702             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
1703 
1704             if not attr_found:
1705                 raise AttributeError("Cannot find '%s' on %s object, '%s' is an invalid "
1706                                      "parameter to prefetch_related()" %
1707                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))
1708 
1709             if level == len(through_attrs) - 1 and prefetcher is None:
1710                 # Last one, this *must* resolve to something that supports
1711                 # prefetching, otherwise there is no point adding it and the
1712                 # developer asking for it has made a mistake.
1713                 raise ValueError("'%s' does not resolve to an item that supports "
1714                                  "prefetching - this is an invalid parameter to "
1715                                  "prefetch_related()." % lookup.prefetch_through)
1716 
1717             if prefetcher is not None and not is_fetched:
1718                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
1719                 # We need to ensure we don't keep adding lookups from the
1720                 # same relationships to stop infinite recursion. So, if we
1721                 # are already on an automatically added lookup, don't add
1722                 # the new lookups from relationships we've seen already.
1723                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):
1724                     done_queries[prefetch_to] = obj_list
1725                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)
1726                     auto_lookups.update(new_lookups)
1727                     all_lookups.extend(new_lookups)
1728                 followed_descriptors.add(descriptor)
1729             else:
1730                 # Either a singly related object that has already been fetched
1731                 # (e.g. via select_related), or hopefully some other property
1732                 # that doesn't support prefetching but needs to be traversed.
1733 
1734                 # We replace the current list of parent objects with the list
1735                 # of related objects, filtering out empty or missing values so
1736                 # that we can continue with nullable or reverse relations.
1737                 new_obj_list = []
1738                 for obj in obj_list:
1739                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):
1740                         # If related objects have been prefetched, use the
1741                         # cache rather than the object's through_attr.
1742                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))
1743                     else:
1744                         try:
1745                             new_obj = getattr(obj, through_attr)
1746                         except exceptions.ObjectDoesNotExist:
1747                             continue
1748                     if new_obj is None:
1749                         continue
1750                     # We special-case `list` rather than something more generic
1751                     # like `Iterable` because we don't want to accidentally match
1752                     # user models that define __iter__.
1753                     if isinstance(new_obj, list):
1754                         new_obj_list.extend(new_obj)
1755                     else:
1756                         new_obj_list.append(new_obj)
1757                 obj_list = new_obj_list
1758 
1759 
1760 def get_prefetcher(instance, through_attr, to_attr):
1761     """
1762     For the attribute 'through_attr' on the given instance, find
1763     an object that has a get_prefetch_queryset().
1764     Return a 4 tuple containing:
1765     (the object with get_prefetch_queryset (or None),
1766      the descriptor object representing this relationship (or None),
1767      a boolean that is False if the attribute was not found at all,
1768      a boolean that is True if the attribute has already been fetched)
1769     """
1770     prefetcher = None
1771     is_fetched = False
1772 
1773     # For singly related objects, we have to avoid getting the attribute
1774     # from the object, as this will trigger the query. So we first try
1775     # on the class, in order to get the descriptor object.
1776     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)
1777     if rel_obj_descriptor is None:
1778         attr_found = hasattr(instance, through_attr)
1779     else:
1780         attr_found = True
1781         if rel_obj_descriptor:
1782             # singly related object, descriptor object has the
1783             # get_prefetch_queryset() method.
1784             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
1785                 prefetcher = rel_obj_descriptor
1786                 if rel_obj_descriptor.is_cached(instance):
1787                     is_fetched = True
1788             else:
1789                 # descriptor doesn't support prefetching, so we go ahead and get
1790                 # the attribute on the instance rather than the class to
1791                 # support many related managers
1792                 rel_obj = getattr(instance, through_attr)
1793                 if hasattr(rel_obj, 'get_prefetch_queryset'):
1794                     prefetcher = rel_obj
1795                 if through_attr != to_attr:
1796                     # Special case cached_property instances because hasattr
1797                     # triggers attribute computation and assignment.
1798                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
1799                         is_fetched = to_attr in instance.__dict__
1800                     else:
1801                         is_fetched = hasattr(instance, to_attr)
1802                 else:
1803                     is_fetched = through_attr in instance._prefetched_objects_cache
1804     return prefetcher, rel_obj_descriptor, attr_found, is_fetched
1805 
1806 
1807 def prefetch_one_level(instances, prefetcher, lookup, level):
1808     """
1809     Helper function for prefetch_related_objects().
1810 
1811     Run prefetches on all instances using the prefetcher object,
1812     assigning results to relevant caches in instance.
1813 
1814     Return the prefetched objects along with any additional prefetches that
1815     must be done due to prefetch_related lookups found from default managers.
1816     """
1817     # prefetcher must have a method get_prefetch_queryset() which takes a list
1818     # of instances, and returns a tuple:
1819 
1820     # (queryset of instances of self.model that are related to passed in instances,
1821     #  callable that gets value to be matched for returned instances,
1822     #  callable that gets value to be matched for passed in instances,
1823     #  boolean that is True for singly related objects,
1824     #  cache or field name to assign to,
1825     #  boolean that is True when the previous argument is a cache name vs a field name).
1826 
1827     # The 'values to be matched' must be hashable as they will be used
1828     # in a dictionary.
1829 
1830     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (
1831         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))
1832     # We have to handle the possibility that the QuerySet we just got back
1833     # contains some prefetch_related lookups. We don't want to trigger the
1834     # prefetch_related functionality by evaluating the query. Rather, we need
1835     # to merge in the prefetch_related lookups.
1836     # Copy the lookups in case it is a Prefetch object which could be reused
1837     # later (happens in nested prefetch_related).
1838     additional_lookups = [
1839         copy.copy(additional_lookup) for additional_lookup
1840         in getattr(rel_qs, '_prefetch_related_lookups', ())
1841     ]
1842     if additional_lookups:
1843         # Don't need to clone because the manager should have given us a fresh
1844         # instance, so we access an internal instead of using public interface
1845         # for performance reasons.
1846         rel_qs._prefetch_related_lookups = ()
1847 
1848     all_related_objects = list(rel_qs)
1849 
1850     rel_obj_cache = {}
1851     for rel_obj in all_related_objects:
1852         rel_attr_val = rel_obj_attr(rel_obj)
1853         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)
1854 
1855     to_attr, as_attr = lookup.get_current_to_attr(level)
1856     # Make sure `to_attr` does not conflict with a field.
1857     if as_attr and instances:
1858         # We assume that objects retrieved are homogeneous (which is the premise
1859         # of prefetch_related), so what applies to first object applies to all.
1860         model = instances[0].__class__
1861         try:
1862             model._meta.get_field(to_attr)
1863         except exceptions.FieldDoesNotExist:
1864             pass
1865         else:
1866             msg = 'to_attr={} conflicts with a field on the {} model.'
1867             raise ValueError(msg.format(to_attr, model.__name__))
1868 
1869     # Whether or not we're prefetching the last part of the lookup.
1870     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
1871 
1872     for obj in instances:
1873         instance_attr_val = instance_attr(obj)
1874         vals = rel_obj_cache.get(instance_attr_val, [])
1875 
1876         if single:
1877             val = vals[0] if vals else None
1878             if as_attr:
1879                 # A to_attr has been given for the prefetch.
1880                 setattr(obj, to_attr, val)
1881             elif is_descriptor:
1882                 # cache_name points to a field name in obj.
1883                 # This field is a descriptor for a related object.
1884                 setattr(obj, cache_name, val)
1885             else:
1886                 # No to_attr has been given for this prefetch operation and the
1887                 # cache_name does not point to a descriptor. Store the value of
1888                 # the field in the object's field cache.
1889                 obj._state.fields_cache[cache_name] = val
1890         else:
1891             if as_attr:
1892                 setattr(obj, to_attr, vals)
1893             else:
1894                 manager = getattr(obj, to_attr)
1895                 if leaf and lookup.queryset is not None:
1896                     qs = manager._apply_rel_filters(lookup.queryset)
1897                 else:
1898                     qs = manager.get_queryset()
1899                 qs._result_cache = vals
1900                 # We don't want the individual qs doing prefetch_related now,
1901                 # since we have merged this into the current work.
1902                 qs._prefetch_done = True
1903                 obj._prefetched_objects_cache[cache_name] = qs
1904     return all_related_objects, additional_lookups
1905 
1906 
1907 class RelatedPopulator:
1908     """
1909     RelatedPopulator is used for select_related() object instantiation.
1910 
1911     The idea is that each select_related() model will be populated by a
1912     different RelatedPopulator instance. The RelatedPopulator instances get
1913     klass_info and select (computed in SQLCompiler) plus the used db as
1914     input for initialization. That data is used to compute which columns
1915     to use, how to instantiate the model, and how to populate the links
1916     between the objects.
1917 
1918     The actual creation of the objects is done in populate() method. This
1919     method gets row and from_obj as input and populates the select_related()
1920     model instance.
1921     """
1922     def __init__(self, klass_info, select, db):
1923         self.db = db
1924         # Pre-compute needed attributes. The attributes are:
1925         #  - model_cls: the possibly deferred model class to instantiate
1926         #  - either:
1927         #    - cols_start, cols_end: usually the columns in the row are
1928         #      in the same order model_cls.__init__ expects them, so we
1929         #      can instantiate by model_cls(*row[cols_start:cols_end])
1930         #    - reorder_for_init: When select_related descends to a child
1931         #      class, then we want to reuse the already selected parent
1932         #      data. However, in this case the parent data isn't necessarily
1933         #      in the same order that Model.__init__ expects it to be, so
1934         #      we have to reorder the parent data. The reorder_for_init
1935         #      attribute contains a function used to reorder the field data
1936         #      in the order __init__ expects it.
1937         #  - pk_idx: the index of the primary key field in the reordered
1938         #    model data. Used to check if a related object exists at all.
1939         #  - init_list: the field attnames fetched from the database. For
1940         #    deferred models this isn't the same as all attnames of the
1941         #    model's fields.
1942         #  - related_populators: a list of RelatedPopulator instances if
1943         #    select_related() descends to related models from this model.
1944         #  - local_setter, remote_setter: Methods to set cached values on
1945         #    the object being populated and on the remote object. Usually
1946         #    these are Field.set_cached_value() methods.
1947         select_fields = klass_info['select_fields']
1948         from_parent = klass_info['from_parent']
1949         if not from_parent:
1950             self.cols_start = select_fields[0]
1951             self.cols_end = select_fields[-1] + 1
1952             self.init_list = [
1953                 f[0].target.attname for f in select[self.cols_start:self.cols_end]
1954             ]
1955             self.reorder_for_init = None
1956         else:
1957             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}
1958             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)
1959             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]
1960             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])
1961 
1962         self.model_cls = klass_info['model']
1963         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)
1964         self.related_populators = get_related_populators(klass_info, select, self.db)
1965         self.local_setter = klass_info['local_setter']
1966         self.remote_setter = klass_info['remote_setter']
1967 
1968     def populate(self, row, from_obj):
1969         if self.reorder_for_init:
1970             obj_data = self.reorder_for_init(row)
1971         else:
1972             obj_data = row[self.cols_start:self.cols_end]
1973         if obj_data[self.pk_idx] is None:
1974             obj = None
1975         else:
1976             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)
1977             for rel_iter in self.related_populators:
1978                 rel_iter.populate(row, obj)
1979         self.local_setter(from_obj, obj)
1980         if obj is not None:
1981             self.remote_setter(obj, from_obj)
1982 
1983 
1984 def get_related_populators(klass_info, select, db):
1985     iterators = []
1986     related_klass_infos = klass_info.get('related_klass_infos', [])
1987     for rel_klass_info in related_klass_infos:
1988         rel_cls = RelatedPopulator(rel_klass_info, select, db)
1989         iterators.append(rel_cls)
1990     return iterators

</code><intended_behavior>Considering this file deals with querying, it also might include logic for how model instances are handled post-retrieval from the database, including caching. The intended behavior here would be to audit and potentially adjust the code paths that manage `_state.fields_cache` during or after instantiation and retrieval operations to accommodate the nuances of model instance copying, ensuring that each instance maintains its own independent cache unless explicitly shared. This ensures that operations on copied model instances do not have unintended side effects on the caching of related objects of the original instances.</intended_behavior>]
2025-10-15 18:11:22.808 | INFO     | Search completed. Bug locations: [<file>django/db/models/fields/mixins.py</file>
<class>None</class>
<method>None</method>
<code>
1 from django.core import checks
2 
3 NOT_PROVIDED = object()
4 
5 
6 class FieldCacheMixin:
7     """Provide an API for working with the model's fields value cache."""
8 
9     def get_cache_name(self):
10         raise NotImplementedError
11 
12     def get_cached_value(self, instance, default=NOT_PROVIDED):
13         cache_name = self.get_cache_name()
14         try:
15             return instance._state.fields_cache[cache_name]
16         except KeyError:
17             if default is NOT_PROVIDED:
18                 raise
19             return default
20 
21     def is_cached(self, instance):
22         return self.get_cache_name() in instance._state.fields_cache
23 
24     def set_cached_value(self, instance, value):
25         instance._state.fields_cache[self.get_cache_name()] = value
26 
27     def delete_cached_value(self, instance):
28         del instance._state.fields_cache[self.get_cache_name()]
29 
30 
31 class CheckFieldDefaultMixin:
32     _default_hint = ('<valid default>', '<invalid default>')
33 
34     def _check_default(self):
35         if self.has_default() and self.default is not None and not callable(self.default):
36             return [
37                 checks.Warning(
38                     "%s default should be a callable instead of an instance "
39                     "so that it's not shared between all field instances." % (
40                         self.__class__.__name__,
41                     ),
42                     hint=(
43                         'Use a callable instead, e.g., use `%s` instead of '
44                         '`%s`.' % self._default_hint
45                     ),
46                     obj=self,
47                     id='fields.E010',
48                 )
49             ]
50         else:
51             return []
52 
53     def check(self, **kwargs):
54         errors = super().check(**kwargs)
55         errors.extend(self._check_default())
56         return errors

</code><intended_behavior>This file likely contains mixins used by Django model fields. The code handling the caching of field values in instances, especially regarding ForeignKey fields, needs to ensure that upon copying a model instance, the field cache (`_state.fields_cache`) does not cause unintended sharing of related objects between the original and the copied instance. Specifically, the mechanism in place for caching related field objects should be adjusted or extended to clear or re-initialize the cache for copied instances, ensuring that changes to related fields on the copied instance do not affect the original instance's cached relations. This might involve modifying the mix-in to intercept copy operations and appropriately handle the fields cache.</intended_behavior>, <file>django/db/models/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The main QuerySet implementation. This provides the public API for the ORM.
3 """
4 
5 import copy
6 import operator
7 import warnings
8 from collections import namedtuple
9 from functools import lru_cache
10 from itertools import chain
11 
12 import django
13 from django.conf import settings
14 from django.core import exceptions
15 from django.db import (
16     DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,
17     router, transaction,
18 )
19 from django.db.models import AutoField, DateField, DateTimeField, sql
20 from django.db.models.constants import LOOKUP_SEP
21 from django.db.models.deletion import Collector
22 from django.db.models.expressions import Case, Expression, F, Value, When
23 from django.db.models.functions import Cast, Trunc
24 from django.db.models.query_utils import FilteredRelation, Q
25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
26 from django.db.models.utils import resolve_callables
27 from django.utils import timezone
28 from django.utils.functional import cached_property, partition
29 
30 # The maximum number of results to fetch in a get() query.
31 MAX_GET_RESULTS = 21
32 
33 # The maximum number of items to display in a QuerySet.__repr__
34 REPR_OUTPUT_SIZE = 20
35 
36 
37 class BaseIterable:
38     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
39         self.queryset = queryset
40         self.chunked_fetch = chunked_fetch
41         self.chunk_size = chunk_size
42 
43 
44 class ModelIterable(BaseIterable):
45     """Iterable that yields a model instance for each row."""
46 
47     def __iter__(self):
48         queryset = self.queryset
49         db = queryset.db
50         compiler = queryset.query.get_compiler(using=db)
51         # Execute the query. This will also fill compiler.select, klass_info,
52         # and annotations.
53         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
54         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,
55                                                   compiler.annotation_col_map)
56         model_cls = klass_info['model']
57         select_fields = klass_info['select_fields']
58         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1
59         init_list = [f[0].target.attname
60                      for f in select[model_fields_start:model_fields_end]]
61         related_populators = get_related_populators(klass_info, select, db)
62         known_related_objects = [
63             (field, related_objs, operator.attrgetter(*[
64                 field.attname
65                 if from_field == 'self' else
66                 queryset.model._meta.get_field(from_field).attname
67                 for from_field in field.from_fields
68             ])) for field, related_objs in queryset._known_related_objects.items()
69         ]
70         for row in compiler.results_iter(results):
71             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])
72             for rel_populator in related_populators:
73                 rel_populator.populate(row, obj)
74             if annotation_col_map:
75                 for attr_name, col_pos in annotation_col_map.items():
76                     setattr(obj, attr_name, row[col_pos])
77 
78             # Add the known related objects to the model.
79             for field, rel_objs, rel_getter in known_related_objects:
80                 # Avoid overwriting objects loaded by, e.g., select_related().
81                 if field.is_cached(obj):
82                     continue
83                 rel_obj_id = rel_getter(obj)
84                 try:
85                     rel_obj = rel_objs[rel_obj_id]
86                 except KeyError:
87                     pass  # May happen in qs1 | qs2 scenarios.
88                 else:
89                     setattr(obj, field.name, rel_obj)
90 
91             yield obj
92 
93 
94 class ValuesIterable(BaseIterable):
95     """
96     Iterable returned by QuerySet.values() that yields a dict for each row.
97     """
98 
99     def __iter__(self):
100         queryset = self.queryset
101         query = queryset.query
102         compiler = query.get_compiler(queryset.db)
103 
104         # extra(select=...) cols are always at the start of the row.
105         names = [
106             *query.extra_select,
107             *query.values_select,
108             *query.annotation_select,
109         ]
110         indexes = range(len(names))
111         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
112             yield {names[i]: row[i] for i in indexes}
113 
114 
115 class ValuesListIterable(BaseIterable):
116     """
117     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple
118     for each row.
119     """
120 
121     def __iter__(self):
122         queryset = self.queryset
123         query = queryset.query
124         compiler = query.get_compiler(queryset.db)
125 
126         if queryset._fields:
127             # extra(select=...) cols are always at the start of the row.
128             names = [
129                 *query.extra_select,
130                 *query.values_select,
131                 *query.annotation_select,
132             ]
133             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]
134             if fields != names:
135                 # Reorder according to fields.
136                 index_map = {name: idx for idx, name in enumerate(names)}
137                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])
138                 return map(
139                     rowfactory,
140                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
141                 )
142         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
143 
144 
145 class NamedValuesListIterable(ValuesListIterable):
146     """
147     Iterable returned by QuerySet.values_list(named=True) that yields a
148     namedtuple for each row.
149     """
150 
151     @staticmethod
152     @lru_cache()
153     def create_namedtuple_class(*names):
154         # Cache namedtuple() with @lru_cache() since it's too slow to be
155         # called for every QuerySet evaluation.
156         return namedtuple('Row', names)
157 
158     def __iter__(self):
159         queryset = self.queryset
160         if queryset._fields:
161             names = queryset._fields
162         else:
163             query = queryset.query
164             names = [*query.extra_select, *query.values_select, *query.annotation_select]
165         tuple_class = self.create_namedtuple_class(*names)
166         new = tuple.__new__
167         for row in super().__iter__():
168             yield new(tuple_class, row)
169 
170 
171 class FlatValuesListIterable(BaseIterable):
172     """
173     Iterable returned by QuerySet.values_list(flat=True) that yields single
174     values.
175     """
176 
177     def __iter__(self):
178         queryset = self.queryset
179         compiler = queryset.query.get_compiler(queryset.db)
180         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
181             yield row[0]
182 
183 
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, args, kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, args, kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, args, kwargs)
951 
952     def _filter_or_exclude(self, negate, args, kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, args, kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, args, kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         return self._annotate(args, kwargs, select=True)
1089 
1090     def alias(self, *args, **kwargs):
1091         """
1092         Return a query set with added aliases for extra data or aggregations.
1093         """
1094         self._not_support_combined_queries('alias')
1095         return self._annotate(args, kwargs, select=False)
1096 
1097     def _annotate(self, args, kwargs, select=True):
1098         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1099         annotations = {}
1100         for arg in args:
1101             # The default_alias property may raise a TypeError.
1102             try:
1103                 if arg.default_alias in kwargs:
1104                     raise ValueError("The named annotation '%s' conflicts with the "
1105                                      "default name for another annotation."
1106                                      % arg.default_alias)
1107             except TypeError:
1108                 raise TypeError("Complex annotations require an alias")
1109             annotations[arg.default_alias] = arg
1110         annotations.update(kwargs)
1111 
1112         clone = self._chain()
1113         names = self._fields
1114         if names is None:
1115             names = set(chain.from_iterable(
1116                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1117                 for field in self.model._meta.get_fields()
1118             ))
1119 
1120         for alias, annotation in annotations.items():
1121             if alias in names:
1122                 raise ValueError("The annotation '%s' conflicts with a field on "
1123                                  "the model." % alias)
1124             if isinstance(annotation, FilteredRelation):
1125                 clone.query.add_filtered_relation(annotation, alias)
1126             else:
1127                 clone.query.add_annotation(
1128                     annotation, alias, is_summary=False, select=select,
1129                 )
1130         for alias, annotation in clone.query.annotations.items():
1131             if alias in annotations and annotation.contains_aggregate:
1132                 if clone._fields is None:
1133                     clone.query.group_by = True
1134                 else:
1135                     clone.query.set_group_by()
1136                 break
1137 
1138         return clone
1139 
1140     def order_by(self, *field_names):
1141         """Return a new QuerySet instance with the ordering changed."""
1142         assert not self.query.is_sliced, \
1143             "Cannot reorder a query once a slice has been taken."
1144         obj = self._chain()
1145         obj.query.clear_ordering(force_empty=False)
1146         obj.query.add_ordering(*field_names)
1147         return obj
1148 
1149     def distinct(self, *field_names):
1150         """
1151         Return a new QuerySet instance that will select only distinct results.
1152         """
1153         self._not_support_combined_queries('distinct')
1154         assert not self.query.is_sliced, \
1155             "Cannot create distinct fields once a slice has been taken."
1156         obj = self._chain()
1157         obj.query.add_distinct_fields(*field_names)
1158         return obj
1159 
1160     def extra(self, select=None, where=None, params=None, tables=None,
1161               order_by=None, select_params=None):
1162         """Add extra SQL fragments to the query."""
1163         self._not_support_combined_queries('extra')
1164         assert not self.query.is_sliced, \
1165             "Cannot change a query once a slice has been taken"
1166         clone = self._chain()
1167         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1168         return clone
1169 
1170     def reverse(self):
1171         """Reverse the ordering of the QuerySet."""
1172         if self.query.is_sliced:
1173             raise TypeError('Cannot reverse a query once a slice has been taken.')
1174         clone = self._chain()
1175         clone.query.standard_ordering = not clone.query.standard_ordering
1176         return clone
1177 
1178     def defer(self, *fields):
1179         """
1180         Defer the loading of data for certain fields until they are accessed.
1181         Add the set of deferred fields to any existing set of deferred fields.
1182         The only exception to this is if None is passed in as the only
1183         parameter, in which case removal all deferrals.
1184         """
1185         self._not_support_combined_queries('defer')
1186         if self._fields is not None:
1187             raise TypeError("Cannot call defer() after .values() or .values_list()")
1188         clone = self._chain()
1189         if fields == (None,):
1190             clone.query.clear_deferred_loading()
1191         else:
1192             clone.query.add_deferred_loading(fields)
1193         return clone
1194 
1195     def only(self, *fields):
1196         """
1197         Essentially, the opposite of defer(). Only the fields passed into this
1198         method and that are not already specified as deferred are loaded
1199         immediately when the queryset is evaluated.
1200         """
1201         self._not_support_combined_queries('only')
1202         if self._fields is not None:
1203             raise TypeError("Cannot call only() after .values() or .values_list()")
1204         if fields == (None,):
1205             # Can only pass None to defer(), not only(), as the rest option.
1206             # That won't stop people trying to do this, so let's be explicit.
1207             raise TypeError("Cannot pass None as an argument to only().")
1208         for field in fields:
1209             field = field.split(LOOKUP_SEP, 1)[0]
1210             if field in self.query._filtered_relations:
1211                 raise ValueError('only() is not supported with FilteredRelation.')
1212         clone = self._chain()
1213         clone.query.add_immediate_loading(fields)
1214         return clone
1215 
1216     def using(self, alias):
1217         """Select which database this QuerySet should execute against."""
1218         clone = self._chain()
1219         clone._db = alias
1220         return clone
1221 
1222     ###################################
1223     # PUBLIC INTROSPECTION ATTRIBUTES #
1224     ###################################
1225 
1226     @property
1227     def ordered(self):
1228         """
1229         Return True if the QuerySet is ordered -- i.e. has an order_by()
1230         clause or a default ordering on the model (or is empty).
1231         """
1232         if isinstance(self, EmptyQuerySet):
1233             return True
1234         if self.query.extra_order_by or self.query.order_by:
1235             return True
1236         elif self.query.default_ordering and self.query.get_meta().ordering:
1237             return True
1238         else:
1239             return False
1240 
1241     @property
1242     def db(self):
1243         """Return the database used if this query is executed now."""
1244         if self._for_write:
1245             return self._db or router.db_for_write(self.model, **self._hints)
1246         return self._db or router.db_for_read(self.model, **self._hints)
1247 
1248     ###################
1249     # PRIVATE METHODS #
1250     ###################
1251 
1252     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1253         """
1254         Insert a new record for the given model. This provides an interface to
1255         the InsertQuery class and is how Model.save() is implemented.
1256         """
1257         self._for_write = True
1258         if using is None:
1259             using = self.db
1260         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1261         query.insert_values(fields, objs, raw=raw)
1262         return query.get_compiler(using=using).execute_sql(returning_fields)
1263     _insert.alters_data = True
1264     _insert.queryset_only = False
1265 
1266     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1267         """
1268         Helper method for bulk_create() to insert objs one batch at a time.
1269         """
1270         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1271             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1272         ops = connections[self.db].ops
1273         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1274         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1275         inserted_rows = []
1276         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1277         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1278             if bulk_return and not ignore_conflicts:
1279                 inserted_rows.extend(self._insert(
1280                     item, fields=fields, using=self.db,
1281                     returning_fields=self.model._meta.db_returning_fields,
1282                     ignore_conflicts=ignore_conflicts,
1283                 ))
1284             else:
1285                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1286         return inserted_rows
1287 
1288     def _chain(self, **kwargs):
1289         """
1290         Return a copy of the current QuerySet that's ready for another
1291         operation.
1292         """
1293         obj = self._clone()
1294         if obj._sticky_filter:
1295             obj.query.filter_is_sticky = True
1296             obj._sticky_filter = False
1297         obj.__dict__.update(kwargs)
1298         return obj
1299 
1300     def _clone(self):
1301         """
1302         Return a copy of the current QuerySet. A lightweight alternative
1303         to deepcopy().
1304         """
1305         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1306         c._sticky_filter = self._sticky_filter
1307         c._for_write = self._for_write
1308         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1309         c._known_related_objects = self._known_related_objects
1310         c._iterable_class = self._iterable_class
1311         c._fields = self._fields
1312         return c
1313 
1314     def _fetch_all(self):
1315         if self._result_cache is None:
1316             self._result_cache = list(self._iterable_class(self))
1317         if self._prefetch_related_lookups and not self._prefetch_done:
1318             self._prefetch_related_objects()
1319 
1320     def _next_is_sticky(self):
1321         """
1322         Indicate that the next filter call and the one following that should
1323         be treated as a single filter. This is only important when it comes to
1324         determining when to reuse tables for many-to-many filters. Required so
1325         that we can filter naturally on the results of related managers.
1326 
1327         This doesn't return a clone of the current QuerySet (it returns
1328         "self"). The method is only used internally and should be immediately
1329         followed by a filter() that does create a clone.
1330         """
1331         self._sticky_filter = True
1332         return self
1333 
1334     def _merge_sanity_check(self, other):
1335         """Check that two QuerySet classes may be merged."""
1336         if self._fields is not None and (
1337                 set(self.query.values_select) != set(other.query.values_select) or
1338                 set(self.query.extra_select) != set(other.query.extra_select) or
1339                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1340             raise TypeError(
1341                 "Merging '%s' classes must involve the same values in each case."
1342                 % self.__class__.__name__
1343             )
1344 
1345     def _merge_known_related_objects(self, other):
1346         """
1347         Keep track of all known related objects from either QuerySet instance.
1348         """
1349         for field, objects in other._known_related_objects.items():
1350             self._known_related_objects.setdefault(field, {}).update(objects)
1351 
1352     def resolve_expression(self, *args, **kwargs):
1353         if self._fields and len(self._fields) > 1:
1354             # values() queryset can only be used as nested queries
1355             # if they are set up to select only a single field.
1356             raise TypeError('Cannot use multi-field values as a filter value.')
1357         query = self.query.resolve_expression(*args, **kwargs)
1358         query._db = self._db
1359         return query
1360     resolve_expression.queryset_only = True
1361 
1362     def _add_hints(self, **hints):
1363         """
1364         Update hinting information for use by routers. Add new key/values or
1365         overwrite existing key/values.
1366         """
1367         self._hints.update(hints)
1368 
1369     def _has_filters(self):
1370         """
1371         Check if this QuerySet has any filtering going on. This isn't
1372         equivalent with checking if all objects are present in results, for
1373         example, qs[1:]._has_filters() -> False.
1374         """
1375         return self.query.has_filters()
1376 
1377     @staticmethod
1378     def _validate_values_are_expressions(values, method_name):
1379         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1380         if invalid_args:
1381             raise TypeError(
1382                 'QuerySet.%s() received non-expression(s): %s.' % (
1383                     method_name,
1384                     ', '.join(invalid_args),
1385                 )
1386             )
1387 
1388     def _not_support_combined_queries(self, operation_name):
1389         if self.query.combinator:
1390             raise NotSupportedError(
1391                 'Calling QuerySet.%s() after %s() is not supported.'
1392                 % (operation_name, self.query.combinator)
1393             )
1394 
1395 
1396 class InstanceCheckMeta(type):
1397     def __instancecheck__(self, instance):
1398         return isinstance(instance, QuerySet) and instance.query.is_empty()
1399 
1400 
1401 class EmptyQuerySet(metaclass=InstanceCheckMeta):
1402     """
1403     Marker class to checking if a queryset is empty by .none():
1404         isinstance(qs.none(), EmptyQuerySet) -> True
1405     """
1406 
1407     def __init__(self, *args, **kwargs):
1408         raise TypeError("EmptyQuerySet can't be instantiated")
1409 
1410 
1411 class RawQuerySet:
1412     """
1413     Provide an iterator which converts the results of raw SQL queries into
1414     annotated model instances.
1415     """
1416     def __init__(self, raw_query, model=None, query=None, params=None,
1417                  translations=None, using=None, hints=None):
1418         self.raw_query = raw_query
1419         self.model = model
1420         self._db = using
1421         self._hints = hints or {}
1422         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
1423         self.params = params or ()
1424         self.translations = translations or {}
1425         self._result_cache = None
1426         self._prefetch_related_lookups = ()
1427         self._prefetch_done = False
1428 
1429     def resolve_model_init_order(self):
1430         """Resolve the init field names and value positions."""
1431         converter = connections[self.db].introspection.identifier_converter
1432         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]
1433         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)
1434                              if column not in self.model_fields]
1435         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]
1436         model_init_names = [f.attname for f in model_init_fields]
1437         return model_init_names, model_init_order, annotation_fields
1438 
1439     def prefetch_related(self, *lookups):
1440         """Same as QuerySet.prefetch_related()"""
1441         clone = self._clone()
1442         if lookups == (None,):
1443             clone._prefetch_related_lookups = ()
1444         else:
1445             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1446         return clone
1447 
1448     def _prefetch_related_objects(self):
1449         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1450         self._prefetch_done = True
1451 
1452     def _clone(self):
1453         """Same as QuerySet._clone()"""
1454         c = self.__class__(
1455             self.raw_query, model=self.model, query=self.query, params=self.params,
1456             translations=self.translations, using=self._db, hints=self._hints
1457         )
1458         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1459         return c
1460 
1461     def _fetch_all(self):
1462         if self._result_cache is None:
1463             self._result_cache = list(self.iterator())
1464         if self._prefetch_related_lookups and not self._prefetch_done:
1465             self._prefetch_related_objects()
1466 
1467     def __len__(self):
1468         self._fetch_all()
1469         return len(self._result_cache)
1470 
1471     def __bool__(self):
1472         self._fetch_all()
1473         return bool(self._result_cache)
1474 
1475     def __iter__(self):
1476         self._fetch_all()
1477         return iter(self._result_cache)
1478 
1479     def iterator(self):
1480         # Cache some things for performance reasons outside the loop.
1481         db = self.db
1482         compiler = connections[db].ops.compiler('SQLCompiler')(
1483             self.query, connections[db], db
1484         )
1485 
1486         query = iter(self.query)
1487 
1488         try:
1489             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
1490             if self.model._meta.pk.attname not in model_init_names:
1491                 raise exceptions.FieldDoesNotExist(
1492                     'Raw query must include the primary key'
1493                 )
1494             model_cls = self.model
1495             fields = [self.model_fields.get(c) for c in self.columns]
1496             converters = compiler.get_converters([
1497                 f.get_col(f.model._meta.db_table) if f else None for f in fields
1498             ])
1499             if converters:
1500                 query = compiler.apply_converters(query, converters)
1501             for values in query:
1502                 # Associate fields to values
1503                 model_init_values = [values[pos] for pos in model_init_pos]
1504                 instance = model_cls.from_db(db, model_init_names, model_init_values)
1505                 if annotation_fields:
1506                     for column, pos in annotation_fields:
1507                         setattr(instance, column, values[pos])
1508                 yield instance
1509         finally:
1510             # Done iterating the Query. If it has its own cursor, close it.
1511             if hasattr(self.query, 'cursor') and self.query.cursor:
1512                 self.query.cursor.close()
1513 
1514     def __repr__(self):
1515         return "<%s: %s>" % (self.__class__.__name__, self.query)
1516 
1517     def __getitem__(self, k):
1518         return list(self)[k]
1519 
1520     @property
1521     def db(self):
1522         """Return the database used if this query is executed now."""
1523         return self._db or router.db_for_read(self.model, **self._hints)
1524 
1525     def using(self, alias):
1526         """Select the database this RawQuerySet should execute against."""
1527         return RawQuerySet(
1528             self.raw_query, model=self.model,
1529             query=self.query.chain(using=alias),
1530             params=self.params, translations=self.translations,
1531             using=alias,
1532         )
1533 
1534     @cached_property
1535     def columns(self):
1536         """
1537         A list of model field names in the order they'll appear in the
1538         query results.
1539         """
1540         columns = self.query.get_columns()
1541         # Adjust any column names which don't match field names
1542         for (query_name, model_name) in self.translations.items():
1543             # Ignore translations for nonexistent column names
1544             try:
1545                 index = columns.index(query_name)
1546             except ValueError:
1547                 pass
1548             else:
1549                 columns[index] = model_name
1550         return columns
1551 
1552     @cached_property
1553     def model_fields(self):
1554         """A dict mapping column names to model field names."""
1555         converter = connections[self.db].introspection.identifier_converter
1556         model_fields = {}
1557         for field in self.model._meta.fields:
1558             name, column = field.get_attname_column()
1559             model_fields[converter(column)] = field
1560         return model_fields
1561 
1562 
1563 class Prefetch:
1564     def __init__(self, lookup, queryset=None, to_attr=None):
1565         # `prefetch_through` is the path we traverse to perform the prefetch.
1566         self.prefetch_through = lookup
1567         # `prefetch_to` is the path to the attribute that stores the result.
1568         self.prefetch_to = lookup
1569         if queryset is not None and (
1570             isinstance(queryset, RawQuerySet) or (
1571                 hasattr(queryset, '_iterable_class') and
1572                 not issubclass(queryset._iterable_class, ModelIterable)
1573             )
1574         ):
1575             raise ValueError(
1576                 'Prefetch querysets cannot use raw(), values(), and '
1577                 'values_list().'
1578             )
1579         if to_attr:
1580             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])
1581 
1582         self.queryset = queryset
1583         self.to_attr = to_attr
1584 
1585     def __getstate__(self):
1586         obj_dict = self.__dict__.copy()
1587         if self.queryset is not None:
1588             # Prevent the QuerySet from being evaluated
1589             obj_dict['queryset'] = self.queryset._chain(
1590                 _result_cache=[],
1591                 _prefetch_done=True,
1592             )
1593         return obj_dict
1594 
1595     def add_prefix(self, prefix):
1596         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through
1597         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to
1598 
1599     def get_current_prefetch_to(self, level):
1600         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])
1601 
1602     def get_current_to_attr(self, level):
1603         parts = self.prefetch_to.split(LOOKUP_SEP)
1604         to_attr = parts[level]
1605         as_attr = self.to_attr and level == len(parts) - 1
1606         return to_attr, as_attr
1607 
1608     def get_current_queryset(self, level):
1609         if self.get_current_prefetch_to(level) == self.prefetch_to:
1610             return self.queryset
1611         return None
1612 
1613     def __eq__(self, other):
1614         if not isinstance(other, Prefetch):
1615             return NotImplemented
1616         return self.prefetch_to == other.prefetch_to
1617 
1618     def __hash__(self):
1619         return hash((self.__class__, self.prefetch_to))
1620 
1621 
1622 def normalize_prefetch_lookups(lookups, prefix=None):
1623     """Normalize lookups into Prefetch objects."""
1624     ret = []
1625     for lookup in lookups:
1626         if not isinstance(lookup, Prefetch):
1627             lookup = Prefetch(lookup)
1628         if prefix:
1629             lookup.add_prefix(prefix)
1630         ret.append(lookup)
1631     return ret
1632 
1633 
1634 def prefetch_related_objects(model_instances, *related_lookups):
1635     """
1636     Populate prefetched object caches for a list of model instances based on
1637     the lookups/Prefetch instances given.
1638     """
1639     if not model_instances:
1640         return  # nothing to do
1641 
1642     # We need to be able to dynamically add to the list of prefetch_related
1643     # lookups that we look up (see below).  So we need some book keeping to
1644     # ensure we don't do duplicate work.
1645     done_queries = {}    # dictionary of things like 'foo__bar': [results]
1646 
1647     auto_lookups = set()  # we add to this as we go through.
1648     followed_descriptors = set()  # recursion protection
1649 
1650     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))
1651     while all_lookups:
1652         lookup = all_lookups.pop()
1653         if lookup.prefetch_to in done_queries:
1654             if lookup.queryset is not None:
1655                 raise ValueError("'%s' lookup was already seen with a different queryset. "
1656                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)
1657 
1658             continue
1659 
1660         # Top level, the list of objects to decorate is the result cache
1661         # from the primary QuerySet. It won't be for deeper levels.
1662         obj_list = model_instances
1663 
1664         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)
1665         for level, through_attr in enumerate(through_attrs):
1666             # Prepare main instances
1667             if not obj_list:
1668                 break
1669 
1670             prefetch_to = lookup.get_current_prefetch_to(level)
1671             if prefetch_to in done_queries:
1672                 # Skip any prefetching, and any object preparation
1673                 obj_list = done_queries[prefetch_to]
1674                 continue
1675 
1676             # Prepare objects:
1677             good_objects = True
1678             for obj in obj_list:
1679                 # Since prefetching can re-use instances, it is possible to have
1680                 # the same instance multiple times in obj_list, so obj might
1681                 # already be prepared.
1682                 if not hasattr(obj, '_prefetched_objects_cache'):
1683                     try:
1684                         obj._prefetched_objects_cache = {}
1685                     except (AttributeError, TypeError):
1686                         # Must be an immutable object from
1687                         # values_list(flat=True), for example (TypeError) or
1688                         # a QuerySet subclass that isn't returning Model
1689                         # instances (AttributeError), either in Django or a 3rd
1690                         # party. prefetch_related() doesn't make sense, so quit.
1691                         good_objects = False
1692                         break
1693             if not good_objects:
1694                 break
1695 
1696             # Descend down tree
1697 
1698             # We assume that objects retrieved are homogeneous (which is the premise
1699             # of prefetch_related), so what applies to first object applies to all.
1700             first_obj = obj_list[0]
1701             to_attr = lookup.get_current_to_attr(level)[0]
1702             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
1703 
1704             if not attr_found:
1705                 raise AttributeError("Cannot find '%s' on %s object, '%s' is an invalid "
1706                                      "parameter to prefetch_related()" %
1707                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))
1708 
1709             if level == len(through_attrs) - 1 and prefetcher is None:
1710                 # Last one, this *must* resolve to something that supports
1711                 # prefetching, otherwise there is no point adding it and the
1712                 # developer asking for it has made a mistake.
1713                 raise ValueError("'%s' does not resolve to an item that supports "
1714                                  "prefetching - this is an invalid parameter to "
1715                                  "prefetch_related()." % lookup.prefetch_through)
1716 
1717             if prefetcher is not None and not is_fetched:
1718                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
1719                 # We need to ensure we don't keep adding lookups from the
1720                 # same relationships to stop infinite recursion. So, if we
1721                 # are already on an automatically added lookup, don't add
1722                 # the new lookups from relationships we've seen already.
1723                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):
1724                     done_queries[prefetch_to] = obj_list
1725                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)
1726                     auto_lookups.update(new_lookups)
1727                     all_lookups.extend(new_lookups)
1728                 followed_descriptors.add(descriptor)
1729             else:
1730                 # Either a singly related object that has already been fetched
1731                 # (e.g. via select_related), or hopefully some other property
1732                 # that doesn't support prefetching but needs to be traversed.
1733 
1734                 # We replace the current list of parent objects with the list
1735                 # of related objects, filtering out empty or missing values so
1736                 # that we can continue with nullable or reverse relations.
1737                 new_obj_list = []
1738                 for obj in obj_list:
1739                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):
1740                         # If related objects have been prefetched, use the
1741                         # cache rather than the object's through_attr.
1742                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))
1743                     else:
1744                         try:
1745                             new_obj = getattr(obj, through_attr)
1746                         except exceptions.ObjectDoesNotExist:
1747                             continue
1748                     if new_obj is None:
1749                         continue
1750                     # We special-case `list` rather than something more generic
1751                     # like `Iterable` because we don't want to accidentally match
1752                     # user models that define __iter__.
1753                     if isinstance(new_obj, list):
1754                         new_obj_list.extend(new_obj)
1755                     else:
1756                         new_obj_list.append(new_obj)
1757                 obj_list = new_obj_list
1758 
1759 
1760 def get_prefetcher(instance, through_attr, to_attr):
1761     """
1762     For the attribute 'through_attr' on the given instance, find
1763     an object that has a get_prefetch_queryset().
1764     Return a 4 tuple containing:
1765     (the object with get_prefetch_queryset (or None),
1766      the descriptor object representing this relationship (or None),
1767      a boolean that is False if the attribute was not found at all,
1768      a boolean that is True if the attribute has already been fetched)
1769     """
1770     prefetcher = None
1771     is_fetched = False
1772 
1773     # For singly related objects, we have to avoid getting the attribute
1774     # from the object, as this will trigger the query. So we first try
1775     # on the class, in order to get the descriptor object.
1776     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)
1777     if rel_obj_descriptor is None:
1778         attr_found = hasattr(instance, through_attr)
1779     else:
1780         attr_found = True
1781         if rel_obj_descriptor:
1782             # singly related object, descriptor object has the
1783             # get_prefetch_queryset() method.
1784             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
1785                 prefetcher = rel_obj_descriptor
1786                 if rel_obj_descriptor.is_cached(instance):
1787                     is_fetched = True
1788             else:
1789                 # descriptor doesn't support prefetching, so we go ahead and get
1790                 # the attribute on the instance rather than the class to
1791                 # support many related managers
1792                 rel_obj = getattr(instance, through_attr)
1793                 if hasattr(rel_obj, 'get_prefetch_queryset'):
1794                     prefetcher = rel_obj
1795                 if through_attr != to_attr:
1796                     # Special case cached_property instances because hasattr
1797                     # triggers attribute computation and assignment.
1798                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
1799                         is_fetched = to_attr in instance.__dict__
1800                     else:
1801                         is_fetched = hasattr(instance, to_attr)
1802                 else:
1803                     is_fetched = through_attr in instance._prefetched_objects_cache
1804     return prefetcher, rel_obj_descriptor, attr_found, is_fetched
1805 
1806 
1807 def prefetch_one_level(instances, prefetcher, lookup, level):
1808     """
1809     Helper function for prefetch_related_objects().
1810 
1811     Run prefetches on all instances using the prefetcher object,
1812     assigning results to relevant caches in instance.
1813 
1814     Return the prefetched objects along with any additional prefetches that
1815     must be done due to prefetch_related lookups found from default managers.
1816     """
1817     # prefetcher must have a method get_prefetch_queryset() which takes a list
1818     # of instances, and returns a tuple:
1819 
1820     # (queryset of instances of self.model that are related to passed in instances,
1821     #  callable that gets value to be matched for returned instances,
1822     #  callable that gets value to be matched for passed in instances,
1823     #  boolean that is True for singly related objects,
1824     #  cache or field name to assign to,
1825     #  boolean that is True when the previous argument is a cache name vs a field name).
1826 
1827     # The 'values to be matched' must be hashable as they will be used
1828     # in a dictionary.
1829 
1830     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (
1831         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))
1832     # We have to handle the possibility that the QuerySet we just got back
1833     # contains some prefetch_related lookups. We don't want to trigger the
1834     # prefetch_related functionality by evaluating the query. Rather, we need
1835     # to merge in the prefetch_related lookups.
1836     # Copy the lookups in case it is a Prefetch object which could be reused
1837     # later (happens in nested prefetch_related).
1838     additional_lookups = [
1839         copy.copy(additional_lookup) for additional_lookup
1840         in getattr(rel_qs, '_prefetch_related_lookups', ())
1841     ]
1842     if additional_lookups:
1843         # Don't need to clone because the manager should have given us a fresh
1844         # instance, so we access an internal instead of using public interface
1845         # for performance reasons.
1846         rel_qs._prefetch_related_lookups = ()
1847 
1848     all_related_objects = list(rel_qs)
1849 
1850     rel_obj_cache = {}
1851     for rel_obj in all_related_objects:
1852         rel_attr_val = rel_obj_attr(rel_obj)
1853         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)
1854 
1855     to_attr, as_attr = lookup.get_current_to_attr(level)
1856     # Make sure `to_attr` does not conflict with a field.
1857     if as_attr and instances:
1858         # We assume that objects retrieved are homogeneous (which is the premise
1859         # of prefetch_related), so what applies to first object applies to all.
1860         model = instances[0].__class__
1861         try:
1862             model._meta.get_field(to_attr)
1863         except exceptions.FieldDoesNotExist:
1864             pass
1865         else:
1866             msg = 'to_attr={} conflicts with a field on the {} model.'
1867             raise ValueError(msg.format(to_attr, model.__name__))
1868 
1869     # Whether or not we're prefetching the last part of the lookup.
1870     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
1871 
1872     for obj in instances:
1873         instance_attr_val = instance_attr(obj)
1874         vals = rel_obj_cache.get(instance_attr_val, [])
1875 
1876         if single:
1877             val = vals[0] if vals else None
1878             if as_attr:
1879                 # A to_attr has been given for the prefetch.
1880                 setattr(obj, to_attr, val)
1881             elif is_descriptor:
1882                 # cache_name points to a field name in obj.
1883                 # This field is a descriptor for a related object.
1884                 setattr(obj, cache_name, val)
1885             else:
1886                 # No to_attr has been given for this prefetch operation and the
1887                 # cache_name does not point to a descriptor. Store the value of
1888                 # the field in the object's field cache.
1889                 obj._state.fields_cache[cache_name] = val
1890         else:
1891             if as_attr:
1892                 setattr(obj, to_attr, vals)
1893             else:
1894                 manager = getattr(obj, to_attr)
1895                 if leaf and lookup.queryset is not None:
1896                     qs = manager._apply_rel_filters(lookup.queryset)
1897                 else:
1898                     qs = manager.get_queryset()
1899                 qs._result_cache = vals
1900                 # We don't want the individual qs doing prefetch_related now,
1901                 # since we have merged this into the current work.
1902                 qs._prefetch_done = True
1903                 obj._prefetched_objects_cache[cache_name] = qs
1904     return all_related_objects, additional_lookups
1905 
1906 
1907 class RelatedPopulator:
1908     """
1909     RelatedPopulator is used for select_related() object instantiation.
1910 
1911     The idea is that each select_related() model will be populated by a
1912     different RelatedPopulator instance. The RelatedPopulator instances get
1913     klass_info and select (computed in SQLCompiler) plus the used db as
1914     input for initialization. That data is used to compute which columns
1915     to use, how to instantiate the model, and how to populate the links
1916     between the objects.
1917 
1918     The actual creation of the objects is done in populate() method. This
1919     method gets row and from_obj as input and populates the select_related()
1920     model instance.
1921     """
1922     def __init__(self, klass_info, select, db):
1923         self.db = db
1924         # Pre-compute needed attributes. The attributes are:
1925         #  - model_cls: the possibly deferred model class to instantiate
1926         #  - either:
1927         #    - cols_start, cols_end: usually the columns in the row are
1928         #      in the same order model_cls.__init__ expects them, so we
1929         #      can instantiate by model_cls(*row[cols_start:cols_end])
1930         #    - reorder_for_init: When select_related descends to a child
1931         #      class, then we want to reuse the already selected parent
1932         #      data. However, in this case the parent data isn't necessarily
1933         #      in the same order that Model.__init__ expects it to be, so
1934         #      we have to reorder the parent data. The reorder_for_init
1935         #      attribute contains a function used to reorder the field data
1936         #      in the order __init__ expects it.
1937         #  - pk_idx: the index of the primary key field in the reordered
1938         #    model data. Used to check if a related object exists at all.
1939         #  - init_list: the field attnames fetched from the database. For
1940         #    deferred models this isn't the same as all attnames of the
1941         #    model's fields.
1942         #  - related_populators: a list of RelatedPopulator instances if
1943         #    select_related() descends to related models from this model.
1944         #  - local_setter, remote_setter: Methods to set cached values on
1945         #    the object being populated and on the remote object. Usually
1946         #    these are Field.set_cached_value() methods.
1947         select_fields = klass_info['select_fields']
1948         from_parent = klass_info['from_parent']
1949         if not from_parent:
1950             self.cols_start = select_fields[0]
1951             self.cols_end = select_fields[-1] + 1
1952             self.init_list = [
1953                 f[0].target.attname for f in select[self.cols_start:self.cols_end]
1954             ]
1955             self.reorder_for_init = None
1956         else:
1957             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}
1958             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)
1959             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]
1960             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])
1961 
1962         self.model_cls = klass_info['model']
1963         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)
1964         self.related_populators = get_related_populators(klass_info, select, self.db)
1965         self.local_setter = klass_info['local_setter']
1966         self.remote_setter = klass_info['remote_setter']
1967 
1968     def populate(self, row, from_obj):
1969         if self.reorder_for_init:
1970             obj_data = self.reorder_for_init(row)
1971         else:
1972             obj_data = row[self.cols_start:self.cols_end]
1973         if obj_data[self.pk_idx] is None:
1974             obj = None
1975         else:
1976             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)
1977             for rel_iter in self.related_populators:
1978                 rel_iter.populate(row, obj)
1979         self.local_setter(from_obj, obj)
1980         if obj is not None:
1981             self.remote_setter(obj, from_obj)
1982 
1983 
1984 def get_related_populators(klass_info, select, db):
1985     iterators = []
1986     related_klass_infos = klass_info.get('related_klass_infos', [])
1987     for rel_klass_info in related_klass_infos:
1988         rel_cls = RelatedPopulator(rel_klass_info, select, db)
1989         iterators.append(rel_cls)
1990     return iterators

</code><intended_behavior>Considering this file deals with querying, it also might include logic for how model instances are handled post-retrieval from the database, including caching. The intended behavior here would be to audit and potentially adjust the code paths that manage `_state.fields_cache` during or after instantiation and retrieval operations to accommodate the nuances of model instance copying, ensuring that each instance maintains its own independent cache unless explicitly shared. This ensures that operations on copied model instances do not have unintended side effects on the caching of related objects of the original instances.</intended_behavior>]
2025-10-15 18:11:22.809 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 18:11:22.809 | INFO     | Search-only: skipping patch selection
2025-10-15 18:11:22.944 | INFO     | Task django__django-13281 completed successfully.
2025-10-15 18:11:22.945 | INFO     | No patch generated. You can try running ACR again.
