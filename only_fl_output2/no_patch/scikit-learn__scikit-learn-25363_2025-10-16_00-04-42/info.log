2025-10-16 00:04:42.698 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-16 00:04:46.424 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=231, output_tokens=12, cost=0.002670
2025-10-16 00:04:46.425 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-16 00:04:46.429 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-16 00:04:55.146 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=539, output_tokens=320, cost=0.014990
2025-10-16 00:04:55.150 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-16 00:04:57.823 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=674, output_tokens=70, cost=0.008840
2025-10-16 00:04:57.824 | DEBUG    | Extracted a valid json.
2025-10-16 00:04:57.824 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"Parallel(\")",
        "search_code(\"delayed(\")",
        "search_code(\"config\")",
        "search_code(\"context\")",
        "search_code(\"scikit-learn\")",
        "search_code(\"warning\")"
    ],
    "bug_locations": []
}
2025-10-16 00:04:59.047 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-16 00:04:59.048 | DEBUG    | <Agent search> Analyzing search results.
2025-10-16 00:05:22.187 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9940, output_tokens=840, cost=0.124600
2025-10-16 00:05:22.195 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-16 00:05:35.405 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=10996, output_tokens=446, cost=0.123340
2025-10-16 00:05:35.413 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-16 00:05:42.929 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=800, output_tokens=253, cost=0.015590
2025-10-16 00:05:42.931 | DEBUG    | Extracted a valid json.
2025-10-16 00:05:42.931 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/model_selection/_validation.py",
            "class": "",
            "method": "",
            "intended_behavior": "The intended behavior here is to ensure that configurations, especially those crucial for the operation of Parallel executed tasks, are explicitly passed to each delayed call. This might require wrapping the delayed function calls within a context that ensures the current configuration state is captured and then applied within each parallelly executed task. In essence, the fix involves ensuring that any configuration context is not just assumed but is explicitly conveyed and reconstructed within each task's execution environment."
        },
        {
            "file": "sklearn/ensemble/_forest.py",
            "class": "",
            "method": "",
            "intended_behavior": "Similar to the above, modifications should ensure that configurations pertinent to the fitting process (possibly including random state settings, number of jobs, etc.) are explicitly passed to each parallel execution context initiated via delayed. This may include augmenting the delayed call to ensure it carries over or applies the current configuration settings within each executed task, preventing misconfigurations when tasks run in threads/processes different from the main thread initializing them."
        }
    ]
}
2025-10-16 00:05:42.945 | DEBUG    | Bug location extracted successfully: [<file>sklearn/model_selection/_validation.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The :mod:`sklearn.model_selection._validation` module includes classes and
3 functions to validate the model.
4 """
5 
6 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
7 #         Gael Varoquaux <gael.varoquaux@normalesup.org>
8 #         Olivier Grisel <olivier.grisel@ensta.org>
9 #         Raghav RV <rvraghav93@gmail.com>
10 #         Michal Karbownik <michakarbownik@gmail.com>
11 # License: BSD 3 clause
12 
13 
14 import warnings
15 import numbers
16 import time
17 from functools import partial
18 from traceback import format_exc
19 from contextlib import suppress
20 from collections import Counter
21 
22 import numpy as np
23 import scipy.sparse as sp
24 from joblib import Parallel, logger
25 
26 from ..base import is_classifier, clone
27 from ..utils import indexable, check_random_state, _safe_indexing
28 from ..utils.validation import _check_fit_params
29 from ..utils.validation import _num_samples
30 from ..utils.fixes import delayed
31 from ..utils.metaestimators import _safe_split
32 from ..metrics import check_scoring
33 from ..metrics._scorer import _check_multimetric_scoring, _MultimetricScorer
34 from ..exceptions import FitFailedWarning
35 from ._split import check_cv
36 from ..preprocessing import LabelEncoder
37 
38 
39 __all__ = [
40     "cross_validate",
41     "cross_val_score",
42     "cross_val_predict",
43     "permutation_test_score",
44     "learning_curve",
45     "validation_curve",
46 ]
47 
48 
49 def cross_validate(
50     estimator,
51     X,
52     y=None,
53     *,
54     groups=None,
55     scoring=None,
56     cv=None,
57     n_jobs=None,
58     verbose=0,
59     fit_params=None,
60     pre_dispatch="2*n_jobs",
61     return_train_score=False,
62     return_estimator=False,
63     error_score=np.nan,
64 ):
65     """Evaluate metric(s) by cross-validation and also record fit/score times.
66 
67     Read more in the :ref:`User Guide <multimetric_cross_validation>`.
68 
69     Parameters
70     ----------
71     estimator : estimator object implementing 'fit'
72         The object to use to fit the data.
73 
74     X : array-like of shape (n_samples, n_features)
75         The data to fit. Can be for example a list, or an array.
76 
77     y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None
78         The target variable to try to predict in the case of
79         supervised learning.
80 
81     groups : array-like of shape (n_samples,), default=None
82         Group labels for the samples used while splitting the dataset into
83         train/test set. Only used in conjunction with a "Group" :term:`cv`
84         instance (e.g., :class:`GroupKFold`).
85 
86     scoring : str, callable, list, tuple, or dict, default=None
87         Strategy to evaluate the performance of the cross-validated model on
88         the test set.
89 
90         If `scoring` represents a single score, one can use:
91 
92         - a single string (see :ref:`scoring_parameter`);
93         - a callable (see :ref:`scoring`) that returns a single value.
94 
95         If `scoring` represents multiple scores, one can use:
96 
97         - a list or tuple of unique strings;
98         - a callable returning a dictionary where the keys are the metric
99           names and the values are the metric scores;
100         - a dictionary with metric names as keys and callables a values.
101 
102         See :ref:`multimetric_grid_search` for an example.
103 
104     cv : int, cross-validation generator or an iterable, default=None
105         Determines the cross-validation splitting strategy.
106         Possible inputs for cv are:
107 
108         - None, to use the default 5-fold cross validation,
109         - int, to specify the number of folds in a `(Stratified)KFold`,
110         - :term:`CV splitter`,
111         - An iterable yielding (train, test) splits as arrays of indices.
112 
113         For int/None inputs, if the estimator is a classifier and ``y`` is
114         either binary or multiclass, :class:`StratifiedKFold` is used. In all
115         other cases, :class:`KFold` is used. These splitters are instantiated
116         with `shuffle=False` so the splits will be the same across calls.
117 
118         Refer :ref:`User Guide <cross_validation>` for the various
119         cross-validation strategies that can be used here.
120 
121         .. versionchanged:: 0.22
122             ``cv`` default value if None changed from 3-fold to 5-fold.
123 
124     n_jobs : int, default=None
125         Number of jobs to run in parallel. Training the estimator and computing
126         the score are parallelized over the cross-validation splits.
127         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
128         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
129         for more details.
130 
131     verbose : int, default=0
132         The verbosity level.
133 
134     fit_params : dict, default=None
135         Parameters to pass to the fit method of the estimator.
136 
137     pre_dispatch : int or str, default='2*n_jobs'
138         Controls the number of jobs that get dispatched during parallel
139         execution. Reducing this number can be useful to avoid an
140         explosion of memory consumption when more jobs get dispatched
141         than CPUs can process. This parameter can be:
142 
143             - None, in which case all the jobs are immediately
144               created and spawned. Use this for lightweight and
145               fast-running jobs, to avoid delays due to on-demand
146               spawning of the jobs
147 
148             - An int, giving the exact number of total jobs that are
149               spawned
150 
151             - A str, giving an expression as a function of n_jobs,
152               as in '2*n_jobs'
153 
154     return_train_score : bool, default=False
155         Whether to include train scores.
156         Computing training scores is used to get insights on how different
157         parameter settings impact the overfitting/underfitting trade-off.
158         However computing the scores on the training set can be computationally
159         expensive and is not strictly required to select the parameters that
160         yield the best generalization performance.
161 
162         .. versionadded:: 0.19
163 
164         .. versionchanged:: 0.21
165             Default value was changed from ``True`` to ``False``
166 
167     return_estimator : bool, default=False
168         Whether to return the estimators fitted on each split.
169 
170         .. versionadded:: 0.20
171 
172     error_score : 'raise' or numeric, default=np.nan
173         Value to assign to the score if an error occurs in estimator fitting.
174         If set to 'raise', the error is raised.
175         If a numeric value is given, FitFailedWarning is raised.
176 
177         .. versionadded:: 0.20
178 
179     Returns
180     -------
181     scores : dict of float arrays of shape (n_splits,)
182         Array of scores of the estimator for each run of the cross validation.
183 
184         A dict of arrays containing the score/time arrays for each scorer is
185         returned. The possible keys for this ``dict`` are:
186 
187             ``test_score``
188                 The score array for test scores on each cv split.
189                 Suffix ``_score`` in ``test_score`` changes to a specific
190                 metric like ``test_r2`` or ``test_auc`` if there are
191                 multiple scoring metrics in the scoring parameter.
192             ``train_score``
193                 The score array for train scores on each cv split.
194                 Suffix ``_score`` in ``train_score`` changes to a specific
195                 metric like ``train_r2`` or ``train_auc`` if there are
196                 multiple scoring metrics in the scoring parameter.
197                 This is available only if ``return_train_score`` parameter
198                 is ``True``.
199             ``fit_time``
200                 The time for fitting the estimator on the train
201                 set for each cv split.
202             ``score_time``
203                 The time for scoring the estimator on the test set for each
204                 cv split. (Note time for scoring on the train set is not
205                 included even if ``return_train_score`` is set to ``True``
206             ``estimator``
207                 The estimator objects for each cv split.
208                 This is available only if ``return_estimator`` parameter
209                 is set to ``True``.
210 
211     See Also
212     --------
213     cross_val_score : Run cross-validation for single metric evaluation.
214 
215     cross_val_predict : Get predictions from each split of cross-validation for
216         diagnostic purposes.
217 
218     sklearn.metrics.make_scorer : Make a scorer from a performance metric or
219         loss function.
220 
221     Examples
222     --------
223     >>> from sklearn import datasets, linear_model
224     >>> from sklearn.model_selection import cross_validate
225     >>> from sklearn.metrics import make_scorer
226     >>> from sklearn.metrics import confusion_matrix
227     >>> from sklearn.svm import LinearSVC
228     >>> diabetes = datasets.load_diabetes()
229     >>> X = diabetes.data[:150]
230     >>> y = diabetes.target[:150]
231     >>> lasso = linear_model.Lasso()
232 
233     Single metric evaluation using ``cross_validate``
234 
235     >>> cv_results = cross_validate(lasso, X, y, cv=3)
236     >>> sorted(cv_results.keys())
237     ['fit_time', 'score_time', 'test_score']
238     >>> cv_results['test_score']
239     array([0.3315057 , 0.08022103, 0.03531816])
240 
241     Multiple metric evaluation using ``cross_validate``
242     (please refer the ``scoring`` parameter doc for more information)
243 
244     >>> scores = cross_validate(lasso, X, y, cv=3,
245     ...                         scoring=('r2', 'neg_mean_squared_error'),
246     ...                         return_train_score=True)
247     >>> print(scores['test_neg_mean_squared_error'])
248     [-3635.5... -3573.3... -6114.7...]
249     >>> print(scores['train_r2'])
250     [0.28009951 0.3908844  0.22784907]
251     """
252     X, y, groups = indexable(X, y, groups)
253 
254     cv = check_cv(cv, y, classifier=is_classifier(estimator))
255 
256     if callable(scoring):
257         scorers = scoring
258     elif scoring is None or isinstance(scoring, str):
259         scorers = check_scoring(estimator, scoring)
260     else:
261         scorers = _check_multimetric_scoring(estimator, scoring)
262 
263     # We clone the estimator to make sure that all the folds are
264     # independent, and that it is pickle-able.
265     parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
266     results = parallel(
267         delayed(_fit_and_score)(
268             clone(estimator),
269             X,
270             y,
271             scorers,
272             train,
273             test,
274             verbose,
275             None,
276             fit_params,
277             return_train_score=return_train_score,
278             return_times=True,
279             return_estimator=return_estimator,
280             error_score=error_score,
281         )
282         for train, test in cv.split(X, y, groups)
283     )
284 
285     _warn_or_raise_about_fit_failures(results, error_score)
286 
287     # For callabe scoring, the return type is only know after calling. If the
288     # return type is a dictionary, the error scores can now be inserted with
289     # the correct key.
290     if callable(scoring):
291         _insert_error_scores(results, error_score)
292 
293     results = _aggregate_score_dicts(results)
294 
295     ret = {}
296     ret["fit_time"] = results["fit_time"]
297     ret["score_time"] = results["score_time"]
298 
299     if return_estimator:
300         ret["estimator"] = results["estimator"]
301 
302     test_scores_dict = _normalize_score_results(results["test_scores"])
303     if return_train_score:
304         train_scores_dict = _normalize_score_results(results["train_scores"])
305 
306     for name in test_scores_dict:
307         ret["test_%s" % name] = test_scores_dict[name]
308         if return_train_score:
309             key = "train_%s" % name
310             ret[key] = train_scores_dict[name]
311 
312     return ret
313 
314 
315 def _insert_error_scores(results, error_score):
316     """Insert error in `results` by replacing them inplace with `error_score`.
317 
318     This only applies to multimetric scores because `_fit_and_score` will
319     handle the single metric case.
320     """
321     successful_score = None
322     failed_indices = []
323     for i, result in enumerate(results):
324         if result["fit_error"] is not None:
325             failed_indices.append(i)
326         elif successful_score is None:
327             successful_score = result["test_scores"]
328 
329     if isinstance(successful_score, dict):
330         formatted_error = {name: error_score for name in successful_score}
331         for i in failed_indices:
332             results[i]["test_scores"] = formatted_error.copy()
333             if "train_scores" in results[i]:
334                 results[i]["train_scores"] = formatted_error.copy()
335 
336 
337 def _normalize_score_results(scores, scaler_score_key="score"):
338     """Creates a scoring dictionary based on the type of `scores`"""
339     if isinstance(scores[0], dict):
340         # multimetric scoring
341         return _aggregate_score_dicts(scores)
342     # scaler
343     return {scaler_score_key: scores}
344 
345 
346 def _warn_or_raise_about_fit_failures(results, error_score):
347     fit_errors = [
348         result["fit_error"] for result in results if result["fit_error"] is not None
349     ]
350     if fit_errors:
351         num_failed_fits = len(fit_errors)
352         num_fits = len(results)
353         fit_errors_counter = Counter(fit_errors)
354         delimiter = "-" * 80 + "\n"
355         fit_errors_summary = "\n".join(
356             f"{delimiter}{n} fits failed with the following error:\n{error}"
357             for error, n in fit_errors_counter.items()
358         )
359 
360         if num_failed_fits == num_fits:
361             all_fits_failed_message = (
362                 f"\nAll the {num_fits} fits failed.\n"
363                 "It is very likely that your model is misconfigured.\n"
364                 "You can try to debug the error by setting error_score='raise'.\n\n"
365                 f"Below are more details about the failures:\n{fit_errors_summary}"
366             )
367             raise ValueError(all_fits_failed_message)
368 
369         else:
370             some_fits_failed_message = (
371                 f"\n{num_failed_fits} fits failed out of a total of {num_fits}.\n"
372                 "The score on these train-test partitions for these parameters"
373                 f" will be set to {error_score}.\n"
374                 "If these failures are not expected, you can try to debug them "
375                 "by setting error_score='raise'.\n\n"
376                 f"Below are more details about the failures:\n{fit_errors_summary}"
377             )
378             warnings.warn(some_fits_failed_message, FitFailedWarning)
379 
380 
381 def cross_val_score(
382     estimator,
383     X,
384     y=None,
385     *,
386     groups=None,
387     scoring=None,
388     cv=None,
389     n_jobs=None,
390     verbose=0,
391     fit_params=None,
392     pre_dispatch="2*n_jobs",
393     error_score=np.nan,
394 ):
395     """Evaluate a score by cross-validation.
396 
397     Read more in the :ref:`User Guide <cross_validation>`.
398 
399     Parameters
400     ----------
401     estimator : estimator object implementing 'fit'
402         The object to use to fit the data.
403 
404     X : array-like of shape (n_samples, n_features)
405         The data to fit. Can be for example a list, or an array.
406 
407     y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
408             default=None
409         The target variable to try to predict in the case of
410         supervised learning.
411 
412     groups : array-like of shape (n_samples,), default=None
413         Group labels for the samples used while splitting the dataset into
414         train/test set. Only used in conjunction with a "Group" :term:`cv`
415         instance (e.g., :class:`GroupKFold`).
416 
417     scoring : str or callable, default=None
418         A str (see model evaluation documentation) or
419         a scorer callable object / function with signature
420         ``scorer(estimator, X, y)`` which should return only
421         a single value.
422 
423         Similar to :func:`cross_validate`
424         but only a single metric is permitted.
425 
426         If `None`, the estimator's default scorer (if available) is used.
427 
428     cv : int, cross-validation generator or an iterable, default=None
429         Determines the cross-validation splitting strategy.
430         Possible inputs for cv are:
431 
432         - `None`, to use the default 5-fold cross validation,
433         - int, to specify the number of folds in a `(Stratified)KFold`,
434         - :term:`CV splitter`,
435         - An iterable that generates (train, test) splits as arrays of indices.
436 
437         For `int`/`None` inputs, if the estimator is a classifier and `y` is
438         either binary or multiclass, :class:`StratifiedKFold` is used. In all
439         other cases, :class:`KFold` is used. These splitters are instantiated
440         with `shuffle=False` so the splits will be the same across calls.
441 
442         Refer :ref:`User Guide <cross_validation>` for the various
443         cross-validation strategies that can be used here.
444 
445         .. versionchanged:: 0.22
446             `cv` default value if `None` changed from 3-fold to 5-fold.
447 
448     n_jobs : int, default=None
449         Number of jobs to run in parallel. Training the estimator and computing
450         the score are parallelized over the cross-validation splits.
451         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
452         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
453         for more details.
454 
455     verbose : int, default=0
456         The verbosity level.
457 
458     fit_params : dict, default=None
459         Parameters to pass to the fit method of the estimator.
460 
461     pre_dispatch : int or str, default='2*n_jobs'
462         Controls the number of jobs that get dispatched during parallel
463         execution. Reducing this number can be useful to avoid an
464         explosion of memory consumption when more jobs get dispatched
465         than CPUs can process. This parameter can be:
466 
467             - ``None``, in which case all the jobs are immediately
468               created and spawned. Use this for lightweight and
469               fast-running jobs, to avoid delays due to on-demand
470               spawning of the jobs
471 
472             - An int, giving the exact number of total jobs that are
473               spawned
474 
475             - A str, giving an expression as a function of n_jobs,
476               as in '2*n_jobs'
477 
478     error_score : 'raise' or numeric, default=np.nan
479         Value to assign to the score if an error occurs in estimator fitting.
480         If set to 'raise', the error is raised.
481         If a numeric value is given, FitFailedWarning is raised.
482 
483         .. versionadded:: 0.20
484 
485     Returns
486     -------
487     scores : ndarray of float of shape=(len(list(cv)),)
488         Array of scores of the estimator for each run of the cross validation.
489 
490     See Also
491     --------
492     cross_validate : To run cross-validation on multiple metrics and also to
493         return train scores, fit times and score times.
494 
495     cross_val_predict : Get predictions from each split of cross-validation for
496         diagnostic purposes.
497 
498     sklearn.metrics.make_scorer : Make a scorer from a performance metric or
499         loss function.
500 
501     Examples
502     --------
503     >>> from sklearn import datasets, linear_model
504     >>> from sklearn.model_selection import cross_val_score
505     >>> diabetes = datasets.load_diabetes()
506     >>> X = diabetes.data[:150]
507     >>> y = diabetes.target[:150]
508     >>> lasso = linear_model.Lasso()
509     >>> print(cross_val_score(lasso, X, y, cv=3))
510     [0.3315057  0.08022103 0.03531816]
511     """
512     # To ensure multimetric format is not supported
513     scorer = check_scoring(estimator, scoring=scoring)
514 
515     cv_results = cross_validate(
516         estimator=estimator,
517         X=X,
518         y=y,
519         groups=groups,
520         scoring={"score": scorer},
521         cv=cv,
522         n_jobs=n_jobs,
523         verbose=verbose,
524         fit_params=fit_params,
525         pre_dispatch=pre_dispatch,
526         error_score=error_score,
527     )
528     return cv_results["test_score"]
529 
530 
531 def _fit_and_score(
532     estimator,
533     X,
534     y,
535     scorer,
536     train,
537     test,
538     verbose,
539     parameters,
540     fit_params,
541     return_train_score=False,
542     return_parameters=False,
543     return_n_test_samples=False,
544     return_times=False,
545     return_estimator=False,
546     split_progress=None,
547     candidate_progress=None,
548     error_score=np.nan,
549 ):
550 
551     """Fit estimator and compute scores for a given dataset split.
552 
553     Parameters
554     ----------
555     estimator : estimator object implementing 'fit'
556         The object to use to fit the data.
557 
558     X : array-like of shape (n_samples, n_features)
559         The data to fit.
560 
561     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
562         The target variable to try to predict in the case of
563         supervised learning.
564 
565     scorer : A single callable or dict mapping scorer name to the callable
566         If it is a single callable, the return value for ``train_scores`` and
567         ``test_scores`` is a single float.
568 
569         For a dict, it should be one mapping the scorer name to the scorer
570         callable object / function.
571 
572         The callable object / fn should have signature
573         ``scorer(estimator, X, y)``.
574 
575     train : array-like of shape (n_train_samples,)
576         Indices of training samples.
577 
578     test : array-like of shape (n_test_samples,)
579         Indices of test samples.
580 
581     verbose : int
582         The verbosity level.
583 
584     error_score : 'raise' or numeric, default=np.nan
585         Value to assign to the score if an error occurs in estimator fitting.
586         If set to 'raise', the error is raised.
587         If a numeric value is given, FitFailedWarning is raised.
588 
589     parameters : dict or None
590         Parameters to be set on the estimator.
591 
592     fit_params : dict or None
593         Parameters that will be passed to ``estimator.fit``.
594 
595     return_train_score : bool, default=False
596         Compute and return score on training set.
597 
598     return_parameters : bool, default=False
599         Return parameters that has been used for the estimator.
600 
601     split_progress : {list, tuple} of int, default=None
602         A list or tuple of format (<current_split_id>, <total_num_of_splits>).
603 
604     candidate_progress : {list, tuple} of int, default=None
605         A list or tuple of format
606         (<current_candidate_id>, <total_number_of_candidates>).
607 
608     return_n_test_samples : bool, default=False
609         Whether to return the ``n_test_samples``.
610 
611     return_times : bool, default=False
612         Whether to return the fit/score times.
613 
614     return_estimator : bool, default=False
615         Whether to return the fitted estimator.
616 
617     Returns
618     -------
619     result : dict with the following attributes
620         train_scores : dict of scorer name -> float
621             Score on training set (for all the scorers),
622             returned only if `return_train_score` is `True`.
623         test_scores : dict of scorer name -> float
624             Score on testing set (for all the scorers).
625         n_test_samples : int
626             Number of test samples.
627         fit_time : float
628             Time spent for fitting in seconds.
629         score_time : float
630             Time spent for scoring in seconds.
631         parameters : dict or None
632             The parameters that have been evaluated.
633         estimator : estimator object
634             The fitted estimator.
635         fit_error : str or None
636             Traceback str if the fit failed, None if the fit succeeded.
637     """
638     if not isinstance(error_score, numbers.Number) and error_score != "raise":
639         raise ValueError(
640             "error_score must be the string 'raise' or a numeric value. "
641             "(Hint: if using 'raise', please make sure that it has been "
642             "spelled correctly.)"
643         )
644 
645     progress_msg = ""
646     if verbose > 2:
647         if split_progress is not None:
648             progress_msg = f" {split_progress[0]+1}/{split_progress[1]}"
649         if candidate_progress and verbose > 9:
650             progress_msg += f"; {candidate_progress[0]+1}/{candidate_progress[1]}"
651 
652     if verbose > 1:
653         if parameters is None:
654             params_msg = ""
655         else:
656             sorted_keys = sorted(parameters)  # Ensure deterministic o/p
657             params_msg = ", ".join(f"{k}={parameters[k]}" for k in sorted_keys)
658     if verbose > 9:
659         start_msg = f"[CV{progress_msg}] START {params_msg}"
660         print(f"{start_msg}{(80 - len(start_msg)) * '.'}")
661 
662     # Adjust length of sample weights
663     fit_params = fit_params if fit_params is not None else {}
664     fit_params = _check_fit_params(X, fit_params, train)
665 
666     if parameters is not None:
667         # clone after setting parameters in case any parameters
668         # are estimators (like pipeline steps)
669         # because pipeline doesn't clone steps in fit
670         cloned_parameters = {}
671         for k, v in parameters.items():
672             cloned_parameters[k] = clone(v, safe=False)
673 
674         estimator = estimator.set_params(**cloned_parameters)
675 
676     start_time = time.time()
677 
678     X_train, y_train = _safe_split(estimator, X, y, train)
679     X_test, y_test = _safe_split(estimator, X, y, test, train)
680 
681     result = {}
682     try:
683         if y_train is None:
684             estimator.fit(X_train, **fit_params)
685         else:
686             estimator.fit(X_train, y_train, **fit_params)
687 
688     except Exception:
689         # Note fit time as time until error
690         fit_time = time.time() - start_time
691         score_time = 0.0
692         if error_score == "raise":
693             raise
694         elif isinstance(error_score, numbers.Number):
695             if isinstance(scorer, dict):
696                 test_scores = {name: error_score for name in scorer}
697                 if return_train_score:
698                     train_scores = test_scores.copy()
699             else:
700                 test_scores = error_score
701                 if return_train_score:
702                     train_scores = error_score
703         result["fit_error"] = format_exc()
704     else:
705         result["fit_error"] = None
706 
707         fit_time = time.time() - start_time
708         test_scores = _score(estimator, X_test, y_test, scorer, error_score)
709         score_time = time.time() - start_time - fit_time
710         if return_train_score:
711             train_scores = _score(estimator, X_train, y_train, scorer, error_score)
712 
713     if verbose > 1:
714         total_time = score_time + fit_time
715         end_msg = f"[CV{progress_msg}] END "
716         result_msg = params_msg + (";" if params_msg else "")
717         if verbose > 2:
718             if isinstance(test_scores, dict):
719                 for scorer_name in sorted(test_scores):
720                     result_msg += f" {scorer_name}: ("
721                     if return_train_score:
722                         scorer_scores = train_scores[scorer_name]
723                         result_msg += f"train={scorer_scores:.3f}, "
724                     result_msg += f"test={test_scores[scorer_name]:.3f})"
725             else:
726                 result_msg += ", score="
727                 if return_train_score:
728                     result_msg += f"(train={train_scores:.3f}, test={test_scores:.3f})"
729                 else:
730                     result_msg += f"{test_scores:.3f}"
731         result_msg += f" total time={logger.short_format_time(total_time)}"
732 
733         # Right align the result_msg
734         end_msg += "." * (80 - len(end_msg) - len(result_msg))
735         end_msg += result_msg
736         print(end_msg)
737 
738     result["test_scores"] = test_scores
739     if return_train_score:
740         result["train_scores"] = train_scores
741     if return_n_test_samples:
742         result["n_test_samples"] = _num_samples(X_test)
743     if return_times:
744         result["fit_time"] = fit_time
745         result["score_time"] = score_time
746     if return_parameters:
747         result["parameters"] = parameters
748     if return_estimator:
749         result["estimator"] = estimator
750     return result
751 
752 
753 def _score(estimator, X_test, y_test, scorer, error_score="raise"):
754     """Compute the score(s) of an estimator on a given test set.
755 
756     Will return a dict of floats if `scorer` is a dict, otherwise a single
757     float is returned.
758     """
759     if isinstance(scorer, dict):
760         # will cache method calls if needed. scorer() returns a dict
761         scorer = _MultimetricScorer(scorers=scorer, raise_exc=(error_score == "raise"))
762 
763     try:
764         if y_test is None:
765             scores = scorer(estimator, X_test)
766         else:
767             scores = scorer(estimator, X_test, y_test)
768     except Exception:
769         if isinstance(scorer, _MultimetricScorer):
770             # If `_MultimetricScorer` raises exception, the `error_score`
771             # parameter is equal to "raise".
772             raise
773         else:
774             if error_score == "raise":
775                 raise
776             else:
777                 scores = error_score
778                 warnings.warn(
779                     "Scoring failed. The score on this train-test partition for "
780                     f"these parameters will be set to {error_score}. Details: \n"
781                     f"{format_exc()}",
782                     UserWarning,
783                 )
784 
785     # Check non-raised error messages in `_MultimetricScorer`
786     if isinstance(scorer, _MultimetricScorer):
787         exception_messages = [
788             (name, str_e) for name, str_e in scores.items() if isinstance(str_e, str)
789         ]
790         if exception_messages:
791             # error_score != "raise"
792             for name, str_e in exception_messages:
793                 scores[name] = error_score
794                 warnings.warn(
795                     "Scoring failed. The score on this train-test partition for "
796                     f"these parameters will be set to {error_score}. Details: \n"
797                     f"{str_e}",
798                     UserWarning,
799                 )
800 
801     error_msg = "scoring must return a number, got %s (%s) instead. (scorer=%s)"
802     if isinstance(scores, dict):
803         for name, score in scores.items():
804             if hasattr(score, "item"):
805                 with suppress(ValueError):
806                     # e.g. unwrap memmapped scalars
807                     score = score.item()
808             if not isinstance(score, numbers.Number):
809                 raise ValueError(error_msg % (score, type(score), name))
810             scores[name] = score
811     else:  # scalar
812         if hasattr(scores, "item"):
813             with suppress(ValueError):
814                 # e.g. unwrap memmapped scalars
815                 scores = scores.item()
816         if not isinstance(scores, numbers.Number):
817             raise ValueError(error_msg % (scores, type(scores), scorer))
818     return scores
819 
820 
821 def cross_val_predict(
822     estimator,
823     X,
824     y=None,
825     *,
826     groups=None,
827     cv=None,
828     n_jobs=None,
829     verbose=0,
830     fit_params=None,
831     pre_dispatch="2*n_jobs",
832     method="predict",
833 ):
834     """Generate cross-validated estimates for each input data point.
835 
836     The data is split according to the cv parameter. Each sample belongs
837     to exactly one test set, and its prediction is computed with an
838     estimator fitted on the corresponding training set.
839 
840     Passing these predictions into an evaluation metric may not be a valid
841     way to measure generalization performance. Results can differ from
842     :func:`cross_validate` and :func:`cross_val_score` unless all tests sets
843     have equal size and the metric decomposes over samples.
844 
845     Read more in the :ref:`User Guide <cross_validation>`.
846 
847     Parameters
848     ----------
849     estimator : estimator object implementing 'fit' and 'predict'
850         The object to use to fit the data.
851 
852     X : array-like of shape (n_samples, n_features)
853         The data to fit. Can be, for example a list, or an array at least 2d.
854 
855     y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
856             default=None
857         The target variable to try to predict in the case of
858         supervised learning.
859 
860     groups : array-like of shape (n_samples,), default=None
861         Group labels for the samples used while splitting the dataset into
862         train/test set. Only used in conjunction with a "Group" :term:`cv`
863         instance (e.g., :class:`GroupKFold`).
864 
865     cv : int, cross-validation generator or an iterable, default=None
866         Determines the cross-validation splitting strategy.
867         Possible inputs for cv are:
868 
869         - None, to use the default 5-fold cross validation,
870         - int, to specify the number of folds in a `(Stratified)KFold`,
871         - :term:`CV splitter`,
872         - An iterable that generates (train, test) splits as arrays of indices.
873 
874         For int/None inputs, if the estimator is a classifier and ``y`` is
875         either binary or multiclass, :class:`StratifiedKFold` is used. In all
876         other cases, :class:`KFold` is used. These splitters are instantiated
877         with `shuffle=False` so the splits will be the same across calls.
878 
879         Refer :ref:`User Guide <cross_validation>` for the various
880         cross-validation strategies that can be used here.
881 
882         .. versionchanged:: 0.22
883             ``cv`` default value if None changed from 3-fold to 5-fold.
884 
885     n_jobs : int, default=None
886         Number of jobs to run in parallel. Training the estimator and
887         predicting are parallelized over the cross-validation splits.
888         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
889         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
890         for more details.
891 
892     verbose : int, default=0
893         The verbosity level.
894 
895     fit_params : dict, default=None
896         Parameters to pass to the fit method of the estimator.
897 
898     pre_dispatch : int or str, default='2*n_jobs'
899         Controls the number of jobs that get dispatched during parallel
900         execution. Reducing this number can be useful to avoid an
901         explosion of memory consumption when more jobs get dispatched
902         than CPUs can process. This parameter can be:
903 
904             - None, in which case all the jobs are immediately
905               created and spawned. Use this for lightweight and
906               fast-running jobs, to avoid delays due to on-demand
907               spawning of the jobs
908 
909             - An int, giving the exact number of total jobs that are
910               spawned
911 
912             - A str, giving an expression as a function of n_jobs,
913               as in '2*n_jobs'
914 
915     method : {'predict', 'predict_proba', 'predict_log_proba', \
916               'decision_function'}, default='predict'
917         The method to be invoked by `estimator`.
918 
919     Returns
920     -------
921     predictions : ndarray
922         This is the result of calling `method`. Shape:
923 
924             - When `method` is 'predict' and in special case where `method` is
925               'decision_function' and the target is binary: (n_samples,)
926             - When `method` is one of {'predict_proba', 'predict_log_proba',
927               'decision_function'} (unless special case above):
928               (n_samples, n_classes)
929             - If `estimator` is :term:`multioutput`, an extra dimension
930               'n_outputs' is added to the end of each shape above.
931 
932     See Also
933     --------
934     cross_val_score : Calculate score for each CV split.
935     cross_validate : Calculate one or more scores and timings for each CV
936         split.
937 
938     Notes
939     -----
940     In the case that one or more classes are absent in a training portion, a
941     default score needs to be assigned to all instances for that class if
942     ``method`` produces columns per class, as in {'decision_function',
943     'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is
944     0.  In order to ensure finite output, we approximate negative infinity by
945     the minimum finite float value for the dtype in other cases.
946 
947     Examples
948     --------
949     >>> from sklearn import datasets, linear_model
950     >>> from sklearn.model_selection import cross_val_predict
951     >>> diabetes = datasets.load_diabetes()
952     >>> X = diabetes.data[:150]
953     >>> y = diabetes.target[:150]
954     >>> lasso = linear_model.Lasso()
955     >>> y_pred = cross_val_predict(lasso, X, y, cv=3)
956     """
957     X, y, groups = indexable(X, y, groups)
958 
959     cv = check_cv(cv, y, classifier=is_classifier(estimator))
960     splits = list(cv.split(X, y, groups))
961 
962     test_indices = np.concatenate([test for _, test in splits])
963     if not _check_is_permutation(test_indices, _num_samples(X)):
964         raise ValueError("cross_val_predict only works for partitions")
965 
966     # If classification methods produce multiple columns of output,
967     # we need to manually encode classes to ensure consistent column ordering.
968     encode = (
969         method in ["decision_function", "predict_proba", "predict_log_proba"]
970         and y is not None
971     )
972     if encode:
973         y = np.asarray(y)
974         if y.ndim == 1:
975             le = LabelEncoder()
976             y = le.fit_transform(y)
977         elif y.ndim == 2:
978             y_enc = np.zeros_like(y, dtype=int)
979             for i_label in range(y.shape[1]):
980                 y_enc[:, i_label] = LabelEncoder().fit_transform(y[:, i_label])
981             y = y_enc
982 
983     # We clone the estimator to make sure that all the folds are
984     # independent, and that it is pickle-able.
985     parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
986     predictions = parallel(
987         delayed(_fit_and_predict)(
988             clone(estimator), X, y, train, test, verbose, fit_params, method
989         )
990         for train, test in splits
991     )
992 
993     inv_test_indices = np.empty(len(test_indices), dtype=int)
994     inv_test_indices[test_indices] = np.arange(len(test_indices))
995 
996     if sp.issparse(predictions[0]):
997         predictions = sp.vstack(predictions, format=predictions[0].format)
998     elif encode and isinstance(predictions[0], list):
999         # `predictions` is a list of method outputs from each fold.
1000         # If each of those is also a list, then treat this as a
1001         # multioutput-multiclass task. We need to separately concatenate
1002         # the method outputs for each label into an `n_labels` long list.
1003         n_labels = y.shape[1]
1004         concat_pred = []
1005         for i_label in range(n_labels):
1006             label_preds = np.concatenate([p[i_label] for p in predictions])
1007             concat_pred.append(label_preds)
1008         predictions = concat_pred
1009     else:
1010         predictions = np.concatenate(predictions)
1011 
1012     if isinstance(predictions, list):
1013         return [p[inv_test_indices] for p in predictions]
1014     else:
1015         return predictions[inv_test_indices]
1016 
1017 
1018 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params, method):
1019     """Fit estimator and predict values for a given dataset split.
1020 
1021     Read more in the :ref:`User Guide <cross_validation>`.
1022 
1023     Parameters
1024     ----------
1025     estimator : estimator object implementing 'fit' and 'predict'
1026         The object to use to fit the data.
1027 
1028     X : array-like of shape (n_samples, n_features)
1029         The data to fit.
1030 
1031         .. versionchanged:: 0.20
1032             X is only required to be an object with finite length or shape now
1033 
1034     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
1035         The target variable to try to predict in the case of
1036         supervised learning.
1037 
1038     train : array-like of shape (n_train_samples,)
1039         Indices of training samples.
1040 
1041     test : array-like of shape (n_test_samples,)
1042         Indices of test samples.
1043 
1044     verbose : int
1045         The verbosity level.
1046 
1047     fit_params : dict or None
1048         Parameters that will be passed to ``estimator.fit``.
1049 
1050     method : str
1051         Invokes the passed method name of the passed estimator.
1052 
1053     Returns
1054     -------
1055     predictions : sequence
1056         Result of calling 'estimator.method'
1057     """
1058     # Adjust length of sample weights
1059     fit_params = fit_params if fit_params is not None else {}
1060     fit_params = _check_fit_params(X, fit_params, train)
1061 
1062     X_train, y_train = _safe_split(estimator, X, y, train)
1063     X_test, _ = _safe_split(estimator, X, y, test, train)
1064 
1065     if y_train is None:
1066         estimator.fit(X_train, **fit_params)
1067     else:
1068         estimator.fit(X_train, y_train, **fit_params)
1069     func = getattr(estimator, method)
1070     predictions = func(X_test)
1071 
1072     encode = (
1073         method in ["decision_function", "predict_proba", "predict_log_proba"]
1074         and y is not None
1075     )
1076 
1077     if encode:
1078         if isinstance(predictions, list):
1079             predictions = [
1080                 _enforce_prediction_order(
1081                     estimator.classes_[i_label],
1082                     predictions[i_label],
1083                     n_classes=len(set(y[:, i_label])),
1084                     method=method,
1085                 )
1086                 for i_label in range(len(predictions))
1087             ]
1088         else:
1089             # A 2D y array should be a binary label indicator matrix
1090             n_classes = len(set(y)) if y.ndim == 1 else y.shape[1]
1091             predictions = _enforce_prediction_order(
1092                 estimator.classes_, predictions, n_classes, method
1093             )
1094     return predictions
1095 
1096 
1097 def _enforce_prediction_order(classes, predictions, n_classes, method):
1098     """Ensure that prediction arrays have correct column order
1099 
1100     When doing cross-validation, if one or more classes are
1101     not present in the subset of data used for training,
1102     then the output prediction array might not have the same
1103     columns as other folds. Use the list of class names
1104     (assumed to be ints) to enforce the correct column order.
1105 
1106     Note that `classes` is the list of classes in this fold
1107     (a subset of the classes in the full training set)
1108     and `n_classes` is the number of classes in the full training set.
1109     """
1110     if n_classes != len(classes):
1111         recommendation = (
1112             "To fix this, use a cross-validation "
1113             "technique resulting in properly "
1114             "stratified folds"
1115         )
1116         warnings.warn(
1117             "Number of classes in training fold ({}) does "
1118             "not match total number of classes ({}). "
1119             "Results may not be appropriate for your use case. "
1120             "{}".format(len(classes), n_classes, recommendation),
1121             RuntimeWarning,
1122         )
1123         if method == "decision_function":
1124             if predictions.ndim == 2 and predictions.shape[1] != len(classes):
1125                 # This handles the case when the shape of predictions
1126                 # does not match the number of classes used to train
1127                 # it with. This case is found when sklearn.svm.SVC is
1128                 # set to `decision_function_shape='ovo'`.
1129                 raise ValueError(
1130                     "Output shape {} of {} does not match "
1131                     "number of classes ({}) in fold. "
1132                     "Irregular decision_function outputs "
1133                     "are not currently supported by "
1134                     "cross_val_predict".format(predictions.shape, method, len(classes))
1135                 )
1136             if len(classes) <= 2:
1137                 # In this special case, `predictions` contains a 1D array.
1138                 raise ValueError(
1139                     "Only {} class/es in training fold, but {} "
1140                     "in overall dataset. This "
1141                     "is not supported for decision_function "
1142                     "with imbalanced folds. {}".format(
1143                         len(classes), n_classes, recommendation
1144                     )
1145                 )
1146 
1147         float_min = np.finfo(predictions.dtype).min
1148         default_values = {
1149             "decision_function": float_min,
1150             "predict_log_proba": float_min,
1151             "predict_proba": 0,
1152         }
1153         predictions_for_all_classes = np.full(
1154             (_num_samples(predictions), n_classes),
1155             default_values[method],
1156             dtype=predictions.dtype,
1157         )
1158         predictions_for_all_classes[:, classes] = predictions
1159         predictions = predictions_for_all_classes
1160     return predictions
1161 
1162 
1163 def _check_is_permutation(indices, n_samples):
1164     """Check whether indices is a reordering of the array np.arange(n_samples)
1165 
1166     Parameters
1167     ----------
1168     indices : ndarray
1169         int array to test
1170     n_samples : int
1171         number of expected elements
1172 
1173     Returns
1174     -------
1175     is_partition : bool
1176         True iff sorted(indices) is np.arange(n)
1177     """
1178     if len(indices) != n_samples:
1179         return False
1180     hit = np.zeros(n_samples, dtype=bool)
1181     hit[indices] = True
1182     if not np.all(hit):
1183         return False
1184     return True
1185 
1186 
1187 def permutation_test_score(
1188     estimator,
1189     X,
1190     y,
1191     *,
1192     groups=None,
1193     cv=None,
1194     n_permutations=100,
1195     n_jobs=None,
1196     random_state=0,
1197     verbose=0,
1198     scoring=None,
1199     fit_params=None,
1200 ):
1201     """Evaluate the significance of a cross-validated score with permutations.
1202 
1203     Permutes targets to generate 'randomized data' and compute the empirical
1204     p-value against the null hypothesis that features and targets are
1205     independent.
1206 
1207     The p-value represents the fraction of randomized data sets where the
1208     estimator performed as well or better than in the original data. A small
1209     p-value suggests that there is a real dependency between features and
1210     targets which has been used by the estimator to give good predictions.
1211     A large p-value may be due to lack of real dependency between features
1212     and targets or the estimator was not able to use the dependency to
1213     give good predictions.
1214 
1215     Read more in the :ref:`User Guide <permutation_test_score>`.
1216 
1217     Parameters
1218     ----------
1219     estimator : estimator object implementing 'fit'
1220         The object to use to fit the data.
1221 
1222     X : array-like of shape at least 2D
1223         The data to fit.
1224 
1225     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
1226         The target variable to try to predict in the case of
1227         supervised learning.
1228 
1229     groups : array-like of shape (n_samples,), default=None
1230         Labels to constrain permutation within groups, i.e. ``y`` values
1231         are permuted among samples with the same group identifier.
1232         When not specified, ``y`` values are permuted among all samples.
1233 
1234         When a grouped cross-validator is used, the group labels are
1235         also passed on to the ``split`` method of the cross-validator. The
1236         cross-validator uses them for grouping the samples  while splitting
1237         the dataset into train/test set.
1238 
1239     cv : int, cross-validation generator or an iterable, default=None
1240         Determines the cross-validation splitting strategy.
1241         Possible inputs for cv are:
1242 
1243         - `None`, to use the default 5-fold cross validation,
1244         - int, to specify the number of folds in a `(Stratified)KFold`,
1245         - :term:`CV splitter`,
1246         - An iterable yielding (train, test) splits as arrays of indices.
1247 
1248         For `int`/`None` inputs, if the estimator is a classifier and `y` is
1249         either binary or multiclass, :class:`StratifiedKFold` is used. In all
1250         other cases, :class:`KFold` is used. These splitters are instantiated
1251         with `shuffle=False` so the splits will be the same across calls.
1252 
1253         Refer :ref:`User Guide <cross_validation>` for the various
1254         cross-validation strategies that can be used here.
1255 
1256         .. versionchanged:: 0.22
1257             `cv` default value if `None` changed from 3-fold to 5-fold.
1258 
1259     n_permutations : int, default=100
1260         Number of times to permute ``y``.
1261 
1262     n_jobs : int, default=None
1263         Number of jobs to run in parallel. Training the estimator and computing
1264         the cross-validated score are parallelized over the permutations.
1265         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1266         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1267         for more details.
1268 
1269     random_state : int, RandomState instance or None, default=0
1270         Pass an int for reproducible output for permutation of
1271         ``y`` values among samples. See :term:`Glossary <random_state>`.
1272 
1273     verbose : int, default=0
1274         The verbosity level.
1275 
1276     scoring : str or callable, default=None
1277         A single str (see :ref:`scoring_parameter`) or a callable
1278         (see :ref:`scoring`) to evaluate the predictions on the test set.
1279 
1280         If `None` the estimator's score method is used.
1281 
1282     fit_params : dict, default=None
1283         Parameters to pass to the fit method of the estimator.
1284 
1285         .. versionadded:: 0.24
1286 
1287     Returns
1288     -------
1289     score : float
1290         The true score without permuting targets.
1291 
1292     permutation_scores : array of shape (n_permutations,)
1293         The scores obtained for each permutations.
1294 
1295     pvalue : float
1296         The p-value, which approximates the probability that the score would
1297         be obtained by chance. This is calculated as:
1298 
1299         `(C + 1) / (n_permutations + 1)`
1300 
1301         Where C is the number of permutations whose score >= the true score.
1302 
1303         The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.
1304 
1305     Notes
1306     -----
1307     This function implements Test 1 in:
1308 
1309         Ojala and Garriga. `Permutation Tests for Studying Classifier
1310         Performance
1311         <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The
1312         Journal of Machine Learning Research (2010) vol. 11
1313     """
1314     X, y, groups = indexable(X, y, groups)
1315 
1316     cv = check_cv(cv, y, classifier=is_classifier(estimator))
1317     scorer = check_scoring(estimator, scoring=scoring)
1318     random_state = check_random_state(random_state)
1319 
1320     # We clone the estimator to make sure that all the folds are
1321     # independent, and that it is pickle-able.
1322     score = _permutation_test_score(
1323         clone(estimator), X, y, groups, cv, scorer, fit_params=fit_params
1324     )
1325     permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(
1326         delayed(_permutation_test_score)(
1327             clone(estimator),
1328             X,
1329             _shuffle(y, groups, random_state),
1330             groups,
1331             cv,
1332             scorer,
1333             fit_params=fit_params,
1334         )
1335         for _ in range(n_permutations)
1336     )
1337     permutation_scores = np.array(permutation_scores)
1338     pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)
1339     return score, permutation_scores, pvalue
1340 
1341 
1342 def _permutation_test_score(estimator, X, y, groups, cv, scorer, fit_params):
1343     """Auxiliary function for permutation_test_score"""
1344     # Adjust length of sample weights
1345     fit_params = fit_params if fit_params is not None else {}
1346     avg_score = []
1347     for train, test in cv.split(X, y, groups):
1348         X_train, y_train = _safe_split(estimator, X, y, train)
1349         X_test, y_test = _safe_split(estimator, X, y, test, train)
1350         fit_params = _check_fit_params(X, fit_params, train)
1351         estimator.fit(X_train, y_train, **fit_params)
1352         avg_score.append(scorer(estimator, X_test, y_test))
1353     return np.mean(avg_score)
1354 
1355 
1356 def _shuffle(y, groups, random_state):
1357     """Return a shuffled copy of y eventually shuffle among same groups."""
1358     if groups is None:
1359         indices = random_state.permutation(len(y))
1360     else:
1361         indices = np.arange(len(groups))
1362         for group in np.unique(groups):
1363             this_mask = groups == group
1364             indices[this_mask] = random_state.permutation(indices[this_mask])
1365     return _safe_indexing(y, indices)
1366 
1367 
1368 def learning_curve(
1369     estimator,
1370     X,
1371     y,
1372     *,
1373     groups=None,
1374     train_sizes=np.linspace(0.1, 1.0, 5),
1375     cv=None,
1376     scoring=None,
1377     exploit_incremental_learning=False,
1378     n_jobs=None,
1379     pre_dispatch="all",
1380     verbose=0,
1381     shuffle=False,
1382     random_state=None,
1383     error_score=np.nan,
1384     return_times=False,
1385     fit_params=None,
1386 ):
1387     """Learning curve.
1388 
1389     Determines cross-validated training and test scores for different training
1390     set sizes.
1391 
1392     A cross-validation generator splits the whole dataset k times in training
1393     and test data. Subsets of the training set with varying sizes will be used
1394     to train the estimator and a score for each training subset size and the
1395     test set will be computed. Afterwards, the scores will be averaged over
1396     all k runs for each training subset size.
1397 
1398     Read more in the :ref:`User Guide <learning_curve>`.
1399 
1400     Parameters
1401     ----------
1402     estimator : object type that implements the "fit" and "predict" methods
1403         An object of that type which is cloned for each validation.
1404 
1405     X : array-like of shape (n_samples, n_features)
1406         Training vector, where `n_samples` is the number of samples and
1407         `n_features` is the number of features.
1408 
1409     y : array-like of shape (n_samples,) or (n_samples, n_outputs)
1410         Target relative to X for classification or regression;
1411         None for unsupervised learning.
1412 
1413     groups : array-like of  shape (n_samples,), default=None
1414         Group labels for the samples used while splitting the dataset into
1415         train/test set. Only used in conjunction with a "Group" :term:`cv`
1416         instance (e.g., :class:`GroupKFold`).
1417 
1418     train_sizes : array-like of shape (n_ticks,), \
1419             default=np.linspace(0.1, 1.0, 5)
1420         Relative or absolute numbers of training examples that will be used to
1421         generate the learning curve. If the dtype is float, it is regarded as a
1422         fraction of the maximum size of the training set (that is determined
1423         by the selected validation method), i.e. it has to be within (0, 1].
1424         Otherwise it is interpreted as absolute sizes of the training sets.
1425         Note that for classification the number of samples usually have to
1426         be big enough to contain at least one sample from each class.
1427 
1428     cv : int, cross-validation generator or an iterable, default=None
1429         Determines the cross-validation splitting strategy.
1430         Possible inputs for cv are:
1431 
1432         - None, to use the default 5-fold cross validation,
1433         - int, to specify the number of folds in a `(Stratified)KFold`,
1434         - :term:`CV splitter`,
1435         - An iterable yielding (train, test) splits as arrays of indices.
1436 
1437         For int/None inputs, if the estimator is a classifier and ``y`` is
1438         either binary or multiclass, :class:`StratifiedKFold` is used. In all
1439         other cases, :class:`KFold` is used. These splitters are instantiated
1440         with `shuffle=False` so the splits will be the same across calls.
1441 
1442         Refer :ref:`User Guide <cross_validation>` for the various
1443         cross-validation strategies that can be used here.
1444 
1445         .. versionchanged:: 0.22
1446             ``cv`` default value if None changed from 3-fold to 5-fold.
1447 
1448     scoring : str or callable, default=None
1449         A str (see model evaluation documentation) or
1450         a scorer callable object / function with signature
1451         ``scorer(estimator, X, y)``.
1452 
1453     exploit_incremental_learning : bool, default=False
1454         If the estimator supports incremental learning, this will be
1455         used to speed up fitting for different training set sizes.
1456 
1457     n_jobs : int, default=None
1458         Number of jobs to run in parallel. Training the estimator and computing
1459         the score are parallelized over the different training and test sets.
1460         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1461         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1462         for more details.
1463 
1464     pre_dispatch : int or str, default='all'
1465         Number of predispatched jobs for parallel execution (default is
1466         all). The option can reduce the allocated memory. The str can
1467         be an expression like '2*n_jobs'.
1468 
1469     verbose : int, default=0
1470         Controls the verbosity: the higher, the more messages.
1471 
1472     shuffle : bool, default=False
1473         Whether to shuffle training data before taking prefixes of it
1474         based on``train_sizes``.
1475 
1476     random_state : int, RandomState instance or None, default=None
1477         Used when ``shuffle`` is True. Pass an int for reproducible
1478         output across multiple function calls.
1479         See :term:`Glossary <random_state>`.
1480 
1481     error_score : 'raise' or numeric, default=np.nan
1482         Value to assign to the score if an error occurs in estimator fitting.
1483         If set to 'raise', the error is raised.
1484         If a numeric value is given, FitFailedWarning is raised.
1485 
1486         .. versionadded:: 0.20
1487 
1488     return_times : bool, default=False
1489         Whether to return the fit and score times.
1490 
1491     fit_params : dict, default=None
1492         Parameters to pass to the fit method of the estimator.
1493 
1494         .. versionadded:: 0.24
1495 
1496     Returns
1497     -------
1498     train_sizes_abs : array of shape (n_unique_ticks,)
1499         Numbers of training examples that has been used to generate the
1500         learning curve. Note that the number of ticks might be less
1501         than n_ticks because duplicate entries will be removed.
1502 
1503     train_scores : array of shape (n_ticks, n_cv_folds)
1504         Scores on training sets.
1505 
1506     test_scores : array of shape (n_ticks, n_cv_folds)
1507         Scores on test set.
1508 
1509     fit_times : array of shape (n_ticks, n_cv_folds)
1510         Times spent for fitting in seconds. Only present if ``return_times``
1511         is True.
1512 
1513     score_times : array of shape (n_ticks, n_cv_folds)
1514         Times spent for scoring in seconds. Only present if ``return_times``
1515         is True.
1516 
1517     Examples
1518     --------
1519     >>> from sklearn.datasets import make_classification
1520     >>> from sklearn.tree import DecisionTreeClassifier
1521     >>> from sklearn.model_selection import learning_curve
1522     >>> X, y = make_classification(n_samples=100, n_features=10, random_state=42)
1523     >>> tree = DecisionTreeClassifier(max_depth=4, random_state=42)
1524     >>> train_size_abs, train_scores, test_scores = learning_curve(
1525     ...     tree, X, y, train_sizes=[0.3, 0.6, 0.9]
1526     ... )
1527     >>> for train_size, cv_train_scores, cv_test_scores in zip(
1528     ...     train_size_abs, train_scores, test_scores
1529     ... ):
1530     ...     print(f"{train_size} samples were used to train the model")
1531     ...     print(f"The average train accuracy is {cv_train_scores.mean():.2f}")
1532     ...     print(f"The average test accuracy is {cv_test_scores.mean():.2f}")
1533     24 samples were used to train the model
1534     The average train accuracy is 1.00
1535     The average test accuracy is 0.85
1536     48 samples were used to train the model
1537     The average train accuracy is 1.00
1538     The average test accuracy is 0.90
1539     72 samples were used to train the model
1540     The average train accuracy is 1.00
1541     The average test accuracy is 0.93
1542     """
1543     if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):
1544         raise ValueError(
1545             "An estimator must support the partial_fit interface "
1546             "to exploit incremental learning"
1547         )
1548     X, y, groups = indexable(X, y, groups)
1549 
1550     cv = check_cv(cv, y, classifier=is_classifier(estimator))
1551     # Store it as list as we will be iterating over the list multiple times
1552     cv_iter = list(cv.split(X, y, groups))
1553 
1554     scorer = check_scoring(estimator, scoring=scoring)
1555 
1556     n_max_training_samples = len(cv_iter[0][0])
1557     # Because the lengths of folds can be significantly different, it is
1558     # not guaranteed that we use all of the available training data when we
1559     # use the first 'n_max_training_samples' samples.
1560     train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)
1561     n_unique_ticks = train_sizes_abs.shape[0]
1562     if verbose > 0:
1563         print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
1564 
1565     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
1566 
1567     if shuffle:
1568         rng = check_random_state(random_state)
1569         cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)
1570 
1571     if exploit_incremental_learning:
1572         classes = np.unique(y) if is_classifier(estimator) else None
1573         out = parallel(
1574             delayed(_incremental_fit_estimator)(
1575                 clone(estimator),
1576                 X,
1577                 y,
1578                 classes,
1579                 train,
1580                 test,
1581                 train_sizes_abs,
1582                 scorer,
1583                 verbose,
1584                 return_times,
1585                 error_score=error_score,
1586                 fit_params=fit_params,
1587             )
1588             for train, test in cv_iter
1589         )
1590         out = np.asarray(out).transpose((2, 1, 0))
1591     else:
1592         train_test_proportions = []
1593         for train, test in cv_iter:
1594             for n_train_samples in train_sizes_abs:
1595                 train_test_proportions.append((train[:n_train_samples], test))
1596 
1597         results = parallel(
1598             delayed(_fit_and_score)(
1599                 clone(estimator),
1600                 X,
1601                 y,
1602                 scorer,
1603                 train,
1604                 test,
1605                 verbose,
1606                 parameters=None,
1607                 fit_params=fit_params,
1608                 return_train_score=True,
1609                 error_score=error_score,
1610                 return_times=return_times,
1611             )
1612             for train, test in train_test_proportions
1613         )
1614         results = _aggregate_score_dicts(results)
1615         train_scores = results["train_scores"].reshape(-1, n_unique_ticks).T
1616         test_scores = results["test_scores"].reshape(-1, n_unique_ticks).T
1617         out = [train_scores, test_scores]
1618 
1619         if return_times:
1620             fit_times = results["fit_time"].reshape(-1, n_unique_ticks).T
1621             score_times = results["score_time"].reshape(-1, n_unique_ticks).T
1622             out.extend([fit_times, score_times])
1623 
1624     ret = train_sizes_abs, out[0], out[1]
1625 
1626     if return_times:
1627         ret = ret + (out[2], out[3])
1628 
1629     return ret
1630 
1631 
1632 def _translate_train_sizes(train_sizes, n_max_training_samples):
1633     """Determine absolute sizes of training subsets and validate 'train_sizes'.
1634 
1635     Examples:
1636         _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]
1637         _translate_train_sizes([5, 10], 10) -> [5, 10]
1638 
1639     Parameters
1640     ----------
1641     train_sizes : array-like of shape (n_ticks,)
1642         Numbers of training examples that will be used to generate the
1643         learning curve. If the dtype is float, it is regarded as a
1644         fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].
1645 
1646     n_max_training_samples : int
1647         Maximum number of training samples (upper bound of 'train_sizes').
1648 
1649     Returns
1650     -------
1651     train_sizes_abs : array of shape (n_unique_ticks,)
1652         Numbers of training examples that will be used to generate the
1653         learning curve. Note that the number of ticks might be less
1654         than n_ticks because duplicate entries will be removed.
1655     """
1656     train_sizes_abs = np.asarray(train_sizes)
1657     n_ticks = train_sizes_abs.shape[0]
1658     n_min_required_samples = np.min(train_sizes_abs)
1659     n_max_required_samples = np.max(train_sizes_abs)
1660     if np.issubdtype(train_sizes_abs.dtype, np.floating):
1661         if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:
1662             raise ValueError(
1663                 "train_sizes has been interpreted as fractions "
1664                 "of the maximum number of training samples and "
1665                 "must be within (0, 1], but is within [%f, %f]."
1666                 % (n_min_required_samples, n_max_required_samples)
1667             )
1668         train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(
1669             dtype=int, copy=False
1670         )
1671         train_sizes_abs = np.clip(train_sizes_abs, 1, n_max_training_samples)
1672     else:
1673         if (
1674             n_min_required_samples <= 0
1675             or n_max_required_samples > n_max_training_samples
1676         ):
1677             raise ValueError(
1678                 "train_sizes has been interpreted as absolute "
1679                 "numbers of training samples and must be within "
1680                 "(0, %d], but is within [%d, %d]."
1681                 % (
1682                     n_max_training_samples,
1683                     n_min_required_samples,
1684                     n_max_required_samples,
1685                 )
1686             )
1687 
1688     train_sizes_abs = np.unique(train_sizes_abs)
1689     if n_ticks > train_sizes_abs.shape[0]:
1690         warnings.warn(
1691             "Removed duplicate entries from 'train_sizes'. Number "
1692             "of ticks will be less than the size of "
1693             "'train_sizes': %d instead of %d." % (train_sizes_abs.shape[0], n_ticks),
1694             RuntimeWarning,
1695         )
1696 
1697     return train_sizes_abs
1698 
1699 
1700 def _incremental_fit_estimator(
1701     estimator,
1702     X,
1703     y,
1704     classes,
1705     train,
1706     test,
1707     train_sizes,
1708     scorer,
1709     verbose,
1710     return_times,
1711     error_score,
1712     fit_params,
1713 ):
1714     """Train estimator on training subsets incrementally and compute scores."""
1715     train_scores, test_scores, fit_times, score_times = [], [], [], []
1716     partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])
1717     if fit_params is None:
1718         fit_params = {}
1719     if classes is None:
1720         partial_fit_func = partial(estimator.partial_fit, **fit_params)
1721     else:
1722         partial_fit_func = partial(estimator.partial_fit, classes=classes, **fit_params)
1723 
1724     for n_train_samples, partial_train in partitions:
1725         train_subset = train[:n_train_samples]
1726         X_train, y_train = _safe_split(estimator, X, y, train_subset)
1727         X_partial_train, y_partial_train = _safe_split(estimator, X, y, partial_train)
1728         X_test, y_test = _safe_split(estimator, X, y, test, train_subset)
1729         start_fit = time.time()
1730         if y_partial_train is None:
1731             partial_fit_func(X_partial_train)
1732         else:
1733             partial_fit_func(X_partial_train, y_partial_train)
1734         fit_time = time.time() - start_fit
1735         fit_times.append(fit_time)
1736 
1737         start_score = time.time()
1738 
1739         test_scores.append(_score(estimator, X_test, y_test, scorer, error_score))
1740         train_scores.append(_score(estimator, X_train, y_train, scorer, error_score))
1741 
1742         score_time = time.time() - start_score
1743         score_times.append(score_time)
1744 
1745     ret = (
1746         (train_scores, test_scores, fit_times, score_times)
1747         if return_times
1748         else (train_scores, test_scores)
1749     )
1750 
1751     return np.array(ret).T
1752 
1753 
1754 def validation_curve(
1755     estimator,
1756     X,
1757     y,
1758     *,
1759     param_name,
1760     param_range,
1761     groups=None,
1762     cv=None,
1763     scoring=None,
1764     n_jobs=None,
1765     pre_dispatch="all",
1766     verbose=0,
1767     error_score=np.nan,
1768     fit_params=None,
1769 ):
1770     """Validation curve.
1771 
1772     Determine training and test scores for varying parameter values.
1773 
1774     Compute scores for an estimator with different values of a specified
1775     parameter. This is similar to grid search with one parameter. However, this
1776     will also compute training scores and is merely a utility for plotting the
1777     results.
1778 
1779     Read more in the :ref:`User Guide <validation_curve>`.
1780 
1781     Parameters
1782     ----------
1783     estimator : object type that implements the "fit" and "predict" methods
1784         An object of that type which is cloned for each validation.
1785 
1786     X : array-like of shape (n_samples, n_features)
1787         Training vector, where `n_samples` is the number of samples and
1788         `n_features` is the number of features.
1789 
1790     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
1791         Target relative to X for classification or regression;
1792         None for unsupervised learning.
1793 
1794     param_name : str
1795         Name of the parameter that will be varied.
1796 
1797     param_range : array-like of shape (n_values,)
1798         The values of the parameter that will be evaluated.
1799 
1800     groups : array-like of shape (n_samples,), default=None
1801         Group labels for the samples used while splitting the dataset into
1802         train/test set. Only used in conjunction with a "Group" :term:`cv`
1803         instance (e.g., :class:`GroupKFold`).
1804 
1805     cv : int, cross-validation generator or an iterable, default=None
1806         Determines the cross-validation splitting strategy.
1807         Possible inputs for cv are:
1808 
1809         - None, to use the default 5-fold cross validation,
1810         - int, to specify the number of folds in a `(Stratified)KFold`,
1811         - :term:`CV splitter`,
1812         - An iterable yielding (train, test) splits as arrays of indices.
1813 
1814         For int/None inputs, if the estimator is a classifier and ``y`` is
1815         either binary or multiclass, :class:`StratifiedKFold` is used. In all
1816         other cases, :class:`KFold` is used. These splitters are instantiated
1817         with `shuffle=False` so the splits will be the same across calls.
1818 
1819         Refer :ref:`User Guide <cross_validation>` for the various
1820         cross-validation strategies that can be used here.
1821 
1822         .. versionchanged:: 0.22
1823             ``cv`` default value if None changed from 3-fold to 5-fold.
1824 
1825     scoring : str or callable, default=None
1826         A str (see model evaluation documentation) or
1827         a scorer callable object / function with signature
1828         ``scorer(estimator, X, y)``.
1829 
1830     n_jobs : int, default=None
1831         Number of jobs to run in parallel. Training the estimator and computing
1832         the score are parallelized over the combinations of each parameter
1833         value and each cross-validation split.
1834         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1835         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1836         for more details.
1837 
1838     pre_dispatch : int or str, default='all'
1839         Number of predispatched jobs for parallel execution (default is
1840         all). The option can reduce the allocated memory. The str can
1841         be an expression like '2*n_jobs'.
1842 
1843     verbose : int, default=0
1844         Controls the verbosity: the higher, the more messages.
1845 
1846     error_score : 'raise' or numeric, default=np.nan
1847         Value to assign to the score if an error occurs in estimator fitting.
1848         If set to 'raise', the error is raised.
1849         If a numeric value is given, FitFailedWarning is raised.
1850 
1851         .. versionadded:: 0.20
1852 
1853     fit_params : dict, default=None
1854         Parameters to pass to the fit method of the estimator.
1855 
1856         .. versionadded:: 0.24
1857 
1858     Returns
1859     -------
1860     train_scores : array of shape (n_ticks, n_cv_folds)
1861         Scores on training sets.
1862 
1863     test_scores : array of shape (n_ticks, n_cv_folds)
1864         Scores on test set.
1865 
1866     Notes
1867     -----
1868     See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`
1869     """
1870     X, y, groups = indexable(X, y, groups)
1871 
1872     cv = check_cv(cv, y, classifier=is_classifier(estimator))
1873     scorer = check_scoring(estimator, scoring=scoring)
1874 
1875     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
1876     results = parallel(
1877         delayed(_fit_and_score)(
1878             clone(estimator),
1879             X,
1880             y,
1881             scorer,
1882             train,
1883             test,
1884             verbose,
1885             parameters={param_name: v},
1886             fit_params=fit_params,
1887             return_train_score=True,
1888             error_score=error_score,
1889         )
1890         # NOTE do not change order of iteration to allow one time cv splitters
1891         for train, test in cv.split(X, y, groups)
1892         for v in param_range
1893     )
1894     n_params = len(param_range)
1895 
1896     results = _aggregate_score_dicts(results)
1897     train_scores = results["train_scores"].reshape(-1, n_params).T
1898     test_scores = results["test_scores"].reshape(-1, n_params).T
1899 
1900     return train_scores, test_scores
1901 
1902 
1903 def _aggregate_score_dicts(scores):
1904     """Aggregate the list of dict to dict of np ndarray
1905 
1906     The aggregated output of _aggregate_score_dicts will be a list of dict
1907     of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]
1908     Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}
1909 
1910     Parameters
1911     ----------
1912 
1913     scores : list of dict
1914         List of dicts of the scores for all scorers. This is a flat list,
1915         assumed originally to be of row major order.
1916 
1917     Example
1918     -------
1919 
1920     >>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3},
1921     ...           {'a': 10, 'b': 10}]                         # doctest: +SKIP
1922     >>> _aggregate_score_dicts(scores)                        # doctest: +SKIP
1923     {'a': array([1, 2, 3, 10]),
1924      'b': array([10, 2, 3, 10])}
1925     """
1926     return {
1927         key: np.asarray([score[key] for score in scores])
1928         if isinstance(scores[0][key], numbers.Number)
1929         else [score[key] for score in scores]
1930         for key in scores[0]
1931     }

</code><intended_behavior>The intended behavior here is to ensure that configurations, especially those crucial for the operation of Parallel executed tasks, are explicitly passed to each delayed call. This might require wrapping the delayed function calls within a context that ensures the current configuration state is captured and then applied within each parallelly executed task. In essence, the fix involves ensuring that any configuration context is not just assumed but is explicitly conveyed and reconstructed within each task's execution environment.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Forest of trees-based ensemble methods.
3 
4 Those methods include random forests and extremely randomized trees.
5 
6 The module structure is the following:
7 
8 - The ``BaseForest`` base class implements a common ``fit`` method for all
9   the estimators in the module. The ``fit`` method of the base ``Forest``
10   class calls the ``fit`` method of each sub-estimator on random samples
11   (with replacement, a.k.a. bootstrap) of the training set.
12 
13   The init of the sub-estimator is further delegated to the
14   ``BaseEnsemble`` constructor.
15 
16 - The ``ForestClassifier`` and ``ForestRegressor`` base classes further
17   implement the prediction logic by computing an average of the predicted
18   outcomes of the sub-estimators.
19 
20 - The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived
21   classes provide the user with concrete implementations of
22   the forest ensemble method using classical, deterministic
23   ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as
24   sub-estimator implementations.
25 
26 - The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived
27   classes provide the user with concrete implementations of the
28   forest ensemble method using the extremely randomized trees
29   ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as
30   sub-estimator implementations.
31 
32 Single and multi-output problems are both handled.
33 """
34 
35 # Authors: Gilles Louppe <g.louppe@gmail.com>
36 #          Brian Holt <bdholt1@gmail.com>
37 #          Joly Arnaud <arnaud.v.joly@gmail.com>
38 #          Fares Hedayati <fares.hedayati@gmail.com>
39 #
40 # License: BSD 3 clause
41 
42 
43 from numbers import Integral, Real
44 from warnings import catch_warnings, simplefilter, warn
45 import threading
46 
47 from abc import ABCMeta, abstractmethod
48 import numpy as np
49 from scipy.sparse import issparse
50 from scipy.sparse import hstack as sparse_hstack
51 from joblib import Parallel
52 
53 from ..base import is_classifier
54 from ..base import ClassifierMixin, MultiOutputMixin, RegressorMixin, TransformerMixin
55 
56 from ..metrics import accuracy_score, r2_score
57 from ..preprocessing import OneHotEncoder
58 from ..tree import (
59     BaseDecisionTree,
60     DecisionTreeClassifier,
61     DecisionTreeRegressor,
62     ExtraTreeClassifier,
63     ExtraTreeRegressor,
64 )
65 from ..tree._tree import DTYPE, DOUBLE
66 from ..utils import check_random_state, compute_sample_weight
67 from ..exceptions import DataConversionWarning
68 from ._base import BaseEnsemble, _partition_estimators
69 from ..utils.fixes import delayed
70 from ..utils.multiclass import check_classification_targets, type_of_target
71 from ..utils.validation import (
72     check_is_fitted,
73     _check_sample_weight,
74     _check_feature_names_in,
75 )
76 from ..utils.validation import _num_samples
77 from ..utils._param_validation import Interval, StrOptions
78 
79 
80 __all__ = [
81     "RandomForestClassifier",
82     "RandomForestRegressor",
83     "ExtraTreesClassifier",
84     "ExtraTreesRegressor",
85     "RandomTreesEmbedding",
86 ]
87 
88 MAX_INT = np.iinfo(np.int32).max
89 
90 
91 def _get_n_samples_bootstrap(n_samples, max_samples):
92     """
93     Get the number of samples in a bootstrap sample.
94 
95     Parameters
96     ----------
97     n_samples : int
98         Number of samples in the dataset.
99     max_samples : int or float
100         The maximum number of samples to draw from the total available:
101             - if float, this indicates a fraction of the total and should be
102               the interval `(0.0, 1.0]`;
103             - if int, this indicates the exact number of samples;
104             - if None, this indicates the total number of samples.
105 
106     Returns
107     -------
108     n_samples_bootstrap : int
109         The total number of samples to draw for the bootstrap sample.
110     """
111     if max_samples is None:
112         return n_samples
113 
114     if isinstance(max_samples, Integral):
115         if max_samples > n_samples:
116             msg = "`max_samples` must be <= n_samples={} but got value {}"
117             raise ValueError(msg.format(n_samples, max_samples))
118         return max_samples
119 
120     if isinstance(max_samples, Real):
121         return round(n_samples * max_samples)
122 
123 
124 def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):
125     """
126     Private function used to _parallel_build_trees function."""
127 
128     random_instance = check_random_state(random_state)
129     sample_indices = random_instance.randint(0, n_samples, n_samples_bootstrap)
130 
131     return sample_indices
132 
133 
134 def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):
135     """
136     Private function used to forest._set_oob_score function."""
137     sample_indices = _generate_sample_indices(
138         random_state, n_samples, n_samples_bootstrap
139     )
140     sample_counts = np.bincount(sample_indices, minlength=n_samples)
141     unsampled_mask = sample_counts == 0
142     indices_range = np.arange(n_samples)
143     unsampled_indices = indices_range[unsampled_mask]
144 
145     return unsampled_indices
146 
147 
148 def _parallel_build_trees(
149     tree,
150     bootstrap,
151     X,
152     y,
153     sample_weight,
154     tree_idx,
155     n_trees,
156     verbose=0,
157     class_weight=None,
158     n_samples_bootstrap=None,
159 ):
160     """
161     Private function used to fit a single tree in parallel."""
162     if verbose > 1:
163         print("building tree %d of %d" % (tree_idx + 1, n_trees))
164 
165     if bootstrap:
166         n_samples = X.shape[0]
167         if sample_weight is None:
168             curr_sample_weight = np.ones((n_samples,), dtype=np.float64)
169         else:
170             curr_sample_weight = sample_weight.copy()
171 
172         indices = _generate_sample_indices(
173             tree.random_state, n_samples, n_samples_bootstrap
174         )
175         sample_counts = np.bincount(indices, minlength=n_samples)
176         curr_sample_weight *= sample_counts
177 
178         if class_weight == "subsample":
179             with catch_warnings():
180                 simplefilter("ignore", DeprecationWarning)
181                 curr_sample_weight *= compute_sample_weight("auto", y, indices=indices)
182         elif class_weight == "balanced_subsample":
183             curr_sample_weight *= compute_sample_weight("balanced", y, indices=indices)
184 
185         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
186     else:
187         tree.fit(X, y, sample_weight=sample_weight, check_input=False)
188 
189     return tree
190 
191 
192 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):
193     """
194     Base class for forests of trees.
195 
196     Warning: This class should not be used directly. Use derived classes
197     instead.
198     """
199 
200     _parameter_constraints: dict = {
201         "n_estimators": [Interval(Integral, 1, None, closed="left")],
202         "bootstrap": ["boolean"],
203         "oob_score": ["boolean", callable],
204         "n_jobs": [Integral, None],
205         "random_state": ["random_state"],
206         "verbose": ["verbose"],
207         "warm_start": ["boolean"],
208         "max_samples": [
209             None,
210             Interval(Real, 0.0, 1.0, closed="right"),
211             Interval(Integral, 1, None, closed="left"),
212         ],
213     }
214 
215     @abstractmethod
216     def __init__(
217         self,
218         estimator,
219         n_estimators=100,
220         *,
221         estimator_params=tuple(),
222         bootstrap=False,
223         oob_score=False,
224         n_jobs=None,
225         random_state=None,
226         verbose=0,
227         warm_start=False,
228         class_weight=None,
229         max_samples=None,
230         base_estimator="deprecated",
231     ):
232         super().__init__(
233             estimator=estimator,
234             n_estimators=n_estimators,
235             estimator_params=estimator_params,
236             base_estimator=base_estimator,
237         )
238 
239         self.bootstrap = bootstrap
240         self.oob_score = oob_score
241         self.n_jobs = n_jobs
242         self.random_state = random_state
243         self.verbose = verbose
244         self.warm_start = warm_start
245         self.class_weight = class_weight
246         self.max_samples = max_samples
247 
248     def apply(self, X):
249         """
250         Apply trees in the forest to X, return leaf indices.
251 
252         Parameters
253         ----------
254         X : {array-like, sparse matrix} of shape (n_samples, n_features)
255             The input samples. Internally, its dtype will be converted to
256             ``dtype=np.float32``. If a sparse matrix is provided, it will be
257             converted into a sparse ``csr_matrix``.
258 
259         Returns
260         -------
261         X_leaves : ndarray of shape (n_samples, n_estimators)
262             For each datapoint x in X and for each tree in the forest,
263             return the index of the leaf x ends up in.
264         """
265         X = self._validate_X_predict(X)
266         results = Parallel(
267             n_jobs=self.n_jobs,
268             verbose=self.verbose,
269             prefer="threads",
270         )(delayed(tree.apply)(X, check_input=False) for tree in self.estimators_)
271 
272         return np.array(results).T
273 
274     def decision_path(self, X):
275         """
276         Return the decision path in the forest.
277 
278         .. versionadded:: 0.18
279 
280         Parameters
281         ----------
282         X : {array-like, sparse matrix} of shape (n_samples, n_features)
283             The input samples. Internally, its dtype will be converted to
284             ``dtype=np.float32``. If a sparse matrix is provided, it will be
285             converted into a sparse ``csr_matrix``.
286 
287         Returns
288         -------
289         indicator : sparse matrix of shape (n_samples, n_nodes)
290             Return a node indicator matrix where non zero elements indicates
291             that the samples goes through the nodes. The matrix is of CSR
292             format.
293 
294         n_nodes_ptr : ndarray of shape (n_estimators + 1,)
295             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
296             gives the indicator value for the i-th estimator.
297         """
298         X = self._validate_X_predict(X)
299         indicators = Parallel(
300             n_jobs=self.n_jobs,
301             verbose=self.verbose,
302             prefer="threads",
303         )(
304             delayed(tree.decision_path)(X, check_input=False)
305             for tree in self.estimators_
306         )
307 
308         n_nodes = [0]
309         n_nodes.extend([i.shape[1] for i in indicators])
310         n_nodes_ptr = np.array(n_nodes).cumsum()
311 
312         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
313 
314     def fit(self, X, y, sample_weight=None):
315         """
316         Build a forest of trees from the training set (X, y).
317 
318         Parameters
319         ----------
320         X : {array-like, sparse matrix} of shape (n_samples, n_features)
321             The training input samples. Internally, its dtype will be converted
322             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
323             converted into a sparse ``csc_matrix``.
324 
325         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
326             The target values (class labels in classification, real numbers in
327             regression).
328 
329         sample_weight : array-like of shape (n_samples,), default=None
330             Sample weights. If None, then samples are equally weighted. Splits
331             that would create child nodes with net zero or negative weight are
332             ignored while searching for a split in each node. In the case of
333             classification, splits are also ignored if they would result in any
334             single class carrying a negative weight in either child node.
335 
336         Returns
337         -------
338         self : object
339             Fitted estimator.
340         """
341         self._validate_params()
342 
343         # Validate or convert input data
344         if issparse(y):
345             raise ValueError("sparse multilabel-indicator for y is not supported.")
346         X, y = self._validate_data(
347             X, y, multi_output=True, accept_sparse="csc", dtype=DTYPE
348         )
349         if sample_weight is not None:
350             sample_weight = _check_sample_weight(sample_weight, X)
351 
352         if issparse(X):
353             # Pre-sort indices to avoid that each individual tree of the
354             # ensemble sorts the indices.
355             X.sort_indices()
356 
357         y = np.atleast_1d(y)
358         if y.ndim == 2 and y.shape[1] == 1:
359             warn(
360                 "A column-vector y was passed when a 1d array was"
361                 " expected. Please change the shape of y to "
362                 "(n_samples,), for example using ravel().",
363                 DataConversionWarning,
364                 stacklevel=2,
365             )
366 
367         if y.ndim == 1:
368             # reshape is necessary to preserve the data contiguity against vs
369             # [:, np.newaxis] that does not.
370             y = np.reshape(y, (-1, 1))
371 
372         if self.criterion == "poisson":
373             if np.any(y < 0):
374                 raise ValueError(
375                     "Some value(s) of y are negative which is "
376                     "not allowed for Poisson regression."
377                 )
378             if np.sum(y) <= 0:
379                 raise ValueError(
380                     "Sum of y is not strictly positive which "
381                     "is necessary for Poisson regression."
382                 )
383 
384         self.n_outputs_ = y.shape[1]
385 
386         y, expanded_class_weight = self._validate_y_class_weight(y)
387 
388         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
389             y = np.ascontiguousarray(y, dtype=DOUBLE)
390 
391         if expanded_class_weight is not None:
392             if sample_weight is not None:
393                 sample_weight = sample_weight * expanded_class_weight
394             else:
395                 sample_weight = expanded_class_weight
396 
397         if not self.bootstrap and self.max_samples is not None:
398             raise ValueError(
399                 "`max_sample` cannot be set if `bootstrap=False`. "
400                 "Either switch to `bootstrap=True` or set "
401                 "`max_sample=None`."
402             )
403         elif self.bootstrap:
404             n_samples_bootstrap = _get_n_samples_bootstrap(
405                 n_samples=X.shape[0], max_samples=self.max_samples
406             )
407         else:
408             n_samples_bootstrap = None
409 
410         self._validate_estimator()
411         if isinstance(self, (RandomForestRegressor, ExtraTreesRegressor)):
412             # TODO(1.3): Remove "auto"
413             if self.max_features == "auto":
414                 warn(
415                     "`max_features='auto'` has been deprecated in 1.1 "
416                     "and will be removed in 1.3. To keep the past behaviour, "
417                     "explicitly set `max_features=1.0` or remove this "
418                     "parameter as it is also the default value for "
419                     "RandomForestRegressors and ExtraTreesRegressors.",
420                     FutureWarning,
421                 )
422         elif isinstance(self, (RandomForestClassifier, ExtraTreesClassifier)):
423             # TODO(1.3): Remove "auto"
424             if self.max_features == "auto":
425                 warn(
426                     "`max_features='auto'` has been deprecated in 1.1 "
427                     "and will be removed in 1.3. To keep the past behaviour, "
428                     "explicitly set `max_features='sqrt'` or remove this "
429                     "parameter as it is also the default value for "
430                     "RandomForestClassifiers and ExtraTreesClassifiers.",
431                     FutureWarning,
432                 )
433 
434         if not self.bootstrap and self.oob_score:
435             raise ValueError("Out of bag estimation only available if bootstrap=True")
436 
437         random_state = check_random_state(self.random_state)
438 
439         if not self.warm_start or not hasattr(self, "estimators_"):
440             # Free allocated memory, if any
441             self.estimators_ = []
442 
443         n_more_estimators = self.n_estimators - len(self.estimators_)
444 
445         if n_more_estimators < 0:
446             raise ValueError(
447                 "n_estimators=%d must be larger or equal to "
448                 "len(estimators_)=%d when warm_start==True"
449                 % (self.n_estimators, len(self.estimators_))
450             )
451 
452         elif n_more_estimators == 0:
453             warn(
454                 "Warm-start fitting without increasing n_estimators does not "
455                 "fit new trees."
456             )
457         else:
458             if self.warm_start and len(self.estimators_) > 0:
459                 # We draw from the random state to get the random state we
460                 # would have got if we hadn't used a warm_start.
461                 random_state.randint(MAX_INT, size=len(self.estimators_))
462 
463             trees = [
464                 self._make_estimator(append=False, random_state=random_state)
465                 for i in range(n_more_estimators)
466             ]
467 
468             # Parallel loop: we prefer the threading backend as the Cython code
469             # for fitting the trees is internally releasing the Python GIL
470             # making threading more efficient than multiprocessing in
471             # that case. However, for joblib 0.12+ we respect any
472             # parallel_backend contexts set at a higher level,
473             # since correctness does not rely on using threads.
474             trees = Parallel(
475                 n_jobs=self.n_jobs,
476                 verbose=self.verbose,
477                 prefer="threads",
478             )(
479                 delayed(_parallel_build_trees)(
480                     t,
481                     self.bootstrap,
482                     X,
483                     y,
484                     sample_weight,
485                     i,
486                     len(trees),
487                     verbose=self.verbose,
488                     class_weight=self.class_weight,
489                     n_samples_bootstrap=n_samples_bootstrap,
490                 )
491                 for i, t in enumerate(trees)
492             )
493 
494             # Collect newly grown trees
495             self.estimators_.extend(trees)
496 
497         if self.oob_score:
498             y_type = type_of_target(y)
499             if y_type in ("multiclass-multioutput", "unknown"):
500                 # FIXME: we could consider to support multiclass-multioutput if
501                 # we introduce or reuse a constructor parameter (e.g.
502                 # oob_score) allowing our user to pass a callable defining the
503                 # scoring strategy on OOB sample.
504                 raise ValueError(
505                     "The type of target cannot be used to compute OOB "
506                     f"estimates. Got {y_type} while only the following are "
507                     "supported: continuous, continuous-multioutput, binary, "
508                     "multiclass, multilabel-indicator."
509                 )
510 
511             if callable(self.oob_score):
512                 self._set_oob_score_and_attributes(
513                     X, y, scoring_function=self.oob_score
514                 )
515             else:
516                 self._set_oob_score_and_attributes(X, y)
517 
518         # Decapsulate classes_ attributes
519         if hasattr(self, "classes_") and self.n_outputs_ == 1:
520             self.n_classes_ = self.n_classes_[0]
521             self.classes_ = self.classes_[0]
522 
523         return self
524 
525     @abstractmethod
526     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
527         """Compute and set the OOB score and attributes.
528 
529         Parameters
530         ----------
531         X : array-like of shape (n_samples, n_features)
532             The data matrix.
533         y : ndarray of shape (n_samples, n_outputs)
534             The target matrix.
535         scoring_function : callable, default=None
536             Scoring function for OOB score. Default depends on whether
537             this is a regression (R2 score) or classification problem
538             (accuracy score).
539         """
540 
541     def _compute_oob_predictions(self, X, y):
542         """Compute and set the OOB score.
543 
544         Parameters
545         ----------
546         X : array-like of shape (n_samples, n_features)
547             The data matrix.
548         y : ndarray of shape (n_samples, n_outputs)
549             The target matrix.
550 
551         Returns
552         -------
553         oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or \
554                 (n_samples, 1, n_outputs)
555             The OOB predictions.
556         """
557         # Prediction requires X to be in CSR format
558         if issparse(X):
559             X = X.tocsr()
560 
561         n_samples = y.shape[0]
562         n_outputs = self.n_outputs_
563         if is_classifier(self) and hasattr(self, "n_classes_"):
564             # n_classes_ is a ndarray at this stage
565             # all the supported type of target will have the same number of
566             # classes in all outputs
567             oob_pred_shape = (n_samples, self.n_classes_[0], n_outputs)
568         else:
569             # for regression, n_classes_ does not exist and we create an empty
570             # axis to be consistent with the classification case and make
571             # the array operations compatible with the 2 settings
572             oob_pred_shape = (n_samples, 1, n_outputs)
573 
574         oob_pred = np.zeros(shape=oob_pred_shape, dtype=np.float64)
575         n_oob_pred = np.zeros((n_samples, n_outputs), dtype=np.int64)
576 
577         n_samples_bootstrap = _get_n_samples_bootstrap(
578             n_samples,
579             self.max_samples,
580         )
581         for estimator in self.estimators_:
582             unsampled_indices = _generate_unsampled_indices(
583                 estimator.random_state,
584                 n_samples,
585                 n_samples_bootstrap,
586             )
587 
588             y_pred = self._get_oob_predictions(estimator, X[unsampled_indices, :])
589             oob_pred[unsampled_indices, ...] += y_pred
590             n_oob_pred[unsampled_indices, :] += 1
591 
592         for k in range(n_outputs):
593             if (n_oob_pred == 0).any():
594                 warn(
595                     "Some inputs do not have OOB scores. This probably means "
596                     "too few trees were used to compute any reliable OOB "
597                     "estimates.",
598                     UserWarning,
599                 )
600                 n_oob_pred[n_oob_pred == 0] = 1
601             oob_pred[..., k] /= n_oob_pred[..., [k]]
602 
603         return oob_pred
604 
605     def _validate_y_class_weight(self, y):
606         # Default implementation
607         return y, None
608 
609     def _validate_X_predict(self, X):
610         """
611         Validate X whenever one tries to predict, apply, predict_proba."""
612         check_is_fitted(self)
613         X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
614         if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):
615             raise ValueError("No support for np.int64 index based sparse matrices")
616         return X
617 
618     @property
619     def feature_importances_(self):
620         """
621         The impurity-based feature importances.
622 
623         The higher, the more important the feature.
624         The importance of a feature is computed as the (normalized)
625         total reduction of the criterion brought by that feature.  It is also
626         known as the Gini importance.
627 
628         Warning: impurity-based feature importances can be misleading for
629         high cardinality features (many unique values). See
630         :func:`sklearn.inspection.permutation_importance` as an alternative.
631 
632         Returns
633         -------
634         feature_importances_ : ndarray of shape (n_features,)
635             The values of this array sum to 1, unless all trees are single node
636             trees consisting of only the root node, in which case it will be an
637             array of zeros.
638         """
639         check_is_fitted(self)
640 
641         all_importances = Parallel(n_jobs=self.n_jobs, prefer="threads")(
642             delayed(getattr)(tree, "feature_importances_")
643             for tree in self.estimators_
644             if tree.tree_.node_count > 1
645         )
646 
647         if not all_importances:
648             return np.zeros(self.n_features_in_, dtype=np.float64)
649 
650         all_importances = np.mean(all_importances, axis=0, dtype=np.float64)
651         return all_importances / np.sum(all_importances)
652 
653 
654 def _accumulate_prediction(predict, X, out, lock):
655     """
656     This is a utility function for joblib's Parallel.
657 
658     It can't go locally in ForestClassifier or ForestRegressor, because joblib
659     complains that it cannot pickle it when placed there.
660     """
661     prediction = predict(X, check_input=False)
662     with lock:
663         if len(out) == 1:
664             out[0] += prediction
665         else:
666             for i in range(len(out)):
667                 out[i] += prediction[i]
668 
669 
670 class ForestClassifier(ClassifierMixin, BaseForest, metaclass=ABCMeta):
671     """
672     Base class for forest of trees-based classifiers.
673 
674     Warning: This class should not be used directly. Use derived classes
675     instead.
676     """
677 
678     @abstractmethod
679     def __init__(
680         self,
681         estimator,
682         n_estimators=100,
683         *,
684         estimator_params=tuple(),
685         bootstrap=False,
686         oob_score=False,
687         n_jobs=None,
688         random_state=None,
689         verbose=0,
690         warm_start=False,
691         class_weight=None,
692         max_samples=None,
693         base_estimator="deprecated",
694     ):
695         super().__init__(
696             estimator=estimator,
697             n_estimators=n_estimators,
698             estimator_params=estimator_params,
699             bootstrap=bootstrap,
700             oob_score=oob_score,
701             n_jobs=n_jobs,
702             random_state=random_state,
703             verbose=verbose,
704             warm_start=warm_start,
705             class_weight=class_weight,
706             max_samples=max_samples,
707             base_estimator=base_estimator,
708         )
709 
710     @staticmethod
711     def _get_oob_predictions(tree, X):
712         """Compute the OOB predictions for an individual tree.
713 
714         Parameters
715         ----------
716         tree : DecisionTreeClassifier object
717             A single decision tree classifier.
718         X : ndarray of shape (n_samples, n_features)
719             The OOB samples.
720 
721         Returns
722         -------
723         y_pred : ndarray of shape (n_samples, n_classes, n_outputs)
724             The OOB associated predictions.
725         """
726         y_pred = tree.predict_proba(X, check_input=False)
727         y_pred = np.array(y_pred, copy=False)
728         if y_pred.ndim == 2:
729             # binary and multiclass
730             y_pred = y_pred[..., np.newaxis]
731         else:
732             # Roll the first `n_outputs` axis to the last axis. We will reshape
733             # from a shape of (n_outputs, n_samples, n_classes) to a shape of
734             # (n_samples, n_classes, n_outputs).
735             y_pred = np.rollaxis(y_pred, axis=0, start=3)
736         return y_pred
737 
738     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
739         """Compute and set the OOB score and attributes.
740 
741         Parameters
742         ----------
743         X : array-like of shape (n_samples, n_features)
744             The data matrix.
745         y : ndarray of shape (n_samples, n_outputs)
746             The target matrix.
747         scoring_function : callable, default=None
748             Scoring function for OOB score. Defaults to `accuracy_score`.
749         """
750         self.oob_decision_function_ = super()._compute_oob_predictions(X, y)
751         if self.oob_decision_function_.shape[-1] == 1:
752             # drop the n_outputs axis if there is a single output
753             self.oob_decision_function_ = self.oob_decision_function_.squeeze(axis=-1)
754 
755         if scoring_function is None:
756             scoring_function = accuracy_score
757 
758         self.oob_score_ = scoring_function(
759             y, np.argmax(self.oob_decision_function_, axis=1)
760         )
761 
762     def _validate_y_class_weight(self, y):
763         check_classification_targets(y)
764 
765         y = np.copy(y)
766         expanded_class_weight = None
767 
768         if self.class_weight is not None:
769             y_original = np.copy(y)
770 
771         self.classes_ = []
772         self.n_classes_ = []
773 
774         y_store_unique_indices = np.zeros(y.shape, dtype=int)
775         for k in range(self.n_outputs_):
776             classes_k, y_store_unique_indices[:, k] = np.unique(
777                 y[:, k], return_inverse=True
778             )
779             self.classes_.append(classes_k)
780             self.n_classes_.append(classes_k.shape[0])
781         y = y_store_unique_indices
782 
783         if self.class_weight is not None:
784             valid_presets = ("balanced", "balanced_subsample")
785             if isinstance(self.class_weight, str):
786                 if self.class_weight not in valid_presets:
787                     raise ValueError(
788                         "Valid presets for class_weight include "
789                         '"balanced" and "balanced_subsample".'
790                         'Given "%s".'
791                         % self.class_weight
792                     )
793                 if self.warm_start:
794                     warn(
795                         'class_weight presets "balanced" or '
796                         '"balanced_subsample" are '
797                         "not recommended for warm_start if the fitted data "
798                         "differs from the full dataset. In order to use "
799                         '"balanced" weights, use compute_class_weight '
800                         '("balanced", classes, y). In place of y you can use '
801                         "a large enough sample of the full training set "
802                         "target to properly estimate the class frequency "
803                         "distributions. Pass the resulting weights as the "
804                         "class_weight parameter."
805                     )
806 
807             if self.class_weight != "balanced_subsample" or not self.bootstrap:
808                 if self.class_weight == "balanced_subsample":
809                     class_weight = "balanced"
810                 else:
811                     class_weight = self.class_weight
812                 expanded_class_weight = compute_sample_weight(class_weight, y_original)
813 
814         return y, expanded_class_weight
815 
816     def predict(self, X):
817         """
818         Predict class for X.
819 
820         The predicted class of an input sample is a vote by the trees in
821         the forest, weighted by their probability estimates. That is,
822         the predicted class is the one with highest mean probability
823         estimate across the trees.
824 
825         Parameters
826         ----------
827         X : {array-like, sparse matrix} of shape (n_samples, n_features)
828             The input samples. Internally, its dtype will be converted to
829             ``dtype=np.float32``. If a sparse matrix is provided, it will be
830             converted into a sparse ``csr_matrix``.
831 
832         Returns
833         -------
834         y : ndarray of shape (n_samples,) or (n_samples, n_outputs)
835             The predicted classes.
836         """
837         proba = self.predict_proba(X)
838 
839         if self.n_outputs_ == 1:
840             return self.classes_.take(np.argmax(proba, axis=1), axis=0)
841 
842         else:
843             n_samples = proba[0].shape[0]
844             # all dtypes should be the same, so just take the first
845             class_type = self.classes_[0].dtype
846             predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)
847 
848             for k in range(self.n_outputs_):
849                 predictions[:, k] = self.classes_[k].take(
850                     np.argmax(proba[k], axis=1), axis=0
851                 )
852 
853             return predictions
854 
855     def predict_proba(self, X):
856         """
857         Predict class probabilities for X.
858 
859         The predicted class probabilities of an input sample are computed as
860         the mean predicted class probabilities of the trees in the forest.
861         The class probability of a single tree is the fraction of samples of
862         the same class in a leaf.
863 
864         Parameters
865         ----------
866         X : {array-like, sparse matrix} of shape (n_samples, n_features)
867             The input samples. Internally, its dtype will be converted to
868             ``dtype=np.float32``. If a sparse matrix is provided, it will be
869             converted into a sparse ``csr_matrix``.
870 
871         Returns
872         -------
873         p : ndarray of shape (n_samples, n_classes), or a list of such arrays
874             The class probabilities of the input samples. The order of the
875             classes corresponds to that in the attribute :term:`classes_`.
876         """
877         check_is_fitted(self)
878         # Check data
879         X = self._validate_X_predict(X)
880 
881         # Assign chunk of trees to jobs
882         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
883 
884         # avoid storing the output of every estimator by summing them here
885         all_proba = [
886             np.zeros((X.shape[0], j), dtype=np.float64)
887             for j in np.atleast_1d(self.n_classes_)
888         ]
889         lock = threading.Lock()
890         Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
891             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)
892             for e in self.estimators_
893         )
894 
895         for proba in all_proba:
896             proba /= len(self.estimators_)
897 
898         if len(all_proba) == 1:
899             return all_proba[0]
900         else:
901             return all_proba
902 
903     def predict_log_proba(self, X):
904         """
905         Predict class log-probabilities for X.
906 
907         The predicted class log-probabilities of an input sample is computed as
908         the log of the mean predicted class probabilities of the trees in the
909         forest.
910 
911         Parameters
912         ----------
913         X : {array-like, sparse matrix} of shape (n_samples, n_features)
914             The input samples. Internally, its dtype will be converted to
915             ``dtype=np.float32``. If a sparse matrix is provided, it will be
916             converted into a sparse ``csr_matrix``.
917 
918         Returns
919         -------
920         p : ndarray of shape (n_samples, n_classes), or a list of such arrays
921             The class probabilities of the input samples. The order of the
922             classes corresponds to that in the attribute :term:`classes_`.
923         """
924         proba = self.predict_proba(X)
925 
926         if self.n_outputs_ == 1:
927             return np.log(proba)
928 
929         else:
930             for k in range(self.n_outputs_):
931                 proba[k] = np.log(proba[k])
932 
933             return proba
934 
935     def _more_tags(self):
936         return {"multilabel": True}
937 
938 
939 class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):
940     """
941     Base class for forest of trees-based regressors.
942 
943     Warning: This class should not be used directly. Use derived classes
944     instead.
945     """
946 
947     @abstractmethod
948     def __init__(
949         self,
950         estimator,
951         n_estimators=100,
952         *,
953         estimator_params=tuple(),
954         bootstrap=False,
955         oob_score=False,
956         n_jobs=None,
957         random_state=None,
958         verbose=0,
959         warm_start=False,
960         max_samples=None,
961         base_estimator="deprecated",
962     ):
963         super().__init__(
964             estimator,
965             n_estimators=n_estimators,
966             estimator_params=estimator_params,
967             bootstrap=bootstrap,
968             oob_score=oob_score,
969             n_jobs=n_jobs,
970             random_state=random_state,
971             verbose=verbose,
972             warm_start=warm_start,
973             max_samples=max_samples,
974             base_estimator=base_estimator,
975         )
976 
977     def predict(self, X):
978         """
979         Predict regression target for X.
980 
981         The predicted regression target of an input sample is computed as the
982         mean predicted regression targets of the trees in the forest.
983 
984         Parameters
985         ----------
986         X : {array-like, sparse matrix} of shape (n_samples, n_features)
987             The input samples. Internally, its dtype will be converted to
988             ``dtype=np.float32``. If a sparse matrix is provided, it will be
989             converted into a sparse ``csr_matrix``.
990 
991         Returns
992         -------
993         y : ndarray of shape (n_samples,) or (n_samples, n_outputs)
994             The predicted values.
995         """
996         check_is_fitted(self)
997         # Check data
998         X = self._validate_X_predict(X)
999 
1000         # Assign chunk of trees to jobs
1001         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
1002 
1003         # avoid storing the output of every estimator by summing them here
1004         if self.n_outputs_ > 1:
1005             y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)
1006         else:
1007             y_hat = np.zeros((X.shape[0]), dtype=np.float64)
1008 
1009         # Parallel loop
1010         lock = threading.Lock()
1011         Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
1012             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)
1013             for e in self.estimators_
1014         )
1015 
1016         y_hat /= len(self.estimators_)
1017 
1018         return y_hat
1019 
1020     @staticmethod
1021     def _get_oob_predictions(tree, X):
1022         """Compute the OOB predictions for an individual tree.
1023 
1024         Parameters
1025         ----------
1026         tree : DecisionTreeRegressor object
1027             A single decision tree regressor.
1028         X : ndarray of shape (n_samples, n_features)
1029             The OOB samples.
1030 
1031         Returns
1032         -------
1033         y_pred : ndarray of shape (n_samples, 1, n_outputs)
1034             The OOB associated predictions.
1035         """
1036         y_pred = tree.predict(X, check_input=False)
1037         if y_pred.ndim == 1:
1038             # single output regression
1039             y_pred = y_pred[:, np.newaxis, np.newaxis]
1040         else:
1041             # multioutput regression
1042             y_pred = y_pred[:, np.newaxis, :]
1043         return y_pred
1044 
1045     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
1046         """Compute and set the OOB score and attributes.
1047 
1048         Parameters
1049         ----------
1050         X : array-like of shape (n_samples, n_features)
1051             The data matrix.
1052         y : ndarray of shape (n_samples, n_outputs)
1053             The target matrix.
1054         scoring_function : callable, default=None
1055             Scoring function for OOB score. Defaults to `r2_score`.
1056         """
1057         self.oob_prediction_ = super()._compute_oob_predictions(X, y).squeeze(axis=1)
1058         if self.oob_prediction_.shape[-1] == 1:
1059             # drop the n_outputs axis if there is a single output
1060             self.oob_prediction_ = self.oob_prediction_.squeeze(axis=-1)
1061 
1062         if scoring_function is None:
1063             scoring_function = r2_score
1064 
1065         self.oob_score_ = scoring_function(y, self.oob_prediction_)
1066 
1067     def _compute_partial_dependence_recursion(self, grid, target_features):
1068         """Fast partial dependence computation.
1069 
1070         Parameters
1071         ----------
1072         grid : ndarray of shape (n_samples, n_target_features)
1073             The grid points on which the partial dependence should be
1074             evaluated.
1075         target_features : ndarray of shape (n_target_features)
1076             The set of target features for which the partial dependence
1077             should be evaluated.
1078 
1079         Returns
1080         -------
1081         averaged_predictions : ndarray of shape (n_samples,)
1082             The value of the partial dependence function on each grid point.
1083         """
1084         grid = np.asarray(grid, dtype=DTYPE, order="C")
1085         averaged_predictions = np.zeros(
1086             shape=grid.shape[0], dtype=np.float64, order="C"
1087         )
1088 
1089         for tree in self.estimators_:
1090             # Note: we don't sum in parallel because the GIL isn't released in
1091             # the fast method.
1092             tree.tree_.compute_partial_dependence(
1093                 grid, target_features, averaged_predictions
1094             )
1095         # Average over the forest
1096         averaged_predictions /= len(self.estimators_)
1097 
1098         return averaged_predictions
1099 
1100     def _more_tags(self):
1101         return {"multilabel": True}
1102 
1103 
1104 class RandomForestClassifier(ForestClassifier):
1105     """
1106     A random forest classifier.
1107 
1108     A random forest is a meta estimator that fits a number of decision tree
1109     classifiers on various sub-samples of the dataset and uses averaging to
1110     improve the predictive accuracy and control over-fitting.
1111     The sub-sample size is controlled with the `max_samples` parameter if
1112     `bootstrap=True` (default), otherwise the whole dataset is used to build
1113     each tree.
1114 
1115     Read more in the :ref:`User Guide <forest>`.
1116 
1117     Parameters
1118     ----------
1119     n_estimators : int, default=100
1120         The number of trees in the forest.
1121 
1122         .. versionchanged:: 0.22
1123            The default value of ``n_estimators`` changed from 10 to 100
1124            in 0.22.
1125 
1126     criterion : {"gini", "entropy", "log_loss"}, default="gini"
1127         The function to measure the quality of a split. Supported criteria are
1128         "gini" for the Gini impurity and "log_loss" and "entropy" both for the
1129         Shannon information gain, see :ref:`tree_mathematical_formulation`.
1130         Note: This parameter is tree-specific.
1131 
1132     max_depth : int, default=None
1133         The maximum depth of the tree. If None, then nodes are expanded until
1134         all leaves are pure or until all leaves contain less than
1135         min_samples_split samples.
1136 
1137     min_samples_split : int or float, default=2
1138         The minimum number of samples required to split an internal node:
1139 
1140         - If int, then consider `min_samples_split` as the minimum number.
1141         - If float, then `min_samples_split` is a fraction and
1142           `ceil(min_samples_split * n_samples)` are the minimum
1143           number of samples for each split.
1144 
1145         .. versionchanged:: 0.18
1146            Added float values for fractions.
1147 
1148     min_samples_leaf : int or float, default=1
1149         The minimum number of samples required to be at a leaf node.
1150         A split point at any depth will only be considered if it leaves at
1151         least ``min_samples_leaf`` training samples in each of the left and
1152         right branches.  This may have the effect of smoothing the model,
1153         especially in regression.
1154 
1155         - If int, then consider `min_samples_leaf` as the minimum number.
1156         - If float, then `min_samples_leaf` is a fraction and
1157           `ceil(min_samples_leaf * n_samples)` are the minimum
1158           number of samples for each node.
1159 
1160         .. versionchanged:: 0.18
1161            Added float values for fractions.
1162 
1163     min_weight_fraction_leaf : float, default=0.0
1164         The minimum weighted fraction of the sum total of weights (of all
1165         the input samples) required to be at a leaf node. Samples have
1166         equal weight when sample_weight is not provided.
1167 
1168     max_features : {"sqrt", "log2", None}, int or float, default="sqrt"
1169         The number of features to consider when looking for the best split:
1170 
1171         - If int, then consider `max_features` features at each split.
1172         - If float, then `max_features` is a fraction and
1173           `max(1, int(max_features * n_features_in_))` features are considered at each
1174           split.
1175         - If "auto", then `max_features=sqrt(n_features)`.
1176         - If "sqrt", then `max_features=sqrt(n_features)`.
1177         - If "log2", then `max_features=log2(n_features)`.
1178         - If None, then `max_features=n_features`.
1179 
1180         .. versionchanged:: 1.1
1181             The default of `max_features` changed from `"auto"` to `"sqrt"`.
1182 
1183         .. deprecated:: 1.1
1184             The `"auto"` option was deprecated in 1.1 and will be removed
1185             in 1.3.
1186 
1187         Note: the search for a split does not stop until at least one
1188         valid partition of the node samples is found, even if it requires to
1189         effectively inspect more than ``max_features`` features.
1190 
1191     max_leaf_nodes : int, default=None
1192         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1193         Best nodes are defined as relative reduction in impurity.
1194         If None then unlimited number of leaf nodes.
1195 
1196     min_impurity_decrease : float, default=0.0
1197         A node will be split if this split induces a decrease of the impurity
1198         greater than or equal to this value.
1199 
1200         The weighted impurity decrease equation is the following::
1201 
1202             N_t / N * (impurity - N_t_R / N_t * right_impurity
1203                                 - N_t_L / N_t * left_impurity)
1204 
1205         where ``N`` is the total number of samples, ``N_t`` is the number of
1206         samples at the current node, ``N_t_L`` is the number of samples in the
1207         left child, and ``N_t_R`` is the number of samples in the right child.
1208 
1209         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1210         if ``sample_weight`` is passed.
1211 
1212         .. versionadded:: 0.19
1213 
1214     bootstrap : bool, default=True
1215         Whether bootstrap samples are used when building trees. If False, the
1216         whole dataset is used to build each tree.
1217 
1218     oob_score : bool or callable, default=False
1219         Whether to use out-of-bag samples to estimate the generalization score.
1220         By default, :func:`~sklearn.metrics.accuracy_score` is used.
1221         Provide a callable with signature `metric(y_true, y_pred)` to use a
1222         custom metric. Only available if `bootstrap=True`.
1223 
1224     n_jobs : int, default=None
1225         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
1226         :meth:`decision_path` and :meth:`apply` are all parallelized over the
1227         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
1228         context. ``-1`` means using all processors. See :term:`Glossary
1229         <n_jobs>` for more details.
1230 
1231     random_state : int, RandomState instance or None, default=None
1232         Controls both the randomness of the bootstrapping of the samples used
1233         when building trees (if ``bootstrap=True``) and the sampling of the
1234         features to consider when looking for the best split at each node
1235         (if ``max_features < n_features``).
1236         See :term:`Glossary <random_state>` for details.
1237 
1238     verbose : int, default=0
1239         Controls the verbosity when fitting and predicting.
1240 
1241     warm_start : bool, default=False
1242         When set to ``True``, reuse the solution of the previous call to fit
1243         and add more estimators to the ensemble, otherwise, just fit a whole
1244         new forest. See :term:`Glossary <warm_start>` and
1245         :ref:`gradient_boosting_warm_start` for details.
1246 
1247     class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts, \
1248             default=None
1249         Weights associated with classes in the form ``{class_label: weight}``.
1250         If not given, all classes are supposed to have weight one. For
1251         multi-output problems, a list of dicts can be provided in the same
1252         order as the columns of y.
1253 
1254         Note that for multioutput (including multilabel) weights should be
1255         defined for each class of every column in its own dict. For example,
1256         for four-class multilabel classification weights should be
1257         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
1258         [{1:1}, {2:5}, {3:1}, {4:1}].
1259 
1260         The "balanced" mode uses the values of y to automatically adjust
1261         weights inversely proportional to class frequencies in the input data
1262         as ``n_samples / (n_classes * np.bincount(y))``
1263 
1264         The "balanced_subsample" mode is the same as "balanced" except that
1265         weights are computed based on the bootstrap sample for every tree
1266         grown.
1267 
1268         For multi-output, the weights of each column of y will be multiplied.
1269 
1270         Note that these weights will be multiplied with sample_weight (passed
1271         through the fit method) if sample_weight is specified.
1272 
1273     ccp_alpha : non-negative float, default=0.0
1274         Complexity parameter used for Minimal Cost-Complexity Pruning. The
1275         subtree with the largest cost complexity that is smaller than
1276         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
1277         :ref:`minimal_cost_complexity_pruning` for details.
1278 
1279         .. versionadded:: 0.22
1280 
1281     max_samples : int or float, default=None
1282         If bootstrap is True, the number of samples to draw from X
1283         to train each base estimator.
1284 
1285         - If None (default), then draw `X.shape[0]` samples.
1286         - If int, then draw `max_samples` samples.
1287         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
1288           `max_samples` should be in the interval `(0.0, 1.0]`.
1289 
1290         .. versionadded:: 0.22
1291 
1292     Attributes
1293     ----------
1294     estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`
1295         The child estimator template used to create the collection of fitted
1296         sub-estimators.
1297 
1298         .. versionadded:: 1.2
1299            `base_estimator_` was renamed to `estimator_`.
1300 
1301     base_estimator_ : DecisionTreeClassifier
1302         The child estimator template used to create the collection of fitted
1303         sub-estimators.
1304 
1305         .. deprecated:: 1.2
1306             `base_estimator_` is deprecated and will be removed in 1.4.
1307             Use `estimator_` instead.
1308 
1309     estimators_ : list of DecisionTreeClassifier
1310         The collection of fitted sub-estimators.
1311 
1312     classes_ : ndarray of shape (n_classes,) or a list of such arrays
1313         The classes labels (single output problem), or a list of arrays of
1314         class labels (multi-output problem).
1315 
1316     n_classes_ : int or list
1317         The number of classes (single output problem), or a list containing the
1318         number of classes for each output (multi-output problem).
1319 
1320     n_features_in_ : int
1321         Number of features seen during :term:`fit`.
1322 
1323         .. versionadded:: 0.24
1324 
1325     feature_names_in_ : ndarray of shape (`n_features_in_`,)
1326         Names of features seen during :term:`fit`. Defined only when `X`
1327         has feature names that are all strings.
1328 
1329         .. versionadded:: 1.0
1330 
1331     n_outputs_ : int
1332         The number of outputs when ``fit`` is performed.
1333 
1334     feature_importances_ : ndarray of shape (n_features,)
1335         The impurity-based feature importances.
1336         The higher, the more important the feature.
1337         The importance of a feature is computed as the (normalized)
1338         total reduction of the criterion brought by that feature.  It is also
1339         known as the Gini importance.
1340 
1341         Warning: impurity-based feature importances can be misleading for
1342         high cardinality features (many unique values). See
1343         :func:`sklearn.inspection.permutation_importance` as an alternative.
1344 
1345     oob_score_ : float
1346         Score of the training dataset obtained using an out-of-bag estimate.
1347         This attribute exists only when ``oob_score`` is True.
1348 
1349     oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \
1350             (n_samples, n_classes, n_outputs)
1351         Decision function computed with out-of-bag estimate on the training
1352         set. If n_estimators is small it might be possible that a data point
1353         was never left out during the bootstrap. In this case,
1354         `oob_decision_function_` might contain NaN. This attribute exists
1355         only when ``oob_score`` is True.
1356 
1357     See Also
1358     --------
1359     sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
1360     sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized
1361         tree classifiers.
1362 
1363     Notes
1364     -----
1365     The default values for the parameters controlling the size of the trees
1366     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1367     unpruned trees which can potentially be very large on some data sets. To
1368     reduce memory consumption, the complexity and size of the trees should be
1369     controlled by setting those parameter values.
1370 
1371     The features are always randomly permuted at each split. Therefore,
1372     the best found split may vary, even with the same training data,
1373     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
1374     of the criterion is identical for several splits enumerated during the
1375     search of the best split. To obtain a deterministic behaviour during
1376     fitting, ``random_state`` has to be fixed.
1377 
1378     References
1379     ----------
1380     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
1381 
1382     Examples
1383     --------
1384     >>> from sklearn.ensemble import RandomForestClassifier
1385     >>> from sklearn.datasets import make_classification
1386     >>> X, y = make_classification(n_samples=1000, n_features=4,
1387     ...                            n_informative=2, n_redundant=0,
1388     ...                            random_state=0, shuffle=False)
1389     >>> clf = RandomForestClassifier(max_depth=2, random_state=0)
1390     >>> clf.fit(X, y)
1391     RandomForestClassifier(...)
1392     >>> print(clf.predict([[0, 0, 0, 0]]))
1393     [1]
1394     """
1395 
1396     _parameter_constraints: dict = {
1397         **ForestClassifier._parameter_constraints,
1398         **DecisionTreeClassifier._parameter_constraints,
1399         "class_weight": [
1400             StrOptions({"balanced_subsample", "balanced"}),
1401             dict,
1402             list,
1403             None,
1404         ],
1405     }
1406     _parameter_constraints.pop("splitter")
1407 
1408     def __init__(
1409         self,
1410         n_estimators=100,
1411         *,
1412         criterion="gini",
1413         max_depth=None,
1414         min_samples_split=2,
1415         min_samples_leaf=1,
1416         min_weight_fraction_leaf=0.0,
1417         max_features="sqrt",
1418         max_leaf_nodes=None,
1419         min_impurity_decrease=0.0,
1420         bootstrap=True,
1421         oob_score=False,
1422         n_jobs=None,
1423         random_state=None,
1424         verbose=0,
1425         warm_start=False,
1426         class_weight=None,
1427         ccp_alpha=0.0,
1428         max_samples=None,
1429     ):
1430         super().__init__(
1431             estimator=DecisionTreeClassifier(),
1432             n_estimators=n_estimators,
1433             estimator_params=(
1434                 "criterion",
1435                 "max_depth",
1436                 "min_samples_split",
1437                 "min_samples_leaf",
1438                 "min_weight_fraction_leaf",
1439                 "max_features",
1440                 "max_leaf_nodes",
1441                 "min_impurity_decrease",
1442                 "random_state",
1443                 "ccp_alpha",
1444             ),
1445             bootstrap=bootstrap,
1446             oob_score=oob_score,
1447             n_jobs=n_jobs,
1448             random_state=random_state,
1449             verbose=verbose,
1450             warm_start=warm_start,
1451             class_weight=class_weight,
1452             max_samples=max_samples,
1453         )
1454 
1455         self.criterion = criterion
1456         self.max_depth = max_depth
1457         self.min_samples_split = min_samples_split
1458         self.min_samples_leaf = min_samples_leaf
1459         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1460         self.max_features = max_features
1461         self.max_leaf_nodes = max_leaf_nodes
1462         self.min_impurity_decrease = min_impurity_decrease
1463         self.ccp_alpha = ccp_alpha
1464 
1465 
1466 class RandomForestRegressor(ForestRegressor):
1467     """
1468     A random forest regressor.
1469 
1470     A random forest is a meta estimator that fits a number of classifying
1471     decision trees on various sub-samples of the dataset and uses averaging
1472     to improve the predictive accuracy and control over-fitting.
1473     The sub-sample size is controlled with the `max_samples` parameter if
1474     `bootstrap=True` (default), otherwise the whole dataset is used to build
1475     each tree.
1476 
1477     Read more in the :ref:`User Guide <forest>`.
1478 
1479     Parameters
1480     ----------
1481     n_estimators : int, default=100
1482         The number of trees in the forest.
1483 
1484         .. versionchanged:: 0.22
1485            The default value of ``n_estimators`` changed from 10 to 100
1486            in 0.22.
1487 
1488     criterion : {"squared_error", "absolute_error", "friedman_mse", "poisson"}, \
1489             default="squared_error"
1490         The function to measure the quality of a split. Supported criteria
1491         are "squared_error" for the mean squared error, which is equal to
1492         variance reduction as feature selection criterion and minimizes the L2
1493         loss using the mean of each terminal node, "friedman_mse", which uses
1494         mean squared error with Friedman's improvement score for potential
1495         splits, "absolute_error" for the mean absolute error, which minimizes
1496         the L1 loss using the median of each terminal node, and "poisson" which
1497         uses reduction in Poisson deviance to find splits.
1498         Training using "absolute_error" is significantly slower
1499         than when using "squared_error".
1500 
1501         .. versionadded:: 0.18
1502            Mean Absolute Error (MAE) criterion.
1503 
1504         .. versionadded:: 1.0
1505            Poisson criterion.
1506 
1507     max_depth : int, default=None
1508         The maximum depth of the tree. If None, then nodes are expanded until
1509         all leaves are pure or until all leaves contain less than
1510         min_samples_split samples.
1511 
1512     min_samples_split : int or float, default=2
1513         The minimum number of samples required to split an internal node:
1514 
1515         - If int, then consider `min_samples_split` as the minimum number.
1516         - If float, then `min_samples_split` is a fraction and
1517           `ceil(min_samples_split * n_samples)` are the minimum
1518           number of samples for each split.
1519 
1520         .. versionchanged:: 0.18
1521            Added float values for fractions.
1522 
1523     min_samples_leaf : int or float, default=1
1524         The minimum number of samples required to be at a leaf node.
1525         A split point at any depth will only be considered if it leaves at
1526         least ``min_samples_leaf`` training samples in each of the left and
1527         right branches.  This may have the effect of smoothing the model,
1528         especially in regression.
1529 
1530         - If int, then consider `min_samples_leaf` as the minimum number.
1531         - If float, then `min_samples_leaf` is a fraction and
1532           `ceil(min_samples_leaf * n_samples)` are the minimum
1533           number of samples for each node.
1534 
1535         .. versionchanged:: 0.18
1536            Added float values for fractions.
1537 
1538     min_weight_fraction_leaf : float, default=0.0
1539         The minimum weighted fraction of the sum total of weights (of all
1540         the input samples) required to be at a leaf node. Samples have
1541         equal weight when sample_weight is not provided.
1542 
1543     max_features : {"sqrt", "log2", None}, int or float, default=1.0
1544         The number of features to consider when looking for the best split:
1545 
1546         - If int, then consider `max_features` features at each split.
1547         - If float, then `max_features` is a fraction and
1548           `max(1, int(max_features * n_features_in_))` features are considered at each
1549           split.
1550         - If "auto", then `max_features=n_features`.
1551         - If "sqrt", then `max_features=sqrt(n_features)`.
1552         - If "log2", then `max_features=log2(n_features)`.
1553         - If None or 1.0, then `max_features=n_features`.
1554 
1555         .. note::
1556             The default of 1.0 is equivalent to bagged trees and more
1557             randomness can be achieved by setting smaller values, e.g. 0.3.
1558 
1559         .. versionchanged:: 1.1
1560             The default of `max_features` changed from `"auto"` to 1.0.
1561 
1562         .. deprecated:: 1.1
1563             The `"auto"` option was deprecated in 1.1 and will be removed
1564             in 1.3.
1565 
1566         Note: the search for a split does not stop until at least one
1567         valid partition of the node samples is found, even if it requires to
1568         effectively inspect more than ``max_features`` features.
1569 
1570     max_leaf_nodes : int, default=None
1571         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1572         Best nodes are defined as relative reduction in impurity.
1573         If None then unlimited number of leaf nodes.
1574 
1575     min_impurity_decrease : float, default=0.0
1576         A node will be split if this split induces a decrease of the impurity
1577         greater than or equal to this value.
1578 
1579         The weighted impurity decrease equation is the following::
1580 
1581             N_t / N * (impurity - N_t_R / N_t * right_impurity
1582                                 - N_t_L / N_t * left_impurity)
1583 
1584         where ``N`` is the total number of samples, ``N_t`` is the number of
1585         samples at the current node, ``N_t_L`` is the number of samples in the
1586         left child, and ``N_t_R`` is the number of samples in the right child.
1587 
1588         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1589         if ``sample_weight`` is passed.
1590 
1591         .. versionadded:: 0.19
1592 
1593     bootstrap : bool, default=True
1594         Whether bootstrap samples are used when building trees. If False, the
1595         whole dataset is used to build each tree.
1596 
1597     oob_score : bool or callable, default=False
1598         Whether to use out-of-bag samples to estimate the generalization score.
1599         By default, :func:`~sklearn.metrics.r2_score` is used.
1600         Provide a callable with signature `metric(y_true, y_pred)` to use a
1601         custom metric. Only available if `bootstrap=True`.
1602 
1603     n_jobs : int, default=None
1604         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
1605         :meth:`decision_path` and :meth:`apply` are all parallelized over the
1606         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
1607         context. ``-1`` means using all processors. See :term:`Glossary
1608         <n_jobs>` for more details.
1609 
1610     random_state : int, RandomState instance or None, default=None
1611         Controls both the randomness of the bootstrapping of the samples used
1612         when building trees (if ``bootstrap=True``) and the sampling of the
1613         features to consider when looking for the best split at each node
1614         (if ``max_features < n_features``).
1615         See :term:`Glossary <random_state>` for details.
1616 
1617     verbose : int, default=0
1618         Controls the verbosity when fitting and predicting.
1619 
1620     warm_start : bool, default=False
1621         When set to ``True``, reuse the solution of the previous call to fit
1622         and add more estimators to the ensemble, otherwise, just fit a whole
1623         new forest. See :term:`Glossary <warm_start>` and
1624         :ref:`gradient_boosting_warm_start` for details.
1625 
1626     ccp_alpha : non-negative float, default=0.0
1627         Complexity parameter used for Minimal Cost-Complexity Pruning. The
1628         subtree with the largest cost complexity that is smaller than
1629         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
1630         :ref:`minimal_cost_complexity_pruning` for details.
1631 
1632         .. versionadded:: 0.22
1633 
1634     max_samples : int or float, default=None
1635         If bootstrap is True, the number of samples to draw from X
1636         to train each base estimator.
1637 
1638         - If None (default), then draw `X.shape[0]` samples.
1639         - If int, then draw `max_samples` samples.
1640         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
1641           `max_samples` should be in the interval `(0.0, 1.0]`.
1642 
1643         .. versionadded:: 0.22
1644 
1645     Attributes
1646     ----------
1647     estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`
1648         The child estimator template used to create the collection of fitted
1649         sub-estimators.
1650 
1651         .. versionadded:: 1.2
1652            `base_estimator_` was renamed to `estimator_`.
1653 
1654     base_estimator_ : DecisionTreeRegressor
1655         The child estimator template used to create the collection of fitted
1656         sub-estimators.
1657 
1658         .. deprecated:: 1.2
1659             `base_estimator_` is deprecated and will be removed in 1.4.
1660             Use `estimator_` instead.
1661 
1662     estimators_ : list of DecisionTreeRegressor
1663         The collection of fitted sub-estimators.
1664 
1665     feature_importances_ : ndarray of shape (n_features,)
1666         The impurity-based feature importances.
1667         The higher, the more important the feature.
1668         The importance of a feature is computed as the (normalized)
1669         total reduction of the criterion brought by that feature.  It is also
1670         known as the Gini importance.
1671 
1672         Warning: impurity-based feature importances can be misleading for
1673         high cardinality features (many unique values). See
1674         :func:`sklearn.inspection.permutation_importance` as an alternative.
1675 
1676     n_features_in_ : int
1677         Number of features seen during :term:`fit`.
1678 
1679         .. versionadded:: 0.24
1680 
1681     feature_names_in_ : ndarray of shape (`n_features_in_`,)
1682         Names of features seen during :term:`fit`. Defined only when `X`
1683         has feature names that are all strings.
1684 
1685         .. versionadded:: 1.0
1686 
1687     n_outputs_ : int
1688         The number of outputs when ``fit`` is performed.
1689 
1690     oob_score_ : float
1691         Score of the training dataset obtained using an out-of-bag estimate.
1692         This attribute exists only when ``oob_score`` is True.
1693 
1694     oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
1695         Prediction computed with out-of-bag estimate on the training set.
1696         This attribute exists only when ``oob_score`` is True.
1697 
1698     See Also
1699     --------
1700     sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
1701     sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized
1702         tree regressors.
1703 
1704     Notes
1705     -----
1706     The default values for the parameters controlling the size of the trees
1707     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1708     unpruned trees which can potentially be very large on some data sets. To
1709     reduce memory consumption, the complexity and size of the trees should be
1710     controlled by setting those parameter values.
1711 
1712     The features are always randomly permuted at each split. Therefore,
1713     the best found split may vary, even with the same training data,
1714     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
1715     of the criterion is identical for several splits enumerated during the
1716     search of the best split. To obtain a deterministic behaviour during
1717     fitting, ``random_state`` has to be fixed.
1718 
1719     The default value ``max_features="auto"`` uses ``n_features``
1720     rather than ``n_features / 3``. The latter was originally suggested in
1721     [1], whereas the former was more recently justified empirically in [2].
1722 
1723     References
1724     ----------
1725     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
1726 
1727     .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
1728            trees", Machine Learning, 63(1), 3-42, 2006.
1729 
1730     Examples
1731     --------
1732     >>> from sklearn.ensemble import RandomForestRegressor
1733     >>> from sklearn.datasets import make_regression
1734     >>> X, y = make_regression(n_features=4, n_informative=2,
1735     ...                        random_state=0, shuffle=False)
1736     >>> regr = RandomForestRegressor(max_depth=2, random_state=0)
1737     >>> regr.fit(X, y)
1738     RandomForestRegressor(...)
1739     >>> print(regr.predict([[0, 0, 0, 0]]))
1740     [-8.32987858]
1741     """
1742 
1743     _parameter_constraints: dict = {
1744         **ForestRegressor._parameter_constraints,
1745         **DecisionTreeRegressor._parameter_constraints,
1746     }
1747     _parameter_constraints.pop("splitter")
1748 
1749     def __init__(
1750         self,
1751         n_estimators=100,
1752         *,
1753         criterion="squared_error",
1754         max_depth=None,
1755         min_samples_split=2,
1756         min_samples_leaf=1,
1757         min_weight_fraction_leaf=0.0,
1758         max_features=1.0,
1759         max_leaf_nodes=None,
1760         min_impurity_decrease=0.0,
1761         bootstrap=True,
1762         oob_score=False,
1763         n_jobs=None,
1764         random_state=None,
1765         verbose=0,
1766         warm_start=False,
1767         ccp_alpha=0.0,
1768         max_samples=None,
1769     ):
1770         super().__init__(
1771             estimator=DecisionTreeRegressor(),
1772             n_estimators=n_estimators,
1773             estimator_params=(
1774                 "criterion",
1775                 "max_depth",
1776                 "min_samples_split",
1777                 "min_samples_leaf",
1778                 "min_weight_fraction_leaf",
1779                 "max_features",
1780                 "max_leaf_nodes",
1781                 "min_impurity_decrease",
1782                 "random_state",
1783                 "ccp_alpha",
1784             ),
1785             bootstrap=bootstrap,
1786             oob_score=oob_score,
1787             n_jobs=n_jobs,
1788             random_state=random_state,
1789             verbose=verbose,
1790             warm_start=warm_start,
1791             max_samples=max_samples,
1792         )
1793 
1794         self.criterion = criterion
1795         self.max_depth = max_depth
1796         self.min_samples_split = min_samples_split
1797         self.min_samples_leaf = min_samples_leaf
1798         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1799         self.max_features = max_features
1800         self.max_leaf_nodes = max_leaf_nodes
1801         self.min_impurity_decrease = min_impurity_decrease
1802         self.ccp_alpha = ccp_alpha
1803 
1804 
1805 class ExtraTreesClassifier(ForestClassifier):
1806     """
1807     An extra-trees classifier.
1808 
1809     This class implements a meta estimator that fits a number of
1810     randomized decision trees (a.k.a. extra-trees) on various sub-samples
1811     of the dataset and uses averaging to improve the predictive accuracy
1812     and control over-fitting.
1813 
1814     Read more in the :ref:`User Guide <forest>`.
1815 
1816     Parameters
1817     ----------
1818     n_estimators : int, default=100
1819         The number of trees in the forest.
1820 
1821         .. versionchanged:: 0.22
1822            The default value of ``n_estimators`` changed from 10 to 100
1823            in 0.22.
1824 
1825     criterion : {"gini", "entropy", "log_loss"}, default="gini"
1826         The function to measure the quality of a split. Supported criteria are
1827         "gini" for the Gini impurity and "log_loss" and "entropy" both for the
1828         Shannon information gain, see :ref:`tree_mathematical_formulation`.
1829         Note: This parameter is tree-specific.
1830 
1831     max_depth : int, default=None
1832         The maximum depth of the tree. If None, then nodes are expanded until
1833         all leaves are pure or until all leaves contain less than
1834         min_samples_split samples.
1835 
1836     min_samples_split : int or float, default=2
1837         The minimum number of samples required to split an internal node:
1838 
1839         - If int, then consider `min_samples_split` as the minimum number.
1840         - If float, then `min_samples_split` is a fraction and
1841           `ceil(min_samples_split * n_samples)` are the minimum
1842           number of samples for each split.
1843 
1844         .. versionchanged:: 0.18
1845            Added float values for fractions.
1846 
1847     min_samples_leaf : int or float, default=1
1848         The minimum number of samples required to be at a leaf node.
1849         A split point at any depth will only be considered if it leaves at
1850         least ``min_samples_leaf`` training samples in each of the left and
1851         right branches.  This may have the effect of smoothing the model,
1852         especially in regression.
1853 
1854         - If int, then consider `min_samples_leaf` as the minimum number.
1855         - If float, then `min_samples_leaf` is a fraction and
1856           `ceil(min_samples_leaf * n_samples)` are the minimum
1857           number of samples for each node.
1858 
1859         .. versionchanged:: 0.18
1860            Added float values for fractions.
1861 
1862     min_weight_fraction_leaf : float, default=0.0
1863         The minimum weighted fraction of the sum total of weights (of all
1864         the input samples) required to be at a leaf node. Samples have
1865         equal weight when sample_weight is not provided.
1866 
1867     max_features : {"sqrt", "log2", None}, int or float, default="sqrt"
1868         The number of features to consider when looking for the best split:
1869 
1870         - If int, then consider `max_features` features at each split.
1871         - If float, then `max_features` is a fraction and
1872           `max(1, int(max_features * n_features_in_))` features are considered at each
1873           split.
1874         - If "auto", then `max_features=sqrt(n_features)`.
1875         - If "sqrt", then `max_features=sqrt(n_features)`.
1876         - If "log2", then `max_features=log2(n_features)`.
1877         - If None, then `max_features=n_features`.
1878 
1879         .. versionchanged:: 1.1
1880             The default of `max_features` changed from `"auto"` to `"sqrt"`.
1881 
1882         .. deprecated:: 1.1
1883             The `"auto"` option was deprecated in 1.1 and will be removed
1884             in 1.3.
1885 
1886         Note: the search for a split does not stop until at least one
1887         valid partition of the node samples is found, even if it requires to
1888         effectively inspect more than ``max_features`` features.
1889 
1890     max_leaf_nodes : int, default=None
1891         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1892         Best nodes are defined as relative reduction in impurity.
1893         If None then unlimited number of leaf nodes.
1894 
1895     min_impurity_decrease : float, default=0.0
1896         A node will be split if this split induces a decrease of the impurity
1897         greater than or equal to this value.
1898 
1899         The weighted impurity decrease equation is the following::
1900 
1901             N_t / N * (impurity - N_t_R / N_t * right_impurity
1902                                 - N_t_L / N_t * left_impurity)
1903 
1904         where ``N`` is the total number of samples, ``N_t`` is the number of
1905         samples at the current node, ``N_t_L`` is the number of samples in the
1906         left child, and ``N_t_R`` is the number of samples in the right child.
1907 
1908         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1909         if ``sample_weight`` is passed.
1910 
1911         .. versionadded:: 0.19
1912 
1913     bootstrap : bool, default=False
1914         Whether bootstrap samples are used when building trees. If False, the
1915         whole dataset is used to build each tree.
1916 
1917     oob_score : bool or callable, default=False
1918         Whether to use out-of-bag samples to estimate the generalization score.
1919         By default, :func:`~sklearn.metrics.accuracy_score` is used.
1920         Provide a callable with signature `metric(y_true, y_pred)` to use a
1921         custom metric. Only available if `bootstrap=True`.
1922 
1923     n_jobs : int, default=None
1924         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
1925         :meth:`decision_path` and :meth:`apply` are all parallelized over the
1926         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
1927         context. ``-1`` means using all processors. See :term:`Glossary
1928         <n_jobs>` for more details.
1929 
1930     random_state : int, RandomState instance or None, default=None
1931         Controls 3 sources of randomness:
1932 
1933         - the bootstrapping of the samples used when building trees
1934           (if ``bootstrap=True``)
1935         - the sampling of the features to consider when looking for the best
1936           split at each node (if ``max_features < n_features``)
1937         - the draw of the splits for each of the `max_features`
1938 
1939         See :term:`Glossary <random_state>` for details.
1940 
1941     verbose : int, default=0
1942         Controls the verbosity when fitting and predicting.
1943 
1944     warm_start : bool, default=False
1945         When set to ``True``, reuse the solution of the previous call to fit
1946         and add more estimators to the ensemble, otherwise, just fit a whole
1947         new forest. See :term:`Glossary <warm_start>` and
1948         :ref:`gradient_boosting_warm_start` for details.
1949 
1950     class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts, \
1951             default=None
1952         Weights associated with classes in the form ``{class_label: weight}``.
1953         If not given, all classes are supposed to have weight one. For
1954         multi-output problems, a list of dicts can be provided in the same
1955         order as the columns of y.
1956 
1957         Note that for multioutput (including multilabel) weights should be
1958         defined for each class of every column in its own dict. For example,
1959         for four-class multilabel classification weights should be
1960         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
1961         [{1:1}, {2:5}, {3:1}, {4:1}].
1962 
1963         The "balanced" mode uses the values of y to automatically adjust
1964         weights inversely proportional to class frequencies in the input data
1965         as ``n_samples / (n_classes * np.bincount(y))``
1966 
1967         The "balanced_subsample" mode is the same as "balanced" except that
1968         weights are computed based on the bootstrap sample for every tree
1969         grown.
1970 
1971         For multi-output, the weights of each column of y will be multiplied.
1972 
1973         Note that these weights will be multiplied with sample_weight (passed
1974         through the fit method) if sample_weight is specified.
1975 
1976     ccp_alpha : non-negative float, default=0.0
1977         Complexity parameter used for Minimal Cost-Complexity Pruning. The
1978         subtree with the largest cost complexity that is smaller than
1979         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
1980         :ref:`minimal_cost_complexity_pruning` for details.
1981 
1982         .. versionadded:: 0.22
1983 
1984     max_samples : int or float, default=None
1985         If bootstrap is True, the number of samples to draw from X
1986         to train each base estimator.
1987 
1988         - If None (default), then draw `X.shape[0]` samples.
1989         - If int, then draw `max_samples` samples.
1990         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
1991           `max_samples` should be in the interval `(0.0, 1.0]`.
1992 
1993         .. versionadded:: 0.22
1994 
1995     Attributes
1996     ----------
1997     estimator_ : :class:`~sklearn.tree.ExtraTreesClassifier`
1998         The child estimator template used to create the collection of fitted
1999         sub-estimators.
2000 
2001         .. versionadded:: 1.2
2002            `base_estimator_` was renamed to `estimator_`.
2003 
2004     base_estimator_ : ExtraTreesClassifier
2005         The child estimator template used to create the collection of fitted
2006         sub-estimators.
2007 
2008         .. deprecated:: 1.2
2009             `base_estimator_` is deprecated and will be removed in 1.4.
2010             Use `estimator_` instead.
2011 
2012     estimators_ : list of DecisionTreeClassifier
2013         The collection of fitted sub-estimators.
2014 
2015     classes_ : ndarray of shape (n_classes,) or a list of such arrays
2016         The classes labels (single output problem), or a list of arrays of
2017         class labels (multi-output problem).
2018 
2019     n_classes_ : int or list
2020         The number of classes (single output problem), or a list containing the
2021         number of classes for each output (multi-output problem).
2022 
2023     feature_importances_ : ndarray of shape (n_features,)
2024         The impurity-based feature importances.
2025         The higher, the more important the feature.
2026         The importance of a feature is computed as the (normalized)
2027         total reduction of the criterion brought by that feature.  It is also
2028         known as the Gini importance.
2029 
2030         Warning: impurity-based feature importances can be misleading for
2031         high cardinality features (many unique values). See
2032         :func:`sklearn.inspection.permutation_importance` as an alternative.
2033 
2034     n_features_in_ : int
2035         Number of features seen during :term:`fit`.
2036 
2037         .. versionadded:: 0.24
2038 
2039     feature_names_in_ : ndarray of shape (`n_features_in_`,)
2040         Names of features seen during :term:`fit`. Defined only when `X`
2041         has feature names that are all strings.
2042 
2043         .. versionadded:: 1.0
2044 
2045     n_outputs_ : int
2046         The number of outputs when ``fit`` is performed.
2047 
2048     oob_score_ : float
2049         Score of the training dataset obtained using an out-of-bag estimate.
2050         This attribute exists only when ``oob_score`` is True.
2051 
2052     oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \
2053             (n_samples, n_classes, n_outputs)
2054         Decision function computed with out-of-bag estimate on the training
2055         set. If n_estimators is small it might be possible that a data point
2056         was never left out during the bootstrap. In this case,
2057         `oob_decision_function_` might contain NaN. This attribute exists
2058         only when ``oob_score`` is True.
2059 
2060     See Also
2061     --------
2062     ExtraTreesRegressor : An extra-trees regressor with random splits.
2063     RandomForestClassifier : A random forest classifier with optimal splits.
2064     RandomForestRegressor : Ensemble regressor using trees with optimal splits.
2065 
2066     Notes
2067     -----
2068     The default values for the parameters controlling the size of the trees
2069     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
2070     unpruned trees which can potentially be very large on some data sets. To
2071     reduce memory consumption, the complexity and size of the trees should be
2072     controlled by setting those parameter values.
2073 
2074     References
2075     ----------
2076     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
2077            trees", Machine Learning, 63(1), 3-42, 2006.
2078 
2079     Examples
2080     --------
2081     >>> from sklearn.ensemble import ExtraTreesClassifier
2082     >>> from sklearn.datasets import make_classification
2083     >>> X, y = make_classification(n_features=4, random_state=0)
2084     >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)
2085     >>> clf.fit(X, y)
2086     ExtraTreesClassifier(random_state=0)
2087     >>> clf.predict([[0, 0, 0, 0]])
2088     array([1])
2089     """
2090 
2091     _parameter_constraints: dict = {
2092         **ForestClassifier._parameter_constraints,
2093         **DecisionTreeClassifier._parameter_constraints,
2094         "class_weight": [
2095             StrOptions({"balanced_subsample", "balanced"}),
2096             dict,
2097             list,
2098             None,
2099         ],
2100     }
2101     _parameter_constraints.pop("splitter")
2102 
2103     def __init__(
2104         self,
2105         n_estimators=100,
2106         *,
2107         criterion="gini",
2108         max_depth=None,
2109         min_samples_split=2,
2110         min_samples_leaf=1,
2111         min_weight_fraction_leaf=0.0,
2112         max_features="sqrt",
2113         max_leaf_nodes=None,
2114         min_impurity_decrease=0.0,
2115         bootstrap=False,
2116         oob_score=False,
2117         n_jobs=None,
2118         random_state=None,
2119         verbose=0,
2120         warm_start=False,
2121         class_weight=None,
2122         ccp_alpha=0.0,
2123         max_samples=None,
2124     ):
2125         super().__init__(
2126             estimator=ExtraTreeClassifier(),
2127             n_estimators=n_estimators,
2128             estimator_params=(
2129                 "criterion",
2130                 "max_depth",
2131                 "min_samples_split",
2132                 "min_samples_leaf",
2133                 "min_weight_fraction_leaf",
2134                 "max_features",
2135                 "max_leaf_nodes",
2136                 "min_impurity_decrease",
2137                 "random_state",
2138                 "ccp_alpha",
2139             ),
2140             bootstrap=bootstrap,
2141             oob_score=oob_score,
2142             n_jobs=n_jobs,
2143             random_state=random_state,
2144             verbose=verbose,
2145             warm_start=warm_start,
2146             class_weight=class_weight,
2147             max_samples=max_samples,
2148         )
2149 
2150         self.criterion = criterion
2151         self.max_depth = max_depth
2152         self.min_samples_split = min_samples_split
2153         self.min_samples_leaf = min_samples_leaf
2154         self.min_weight_fraction_leaf = min_weight_fraction_leaf
2155         self.max_features = max_features
2156         self.max_leaf_nodes = max_leaf_nodes
2157         self.min_impurity_decrease = min_impurity_decrease
2158         self.ccp_alpha = ccp_alpha
2159 
2160 
2161 class ExtraTreesRegressor(ForestRegressor):
2162     """
2163     An extra-trees regressor.
2164 
2165     This class implements a meta estimator that fits a number of
2166     randomized decision trees (a.k.a. extra-trees) on various sub-samples
2167     of the dataset and uses averaging to improve the predictive accuracy
2168     and control over-fitting.
2169 
2170     Read more in the :ref:`User Guide <forest>`.
2171 
2172     Parameters
2173     ----------
2174     n_estimators : int, default=100
2175         The number of trees in the forest.
2176 
2177         .. versionchanged:: 0.22
2178            The default value of ``n_estimators`` changed from 10 to 100
2179            in 0.22.
2180 
2181     criterion : {"squared_error", "absolute_error", "friedman_mse", "poisson"}, \
2182             default="squared_error"
2183         The function to measure the quality of a split. Supported criteria
2184         are "squared_error" for the mean squared error, which is equal to
2185         variance reduction as feature selection criterion and minimizes the L2
2186         loss using the mean of each terminal node, "friedman_mse", which uses
2187         mean squared error with Friedman's improvement score for potential
2188         splits, "absolute_error" for the mean absolute error, which minimizes
2189         the L1 loss using the median of each terminal node, and "poisson" which
2190         uses reduction in Poisson deviance to find splits.
2191         Training using "absolute_error" is significantly slower
2192         than when using "squared_error".
2193 
2194         .. versionadded:: 0.18
2195            Mean Absolute Error (MAE) criterion.
2196 
2197     max_depth : int, default=None
2198         The maximum depth of the tree. If None, then nodes are expanded until
2199         all leaves are pure or until all leaves contain less than
2200         min_samples_split samples.
2201 
2202     min_samples_split : int or float, default=2
2203         The minimum number of samples required to split an internal node:
2204 
2205         - If int, then consider `min_samples_split` as the minimum number.
2206         - If float, then `min_samples_split` is a fraction and
2207           `ceil(min_samples_split * n_samples)` are the minimum
2208           number of samples for each split.
2209 
2210         .. versionchanged:: 0.18
2211            Added float values for fractions.
2212 
2213     min_samples_leaf : int or float, default=1
2214         The minimum number of samples required to be at a leaf node.
2215         A split point at any depth will only be considered if it leaves at
2216         least ``min_samples_leaf`` training samples in each of the left and
2217         right branches.  This may have the effect of smoothing the model,
2218         especially in regression.
2219 
2220         - If int, then consider `min_samples_leaf` as the minimum number.
2221         - If float, then `min_samples_leaf` is a fraction and
2222           `ceil(min_samples_leaf * n_samples)` are the minimum
2223           number of samples for each node.
2224 
2225         .. versionchanged:: 0.18
2226            Added float values for fractions.
2227 
2228     min_weight_fraction_leaf : float, default=0.0
2229         The minimum weighted fraction of the sum total of weights (of all
2230         the input samples) required to be at a leaf node. Samples have
2231         equal weight when sample_weight is not provided.
2232 
2233     max_features : {"sqrt", "log2", None}, int or float, default=1.0
2234         The number of features to consider when looking for the best split:
2235 
2236         - If int, then consider `max_features` features at each split.
2237         - If float, then `max_features` is a fraction and
2238           `max(1, int(max_features * n_features_in_))` features are considered at each
2239           split.
2240         - If "auto", then `max_features=n_features`.
2241         - If "sqrt", then `max_features=sqrt(n_features)`.
2242         - If "log2", then `max_features=log2(n_features)`.
2243         - If None or 1.0, then `max_features=n_features`.
2244 
2245         .. note::
2246             The default of 1.0 is equivalent to bagged trees and more
2247             randomness can be achieved by setting smaller values, e.g. 0.3.
2248 
2249         .. versionchanged:: 1.1
2250             The default of `max_features` changed from `"auto"` to 1.0.
2251 
2252         .. deprecated:: 1.1
2253             The `"auto"` option was deprecated in 1.1 and will be removed
2254             in 1.3.
2255 
2256         Note: the search for a split does not stop until at least one
2257         valid partition of the node samples is found, even if it requires to
2258         effectively inspect more than ``max_features`` features.
2259 
2260     max_leaf_nodes : int, default=None
2261         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2262         Best nodes are defined as relative reduction in impurity.
2263         If None then unlimited number of leaf nodes.
2264 
2265     min_impurity_decrease : float, default=0.0
2266         A node will be split if this split induces a decrease of the impurity
2267         greater than or equal to this value.
2268 
2269         The weighted impurity decrease equation is the following::
2270 
2271             N_t / N * (impurity - N_t_R / N_t * right_impurity
2272                                 - N_t_L / N_t * left_impurity)
2273 
2274         where ``N`` is the total number of samples, ``N_t`` is the number of
2275         samples at the current node, ``N_t_L`` is the number of samples in the
2276         left child, and ``N_t_R`` is the number of samples in the right child.
2277 
2278         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2279         if ``sample_weight`` is passed.
2280 
2281         .. versionadded:: 0.19
2282 
2283     bootstrap : bool, default=False
2284         Whether bootstrap samples are used when building trees. If False, the
2285         whole dataset is used to build each tree.
2286 
2287     oob_score : bool or callable, default=False
2288         Whether to use out-of-bag samples to estimate the generalization score.
2289         By default, :func:`~sklearn.metrics.r2_score` is used.
2290         Provide a callable with signature `metric(y_true, y_pred)` to use a
2291         custom metric. Only available if `bootstrap=True`.
2292 
2293     n_jobs : int, default=None
2294         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
2295         :meth:`decision_path` and :meth:`apply` are all parallelized over the
2296         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
2297         context. ``-1`` means using all processors. See :term:`Glossary
2298         <n_jobs>` for more details.
2299 
2300     random_state : int, RandomState instance or None, default=None
2301         Controls 3 sources of randomness:
2302 
2303         - the bootstrapping of the samples used when building trees
2304           (if ``bootstrap=True``)
2305         - the sampling of the features to consider when looking for the best
2306           split at each node (if ``max_features < n_features``)
2307         - the draw of the splits for each of the `max_features`
2308 
2309         See :term:`Glossary <random_state>` for details.
2310 
2311     verbose : int, default=0
2312         Controls the verbosity when fitting and predicting.
2313 
2314     warm_start : bool, default=False
2315         When set to ``True``, reuse the solution of the previous call to fit
2316         and add more estimators to the ensemble, otherwise, just fit a whole
2317         new forest. See :term:`Glossary <warm_start>` and
2318         :ref:`gradient_boosting_warm_start` for details.
2319 
2320     ccp_alpha : non-negative float, default=0.0
2321         Complexity parameter used for Minimal Cost-Complexity Pruning. The
2322         subtree with the largest cost complexity that is smaller than
2323         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
2324         :ref:`minimal_cost_complexity_pruning` for details.
2325 
2326         .. versionadded:: 0.22
2327 
2328     max_samples : int or float, default=None
2329         If bootstrap is True, the number of samples to draw from X
2330         to train each base estimator.
2331 
2332         - If None (default), then draw `X.shape[0]` samples.
2333         - If int, then draw `max_samples` samples.
2334         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
2335           `max_samples` should be in the interval `(0.0, 1.0]`.
2336 
2337         .. versionadded:: 0.22
2338 
2339     Attributes
2340     ----------
2341     estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`
2342         The child estimator template used to create the collection of fitted
2343         sub-estimators.
2344 
2345         .. versionadded:: 1.2
2346            `base_estimator_` was renamed to `estimator_`.
2347 
2348     base_estimator_ : ExtraTreeRegressor
2349         The child estimator template used to create the collection of fitted
2350         sub-estimators.
2351 
2352         .. deprecated:: 1.2
2353             `base_estimator_` is deprecated and will be removed in 1.4.
2354             Use `estimator_` instead.
2355 
2356     estimators_ : list of DecisionTreeRegressor
2357         The collection of fitted sub-estimators.
2358 
2359     feature_importances_ : ndarray of shape (n_features,)
2360         The impurity-based feature importances.
2361         The higher, the more important the feature.
2362         The importance of a feature is computed as the (normalized)
2363         total reduction of the criterion brought by that feature.  It is also
2364         known as the Gini importance.
2365 
2366         Warning: impurity-based feature importances can be misleading for
2367         high cardinality features (many unique values). See
2368         :func:`sklearn.inspection.permutation_importance` as an alternative.
2369 
2370     n_features_in_ : int
2371         Number of features seen during :term:`fit`.
2372 
2373         .. versionadded:: 0.24
2374 
2375     feature_names_in_ : ndarray of shape (`n_features_in_`,)
2376         Names of features seen during :term:`fit`. Defined only when `X`
2377         has feature names that are all strings.
2378 
2379         .. versionadded:: 1.0
2380 
2381     n_outputs_ : int
2382         The number of outputs.
2383 
2384     oob_score_ : float
2385         Score of the training dataset obtained using an out-of-bag estimate.
2386         This attribute exists only when ``oob_score`` is True.
2387 
2388     oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
2389         Prediction computed with out-of-bag estimate on the training set.
2390         This attribute exists only when ``oob_score`` is True.
2391 
2392     See Also
2393     --------
2394     ExtraTreesClassifier : An extra-trees classifier with random splits.
2395     RandomForestClassifier : A random forest classifier with optimal splits.
2396     RandomForestRegressor : Ensemble regressor using trees with optimal splits.
2397 
2398     Notes
2399     -----
2400     The default values for the parameters controlling the size of the trees
2401     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
2402     unpruned trees which can potentially be very large on some data sets. To
2403     reduce memory consumption, the complexity and size of the trees should be
2404     controlled by setting those parameter values.
2405 
2406     References
2407     ----------
2408     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
2409            Machine Learning, 63(1), 3-42, 2006.
2410 
2411     Examples
2412     --------
2413     >>> from sklearn.datasets import load_diabetes
2414     >>> from sklearn.model_selection import train_test_split
2415     >>> from sklearn.ensemble import ExtraTreesRegressor
2416     >>> X, y = load_diabetes(return_X_y=True)
2417     >>> X_train, X_test, y_train, y_test = train_test_split(
2418     ...     X, y, random_state=0)
2419     >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(
2420     ...    X_train, y_train)
2421     >>> reg.score(X_test, y_test)
2422     0.2727...
2423     """
2424 
2425     _parameter_constraints: dict = {
2426         **ForestRegressor._parameter_constraints,
2427         **DecisionTreeRegressor._parameter_constraints,
2428     }
2429     _parameter_constraints.pop("splitter")
2430 
2431     def __init__(
2432         self,
2433         n_estimators=100,
2434         *,
2435         criterion="squared_error",
2436         max_depth=None,
2437         min_samples_split=2,
2438         min_samples_leaf=1,
2439         min_weight_fraction_leaf=0.0,
2440         max_features=1.0,
2441         max_leaf_nodes=None,
2442         min_impurity_decrease=0.0,
2443         bootstrap=False,
2444         oob_score=False,
2445         n_jobs=None,
2446         random_state=None,
2447         verbose=0,
2448         warm_start=False,
2449         ccp_alpha=0.0,
2450         max_samples=None,
2451     ):
2452         super().__init__(
2453             estimator=ExtraTreeRegressor(),
2454             n_estimators=n_estimators,
2455             estimator_params=(
2456                 "criterion",
2457                 "max_depth",
2458                 "min_samples_split",
2459                 "min_samples_leaf",
2460                 "min_weight_fraction_leaf",
2461                 "max_features",
2462                 "max_leaf_nodes",
2463                 "min_impurity_decrease",
2464                 "random_state",
2465                 "ccp_alpha",
2466             ),
2467             bootstrap=bootstrap,
2468             oob_score=oob_score,
2469             n_jobs=n_jobs,
2470             random_state=random_state,
2471             verbose=verbose,
2472             warm_start=warm_start,
2473             max_samples=max_samples,
2474         )
2475 
2476         self.criterion = criterion
2477         self.max_depth = max_depth
2478         self.min_samples_split = min_samples_split
2479         self.min_samples_leaf = min_samples_leaf
2480         self.min_weight_fraction_leaf = min_weight_fraction_leaf
2481         self.max_features = max_features
2482         self.max_leaf_nodes = max_leaf_nodes
2483         self.min_impurity_decrease = min_impurity_decrease
2484         self.ccp_alpha = ccp_alpha
2485 
2486 
2487 class RandomTreesEmbedding(TransformerMixin, BaseForest):
2488     """
2489     An ensemble of totally random trees.
2490 
2491     An unsupervised transformation of a dataset to a high-dimensional
2492     sparse representation. A datapoint is coded according to which leaf of
2493     each tree it is sorted into. Using a one-hot encoding of the leaves,
2494     this leads to a binary coding with as many ones as there are trees in
2495     the forest.
2496 
2497     The dimensionality of the resulting representation is
2498     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
2499     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.
2500 
2501     Read more in the :ref:`User Guide <random_trees_embedding>`.
2502 
2503     Parameters
2504     ----------
2505     n_estimators : int, default=100
2506         Number of trees in the forest.
2507 
2508         .. versionchanged:: 0.22
2509            The default value of ``n_estimators`` changed from 10 to 100
2510            in 0.22.
2511 
2512     max_depth : int, default=5
2513         The maximum depth of each tree. If None, then nodes are expanded until
2514         all leaves are pure or until all leaves contain less than
2515         min_samples_split samples.
2516 
2517     min_samples_split : int or float, default=2
2518         The minimum number of samples required to split an internal node:
2519 
2520         - If int, then consider `min_samples_split` as the minimum number.
2521         - If float, then `min_samples_split` is a fraction and
2522           `ceil(min_samples_split * n_samples)` is the minimum
2523           number of samples for each split.
2524 
2525         .. versionchanged:: 0.18
2526            Added float values for fractions.
2527 
2528     min_samples_leaf : int or float, default=1
2529         The minimum number of samples required to be at a leaf node.
2530         A split point at any depth will only be considered if it leaves at
2531         least ``min_samples_leaf`` training samples in each of the left and
2532         right branches.  This may have the effect of smoothing the model,
2533         especially in regression.
2534 
2535         - If int, then consider `min_samples_leaf` as the minimum number.
2536         - If float, then `min_samples_leaf` is a fraction and
2537           `ceil(min_samples_leaf * n_samples)` is the minimum
2538           number of samples for each node.
2539 
2540         .. versionchanged:: 0.18
2541            Added float values for fractions.
2542 
2543     min_weight_fraction_leaf : float, default=0.0
2544         The minimum weighted fraction of the sum total of weights (of all
2545         the input samples) required to be at a leaf node. Samples have
2546         equal weight when sample_weight is not provided.
2547 
2548     max_leaf_nodes : int, default=None
2549         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2550         Best nodes are defined as relative reduction in impurity.
2551         If None then unlimited number of leaf nodes.
2552 
2553     min_impurity_decrease : float, default=0.0
2554         A node will be split if this split induces a decrease of the impurity
2555         greater than or equal to this value.
2556 
2557         The weighted impurity decrease equation is the following::
2558 
2559             N_t / N * (impurity - N_t_R / N_t * right_impurity
2560                                 - N_t_L / N_t * left_impurity)
2561 
2562         where ``N`` is the total number of samples, ``N_t`` is the number of
2563         samples at the current node, ``N_t_L`` is the number of samples in the
2564         left child, and ``N_t_R`` is the number of samples in the right child.
2565 
2566         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2567         if ``sample_weight`` is passed.
2568 
2569         .. versionadded:: 0.19
2570 
2571     sparse_output : bool, default=True
2572         Whether or not to return a sparse CSR matrix, as default behavior,
2573         or to return a dense array compatible with dense pipeline operators.
2574 
2575     n_jobs : int, default=None
2576         The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,
2577         :meth:`decision_path` and :meth:`apply` are all parallelized over the
2578         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
2579         context. ``-1`` means using all processors. See :term:`Glossary
2580         <n_jobs>` for more details.
2581 
2582     random_state : int, RandomState instance or None, default=None
2583         Controls the generation of the random `y` used to fit the trees
2584         and the draw of the splits for each feature at the trees' nodes.
2585         See :term:`Glossary <random_state>` for details.
2586 
2587     verbose : int, default=0
2588         Controls the verbosity when fitting and predicting.
2589 
2590     warm_start : bool, default=False
2591         When set to ``True``, reuse the solution of the previous call to fit
2592         and add more estimators to the ensemble, otherwise, just fit a whole
2593         new forest. See :term:`Glossary <warm_start>` and
2594         :ref:`gradient_boosting_warm_start` for details.
2595 
2596     Attributes
2597     ----------
2598     estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
2599         The child estimator template used to create the collection of fitted
2600         sub-estimators.
2601 
2602         .. versionadded:: 1.2
2603            `base_estimator_` was renamed to `estimator_`.
2604 
2605     base_estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
2606         The child estimator template used to create the collection of fitted
2607         sub-estimators.
2608 
2609         .. deprecated:: 1.2
2610             `base_estimator_` is deprecated and will be removed in 1.4.
2611             Use `estimator_` instead.
2612 
2613     estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances
2614         The collection of fitted sub-estimators.
2615 
2616     feature_importances_ : ndarray of shape (n_features,)
2617         The feature importances (the higher, the more important the feature).
2618 
2619     n_features_in_ : int
2620         Number of features seen during :term:`fit`.
2621 
2622         .. versionadded:: 0.24
2623 
2624     feature_names_in_ : ndarray of shape (`n_features_in_`,)
2625         Names of features seen during :term:`fit`. Defined only when `X`
2626         has feature names that are all strings.
2627 
2628         .. versionadded:: 1.0
2629 
2630     n_outputs_ : int
2631         The number of outputs when ``fit`` is performed.
2632 
2633     one_hot_encoder_ : OneHotEncoder instance
2634         One-hot encoder used to create the sparse embedding.
2635 
2636     See Also
2637     --------
2638     ExtraTreesClassifier : An extra-trees classifier.
2639     ExtraTreesRegressor : An extra-trees regressor.
2640     RandomForestClassifier : A random forest classifier.
2641     RandomForestRegressor : A random forest regressor.
2642     sklearn.tree.ExtraTreeClassifier: An extremely randomized
2643         tree classifier.
2644     sklearn.tree.ExtraTreeRegressor : An extremely randomized
2645         tree regressor.
2646 
2647     References
2648     ----------
2649     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
2650            Machine Learning, 63(1), 3-42, 2006.
2651     .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  "Fast discriminative
2652            visual codebooks using randomized clustering forests"
2653            NIPS 2007
2654 
2655     Examples
2656     --------
2657     >>> from sklearn.ensemble import RandomTreesEmbedding
2658     >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]
2659     >>> random_trees = RandomTreesEmbedding(
2660     ...    n_estimators=5, random_state=0, max_depth=1).fit(X)
2661     >>> X_sparse_embedding = random_trees.transform(X)
2662     >>> X_sparse_embedding.toarray()
2663     array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
2664            [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
2665            [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
2666            [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],
2667            [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])
2668     """
2669 
2670     _parameter_constraints: dict = {
2671         "n_estimators": [Interval(Integral, 1, None, closed="left")],
2672         "n_jobs": [Integral, None],
2673         "verbose": ["verbose"],
2674         "warm_start": ["boolean"],
2675         **BaseDecisionTree._parameter_constraints,
2676         "sparse_output": ["boolean"],
2677     }
2678     for param in ("max_features", "ccp_alpha", "splitter"):
2679         _parameter_constraints.pop(param)
2680 
2681     criterion = "squared_error"
2682     max_features = 1
2683 
2684     def __init__(
2685         self,
2686         n_estimators=100,
2687         *,
2688         max_depth=5,
2689         min_samples_split=2,
2690         min_samples_leaf=1,
2691         min_weight_fraction_leaf=0.0,
2692         max_leaf_nodes=None,
2693         min_impurity_decrease=0.0,
2694         sparse_output=True,
2695         n_jobs=None,
2696         random_state=None,
2697         verbose=0,
2698         warm_start=False,
2699     ):
2700         super().__init__(
2701             estimator=ExtraTreeRegressor(),
2702             n_estimators=n_estimators,
2703             estimator_params=(
2704                 "criterion",
2705                 "max_depth",
2706                 "min_samples_split",
2707                 "min_samples_leaf",
2708                 "min_weight_fraction_leaf",
2709                 "max_features",
2710                 "max_leaf_nodes",
2711                 "min_impurity_decrease",
2712                 "random_state",
2713             ),
2714             bootstrap=False,
2715             oob_score=False,
2716             n_jobs=n_jobs,
2717             random_state=random_state,
2718             verbose=verbose,
2719             warm_start=warm_start,
2720             max_samples=None,
2721         )
2722 
2723         self.max_depth = max_depth
2724         self.min_samples_split = min_samples_split
2725         self.min_samples_leaf = min_samples_leaf
2726         self.min_weight_fraction_leaf = min_weight_fraction_leaf
2727         self.max_leaf_nodes = max_leaf_nodes
2728         self.min_impurity_decrease = min_impurity_decrease
2729         self.sparse_output = sparse_output
2730 
2731     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
2732         raise NotImplementedError("OOB score not supported by tree embedding")
2733 
2734     def fit(self, X, y=None, sample_weight=None):
2735         """
2736         Fit estimator.
2737 
2738         Parameters
2739         ----------
2740         X : {array-like, sparse matrix} of shape (n_samples, n_features)
2741             The input samples. Use ``dtype=np.float32`` for maximum
2742             efficiency. Sparse matrices are also supported, use sparse
2743             ``csc_matrix`` for maximum efficiency.
2744 
2745         y : Ignored
2746             Not used, present for API consistency by convention.
2747 
2748         sample_weight : array-like of shape (n_samples,), default=None
2749             Sample weights. If None, then samples are equally weighted. Splits
2750             that would create child nodes with net zero or negative weight are
2751             ignored while searching for a split in each node. In the case of
2752             classification, splits are also ignored if they would result in any
2753             single class carrying a negative weight in either child node.
2754 
2755         Returns
2756         -------
2757         self : object
2758             Returns the instance itself.
2759         """
2760         # Parameters are validated in fit_transform
2761         self.fit_transform(X, y, sample_weight=sample_weight)
2762         return self
2763 
2764     def fit_transform(self, X, y=None, sample_weight=None):
2765         """
2766         Fit estimator and transform dataset.
2767 
2768         Parameters
2769         ----------
2770         X : {array-like, sparse matrix} of shape (n_samples, n_features)
2771             Input data used to build forests. Use ``dtype=np.float32`` for
2772             maximum efficiency.
2773 
2774         y : Ignored
2775             Not used, present for API consistency by convention.
2776 
2777         sample_weight : array-like of shape (n_samples,), default=None
2778             Sample weights. If None, then samples are equally weighted. Splits
2779             that would create child nodes with net zero or negative weight are
2780             ignored while searching for a split in each node. In the case of
2781             classification, splits are also ignored if they would result in any
2782             single class carrying a negative weight in either child node.
2783 
2784         Returns
2785         -------
2786         X_transformed : sparse matrix of shape (n_samples, n_out)
2787             Transformed dataset.
2788         """
2789         self._validate_params()
2790 
2791         rnd = check_random_state(self.random_state)
2792         y = rnd.uniform(size=_num_samples(X))
2793         super().fit(X, y, sample_weight=sample_weight)
2794 
2795         self.one_hot_encoder_ = OneHotEncoder(sparse_output=self.sparse_output)
2796         output = self.one_hot_encoder_.fit_transform(self.apply(X))
2797         self._n_features_out = output.shape[1]
2798         return output
2799 
2800     def get_feature_names_out(self, input_features=None):
2801         """Get output feature names for transformation.
2802 
2803         Parameters
2804         ----------
2805         input_features : array-like of str or None, default=None
2806             Only used to validate feature names with the names seen in :meth:`fit`.
2807 
2808         Returns
2809         -------
2810         feature_names_out : ndarray of str objects
2811             Transformed feature names, in the format of
2812             `randomtreesembedding_{tree}_{leaf}`, where `tree` is the tree used
2813             to generate the leaf and `leaf` is the index of a leaf node
2814             in that tree. Note that the node indexing scheme is used to
2815             index both nodes with children (split nodes) and leaf nodes.
2816             Only the latter can be present as output features.
2817             As a consequence, there are missing indices in the output
2818             feature names.
2819         """
2820         check_is_fitted(self, "_n_features_out")
2821         _check_feature_names_in(
2822             self, input_features=input_features, generate_names=False
2823         )
2824 
2825         feature_names = [
2826             f"randomtreesembedding_{tree}_{leaf}"
2827             for tree in range(self.n_estimators)
2828             for leaf in self.one_hot_encoder_.categories_[tree]
2829         ]
2830         return np.asarray(feature_names, dtype=object)
2831 
2832     def transform(self, X):
2833         """
2834         Transform dataset.
2835 
2836         Parameters
2837         ----------
2838         X : {array-like, sparse matrix} of shape (n_samples, n_features)
2839             Input data to be transformed. Use ``dtype=np.float32`` for maximum
2840             efficiency. Sparse matrices are also supported, use sparse
2841             ``csr_matrix`` for maximum efficiency.
2842 
2843         Returns
2844         -------
2845         X_transformed : sparse matrix of shape (n_samples, n_out)
2846             Transformed dataset.
2847         """
2848         check_is_fitted(self)
2849         return self.one_hot_encoder_.transform(self.apply(X))

</code><intended_behavior>Similar to the above, modifications should ensure that configurations pertinent to the fitting process (possibly including random state settings, number of jobs, etc.) are explicitly passed to each parallel execution context initiated via delayed. This may include augmenting the delayed call to ensure it carries over or applies the current configuration settings within each executed task, preventing misconfigurations when tasks run in threads/processes different from the main thread initializing them.</intended_behavior>]
2025-10-16 00:05:42.946 | INFO     | Search completed. Bug locations: [<file>sklearn/model_selection/_validation.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The :mod:`sklearn.model_selection._validation` module includes classes and
3 functions to validate the model.
4 """
5 
6 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
7 #         Gael Varoquaux <gael.varoquaux@normalesup.org>
8 #         Olivier Grisel <olivier.grisel@ensta.org>
9 #         Raghav RV <rvraghav93@gmail.com>
10 #         Michal Karbownik <michakarbownik@gmail.com>
11 # License: BSD 3 clause
12 
13 
14 import warnings
15 import numbers
16 import time
17 from functools import partial
18 from traceback import format_exc
19 from contextlib import suppress
20 from collections import Counter
21 
22 import numpy as np
23 import scipy.sparse as sp
24 from joblib import Parallel, logger
25 
26 from ..base import is_classifier, clone
27 from ..utils import indexable, check_random_state, _safe_indexing
28 from ..utils.validation import _check_fit_params
29 from ..utils.validation import _num_samples
30 from ..utils.fixes import delayed
31 from ..utils.metaestimators import _safe_split
32 from ..metrics import check_scoring
33 from ..metrics._scorer import _check_multimetric_scoring, _MultimetricScorer
34 from ..exceptions import FitFailedWarning
35 from ._split import check_cv
36 from ..preprocessing import LabelEncoder
37 
38 
39 __all__ = [
40     "cross_validate",
41     "cross_val_score",
42     "cross_val_predict",
43     "permutation_test_score",
44     "learning_curve",
45     "validation_curve",
46 ]
47 
48 
49 def cross_validate(
50     estimator,
51     X,
52     y=None,
53     *,
54     groups=None,
55     scoring=None,
56     cv=None,
57     n_jobs=None,
58     verbose=0,
59     fit_params=None,
60     pre_dispatch="2*n_jobs",
61     return_train_score=False,
62     return_estimator=False,
63     error_score=np.nan,
64 ):
65     """Evaluate metric(s) by cross-validation and also record fit/score times.
66 
67     Read more in the :ref:`User Guide <multimetric_cross_validation>`.
68 
69     Parameters
70     ----------
71     estimator : estimator object implementing 'fit'
72         The object to use to fit the data.
73 
74     X : array-like of shape (n_samples, n_features)
75         The data to fit. Can be for example a list, or an array.
76 
77     y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None
78         The target variable to try to predict in the case of
79         supervised learning.
80 
81     groups : array-like of shape (n_samples,), default=None
82         Group labels for the samples used while splitting the dataset into
83         train/test set. Only used in conjunction with a "Group" :term:`cv`
84         instance (e.g., :class:`GroupKFold`).
85 
86     scoring : str, callable, list, tuple, or dict, default=None
87         Strategy to evaluate the performance of the cross-validated model on
88         the test set.
89 
90         If `scoring` represents a single score, one can use:
91 
92         - a single string (see :ref:`scoring_parameter`);
93         - a callable (see :ref:`scoring`) that returns a single value.
94 
95         If `scoring` represents multiple scores, one can use:
96 
97         - a list or tuple of unique strings;
98         - a callable returning a dictionary where the keys are the metric
99           names and the values are the metric scores;
100         - a dictionary with metric names as keys and callables a values.
101 
102         See :ref:`multimetric_grid_search` for an example.
103 
104     cv : int, cross-validation generator or an iterable, default=None
105         Determines the cross-validation splitting strategy.
106         Possible inputs for cv are:
107 
108         - None, to use the default 5-fold cross validation,
109         - int, to specify the number of folds in a `(Stratified)KFold`,
110         - :term:`CV splitter`,
111         - An iterable yielding (train, test) splits as arrays of indices.
112 
113         For int/None inputs, if the estimator is a classifier and ``y`` is
114         either binary or multiclass, :class:`StratifiedKFold` is used. In all
115         other cases, :class:`KFold` is used. These splitters are instantiated
116         with `shuffle=False` so the splits will be the same across calls.
117 
118         Refer :ref:`User Guide <cross_validation>` for the various
119         cross-validation strategies that can be used here.
120 
121         .. versionchanged:: 0.22
122             ``cv`` default value if None changed from 3-fold to 5-fold.
123 
124     n_jobs : int, default=None
125         Number of jobs to run in parallel. Training the estimator and computing
126         the score are parallelized over the cross-validation splits.
127         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
128         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
129         for more details.
130 
131     verbose : int, default=0
132         The verbosity level.
133 
134     fit_params : dict, default=None
135         Parameters to pass to the fit method of the estimator.
136 
137     pre_dispatch : int or str, default='2*n_jobs'
138         Controls the number of jobs that get dispatched during parallel
139         execution. Reducing this number can be useful to avoid an
140         explosion of memory consumption when more jobs get dispatched
141         than CPUs can process. This parameter can be:
142 
143             - None, in which case all the jobs are immediately
144               created and spawned. Use this for lightweight and
145               fast-running jobs, to avoid delays due to on-demand
146               spawning of the jobs
147 
148             - An int, giving the exact number of total jobs that are
149               spawned
150 
151             - A str, giving an expression as a function of n_jobs,
152               as in '2*n_jobs'
153 
154     return_train_score : bool, default=False
155         Whether to include train scores.
156         Computing training scores is used to get insights on how different
157         parameter settings impact the overfitting/underfitting trade-off.
158         However computing the scores on the training set can be computationally
159         expensive and is not strictly required to select the parameters that
160         yield the best generalization performance.
161 
162         .. versionadded:: 0.19
163 
164         .. versionchanged:: 0.21
165             Default value was changed from ``True`` to ``False``
166 
167     return_estimator : bool, default=False
168         Whether to return the estimators fitted on each split.
169 
170         .. versionadded:: 0.20
171 
172     error_score : 'raise' or numeric, default=np.nan
173         Value to assign to the score if an error occurs in estimator fitting.
174         If set to 'raise', the error is raised.
175         If a numeric value is given, FitFailedWarning is raised.
176 
177         .. versionadded:: 0.20
178 
179     Returns
180     -------
181     scores : dict of float arrays of shape (n_splits,)
182         Array of scores of the estimator for each run of the cross validation.
183 
184         A dict of arrays containing the score/time arrays for each scorer is
185         returned. The possible keys for this ``dict`` are:
186 
187             ``test_score``
188                 The score array for test scores on each cv split.
189                 Suffix ``_score`` in ``test_score`` changes to a specific
190                 metric like ``test_r2`` or ``test_auc`` if there are
191                 multiple scoring metrics in the scoring parameter.
192             ``train_score``
193                 The score array for train scores on each cv split.
194                 Suffix ``_score`` in ``train_score`` changes to a specific
195                 metric like ``train_r2`` or ``train_auc`` if there are
196                 multiple scoring metrics in the scoring parameter.
197                 This is available only if ``return_train_score`` parameter
198                 is ``True``.
199             ``fit_time``
200                 The time for fitting the estimator on the train
201                 set for each cv split.
202             ``score_time``
203                 The time for scoring the estimator on the test set for each
204                 cv split. (Note time for scoring on the train set is not
205                 included even if ``return_train_score`` is set to ``True``
206             ``estimator``
207                 The estimator objects for each cv split.
208                 This is available only if ``return_estimator`` parameter
209                 is set to ``True``.
210 
211     See Also
212     --------
213     cross_val_score : Run cross-validation for single metric evaluation.
214 
215     cross_val_predict : Get predictions from each split of cross-validation for
216         diagnostic purposes.
217 
218     sklearn.metrics.make_scorer : Make a scorer from a performance metric or
219         loss function.
220 
221     Examples
222     --------
223     >>> from sklearn import datasets, linear_model
224     >>> from sklearn.model_selection import cross_validate
225     >>> from sklearn.metrics import make_scorer
226     >>> from sklearn.metrics import confusion_matrix
227     >>> from sklearn.svm import LinearSVC
228     >>> diabetes = datasets.load_diabetes()
229     >>> X = diabetes.data[:150]
230     >>> y = diabetes.target[:150]
231     >>> lasso = linear_model.Lasso()
232 
233     Single metric evaluation using ``cross_validate``
234 
235     >>> cv_results = cross_validate(lasso, X, y, cv=3)
236     >>> sorted(cv_results.keys())
237     ['fit_time', 'score_time', 'test_score']
238     >>> cv_results['test_score']
239     array([0.3315057 , 0.08022103, 0.03531816])
240 
241     Multiple metric evaluation using ``cross_validate``
242     (please refer the ``scoring`` parameter doc for more information)
243 
244     >>> scores = cross_validate(lasso, X, y, cv=3,
245     ...                         scoring=('r2', 'neg_mean_squared_error'),
246     ...                         return_train_score=True)
247     >>> print(scores['test_neg_mean_squared_error'])
248     [-3635.5... -3573.3... -6114.7...]
249     >>> print(scores['train_r2'])
250     [0.28009951 0.3908844  0.22784907]
251     """
252     X, y, groups = indexable(X, y, groups)
253 
254     cv = check_cv(cv, y, classifier=is_classifier(estimator))
255 
256     if callable(scoring):
257         scorers = scoring
258     elif scoring is None or isinstance(scoring, str):
259         scorers = check_scoring(estimator, scoring)
260     else:
261         scorers = _check_multimetric_scoring(estimator, scoring)
262 
263     # We clone the estimator to make sure that all the folds are
264     # independent, and that it is pickle-able.
265     parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
266     results = parallel(
267         delayed(_fit_and_score)(
268             clone(estimator),
269             X,
270             y,
271             scorers,
272             train,
273             test,
274             verbose,
275             None,
276             fit_params,
277             return_train_score=return_train_score,
278             return_times=True,
279             return_estimator=return_estimator,
280             error_score=error_score,
281         )
282         for train, test in cv.split(X, y, groups)
283     )
284 
285     _warn_or_raise_about_fit_failures(results, error_score)
286 
287     # For callabe scoring, the return type is only know after calling. If the
288     # return type is a dictionary, the error scores can now be inserted with
289     # the correct key.
290     if callable(scoring):
291         _insert_error_scores(results, error_score)
292 
293     results = _aggregate_score_dicts(results)
294 
295     ret = {}
296     ret["fit_time"] = results["fit_time"]
297     ret["score_time"] = results["score_time"]
298 
299     if return_estimator:
300         ret["estimator"] = results["estimator"]
301 
302     test_scores_dict = _normalize_score_results(results["test_scores"])
303     if return_train_score:
304         train_scores_dict = _normalize_score_results(results["train_scores"])
305 
306     for name in test_scores_dict:
307         ret["test_%s" % name] = test_scores_dict[name]
308         if return_train_score:
309             key = "train_%s" % name
310             ret[key] = train_scores_dict[name]
311 
312     return ret
313 
314 
315 def _insert_error_scores(results, error_score):
316     """Insert error in `results` by replacing them inplace with `error_score`.
317 
318     This only applies to multimetric scores because `_fit_and_score` will
319     handle the single metric case.
320     """
321     successful_score = None
322     failed_indices = []
323     for i, result in enumerate(results):
324         if result["fit_error"] is not None:
325             failed_indices.append(i)
326         elif successful_score is None:
327             successful_score = result["test_scores"]
328 
329     if isinstance(successful_score, dict):
330         formatted_error = {name: error_score for name in successful_score}
331         for i in failed_indices:
332             results[i]["test_scores"] = formatted_error.copy()
333             if "train_scores" in results[i]:
334                 results[i]["train_scores"] = formatted_error.copy()
335 
336 
337 def _normalize_score_results(scores, scaler_score_key="score"):
338     """Creates a scoring dictionary based on the type of `scores`"""
339     if isinstance(scores[0], dict):
340         # multimetric scoring
341         return _aggregate_score_dicts(scores)
342     # scaler
343     return {scaler_score_key: scores}
344 
345 
346 def _warn_or_raise_about_fit_failures(results, error_score):
347     fit_errors = [
348         result["fit_error"] for result in results if result["fit_error"] is not None
349     ]
350     if fit_errors:
351         num_failed_fits = len(fit_errors)
352         num_fits = len(results)
353         fit_errors_counter = Counter(fit_errors)
354         delimiter = "-" * 80 + "\n"
355         fit_errors_summary = "\n".join(
356             f"{delimiter}{n} fits failed with the following error:\n{error}"
357             for error, n in fit_errors_counter.items()
358         )
359 
360         if num_failed_fits == num_fits:
361             all_fits_failed_message = (
362                 f"\nAll the {num_fits} fits failed.\n"
363                 "It is very likely that your model is misconfigured.\n"
364                 "You can try to debug the error by setting error_score='raise'.\n\n"
365                 f"Below are more details about the failures:\n{fit_errors_summary}"
366             )
367             raise ValueError(all_fits_failed_message)
368 
369         else:
370             some_fits_failed_message = (
371                 f"\n{num_failed_fits} fits failed out of a total of {num_fits}.\n"
372                 "The score on these train-test partitions for these parameters"
373                 f" will be set to {error_score}.\n"
374                 "If these failures are not expected, you can try to debug them "
375                 "by setting error_score='raise'.\n\n"
376                 f"Below are more details about the failures:\n{fit_errors_summary}"
377             )
378             warnings.warn(some_fits_failed_message, FitFailedWarning)
379 
380 
381 def cross_val_score(
382     estimator,
383     X,
384     y=None,
385     *,
386     groups=None,
387     scoring=None,
388     cv=None,
389     n_jobs=None,
390     verbose=0,
391     fit_params=None,
392     pre_dispatch="2*n_jobs",
393     error_score=np.nan,
394 ):
395     """Evaluate a score by cross-validation.
396 
397     Read more in the :ref:`User Guide <cross_validation>`.
398 
399     Parameters
400     ----------
401     estimator : estimator object implementing 'fit'
402         The object to use to fit the data.
403 
404     X : array-like of shape (n_samples, n_features)
405         The data to fit. Can be for example a list, or an array.
406 
407     y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
408             default=None
409         The target variable to try to predict in the case of
410         supervised learning.
411 
412     groups : array-like of shape (n_samples,), default=None
413         Group labels for the samples used while splitting the dataset into
414         train/test set. Only used in conjunction with a "Group" :term:`cv`
415         instance (e.g., :class:`GroupKFold`).
416 
417     scoring : str or callable, default=None
418         A str (see model evaluation documentation) or
419         a scorer callable object / function with signature
420         ``scorer(estimator, X, y)`` which should return only
421         a single value.
422 
423         Similar to :func:`cross_validate`
424         but only a single metric is permitted.
425 
426         If `None`, the estimator's default scorer (if available) is used.
427 
428     cv : int, cross-validation generator or an iterable, default=None
429         Determines the cross-validation splitting strategy.
430         Possible inputs for cv are:
431 
432         - `None`, to use the default 5-fold cross validation,
433         - int, to specify the number of folds in a `(Stratified)KFold`,
434         - :term:`CV splitter`,
435         - An iterable that generates (train, test) splits as arrays of indices.
436 
437         For `int`/`None` inputs, if the estimator is a classifier and `y` is
438         either binary or multiclass, :class:`StratifiedKFold` is used. In all
439         other cases, :class:`KFold` is used. These splitters are instantiated
440         with `shuffle=False` so the splits will be the same across calls.
441 
442         Refer :ref:`User Guide <cross_validation>` for the various
443         cross-validation strategies that can be used here.
444 
445         .. versionchanged:: 0.22
446             `cv` default value if `None` changed from 3-fold to 5-fold.
447 
448     n_jobs : int, default=None
449         Number of jobs to run in parallel. Training the estimator and computing
450         the score are parallelized over the cross-validation splits.
451         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
452         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
453         for more details.
454 
455     verbose : int, default=0
456         The verbosity level.
457 
458     fit_params : dict, default=None
459         Parameters to pass to the fit method of the estimator.
460 
461     pre_dispatch : int or str, default='2*n_jobs'
462         Controls the number of jobs that get dispatched during parallel
463         execution. Reducing this number can be useful to avoid an
464         explosion of memory consumption when more jobs get dispatched
465         than CPUs can process. This parameter can be:
466 
467             - ``None``, in which case all the jobs are immediately
468               created and spawned. Use this for lightweight and
469               fast-running jobs, to avoid delays due to on-demand
470               spawning of the jobs
471 
472             - An int, giving the exact number of total jobs that are
473               spawned
474 
475             - A str, giving an expression as a function of n_jobs,
476               as in '2*n_jobs'
477 
478     error_score : 'raise' or numeric, default=np.nan
479         Value to assign to the score if an error occurs in estimator fitting.
480         If set to 'raise', the error is raised.
481         If a numeric value is given, FitFailedWarning is raised.
482 
483         .. versionadded:: 0.20
484 
485     Returns
486     -------
487     scores : ndarray of float of shape=(len(list(cv)),)
488         Array of scores of the estimator for each run of the cross validation.
489 
490     See Also
491     --------
492     cross_validate : To run cross-validation on multiple metrics and also to
493         return train scores, fit times and score times.
494 
495     cross_val_predict : Get predictions from each split of cross-validation for
496         diagnostic purposes.
497 
498     sklearn.metrics.make_scorer : Make a scorer from a performance metric or
499         loss function.
500 
501     Examples
502     --------
503     >>> from sklearn import datasets, linear_model
504     >>> from sklearn.model_selection import cross_val_score
505     >>> diabetes = datasets.load_diabetes()
506     >>> X = diabetes.data[:150]
507     >>> y = diabetes.target[:150]
508     >>> lasso = linear_model.Lasso()
509     >>> print(cross_val_score(lasso, X, y, cv=3))
510     [0.3315057  0.08022103 0.03531816]
511     """
512     # To ensure multimetric format is not supported
513     scorer = check_scoring(estimator, scoring=scoring)
514 
515     cv_results = cross_validate(
516         estimator=estimator,
517         X=X,
518         y=y,
519         groups=groups,
520         scoring={"score": scorer},
521         cv=cv,
522         n_jobs=n_jobs,
523         verbose=verbose,
524         fit_params=fit_params,
525         pre_dispatch=pre_dispatch,
526         error_score=error_score,
527     )
528     return cv_results["test_score"]
529 
530 
531 def _fit_and_score(
532     estimator,
533     X,
534     y,
535     scorer,
536     train,
537     test,
538     verbose,
539     parameters,
540     fit_params,
541     return_train_score=False,
542     return_parameters=False,
543     return_n_test_samples=False,
544     return_times=False,
545     return_estimator=False,
546     split_progress=None,
547     candidate_progress=None,
548     error_score=np.nan,
549 ):
550 
551     """Fit estimator and compute scores for a given dataset split.
552 
553     Parameters
554     ----------
555     estimator : estimator object implementing 'fit'
556         The object to use to fit the data.
557 
558     X : array-like of shape (n_samples, n_features)
559         The data to fit.
560 
561     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
562         The target variable to try to predict in the case of
563         supervised learning.
564 
565     scorer : A single callable or dict mapping scorer name to the callable
566         If it is a single callable, the return value for ``train_scores`` and
567         ``test_scores`` is a single float.
568 
569         For a dict, it should be one mapping the scorer name to the scorer
570         callable object / function.
571 
572         The callable object / fn should have signature
573         ``scorer(estimator, X, y)``.
574 
575     train : array-like of shape (n_train_samples,)
576         Indices of training samples.
577 
578     test : array-like of shape (n_test_samples,)
579         Indices of test samples.
580 
581     verbose : int
582         The verbosity level.
583 
584     error_score : 'raise' or numeric, default=np.nan
585         Value to assign to the score if an error occurs in estimator fitting.
586         If set to 'raise', the error is raised.
587         If a numeric value is given, FitFailedWarning is raised.
588 
589     parameters : dict or None
590         Parameters to be set on the estimator.
591 
592     fit_params : dict or None
593         Parameters that will be passed to ``estimator.fit``.
594 
595     return_train_score : bool, default=False
596         Compute and return score on training set.
597 
598     return_parameters : bool, default=False
599         Return parameters that has been used for the estimator.
600 
601     split_progress : {list, tuple} of int, default=None
602         A list or tuple of format (<current_split_id>, <total_num_of_splits>).
603 
604     candidate_progress : {list, tuple} of int, default=None
605         A list or tuple of format
606         (<current_candidate_id>, <total_number_of_candidates>).
607 
608     return_n_test_samples : bool, default=False
609         Whether to return the ``n_test_samples``.
610 
611     return_times : bool, default=False
612         Whether to return the fit/score times.
613 
614     return_estimator : bool, default=False
615         Whether to return the fitted estimator.
616 
617     Returns
618     -------
619     result : dict with the following attributes
620         train_scores : dict of scorer name -> float
621             Score on training set (for all the scorers),
622             returned only if `return_train_score` is `True`.
623         test_scores : dict of scorer name -> float
624             Score on testing set (for all the scorers).
625         n_test_samples : int
626             Number of test samples.
627         fit_time : float
628             Time spent for fitting in seconds.
629         score_time : float
630             Time spent for scoring in seconds.
631         parameters : dict or None
632             The parameters that have been evaluated.
633         estimator : estimator object
634             The fitted estimator.
635         fit_error : str or None
636             Traceback str if the fit failed, None if the fit succeeded.
637     """
638     if not isinstance(error_score, numbers.Number) and error_score != "raise":
639         raise ValueError(
640             "error_score must be the string 'raise' or a numeric value. "
641             "(Hint: if using 'raise', please make sure that it has been "
642             "spelled correctly.)"
643         )
644 
645     progress_msg = ""
646     if verbose > 2:
647         if split_progress is not None:
648             progress_msg = f" {split_progress[0]+1}/{split_progress[1]}"
649         if candidate_progress and verbose > 9:
650             progress_msg += f"; {candidate_progress[0]+1}/{candidate_progress[1]}"
651 
652     if verbose > 1:
653         if parameters is None:
654             params_msg = ""
655         else:
656             sorted_keys = sorted(parameters)  # Ensure deterministic o/p
657             params_msg = ", ".join(f"{k}={parameters[k]}" for k in sorted_keys)
658     if verbose > 9:
659         start_msg = f"[CV{progress_msg}] START {params_msg}"
660         print(f"{start_msg}{(80 - len(start_msg)) * '.'}")
661 
662     # Adjust length of sample weights
663     fit_params = fit_params if fit_params is not None else {}
664     fit_params = _check_fit_params(X, fit_params, train)
665 
666     if parameters is not None:
667         # clone after setting parameters in case any parameters
668         # are estimators (like pipeline steps)
669         # because pipeline doesn't clone steps in fit
670         cloned_parameters = {}
671         for k, v in parameters.items():
672             cloned_parameters[k] = clone(v, safe=False)
673 
674         estimator = estimator.set_params(**cloned_parameters)
675 
676     start_time = time.time()
677 
678     X_train, y_train = _safe_split(estimator, X, y, train)
679     X_test, y_test = _safe_split(estimator, X, y, test, train)
680 
681     result = {}
682     try:
683         if y_train is None:
684             estimator.fit(X_train, **fit_params)
685         else:
686             estimator.fit(X_train, y_train, **fit_params)
687 
688     except Exception:
689         # Note fit time as time until error
690         fit_time = time.time() - start_time
691         score_time = 0.0
692         if error_score == "raise":
693             raise
694         elif isinstance(error_score, numbers.Number):
695             if isinstance(scorer, dict):
696                 test_scores = {name: error_score for name in scorer}
697                 if return_train_score:
698                     train_scores = test_scores.copy()
699             else:
700                 test_scores = error_score
701                 if return_train_score:
702                     train_scores = error_score
703         result["fit_error"] = format_exc()
704     else:
705         result["fit_error"] = None
706 
707         fit_time = time.time() - start_time
708         test_scores = _score(estimator, X_test, y_test, scorer, error_score)
709         score_time = time.time() - start_time - fit_time
710         if return_train_score:
711             train_scores = _score(estimator, X_train, y_train, scorer, error_score)
712 
713     if verbose > 1:
714         total_time = score_time + fit_time
715         end_msg = f"[CV{progress_msg}] END "
716         result_msg = params_msg + (";" if params_msg else "")
717         if verbose > 2:
718             if isinstance(test_scores, dict):
719                 for scorer_name in sorted(test_scores):
720                     result_msg += f" {scorer_name}: ("
721                     if return_train_score:
722                         scorer_scores = train_scores[scorer_name]
723                         result_msg += f"train={scorer_scores:.3f}, "
724                     result_msg += f"test={test_scores[scorer_name]:.3f})"
725             else:
726                 result_msg += ", score="
727                 if return_train_score:
728                     result_msg += f"(train={train_scores:.3f}, test={test_scores:.3f})"
729                 else:
730                     result_msg += f"{test_scores:.3f}"
731         result_msg += f" total time={logger.short_format_time(total_time)}"
732 
733         # Right align the result_msg
734         end_msg += "." * (80 - len(end_msg) - len(result_msg))
735         end_msg += result_msg
736         print(end_msg)
737 
738     result["test_scores"] = test_scores
739     if return_train_score:
740         result["train_scores"] = train_scores
741     if return_n_test_samples:
742         result["n_test_samples"] = _num_samples(X_test)
743     if return_times:
744         result["fit_time"] = fit_time
745         result["score_time"] = score_time
746     if return_parameters:
747         result["parameters"] = parameters
748     if return_estimator:
749         result["estimator"] = estimator
750     return result
751 
752 
753 def _score(estimator, X_test, y_test, scorer, error_score="raise"):
754     """Compute the score(s) of an estimator on a given test set.
755 
756     Will return a dict of floats if `scorer` is a dict, otherwise a single
757     float is returned.
758     """
759     if isinstance(scorer, dict):
760         # will cache method calls if needed. scorer() returns a dict
761         scorer = _MultimetricScorer(scorers=scorer, raise_exc=(error_score == "raise"))
762 
763     try:
764         if y_test is None:
765             scores = scorer(estimator, X_test)
766         else:
767             scores = scorer(estimator, X_test, y_test)
768     except Exception:
769         if isinstance(scorer, _MultimetricScorer):
770             # If `_MultimetricScorer` raises exception, the `error_score`
771             # parameter is equal to "raise".
772             raise
773         else:
774             if error_score == "raise":
775                 raise
776             else:
777                 scores = error_score
778                 warnings.warn(
779                     "Scoring failed. The score on this train-test partition for "
780                     f"these parameters will be set to {error_score}. Details: \n"
781                     f"{format_exc()}",
782                     UserWarning,
783                 )
784 
785     # Check non-raised error messages in `_MultimetricScorer`
786     if isinstance(scorer, _MultimetricScorer):
787         exception_messages = [
788             (name, str_e) for name, str_e in scores.items() if isinstance(str_e, str)
789         ]
790         if exception_messages:
791             # error_score != "raise"
792             for name, str_e in exception_messages:
793                 scores[name] = error_score
794                 warnings.warn(
795                     "Scoring failed. The score on this train-test partition for "
796                     f"these parameters will be set to {error_score}. Details: \n"
797                     f"{str_e}",
798                     UserWarning,
799                 )
800 
801     error_msg = "scoring must return a number, got %s (%s) instead. (scorer=%s)"
802     if isinstance(scores, dict):
803         for name, score in scores.items():
804             if hasattr(score, "item"):
805                 with suppress(ValueError):
806                     # e.g. unwrap memmapped scalars
807                     score = score.item()
808             if not isinstance(score, numbers.Number):
809                 raise ValueError(error_msg % (score, type(score), name))
810             scores[name] = score
811     else:  # scalar
812         if hasattr(scores, "item"):
813             with suppress(ValueError):
814                 # e.g. unwrap memmapped scalars
815                 scores = scores.item()
816         if not isinstance(scores, numbers.Number):
817             raise ValueError(error_msg % (scores, type(scores), scorer))
818     return scores
819 
820 
821 def cross_val_predict(
822     estimator,
823     X,
824     y=None,
825     *,
826     groups=None,
827     cv=None,
828     n_jobs=None,
829     verbose=0,
830     fit_params=None,
831     pre_dispatch="2*n_jobs",
832     method="predict",
833 ):
834     """Generate cross-validated estimates for each input data point.
835 
836     The data is split according to the cv parameter. Each sample belongs
837     to exactly one test set, and its prediction is computed with an
838     estimator fitted on the corresponding training set.
839 
840     Passing these predictions into an evaluation metric may not be a valid
841     way to measure generalization performance. Results can differ from
842     :func:`cross_validate` and :func:`cross_val_score` unless all tests sets
843     have equal size and the metric decomposes over samples.
844 
845     Read more in the :ref:`User Guide <cross_validation>`.
846 
847     Parameters
848     ----------
849     estimator : estimator object implementing 'fit' and 'predict'
850         The object to use to fit the data.
851 
852     X : array-like of shape (n_samples, n_features)
853         The data to fit. Can be, for example a list, or an array at least 2d.
854 
855     y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
856             default=None
857         The target variable to try to predict in the case of
858         supervised learning.
859 
860     groups : array-like of shape (n_samples,), default=None
861         Group labels for the samples used while splitting the dataset into
862         train/test set. Only used in conjunction with a "Group" :term:`cv`
863         instance (e.g., :class:`GroupKFold`).
864 
865     cv : int, cross-validation generator or an iterable, default=None
866         Determines the cross-validation splitting strategy.
867         Possible inputs for cv are:
868 
869         - None, to use the default 5-fold cross validation,
870         - int, to specify the number of folds in a `(Stratified)KFold`,
871         - :term:`CV splitter`,
872         - An iterable that generates (train, test) splits as arrays of indices.
873 
874         For int/None inputs, if the estimator is a classifier and ``y`` is
875         either binary or multiclass, :class:`StratifiedKFold` is used. In all
876         other cases, :class:`KFold` is used. These splitters are instantiated
877         with `shuffle=False` so the splits will be the same across calls.
878 
879         Refer :ref:`User Guide <cross_validation>` for the various
880         cross-validation strategies that can be used here.
881 
882         .. versionchanged:: 0.22
883             ``cv`` default value if None changed from 3-fold to 5-fold.
884 
885     n_jobs : int, default=None
886         Number of jobs to run in parallel. Training the estimator and
887         predicting are parallelized over the cross-validation splits.
888         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
889         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
890         for more details.
891 
892     verbose : int, default=0
893         The verbosity level.
894 
895     fit_params : dict, default=None
896         Parameters to pass to the fit method of the estimator.
897 
898     pre_dispatch : int or str, default='2*n_jobs'
899         Controls the number of jobs that get dispatched during parallel
900         execution. Reducing this number can be useful to avoid an
901         explosion of memory consumption when more jobs get dispatched
902         than CPUs can process. This parameter can be:
903 
904             - None, in which case all the jobs are immediately
905               created and spawned. Use this for lightweight and
906               fast-running jobs, to avoid delays due to on-demand
907               spawning of the jobs
908 
909             - An int, giving the exact number of total jobs that are
910               spawned
911 
912             - A str, giving an expression as a function of n_jobs,
913               as in '2*n_jobs'
914 
915     method : {'predict', 'predict_proba', 'predict_log_proba', \
916               'decision_function'}, default='predict'
917         The method to be invoked by `estimator`.
918 
919     Returns
920     -------
921     predictions : ndarray
922         This is the result of calling `method`. Shape:
923 
924             - When `method` is 'predict' and in special case where `method` is
925               'decision_function' and the target is binary: (n_samples,)
926             - When `method` is one of {'predict_proba', 'predict_log_proba',
927               'decision_function'} (unless special case above):
928               (n_samples, n_classes)
929             - If `estimator` is :term:`multioutput`, an extra dimension
930               'n_outputs' is added to the end of each shape above.
931 
932     See Also
933     --------
934     cross_val_score : Calculate score for each CV split.
935     cross_validate : Calculate one or more scores and timings for each CV
936         split.
937 
938     Notes
939     -----
940     In the case that one or more classes are absent in a training portion, a
941     default score needs to be assigned to all instances for that class if
942     ``method`` produces columns per class, as in {'decision_function',
943     'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is
944     0.  In order to ensure finite output, we approximate negative infinity by
945     the minimum finite float value for the dtype in other cases.
946 
947     Examples
948     --------
949     >>> from sklearn import datasets, linear_model
950     >>> from sklearn.model_selection import cross_val_predict
951     >>> diabetes = datasets.load_diabetes()
952     >>> X = diabetes.data[:150]
953     >>> y = diabetes.target[:150]
954     >>> lasso = linear_model.Lasso()
955     >>> y_pred = cross_val_predict(lasso, X, y, cv=3)
956     """
957     X, y, groups = indexable(X, y, groups)
958 
959     cv = check_cv(cv, y, classifier=is_classifier(estimator))
960     splits = list(cv.split(X, y, groups))
961 
962     test_indices = np.concatenate([test for _, test in splits])
963     if not _check_is_permutation(test_indices, _num_samples(X)):
964         raise ValueError("cross_val_predict only works for partitions")
965 
966     # If classification methods produce multiple columns of output,
967     # we need to manually encode classes to ensure consistent column ordering.
968     encode = (
969         method in ["decision_function", "predict_proba", "predict_log_proba"]
970         and y is not None
971     )
972     if encode:
973         y = np.asarray(y)
974         if y.ndim == 1:
975             le = LabelEncoder()
976             y = le.fit_transform(y)
977         elif y.ndim == 2:
978             y_enc = np.zeros_like(y, dtype=int)
979             for i_label in range(y.shape[1]):
980                 y_enc[:, i_label] = LabelEncoder().fit_transform(y[:, i_label])
981             y = y_enc
982 
983     # We clone the estimator to make sure that all the folds are
984     # independent, and that it is pickle-able.
985     parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
986     predictions = parallel(
987         delayed(_fit_and_predict)(
988             clone(estimator), X, y, train, test, verbose, fit_params, method
989         )
990         for train, test in splits
991     )
992 
993     inv_test_indices = np.empty(len(test_indices), dtype=int)
994     inv_test_indices[test_indices] = np.arange(len(test_indices))
995 
996     if sp.issparse(predictions[0]):
997         predictions = sp.vstack(predictions, format=predictions[0].format)
998     elif encode and isinstance(predictions[0], list):
999         # `predictions` is a list of method outputs from each fold.
1000         # If each of those is also a list, then treat this as a
1001         # multioutput-multiclass task. We need to separately concatenate
1002         # the method outputs for each label into an `n_labels` long list.
1003         n_labels = y.shape[1]
1004         concat_pred = []
1005         for i_label in range(n_labels):
1006             label_preds = np.concatenate([p[i_label] for p in predictions])
1007             concat_pred.append(label_preds)
1008         predictions = concat_pred
1009     else:
1010         predictions = np.concatenate(predictions)
1011 
1012     if isinstance(predictions, list):
1013         return [p[inv_test_indices] for p in predictions]
1014     else:
1015         return predictions[inv_test_indices]
1016 
1017 
1018 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params, method):
1019     """Fit estimator and predict values for a given dataset split.
1020 
1021     Read more in the :ref:`User Guide <cross_validation>`.
1022 
1023     Parameters
1024     ----------
1025     estimator : estimator object implementing 'fit' and 'predict'
1026         The object to use to fit the data.
1027 
1028     X : array-like of shape (n_samples, n_features)
1029         The data to fit.
1030 
1031         .. versionchanged:: 0.20
1032             X is only required to be an object with finite length or shape now
1033 
1034     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
1035         The target variable to try to predict in the case of
1036         supervised learning.
1037 
1038     train : array-like of shape (n_train_samples,)
1039         Indices of training samples.
1040 
1041     test : array-like of shape (n_test_samples,)
1042         Indices of test samples.
1043 
1044     verbose : int
1045         The verbosity level.
1046 
1047     fit_params : dict or None
1048         Parameters that will be passed to ``estimator.fit``.
1049 
1050     method : str
1051         Invokes the passed method name of the passed estimator.
1052 
1053     Returns
1054     -------
1055     predictions : sequence
1056         Result of calling 'estimator.method'
1057     """
1058     # Adjust length of sample weights
1059     fit_params = fit_params if fit_params is not None else {}
1060     fit_params = _check_fit_params(X, fit_params, train)
1061 
1062     X_train, y_train = _safe_split(estimator, X, y, train)
1063     X_test, _ = _safe_split(estimator, X, y, test, train)
1064 
1065     if y_train is None:
1066         estimator.fit(X_train, **fit_params)
1067     else:
1068         estimator.fit(X_train, y_train, **fit_params)
1069     func = getattr(estimator, method)
1070     predictions = func(X_test)
1071 
1072     encode = (
1073         method in ["decision_function", "predict_proba", "predict_log_proba"]
1074         and y is not None
1075     )
1076 
1077     if encode:
1078         if isinstance(predictions, list):
1079             predictions = [
1080                 _enforce_prediction_order(
1081                     estimator.classes_[i_label],
1082                     predictions[i_label],
1083                     n_classes=len(set(y[:, i_label])),
1084                     method=method,
1085                 )
1086                 for i_label in range(len(predictions))
1087             ]
1088         else:
1089             # A 2D y array should be a binary label indicator matrix
1090             n_classes = len(set(y)) if y.ndim == 1 else y.shape[1]
1091             predictions = _enforce_prediction_order(
1092                 estimator.classes_, predictions, n_classes, method
1093             )
1094     return predictions
1095 
1096 
1097 def _enforce_prediction_order(classes, predictions, n_classes, method):
1098     """Ensure that prediction arrays have correct column order
1099 
1100     When doing cross-validation, if one or more classes are
1101     not present in the subset of data used for training,
1102     then the output prediction array might not have the same
1103     columns as other folds. Use the list of class names
1104     (assumed to be ints) to enforce the correct column order.
1105 
1106     Note that `classes` is the list of classes in this fold
1107     (a subset of the classes in the full training set)
1108     and `n_classes` is the number of classes in the full training set.
1109     """
1110     if n_classes != len(classes):
1111         recommendation = (
1112             "To fix this, use a cross-validation "
1113             "technique resulting in properly "
1114             "stratified folds"
1115         )
1116         warnings.warn(
1117             "Number of classes in training fold ({}) does "
1118             "not match total number of classes ({}). "
1119             "Results may not be appropriate for your use case. "
1120             "{}".format(len(classes), n_classes, recommendation),
1121             RuntimeWarning,
1122         )
1123         if method == "decision_function":
1124             if predictions.ndim == 2 and predictions.shape[1] != len(classes):
1125                 # This handles the case when the shape of predictions
1126                 # does not match the number of classes used to train
1127                 # it with. This case is found when sklearn.svm.SVC is
1128                 # set to `decision_function_shape='ovo'`.
1129                 raise ValueError(
1130                     "Output shape {} of {} does not match "
1131                     "number of classes ({}) in fold. "
1132                     "Irregular decision_function outputs "
1133                     "are not currently supported by "
1134                     "cross_val_predict".format(predictions.shape, method, len(classes))
1135                 )
1136             if len(classes) <= 2:
1137                 # In this special case, `predictions` contains a 1D array.
1138                 raise ValueError(
1139                     "Only {} class/es in training fold, but {} "
1140                     "in overall dataset. This "
1141                     "is not supported for decision_function "
1142                     "with imbalanced folds. {}".format(
1143                         len(classes), n_classes, recommendation
1144                     )
1145                 )
1146 
1147         float_min = np.finfo(predictions.dtype).min
1148         default_values = {
1149             "decision_function": float_min,
1150             "predict_log_proba": float_min,
1151             "predict_proba": 0,
1152         }
1153         predictions_for_all_classes = np.full(
1154             (_num_samples(predictions), n_classes),
1155             default_values[method],
1156             dtype=predictions.dtype,
1157         )
1158         predictions_for_all_classes[:, classes] = predictions
1159         predictions = predictions_for_all_classes
1160     return predictions
1161 
1162 
1163 def _check_is_permutation(indices, n_samples):
1164     """Check whether indices is a reordering of the array np.arange(n_samples)
1165 
1166     Parameters
1167     ----------
1168     indices : ndarray
1169         int array to test
1170     n_samples : int
1171         number of expected elements
1172 
1173     Returns
1174     -------
1175     is_partition : bool
1176         True iff sorted(indices) is np.arange(n)
1177     """
1178     if len(indices) != n_samples:
1179         return False
1180     hit = np.zeros(n_samples, dtype=bool)
1181     hit[indices] = True
1182     if not np.all(hit):
1183         return False
1184     return True
1185 
1186 
1187 def permutation_test_score(
1188     estimator,
1189     X,
1190     y,
1191     *,
1192     groups=None,
1193     cv=None,
1194     n_permutations=100,
1195     n_jobs=None,
1196     random_state=0,
1197     verbose=0,
1198     scoring=None,
1199     fit_params=None,
1200 ):
1201     """Evaluate the significance of a cross-validated score with permutations.
1202 
1203     Permutes targets to generate 'randomized data' and compute the empirical
1204     p-value against the null hypothesis that features and targets are
1205     independent.
1206 
1207     The p-value represents the fraction of randomized data sets where the
1208     estimator performed as well or better than in the original data. A small
1209     p-value suggests that there is a real dependency between features and
1210     targets which has been used by the estimator to give good predictions.
1211     A large p-value may be due to lack of real dependency between features
1212     and targets or the estimator was not able to use the dependency to
1213     give good predictions.
1214 
1215     Read more in the :ref:`User Guide <permutation_test_score>`.
1216 
1217     Parameters
1218     ----------
1219     estimator : estimator object implementing 'fit'
1220         The object to use to fit the data.
1221 
1222     X : array-like of shape at least 2D
1223         The data to fit.
1224 
1225     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
1226         The target variable to try to predict in the case of
1227         supervised learning.
1228 
1229     groups : array-like of shape (n_samples,), default=None
1230         Labels to constrain permutation within groups, i.e. ``y`` values
1231         are permuted among samples with the same group identifier.
1232         When not specified, ``y`` values are permuted among all samples.
1233 
1234         When a grouped cross-validator is used, the group labels are
1235         also passed on to the ``split`` method of the cross-validator. The
1236         cross-validator uses them for grouping the samples  while splitting
1237         the dataset into train/test set.
1238 
1239     cv : int, cross-validation generator or an iterable, default=None
1240         Determines the cross-validation splitting strategy.
1241         Possible inputs for cv are:
1242 
1243         - `None`, to use the default 5-fold cross validation,
1244         - int, to specify the number of folds in a `(Stratified)KFold`,
1245         - :term:`CV splitter`,
1246         - An iterable yielding (train, test) splits as arrays of indices.
1247 
1248         For `int`/`None` inputs, if the estimator is a classifier and `y` is
1249         either binary or multiclass, :class:`StratifiedKFold` is used. In all
1250         other cases, :class:`KFold` is used. These splitters are instantiated
1251         with `shuffle=False` so the splits will be the same across calls.
1252 
1253         Refer :ref:`User Guide <cross_validation>` for the various
1254         cross-validation strategies that can be used here.
1255 
1256         .. versionchanged:: 0.22
1257             `cv` default value if `None` changed from 3-fold to 5-fold.
1258 
1259     n_permutations : int, default=100
1260         Number of times to permute ``y``.
1261 
1262     n_jobs : int, default=None
1263         Number of jobs to run in parallel. Training the estimator and computing
1264         the cross-validated score are parallelized over the permutations.
1265         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1266         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1267         for more details.
1268 
1269     random_state : int, RandomState instance or None, default=0
1270         Pass an int for reproducible output for permutation of
1271         ``y`` values among samples. See :term:`Glossary <random_state>`.
1272 
1273     verbose : int, default=0
1274         The verbosity level.
1275 
1276     scoring : str or callable, default=None
1277         A single str (see :ref:`scoring_parameter`) or a callable
1278         (see :ref:`scoring`) to evaluate the predictions on the test set.
1279 
1280         If `None` the estimator's score method is used.
1281 
1282     fit_params : dict, default=None
1283         Parameters to pass to the fit method of the estimator.
1284 
1285         .. versionadded:: 0.24
1286 
1287     Returns
1288     -------
1289     score : float
1290         The true score without permuting targets.
1291 
1292     permutation_scores : array of shape (n_permutations,)
1293         The scores obtained for each permutations.
1294 
1295     pvalue : float
1296         The p-value, which approximates the probability that the score would
1297         be obtained by chance. This is calculated as:
1298 
1299         `(C + 1) / (n_permutations + 1)`
1300 
1301         Where C is the number of permutations whose score >= the true score.
1302 
1303         The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.
1304 
1305     Notes
1306     -----
1307     This function implements Test 1 in:
1308 
1309         Ojala and Garriga. `Permutation Tests for Studying Classifier
1310         Performance
1311         <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The
1312         Journal of Machine Learning Research (2010) vol. 11
1313     """
1314     X, y, groups = indexable(X, y, groups)
1315 
1316     cv = check_cv(cv, y, classifier=is_classifier(estimator))
1317     scorer = check_scoring(estimator, scoring=scoring)
1318     random_state = check_random_state(random_state)
1319 
1320     # We clone the estimator to make sure that all the folds are
1321     # independent, and that it is pickle-able.
1322     score = _permutation_test_score(
1323         clone(estimator), X, y, groups, cv, scorer, fit_params=fit_params
1324     )
1325     permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(
1326         delayed(_permutation_test_score)(
1327             clone(estimator),
1328             X,
1329             _shuffle(y, groups, random_state),
1330             groups,
1331             cv,
1332             scorer,
1333             fit_params=fit_params,
1334         )
1335         for _ in range(n_permutations)
1336     )
1337     permutation_scores = np.array(permutation_scores)
1338     pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)
1339     return score, permutation_scores, pvalue
1340 
1341 
1342 def _permutation_test_score(estimator, X, y, groups, cv, scorer, fit_params):
1343     """Auxiliary function for permutation_test_score"""
1344     # Adjust length of sample weights
1345     fit_params = fit_params if fit_params is not None else {}
1346     avg_score = []
1347     for train, test in cv.split(X, y, groups):
1348         X_train, y_train = _safe_split(estimator, X, y, train)
1349         X_test, y_test = _safe_split(estimator, X, y, test, train)
1350         fit_params = _check_fit_params(X, fit_params, train)
1351         estimator.fit(X_train, y_train, **fit_params)
1352         avg_score.append(scorer(estimator, X_test, y_test))
1353     return np.mean(avg_score)
1354 
1355 
1356 def _shuffle(y, groups, random_state):
1357     """Return a shuffled copy of y eventually shuffle among same groups."""
1358     if groups is None:
1359         indices = random_state.permutation(len(y))
1360     else:
1361         indices = np.arange(len(groups))
1362         for group in np.unique(groups):
1363             this_mask = groups == group
1364             indices[this_mask] = random_state.permutation(indices[this_mask])
1365     return _safe_indexing(y, indices)
1366 
1367 
1368 def learning_curve(
1369     estimator,
1370     X,
1371     y,
1372     *,
1373     groups=None,
1374     train_sizes=np.linspace(0.1, 1.0, 5),
1375     cv=None,
1376     scoring=None,
1377     exploit_incremental_learning=False,
1378     n_jobs=None,
1379     pre_dispatch="all",
1380     verbose=0,
1381     shuffle=False,
1382     random_state=None,
1383     error_score=np.nan,
1384     return_times=False,
1385     fit_params=None,
1386 ):
1387     """Learning curve.
1388 
1389     Determines cross-validated training and test scores for different training
1390     set sizes.
1391 
1392     A cross-validation generator splits the whole dataset k times in training
1393     and test data. Subsets of the training set with varying sizes will be used
1394     to train the estimator and a score for each training subset size and the
1395     test set will be computed. Afterwards, the scores will be averaged over
1396     all k runs for each training subset size.
1397 
1398     Read more in the :ref:`User Guide <learning_curve>`.
1399 
1400     Parameters
1401     ----------
1402     estimator : object type that implements the "fit" and "predict" methods
1403         An object of that type which is cloned for each validation.
1404 
1405     X : array-like of shape (n_samples, n_features)
1406         Training vector, where `n_samples` is the number of samples and
1407         `n_features` is the number of features.
1408 
1409     y : array-like of shape (n_samples,) or (n_samples, n_outputs)
1410         Target relative to X for classification or regression;
1411         None for unsupervised learning.
1412 
1413     groups : array-like of  shape (n_samples,), default=None
1414         Group labels for the samples used while splitting the dataset into
1415         train/test set. Only used in conjunction with a "Group" :term:`cv`
1416         instance (e.g., :class:`GroupKFold`).
1417 
1418     train_sizes : array-like of shape (n_ticks,), \
1419             default=np.linspace(0.1, 1.0, 5)
1420         Relative or absolute numbers of training examples that will be used to
1421         generate the learning curve. If the dtype is float, it is regarded as a
1422         fraction of the maximum size of the training set (that is determined
1423         by the selected validation method), i.e. it has to be within (0, 1].
1424         Otherwise it is interpreted as absolute sizes of the training sets.
1425         Note that for classification the number of samples usually have to
1426         be big enough to contain at least one sample from each class.
1427 
1428     cv : int, cross-validation generator or an iterable, default=None
1429         Determines the cross-validation splitting strategy.
1430         Possible inputs for cv are:
1431 
1432         - None, to use the default 5-fold cross validation,
1433         - int, to specify the number of folds in a `(Stratified)KFold`,
1434         - :term:`CV splitter`,
1435         - An iterable yielding (train, test) splits as arrays of indices.
1436 
1437         For int/None inputs, if the estimator is a classifier and ``y`` is
1438         either binary or multiclass, :class:`StratifiedKFold` is used. In all
1439         other cases, :class:`KFold` is used. These splitters are instantiated
1440         with `shuffle=False` so the splits will be the same across calls.
1441 
1442         Refer :ref:`User Guide <cross_validation>` for the various
1443         cross-validation strategies that can be used here.
1444 
1445         .. versionchanged:: 0.22
1446             ``cv`` default value if None changed from 3-fold to 5-fold.
1447 
1448     scoring : str or callable, default=None
1449         A str (see model evaluation documentation) or
1450         a scorer callable object / function with signature
1451         ``scorer(estimator, X, y)``.
1452 
1453     exploit_incremental_learning : bool, default=False
1454         If the estimator supports incremental learning, this will be
1455         used to speed up fitting for different training set sizes.
1456 
1457     n_jobs : int, default=None
1458         Number of jobs to run in parallel. Training the estimator and computing
1459         the score are parallelized over the different training and test sets.
1460         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1461         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1462         for more details.
1463 
1464     pre_dispatch : int or str, default='all'
1465         Number of predispatched jobs for parallel execution (default is
1466         all). The option can reduce the allocated memory. The str can
1467         be an expression like '2*n_jobs'.
1468 
1469     verbose : int, default=0
1470         Controls the verbosity: the higher, the more messages.
1471 
1472     shuffle : bool, default=False
1473         Whether to shuffle training data before taking prefixes of it
1474         based on``train_sizes``.
1475 
1476     random_state : int, RandomState instance or None, default=None
1477         Used when ``shuffle`` is True. Pass an int for reproducible
1478         output across multiple function calls.
1479         See :term:`Glossary <random_state>`.
1480 
1481     error_score : 'raise' or numeric, default=np.nan
1482         Value to assign to the score if an error occurs in estimator fitting.
1483         If set to 'raise', the error is raised.
1484         If a numeric value is given, FitFailedWarning is raised.
1485 
1486         .. versionadded:: 0.20
1487 
1488     return_times : bool, default=False
1489         Whether to return the fit and score times.
1490 
1491     fit_params : dict, default=None
1492         Parameters to pass to the fit method of the estimator.
1493 
1494         .. versionadded:: 0.24
1495 
1496     Returns
1497     -------
1498     train_sizes_abs : array of shape (n_unique_ticks,)
1499         Numbers of training examples that has been used to generate the
1500         learning curve. Note that the number of ticks might be less
1501         than n_ticks because duplicate entries will be removed.
1502 
1503     train_scores : array of shape (n_ticks, n_cv_folds)
1504         Scores on training sets.
1505 
1506     test_scores : array of shape (n_ticks, n_cv_folds)
1507         Scores on test set.
1508 
1509     fit_times : array of shape (n_ticks, n_cv_folds)
1510         Times spent for fitting in seconds. Only present if ``return_times``
1511         is True.
1512 
1513     score_times : array of shape (n_ticks, n_cv_folds)
1514         Times spent for scoring in seconds. Only present if ``return_times``
1515         is True.
1516 
1517     Examples
1518     --------
1519     >>> from sklearn.datasets import make_classification
1520     >>> from sklearn.tree import DecisionTreeClassifier
1521     >>> from sklearn.model_selection import learning_curve
1522     >>> X, y = make_classification(n_samples=100, n_features=10, random_state=42)
1523     >>> tree = DecisionTreeClassifier(max_depth=4, random_state=42)
1524     >>> train_size_abs, train_scores, test_scores = learning_curve(
1525     ...     tree, X, y, train_sizes=[0.3, 0.6, 0.9]
1526     ... )
1527     >>> for train_size, cv_train_scores, cv_test_scores in zip(
1528     ...     train_size_abs, train_scores, test_scores
1529     ... ):
1530     ...     print(f"{train_size} samples were used to train the model")
1531     ...     print(f"The average train accuracy is {cv_train_scores.mean():.2f}")
1532     ...     print(f"The average test accuracy is {cv_test_scores.mean():.2f}")
1533     24 samples were used to train the model
1534     The average train accuracy is 1.00
1535     The average test accuracy is 0.85
1536     48 samples were used to train the model
1537     The average train accuracy is 1.00
1538     The average test accuracy is 0.90
1539     72 samples were used to train the model
1540     The average train accuracy is 1.00
1541     The average test accuracy is 0.93
1542     """
1543     if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):
1544         raise ValueError(
1545             "An estimator must support the partial_fit interface "
1546             "to exploit incremental learning"
1547         )
1548     X, y, groups = indexable(X, y, groups)
1549 
1550     cv = check_cv(cv, y, classifier=is_classifier(estimator))
1551     # Store it as list as we will be iterating over the list multiple times
1552     cv_iter = list(cv.split(X, y, groups))
1553 
1554     scorer = check_scoring(estimator, scoring=scoring)
1555 
1556     n_max_training_samples = len(cv_iter[0][0])
1557     # Because the lengths of folds can be significantly different, it is
1558     # not guaranteed that we use all of the available training data when we
1559     # use the first 'n_max_training_samples' samples.
1560     train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)
1561     n_unique_ticks = train_sizes_abs.shape[0]
1562     if verbose > 0:
1563         print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
1564 
1565     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
1566 
1567     if shuffle:
1568         rng = check_random_state(random_state)
1569         cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)
1570 
1571     if exploit_incremental_learning:
1572         classes = np.unique(y) if is_classifier(estimator) else None
1573         out = parallel(
1574             delayed(_incremental_fit_estimator)(
1575                 clone(estimator),
1576                 X,
1577                 y,
1578                 classes,
1579                 train,
1580                 test,
1581                 train_sizes_abs,
1582                 scorer,
1583                 verbose,
1584                 return_times,
1585                 error_score=error_score,
1586                 fit_params=fit_params,
1587             )
1588             for train, test in cv_iter
1589         )
1590         out = np.asarray(out).transpose((2, 1, 0))
1591     else:
1592         train_test_proportions = []
1593         for train, test in cv_iter:
1594             for n_train_samples in train_sizes_abs:
1595                 train_test_proportions.append((train[:n_train_samples], test))
1596 
1597         results = parallel(
1598             delayed(_fit_and_score)(
1599                 clone(estimator),
1600                 X,
1601                 y,
1602                 scorer,
1603                 train,
1604                 test,
1605                 verbose,
1606                 parameters=None,
1607                 fit_params=fit_params,
1608                 return_train_score=True,
1609                 error_score=error_score,
1610                 return_times=return_times,
1611             )
1612             for train, test in train_test_proportions
1613         )
1614         results = _aggregate_score_dicts(results)
1615         train_scores = results["train_scores"].reshape(-1, n_unique_ticks).T
1616         test_scores = results["test_scores"].reshape(-1, n_unique_ticks).T
1617         out = [train_scores, test_scores]
1618 
1619         if return_times:
1620             fit_times = results["fit_time"].reshape(-1, n_unique_ticks).T
1621             score_times = results["score_time"].reshape(-1, n_unique_ticks).T
1622             out.extend([fit_times, score_times])
1623 
1624     ret = train_sizes_abs, out[0], out[1]
1625 
1626     if return_times:
1627         ret = ret + (out[2], out[3])
1628 
1629     return ret
1630 
1631 
1632 def _translate_train_sizes(train_sizes, n_max_training_samples):
1633     """Determine absolute sizes of training subsets and validate 'train_sizes'.
1634 
1635     Examples:
1636         _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]
1637         _translate_train_sizes([5, 10], 10) -> [5, 10]
1638 
1639     Parameters
1640     ----------
1641     train_sizes : array-like of shape (n_ticks,)
1642         Numbers of training examples that will be used to generate the
1643         learning curve. If the dtype is float, it is regarded as a
1644         fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].
1645 
1646     n_max_training_samples : int
1647         Maximum number of training samples (upper bound of 'train_sizes').
1648 
1649     Returns
1650     -------
1651     train_sizes_abs : array of shape (n_unique_ticks,)
1652         Numbers of training examples that will be used to generate the
1653         learning curve. Note that the number of ticks might be less
1654         than n_ticks because duplicate entries will be removed.
1655     """
1656     train_sizes_abs = np.asarray(train_sizes)
1657     n_ticks = train_sizes_abs.shape[0]
1658     n_min_required_samples = np.min(train_sizes_abs)
1659     n_max_required_samples = np.max(train_sizes_abs)
1660     if np.issubdtype(train_sizes_abs.dtype, np.floating):
1661         if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:
1662             raise ValueError(
1663                 "train_sizes has been interpreted as fractions "
1664                 "of the maximum number of training samples and "
1665                 "must be within (0, 1], but is within [%f, %f]."
1666                 % (n_min_required_samples, n_max_required_samples)
1667             )
1668         train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(
1669             dtype=int, copy=False
1670         )
1671         train_sizes_abs = np.clip(train_sizes_abs, 1, n_max_training_samples)
1672     else:
1673         if (
1674             n_min_required_samples <= 0
1675             or n_max_required_samples > n_max_training_samples
1676         ):
1677             raise ValueError(
1678                 "train_sizes has been interpreted as absolute "
1679                 "numbers of training samples and must be within "
1680                 "(0, %d], but is within [%d, %d]."
1681                 % (
1682                     n_max_training_samples,
1683                     n_min_required_samples,
1684                     n_max_required_samples,
1685                 )
1686             )
1687 
1688     train_sizes_abs = np.unique(train_sizes_abs)
1689     if n_ticks > train_sizes_abs.shape[0]:
1690         warnings.warn(
1691             "Removed duplicate entries from 'train_sizes'. Number "
1692             "of ticks will be less than the size of "
1693             "'train_sizes': %d instead of %d." % (train_sizes_abs.shape[0], n_ticks),
1694             RuntimeWarning,
1695         )
1696 
1697     return train_sizes_abs
1698 
1699 
1700 def _incremental_fit_estimator(
1701     estimator,
1702     X,
1703     y,
1704     classes,
1705     train,
1706     test,
1707     train_sizes,
1708     scorer,
1709     verbose,
1710     return_times,
1711     error_score,
1712     fit_params,
1713 ):
1714     """Train estimator on training subsets incrementally and compute scores."""
1715     train_scores, test_scores, fit_times, score_times = [], [], [], []
1716     partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])
1717     if fit_params is None:
1718         fit_params = {}
1719     if classes is None:
1720         partial_fit_func = partial(estimator.partial_fit, **fit_params)
1721     else:
1722         partial_fit_func = partial(estimator.partial_fit, classes=classes, **fit_params)
1723 
1724     for n_train_samples, partial_train in partitions:
1725         train_subset = train[:n_train_samples]
1726         X_train, y_train = _safe_split(estimator, X, y, train_subset)
1727         X_partial_train, y_partial_train = _safe_split(estimator, X, y, partial_train)
1728         X_test, y_test = _safe_split(estimator, X, y, test, train_subset)
1729         start_fit = time.time()
1730         if y_partial_train is None:
1731             partial_fit_func(X_partial_train)
1732         else:
1733             partial_fit_func(X_partial_train, y_partial_train)
1734         fit_time = time.time() - start_fit
1735         fit_times.append(fit_time)
1736 
1737         start_score = time.time()
1738 
1739         test_scores.append(_score(estimator, X_test, y_test, scorer, error_score))
1740         train_scores.append(_score(estimator, X_train, y_train, scorer, error_score))
1741 
1742         score_time = time.time() - start_score
1743         score_times.append(score_time)
1744 
1745     ret = (
1746         (train_scores, test_scores, fit_times, score_times)
1747         if return_times
1748         else (train_scores, test_scores)
1749     )
1750 
1751     return np.array(ret).T
1752 
1753 
1754 def validation_curve(
1755     estimator,
1756     X,
1757     y,
1758     *,
1759     param_name,
1760     param_range,
1761     groups=None,
1762     cv=None,
1763     scoring=None,
1764     n_jobs=None,
1765     pre_dispatch="all",
1766     verbose=0,
1767     error_score=np.nan,
1768     fit_params=None,
1769 ):
1770     """Validation curve.
1771 
1772     Determine training and test scores for varying parameter values.
1773 
1774     Compute scores for an estimator with different values of a specified
1775     parameter. This is similar to grid search with one parameter. However, this
1776     will also compute training scores and is merely a utility for plotting the
1777     results.
1778 
1779     Read more in the :ref:`User Guide <validation_curve>`.
1780 
1781     Parameters
1782     ----------
1783     estimator : object type that implements the "fit" and "predict" methods
1784         An object of that type which is cloned for each validation.
1785 
1786     X : array-like of shape (n_samples, n_features)
1787         Training vector, where `n_samples` is the number of samples and
1788         `n_features` is the number of features.
1789 
1790     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
1791         Target relative to X for classification or regression;
1792         None for unsupervised learning.
1793 
1794     param_name : str
1795         Name of the parameter that will be varied.
1796 
1797     param_range : array-like of shape (n_values,)
1798         The values of the parameter that will be evaluated.
1799 
1800     groups : array-like of shape (n_samples,), default=None
1801         Group labels for the samples used while splitting the dataset into
1802         train/test set. Only used in conjunction with a "Group" :term:`cv`
1803         instance (e.g., :class:`GroupKFold`).
1804 
1805     cv : int, cross-validation generator or an iterable, default=None
1806         Determines the cross-validation splitting strategy.
1807         Possible inputs for cv are:
1808 
1809         - None, to use the default 5-fold cross validation,
1810         - int, to specify the number of folds in a `(Stratified)KFold`,
1811         - :term:`CV splitter`,
1812         - An iterable yielding (train, test) splits as arrays of indices.
1813 
1814         For int/None inputs, if the estimator is a classifier and ``y`` is
1815         either binary or multiclass, :class:`StratifiedKFold` is used. In all
1816         other cases, :class:`KFold` is used. These splitters are instantiated
1817         with `shuffle=False` so the splits will be the same across calls.
1818 
1819         Refer :ref:`User Guide <cross_validation>` for the various
1820         cross-validation strategies that can be used here.
1821 
1822         .. versionchanged:: 0.22
1823             ``cv`` default value if None changed from 3-fold to 5-fold.
1824 
1825     scoring : str or callable, default=None
1826         A str (see model evaluation documentation) or
1827         a scorer callable object / function with signature
1828         ``scorer(estimator, X, y)``.
1829 
1830     n_jobs : int, default=None
1831         Number of jobs to run in parallel. Training the estimator and computing
1832         the score are parallelized over the combinations of each parameter
1833         value and each cross-validation split.
1834         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1835         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1836         for more details.
1837 
1838     pre_dispatch : int or str, default='all'
1839         Number of predispatched jobs for parallel execution (default is
1840         all). The option can reduce the allocated memory. The str can
1841         be an expression like '2*n_jobs'.
1842 
1843     verbose : int, default=0
1844         Controls the verbosity: the higher, the more messages.
1845 
1846     error_score : 'raise' or numeric, default=np.nan
1847         Value to assign to the score if an error occurs in estimator fitting.
1848         If set to 'raise', the error is raised.
1849         If a numeric value is given, FitFailedWarning is raised.
1850 
1851         .. versionadded:: 0.20
1852 
1853     fit_params : dict, default=None
1854         Parameters to pass to the fit method of the estimator.
1855 
1856         .. versionadded:: 0.24
1857 
1858     Returns
1859     -------
1860     train_scores : array of shape (n_ticks, n_cv_folds)
1861         Scores on training sets.
1862 
1863     test_scores : array of shape (n_ticks, n_cv_folds)
1864         Scores on test set.
1865 
1866     Notes
1867     -----
1868     See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`
1869     """
1870     X, y, groups = indexable(X, y, groups)
1871 
1872     cv = check_cv(cv, y, classifier=is_classifier(estimator))
1873     scorer = check_scoring(estimator, scoring=scoring)
1874 
1875     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
1876     results = parallel(
1877         delayed(_fit_and_score)(
1878             clone(estimator),
1879             X,
1880             y,
1881             scorer,
1882             train,
1883             test,
1884             verbose,
1885             parameters={param_name: v},
1886             fit_params=fit_params,
1887             return_train_score=True,
1888             error_score=error_score,
1889         )
1890         # NOTE do not change order of iteration to allow one time cv splitters
1891         for train, test in cv.split(X, y, groups)
1892         for v in param_range
1893     )
1894     n_params = len(param_range)
1895 
1896     results = _aggregate_score_dicts(results)
1897     train_scores = results["train_scores"].reshape(-1, n_params).T
1898     test_scores = results["test_scores"].reshape(-1, n_params).T
1899 
1900     return train_scores, test_scores
1901 
1902 
1903 def _aggregate_score_dicts(scores):
1904     """Aggregate the list of dict to dict of np ndarray
1905 
1906     The aggregated output of _aggregate_score_dicts will be a list of dict
1907     of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]
1908     Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}
1909 
1910     Parameters
1911     ----------
1912 
1913     scores : list of dict
1914         List of dicts of the scores for all scorers. This is a flat list,
1915         assumed originally to be of row major order.
1916 
1917     Example
1918     -------
1919 
1920     >>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3},
1921     ...           {'a': 10, 'b': 10}]                         # doctest: +SKIP
1922     >>> _aggregate_score_dicts(scores)                        # doctest: +SKIP
1923     {'a': array([1, 2, 3, 10]),
1924      'b': array([10, 2, 3, 10])}
1925     """
1926     return {
1927         key: np.asarray([score[key] for score in scores])
1928         if isinstance(scores[0][key], numbers.Number)
1929         else [score[key] for score in scores]
1930         for key in scores[0]
1931     }

</code><intended_behavior>The intended behavior here is to ensure that configurations, especially those crucial for the operation of Parallel executed tasks, are explicitly passed to each delayed call. This might require wrapping the delayed function calls within a context that ensures the current configuration state is captured and then applied within each parallelly executed task. In essence, the fix involves ensuring that any configuration context is not just assumed but is explicitly conveyed and reconstructed within each task's execution environment.</intended_behavior>, <file>sklearn/ensemble/_forest.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Forest of trees-based ensemble methods.
3 
4 Those methods include random forests and extremely randomized trees.
5 
6 The module structure is the following:
7 
8 - The ``BaseForest`` base class implements a common ``fit`` method for all
9   the estimators in the module. The ``fit`` method of the base ``Forest``
10   class calls the ``fit`` method of each sub-estimator on random samples
11   (with replacement, a.k.a. bootstrap) of the training set.
12 
13   The init of the sub-estimator is further delegated to the
14   ``BaseEnsemble`` constructor.
15 
16 - The ``ForestClassifier`` and ``ForestRegressor`` base classes further
17   implement the prediction logic by computing an average of the predicted
18   outcomes of the sub-estimators.
19 
20 - The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived
21   classes provide the user with concrete implementations of
22   the forest ensemble method using classical, deterministic
23   ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as
24   sub-estimator implementations.
25 
26 - The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived
27   classes provide the user with concrete implementations of the
28   forest ensemble method using the extremely randomized trees
29   ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as
30   sub-estimator implementations.
31 
32 Single and multi-output problems are both handled.
33 """
34 
35 # Authors: Gilles Louppe <g.louppe@gmail.com>
36 #          Brian Holt <bdholt1@gmail.com>
37 #          Joly Arnaud <arnaud.v.joly@gmail.com>
38 #          Fares Hedayati <fares.hedayati@gmail.com>
39 #
40 # License: BSD 3 clause
41 
42 
43 from numbers import Integral, Real
44 from warnings import catch_warnings, simplefilter, warn
45 import threading
46 
47 from abc import ABCMeta, abstractmethod
48 import numpy as np
49 from scipy.sparse import issparse
50 from scipy.sparse import hstack as sparse_hstack
51 from joblib import Parallel
52 
53 from ..base import is_classifier
54 from ..base import ClassifierMixin, MultiOutputMixin, RegressorMixin, TransformerMixin
55 
56 from ..metrics import accuracy_score, r2_score
57 from ..preprocessing import OneHotEncoder
58 from ..tree import (
59     BaseDecisionTree,
60     DecisionTreeClassifier,
61     DecisionTreeRegressor,
62     ExtraTreeClassifier,
63     ExtraTreeRegressor,
64 )
65 from ..tree._tree import DTYPE, DOUBLE
66 from ..utils import check_random_state, compute_sample_weight
67 from ..exceptions import DataConversionWarning
68 from ._base import BaseEnsemble, _partition_estimators
69 from ..utils.fixes import delayed
70 from ..utils.multiclass import check_classification_targets, type_of_target
71 from ..utils.validation import (
72     check_is_fitted,
73     _check_sample_weight,
74     _check_feature_names_in,
75 )
76 from ..utils.validation import _num_samples
77 from ..utils._param_validation import Interval, StrOptions
78 
79 
80 __all__ = [
81     "RandomForestClassifier",
82     "RandomForestRegressor",
83     "ExtraTreesClassifier",
84     "ExtraTreesRegressor",
85     "RandomTreesEmbedding",
86 ]
87 
88 MAX_INT = np.iinfo(np.int32).max
89 
90 
91 def _get_n_samples_bootstrap(n_samples, max_samples):
92     """
93     Get the number of samples in a bootstrap sample.
94 
95     Parameters
96     ----------
97     n_samples : int
98         Number of samples in the dataset.
99     max_samples : int or float
100         The maximum number of samples to draw from the total available:
101             - if float, this indicates a fraction of the total and should be
102               the interval `(0.0, 1.0]`;
103             - if int, this indicates the exact number of samples;
104             - if None, this indicates the total number of samples.
105 
106     Returns
107     -------
108     n_samples_bootstrap : int
109         The total number of samples to draw for the bootstrap sample.
110     """
111     if max_samples is None:
112         return n_samples
113 
114     if isinstance(max_samples, Integral):
115         if max_samples > n_samples:
116             msg = "`max_samples` must be <= n_samples={} but got value {}"
117             raise ValueError(msg.format(n_samples, max_samples))
118         return max_samples
119 
120     if isinstance(max_samples, Real):
121         return round(n_samples * max_samples)
122 
123 
124 def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):
125     """
126     Private function used to _parallel_build_trees function."""
127 
128     random_instance = check_random_state(random_state)
129     sample_indices = random_instance.randint(0, n_samples, n_samples_bootstrap)
130 
131     return sample_indices
132 
133 
134 def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):
135     """
136     Private function used to forest._set_oob_score function."""
137     sample_indices = _generate_sample_indices(
138         random_state, n_samples, n_samples_bootstrap
139     )
140     sample_counts = np.bincount(sample_indices, minlength=n_samples)
141     unsampled_mask = sample_counts == 0
142     indices_range = np.arange(n_samples)
143     unsampled_indices = indices_range[unsampled_mask]
144 
145     return unsampled_indices
146 
147 
148 def _parallel_build_trees(
149     tree,
150     bootstrap,
151     X,
152     y,
153     sample_weight,
154     tree_idx,
155     n_trees,
156     verbose=0,
157     class_weight=None,
158     n_samples_bootstrap=None,
159 ):
160     """
161     Private function used to fit a single tree in parallel."""
162     if verbose > 1:
163         print("building tree %d of %d" % (tree_idx + 1, n_trees))
164 
165     if bootstrap:
166         n_samples = X.shape[0]
167         if sample_weight is None:
168             curr_sample_weight = np.ones((n_samples,), dtype=np.float64)
169         else:
170             curr_sample_weight = sample_weight.copy()
171 
172         indices = _generate_sample_indices(
173             tree.random_state, n_samples, n_samples_bootstrap
174         )
175         sample_counts = np.bincount(indices, minlength=n_samples)
176         curr_sample_weight *= sample_counts
177 
178         if class_weight == "subsample":
179             with catch_warnings():
180                 simplefilter("ignore", DeprecationWarning)
181                 curr_sample_weight *= compute_sample_weight("auto", y, indices=indices)
182         elif class_weight == "balanced_subsample":
183             curr_sample_weight *= compute_sample_weight("balanced", y, indices=indices)
184 
185         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
186     else:
187         tree.fit(X, y, sample_weight=sample_weight, check_input=False)
188 
189     return tree
190 
191 
192 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):
193     """
194     Base class for forests of trees.
195 
196     Warning: This class should not be used directly. Use derived classes
197     instead.
198     """
199 
200     _parameter_constraints: dict = {
201         "n_estimators": [Interval(Integral, 1, None, closed="left")],
202         "bootstrap": ["boolean"],
203         "oob_score": ["boolean", callable],
204         "n_jobs": [Integral, None],
205         "random_state": ["random_state"],
206         "verbose": ["verbose"],
207         "warm_start": ["boolean"],
208         "max_samples": [
209             None,
210             Interval(Real, 0.0, 1.0, closed="right"),
211             Interval(Integral, 1, None, closed="left"),
212         ],
213     }
214 
215     @abstractmethod
216     def __init__(
217         self,
218         estimator,
219         n_estimators=100,
220         *,
221         estimator_params=tuple(),
222         bootstrap=False,
223         oob_score=False,
224         n_jobs=None,
225         random_state=None,
226         verbose=0,
227         warm_start=False,
228         class_weight=None,
229         max_samples=None,
230         base_estimator="deprecated",
231     ):
232         super().__init__(
233             estimator=estimator,
234             n_estimators=n_estimators,
235             estimator_params=estimator_params,
236             base_estimator=base_estimator,
237         )
238 
239         self.bootstrap = bootstrap
240         self.oob_score = oob_score
241         self.n_jobs = n_jobs
242         self.random_state = random_state
243         self.verbose = verbose
244         self.warm_start = warm_start
245         self.class_weight = class_weight
246         self.max_samples = max_samples
247 
248     def apply(self, X):
249         """
250         Apply trees in the forest to X, return leaf indices.
251 
252         Parameters
253         ----------
254         X : {array-like, sparse matrix} of shape (n_samples, n_features)
255             The input samples. Internally, its dtype will be converted to
256             ``dtype=np.float32``. If a sparse matrix is provided, it will be
257             converted into a sparse ``csr_matrix``.
258 
259         Returns
260         -------
261         X_leaves : ndarray of shape (n_samples, n_estimators)
262             For each datapoint x in X and for each tree in the forest,
263             return the index of the leaf x ends up in.
264         """
265         X = self._validate_X_predict(X)
266         results = Parallel(
267             n_jobs=self.n_jobs,
268             verbose=self.verbose,
269             prefer="threads",
270         )(delayed(tree.apply)(X, check_input=False) for tree in self.estimators_)
271 
272         return np.array(results).T
273 
274     def decision_path(self, X):
275         """
276         Return the decision path in the forest.
277 
278         .. versionadded:: 0.18
279 
280         Parameters
281         ----------
282         X : {array-like, sparse matrix} of shape (n_samples, n_features)
283             The input samples. Internally, its dtype will be converted to
284             ``dtype=np.float32``. If a sparse matrix is provided, it will be
285             converted into a sparse ``csr_matrix``.
286 
287         Returns
288         -------
289         indicator : sparse matrix of shape (n_samples, n_nodes)
290             Return a node indicator matrix where non zero elements indicates
291             that the samples goes through the nodes. The matrix is of CSR
292             format.
293 
294         n_nodes_ptr : ndarray of shape (n_estimators + 1,)
295             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
296             gives the indicator value for the i-th estimator.
297         """
298         X = self._validate_X_predict(X)
299         indicators = Parallel(
300             n_jobs=self.n_jobs,
301             verbose=self.verbose,
302             prefer="threads",
303         )(
304             delayed(tree.decision_path)(X, check_input=False)
305             for tree in self.estimators_
306         )
307 
308         n_nodes = [0]
309         n_nodes.extend([i.shape[1] for i in indicators])
310         n_nodes_ptr = np.array(n_nodes).cumsum()
311 
312         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
313 
314     def fit(self, X, y, sample_weight=None):
315         """
316         Build a forest of trees from the training set (X, y).
317 
318         Parameters
319         ----------
320         X : {array-like, sparse matrix} of shape (n_samples, n_features)
321             The training input samples. Internally, its dtype will be converted
322             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
323             converted into a sparse ``csc_matrix``.
324 
325         y : array-like of shape (n_samples,) or (n_samples, n_outputs)
326             The target values (class labels in classification, real numbers in
327             regression).
328 
329         sample_weight : array-like of shape (n_samples,), default=None
330             Sample weights. If None, then samples are equally weighted. Splits
331             that would create child nodes with net zero or negative weight are
332             ignored while searching for a split in each node. In the case of
333             classification, splits are also ignored if they would result in any
334             single class carrying a negative weight in either child node.
335 
336         Returns
337         -------
338         self : object
339             Fitted estimator.
340         """
341         self._validate_params()
342 
343         # Validate or convert input data
344         if issparse(y):
345             raise ValueError("sparse multilabel-indicator for y is not supported.")
346         X, y = self._validate_data(
347             X, y, multi_output=True, accept_sparse="csc", dtype=DTYPE
348         )
349         if sample_weight is not None:
350             sample_weight = _check_sample_weight(sample_weight, X)
351 
352         if issparse(X):
353             # Pre-sort indices to avoid that each individual tree of the
354             # ensemble sorts the indices.
355             X.sort_indices()
356 
357         y = np.atleast_1d(y)
358         if y.ndim == 2 and y.shape[1] == 1:
359             warn(
360                 "A column-vector y was passed when a 1d array was"
361                 " expected. Please change the shape of y to "
362                 "(n_samples,), for example using ravel().",
363                 DataConversionWarning,
364                 stacklevel=2,
365             )
366 
367         if y.ndim == 1:
368             # reshape is necessary to preserve the data contiguity against vs
369             # [:, np.newaxis] that does not.
370             y = np.reshape(y, (-1, 1))
371 
372         if self.criterion == "poisson":
373             if np.any(y < 0):
374                 raise ValueError(
375                     "Some value(s) of y are negative which is "
376                     "not allowed for Poisson regression."
377                 )
378             if np.sum(y) <= 0:
379                 raise ValueError(
380                     "Sum of y is not strictly positive which "
381                     "is necessary for Poisson regression."
382                 )
383 
384         self.n_outputs_ = y.shape[1]
385 
386         y, expanded_class_weight = self._validate_y_class_weight(y)
387 
388         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
389             y = np.ascontiguousarray(y, dtype=DOUBLE)
390 
391         if expanded_class_weight is not None:
392             if sample_weight is not None:
393                 sample_weight = sample_weight * expanded_class_weight
394             else:
395                 sample_weight = expanded_class_weight
396 
397         if not self.bootstrap and self.max_samples is not None:
398             raise ValueError(
399                 "`max_sample` cannot be set if `bootstrap=False`. "
400                 "Either switch to `bootstrap=True` or set "
401                 "`max_sample=None`."
402             )
403         elif self.bootstrap:
404             n_samples_bootstrap = _get_n_samples_bootstrap(
405                 n_samples=X.shape[0], max_samples=self.max_samples
406             )
407         else:
408             n_samples_bootstrap = None
409 
410         self._validate_estimator()
411         if isinstance(self, (RandomForestRegressor, ExtraTreesRegressor)):
412             # TODO(1.3): Remove "auto"
413             if self.max_features == "auto":
414                 warn(
415                     "`max_features='auto'` has been deprecated in 1.1 "
416                     "and will be removed in 1.3. To keep the past behaviour, "
417                     "explicitly set `max_features=1.0` or remove this "
418                     "parameter as it is also the default value for "
419                     "RandomForestRegressors and ExtraTreesRegressors.",
420                     FutureWarning,
421                 )
422         elif isinstance(self, (RandomForestClassifier, ExtraTreesClassifier)):
423             # TODO(1.3): Remove "auto"
424             if self.max_features == "auto":
425                 warn(
426                     "`max_features='auto'` has been deprecated in 1.1 "
427                     "and will be removed in 1.3. To keep the past behaviour, "
428                     "explicitly set `max_features='sqrt'` or remove this "
429                     "parameter as it is also the default value for "
430                     "RandomForestClassifiers and ExtraTreesClassifiers.",
431                     FutureWarning,
432                 )
433 
434         if not self.bootstrap and self.oob_score:
435             raise ValueError("Out of bag estimation only available if bootstrap=True")
436 
437         random_state = check_random_state(self.random_state)
438 
439         if not self.warm_start or not hasattr(self, "estimators_"):
440             # Free allocated memory, if any
441             self.estimators_ = []
442 
443         n_more_estimators = self.n_estimators - len(self.estimators_)
444 
445         if n_more_estimators < 0:
446             raise ValueError(
447                 "n_estimators=%d must be larger or equal to "
448                 "len(estimators_)=%d when warm_start==True"
449                 % (self.n_estimators, len(self.estimators_))
450             )
451 
452         elif n_more_estimators == 0:
453             warn(
454                 "Warm-start fitting without increasing n_estimators does not "
455                 "fit new trees."
456             )
457         else:
458             if self.warm_start and len(self.estimators_) > 0:
459                 # We draw from the random state to get the random state we
460                 # would have got if we hadn't used a warm_start.
461                 random_state.randint(MAX_INT, size=len(self.estimators_))
462 
463             trees = [
464                 self._make_estimator(append=False, random_state=random_state)
465                 for i in range(n_more_estimators)
466             ]
467 
468             # Parallel loop: we prefer the threading backend as the Cython code
469             # for fitting the trees is internally releasing the Python GIL
470             # making threading more efficient than multiprocessing in
471             # that case. However, for joblib 0.12+ we respect any
472             # parallel_backend contexts set at a higher level,
473             # since correctness does not rely on using threads.
474             trees = Parallel(
475                 n_jobs=self.n_jobs,
476                 verbose=self.verbose,
477                 prefer="threads",
478             )(
479                 delayed(_parallel_build_trees)(
480                     t,
481                     self.bootstrap,
482                     X,
483                     y,
484                     sample_weight,
485                     i,
486                     len(trees),
487                     verbose=self.verbose,
488                     class_weight=self.class_weight,
489                     n_samples_bootstrap=n_samples_bootstrap,
490                 )
491                 for i, t in enumerate(trees)
492             )
493 
494             # Collect newly grown trees
495             self.estimators_.extend(trees)
496 
497         if self.oob_score:
498             y_type = type_of_target(y)
499             if y_type in ("multiclass-multioutput", "unknown"):
500                 # FIXME: we could consider to support multiclass-multioutput if
501                 # we introduce or reuse a constructor parameter (e.g.
502                 # oob_score) allowing our user to pass a callable defining the
503                 # scoring strategy on OOB sample.
504                 raise ValueError(
505                     "The type of target cannot be used to compute OOB "
506                     f"estimates. Got {y_type} while only the following are "
507                     "supported: continuous, continuous-multioutput, binary, "
508                     "multiclass, multilabel-indicator."
509                 )
510 
511             if callable(self.oob_score):
512                 self._set_oob_score_and_attributes(
513                     X, y, scoring_function=self.oob_score
514                 )
515             else:
516                 self._set_oob_score_and_attributes(X, y)
517 
518         # Decapsulate classes_ attributes
519         if hasattr(self, "classes_") and self.n_outputs_ == 1:
520             self.n_classes_ = self.n_classes_[0]
521             self.classes_ = self.classes_[0]
522 
523         return self
524 
525     @abstractmethod
526     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
527         """Compute and set the OOB score and attributes.
528 
529         Parameters
530         ----------
531         X : array-like of shape (n_samples, n_features)
532             The data matrix.
533         y : ndarray of shape (n_samples, n_outputs)
534             The target matrix.
535         scoring_function : callable, default=None
536             Scoring function for OOB score. Default depends on whether
537             this is a regression (R2 score) or classification problem
538             (accuracy score).
539         """
540 
541     def _compute_oob_predictions(self, X, y):
542         """Compute and set the OOB score.
543 
544         Parameters
545         ----------
546         X : array-like of shape (n_samples, n_features)
547             The data matrix.
548         y : ndarray of shape (n_samples, n_outputs)
549             The target matrix.
550 
551         Returns
552         -------
553         oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or \
554                 (n_samples, 1, n_outputs)
555             The OOB predictions.
556         """
557         # Prediction requires X to be in CSR format
558         if issparse(X):
559             X = X.tocsr()
560 
561         n_samples = y.shape[0]
562         n_outputs = self.n_outputs_
563         if is_classifier(self) and hasattr(self, "n_classes_"):
564             # n_classes_ is a ndarray at this stage
565             # all the supported type of target will have the same number of
566             # classes in all outputs
567             oob_pred_shape = (n_samples, self.n_classes_[0], n_outputs)
568         else:
569             # for regression, n_classes_ does not exist and we create an empty
570             # axis to be consistent with the classification case and make
571             # the array operations compatible with the 2 settings
572             oob_pred_shape = (n_samples, 1, n_outputs)
573 
574         oob_pred = np.zeros(shape=oob_pred_shape, dtype=np.float64)
575         n_oob_pred = np.zeros((n_samples, n_outputs), dtype=np.int64)
576 
577         n_samples_bootstrap = _get_n_samples_bootstrap(
578             n_samples,
579             self.max_samples,
580         )
581         for estimator in self.estimators_:
582             unsampled_indices = _generate_unsampled_indices(
583                 estimator.random_state,
584                 n_samples,
585                 n_samples_bootstrap,
586             )
587 
588             y_pred = self._get_oob_predictions(estimator, X[unsampled_indices, :])
589             oob_pred[unsampled_indices, ...] += y_pred
590             n_oob_pred[unsampled_indices, :] += 1
591 
592         for k in range(n_outputs):
593             if (n_oob_pred == 0).any():
594                 warn(
595                     "Some inputs do not have OOB scores. This probably means "
596                     "too few trees were used to compute any reliable OOB "
597                     "estimates.",
598                     UserWarning,
599                 )
600                 n_oob_pred[n_oob_pred == 0] = 1
601             oob_pred[..., k] /= n_oob_pred[..., [k]]
602 
603         return oob_pred
604 
605     def _validate_y_class_weight(self, y):
606         # Default implementation
607         return y, None
608 
609     def _validate_X_predict(self, X):
610         """
611         Validate X whenever one tries to predict, apply, predict_proba."""
612         check_is_fitted(self)
613         X = self._validate_data(X, dtype=DTYPE, accept_sparse="csr", reset=False)
614         if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):
615             raise ValueError("No support for np.int64 index based sparse matrices")
616         return X
617 
618     @property
619     def feature_importances_(self):
620         """
621         The impurity-based feature importances.
622 
623         The higher, the more important the feature.
624         The importance of a feature is computed as the (normalized)
625         total reduction of the criterion brought by that feature.  It is also
626         known as the Gini importance.
627 
628         Warning: impurity-based feature importances can be misleading for
629         high cardinality features (many unique values). See
630         :func:`sklearn.inspection.permutation_importance` as an alternative.
631 
632         Returns
633         -------
634         feature_importances_ : ndarray of shape (n_features,)
635             The values of this array sum to 1, unless all trees are single node
636             trees consisting of only the root node, in which case it will be an
637             array of zeros.
638         """
639         check_is_fitted(self)
640 
641         all_importances = Parallel(n_jobs=self.n_jobs, prefer="threads")(
642             delayed(getattr)(tree, "feature_importances_")
643             for tree in self.estimators_
644             if tree.tree_.node_count > 1
645         )
646 
647         if not all_importances:
648             return np.zeros(self.n_features_in_, dtype=np.float64)
649 
650         all_importances = np.mean(all_importances, axis=0, dtype=np.float64)
651         return all_importances / np.sum(all_importances)
652 
653 
654 def _accumulate_prediction(predict, X, out, lock):
655     """
656     This is a utility function for joblib's Parallel.
657 
658     It can't go locally in ForestClassifier or ForestRegressor, because joblib
659     complains that it cannot pickle it when placed there.
660     """
661     prediction = predict(X, check_input=False)
662     with lock:
663         if len(out) == 1:
664             out[0] += prediction
665         else:
666             for i in range(len(out)):
667                 out[i] += prediction[i]
668 
669 
670 class ForestClassifier(ClassifierMixin, BaseForest, metaclass=ABCMeta):
671     """
672     Base class for forest of trees-based classifiers.
673 
674     Warning: This class should not be used directly. Use derived classes
675     instead.
676     """
677 
678     @abstractmethod
679     def __init__(
680         self,
681         estimator,
682         n_estimators=100,
683         *,
684         estimator_params=tuple(),
685         bootstrap=False,
686         oob_score=False,
687         n_jobs=None,
688         random_state=None,
689         verbose=0,
690         warm_start=False,
691         class_weight=None,
692         max_samples=None,
693         base_estimator="deprecated",
694     ):
695         super().__init__(
696             estimator=estimator,
697             n_estimators=n_estimators,
698             estimator_params=estimator_params,
699             bootstrap=bootstrap,
700             oob_score=oob_score,
701             n_jobs=n_jobs,
702             random_state=random_state,
703             verbose=verbose,
704             warm_start=warm_start,
705             class_weight=class_weight,
706             max_samples=max_samples,
707             base_estimator=base_estimator,
708         )
709 
710     @staticmethod
711     def _get_oob_predictions(tree, X):
712         """Compute the OOB predictions for an individual tree.
713 
714         Parameters
715         ----------
716         tree : DecisionTreeClassifier object
717             A single decision tree classifier.
718         X : ndarray of shape (n_samples, n_features)
719             The OOB samples.
720 
721         Returns
722         -------
723         y_pred : ndarray of shape (n_samples, n_classes, n_outputs)
724             The OOB associated predictions.
725         """
726         y_pred = tree.predict_proba(X, check_input=False)
727         y_pred = np.array(y_pred, copy=False)
728         if y_pred.ndim == 2:
729             # binary and multiclass
730             y_pred = y_pred[..., np.newaxis]
731         else:
732             # Roll the first `n_outputs` axis to the last axis. We will reshape
733             # from a shape of (n_outputs, n_samples, n_classes) to a shape of
734             # (n_samples, n_classes, n_outputs).
735             y_pred = np.rollaxis(y_pred, axis=0, start=3)
736         return y_pred
737 
738     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
739         """Compute and set the OOB score and attributes.
740 
741         Parameters
742         ----------
743         X : array-like of shape (n_samples, n_features)
744             The data matrix.
745         y : ndarray of shape (n_samples, n_outputs)
746             The target matrix.
747         scoring_function : callable, default=None
748             Scoring function for OOB score. Defaults to `accuracy_score`.
749         """
750         self.oob_decision_function_ = super()._compute_oob_predictions(X, y)
751         if self.oob_decision_function_.shape[-1] == 1:
752             # drop the n_outputs axis if there is a single output
753             self.oob_decision_function_ = self.oob_decision_function_.squeeze(axis=-1)
754 
755         if scoring_function is None:
756             scoring_function = accuracy_score
757 
758         self.oob_score_ = scoring_function(
759             y, np.argmax(self.oob_decision_function_, axis=1)
760         )
761 
762     def _validate_y_class_weight(self, y):
763         check_classification_targets(y)
764 
765         y = np.copy(y)
766         expanded_class_weight = None
767 
768         if self.class_weight is not None:
769             y_original = np.copy(y)
770 
771         self.classes_ = []
772         self.n_classes_ = []
773 
774         y_store_unique_indices = np.zeros(y.shape, dtype=int)
775         for k in range(self.n_outputs_):
776             classes_k, y_store_unique_indices[:, k] = np.unique(
777                 y[:, k], return_inverse=True
778             )
779             self.classes_.append(classes_k)
780             self.n_classes_.append(classes_k.shape[0])
781         y = y_store_unique_indices
782 
783         if self.class_weight is not None:
784             valid_presets = ("balanced", "balanced_subsample")
785             if isinstance(self.class_weight, str):
786                 if self.class_weight not in valid_presets:
787                     raise ValueError(
788                         "Valid presets for class_weight include "
789                         '"balanced" and "balanced_subsample".'
790                         'Given "%s".'
791                         % self.class_weight
792                     )
793                 if self.warm_start:
794                     warn(
795                         'class_weight presets "balanced" or '
796                         '"balanced_subsample" are '
797                         "not recommended for warm_start if the fitted data "
798                         "differs from the full dataset. In order to use "
799                         '"balanced" weights, use compute_class_weight '
800                         '("balanced", classes, y). In place of y you can use '
801                         "a large enough sample of the full training set "
802                         "target to properly estimate the class frequency "
803                         "distributions. Pass the resulting weights as the "
804                         "class_weight parameter."
805                     )
806 
807             if self.class_weight != "balanced_subsample" or not self.bootstrap:
808                 if self.class_weight == "balanced_subsample":
809                     class_weight = "balanced"
810                 else:
811                     class_weight = self.class_weight
812                 expanded_class_weight = compute_sample_weight(class_weight, y_original)
813 
814         return y, expanded_class_weight
815 
816     def predict(self, X):
817         """
818         Predict class for X.
819 
820         The predicted class of an input sample is a vote by the trees in
821         the forest, weighted by their probability estimates. That is,
822         the predicted class is the one with highest mean probability
823         estimate across the trees.
824 
825         Parameters
826         ----------
827         X : {array-like, sparse matrix} of shape (n_samples, n_features)
828             The input samples. Internally, its dtype will be converted to
829             ``dtype=np.float32``. If a sparse matrix is provided, it will be
830             converted into a sparse ``csr_matrix``.
831 
832         Returns
833         -------
834         y : ndarray of shape (n_samples,) or (n_samples, n_outputs)
835             The predicted classes.
836         """
837         proba = self.predict_proba(X)
838 
839         if self.n_outputs_ == 1:
840             return self.classes_.take(np.argmax(proba, axis=1), axis=0)
841 
842         else:
843             n_samples = proba[0].shape[0]
844             # all dtypes should be the same, so just take the first
845             class_type = self.classes_[0].dtype
846             predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)
847 
848             for k in range(self.n_outputs_):
849                 predictions[:, k] = self.classes_[k].take(
850                     np.argmax(proba[k], axis=1), axis=0
851                 )
852 
853             return predictions
854 
855     def predict_proba(self, X):
856         """
857         Predict class probabilities for X.
858 
859         The predicted class probabilities of an input sample are computed as
860         the mean predicted class probabilities of the trees in the forest.
861         The class probability of a single tree is the fraction of samples of
862         the same class in a leaf.
863 
864         Parameters
865         ----------
866         X : {array-like, sparse matrix} of shape (n_samples, n_features)
867             The input samples. Internally, its dtype will be converted to
868             ``dtype=np.float32``. If a sparse matrix is provided, it will be
869             converted into a sparse ``csr_matrix``.
870 
871         Returns
872         -------
873         p : ndarray of shape (n_samples, n_classes), or a list of such arrays
874             The class probabilities of the input samples. The order of the
875             classes corresponds to that in the attribute :term:`classes_`.
876         """
877         check_is_fitted(self)
878         # Check data
879         X = self._validate_X_predict(X)
880 
881         # Assign chunk of trees to jobs
882         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
883 
884         # avoid storing the output of every estimator by summing them here
885         all_proba = [
886             np.zeros((X.shape[0], j), dtype=np.float64)
887             for j in np.atleast_1d(self.n_classes_)
888         ]
889         lock = threading.Lock()
890         Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
891             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)
892             for e in self.estimators_
893         )
894 
895         for proba in all_proba:
896             proba /= len(self.estimators_)
897 
898         if len(all_proba) == 1:
899             return all_proba[0]
900         else:
901             return all_proba
902 
903     def predict_log_proba(self, X):
904         """
905         Predict class log-probabilities for X.
906 
907         The predicted class log-probabilities of an input sample is computed as
908         the log of the mean predicted class probabilities of the trees in the
909         forest.
910 
911         Parameters
912         ----------
913         X : {array-like, sparse matrix} of shape (n_samples, n_features)
914             The input samples. Internally, its dtype will be converted to
915             ``dtype=np.float32``. If a sparse matrix is provided, it will be
916             converted into a sparse ``csr_matrix``.
917 
918         Returns
919         -------
920         p : ndarray of shape (n_samples, n_classes), or a list of such arrays
921             The class probabilities of the input samples. The order of the
922             classes corresponds to that in the attribute :term:`classes_`.
923         """
924         proba = self.predict_proba(X)
925 
926         if self.n_outputs_ == 1:
927             return np.log(proba)
928 
929         else:
930             for k in range(self.n_outputs_):
931                 proba[k] = np.log(proba[k])
932 
933             return proba
934 
935     def _more_tags(self):
936         return {"multilabel": True}
937 
938 
939 class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):
940     """
941     Base class for forest of trees-based regressors.
942 
943     Warning: This class should not be used directly. Use derived classes
944     instead.
945     """
946 
947     @abstractmethod
948     def __init__(
949         self,
950         estimator,
951         n_estimators=100,
952         *,
953         estimator_params=tuple(),
954         bootstrap=False,
955         oob_score=False,
956         n_jobs=None,
957         random_state=None,
958         verbose=0,
959         warm_start=False,
960         max_samples=None,
961         base_estimator="deprecated",
962     ):
963         super().__init__(
964             estimator,
965             n_estimators=n_estimators,
966             estimator_params=estimator_params,
967             bootstrap=bootstrap,
968             oob_score=oob_score,
969             n_jobs=n_jobs,
970             random_state=random_state,
971             verbose=verbose,
972             warm_start=warm_start,
973             max_samples=max_samples,
974             base_estimator=base_estimator,
975         )
976 
977     def predict(self, X):
978         """
979         Predict regression target for X.
980 
981         The predicted regression target of an input sample is computed as the
982         mean predicted regression targets of the trees in the forest.
983 
984         Parameters
985         ----------
986         X : {array-like, sparse matrix} of shape (n_samples, n_features)
987             The input samples. Internally, its dtype will be converted to
988             ``dtype=np.float32``. If a sparse matrix is provided, it will be
989             converted into a sparse ``csr_matrix``.
990 
991         Returns
992         -------
993         y : ndarray of shape (n_samples,) or (n_samples, n_outputs)
994             The predicted values.
995         """
996         check_is_fitted(self)
997         # Check data
998         X = self._validate_X_predict(X)
999 
1000         # Assign chunk of trees to jobs
1001         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
1002 
1003         # avoid storing the output of every estimator by summing them here
1004         if self.n_outputs_ > 1:
1005             y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)
1006         else:
1007             y_hat = np.zeros((X.shape[0]), dtype=np.float64)
1008 
1009         # Parallel loop
1010         lock = threading.Lock()
1011         Parallel(n_jobs=n_jobs, verbose=self.verbose, require="sharedmem")(
1012             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)
1013             for e in self.estimators_
1014         )
1015 
1016         y_hat /= len(self.estimators_)
1017 
1018         return y_hat
1019 
1020     @staticmethod
1021     def _get_oob_predictions(tree, X):
1022         """Compute the OOB predictions for an individual tree.
1023 
1024         Parameters
1025         ----------
1026         tree : DecisionTreeRegressor object
1027             A single decision tree regressor.
1028         X : ndarray of shape (n_samples, n_features)
1029             The OOB samples.
1030 
1031         Returns
1032         -------
1033         y_pred : ndarray of shape (n_samples, 1, n_outputs)
1034             The OOB associated predictions.
1035         """
1036         y_pred = tree.predict(X, check_input=False)
1037         if y_pred.ndim == 1:
1038             # single output regression
1039             y_pred = y_pred[:, np.newaxis, np.newaxis]
1040         else:
1041             # multioutput regression
1042             y_pred = y_pred[:, np.newaxis, :]
1043         return y_pred
1044 
1045     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
1046         """Compute and set the OOB score and attributes.
1047 
1048         Parameters
1049         ----------
1050         X : array-like of shape (n_samples, n_features)
1051             The data matrix.
1052         y : ndarray of shape (n_samples, n_outputs)
1053             The target matrix.
1054         scoring_function : callable, default=None
1055             Scoring function for OOB score. Defaults to `r2_score`.
1056         """
1057         self.oob_prediction_ = super()._compute_oob_predictions(X, y).squeeze(axis=1)
1058         if self.oob_prediction_.shape[-1] == 1:
1059             # drop the n_outputs axis if there is a single output
1060             self.oob_prediction_ = self.oob_prediction_.squeeze(axis=-1)
1061 
1062         if scoring_function is None:
1063             scoring_function = r2_score
1064 
1065         self.oob_score_ = scoring_function(y, self.oob_prediction_)
1066 
1067     def _compute_partial_dependence_recursion(self, grid, target_features):
1068         """Fast partial dependence computation.
1069 
1070         Parameters
1071         ----------
1072         grid : ndarray of shape (n_samples, n_target_features)
1073             The grid points on which the partial dependence should be
1074             evaluated.
1075         target_features : ndarray of shape (n_target_features)
1076             The set of target features for which the partial dependence
1077             should be evaluated.
1078 
1079         Returns
1080         -------
1081         averaged_predictions : ndarray of shape (n_samples,)
1082             The value of the partial dependence function on each grid point.
1083         """
1084         grid = np.asarray(grid, dtype=DTYPE, order="C")
1085         averaged_predictions = np.zeros(
1086             shape=grid.shape[0], dtype=np.float64, order="C"
1087         )
1088 
1089         for tree in self.estimators_:
1090             # Note: we don't sum in parallel because the GIL isn't released in
1091             # the fast method.
1092             tree.tree_.compute_partial_dependence(
1093                 grid, target_features, averaged_predictions
1094             )
1095         # Average over the forest
1096         averaged_predictions /= len(self.estimators_)
1097 
1098         return averaged_predictions
1099 
1100     def _more_tags(self):
1101         return {"multilabel": True}
1102 
1103 
1104 class RandomForestClassifier(ForestClassifier):
1105     """
1106     A random forest classifier.
1107 
1108     A random forest is a meta estimator that fits a number of decision tree
1109     classifiers on various sub-samples of the dataset and uses averaging to
1110     improve the predictive accuracy and control over-fitting.
1111     The sub-sample size is controlled with the `max_samples` parameter if
1112     `bootstrap=True` (default), otherwise the whole dataset is used to build
1113     each tree.
1114 
1115     Read more in the :ref:`User Guide <forest>`.
1116 
1117     Parameters
1118     ----------
1119     n_estimators : int, default=100
1120         The number of trees in the forest.
1121 
1122         .. versionchanged:: 0.22
1123            The default value of ``n_estimators`` changed from 10 to 100
1124            in 0.22.
1125 
1126     criterion : {"gini", "entropy", "log_loss"}, default="gini"
1127         The function to measure the quality of a split. Supported criteria are
1128         "gini" for the Gini impurity and "log_loss" and "entropy" both for the
1129         Shannon information gain, see :ref:`tree_mathematical_formulation`.
1130         Note: This parameter is tree-specific.
1131 
1132     max_depth : int, default=None
1133         The maximum depth of the tree. If None, then nodes are expanded until
1134         all leaves are pure or until all leaves contain less than
1135         min_samples_split samples.
1136 
1137     min_samples_split : int or float, default=2
1138         The minimum number of samples required to split an internal node:
1139 
1140         - If int, then consider `min_samples_split` as the minimum number.
1141         - If float, then `min_samples_split` is a fraction and
1142           `ceil(min_samples_split * n_samples)` are the minimum
1143           number of samples for each split.
1144 
1145         .. versionchanged:: 0.18
1146            Added float values for fractions.
1147 
1148     min_samples_leaf : int or float, default=1
1149         The minimum number of samples required to be at a leaf node.
1150         A split point at any depth will only be considered if it leaves at
1151         least ``min_samples_leaf`` training samples in each of the left and
1152         right branches.  This may have the effect of smoothing the model,
1153         especially in regression.
1154 
1155         - If int, then consider `min_samples_leaf` as the minimum number.
1156         - If float, then `min_samples_leaf` is a fraction and
1157           `ceil(min_samples_leaf * n_samples)` are the minimum
1158           number of samples for each node.
1159 
1160         .. versionchanged:: 0.18
1161            Added float values for fractions.
1162 
1163     min_weight_fraction_leaf : float, default=0.0
1164         The minimum weighted fraction of the sum total of weights (of all
1165         the input samples) required to be at a leaf node. Samples have
1166         equal weight when sample_weight is not provided.
1167 
1168     max_features : {"sqrt", "log2", None}, int or float, default="sqrt"
1169         The number of features to consider when looking for the best split:
1170 
1171         - If int, then consider `max_features` features at each split.
1172         - If float, then `max_features` is a fraction and
1173           `max(1, int(max_features * n_features_in_))` features are considered at each
1174           split.
1175         - If "auto", then `max_features=sqrt(n_features)`.
1176         - If "sqrt", then `max_features=sqrt(n_features)`.
1177         - If "log2", then `max_features=log2(n_features)`.
1178         - If None, then `max_features=n_features`.
1179 
1180         .. versionchanged:: 1.1
1181             The default of `max_features` changed from `"auto"` to `"sqrt"`.
1182 
1183         .. deprecated:: 1.1
1184             The `"auto"` option was deprecated in 1.1 and will be removed
1185             in 1.3.
1186 
1187         Note: the search for a split does not stop until at least one
1188         valid partition of the node samples is found, even if it requires to
1189         effectively inspect more than ``max_features`` features.
1190 
1191     max_leaf_nodes : int, default=None
1192         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1193         Best nodes are defined as relative reduction in impurity.
1194         If None then unlimited number of leaf nodes.
1195 
1196     min_impurity_decrease : float, default=0.0
1197         A node will be split if this split induces a decrease of the impurity
1198         greater than or equal to this value.
1199 
1200         The weighted impurity decrease equation is the following::
1201 
1202             N_t / N * (impurity - N_t_R / N_t * right_impurity
1203                                 - N_t_L / N_t * left_impurity)
1204 
1205         where ``N`` is the total number of samples, ``N_t`` is the number of
1206         samples at the current node, ``N_t_L`` is the number of samples in the
1207         left child, and ``N_t_R`` is the number of samples in the right child.
1208 
1209         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1210         if ``sample_weight`` is passed.
1211 
1212         .. versionadded:: 0.19
1213 
1214     bootstrap : bool, default=True
1215         Whether bootstrap samples are used when building trees. If False, the
1216         whole dataset is used to build each tree.
1217 
1218     oob_score : bool or callable, default=False
1219         Whether to use out-of-bag samples to estimate the generalization score.
1220         By default, :func:`~sklearn.metrics.accuracy_score` is used.
1221         Provide a callable with signature `metric(y_true, y_pred)` to use a
1222         custom metric. Only available if `bootstrap=True`.
1223 
1224     n_jobs : int, default=None
1225         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
1226         :meth:`decision_path` and :meth:`apply` are all parallelized over the
1227         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
1228         context. ``-1`` means using all processors. See :term:`Glossary
1229         <n_jobs>` for more details.
1230 
1231     random_state : int, RandomState instance or None, default=None
1232         Controls both the randomness of the bootstrapping of the samples used
1233         when building trees (if ``bootstrap=True``) and the sampling of the
1234         features to consider when looking for the best split at each node
1235         (if ``max_features < n_features``).
1236         See :term:`Glossary <random_state>` for details.
1237 
1238     verbose : int, default=0
1239         Controls the verbosity when fitting and predicting.
1240 
1241     warm_start : bool, default=False
1242         When set to ``True``, reuse the solution of the previous call to fit
1243         and add more estimators to the ensemble, otherwise, just fit a whole
1244         new forest. See :term:`Glossary <warm_start>` and
1245         :ref:`gradient_boosting_warm_start` for details.
1246 
1247     class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts, \
1248             default=None
1249         Weights associated with classes in the form ``{class_label: weight}``.
1250         If not given, all classes are supposed to have weight one. For
1251         multi-output problems, a list of dicts can be provided in the same
1252         order as the columns of y.
1253 
1254         Note that for multioutput (including multilabel) weights should be
1255         defined for each class of every column in its own dict. For example,
1256         for four-class multilabel classification weights should be
1257         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
1258         [{1:1}, {2:5}, {3:1}, {4:1}].
1259 
1260         The "balanced" mode uses the values of y to automatically adjust
1261         weights inversely proportional to class frequencies in the input data
1262         as ``n_samples / (n_classes * np.bincount(y))``
1263 
1264         The "balanced_subsample" mode is the same as "balanced" except that
1265         weights are computed based on the bootstrap sample for every tree
1266         grown.
1267 
1268         For multi-output, the weights of each column of y will be multiplied.
1269 
1270         Note that these weights will be multiplied with sample_weight (passed
1271         through the fit method) if sample_weight is specified.
1272 
1273     ccp_alpha : non-negative float, default=0.0
1274         Complexity parameter used for Minimal Cost-Complexity Pruning. The
1275         subtree with the largest cost complexity that is smaller than
1276         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
1277         :ref:`minimal_cost_complexity_pruning` for details.
1278 
1279         .. versionadded:: 0.22
1280 
1281     max_samples : int or float, default=None
1282         If bootstrap is True, the number of samples to draw from X
1283         to train each base estimator.
1284 
1285         - If None (default), then draw `X.shape[0]` samples.
1286         - If int, then draw `max_samples` samples.
1287         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
1288           `max_samples` should be in the interval `(0.0, 1.0]`.
1289 
1290         .. versionadded:: 0.22
1291 
1292     Attributes
1293     ----------
1294     estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`
1295         The child estimator template used to create the collection of fitted
1296         sub-estimators.
1297 
1298         .. versionadded:: 1.2
1299            `base_estimator_` was renamed to `estimator_`.
1300 
1301     base_estimator_ : DecisionTreeClassifier
1302         The child estimator template used to create the collection of fitted
1303         sub-estimators.
1304 
1305         .. deprecated:: 1.2
1306             `base_estimator_` is deprecated and will be removed in 1.4.
1307             Use `estimator_` instead.
1308 
1309     estimators_ : list of DecisionTreeClassifier
1310         The collection of fitted sub-estimators.
1311 
1312     classes_ : ndarray of shape (n_classes,) or a list of such arrays
1313         The classes labels (single output problem), or a list of arrays of
1314         class labels (multi-output problem).
1315 
1316     n_classes_ : int or list
1317         The number of classes (single output problem), or a list containing the
1318         number of classes for each output (multi-output problem).
1319 
1320     n_features_in_ : int
1321         Number of features seen during :term:`fit`.
1322 
1323         .. versionadded:: 0.24
1324 
1325     feature_names_in_ : ndarray of shape (`n_features_in_`,)
1326         Names of features seen during :term:`fit`. Defined only when `X`
1327         has feature names that are all strings.
1328 
1329         .. versionadded:: 1.0
1330 
1331     n_outputs_ : int
1332         The number of outputs when ``fit`` is performed.
1333 
1334     feature_importances_ : ndarray of shape (n_features,)
1335         The impurity-based feature importances.
1336         The higher, the more important the feature.
1337         The importance of a feature is computed as the (normalized)
1338         total reduction of the criterion brought by that feature.  It is also
1339         known as the Gini importance.
1340 
1341         Warning: impurity-based feature importances can be misleading for
1342         high cardinality features (many unique values). See
1343         :func:`sklearn.inspection.permutation_importance` as an alternative.
1344 
1345     oob_score_ : float
1346         Score of the training dataset obtained using an out-of-bag estimate.
1347         This attribute exists only when ``oob_score`` is True.
1348 
1349     oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \
1350             (n_samples, n_classes, n_outputs)
1351         Decision function computed with out-of-bag estimate on the training
1352         set. If n_estimators is small it might be possible that a data point
1353         was never left out during the bootstrap. In this case,
1354         `oob_decision_function_` might contain NaN. This attribute exists
1355         only when ``oob_score`` is True.
1356 
1357     See Also
1358     --------
1359     sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
1360     sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized
1361         tree classifiers.
1362 
1363     Notes
1364     -----
1365     The default values for the parameters controlling the size of the trees
1366     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1367     unpruned trees which can potentially be very large on some data sets. To
1368     reduce memory consumption, the complexity and size of the trees should be
1369     controlled by setting those parameter values.
1370 
1371     The features are always randomly permuted at each split. Therefore,
1372     the best found split may vary, even with the same training data,
1373     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
1374     of the criterion is identical for several splits enumerated during the
1375     search of the best split. To obtain a deterministic behaviour during
1376     fitting, ``random_state`` has to be fixed.
1377 
1378     References
1379     ----------
1380     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
1381 
1382     Examples
1383     --------
1384     >>> from sklearn.ensemble import RandomForestClassifier
1385     >>> from sklearn.datasets import make_classification
1386     >>> X, y = make_classification(n_samples=1000, n_features=4,
1387     ...                            n_informative=2, n_redundant=0,
1388     ...                            random_state=0, shuffle=False)
1389     >>> clf = RandomForestClassifier(max_depth=2, random_state=0)
1390     >>> clf.fit(X, y)
1391     RandomForestClassifier(...)
1392     >>> print(clf.predict([[0, 0, 0, 0]]))
1393     [1]
1394     """
1395 
1396     _parameter_constraints: dict = {
1397         **ForestClassifier._parameter_constraints,
1398         **DecisionTreeClassifier._parameter_constraints,
1399         "class_weight": [
1400             StrOptions({"balanced_subsample", "balanced"}),
1401             dict,
1402             list,
1403             None,
1404         ],
1405     }
1406     _parameter_constraints.pop("splitter")
1407 
1408     def __init__(
1409         self,
1410         n_estimators=100,
1411         *,
1412         criterion="gini",
1413         max_depth=None,
1414         min_samples_split=2,
1415         min_samples_leaf=1,
1416         min_weight_fraction_leaf=0.0,
1417         max_features="sqrt",
1418         max_leaf_nodes=None,
1419         min_impurity_decrease=0.0,
1420         bootstrap=True,
1421         oob_score=False,
1422         n_jobs=None,
1423         random_state=None,
1424         verbose=0,
1425         warm_start=False,
1426         class_weight=None,
1427         ccp_alpha=0.0,
1428         max_samples=None,
1429     ):
1430         super().__init__(
1431             estimator=DecisionTreeClassifier(),
1432             n_estimators=n_estimators,
1433             estimator_params=(
1434                 "criterion",
1435                 "max_depth",
1436                 "min_samples_split",
1437                 "min_samples_leaf",
1438                 "min_weight_fraction_leaf",
1439                 "max_features",
1440                 "max_leaf_nodes",
1441                 "min_impurity_decrease",
1442                 "random_state",
1443                 "ccp_alpha",
1444             ),
1445             bootstrap=bootstrap,
1446             oob_score=oob_score,
1447             n_jobs=n_jobs,
1448             random_state=random_state,
1449             verbose=verbose,
1450             warm_start=warm_start,
1451             class_weight=class_weight,
1452             max_samples=max_samples,
1453         )
1454 
1455         self.criterion = criterion
1456         self.max_depth = max_depth
1457         self.min_samples_split = min_samples_split
1458         self.min_samples_leaf = min_samples_leaf
1459         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1460         self.max_features = max_features
1461         self.max_leaf_nodes = max_leaf_nodes
1462         self.min_impurity_decrease = min_impurity_decrease
1463         self.ccp_alpha = ccp_alpha
1464 
1465 
1466 class RandomForestRegressor(ForestRegressor):
1467     """
1468     A random forest regressor.
1469 
1470     A random forest is a meta estimator that fits a number of classifying
1471     decision trees on various sub-samples of the dataset and uses averaging
1472     to improve the predictive accuracy and control over-fitting.
1473     The sub-sample size is controlled with the `max_samples` parameter if
1474     `bootstrap=True` (default), otherwise the whole dataset is used to build
1475     each tree.
1476 
1477     Read more in the :ref:`User Guide <forest>`.
1478 
1479     Parameters
1480     ----------
1481     n_estimators : int, default=100
1482         The number of trees in the forest.
1483 
1484         .. versionchanged:: 0.22
1485            The default value of ``n_estimators`` changed from 10 to 100
1486            in 0.22.
1487 
1488     criterion : {"squared_error", "absolute_error", "friedman_mse", "poisson"}, \
1489             default="squared_error"
1490         The function to measure the quality of a split. Supported criteria
1491         are "squared_error" for the mean squared error, which is equal to
1492         variance reduction as feature selection criterion and minimizes the L2
1493         loss using the mean of each terminal node, "friedman_mse", which uses
1494         mean squared error with Friedman's improvement score for potential
1495         splits, "absolute_error" for the mean absolute error, which minimizes
1496         the L1 loss using the median of each terminal node, and "poisson" which
1497         uses reduction in Poisson deviance to find splits.
1498         Training using "absolute_error" is significantly slower
1499         than when using "squared_error".
1500 
1501         .. versionadded:: 0.18
1502            Mean Absolute Error (MAE) criterion.
1503 
1504         .. versionadded:: 1.0
1505            Poisson criterion.
1506 
1507     max_depth : int, default=None
1508         The maximum depth of the tree. If None, then nodes are expanded until
1509         all leaves are pure or until all leaves contain less than
1510         min_samples_split samples.
1511 
1512     min_samples_split : int or float, default=2
1513         The minimum number of samples required to split an internal node:
1514 
1515         - If int, then consider `min_samples_split` as the minimum number.
1516         - If float, then `min_samples_split` is a fraction and
1517           `ceil(min_samples_split * n_samples)` are the minimum
1518           number of samples for each split.
1519 
1520         .. versionchanged:: 0.18
1521            Added float values for fractions.
1522 
1523     min_samples_leaf : int or float, default=1
1524         The minimum number of samples required to be at a leaf node.
1525         A split point at any depth will only be considered if it leaves at
1526         least ``min_samples_leaf`` training samples in each of the left and
1527         right branches.  This may have the effect of smoothing the model,
1528         especially in regression.
1529 
1530         - If int, then consider `min_samples_leaf` as the minimum number.
1531         - If float, then `min_samples_leaf` is a fraction and
1532           `ceil(min_samples_leaf * n_samples)` are the minimum
1533           number of samples for each node.
1534 
1535         .. versionchanged:: 0.18
1536            Added float values for fractions.
1537 
1538     min_weight_fraction_leaf : float, default=0.0
1539         The minimum weighted fraction of the sum total of weights (of all
1540         the input samples) required to be at a leaf node. Samples have
1541         equal weight when sample_weight is not provided.
1542 
1543     max_features : {"sqrt", "log2", None}, int or float, default=1.0
1544         The number of features to consider when looking for the best split:
1545 
1546         - If int, then consider `max_features` features at each split.
1547         - If float, then `max_features` is a fraction and
1548           `max(1, int(max_features * n_features_in_))` features are considered at each
1549           split.
1550         - If "auto", then `max_features=n_features`.
1551         - If "sqrt", then `max_features=sqrt(n_features)`.
1552         - If "log2", then `max_features=log2(n_features)`.
1553         - If None or 1.0, then `max_features=n_features`.
1554 
1555         .. note::
1556             The default of 1.0 is equivalent to bagged trees and more
1557             randomness can be achieved by setting smaller values, e.g. 0.3.
1558 
1559         .. versionchanged:: 1.1
1560             The default of `max_features` changed from `"auto"` to 1.0.
1561 
1562         .. deprecated:: 1.1
1563             The `"auto"` option was deprecated in 1.1 and will be removed
1564             in 1.3.
1565 
1566         Note: the search for a split does not stop until at least one
1567         valid partition of the node samples is found, even if it requires to
1568         effectively inspect more than ``max_features`` features.
1569 
1570     max_leaf_nodes : int, default=None
1571         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1572         Best nodes are defined as relative reduction in impurity.
1573         If None then unlimited number of leaf nodes.
1574 
1575     min_impurity_decrease : float, default=0.0
1576         A node will be split if this split induces a decrease of the impurity
1577         greater than or equal to this value.
1578 
1579         The weighted impurity decrease equation is the following::
1580 
1581             N_t / N * (impurity - N_t_R / N_t * right_impurity
1582                                 - N_t_L / N_t * left_impurity)
1583 
1584         where ``N`` is the total number of samples, ``N_t`` is the number of
1585         samples at the current node, ``N_t_L`` is the number of samples in the
1586         left child, and ``N_t_R`` is the number of samples in the right child.
1587 
1588         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1589         if ``sample_weight`` is passed.
1590 
1591         .. versionadded:: 0.19
1592 
1593     bootstrap : bool, default=True
1594         Whether bootstrap samples are used when building trees. If False, the
1595         whole dataset is used to build each tree.
1596 
1597     oob_score : bool or callable, default=False
1598         Whether to use out-of-bag samples to estimate the generalization score.
1599         By default, :func:`~sklearn.metrics.r2_score` is used.
1600         Provide a callable with signature `metric(y_true, y_pred)` to use a
1601         custom metric. Only available if `bootstrap=True`.
1602 
1603     n_jobs : int, default=None
1604         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
1605         :meth:`decision_path` and :meth:`apply` are all parallelized over the
1606         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
1607         context. ``-1`` means using all processors. See :term:`Glossary
1608         <n_jobs>` for more details.
1609 
1610     random_state : int, RandomState instance or None, default=None
1611         Controls both the randomness of the bootstrapping of the samples used
1612         when building trees (if ``bootstrap=True``) and the sampling of the
1613         features to consider when looking for the best split at each node
1614         (if ``max_features < n_features``).
1615         See :term:`Glossary <random_state>` for details.
1616 
1617     verbose : int, default=0
1618         Controls the verbosity when fitting and predicting.
1619 
1620     warm_start : bool, default=False
1621         When set to ``True``, reuse the solution of the previous call to fit
1622         and add more estimators to the ensemble, otherwise, just fit a whole
1623         new forest. See :term:`Glossary <warm_start>` and
1624         :ref:`gradient_boosting_warm_start` for details.
1625 
1626     ccp_alpha : non-negative float, default=0.0
1627         Complexity parameter used for Minimal Cost-Complexity Pruning. The
1628         subtree with the largest cost complexity that is smaller than
1629         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
1630         :ref:`minimal_cost_complexity_pruning` for details.
1631 
1632         .. versionadded:: 0.22
1633 
1634     max_samples : int or float, default=None
1635         If bootstrap is True, the number of samples to draw from X
1636         to train each base estimator.
1637 
1638         - If None (default), then draw `X.shape[0]` samples.
1639         - If int, then draw `max_samples` samples.
1640         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
1641           `max_samples` should be in the interval `(0.0, 1.0]`.
1642 
1643         .. versionadded:: 0.22
1644 
1645     Attributes
1646     ----------
1647     estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`
1648         The child estimator template used to create the collection of fitted
1649         sub-estimators.
1650 
1651         .. versionadded:: 1.2
1652            `base_estimator_` was renamed to `estimator_`.
1653 
1654     base_estimator_ : DecisionTreeRegressor
1655         The child estimator template used to create the collection of fitted
1656         sub-estimators.
1657 
1658         .. deprecated:: 1.2
1659             `base_estimator_` is deprecated and will be removed in 1.4.
1660             Use `estimator_` instead.
1661 
1662     estimators_ : list of DecisionTreeRegressor
1663         The collection of fitted sub-estimators.
1664 
1665     feature_importances_ : ndarray of shape (n_features,)
1666         The impurity-based feature importances.
1667         The higher, the more important the feature.
1668         The importance of a feature is computed as the (normalized)
1669         total reduction of the criterion brought by that feature.  It is also
1670         known as the Gini importance.
1671 
1672         Warning: impurity-based feature importances can be misleading for
1673         high cardinality features (many unique values). See
1674         :func:`sklearn.inspection.permutation_importance` as an alternative.
1675 
1676     n_features_in_ : int
1677         Number of features seen during :term:`fit`.
1678 
1679         .. versionadded:: 0.24
1680 
1681     feature_names_in_ : ndarray of shape (`n_features_in_`,)
1682         Names of features seen during :term:`fit`. Defined only when `X`
1683         has feature names that are all strings.
1684 
1685         .. versionadded:: 1.0
1686 
1687     n_outputs_ : int
1688         The number of outputs when ``fit`` is performed.
1689 
1690     oob_score_ : float
1691         Score of the training dataset obtained using an out-of-bag estimate.
1692         This attribute exists only when ``oob_score`` is True.
1693 
1694     oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
1695         Prediction computed with out-of-bag estimate on the training set.
1696         This attribute exists only when ``oob_score`` is True.
1697 
1698     See Also
1699     --------
1700     sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
1701     sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized
1702         tree regressors.
1703 
1704     Notes
1705     -----
1706     The default values for the parameters controlling the size of the trees
1707     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1708     unpruned trees which can potentially be very large on some data sets. To
1709     reduce memory consumption, the complexity and size of the trees should be
1710     controlled by setting those parameter values.
1711 
1712     The features are always randomly permuted at each split. Therefore,
1713     the best found split may vary, even with the same training data,
1714     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
1715     of the criterion is identical for several splits enumerated during the
1716     search of the best split. To obtain a deterministic behaviour during
1717     fitting, ``random_state`` has to be fixed.
1718 
1719     The default value ``max_features="auto"`` uses ``n_features``
1720     rather than ``n_features / 3``. The latter was originally suggested in
1721     [1], whereas the former was more recently justified empirically in [2].
1722 
1723     References
1724     ----------
1725     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
1726 
1727     .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
1728            trees", Machine Learning, 63(1), 3-42, 2006.
1729 
1730     Examples
1731     --------
1732     >>> from sklearn.ensemble import RandomForestRegressor
1733     >>> from sklearn.datasets import make_regression
1734     >>> X, y = make_regression(n_features=4, n_informative=2,
1735     ...                        random_state=0, shuffle=False)
1736     >>> regr = RandomForestRegressor(max_depth=2, random_state=0)
1737     >>> regr.fit(X, y)
1738     RandomForestRegressor(...)
1739     >>> print(regr.predict([[0, 0, 0, 0]]))
1740     [-8.32987858]
1741     """
1742 
1743     _parameter_constraints: dict = {
1744         **ForestRegressor._parameter_constraints,
1745         **DecisionTreeRegressor._parameter_constraints,
1746     }
1747     _parameter_constraints.pop("splitter")
1748 
1749     def __init__(
1750         self,
1751         n_estimators=100,
1752         *,
1753         criterion="squared_error",
1754         max_depth=None,
1755         min_samples_split=2,
1756         min_samples_leaf=1,
1757         min_weight_fraction_leaf=0.0,
1758         max_features=1.0,
1759         max_leaf_nodes=None,
1760         min_impurity_decrease=0.0,
1761         bootstrap=True,
1762         oob_score=False,
1763         n_jobs=None,
1764         random_state=None,
1765         verbose=0,
1766         warm_start=False,
1767         ccp_alpha=0.0,
1768         max_samples=None,
1769     ):
1770         super().__init__(
1771             estimator=DecisionTreeRegressor(),
1772             n_estimators=n_estimators,
1773             estimator_params=(
1774                 "criterion",
1775                 "max_depth",
1776                 "min_samples_split",
1777                 "min_samples_leaf",
1778                 "min_weight_fraction_leaf",
1779                 "max_features",
1780                 "max_leaf_nodes",
1781                 "min_impurity_decrease",
1782                 "random_state",
1783                 "ccp_alpha",
1784             ),
1785             bootstrap=bootstrap,
1786             oob_score=oob_score,
1787             n_jobs=n_jobs,
1788             random_state=random_state,
1789             verbose=verbose,
1790             warm_start=warm_start,
1791             max_samples=max_samples,
1792         )
1793 
1794         self.criterion = criterion
1795         self.max_depth = max_depth
1796         self.min_samples_split = min_samples_split
1797         self.min_samples_leaf = min_samples_leaf
1798         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1799         self.max_features = max_features
1800         self.max_leaf_nodes = max_leaf_nodes
1801         self.min_impurity_decrease = min_impurity_decrease
1802         self.ccp_alpha = ccp_alpha
1803 
1804 
1805 class ExtraTreesClassifier(ForestClassifier):
1806     """
1807     An extra-trees classifier.
1808 
1809     This class implements a meta estimator that fits a number of
1810     randomized decision trees (a.k.a. extra-trees) on various sub-samples
1811     of the dataset and uses averaging to improve the predictive accuracy
1812     and control over-fitting.
1813 
1814     Read more in the :ref:`User Guide <forest>`.
1815 
1816     Parameters
1817     ----------
1818     n_estimators : int, default=100
1819         The number of trees in the forest.
1820 
1821         .. versionchanged:: 0.22
1822            The default value of ``n_estimators`` changed from 10 to 100
1823            in 0.22.
1824 
1825     criterion : {"gini", "entropy", "log_loss"}, default="gini"
1826         The function to measure the quality of a split. Supported criteria are
1827         "gini" for the Gini impurity and "log_loss" and "entropy" both for the
1828         Shannon information gain, see :ref:`tree_mathematical_formulation`.
1829         Note: This parameter is tree-specific.
1830 
1831     max_depth : int, default=None
1832         The maximum depth of the tree. If None, then nodes are expanded until
1833         all leaves are pure or until all leaves contain less than
1834         min_samples_split samples.
1835 
1836     min_samples_split : int or float, default=2
1837         The minimum number of samples required to split an internal node:
1838 
1839         - If int, then consider `min_samples_split` as the minimum number.
1840         - If float, then `min_samples_split` is a fraction and
1841           `ceil(min_samples_split * n_samples)` are the minimum
1842           number of samples for each split.
1843 
1844         .. versionchanged:: 0.18
1845            Added float values for fractions.
1846 
1847     min_samples_leaf : int or float, default=1
1848         The minimum number of samples required to be at a leaf node.
1849         A split point at any depth will only be considered if it leaves at
1850         least ``min_samples_leaf`` training samples in each of the left and
1851         right branches.  This may have the effect of smoothing the model,
1852         especially in regression.
1853 
1854         - If int, then consider `min_samples_leaf` as the minimum number.
1855         - If float, then `min_samples_leaf` is a fraction and
1856           `ceil(min_samples_leaf * n_samples)` are the minimum
1857           number of samples for each node.
1858 
1859         .. versionchanged:: 0.18
1860            Added float values for fractions.
1861 
1862     min_weight_fraction_leaf : float, default=0.0
1863         The minimum weighted fraction of the sum total of weights (of all
1864         the input samples) required to be at a leaf node. Samples have
1865         equal weight when sample_weight is not provided.
1866 
1867     max_features : {"sqrt", "log2", None}, int or float, default="sqrt"
1868         The number of features to consider when looking for the best split:
1869 
1870         - If int, then consider `max_features` features at each split.
1871         - If float, then `max_features` is a fraction and
1872           `max(1, int(max_features * n_features_in_))` features are considered at each
1873           split.
1874         - If "auto", then `max_features=sqrt(n_features)`.
1875         - If "sqrt", then `max_features=sqrt(n_features)`.
1876         - If "log2", then `max_features=log2(n_features)`.
1877         - If None, then `max_features=n_features`.
1878 
1879         .. versionchanged:: 1.1
1880             The default of `max_features` changed from `"auto"` to `"sqrt"`.
1881 
1882         .. deprecated:: 1.1
1883             The `"auto"` option was deprecated in 1.1 and will be removed
1884             in 1.3.
1885 
1886         Note: the search for a split does not stop until at least one
1887         valid partition of the node samples is found, even if it requires to
1888         effectively inspect more than ``max_features`` features.
1889 
1890     max_leaf_nodes : int, default=None
1891         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1892         Best nodes are defined as relative reduction in impurity.
1893         If None then unlimited number of leaf nodes.
1894 
1895     min_impurity_decrease : float, default=0.0
1896         A node will be split if this split induces a decrease of the impurity
1897         greater than or equal to this value.
1898 
1899         The weighted impurity decrease equation is the following::
1900 
1901             N_t / N * (impurity - N_t_R / N_t * right_impurity
1902                                 - N_t_L / N_t * left_impurity)
1903 
1904         where ``N`` is the total number of samples, ``N_t`` is the number of
1905         samples at the current node, ``N_t_L`` is the number of samples in the
1906         left child, and ``N_t_R`` is the number of samples in the right child.
1907 
1908         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1909         if ``sample_weight`` is passed.
1910 
1911         .. versionadded:: 0.19
1912 
1913     bootstrap : bool, default=False
1914         Whether bootstrap samples are used when building trees. If False, the
1915         whole dataset is used to build each tree.
1916 
1917     oob_score : bool or callable, default=False
1918         Whether to use out-of-bag samples to estimate the generalization score.
1919         By default, :func:`~sklearn.metrics.accuracy_score` is used.
1920         Provide a callable with signature `metric(y_true, y_pred)` to use a
1921         custom metric. Only available if `bootstrap=True`.
1922 
1923     n_jobs : int, default=None
1924         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
1925         :meth:`decision_path` and :meth:`apply` are all parallelized over the
1926         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
1927         context. ``-1`` means using all processors. See :term:`Glossary
1928         <n_jobs>` for more details.
1929 
1930     random_state : int, RandomState instance or None, default=None
1931         Controls 3 sources of randomness:
1932 
1933         - the bootstrapping of the samples used when building trees
1934           (if ``bootstrap=True``)
1935         - the sampling of the features to consider when looking for the best
1936           split at each node (if ``max_features < n_features``)
1937         - the draw of the splits for each of the `max_features`
1938 
1939         See :term:`Glossary <random_state>` for details.
1940 
1941     verbose : int, default=0
1942         Controls the verbosity when fitting and predicting.
1943 
1944     warm_start : bool, default=False
1945         When set to ``True``, reuse the solution of the previous call to fit
1946         and add more estimators to the ensemble, otherwise, just fit a whole
1947         new forest. See :term:`Glossary <warm_start>` and
1948         :ref:`gradient_boosting_warm_start` for details.
1949 
1950     class_weight : {"balanced", "balanced_subsample"}, dict or list of dicts, \
1951             default=None
1952         Weights associated with classes in the form ``{class_label: weight}``.
1953         If not given, all classes are supposed to have weight one. For
1954         multi-output problems, a list of dicts can be provided in the same
1955         order as the columns of y.
1956 
1957         Note that for multioutput (including multilabel) weights should be
1958         defined for each class of every column in its own dict. For example,
1959         for four-class multilabel classification weights should be
1960         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
1961         [{1:1}, {2:5}, {3:1}, {4:1}].
1962 
1963         The "balanced" mode uses the values of y to automatically adjust
1964         weights inversely proportional to class frequencies in the input data
1965         as ``n_samples / (n_classes * np.bincount(y))``
1966 
1967         The "balanced_subsample" mode is the same as "balanced" except that
1968         weights are computed based on the bootstrap sample for every tree
1969         grown.
1970 
1971         For multi-output, the weights of each column of y will be multiplied.
1972 
1973         Note that these weights will be multiplied with sample_weight (passed
1974         through the fit method) if sample_weight is specified.
1975 
1976     ccp_alpha : non-negative float, default=0.0
1977         Complexity parameter used for Minimal Cost-Complexity Pruning. The
1978         subtree with the largest cost complexity that is smaller than
1979         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
1980         :ref:`minimal_cost_complexity_pruning` for details.
1981 
1982         .. versionadded:: 0.22
1983 
1984     max_samples : int or float, default=None
1985         If bootstrap is True, the number of samples to draw from X
1986         to train each base estimator.
1987 
1988         - If None (default), then draw `X.shape[0]` samples.
1989         - If int, then draw `max_samples` samples.
1990         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
1991           `max_samples` should be in the interval `(0.0, 1.0]`.
1992 
1993         .. versionadded:: 0.22
1994 
1995     Attributes
1996     ----------
1997     estimator_ : :class:`~sklearn.tree.ExtraTreesClassifier`
1998         The child estimator template used to create the collection of fitted
1999         sub-estimators.
2000 
2001         .. versionadded:: 1.2
2002            `base_estimator_` was renamed to `estimator_`.
2003 
2004     base_estimator_ : ExtraTreesClassifier
2005         The child estimator template used to create the collection of fitted
2006         sub-estimators.
2007 
2008         .. deprecated:: 1.2
2009             `base_estimator_` is deprecated and will be removed in 1.4.
2010             Use `estimator_` instead.
2011 
2012     estimators_ : list of DecisionTreeClassifier
2013         The collection of fitted sub-estimators.
2014 
2015     classes_ : ndarray of shape (n_classes,) or a list of such arrays
2016         The classes labels (single output problem), or a list of arrays of
2017         class labels (multi-output problem).
2018 
2019     n_classes_ : int or list
2020         The number of classes (single output problem), or a list containing the
2021         number of classes for each output (multi-output problem).
2022 
2023     feature_importances_ : ndarray of shape (n_features,)
2024         The impurity-based feature importances.
2025         The higher, the more important the feature.
2026         The importance of a feature is computed as the (normalized)
2027         total reduction of the criterion brought by that feature.  It is also
2028         known as the Gini importance.
2029 
2030         Warning: impurity-based feature importances can be misleading for
2031         high cardinality features (many unique values). See
2032         :func:`sklearn.inspection.permutation_importance` as an alternative.
2033 
2034     n_features_in_ : int
2035         Number of features seen during :term:`fit`.
2036 
2037         .. versionadded:: 0.24
2038 
2039     feature_names_in_ : ndarray of shape (`n_features_in_`,)
2040         Names of features seen during :term:`fit`. Defined only when `X`
2041         has feature names that are all strings.
2042 
2043         .. versionadded:: 1.0
2044 
2045     n_outputs_ : int
2046         The number of outputs when ``fit`` is performed.
2047 
2048     oob_score_ : float
2049         Score of the training dataset obtained using an out-of-bag estimate.
2050         This attribute exists only when ``oob_score`` is True.
2051 
2052     oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \
2053             (n_samples, n_classes, n_outputs)
2054         Decision function computed with out-of-bag estimate on the training
2055         set. If n_estimators is small it might be possible that a data point
2056         was never left out during the bootstrap. In this case,
2057         `oob_decision_function_` might contain NaN. This attribute exists
2058         only when ``oob_score`` is True.
2059 
2060     See Also
2061     --------
2062     ExtraTreesRegressor : An extra-trees regressor with random splits.
2063     RandomForestClassifier : A random forest classifier with optimal splits.
2064     RandomForestRegressor : Ensemble regressor using trees with optimal splits.
2065 
2066     Notes
2067     -----
2068     The default values for the parameters controlling the size of the trees
2069     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
2070     unpruned trees which can potentially be very large on some data sets. To
2071     reduce memory consumption, the complexity and size of the trees should be
2072     controlled by setting those parameter values.
2073 
2074     References
2075     ----------
2076     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
2077            trees", Machine Learning, 63(1), 3-42, 2006.
2078 
2079     Examples
2080     --------
2081     >>> from sklearn.ensemble import ExtraTreesClassifier
2082     >>> from sklearn.datasets import make_classification
2083     >>> X, y = make_classification(n_features=4, random_state=0)
2084     >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)
2085     >>> clf.fit(X, y)
2086     ExtraTreesClassifier(random_state=0)
2087     >>> clf.predict([[0, 0, 0, 0]])
2088     array([1])
2089     """
2090 
2091     _parameter_constraints: dict = {
2092         **ForestClassifier._parameter_constraints,
2093         **DecisionTreeClassifier._parameter_constraints,
2094         "class_weight": [
2095             StrOptions({"balanced_subsample", "balanced"}),
2096             dict,
2097             list,
2098             None,
2099         ],
2100     }
2101     _parameter_constraints.pop("splitter")
2102 
2103     def __init__(
2104         self,
2105         n_estimators=100,
2106         *,
2107         criterion="gini",
2108         max_depth=None,
2109         min_samples_split=2,
2110         min_samples_leaf=1,
2111         min_weight_fraction_leaf=0.0,
2112         max_features="sqrt",
2113         max_leaf_nodes=None,
2114         min_impurity_decrease=0.0,
2115         bootstrap=False,
2116         oob_score=False,
2117         n_jobs=None,
2118         random_state=None,
2119         verbose=0,
2120         warm_start=False,
2121         class_weight=None,
2122         ccp_alpha=0.0,
2123         max_samples=None,
2124     ):
2125         super().__init__(
2126             estimator=ExtraTreeClassifier(),
2127             n_estimators=n_estimators,
2128             estimator_params=(
2129                 "criterion",
2130                 "max_depth",
2131                 "min_samples_split",
2132                 "min_samples_leaf",
2133                 "min_weight_fraction_leaf",
2134                 "max_features",
2135                 "max_leaf_nodes",
2136                 "min_impurity_decrease",
2137                 "random_state",
2138                 "ccp_alpha",
2139             ),
2140             bootstrap=bootstrap,
2141             oob_score=oob_score,
2142             n_jobs=n_jobs,
2143             random_state=random_state,
2144             verbose=verbose,
2145             warm_start=warm_start,
2146             class_weight=class_weight,
2147             max_samples=max_samples,
2148         )
2149 
2150         self.criterion = criterion
2151         self.max_depth = max_depth
2152         self.min_samples_split = min_samples_split
2153         self.min_samples_leaf = min_samples_leaf
2154         self.min_weight_fraction_leaf = min_weight_fraction_leaf
2155         self.max_features = max_features
2156         self.max_leaf_nodes = max_leaf_nodes
2157         self.min_impurity_decrease = min_impurity_decrease
2158         self.ccp_alpha = ccp_alpha
2159 
2160 
2161 class ExtraTreesRegressor(ForestRegressor):
2162     """
2163     An extra-trees regressor.
2164 
2165     This class implements a meta estimator that fits a number of
2166     randomized decision trees (a.k.a. extra-trees) on various sub-samples
2167     of the dataset and uses averaging to improve the predictive accuracy
2168     and control over-fitting.
2169 
2170     Read more in the :ref:`User Guide <forest>`.
2171 
2172     Parameters
2173     ----------
2174     n_estimators : int, default=100
2175         The number of trees in the forest.
2176 
2177         .. versionchanged:: 0.22
2178            The default value of ``n_estimators`` changed from 10 to 100
2179            in 0.22.
2180 
2181     criterion : {"squared_error", "absolute_error", "friedman_mse", "poisson"}, \
2182             default="squared_error"
2183         The function to measure the quality of a split. Supported criteria
2184         are "squared_error" for the mean squared error, which is equal to
2185         variance reduction as feature selection criterion and minimizes the L2
2186         loss using the mean of each terminal node, "friedman_mse", which uses
2187         mean squared error with Friedman's improvement score for potential
2188         splits, "absolute_error" for the mean absolute error, which minimizes
2189         the L1 loss using the median of each terminal node, and "poisson" which
2190         uses reduction in Poisson deviance to find splits.
2191         Training using "absolute_error" is significantly slower
2192         than when using "squared_error".
2193 
2194         .. versionadded:: 0.18
2195            Mean Absolute Error (MAE) criterion.
2196 
2197     max_depth : int, default=None
2198         The maximum depth of the tree. If None, then nodes are expanded until
2199         all leaves are pure or until all leaves contain less than
2200         min_samples_split samples.
2201 
2202     min_samples_split : int or float, default=2
2203         The minimum number of samples required to split an internal node:
2204 
2205         - If int, then consider `min_samples_split` as the minimum number.
2206         - If float, then `min_samples_split` is a fraction and
2207           `ceil(min_samples_split * n_samples)` are the minimum
2208           number of samples for each split.
2209 
2210         .. versionchanged:: 0.18
2211            Added float values for fractions.
2212 
2213     min_samples_leaf : int or float, default=1
2214         The minimum number of samples required to be at a leaf node.
2215         A split point at any depth will only be considered if it leaves at
2216         least ``min_samples_leaf`` training samples in each of the left and
2217         right branches.  This may have the effect of smoothing the model,
2218         especially in regression.
2219 
2220         - If int, then consider `min_samples_leaf` as the minimum number.
2221         - If float, then `min_samples_leaf` is a fraction and
2222           `ceil(min_samples_leaf * n_samples)` are the minimum
2223           number of samples for each node.
2224 
2225         .. versionchanged:: 0.18
2226            Added float values for fractions.
2227 
2228     min_weight_fraction_leaf : float, default=0.0
2229         The minimum weighted fraction of the sum total of weights (of all
2230         the input samples) required to be at a leaf node. Samples have
2231         equal weight when sample_weight is not provided.
2232 
2233     max_features : {"sqrt", "log2", None}, int or float, default=1.0
2234         The number of features to consider when looking for the best split:
2235 
2236         - If int, then consider `max_features` features at each split.
2237         - If float, then `max_features` is a fraction and
2238           `max(1, int(max_features * n_features_in_))` features are considered at each
2239           split.
2240         - If "auto", then `max_features=n_features`.
2241         - If "sqrt", then `max_features=sqrt(n_features)`.
2242         - If "log2", then `max_features=log2(n_features)`.
2243         - If None or 1.0, then `max_features=n_features`.
2244 
2245         .. note::
2246             The default of 1.0 is equivalent to bagged trees and more
2247             randomness can be achieved by setting smaller values, e.g. 0.3.
2248 
2249         .. versionchanged:: 1.1
2250             The default of `max_features` changed from `"auto"` to 1.0.
2251 
2252         .. deprecated:: 1.1
2253             The `"auto"` option was deprecated in 1.1 and will be removed
2254             in 1.3.
2255 
2256         Note: the search for a split does not stop until at least one
2257         valid partition of the node samples is found, even if it requires to
2258         effectively inspect more than ``max_features`` features.
2259 
2260     max_leaf_nodes : int, default=None
2261         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2262         Best nodes are defined as relative reduction in impurity.
2263         If None then unlimited number of leaf nodes.
2264 
2265     min_impurity_decrease : float, default=0.0
2266         A node will be split if this split induces a decrease of the impurity
2267         greater than or equal to this value.
2268 
2269         The weighted impurity decrease equation is the following::
2270 
2271             N_t / N * (impurity - N_t_R / N_t * right_impurity
2272                                 - N_t_L / N_t * left_impurity)
2273 
2274         where ``N`` is the total number of samples, ``N_t`` is the number of
2275         samples at the current node, ``N_t_L`` is the number of samples in the
2276         left child, and ``N_t_R`` is the number of samples in the right child.
2277 
2278         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2279         if ``sample_weight`` is passed.
2280 
2281         .. versionadded:: 0.19
2282 
2283     bootstrap : bool, default=False
2284         Whether bootstrap samples are used when building trees. If False, the
2285         whole dataset is used to build each tree.
2286 
2287     oob_score : bool or callable, default=False
2288         Whether to use out-of-bag samples to estimate the generalization score.
2289         By default, :func:`~sklearn.metrics.r2_score` is used.
2290         Provide a callable with signature `metric(y_true, y_pred)` to use a
2291         custom metric. Only available if `bootstrap=True`.
2292 
2293     n_jobs : int, default=None
2294         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,
2295         :meth:`decision_path` and :meth:`apply` are all parallelized over the
2296         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
2297         context. ``-1`` means using all processors. See :term:`Glossary
2298         <n_jobs>` for more details.
2299 
2300     random_state : int, RandomState instance or None, default=None
2301         Controls 3 sources of randomness:
2302 
2303         - the bootstrapping of the samples used when building trees
2304           (if ``bootstrap=True``)
2305         - the sampling of the features to consider when looking for the best
2306           split at each node (if ``max_features < n_features``)
2307         - the draw of the splits for each of the `max_features`
2308 
2309         See :term:`Glossary <random_state>` for details.
2310 
2311     verbose : int, default=0
2312         Controls the verbosity when fitting and predicting.
2313 
2314     warm_start : bool, default=False
2315         When set to ``True``, reuse the solution of the previous call to fit
2316         and add more estimators to the ensemble, otherwise, just fit a whole
2317         new forest. See :term:`Glossary <warm_start>` and
2318         :ref:`gradient_boosting_warm_start` for details.
2319 
2320     ccp_alpha : non-negative float, default=0.0
2321         Complexity parameter used for Minimal Cost-Complexity Pruning. The
2322         subtree with the largest cost complexity that is smaller than
2323         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See
2324         :ref:`minimal_cost_complexity_pruning` for details.
2325 
2326         .. versionadded:: 0.22
2327 
2328     max_samples : int or float, default=None
2329         If bootstrap is True, the number of samples to draw from X
2330         to train each base estimator.
2331 
2332         - If None (default), then draw `X.shape[0]` samples.
2333         - If int, then draw `max_samples` samples.
2334         - If float, then draw `max_samples * X.shape[0]` samples. Thus,
2335           `max_samples` should be in the interval `(0.0, 1.0]`.
2336 
2337         .. versionadded:: 0.22
2338 
2339     Attributes
2340     ----------
2341     estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`
2342         The child estimator template used to create the collection of fitted
2343         sub-estimators.
2344 
2345         .. versionadded:: 1.2
2346            `base_estimator_` was renamed to `estimator_`.
2347 
2348     base_estimator_ : ExtraTreeRegressor
2349         The child estimator template used to create the collection of fitted
2350         sub-estimators.
2351 
2352         .. deprecated:: 1.2
2353             `base_estimator_` is deprecated and will be removed in 1.4.
2354             Use `estimator_` instead.
2355 
2356     estimators_ : list of DecisionTreeRegressor
2357         The collection of fitted sub-estimators.
2358 
2359     feature_importances_ : ndarray of shape (n_features,)
2360         The impurity-based feature importances.
2361         The higher, the more important the feature.
2362         The importance of a feature is computed as the (normalized)
2363         total reduction of the criterion brought by that feature.  It is also
2364         known as the Gini importance.
2365 
2366         Warning: impurity-based feature importances can be misleading for
2367         high cardinality features (many unique values). See
2368         :func:`sklearn.inspection.permutation_importance` as an alternative.
2369 
2370     n_features_in_ : int
2371         Number of features seen during :term:`fit`.
2372 
2373         .. versionadded:: 0.24
2374 
2375     feature_names_in_ : ndarray of shape (`n_features_in_`,)
2376         Names of features seen during :term:`fit`. Defined only when `X`
2377         has feature names that are all strings.
2378 
2379         .. versionadded:: 1.0
2380 
2381     n_outputs_ : int
2382         The number of outputs.
2383 
2384     oob_score_ : float
2385         Score of the training dataset obtained using an out-of-bag estimate.
2386         This attribute exists only when ``oob_score`` is True.
2387 
2388     oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)
2389         Prediction computed with out-of-bag estimate on the training set.
2390         This attribute exists only when ``oob_score`` is True.
2391 
2392     See Also
2393     --------
2394     ExtraTreesClassifier : An extra-trees classifier with random splits.
2395     RandomForestClassifier : A random forest classifier with optimal splits.
2396     RandomForestRegressor : Ensemble regressor using trees with optimal splits.
2397 
2398     Notes
2399     -----
2400     The default values for the parameters controlling the size of the trees
2401     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
2402     unpruned trees which can potentially be very large on some data sets. To
2403     reduce memory consumption, the complexity and size of the trees should be
2404     controlled by setting those parameter values.
2405 
2406     References
2407     ----------
2408     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
2409            Machine Learning, 63(1), 3-42, 2006.
2410 
2411     Examples
2412     --------
2413     >>> from sklearn.datasets import load_diabetes
2414     >>> from sklearn.model_selection import train_test_split
2415     >>> from sklearn.ensemble import ExtraTreesRegressor
2416     >>> X, y = load_diabetes(return_X_y=True)
2417     >>> X_train, X_test, y_train, y_test = train_test_split(
2418     ...     X, y, random_state=0)
2419     >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(
2420     ...    X_train, y_train)
2421     >>> reg.score(X_test, y_test)
2422     0.2727...
2423     """
2424 
2425     _parameter_constraints: dict = {
2426         **ForestRegressor._parameter_constraints,
2427         **DecisionTreeRegressor._parameter_constraints,
2428     }
2429     _parameter_constraints.pop("splitter")
2430 
2431     def __init__(
2432         self,
2433         n_estimators=100,
2434         *,
2435         criterion="squared_error",
2436         max_depth=None,
2437         min_samples_split=2,
2438         min_samples_leaf=1,
2439         min_weight_fraction_leaf=0.0,
2440         max_features=1.0,
2441         max_leaf_nodes=None,
2442         min_impurity_decrease=0.0,
2443         bootstrap=False,
2444         oob_score=False,
2445         n_jobs=None,
2446         random_state=None,
2447         verbose=0,
2448         warm_start=False,
2449         ccp_alpha=0.0,
2450         max_samples=None,
2451     ):
2452         super().__init__(
2453             estimator=ExtraTreeRegressor(),
2454             n_estimators=n_estimators,
2455             estimator_params=(
2456                 "criterion",
2457                 "max_depth",
2458                 "min_samples_split",
2459                 "min_samples_leaf",
2460                 "min_weight_fraction_leaf",
2461                 "max_features",
2462                 "max_leaf_nodes",
2463                 "min_impurity_decrease",
2464                 "random_state",
2465                 "ccp_alpha",
2466             ),
2467             bootstrap=bootstrap,
2468             oob_score=oob_score,
2469             n_jobs=n_jobs,
2470             random_state=random_state,
2471             verbose=verbose,
2472             warm_start=warm_start,
2473             max_samples=max_samples,
2474         )
2475 
2476         self.criterion = criterion
2477         self.max_depth = max_depth
2478         self.min_samples_split = min_samples_split
2479         self.min_samples_leaf = min_samples_leaf
2480         self.min_weight_fraction_leaf = min_weight_fraction_leaf
2481         self.max_features = max_features
2482         self.max_leaf_nodes = max_leaf_nodes
2483         self.min_impurity_decrease = min_impurity_decrease
2484         self.ccp_alpha = ccp_alpha
2485 
2486 
2487 class RandomTreesEmbedding(TransformerMixin, BaseForest):
2488     """
2489     An ensemble of totally random trees.
2490 
2491     An unsupervised transformation of a dataset to a high-dimensional
2492     sparse representation. A datapoint is coded according to which leaf of
2493     each tree it is sorted into. Using a one-hot encoding of the leaves,
2494     this leads to a binary coding with as many ones as there are trees in
2495     the forest.
2496 
2497     The dimensionality of the resulting representation is
2498     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
2499     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.
2500 
2501     Read more in the :ref:`User Guide <random_trees_embedding>`.
2502 
2503     Parameters
2504     ----------
2505     n_estimators : int, default=100
2506         Number of trees in the forest.
2507 
2508         .. versionchanged:: 0.22
2509            The default value of ``n_estimators`` changed from 10 to 100
2510            in 0.22.
2511 
2512     max_depth : int, default=5
2513         The maximum depth of each tree. If None, then nodes are expanded until
2514         all leaves are pure or until all leaves contain less than
2515         min_samples_split samples.
2516 
2517     min_samples_split : int or float, default=2
2518         The minimum number of samples required to split an internal node:
2519 
2520         - If int, then consider `min_samples_split` as the minimum number.
2521         - If float, then `min_samples_split` is a fraction and
2522           `ceil(min_samples_split * n_samples)` is the minimum
2523           number of samples for each split.
2524 
2525         .. versionchanged:: 0.18
2526            Added float values for fractions.
2527 
2528     min_samples_leaf : int or float, default=1
2529         The minimum number of samples required to be at a leaf node.
2530         A split point at any depth will only be considered if it leaves at
2531         least ``min_samples_leaf`` training samples in each of the left and
2532         right branches.  This may have the effect of smoothing the model,
2533         especially in regression.
2534 
2535         - If int, then consider `min_samples_leaf` as the minimum number.
2536         - If float, then `min_samples_leaf` is a fraction and
2537           `ceil(min_samples_leaf * n_samples)` is the minimum
2538           number of samples for each node.
2539 
2540         .. versionchanged:: 0.18
2541            Added float values for fractions.
2542 
2543     min_weight_fraction_leaf : float, default=0.0
2544         The minimum weighted fraction of the sum total of weights (of all
2545         the input samples) required to be at a leaf node. Samples have
2546         equal weight when sample_weight is not provided.
2547 
2548     max_leaf_nodes : int, default=None
2549         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2550         Best nodes are defined as relative reduction in impurity.
2551         If None then unlimited number of leaf nodes.
2552 
2553     min_impurity_decrease : float, default=0.0
2554         A node will be split if this split induces a decrease of the impurity
2555         greater than or equal to this value.
2556 
2557         The weighted impurity decrease equation is the following::
2558 
2559             N_t / N * (impurity - N_t_R / N_t * right_impurity
2560                                 - N_t_L / N_t * left_impurity)
2561 
2562         where ``N`` is the total number of samples, ``N_t`` is the number of
2563         samples at the current node, ``N_t_L`` is the number of samples in the
2564         left child, and ``N_t_R`` is the number of samples in the right child.
2565 
2566         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2567         if ``sample_weight`` is passed.
2568 
2569         .. versionadded:: 0.19
2570 
2571     sparse_output : bool, default=True
2572         Whether or not to return a sparse CSR matrix, as default behavior,
2573         or to return a dense array compatible with dense pipeline operators.
2574 
2575     n_jobs : int, default=None
2576         The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,
2577         :meth:`decision_path` and :meth:`apply` are all parallelized over the
2578         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`
2579         context. ``-1`` means using all processors. See :term:`Glossary
2580         <n_jobs>` for more details.
2581 
2582     random_state : int, RandomState instance or None, default=None
2583         Controls the generation of the random `y` used to fit the trees
2584         and the draw of the splits for each feature at the trees' nodes.
2585         See :term:`Glossary <random_state>` for details.
2586 
2587     verbose : int, default=0
2588         Controls the verbosity when fitting and predicting.
2589 
2590     warm_start : bool, default=False
2591         When set to ``True``, reuse the solution of the previous call to fit
2592         and add more estimators to the ensemble, otherwise, just fit a whole
2593         new forest. See :term:`Glossary <warm_start>` and
2594         :ref:`gradient_boosting_warm_start` for details.
2595 
2596     Attributes
2597     ----------
2598     estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
2599         The child estimator template used to create the collection of fitted
2600         sub-estimators.
2601 
2602         .. versionadded:: 1.2
2603            `base_estimator_` was renamed to `estimator_`.
2604 
2605     base_estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance
2606         The child estimator template used to create the collection of fitted
2607         sub-estimators.
2608 
2609         .. deprecated:: 1.2
2610             `base_estimator_` is deprecated and will be removed in 1.4.
2611             Use `estimator_` instead.
2612 
2613     estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances
2614         The collection of fitted sub-estimators.
2615 
2616     feature_importances_ : ndarray of shape (n_features,)
2617         The feature importances (the higher, the more important the feature).
2618 
2619     n_features_in_ : int
2620         Number of features seen during :term:`fit`.
2621 
2622         .. versionadded:: 0.24
2623 
2624     feature_names_in_ : ndarray of shape (`n_features_in_`,)
2625         Names of features seen during :term:`fit`. Defined only when `X`
2626         has feature names that are all strings.
2627 
2628         .. versionadded:: 1.0
2629 
2630     n_outputs_ : int
2631         The number of outputs when ``fit`` is performed.
2632 
2633     one_hot_encoder_ : OneHotEncoder instance
2634         One-hot encoder used to create the sparse embedding.
2635 
2636     See Also
2637     --------
2638     ExtraTreesClassifier : An extra-trees classifier.
2639     ExtraTreesRegressor : An extra-trees regressor.
2640     RandomForestClassifier : A random forest classifier.
2641     RandomForestRegressor : A random forest regressor.
2642     sklearn.tree.ExtraTreeClassifier: An extremely randomized
2643         tree classifier.
2644     sklearn.tree.ExtraTreeRegressor : An extremely randomized
2645         tree regressor.
2646 
2647     References
2648     ----------
2649     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
2650            Machine Learning, 63(1), 3-42, 2006.
2651     .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  "Fast discriminative
2652            visual codebooks using randomized clustering forests"
2653            NIPS 2007
2654 
2655     Examples
2656     --------
2657     >>> from sklearn.ensemble import RandomTreesEmbedding
2658     >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]
2659     >>> random_trees = RandomTreesEmbedding(
2660     ...    n_estimators=5, random_state=0, max_depth=1).fit(X)
2661     >>> X_sparse_embedding = random_trees.transform(X)
2662     >>> X_sparse_embedding.toarray()
2663     array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
2664            [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],
2665            [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
2666            [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],
2667            [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])
2668     """
2669 
2670     _parameter_constraints: dict = {
2671         "n_estimators": [Interval(Integral, 1, None, closed="left")],
2672         "n_jobs": [Integral, None],
2673         "verbose": ["verbose"],
2674         "warm_start": ["boolean"],
2675         **BaseDecisionTree._parameter_constraints,
2676         "sparse_output": ["boolean"],
2677     }
2678     for param in ("max_features", "ccp_alpha", "splitter"):
2679         _parameter_constraints.pop(param)
2680 
2681     criterion = "squared_error"
2682     max_features = 1
2683 
2684     def __init__(
2685         self,
2686         n_estimators=100,
2687         *,
2688         max_depth=5,
2689         min_samples_split=2,
2690         min_samples_leaf=1,
2691         min_weight_fraction_leaf=0.0,
2692         max_leaf_nodes=None,
2693         min_impurity_decrease=0.0,
2694         sparse_output=True,
2695         n_jobs=None,
2696         random_state=None,
2697         verbose=0,
2698         warm_start=False,
2699     ):
2700         super().__init__(
2701             estimator=ExtraTreeRegressor(),
2702             n_estimators=n_estimators,
2703             estimator_params=(
2704                 "criterion",
2705                 "max_depth",
2706                 "min_samples_split",
2707                 "min_samples_leaf",
2708                 "min_weight_fraction_leaf",
2709                 "max_features",
2710                 "max_leaf_nodes",
2711                 "min_impurity_decrease",
2712                 "random_state",
2713             ),
2714             bootstrap=False,
2715             oob_score=False,
2716             n_jobs=n_jobs,
2717             random_state=random_state,
2718             verbose=verbose,
2719             warm_start=warm_start,
2720             max_samples=None,
2721         )
2722 
2723         self.max_depth = max_depth
2724         self.min_samples_split = min_samples_split
2725         self.min_samples_leaf = min_samples_leaf
2726         self.min_weight_fraction_leaf = min_weight_fraction_leaf
2727         self.max_leaf_nodes = max_leaf_nodes
2728         self.min_impurity_decrease = min_impurity_decrease
2729         self.sparse_output = sparse_output
2730 
2731     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):
2732         raise NotImplementedError("OOB score not supported by tree embedding")
2733 
2734     def fit(self, X, y=None, sample_weight=None):
2735         """
2736         Fit estimator.
2737 
2738         Parameters
2739         ----------
2740         X : {array-like, sparse matrix} of shape (n_samples, n_features)
2741             The input samples. Use ``dtype=np.float32`` for maximum
2742             efficiency. Sparse matrices are also supported, use sparse
2743             ``csc_matrix`` for maximum efficiency.
2744 
2745         y : Ignored
2746             Not used, present for API consistency by convention.
2747 
2748         sample_weight : array-like of shape (n_samples,), default=None
2749             Sample weights. If None, then samples are equally weighted. Splits
2750             that would create child nodes with net zero or negative weight are
2751             ignored while searching for a split in each node. In the case of
2752             classification, splits are also ignored if they would result in any
2753             single class carrying a negative weight in either child node.
2754 
2755         Returns
2756         -------
2757         self : object
2758             Returns the instance itself.
2759         """
2760         # Parameters are validated in fit_transform
2761         self.fit_transform(X, y, sample_weight=sample_weight)
2762         return self
2763 
2764     def fit_transform(self, X, y=None, sample_weight=None):
2765         """
2766         Fit estimator and transform dataset.
2767 
2768         Parameters
2769         ----------
2770         X : {array-like, sparse matrix} of shape (n_samples, n_features)
2771             Input data used to build forests. Use ``dtype=np.float32`` for
2772             maximum efficiency.
2773 
2774         y : Ignored
2775             Not used, present for API consistency by convention.
2776 
2777         sample_weight : array-like of shape (n_samples,), default=None
2778             Sample weights. If None, then samples are equally weighted. Splits
2779             that would create child nodes with net zero or negative weight are
2780             ignored while searching for a split in each node. In the case of
2781             classification, splits are also ignored if they would result in any
2782             single class carrying a negative weight in either child node.
2783 
2784         Returns
2785         -------
2786         X_transformed : sparse matrix of shape (n_samples, n_out)
2787             Transformed dataset.
2788         """
2789         self._validate_params()
2790 
2791         rnd = check_random_state(self.random_state)
2792         y = rnd.uniform(size=_num_samples(X))
2793         super().fit(X, y, sample_weight=sample_weight)
2794 
2795         self.one_hot_encoder_ = OneHotEncoder(sparse_output=self.sparse_output)
2796         output = self.one_hot_encoder_.fit_transform(self.apply(X))
2797         self._n_features_out = output.shape[1]
2798         return output
2799 
2800     def get_feature_names_out(self, input_features=None):
2801         """Get output feature names for transformation.
2802 
2803         Parameters
2804         ----------
2805         input_features : array-like of str or None, default=None
2806             Only used to validate feature names with the names seen in :meth:`fit`.
2807 
2808         Returns
2809         -------
2810         feature_names_out : ndarray of str objects
2811             Transformed feature names, in the format of
2812             `randomtreesembedding_{tree}_{leaf}`, where `tree` is the tree used
2813             to generate the leaf and `leaf` is the index of a leaf node
2814             in that tree. Note that the node indexing scheme is used to
2815             index both nodes with children (split nodes) and leaf nodes.
2816             Only the latter can be present as output features.
2817             As a consequence, there are missing indices in the output
2818             feature names.
2819         """
2820         check_is_fitted(self, "_n_features_out")
2821         _check_feature_names_in(
2822             self, input_features=input_features, generate_names=False
2823         )
2824 
2825         feature_names = [
2826             f"randomtreesembedding_{tree}_{leaf}"
2827             for tree in range(self.n_estimators)
2828             for leaf in self.one_hot_encoder_.categories_[tree]
2829         ]
2830         return np.asarray(feature_names, dtype=object)
2831 
2832     def transform(self, X):
2833         """
2834         Transform dataset.
2835 
2836         Parameters
2837         ----------
2838         X : {array-like, sparse matrix} of shape (n_samples, n_features)
2839             Input data to be transformed. Use ``dtype=np.float32`` for maximum
2840             efficiency. Sparse matrices are also supported, use sparse
2841             ``csr_matrix`` for maximum efficiency.
2842 
2843         Returns
2844         -------
2845         X_transformed : sparse matrix of shape (n_samples, n_out)
2846             Transformed dataset.
2847         """
2848         check_is_fitted(self)
2849         return self.one_hot_encoder_.transform(self.apply(X))

</code><intended_behavior>Similar to the above, modifications should ensure that configurations pertinent to the fitting process (possibly including random state settings, number of jobs, etc.) are explicitly passed to each parallel execution context initiated via delayed. This may include augmenting the delayed call to ensure it carries over or applies the current configuration settings within each executed task, preventing misconfigurations when tasks run in threads/processes different from the main thread initializing them.</intended_behavior>]
2025-10-16 00:05:42.946 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-16 00:05:42.947 | INFO     | Search-only: skipping patch selection
2025-10-16 00:05:43.025 | INFO     | Task scikit-learn__scikit-learn-25363 completed successfully.
2025-10-16 00:05:43.026 | INFO     | No patch generated. You can try running ACR again.
