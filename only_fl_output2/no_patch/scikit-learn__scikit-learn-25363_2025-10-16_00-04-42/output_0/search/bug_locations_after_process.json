[
    {
        "rel_file_path": "sklearn/model_selection/_validation.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__1.3/sklearn/model_selection/_validation.py",
        "start": 1,
        "end": 1931,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 The :mod:`sklearn.model_selection._validation` module includes classes and\n3 functions to validate the model.\n4 \"\"\"\n5 \n6 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n7 #         Gael Varoquaux <gael.varoquaux@normalesup.org>\n8 #         Olivier Grisel <olivier.grisel@ensta.org>\n9 #         Raghav RV <rvraghav93@gmail.com>\n10 #         Michal Karbownik <michakarbownik@gmail.com>\n11 # License: BSD 3 clause\n12 \n13 \n14 import warnings\n15 import numbers\n16 import time\n17 from functools import partial\n18 from traceback import format_exc\n19 from contextlib import suppress\n20 from collections import Counter\n21 \n22 import numpy as np\n23 import scipy.sparse as sp\n24 from joblib import Parallel, logger\n25 \n26 from ..base import is_classifier, clone\n27 from ..utils import indexable, check_random_state, _safe_indexing\n28 from ..utils.validation import _check_fit_params\n29 from ..utils.validation import _num_samples\n30 from ..utils.fixes import delayed\n31 from ..utils.metaestimators import _safe_split\n32 from ..metrics import check_scoring\n33 from ..metrics._scorer import _check_multimetric_scoring, _MultimetricScorer\n34 from ..exceptions import FitFailedWarning\n35 from ._split import check_cv\n36 from ..preprocessing import LabelEncoder\n37 \n38 \n39 __all__ = [\n40     \"cross_validate\",\n41     \"cross_val_score\",\n42     \"cross_val_predict\",\n43     \"permutation_test_score\",\n44     \"learning_curve\",\n45     \"validation_curve\",\n46 ]\n47 \n48 \n49 def cross_validate(\n50     estimator,\n51     X,\n52     y=None,\n53     *,\n54     groups=None,\n55     scoring=None,\n56     cv=None,\n57     n_jobs=None,\n58     verbose=0,\n59     fit_params=None,\n60     pre_dispatch=\"2*n_jobs\",\n61     return_train_score=False,\n62     return_estimator=False,\n63     error_score=np.nan,\n64 ):\n65     \"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\n66 \n67     Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n68 \n69     Parameters\n70     ----------\n71     estimator : estimator object implementing 'fit'\n72         The object to use to fit the data.\n73 \n74     X : array-like of shape (n_samples, n_features)\n75         The data to fit. Can be for example a list, or an array.\n76 \n77     y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\n78         The target variable to try to predict in the case of\n79         supervised learning.\n80 \n81     groups : array-like of shape (n_samples,), default=None\n82         Group labels for the samples used while splitting the dataset into\n83         train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n84         instance (e.g., :class:`GroupKFold`).\n85 \n86     scoring : str, callable, list, tuple, or dict, default=None\n87         Strategy to evaluate the performance of the cross-validated model on\n88         the test set.\n89 \n90         If `scoring` represents a single score, one can use:\n91 \n92         - a single string (see :ref:`scoring_parameter`);\n93         - a callable (see :ref:`scoring`) that returns a single value.\n94 \n95         If `scoring` represents multiple scores, one can use:\n96 \n97         - a list or tuple of unique strings;\n98         - a callable returning a dictionary where the keys are the metric\n99           names and the values are the metric scores;\n100         - a dictionary with metric names as keys and callables a values.\n101 \n102         See :ref:`multimetric_grid_search` for an example.\n103 \n104     cv : int, cross-validation generator or an iterable, default=None\n105         Determines the cross-validation splitting strategy.\n106         Possible inputs for cv are:\n107 \n108         - None, to use the default 5-fold cross validation,\n109         - int, to specify the number of folds in a `(Stratified)KFold`,\n110         - :term:`CV splitter`,\n111         - An iterable yielding (train, test) splits as arrays of indices.\n112 \n113         For int/None inputs, if the estimator is a classifier and ``y`` is\n114         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n115         other cases, :class:`KFold` is used. These splitters are instantiated\n116         with `shuffle=False` so the splits will be the same across calls.\n117 \n118         Refer :ref:`User Guide <cross_validation>` for the various\n119         cross-validation strategies that can be used here.\n120 \n121         .. versionchanged:: 0.22\n122             ``cv`` default value if None changed from 3-fold to 5-fold.\n123 \n124     n_jobs : int, default=None\n125         Number of jobs to run in parallel. Training the estimator and computing\n126         the score are parallelized over the cross-validation splits.\n127         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n128         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n129         for more details.\n130 \n131     verbose : int, default=0\n132         The verbosity level.\n133 \n134     fit_params : dict, default=None\n135         Parameters to pass to the fit method of the estimator.\n136 \n137     pre_dispatch : int or str, default='2*n_jobs'\n138         Controls the number of jobs that get dispatched during parallel\n139         execution. Reducing this number can be useful to avoid an\n140         explosion of memory consumption when more jobs get dispatched\n141         than CPUs can process. This parameter can be:\n142 \n143             - None, in which case all the jobs are immediately\n144               created and spawned. Use this for lightweight and\n145               fast-running jobs, to avoid delays due to on-demand\n146               spawning of the jobs\n147 \n148             - An int, giving the exact number of total jobs that are\n149               spawned\n150 \n151             - A str, giving an expression as a function of n_jobs,\n152               as in '2*n_jobs'\n153 \n154     return_train_score : bool, default=False\n155         Whether to include train scores.\n156         Computing training scores is used to get insights on how different\n157         parameter settings impact the overfitting/underfitting trade-off.\n158         However computing the scores on the training set can be computationally\n159         expensive and is not strictly required to select the parameters that\n160         yield the best generalization performance.\n161 \n162         .. versionadded:: 0.19\n163 \n164         .. versionchanged:: 0.21\n165             Default value was changed from ``True`` to ``False``\n166 \n167     return_estimator : bool, default=False\n168         Whether to return the estimators fitted on each split.\n169 \n170         .. versionadded:: 0.20\n171 \n172     error_score : 'raise' or numeric, default=np.nan\n173         Value to assign to the score if an error occurs in estimator fitting.\n174         If set to 'raise', the error is raised.\n175         If a numeric value is given, FitFailedWarning is raised.\n176 \n177         .. versionadded:: 0.20\n178 \n179     Returns\n180     -------\n181     scores : dict of float arrays of shape (n_splits,)\n182         Array of scores of the estimator for each run of the cross validation.\n183 \n184         A dict of arrays containing the score/time arrays for each scorer is\n185         returned. The possible keys for this ``dict`` are:\n186 \n187             ``test_score``\n188                 The score array for test scores on each cv split.\n189                 Suffix ``_score`` in ``test_score`` changes to a specific\n190                 metric like ``test_r2`` or ``test_auc`` if there are\n191                 multiple scoring metrics in the scoring parameter.\n192             ``train_score``\n193                 The score array for train scores on each cv split.\n194                 Suffix ``_score`` in ``train_score`` changes to a specific\n195                 metric like ``train_r2`` or ``train_auc`` if there are\n196                 multiple scoring metrics in the scoring parameter.\n197                 This is available only if ``return_train_score`` parameter\n198                 is ``True``.\n199             ``fit_time``\n200                 The time for fitting the estimator on the train\n201                 set for each cv split.\n202             ``score_time``\n203                 The time for scoring the estimator on the test set for each\n204                 cv split. (Note time for scoring on the train set is not\n205                 included even if ``return_train_score`` is set to ``True``\n206             ``estimator``\n207                 The estimator objects for each cv split.\n208                 This is available only if ``return_estimator`` parameter\n209                 is set to ``True``.\n210 \n211     See Also\n212     --------\n213     cross_val_score : Run cross-validation for single metric evaluation.\n214 \n215     cross_val_predict : Get predictions from each split of cross-validation for\n216         diagnostic purposes.\n217 \n218     sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n219         loss function.\n220 \n221     Examples\n222     --------\n223     >>> from sklearn import datasets, linear_model\n224     >>> from sklearn.model_selection import cross_validate\n225     >>> from sklearn.metrics import make_scorer\n226     >>> from sklearn.metrics import confusion_matrix\n227     >>> from sklearn.svm import LinearSVC\n228     >>> diabetes = datasets.load_diabetes()\n229     >>> X = diabetes.data[:150]\n230     >>> y = diabetes.target[:150]\n231     >>> lasso = linear_model.Lasso()\n232 \n233     Single metric evaluation using ``cross_validate``\n234 \n235     >>> cv_results = cross_validate(lasso, X, y, cv=3)\n236     >>> sorted(cv_results.keys())\n237     ['fit_time', 'score_time', 'test_score']\n238     >>> cv_results['test_score']\n239     array([0.3315057 , 0.08022103, 0.03531816])\n240 \n241     Multiple metric evaluation using ``cross_validate``\n242     (please refer the ``scoring`` parameter doc for more information)\n243 \n244     >>> scores = cross_validate(lasso, X, y, cv=3,\n245     ...                         scoring=('r2', 'neg_mean_squared_error'),\n246     ...                         return_train_score=True)\n247     >>> print(scores['test_neg_mean_squared_error'])\n248     [-3635.5... -3573.3... -6114.7...]\n249     >>> print(scores['train_r2'])\n250     [0.28009951 0.3908844  0.22784907]\n251     \"\"\"\n252     X, y, groups = indexable(X, y, groups)\n253 \n254     cv = check_cv(cv, y, classifier=is_classifier(estimator))\n255 \n256     if callable(scoring):\n257         scorers = scoring\n258     elif scoring is None or isinstance(scoring, str):\n259         scorers = check_scoring(estimator, scoring)\n260     else:\n261         scorers = _check_multimetric_scoring(estimator, scoring)\n262 \n263     # We clone the estimator to make sure that all the folds are\n264     # independent, and that it is pickle-able.\n265     parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n266     results = parallel(\n267         delayed(_fit_and_score)(\n268             clone(estimator),\n269             X,\n270             y,\n271             scorers,\n272             train,\n273             test,\n274             verbose,\n275             None,\n276             fit_params,\n277             return_train_score=return_train_score,\n278             return_times=True,\n279             return_estimator=return_estimator,\n280             error_score=error_score,\n281         )\n282         for train, test in cv.split(X, y, groups)\n283     )\n284 \n285     _warn_or_raise_about_fit_failures(results, error_score)\n286 \n287     # For callabe scoring, the return type is only know after calling. If the\n288     # return type is a dictionary, the error scores can now be inserted with\n289     # the correct key.\n290     if callable(scoring):\n291         _insert_error_scores(results, error_score)\n292 \n293     results = _aggregate_score_dicts(results)\n294 \n295     ret = {}\n296     ret[\"fit_time\"] = results[\"fit_time\"]\n297     ret[\"score_time\"] = results[\"score_time\"]\n298 \n299     if return_estimator:\n300         ret[\"estimator\"] = results[\"estimator\"]\n301 \n302     test_scores_dict = _normalize_score_results(results[\"test_scores\"])\n303     if return_train_score:\n304         train_scores_dict = _normalize_score_results(results[\"train_scores\"])\n305 \n306     for name in test_scores_dict:\n307         ret[\"test_%s\" % name] = test_scores_dict[name]\n308         if return_train_score:\n309             key = \"train_%s\" % name\n310             ret[key] = train_scores_dict[name]\n311 \n312     return ret\n313 \n314 \n315 def _insert_error_scores(results, error_score):\n316     \"\"\"Insert error in `results` by replacing them inplace with `error_score`.\n317 \n318     This only applies to multimetric scores because `_fit_and_score` will\n319     handle the single metric case.\n320     \"\"\"\n321     successful_score = None\n322     failed_indices = []\n323     for i, result in enumerate(results):\n324         if result[\"fit_error\"] is not None:\n325             failed_indices.append(i)\n326         elif successful_score is None:\n327             successful_score = result[\"test_scores\"]\n328 \n329     if isinstance(successful_score, dict):\n330         formatted_error = {name: error_score for name in successful_score}\n331         for i in failed_indices:\n332             results[i][\"test_scores\"] = formatted_error.copy()\n333             if \"train_scores\" in results[i]:\n334                 results[i][\"train_scores\"] = formatted_error.copy()\n335 \n336 \n337 def _normalize_score_results(scores, scaler_score_key=\"score\"):\n338     \"\"\"Creates a scoring dictionary based on the type of `scores`\"\"\"\n339     if isinstance(scores[0], dict):\n340         # multimetric scoring\n341         return _aggregate_score_dicts(scores)\n342     # scaler\n343     return {scaler_score_key: scores}\n344 \n345 \n346 def _warn_or_raise_about_fit_failures(results, error_score):\n347     fit_errors = [\n348         result[\"fit_error\"] for result in results if result[\"fit_error\"] is not None\n349     ]\n350     if fit_errors:\n351         num_failed_fits = len(fit_errors)\n352         num_fits = len(results)\n353         fit_errors_counter = Counter(fit_errors)\n354         delimiter = \"-\" * 80 + \"\\n\"\n355         fit_errors_summary = \"\\n\".join(\n356             f\"{delimiter}{n} fits failed with the following error:\\n{error}\"\n357             for error, n in fit_errors_counter.items()\n358         )\n359 \n360         if num_failed_fits == num_fits:\n361             all_fits_failed_message = (\n362                 f\"\\nAll the {num_fits} fits failed.\\n\"\n363                 \"It is very likely that your model is misconfigured.\\n\"\n364                 \"You can try to debug the error by setting error_score='raise'.\\n\\n\"\n365                 f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n366             )\n367             raise ValueError(all_fits_failed_message)\n368 \n369         else:\n370             some_fits_failed_message = (\n371                 f\"\\n{num_failed_fits} fits failed out of a total of {num_fits}.\\n\"\n372                 \"The score on these train-test partitions for these parameters\"\n373                 f\" will be set to {error_score}.\\n\"\n374                 \"If these failures are not expected, you can try to debug them \"\n375                 \"by setting error_score='raise'.\\n\\n\"\n376                 f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n377             )\n378             warnings.warn(some_fits_failed_message, FitFailedWarning)\n379 \n380 \n381 def cross_val_score(\n382     estimator,\n383     X,\n384     y=None,\n385     *,\n386     groups=None,\n387     scoring=None,\n388     cv=None,\n389     n_jobs=None,\n390     verbose=0,\n391     fit_params=None,\n392     pre_dispatch=\"2*n_jobs\",\n393     error_score=np.nan,\n394 ):\n395     \"\"\"Evaluate a score by cross-validation.\n396 \n397     Read more in the :ref:`User Guide <cross_validation>`.\n398 \n399     Parameters\n400     ----------\n401     estimator : estimator object implementing 'fit'\n402         The object to use to fit the data.\n403 \n404     X : array-like of shape (n_samples, n_features)\n405         The data to fit. Can be for example a list, or an array.\n406 \n407     y : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n408             default=None\n409         The target variable to try to predict in the case of\n410         supervised learning.\n411 \n412     groups : array-like of shape (n_samples,), default=None\n413         Group labels for the samples used while splitting the dataset into\n414         train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n415         instance (e.g., :class:`GroupKFold`).\n416 \n417     scoring : str or callable, default=None\n418         A str (see model evaluation documentation) or\n419         a scorer callable object / function with signature\n420         ``scorer(estimator, X, y)`` which should return only\n421         a single value.\n422 \n423         Similar to :func:`cross_validate`\n424         but only a single metric is permitted.\n425 \n426         If `None`, the estimator's default scorer (if available) is used.\n427 \n428     cv : int, cross-validation generator or an iterable, default=None\n429         Determines the cross-validation splitting strategy.\n430         Possible inputs for cv are:\n431 \n432         - `None`, to use the default 5-fold cross validation,\n433         - int, to specify the number of folds in a `(Stratified)KFold`,\n434         - :term:`CV splitter`,\n435         - An iterable that generates (train, test) splits as arrays of indices.\n436 \n437         For `int`/`None` inputs, if the estimator is a classifier and `y` is\n438         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n439         other cases, :class:`KFold` is used. These splitters are instantiated\n440         with `shuffle=False` so the splits will be the same across calls.\n441 \n442         Refer :ref:`User Guide <cross_validation>` for the various\n443         cross-validation strategies that can be used here.\n444 \n445         .. versionchanged:: 0.22\n446             `cv` default value if `None` changed from 3-fold to 5-fold.\n447 \n448     n_jobs : int, default=None\n449         Number of jobs to run in parallel. Training the estimator and computing\n450         the score are parallelized over the cross-validation splits.\n451         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n452         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n453         for more details.\n454 \n455     verbose : int, default=0\n456         The verbosity level.\n457 \n458     fit_params : dict, default=None\n459         Parameters to pass to the fit method of the estimator.\n460 \n461     pre_dispatch : int or str, default='2*n_jobs'\n462         Controls the number of jobs that get dispatched during parallel\n463         execution. Reducing this number can be useful to avoid an\n464         explosion of memory consumption when more jobs get dispatched\n465         than CPUs can process. This parameter can be:\n466 \n467             - ``None``, in which case all the jobs are immediately\n468               created and spawned. Use this for lightweight and\n469               fast-running jobs, to avoid delays due to on-demand\n470               spawning of the jobs\n471 \n472             - An int, giving the exact number of total jobs that are\n473               spawned\n474 \n475             - A str, giving an expression as a function of n_jobs,\n476               as in '2*n_jobs'\n477 \n478     error_score : 'raise' or numeric, default=np.nan\n479         Value to assign to the score if an error occurs in estimator fitting.\n480         If set to 'raise', the error is raised.\n481         If a numeric value is given, FitFailedWarning is raised.\n482 \n483         .. versionadded:: 0.20\n484 \n485     Returns\n486     -------\n487     scores : ndarray of float of shape=(len(list(cv)),)\n488         Array of scores of the estimator for each run of the cross validation.\n489 \n490     See Also\n491     --------\n492     cross_validate : To run cross-validation on multiple metrics and also to\n493         return train scores, fit times and score times.\n494 \n495     cross_val_predict : Get predictions from each split of cross-validation for\n496         diagnostic purposes.\n497 \n498     sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n499         loss function.\n500 \n501     Examples\n502     --------\n503     >>> from sklearn import datasets, linear_model\n504     >>> from sklearn.model_selection import cross_val_score\n505     >>> diabetes = datasets.load_diabetes()\n506     >>> X = diabetes.data[:150]\n507     >>> y = diabetes.target[:150]\n508     >>> lasso = linear_model.Lasso()\n509     >>> print(cross_val_score(lasso, X, y, cv=3))\n510     [0.3315057  0.08022103 0.03531816]\n511     \"\"\"\n512     # To ensure multimetric format is not supported\n513     scorer = check_scoring(estimator, scoring=scoring)\n514 \n515     cv_results = cross_validate(\n516         estimator=estimator,\n517         X=X,\n518         y=y,\n519         groups=groups,\n520         scoring={\"score\": scorer},\n521         cv=cv,\n522         n_jobs=n_jobs,\n523         verbose=verbose,\n524         fit_params=fit_params,\n525         pre_dispatch=pre_dispatch,\n526         error_score=error_score,\n527     )\n528     return cv_results[\"test_score\"]\n529 \n530 \n531 def _fit_and_score(\n532     estimator,\n533     X,\n534     y,\n535     scorer,\n536     train,\n537     test,\n538     verbose,\n539     parameters,\n540     fit_params,\n541     return_train_score=False,\n542     return_parameters=False,\n543     return_n_test_samples=False,\n544     return_times=False,\n545     return_estimator=False,\n546     split_progress=None,\n547     candidate_progress=None,\n548     error_score=np.nan,\n549 ):\n550 \n551     \"\"\"Fit estimator and compute scores for a given dataset split.\n552 \n553     Parameters\n554     ----------\n555     estimator : estimator object implementing 'fit'\n556         The object to use to fit the data.\n557 \n558     X : array-like of shape (n_samples, n_features)\n559         The data to fit.\n560 \n561     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n562         The target variable to try to predict in the case of\n563         supervised learning.\n564 \n565     scorer : A single callable or dict mapping scorer name to the callable\n566         If it is a single callable, the return value for ``train_scores`` and\n567         ``test_scores`` is a single float.\n568 \n569         For a dict, it should be one mapping the scorer name to the scorer\n570         callable object / function.\n571 \n572         The callable object / fn should have signature\n573         ``scorer(estimator, X, y)``.\n574 \n575     train : array-like of shape (n_train_samples,)\n576         Indices of training samples.\n577 \n578     test : array-like of shape (n_test_samples,)\n579         Indices of test samples.\n580 \n581     verbose : int\n582         The verbosity level.\n583 \n584     error_score : 'raise' or numeric, default=np.nan\n585         Value to assign to the score if an error occurs in estimator fitting.\n586         If set to 'raise', the error is raised.\n587         If a numeric value is given, FitFailedWarning is raised.\n588 \n589     parameters : dict or None\n590         Parameters to be set on the estimator.\n591 \n592     fit_params : dict or None\n593         Parameters that will be passed to ``estimator.fit``.\n594 \n595     return_train_score : bool, default=False\n596         Compute and return score on training set.\n597 \n598     return_parameters : bool, default=False\n599         Return parameters that has been used for the estimator.\n600 \n601     split_progress : {list, tuple} of int, default=None\n602         A list or tuple of format (<current_split_id>, <total_num_of_splits>).\n603 \n604     candidate_progress : {list, tuple} of int, default=None\n605         A list or tuple of format\n606         (<current_candidate_id>, <total_number_of_candidates>).\n607 \n608     return_n_test_samples : bool, default=False\n609         Whether to return the ``n_test_samples``.\n610 \n611     return_times : bool, default=False\n612         Whether to return the fit/score times.\n613 \n614     return_estimator : bool, default=False\n615         Whether to return the fitted estimator.\n616 \n617     Returns\n618     -------\n619     result : dict with the following attributes\n620         train_scores : dict of scorer name -> float\n621             Score on training set (for all the scorers),\n622             returned only if `return_train_score` is `True`.\n623         test_scores : dict of scorer name -> float\n624             Score on testing set (for all the scorers).\n625         n_test_samples : int\n626             Number of test samples.\n627         fit_time : float\n628             Time spent for fitting in seconds.\n629         score_time : float\n630             Time spent for scoring in seconds.\n631         parameters : dict or None\n632             The parameters that have been evaluated.\n633         estimator : estimator object\n634             The fitted estimator.\n635         fit_error : str or None\n636             Traceback str if the fit failed, None if the fit succeeded.\n637     \"\"\"\n638     if not isinstance(error_score, numbers.Number) and error_score != \"raise\":\n639         raise ValueError(\n640             \"error_score must be the string 'raise' or a numeric value. \"\n641             \"(Hint: if using 'raise', please make sure that it has been \"\n642             \"spelled correctly.)\"\n643         )\n644 \n645     progress_msg = \"\"\n646     if verbose > 2:\n647         if split_progress is not None:\n648             progress_msg = f\" {split_progress[0]+1}/{split_progress[1]}\"\n649         if candidate_progress and verbose > 9:\n650             progress_msg += f\"; {candidate_progress[0]+1}/{candidate_progress[1]}\"\n651 \n652     if verbose > 1:\n653         if parameters is None:\n654             params_msg = \"\"\n655         else:\n656             sorted_keys = sorted(parameters)  # Ensure deterministic o/p\n657             params_msg = \", \".join(f\"{k}={parameters[k]}\" for k in sorted_keys)\n658     if verbose > 9:\n659         start_msg = f\"[CV{progress_msg}] START {params_msg}\"\n660         print(f\"{start_msg}{(80 - len(start_msg)) * '.'}\")\n661 \n662     # Adjust length of sample weights\n663     fit_params = fit_params if fit_params is not None else {}\n664     fit_params = _check_fit_params(X, fit_params, train)\n665 \n666     if parameters is not None:\n667         # clone after setting parameters in case any parameters\n668         # are estimators (like pipeline steps)\n669         # because pipeline doesn't clone steps in fit\n670         cloned_parameters = {}\n671         for k, v in parameters.items():\n672             cloned_parameters[k] = clone(v, safe=False)\n673 \n674         estimator = estimator.set_params(**cloned_parameters)\n675 \n676     start_time = time.time()\n677 \n678     X_train, y_train = _safe_split(estimator, X, y, train)\n679     X_test, y_test = _safe_split(estimator, X, y, test, train)\n680 \n681     result = {}\n682     try:\n683         if y_train is None:\n684             estimator.fit(X_train, **fit_params)\n685         else:\n686             estimator.fit(X_train, y_train, **fit_params)\n687 \n688     except Exception:\n689         # Note fit time as time until error\n690         fit_time = time.time() - start_time\n691         score_time = 0.0\n692         if error_score == \"raise\":\n693             raise\n694         elif isinstance(error_score, numbers.Number):\n695             if isinstance(scorer, dict):\n696                 test_scores = {name: error_score for name in scorer}\n697                 if return_train_score:\n698                     train_scores = test_scores.copy()\n699             else:\n700                 test_scores = error_score\n701                 if return_train_score:\n702                     train_scores = error_score\n703         result[\"fit_error\"] = format_exc()\n704     else:\n705         result[\"fit_error\"] = None\n706 \n707         fit_time = time.time() - start_time\n708         test_scores = _score(estimator, X_test, y_test, scorer, error_score)\n709         score_time = time.time() - start_time - fit_time\n710         if return_train_score:\n711             train_scores = _score(estimator, X_train, y_train, scorer, error_score)\n712 \n713     if verbose > 1:\n714         total_time = score_time + fit_time\n715         end_msg = f\"[CV{progress_msg}] END \"\n716         result_msg = params_msg + (\";\" if params_msg else \"\")\n717         if verbose > 2:\n718             if isinstance(test_scores, dict):\n719                 for scorer_name in sorted(test_scores):\n720                     result_msg += f\" {scorer_name}: (\"\n721                     if return_train_score:\n722                         scorer_scores = train_scores[scorer_name]\n723                         result_msg += f\"train={scorer_scores:.3f}, \"\n724                     result_msg += f\"test={test_scores[scorer_name]:.3f})\"\n725             else:\n726                 result_msg += \", score=\"\n727                 if return_train_score:\n728                     result_msg += f\"(train={train_scores:.3f}, test={test_scores:.3f})\"\n729                 else:\n730                     result_msg += f\"{test_scores:.3f}\"\n731         result_msg += f\" total time={logger.short_format_time(total_time)}\"\n732 \n733         # Right align the result_msg\n734         end_msg += \".\" * (80 - len(end_msg) - len(result_msg))\n735         end_msg += result_msg\n736         print(end_msg)\n737 \n738     result[\"test_scores\"] = test_scores\n739     if return_train_score:\n740         result[\"train_scores\"] = train_scores\n741     if return_n_test_samples:\n742         result[\"n_test_samples\"] = _num_samples(X_test)\n743     if return_times:\n744         result[\"fit_time\"] = fit_time\n745         result[\"score_time\"] = score_time\n746     if return_parameters:\n747         result[\"parameters\"] = parameters\n748     if return_estimator:\n749         result[\"estimator\"] = estimator\n750     return result\n751 \n752 \n753 def _score(estimator, X_test, y_test, scorer, error_score=\"raise\"):\n754     \"\"\"Compute the score(s) of an estimator on a given test set.\n755 \n756     Will return a dict of floats if `scorer` is a dict, otherwise a single\n757     float is returned.\n758     \"\"\"\n759     if isinstance(scorer, dict):\n760         # will cache method calls if needed. scorer() returns a dict\n761         scorer = _MultimetricScorer(scorers=scorer, raise_exc=(error_score == \"raise\"))\n762 \n763     try:\n764         if y_test is None:\n765             scores = scorer(estimator, X_test)\n766         else:\n767             scores = scorer(estimator, X_test, y_test)\n768     except Exception:\n769         if isinstance(scorer, _MultimetricScorer):\n770             # If `_MultimetricScorer` raises exception, the `error_score`\n771             # parameter is equal to \"raise\".\n772             raise\n773         else:\n774             if error_score == \"raise\":\n775                 raise\n776             else:\n777                 scores = error_score\n778                 warnings.warn(\n779                     \"Scoring failed. The score on this train-test partition for \"\n780                     f\"these parameters will be set to {error_score}. Details: \\n\"\n781                     f\"{format_exc()}\",\n782                     UserWarning,\n783                 )\n784 \n785     # Check non-raised error messages in `_MultimetricScorer`\n786     if isinstance(scorer, _MultimetricScorer):\n787         exception_messages = [\n788             (name, str_e) for name, str_e in scores.items() if isinstance(str_e, str)\n789         ]\n790         if exception_messages:\n791             # error_score != \"raise\"\n792             for name, str_e in exception_messages:\n793                 scores[name] = error_score\n794                 warnings.warn(\n795                     \"Scoring failed. The score on this train-test partition for \"\n796                     f\"these parameters will be set to {error_score}. Details: \\n\"\n797                     f\"{str_e}\",\n798                     UserWarning,\n799                 )\n800 \n801     error_msg = \"scoring must return a number, got %s (%s) instead. (scorer=%s)\"\n802     if isinstance(scores, dict):\n803         for name, score in scores.items():\n804             if hasattr(score, \"item\"):\n805                 with suppress(ValueError):\n806                     # e.g. unwrap memmapped scalars\n807                     score = score.item()\n808             if not isinstance(score, numbers.Number):\n809                 raise ValueError(error_msg % (score, type(score), name))\n810             scores[name] = score\n811     else:  # scalar\n812         if hasattr(scores, \"item\"):\n813             with suppress(ValueError):\n814                 # e.g. unwrap memmapped scalars\n815                 scores = scores.item()\n816         if not isinstance(scores, numbers.Number):\n817             raise ValueError(error_msg % (scores, type(scores), scorer))\n818     return scores\n819 \n820 \n821 def cross_val_predict(\n822     estimator,\n823     X,\n824     y=None,\n825     *,\n826     groups=None,\n827     cv=None,\n828     n_jobs=None,\n829     verbose=0,\n830     fit_params=None,\n831     pre_dispatch=\"2*n_jobs\",\n832     method=\"predict\",\n833 ):\n834     \"\"\"Generate cross-validated estimates for each input data point.\n835 \n836     The data is split according to the cv parameter. Each sample belongs\n837     to exactly one test set, and its prediction is computed with an\n838     estimator fitted on the corresponding training set.\n839 \n840     Passing these predictions into an evaluation metric may not be a valid\n841     way to measure generalization performance. Results can differ from\n842     :func:`cross_validate` and :func:`cross_val_score` unless all tests sets\n843     have equal size and the metric decomposes over samples.\n844 \n845     Read more in the :ref:`User Guide <cross_validation>`.\n846 \n847     Parameters\n848     ----------\n849     estimator : estimator object implementing 'fit' and 'predict'\n850         The object to use to fit the data.\n851 \n852     X : array-like of shape (n_samples, n_features)\n853         The data to fit. Can be, for example a list, or an array at least 2d.\n854 \n855     y : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n856             default=None\n857         The target variable to try to predict in the case of\n858         supervised learning.\n859 \n860     groups : array-like of shape (n_samples,), default=None\n861         Group labels for the samples used while splitting the dataset into\n862         train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n863         instance (e.g., :class:`GroupKFold`).\n864 \n865     cv : int, cross-validation generator or an iterable, default=None\n866         Determines the cross-validation splitting strategy.\n867         Possible inputs for cv are:\n868 \n869         - None, to use the default 5-fold cross validation,\n870         - int, to specify the number of folds in a `(Stratified)KFold`,\n871         - :term:`CV splitter`,\n872         - An iterable that generates (train, test) splits as arrays of indices.\n873 \n874         For int/None inputs, if the estimator is a classifier and ``y`` is\n875         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n876         other cases, :class:`KFold` is used. These splitters are instantiated\n877         with `shuffle=False` so the splits will be the same across calls.\n878 \n879         Refer :ref:`User Guide <cross_validation>` for the various\n880         cross-validation strategies that can be used here.\n881 \n882         .. versionchanged:: 0.22\n883             ``cv`` default value if None changed from 3-fold to 5-fold.\n884 \n885     n_jobs : int, default=None\n886         Number of jobs to run in parallel. Training the estimator and\n887         predicting are parallelized over the cross-validation splits.\n888         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n889         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n890         for more details.\n891 \n892     verbose : int, default=0\n893         The verbosity level.\n894 \n895     fit_params : dict, default=None\n896         Parameters to pass to the fit method of the estimator.\n897 \n898     pre_dispatch : int or str, default='2*n_jobs'\n899         Controls the number of jobs that get dispatched during parallel\n900         execution. Reducing this number can be useful to avoid an\n901         explosion of memory consumption when more jobs get dispatched\n902         than CPUs can process. This parameter can be:\n903 \n904             - None, in which case all the jobs are immediately\n905               created and spawned. Use this for lightweight and\n906               fast-running jobs, to avoid delays due to on-demand\n907               spawning of the jobs\n908 \n909             - An int, giving the exact number of total jobs that are\n910               spawned\n911 \n912             - A str, giving an expression as a function of n_jobs,\n913               as in '2*n_jobs'\n914 \n915     method : {'predict', 'predict_proba', 'predict_log_proba', \\\n916               'decision_function'}, default='predict'\n917         The method to be invoked by `estimator`.\n918 \n919     Returns\n920     -------\n921     predictions : ndarray\n922         This is the result of calling `method`. Shape:\n923 \n924             - When `method` is 'predict' and in special case where `method` is\n925               'decision_function' and the target is binary: (n_samples,)\n926             - When `method` is one of {'predict_proba', 'predict_log_proba',\n927               'decision_function'} (unless special case above):\n928               (n_samples, n_classes)\n929             - If `estimator` is :term:`multioutput`, an extra dimension\n930               'n_outputs' is added to the end of each shape above.\n931 \n932     See Also\n933     --------\n934     cross_val_score : Calculate score for each CV split.\n935     cross_validate : Calculate one or more scores and timings for each CV\n936         split.\n937 \n938     Notes\n939     -----\n940     In the case that one or more classes are absent in a training portion, a\n941     default score needs to be assigned to all instances for that class if\n942     ``method`` produces columns per class, as in {'decision_function',\n943     'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n944     0.  In order to ensure finite output, we approximate negative infinity by\n945     the minimum finite float value for the dtype in other cases.\n946 \n947     Examples\n948     --------\n949     >>> from sklearn import datasets, linear_model\n950     >>> from sklearn.model_selection import cross_val_predict\n951     >>> diabetes = datasets.load_diabetes()\n952     >>> X = diabetes.data[:150]\n953     >>> y = diabetes.target[:150]\n954     >>> lasso = linear_model.Lasso()\n955     >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n956     \"\"\"\n957     X, y, groups = indexable(X, y, groups)\n958 \n959     cv = check_cv(cv, y, classifier=is_classifier(estimator))\n960     splits = list(cv.split(X, y, groups))\n961 \n962     test_indices = np.concatenate([test for _, test in splits])\n963     if not _check_is_permutation(test_indices, _num_samples(X)):\n964         raise ValueError(\"cross_val_predict only works for partitions\")\n965 \n966     # If classification methods produce multiple columns of output,\n967     # we need to manually encode classes to ensure consistent column ordering.\n968     encode = (\n969         method in [\"decision_function\", \"predict_proba\", \"predict_log_proba\"]\n970         and y is not None\n971     )\n972     if encode:\n973         y = np.asarray(y)\n974         if y.ndim == 1:\n975             le = LabelEncoder()\n976             y = le.fit_transform(y)\n977         elif y.ndim == 2:\n978             y_enc = np.zeros_like(y, dtype=int)\n979             for i_label in range(y.shape[1]):\n980                 y_enc[:, i_label] = LabelEncoder().fit_transform(y[:, i_label])\n981             y = y_enc\n982 \n983     # We clone the estimator to make sure that all the folds are\n984     # independent, and that it is pickle-able.\n985     parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n986     predictions = parallel(\n987         delayed(_fit_and_predict)(\n988             clone(estimator), X, y, train, test, verbose, fit_params, method\n989         )\n990         for train, test in splits\n991     )\n992 \n993     inv_test_indices = np.empty(len(test_indices), dtype=int)\n994     inv_test_indices[test_indices] = np.arange(len(test_indices))\n995 \n996     if sp.issparse(predictions[0]):\n997         predictions = sp.vstack(predictions, format=predictions[0].format)\n998     elif encode and isinstance(predictions[0], list):\n999         # `predictions` is a list of method outputs from each fold.\n1000         # If each of those is also a list, then treat this as a\n1001         # multioutput-multiclass task. We need to separately concatenate\n1002         # the method outputs for each label into an `n_labels` long list.\n1003         n_labels = y.shape[1]\n1004         concat_pred = []\n1005         for i_label in range(n_labels):\n1006             label_preds = np.concatenate([p[i_label] for p in predictions])\n1007             concat_pred.append(label_preds)\n1008         predictions = concat_pred\n1009     else:\n1010         predictions = np.concatenate(predictions)\n1011 \n1012     if isinstance(predictions, list):\n1013         return [p[inv_test_indices] for p in predictions]\n1014     else:\n1015         return predictions[inv_test_indices]\n1016 \n1017 \n1018 def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params, method):\n1019     \"\"\"Fit estimator and predict values for a given dataset split.\n1020 \n1021     Read more in the :ref:`User Guide <cross_validation>`.\n1022 \n1023     Parameters\n1024     ----------\n1025     estimator : estimator object implementing 'fit' and 'predict'\n1026         The object to use to fit the data.\n1027 \n1028     X : array-like of shape (n_samples, n_features)\n1029         The data to fit.\n1030 \n1031         .. versionchanged:: 0.20\n1032             X is only required to be an object with finite length or shape now\n1033 \n1034     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n1035         The target variable to try to predict in the case of\n1036         supervised learning.\n1037 \n1038     train : array-like of shape (n_train_samples,)\n1039         Indices of training samples.\n1040 \n1041     test : array-like of shape (n_test_samples,)\n1042         Indices of test samples.\n1043 \n1044     verbose : int\n1045         The verbosity level.\n1046 \n1047     fit_params : dict or None\n1048         Parameters that will be passed to ``estimator.fit``.\n1049 \n1050     method : str\n1051         Invokes the passed method name of the passed estimator.\n1052 \n1053     Returns\n1054     -------\n1055     predictions : sequence\n1056         Result of calling 'estimator.method'\n1057     \"\"\"\n1058     # Adjust length of sample weights\n1059     fit_params = fit_params if fit_params is not None else {}\n1060     fit_params = _check_fit_params(X, fit_params, train)\n1061 \n1062     X_train, y_train = _safe_split(estimator, X, y, train)\n1063     X_test, _ = _safe_split(estimator, X, y, test, train)\n1064 \n1065     if y_train is None:\n1066         estimator.fit(X_train, **fit_params)\n1067     else:\n1068         estimator.fit(X_train, y_train, **fit_params)\n1069     func = getattr(estimator, method)\n1070     predictions = func(X_test)\n1071 \n1072     encode = (\n1073         method in [\"decision_function\", \"predict_proba\", \"predict_log_proba\"]\n1074         and y is not None\n1075     )\n1076 \n1077     if encode:\n1078         if isinstance(predictions, list):\n1079             predictions = [\n1080                 _enforce_prediction_order(\n1081                     estimator.classes_[i_label],\n1082                     predictions[i_label],\n1083                     n_classes=len(set(y[:, i_label])),\n1084                     method=method,\n1085                 )\n1086                 for i_label in range(len(predictions))\n1087             ]\n1088         else:\n1089             # A 2D y array should be a binary label indicator matrix\n1090             n_classes = len(set(y)) if y.ndim == 1 else y.shape[1]\n1091             predictions = _enforce_prediction_order(\n1092                 estimator.classes_, predictions, n_classes, method\n1093             )\n1094     return predictions\n1095 \n1096 \n1097 def _enforce_prediction_order(classes, predictions, n_classes, method):\n1098     \"\"\"Ensure that prediction arrays have correct column order\n1099 \n1100     When doing cross-validation, if one or more classes are\n1101     not present in the subset of data used for training,\n1102     then the output prediction array might not have the same\n1103     columns as other folds. Use the list of class names\n1104     (assumed to be ints) to enforce the correct column order.\n1105 \n1106     Note that `classes` is the list of classes in this fold\n1107     (a subset of the classes in the full training set)\n1108     and `n_classes` is the number of classes in the full training set.\n1109     \"\"\"\n1110     if n_classes != len(classes):\n1111         recommendation = (\n1112             \"To fix this, use a cross-validation \"\n1113             \"technique resulting in properly \"\n1114             \"stratified folds\"\n1115         )\n1116         warnings.warn(\n1117             \"Number of classes in training fold ({}) does \"\n1118             \"not match total number of classes ({}). \"\n1119             \"Results may not be appropriate for your use case. \"\n1120             \"{}\".format(len(classes), n_classes, recommendation),\n1121             RuntimeWarning,\n1122         )\n1123         if method == \"decision_function\":\n1124             if predictions.ndim == 2 and predictions.shape[1] != len(classes):\n1125                 # This handles the case when the shape of predictions\n1126                 # does not match the number of classes used to train\n1127                 # it with. This case is found when sklearn.svm.SVC is\n1128                 # set to `decision_function_shape='ovo'`.\n1129                 raise ValueError(\n1130                     \"Output shape {} of {} does not match \"\n1131                     \"number of classes ({}) in fold. \"\n1132                     \"Irregular decision_function outputs \"\n1133                     \"are not currently supported by \"\n1134                     \"cross_val_predict\".format(predictions.shape, method, len(classes))\n1135                 )\n1136             if len(classes) <= 2:\n1137                 # In this special case, `predictions` contains a 1D array.\n1138                 raise ValueError(\n1139                     \"Only {} class/es in training fold, but {} \"\n1140                     \"in overall dataset. This \"\n1141                     \"is not supported for decision_function \"\n1142                     \"with imbalanced folds. {}\".format(\n1143                         len(classes), n_classes, recommendation\n1144                     )\n1145                 )\n1146 \n1147         float_min = np.finfo(predictions.dtype).min\n1148         default_values = {\n1149             \"decision_function\": float_min,\n1150             \"predict_log_proba\": float_min,\n1151             \"predict_proba\": 0,\n1152         }\n1153         predictions_for_all_classes = np.full(\n1154             (_num_samples(predictions), n_classes),\n1155             default_values[method],\n1156             dtype=predictions.dtype,\n1157         )\n1158         predictions_for_all_classes[:, classes] = predictions\n1159         predictions = predictions_for_all_classes\n1160     return predictions\n1161 \n1162 \n1163 def _check_is_permutation(indices, n_samples):\n1164     \"\"\"Check whether indices is a reordering of the array np.arange(n_samples)\n1165 \n1166     Parameters\n1167     ----------\n1168     indices : ndarray\n1169         int array to test\n1170     n_samples : int\n1171         number of expected elements\n1172 \n1173     Returns\n1174     -------\n1175     is_partition : bool\n1176         True iff sorted(indices) is np.arange(n)\n1177     \"\"\"\n1178     if len(indices) != n_samples:\n1179         return False\n1180     hit = np.zeros(n_samples, dtype=bool)\n1181     hit[indices] = True\n1182     if not np.all(hit):\n1183         return False\n1184     return True\n1185 \n1186 \n1187 def permutation_test_score(\n1188     estimator,\n1189     X,\n1190     y,\n1191     *,\n1192     groups=None,\n1193     cv=None,\n1194     n_permutations=100,\n1195     n_jobs=None,\n1196     random_state=0,\n1197     verbose=0,\n1198     scoring=None,\n1199     fit_params=None,\n1200 ):\n1201     \"\"\"Evaluate the significance of a cross-validated score with permutations.\n1202 \n1203     Permutes targets to generate 'randomized data' and compute the empirical\n1204     p-value against the null hypothesis that features and targets are\n1205     independent.\n1206 \n1207     The p-value represents the fraction of randomized data sets where the\n1208     estimator performed as well or better than in the original data. A small\n1209     p-value suggests that there is a real dependency between features and\n1210     targets which has been used by the estimator to give good predictions.\n1211     A large p-value may be due to lack of real dependency between features\n1212     and targets or the estimator was not able to use the dependency to\n1213     give good predictions.\n1214 \n1215     Read more in the :ref:`User Guide <permutation_test_score>`.\n1216 \n1217     Parameters\n1218     ----------\n1219     estimator : estimator object implementing 'fit'\n1220         The object to use to fit the data.\n1221 \n1222     X : array-like of shape at least 2D\n1223         The data to fit.\n1224 \n1225     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n1226         The target variable to try to predict in the case of\n1227         supervised learning.\n1228 \n1229     groups : array-like of shape (n_samples,), default=None\n1230         Labels to constrain permutation within groups, i.e. ``y`` values\n1231         are permuted among samples with the same group identifier.\n1232         When not specified, ``y`` values are permuted among all samples.\n1233 \n1234         When a grouped cross-validator is used, the group labels are\n1235         also passed on to the ``split`` method of the cross-validator. The\n1236         cross-validator uses them for grouping the samples  while splitting\n1237         the dataset into train/test set.\n1238 \n1239     cv : int, cross-validation generator or an iterable, default=None\n1240         Determines the cross-validation splitting strategy.\n1241         Possible inputs for cv are:\n1242 \n1243         - `None`, to use the default 5-fold cross validation,\n1244         - int, to specify the number of folds in a `(Stratified)KFold`,\n1245         - :term:`CV splitter`,\n1246         - An iterable yielding (train, test) splits as arrays of indices.\n1247 \n1248         For `int`/`None` inputs, if the estimator is a classifier and `y` is\n1249         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n1250         other cases, :class:`KFold` is used. These splitters are instantiated\n1251         with `shuffle=False` so the splits will be the same across calls.\n1252 \n1253         Refer :ref:`User Guide <cross_validation>` for the various\n1254         cross-validation strategies that can be used here.\n1255 \n1256         .. versionchanged:: 0.22\n1257             `cv` default value if `None` changed from 3-fold to 5-fold.\n1258 \n1259     n_permutations : int, default=100\n1260         Number of times to permute ``y``.\n1261 \n1262     n_jobs : int, default=None\n1263         Number of jobs to run in parallel. Training the estimator and computing\n1264         the cross-validated score are parallelized over the permutations.\n1265         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1266         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1267         for more details.\n1268 \n1269     random_state : int, RandomState instance or None, default=0\n1270         Pass an int for reproducible output for permutation of\n1271         ``y`` values among samples. See :term:`Glossary <random_state>`.\n1272 \n1273     verbose : int, default=0\n1274         The verbosity level.\n1275 \n1276     scoring : str or callable, default=None\n1277         A single str (see :ref:`scoring_parameter`) or a callable\n1278         (see :ref:`scoring`) to evaluate the predictions on the test set.\n1279 \n1280         If `None` the estimator's score method is used.\n1281 \n1282     fit_params : dict, default=None\n1283         Parameters to pass to the fit method of the estimator.\n1284 \n1285         .. versionadded:: 0.24\n1286 \n1287     Returns\n1288     -------\n1289     score : float\n1290         The true score without permuting targets.\n1291 \n1292     permutation_scores : array of shape (n_permutations,)\n1293         The scores obtained for each permutations.\n1294 \n1295     pvalue : float\n1296         The p-value, which approximates the probability that the score would\n1297         be obtained by chance. This is calculated as:\n1298 \n1299         `(C + 1) / (n_permutations + 1)`\n1300 \n1301         Where C is the number of permutations whose score >= the true score.\n1302 \n1303         The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n1304 \n1305     Notes\n1306     -----\n1307     This function implements Test 1 in:\n1308 \n1309         Ojala and Garriga. `Permutation Tests for Studying Classifier\n1310         Performance\n1311         <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\n1312         Journal of Machine Learning Research (2010) vol. 11\n1313     \"\"\"\n1314     X, y, groups = indexable(X, y, groups)\n1315 \n1316     cv = check_cv(cv, y, classifier=is_classifier(estimator))\n1317     scorer = check_scoring(estimator, scoring=scoring)\n1318     random_state = check_random_state(random_state)\n1319 \n1320     # We clone the estimator to make sure that all the folds are\n1321     # independent, and that it is pickle-able.\n1322     score = _permutation_test_score(\n1323         clone(estimator), X, y, groups, cv, scorer, fit_params=fit_params\n1324     )\n1325     permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(\n1326         delayed(_permutation_test_score)(\n1327             clone(estimator),\n1328             X,\n1329             _shuffle(y, groups, random_state),\n1330             groups,\n1331             cv,\n1332             scorer,\n1333             fit_params=fit_params,\n1334         )\n1335         for _ in range(n_permutations)\n1336     )\n1337     permutation_scores = np.array(permutation_scores)\n1338     pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)\n1339     return score, permutation_scores, pvalue\n1340 \n1341 \n1342 def _permutation_test_score(estimator, X, y, groups, cv, scorer, fit_params):\n1343     \"\"\"Auxiliary function for permutation_test_score\"\"\"\n1344     # Adjust length of sample weights\n1345     fit_params = fit_params if fit_params is not None else {}\n1346     avg_score = []\n1347     for train, test in cv.split(X, y, groups):\n1348         X_train, y_train = _safe_split(estimator, X, y, train)\n1349         X_test, y_test = _safe_split(estimator, X, y, test, train)\n1350         fit_params = _check_fit_params(X, fit_params, train)\n1351         estimator.fit(X_train, y_train, **fit_params)\n1352         avg_score.append(scorer(estimator, X_test, y_test))\n1353     return np.mean(avg_score)\n1354 \n1355 \n1356 def _shuffle(y, groups, random_state):\n1357     \"\"\"Return a shuffled copy of y eventually shuffle among same groups.\"\"\"\n1358     if groups is None:\n1359         indices = random_state.permutation(len(y))\n1360     else:\n1361         indices = np.arange(len(groups))\n1362         for group in np.unique(groups):\n1363             this_mask = groups == group\n1364             indices[this_mask] = random_state.permutation(indices[this_mask])\n1365     return _safe_indexing(y, indices)\n1366 \n1367 \n1368 def learning_curve(\n1369     estimator,\n1370     X,\n1371     y,\n1372     *,\n1373     groups=None,\n1374     train_sizes=np.linspace(0.1, 1.0, 5),\n1375     cv=None,\n1376     scoring=None,\n1377     exploit_incremental_learning=False,\n1378     n_jobs=None,\n1379     pre_dispatch=\"all\",\n1380     verbose=0,\n1381     shuffle=False,\n1382     random_state=None,\n1383     error_score=np.nan,\n1384     return_times=False,\n1385     fit_params=None,\n1386 ):\n1387     \"\"\"Learning curve.\n1388 \n1389     Determines cross-validated training and test scores for different training\n1390     set sizes.\n1391 \n1392     A cross-validation generator splits the whole dataset k times in training\n1393     and test data. Subsets of the training set with varying sizes will be used\n1394     to train the estimator and a score for each training subset size and the\n1395     test set will be computed. Afterwards, the scores will be averaged over\n1396     all k runs for each training subset size.\n1397 \n1398     Read more in the :ref:`User Guide <learning_curve>`.\n1399 \n1400     Parameters\n1401     ----------\n1402     estimator : object type that implements the \"fit\" and \"predict\" methods\n1403         An object of that type which is cloned for each validation.\n1404 \n1405     X : array-like of shape (n_samples, n_features)\n1406         Training vector, where `n_samples` is the number of samples and\n1407         `n_features` is the number of features.\n1408 \n1409     y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n1410         Target relative to X for classification or regression;\n1411         None for unsupervised learning.\n1412 \n1413     groups : array-like of  shape (n_samples,), default=None\n1414         Group labels for the samples used while splitting the dataset into\n1415         train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n1416         instance (e.g., :class:`GroupKFold`).\n1417 \n1418     train_sizes : array-like of shape (n_ticks,), \\\n1419             default=np.linspace(0.1, 1.0, 5)\n1420         Relative or absolute numbers of training examples that will be used to\n1421         generate the learning curve. If the dtype is float, it is regarded as a\n1422         fraction of the maximum size of the training set (that is determined\n1423         by the selected validation method), i.e. it has to be within (0, 1].\n1424         Otherwise it is interpreted as absolute sizes of the training sets.\n1425         Note that for classification the number of samples usually have to\n1426         be big enough to contain at least one sample from each class.\n1427 \n1428     cv : int, cross-validation generator or an iterable, default=None\n1429         Determines the cross-validation splitting strategy.\n1430         Possible inputs for cv are:\n1431 \n1432         - None, to use the default 5-fold cross validation,\n1433         - int, to specify the number of folds in a `(Stratified)KFold`,\n1434         - :term:`CV splitter`,\n1435         - An iterable yielding (train, test) splits as arrays of indices.\n1436 \n1437         For int/None inputs, if the estimator is a classifier and ``y`` is\n1438         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n1439         other cases, :class:`KFold` is used. These splitters are instantiated\n1440         with `shuffle=False` so the splits will be the same across calls.\n1441 \n1442         Refer :ref:`User Guide <cross_validation>` for the various\n1443         cross-validation strategies that can be used here.\n1444 \n1445         .. versionchanged:: 0.22\n1446             ``cv`` default value if None changed from 3-fold to 5-fold.\n1447 \n1448     scoring : str or callable, default=None\n1449         A str (see model evaluation documentation) or\n1450         a scorer callable object / function with signature\n1451         ``scorer(estimator, X, y)``.\n1452 \n1453     exploit_incremental_learning : bool, default=False\n1454         If the estimator supports incremental learning, this will be\n1455         used to speed up fitting for different training set sizes.\n1456 \n1457     n_jobs : int, default=None\n1458         Number of jobs to run in parallel. Training the estimator and computing\n1459         the score are parallelized over the different training and test sets.\n1460         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1461         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1462         for more details.\n1463 \n1464     pre_dispatch : int or str, default='all'\n1465         Number of predispatched jobs for parallel execution (default is\n1466         all). The option can reduce the allocated memory. The str can\n1467         be an expression like '2*n_jobs'.\n1468 \n1469     verbose : int, default=0\n1470         Controls the verbosity: the higher, the more messages.\n1471 \n1472     shuffle : bool, default=False\n1473         Whether to shuffle training data before taking prefixes of it\n1474         based on``train_sizes``.\n1475 \n1476     random_state : int, RandomState instance or None, default=None\n1477         Used when ``shuffle`` is True. Pass an int for reproducible\n1478         output across multiple function calls.\n1479         See :term:`Glossary <random_state>`.\n1480 \n1481     error_score : 'raise' or numeric, default=np.nan\n1482         Value to assign to the score if an error occurs in estimator fitting.\n1483         If set to 'raise', the error is raised.\n1484         If a numeric value is given, FitFailedWarning is raised.\n1485 \n1486         .. versionadded:: 0.20\n1487 \n1488     return_times : bool, default=False\n1489         Whether to return the fit and score times.\n1490 \n1491     fit_params : dict, default=None\n1492         Parameters to pass to the fit method of the estimator.\n1493 \n1494         .. versionadded:: 0.24\n1495 \n1496     Returns\n1497     -------\n1498     train_sizes_abs : array of shape (n_unique_ticks,)\n1499         Numbers of training examples that has been used to generate the\n1500         learning curve. Note that the number of ticks might be less\n1501         than n_ticks because duplicate entries will be removed.\n1502 \n1503     train_scores : array of shape (n_ticks, n_cv_folds)\n1504         Scores on training sets.\n1505 \n1506     test_scores : array of shape (n_ticks, n_cv_folds)\n1507         Scores on test set.\n1508 \n1509     fit_times : array of shape (n_ticks, n_cv_folds)\n1510         Times spent for fitting in seconds. Only present if ``return_times``\n1511         is True.\n1512 \n1513     score_times : array of shape (n_ticks, n_cv_folds)\n1514         Times spent for scoring in seconds. Only present if ``return_times``\n1515         is True.\n1516 \n1517     Examples\n1518     --------\n1519     >>> from sklearn.datasets import make_classification\n1520     >>> from sklearn.tree import DecisionTreeClassifier\n1521     >>> from sklearn.model_selection import learning_curve\n1522     >>> X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n1523     >>> tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n1524     >>> train_size_abs, train_scores, test_scores = learning_curve(\n1525     ...     tree, X, y, train_sizes=[0.3, 0.6, 0.9]\n1526     ... )\n1527     >>> for train_size, cv_train_scores, cv_test_scores in zip(\n1528     ...     train_size_abs, train_scores, test_scores\n1529     ... ):\n1530     ...     print(f\"{train_size} samples were used to train the model\")\n1531     ...     print(f\"The average train accuracy is {cv_train_scores.mean():.2f}\")\n1532     ...     print(f\"The average test accuracy is {cv_test_scores.mean():.2f}\")\n1533     24 samples were used to train the model\n1534     The average train accuracy is 1.00\n1535     The average test accuracy is 0.85\n1536     48 samples were used to train the model\n1537     The average train accuracy is 1.00\n1538     The average test accuracy is 0.90\n1539     72 samples were used to train the model\n1540     The average train accuracy is 1.00\n1541     The average test accuracy is 0.93\n1542     \"\"\"\n1543     if exploit_incremental_learning and not hasattr(estimator, \"partial_fit\"):\n1544         raise ValueError(\n1545             \"An estimator must support the partial_fit interface \"\n1546             \"to exploit incremental learning\"\n1547         )\n1548     X, y, groups = indexable(X, y, groups)\n1549 \n1550     cv = check_cv(cv, y, classifier=is_classifier(estimator))\n1551     # Store it as list as we will be iterating over the list multiple times\n1552     cv_iter = list(cv.split(X, y, groups))\n1553 \n1554     scorer = check_scoring(estimator, scoring=scoring)\n1555 \n1556     n_max_training_samples = len(cv_iter[0][0])\n1557     # Because the lengths of folds can be significantly different, it is\n1558     # not guaranteed that we use all of the available training data when we\n1559     # use the first 'n_max_training_samples' samples.\n1560     train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)\n1561     n_unique_ticks = train_sizes_abs.shape[0]\n1562     if verbose > 0:\n1563         print(\"[learning_curve] Training set sizes: \" + str(train_sizes_abs))\n1564 \n1565     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)\n1566 \n1567     if shuffle:\n1568         rng = check_random_state(random_state)\n1569         cv_iter = ((rng.permutation(train), test) for train, test in cv_iter)\n1570 \n1571     if exploit_incremental_learning:\n1572         classes = np.unique(y) if is_classifier(estimator) else None\n1573         out = parallel(\n1574             delayed(_incremental_fit_estimator)(\n1575                 clone(estimator),\n1576                 X,\n1577                 y,\n1578                 classes,\n1579                 train,\n1580                 test,\n1581                 train_sizes_abs,\n1582                 scorer,\n1583                 verbose,\n1584                 return_times,\n1585                 error_score=error_score,\n1586                 fit_params=fit_params,\n1587             )\n1588             for train, test in cv_iter\n1589         )\n1590         out = np.asarray(out).transpose((2, 1, 0))\n1591     else:\n1592         train_test_proportions = []\n1593         for train, test in cv_iter:\n1594             for n_train_samples in train_sizes_abs:\n1595                 train_test_proportions.append((train[:n_train_samples], test))\n1596 \n1597         results = parallel(\n1598             delayed(_fit_and_score)(\n1599                 clone(estimator),\n1600                 X,\n1601                 y,\n1602                 scorer,\n1603                 train,\n1604                 test,\n1605                 verbose,\n1606                 parameters=None,\n1607                 fit_params=fit_params,\n1608                 return_train_score=True,\n1609                 error_score=error_score,\n1610                 return_times=return_times,\n1611             )\n1612             for train, test in train_test_proportions\n1613         )\n1614         results = _aggregate_score_dicts(results)\n1615         train_scores = results[\"train_scores\"].reshape(-1, n_unique_ticks).T\n1616         test_scores = results[\"test_scores\"].reshape(-1, n_unique_ticks).T\n1617         out = [train_scores, test_scores]\n1618 \n1619         if return_times:\n1620             fit_times = results[\"fit_time\"].reshape(-1, n_unique_ticks).T\n1621             score_times = results[\"score_time\"].reshape(-1, n_unique_ticks).T\n1622             out.extend([fit_times, score_times])\n1623 \n1624     ret = train_sizes_abs, out[0], out[1]\n1625 \n1626     if return_times:\n1627         ret = ret + (out[2], out[3])\n1628 \n1629     return ret\n1630 \n1631 \n1632 def _translate_train_sizes(train_sizes, n_max_training_samples):\n1633     \"\"\"Determine absolute sizes of training subsets and validate 'train_sizes'.\n1634 \n1635     Examples:\n1636         _translate_train_sizes([0.5, 1.0], 10) -> [5, 10]\n1637         _translate_train_sizes([5, 10], 10) -> [5, 10]\n1638 \n1639     Parameters\n1640     ----------\n1641     train_sizes : array-like of shape (n_ticks,)\n1642         Numbers of training examples that will be used to generate the\n1643         learning curve. If the dtype is float, it is regarded as a\n1644         fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].\n1645 \n1646     n_max_training_samples : int\n1647         Maximum number of training samples (upper bound of 'train_sizes').\n1648 \n1649     Returns\n1650     -------\n1651     train_sizes_abs : array of shape (n_unique_ticks,)\n1652         Numbers of training examples that will be used to generate the\n1653         learning curve. Note that the number of ticks might be less\n1654         than n_ticks because duplicate entries will be removed.\n1655     \"\"\"\n1656     train_sizes_abs = np.asarray(train_sizes)\n1657     n_ticks = train_sizes_abs.shape[0]\n1658     n_min_required_samples = np.min(train_sizes_abs)\n1659     n_max_required_samples = np.max(train_sizes_abs)\n1660     if np.issubdtype(train_sizes_abs.dtype, np.floating):\n1661         if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:\n1662             raise ValueError(\n1663                 \"train_sizes has been interpreted as fractions \"\n1664                 \"of the maximum number of training samples and \"\n1665                 \"must be within (0, 1], but is within [%f, %f].\"\n1666                 % (n_min_required_samples, n_max_required_samples)\n1667             )\n1668         train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(\n1669             dtype=int, copy=False\n1670         )\n1671         train_sizes_abs = np.clip(train_sizes_abs, 1, n_max_training_samples)\n1672     else:\n1673         if (\n1674             n_min_required_samples <= 0\n1675             or n_max_required_samples > n_max_training_samples\n1676         ):\n1677             raise ValueError(\n1678                 \"train_sizes has been interpreted as absolute \"\n1679                 \"numbers of training samples and must be within \"\n1680                 \"(0, %d], but is within [%d, %d].\"\n1681                 % (\n1682                     n_max_training_samples,\n1683                     n_min_required_samples,\n1684                     n_max_required_samples,\n1685                 )\n1686             )\n1687 \n1688     train_sizes_abs = np.unique(train_sizes_abs)\n1689     if n_ticks > train_sizes_abs.shape[0]:\n1690         warnings.warn(\n1691             \"Removed duplicate entries from 'train_sizes'. Number \"\n1692             \"of ticks will be less than the size of \"\n1693             \"'train_sizes': %d instead of %d.\" % (train_sizes_abs.shape[0], n_ticks),\n1694             RuntimeWarning,\n1695         )\n1696 \n1697     return train_sizes_abs\n1698 \n1699 \n1700 def _incremental_fit_estimator(\n1701     estimator,\n1702     X,\n1703     y,\n1704     classes,\n1705     train,\n1706     test,\n1707     train_sizes,\n1708     scorer,\n1709     verbose,\n1710     return_times,\n1711     error_score,\n1712     fit_params,\n1713 ):\n1714     \"\"\"Train estimator on training subsets incrementally and compute scores.\"\"\"\n1715     train_scores, test_scores, fit_times, score_times = [], [], [], []\n1716     partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])\n1717     if fit_params is None:\n1718         fit_params = {}\n1719     if classes is None:\n1720         partial_fit_func = partial(estimator.partial_fit, **fit_params)\n1721     else:\n1722         partial_fit_func = partial(estimator.partial_fit, classes=classes, **fit_params)\n1723 \n1724     for n_train_samples, partial_train in partitions:\n1725         train_subset = train[:n_train_samples]\n1726         X_train, y_train = _safe_split(estimator, X, y, train_subset)\n1727         X_partial_train, y_partial_train = _safe_split(estimator, X, y, partial_train)\n1728         X_test, y_test = _safe_split(estimator, X, y, test, train_subset)\n1729         start_fit = time.time()\n1730         if y_partial_train is None:\n1731             partial_fit_func(X_partial_train)\n1732         else:\n1733             partial_fit_func(X_partial_train, y_partial_train)\n1734         fit_time = time.time() - start_fit\n1735         fit_times.append(fit_time)\n1736 \n1737         start_score = time.time()\n1738 \n1739         test_scores.append(_score(estimator, X_test, y_test, scorer, error_score))\n1740         train_scores.append(_score(estimator, X_train, y_train, scorer, error_score))\n1741 \n1742         score_time = time.time() - start_score\n1743         score_times.append(score_time)\n1744 \n1745     ret = (\n1746         (train_scores, test_scores, fit_times, score_times)\n1747         if return_times\n1748         else (train_scores, test_scores)\n1749     )\n1750 \n1751     return np.array(ret).T\n1752 \n1753 \n1754 def validation_curve(\n1755     estimator,\n1756     X,\n1757     y,\n1758     *,\n1759     param_name,\n1760     param_range,\n1761     groups=None,\n1762     cv=None,\n1763     scoring=None,\n1764     n_jobs=None,\n1765     pre_dispatch=\"all\",\n1766     verbose=0,\n1767     error_score=np.nan,\n1768     fit_params=None,\n1769 ):\n1770     \"\"\"Validation curve.\n1771 \n1772     Determine training and test scores for varying parameter values.\n1773 \n1774     Compute scores for an estimator with different values of a specified\n1775     parameter. This is similar to grid search with one parameter. However, this\n1776     will also compute training scores and is merely a utility for plotting the\n1777     results.\n1778 \n1779     Read more in the :ref:`User Guide <validation_curve>`.\n1780 \n1781     Parameters\n1782     ----------\n1783     estimator : object type that implements the \"fit\" and \"predict\" methods\n1784         An object of that type which is cloned for each validation.\n1785 \n1786     X : array-like of shape (n_samples, n_features)\n1787         Training vector, where `n_samples` is the number of samples and\n1788         `n_features` is the number of features.\n1789 \n1790     y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n1791         Target relative to X for classification or regression;\n1792         None for unsupervised learning.\n1793 \n1794     param_name : str\n1795         Name of the parameter that will be varied.\n1796 \n1797     param_range : array-like of shape (n_values,)\n1798         The values of the parameter that will be evaluated.\n1799 \n1800     groups : array-like of shape (n_samples,), default=None\n1801         Group labels for the samples used while splitting the dataset into\n1802         train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n1803         instance (e.g., :class:`GroupKFold`).\n1804 \n1805     cv : int, cross-validation generator or an iterable, default=None\n1806         Determines the cross-validation splitting strategy.\n1807         Possible inputs for cv are:\n1808 \n1809         - None, to use the default 5-fold cross validation,\n1810         - int, to specify the number of folds in a `(Stratified)KFold`,\n1811         - :term:`CV splitter`,\n1812         - An iterable yielding (train, test) splits as arrays of indices.\n1813 \n1814         For int/None inputs, if the estimator is a classifier and ``y`` is\n1815         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n1816         other cases, :class:`KFold` is used. These splitters are instantiated\n1817         with `shuffle=False` so the splits will be the same across calls.\n1818 \n1819         Refer :ref:`User Guide <cross_validation>` for the various\n1820         cross-validation strategies that can be used here.\n1821 \n1822         .. versionchanged:: 0.22\n1823             ``cv`` default value if None changed from 3-fold to 5-fold.\n1824 \n1825     scoring : str or callable, default=None\n1826         A str (see model evaluation documentation) or\n1827         a scorer callable object / function with signature\n1828         ``scorer(estimator, X, y)``.\n1829 \n1830     n_jobs : int, default=None\n1831         Number of jobs to run in parallel. Training the estimator and computing\n1832         the score are parallelized over the combinations of each parameter\n1833         value and each cross-validation split.\n1834         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n1835         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n1836         for more details.\n1837 \n1838     pre_dispatch : int or str, default='all'\n1839         Number of predispatched jobs for parallel execution (default is\n1840         all). The option can reduce the allocated memory. The str can\n1841         be an expression like '2*n_jobs'.\n1842 \n1843     verbose : int, default=0\n1844         Controls the verbosity: the higher, the more messages.\n1845 \n1846     error_score : 'raise' or numeric, default=np.nan\n1847         Value to assign to the score if an error occurs in estimator fitting.\n1848         If set to 'raise', the error is raised.\n1849         If a numeric value is given, FitFailedWarning is raised.\n1850 \n1851         .. versionadded:: 0.20\n1852 \n1853     fit_params : dict, default=None\n1854         Parameters to pass to the fit method of the estimator.\n1855 \n1856         .. versionadded:: 0.24\n1857 \n1858     Returns\n1859     -------\n1860     train_scores : array of shape (n_ticks, n_cv_folds)\n1861         Scores on training sets.\n1862 \n1863     test_scores : array of shape (n_ticks, n_cv_folds)\n1864         Scores on test set.\n1865 \n1866     Notes\n1867     -----\n1868     See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`\n1869     \"\"\"\n1870     X, y, groups = indexable(X, y, groups)\n1871 \n1872     cv = check_cv(cv, y, classifier=is_classifier(estimator))\n1873     scorer = check_scoring(estimator, scoring=scoring)\n1874 \n1875     parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)\n1876     results = parallel(\n1877         delayed(_fit_and_score)(\n1878             clone(estimator),\n1879             X,\n1880             y,\n1881             scorer,\n1882             train,\n1883             test,\n1884             verbose,\n1885             parameters={param_name: v},\n1886             fit_params=fit_params,\n1887             return_train_score=True,\n1888             error_score=error_score,\n1889         )\n1890         # NOTE do not change order of iteration to allow one time cv splitters\n1891         for train, test in cv.split(X, y, groups)\n1892         for v in param_range\n1893     )\n1894     n_params = len(param_range)\n1895 \n1896     results = _aggregate_score_dicts(results)\n1897     train_scores = results[\"train_scores\"].reshape(-1, n_params).T\n1898     test_scores = results[\"test_scores\"].reshape(-1, n_params).T\n1899 \n1900     return train_scores, test_scores\n1901 \n1902 \n1903 def _aggregate_score_dicts(scores):\n1904     \"\"\"Aggregate the list of dict to dict of np ndarray\n1905 \n1906     The aggregated output of _aggregate_score_dicts will be a list of dict\n1907     of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]\n1908     Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}\n1909 \n1910     Parameters\n1911     ----------\n1912 \n1913     scores : list of dict\n1914         List of dicts of the scores for all scorers. This is a flat list,\n1915         assumed originally to be of row major order.\n1916 \n1917     Example\n1918     -------\n1919 \n1920     >>> scores = [{'a': 1, 'b':10}, {'a': 2, 'b':2}, {'a': 3, 'b':3},\n1921     ...           {'a': 10, 'b': 10}]                         # doctest: +SKIP\n1922     >>> _aggregate_score_dicts(scores)                        # doctest: +SKIP\n1923     {'a': array([1, 2, 3, 10]),\n1924      'b': array([10, 2, 3, 10])}\n1925     \"\"\"\n1926     return {\n1927         key: np.asarray([score[key] for score in scores])\n1928         if isinstance(scores[0][key], numbers.Number)\n1929         else [score[key] for score in scores]\n1930         for key in scores[0]\n1931     }\n",
        "intended_behavior": "The intended behavior here is to ensure that configurations, especially those crucial for the operation of Parallel executed tasks, are explicitly passed to each delayed call. This might require wrapping the delayed function calls within a context that ensures the current configuration state is captured and then applied within each parallelly executed task. In essence, the fix involves ensuring that any configuration context is not just assumed but is explicitly conveyed and reconstructed within each task's execution environment."
    },
    {
        "rel_file_path": "sklearn/ensemble/_forest.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__1.3/sklearn/ensemble/_forest.py",
        "start": 1,
        "end": 2849,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 Forest of trees-based ensemble methods.\n3 \n4 Those methods include random forests and extremely randomized trees.\n5 \n6 The module structure is the following:\n7 \n8 - The ``BaseForest`` base class implements a common ``fit`` method for all\n9   the estimators in the module. The ``fit`` method of the base ``Forest``\n10   class calls the ``fit`` method of each sub-estimator on random samples\n11   (with replacement, a.k.a. bootstrap) of the training set.\n12 \n13   The init of the sub-estimator is further delegated to the\n14   ``BaseEnsemble`` constructor.\n15 \n16 - The ``ForestClassifier`` and ``ForestRegressor`` base classes further\n17   implement the prediction logic by computing an average of the predicted\n18   outcomes of the sub-estimators.\n19 \n20 - The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived\n21   classes provide the user with concrete implementations of\n22   the forest ensemble method using classical, deterministic\n23   ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as\n24   sub-estimator implementations.\n25 \n26 - The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived\n27   classes provide the user with concrete implementations of the\n28   forest ensemble method using the extremely randomized trees\n29   ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as\n30   sub-estimator implementations.\n31 \n32 Single and multi-output problems are both handled.\n33 \"\"\"\n34 \n35 # Authors: Gilles Louppe <g.louppe@gmail.com>\n36 #          Brian Holt <bdholt1@gmail.com>\n37 #          Joly Arnaud <arnaud.v.joly@gmail.com>\n38 #          Fares Hedayati <fares.hedayati@gmail.com>\n39 #\n40 # License: BSD 3 clause\n41 \n42 \n43 from numbers import Integral, Real\n44 from warnings import catch_warnings, simplefilter, warn\n45 import threading\n46 \n47 from abc import ABCMeta, abstractmethod\n48 import numpy as np\n49 from scipy.sparse import issparse\n50 from scipy.sparse import hstack as sparse_hstack\n51 from joblib import Parallel\n52 \n53 from ..base import is_classifier\n54 from ..base import ClassifierMixin, MultiOutputMixin, RegressorMixin, TransformerMixin\n55 \n56 from ..metrics import accuracy_score, r2_score\n57 from ..preprocessing import OneHotEncoder\n58 from ..tree import (\n59     BaseDecisionTree,\n60     DecisionTreeClassifier,\n61     DecisionTreeRegressor,\n62     ExtraTreeClassifier,\n63     ExtraTreeRegressor,\n64 )\n65 from ..tree._tree import DTYPE, DOUBLE\n66 from ..utils import check_random_state, compute_sample_weight\n67 from ..exceptions import DataConversionWarning\n68 from ._base import BaseEnsemble, _partition_estimators\n69 from ..utils.fixes import delayed\n70 from ..utils.multiclass import check_classification_targets, type_of_target\n71 from ..utils.validation import (\n72     check_is_fitted,\n73     _check_sample_weight,\n74     _check_feature_names_in,\n75 )\n76 from ..utils.validation import _num_samples\n77 from ..utils._param_validation import Interval, StrOptions\n78 \n79 \n80 __all__ = [\n81     \"RandomForestClassifier\",\n82     \"RandomForestRegressor\",\n83     \"ExtraTreesClassifier\",\n84     \"ExtraTreesRegressor\",\n85     \"RandomTreesEmbedding\",\n86 ]\n87 \n88 MAX_INT = np.iinfo(np.int32).max\n89 \n90 \n91 def _get_n_samples_bootstrap(n_samples, max_samples):\n92     \"\"\"\n93     Get the number of samples in a bootstrap sample.\n94 \n95     Parameters\n96     ----------\n97     n_samples : int\n98         Number of samples in the dataset.\n99     max_samples : int or float\n100         The maximum number of samples to draw from the total available:\n101             - if float, this indicates a fraction of the total and should be\n102               the interval `(0.0, 1.0]`;\n103             - if int, this indicates the exact number of samples;\n104             - if None, this indicates the total number of samples.\n105 \n106     Returns\n107     -------\n108     n_samples_bootstrap : int\n109         The total number of samples to draw for the bootstrap sample.\n110     \"\"\"\n111     if max_samples is None:\n112         return n_samples\n113 \n114     if isinstance(max_samples, Integral):\n115         if max_samples > n_samples:\n116             msg = \"`max_samples` must be <= n_samples={} but got value {}\"\n117             raise ValueError(msg.format(n_samples, max_samples))\n118         return max_samples\n119 \n120     if isinstance(max_samples, Real):\n121         return round(n_samples * max_samples)\n122 \n123 \n124 def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):\n125     \"\"\"\n126     Private function used to _parallel_build_trees function.\"\"\"\n127 \n128     random_instance = check_random_state(random_state)\n129     sample_indices = random_instance.randint(0, n_samples, n_samples_bootstrap)\n130 \n131     return sample_indices\n132 \n133 \n134 def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):\n135     \"\"\"\n136     Private function used to forest._set_oob_score function.\"\"\"\n137     sample_indices = _generate_sample_indices(\n138         random_state, n_samples, n_samples_bootstrap\n139     )\n140     sample_counts = np.bincount(sample_indices, minlength=n_samples)\n141     unsampled_mask = sample_counts == 0\n142     indices_range = np.arange(n_samples)\n143     unsampled_indices = indices_range[unsampled_mask]\n144 \n145     return unsampled_indices\n146 \n147 \n148 def _parallel_build_trees(\n149     tree,\n150     bootstrap,\n151     X,\n152     y,\n153     sample_weight,\n154     tree_idx,\n155     n_trees,\n156     verbose=0,\n157     class_weight=None,\n158     n_samples_bootstrap=None,\n159 ):\n160     \"\"\"\n161     Private function used to fit a single tree in parallel.\"\"\"\n162     if verbose > 1:\n163         print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n164 \n165     if bootstrap:\n166         n_samples = X.shape[0]\n167         if sample_weight is None:\n168             curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n169         else:\n170             curr_sample_weight = sample_weight.copy()\n171 \n172         indices = _generate_sample_indices(\n173             tree.random_state, n_samples, n_samples_bootstrap\n174         )\n175         sample_counts = np.bincount(indices, minlength=n_samples)\n176         curr_sample_weight *= sample_counts\n177 \n178         if class_weight == \"subsample\":\n179             with catch_warnings():\n180                 simplefilter(\"ignore\", DeprecationWarning)\n181                 curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n182         elif class_weight == \"balanced_subsample\":\n183             curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n184 \n185         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n186     else:\n187         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n188 \n189     return tree\n190 \n191 \n192 class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):\n193     \"\"\"\n194     Base class for forests of trees.\n195 \n196     Warning: This class should not be used directly. Use derived classes\n197     instead.\n198     \"\"\"\n199 \n200     _parameter_constraints: dict = {\n201         \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n202         \"bootstrap\": [\"boolean\"],\n203         \"oob_score\": [\"boolean\", callable],\n204         \"n_jobs\": [Integral, None],\n205         \"random_state\": [\"random_state\"],\n206         \"verbose\": [\"verbose\"],\n207         \"warm_start\": [\"boolean\"],\n208         \"max_samples\": [\n209             None,\n210             Interval(Real, 0.0, 1.0, closed=\"right\"),\n211             Interval(Integral, 1, None, closed=\"left\"),\n212         ],\n213     }\n214 \n215     @abstractmethod\n216     def __init__(\n217         self,\n218         estimator,\n219         n_estimators=100,\n220         *,\n221         estimator_params=tuple(),\n222         bootstrap=False,\n223         oob_score=False,\n224         n_jobs=None,\n225         random_state=None,\n226         verbose=0,\n227         warm_start=False,\n228         class_weight=None,\n229         max_samples=None,\n230         base_estimator=\"deprecated\",\n231     ):\n232         super().__init__(\n233             estimator=estimator,\n234             n_estimators=n_estimators,\n235             estimator_params=estimator_params,\n236             base_estimator=base_estimator,\n237         )\n238 \n239         self.bootstrap = bootstrap\n240         self.oob_score = oob_score\n241         self.n_jobs = n_jobs\n242         self.random_state = random_state\n243         self.verbose = verbose\n244         self.warm_start = warm_start\n245         self.class_weight = class_weight\n246         self.max_samples = max_samples\n247 \n248     def apply(self, X):\n249         \"\"\"\n250         Apply trees in the forest to X, return leaf indices.\n251 \n252         Parameters\n253         ----------\n254         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n255             The input samples. Internally, its dtype will be converted to\n256             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n257             converted into a sparse ``csr_matrix``.\n258 \n259         Returns\n260         -------\n261         X_leaves : ndarray of shape (n_samples, n_estimators)\n262             For each datapoint x in X and for each tree in the forest,\n263             return the index of the leaf x ends up in.\n264         \"\"\"\n265         X = self._validate_X_predict(X)\n266         results = Parallel(\n267             n_jobs=self.n_jobs,\n268             verbose=self.verbose,\n269             prefer=\"threads\",\n270         )(delayed(tree.apply)(X, check_input=False) for tree in self.estimators_)\n271 \n272         return np.array(results).T\n273 \n274     def decision_path(self, X):\n275         \"\"\"\n276         Return the decision path in the forest.\n277 \n278         .. versionadded:: 0.18\n279 \n280         Parameters\n281         ----------\n282         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n283             The input samples. Internally, its dtype will be converted to\n284             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n285             converted into a sparse ``csr_matrix``.\n286 \n287         Returns\n288         -------\n289         indicator : sparse matrix of shape (n_samples, n_nodes)\n290             Return a node indicator matrix where non zero elements indicates\n291             that the samples goes through the nodes. The matrix is of CSR\n292             format.\n293 \n294         n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n295             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n296             gives the indicator value for the i-th estimator.\n297         \"\"\"\n298         X = self._validate_X_predict(X)\n299         indicators = Parallel(\n300             n_jobs=self.n_jobs,\n301             verbose=self.verbose,\n302             prefer=\"threads\",\n303         )(\n304             delayed(tree.decision_path)(X, check_input=False)\n305             for tree in self.estimators_\n306         )\n307 \n308         n_nodes = [0]\n309         n_nodes.extend([i.shape[1] for i in indicators])\n310         n_nodes_ptr = np.array(n_nodes).cumsum()\n311 \n312         return sparse_hstack(indicators).tocsr(), n_nodes_ptr\n313 \n314     def fit(self, X, y, sample_weight=None):\n315         \"\"\"\n316         Build a forest of trees from the training set (X, y).\n317 \n318         Parameters\n319         ----------\n320         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n321             The training input samples. Internally, its dtype will be converted\n322             to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n323             converted into a sparse ``csc_matrix``.\n324 \n325         y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n326             The target values (class labels in classification, real numbers in\n327             regression).\n328 \n329         sample_weight : array-like of shape (n_samples,), default=None\n330             Sample weights. If None, then samples are equally weighted. Splits\n331             that would create child nodes with net zero or negative weight are\n332             ignored while searching for a split in each node. In the case of\n333             classification, splits are also ignored if they would result in any\n334             single class carrying a negative weight in either child node.\n335 \n336         Returns\n337         -------\n338         self : object\n339             Fitted estimator.\n340         \"\"\"\n341         self._validate_params()\n342 \n343         # Validate or convert input data\n344         if issparse(y):\n345             raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n346         X, y = self._validate_data(\n347             X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n348         )\n349         if sample_weight is not None:\n350             sample_weight = _check_sample_weight(sample_weight, X)\n351 \n352         if issparse(X):\n353             # Pre-sort indices to avoid that each individual tree of the\n354             # ensemble sorts the indices.\n355             X.sort_indices()\n356 \n357         y = np.atleast_1d(y)\n358         if y.ndim == 2 and y.shape[1] == 1:\n359             warn(\n360                 \"A column-vector y was passed when a 1d array was\"\n361                 \" expected. Please change the shape of y to \"\n362                 \"(n_samples,), for example using ravel().\",\n363                 DataConversionWarning,\n364                 stacklevel=2,\n365             )\n366 \n367         if y.ndim == 1:\n368             # reshape is necessary to preserve the data contiguity against vs\n369             # [:, np.newaxis] that does not.\n370             y = np.reshape(y, (-1, 1))\n371 \n372         if self.criterion == \"poisson\":\n373             if np.any(y < 0):\n374                 raise ValueError(\n375                     \"Some value(s) of y are negative which is \"\n376                     \"not allowed for Poisson regression.\"\n377                 )\n378             if np.sum(y) <= 0:\n379                 raise ValueError(\n380                     \"Sum of y is not strictly positive which \"\n381                     \"is necessary for Poisson regression.\"\n382                 )\n383 \n384         self.n_outputs_ = y.shape[1]\n385 \n386         y, expanded_class_weight = self._validate_y_class_weight(y)\n387 \n388         if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n389             y = np.ascontiguousarray(y, dtype=DOUBLE)\n390 \n391         if expanded_class_weight is not None:\n392             if sample_weight is not None:\n393                 sample_weight = sample_weight * expanded_class_weight\n394             else:\n395                 sample_weight = expanded_class_weight\n396 \n397         if not self.bootstrap and self.max_samples is not None:\n398             raise ValueError(\n399                 \"`max_sample` cannot be set if `bootstrap=False`. \"\n400                 \"Either switch to `bootstrap=True` or set \"\n401                 \"`max_sample=None`.\"\n402             )\n403         elif self.bootstrap:\n404             n_samples_bootstrap = _get_n_samples_bootstrap(\n405                 n_samples=X.shape[0], max_samples=self.max_samples\n406             )\n407         else:\n408             n_samples_bootstrap = None\n409 \n410         self._validate_estimator()\n411         if isinstance(self, (RandomForestRegressor, ExtraTreesRegressor)):\n412             # TODO(1.3): Remove \"auto\"\n413             if self.max_features == \"auto\":\n414                 warn(\n415                     \"`max_features='auto'` has been deprecated in 1.1 \"\n416                     \"and will be removed in 1.3. To keep the past behaviour, \"\n417                     \"explicitly set `max_features=1.0` or remove this \"\n418                     \"parameter as it is also the default value for \"\n419                     \"RandomForestRegressors and ExtraTreesRegressors.\",\n420                     FutureWarning,\n421                 )\n422         elif isinstance(self, (RandomForestClassifier, ExtraTreesClassifier)):\n423             # TODO(1.3): Remove \"auto\"\n424             if self.max_features == \"auto\":\n425                 warn(\n426                     \"`max_features='auto'` has been deprecated in 1.1 \"\n427                     \"and will be removed in 1.3. To keep the past behaviour, \"\n428                     \"explicitly set `max_features='sqrt'` or remove this \"\n429                     \"parameter as it is also the default value for \"\n430                     \"RandomForestClassifiers and ExtraTreesClassifiers.\",\n431                     FutureWarning,\n432                 )\n433 \n434         if not self.bootstrap and self.oob_score:\n435             raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n436 \n437         random_state = check_random_state(self.random_state)\n438 \n439         if not self.warm_start or not hasattr(self, \"estimators_\"):\n440             # Free allocated memory, if any\n441             self.estimators_ = []\n442 \n443         n_more_estimators = self.n_estimators - len(self.estimators_)\n444 \n445         if n_more_estimators < 0:\n446             raise ValueError(\n447                 \"n_estimators=%d must be larger or equal to \"\n448                 \"len(estimators_)=%d when warm_start==True\"\n449                 % (self.n_estimators, len(self.estimators_))\n450             )\n451 \n452         elif n_more_estimators == 0:\n453             warn(\n454                 \"Warm-start fitting without increasing n_estimators does not \"\n455                 \"fit new trees.\"\n456             )\n457         else:\n458             if self.warm_start and len(self.estimators_) > 0:\n459                 # We draw from the random state to get the random state we\n460                 # would have got if we hadn't used a warm_start.\n461                 random_state.randint(MAX_INT, size=len(self.estimators_))\n462 \n463             trees = [\n464                 self._make_estimator(append=False, random_state=random_state)\n465                 for i in range(n_more_estimators)\n466             ]\n467 \n468             # Parallel loop: we prefer the threading backend as the Cython code\n469             # for fitting the trees is internally releasing the Python GIL\n470             # making threading more efficient than multiprocessing in\n471             # that case. However, for joblib 0.12+ we respect any\n472             # parallel_backend contexts set at a higher level,\n473             # since correctness does not rely on using threads.\n474             trees = Parallel(\n475                 n_jobs=self.n_jobs,\n476                 verbose=self.verbose,\n477                 prefer=\"threads\",\n478             )(\n479                 delayed(_parallel_build_trees)(\n480                     t,\n481                     self.bootstrap,\n482                     X,\n483                     y,\n484                     sample_weight,\n485                     i,\n486                     len(trees),\n487                     verbose=self.verbose,\n488                     class_weight=self.class_weight,\n489                     n_samples_bootstrap=n_samples_bootstrap,\n490                 )\n491                 for i, t in enumerate(trees)\n492             )\n493 \n494             # Collect newly grown trees\n495             self.estimators_.extend(trees)\n496 \n497         if self.oob_score:\n498             y_type = type_of_target(y)\n499             if y_type in (\"multiclass-multioutput\", \"unknown\"):\n500                 # FIXME: we could consider to support multiclass-multioutput if\n501                 # we introduce or reuse a constructor parameter (e.g.\n502                 # oob_score) allowing our user to pass a callable defining the\n503                 # scoring strategy on OOB sample.\n504                 raise ValueError(\n505                     \"The type of target cannot be used to compute OOB \"\n506                     f\"estimates. Got {y_type} while only the following are \"\n507                     \"supported: continuous, continuous-multioutput, binary, \"\n508                     \"multiclass, multilabel-indicator.\"\n509                 )\n510 \n511             if callable(self.oob_score):\n512                 self._set_oob_score_and_attributes(\n513                     X, y, scoring_function=self.oob_score\n514                 )\n515             else:\n516                 self._set_oob_score_and_attributes(X, y)\n517 \n518         # Decapsulate classes_ attributes\n519         if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n520             self.n_classes_ = self.n_classes_[0]\n521             self.classes_ = self.classes_[0]\n522 \n523         return self\n524 \n525     @abstractmethod\n526     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n527         \"\"\"Compute and set the OOB score and attributes.\n528 \n529         Parameters\n530         ----------\n531         X : array-like of shape (n_samples, n_features)\n532             The data matrix.\n533         y : ndarray of shape (n_samples, n_outputs)\n534             The target matrix.\n535         scoring_function : callable, default=None\n536             Scoring function for OOB score. Default depends on whether\n537             this is a regression (R2 score) or classification problem\n538             (accuracy score).\n539         \"\"\"\n540 \n541     def _compute_oob_predictions(self, X, y):\n542         \"\"\"Compute and set the OOB score.\n543 \n544         Parameters\n545         ----------\n546         X : array-like of shape (n_samples, n_features)\n547             The data matrix.\n548         y : ndarray of shape (n_samples, n_outputs)\n549             The target matrix.\n550 \n551         Returns\n552         -------\n553         oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or \\\n554                 (n_samples, 1, n_outputs)\n555             The OOB predictions.\n556         \"\"\"\n557         # Prediction requires X to be in CSR format\n558         if issparse(X):\n559             X = X.tocsr()\n560 \n561         n_samples = y.shape[0]\n562         n_outputs = self.n_outputs_\n563         if is_classifier(self) and hasattr(self, \"n_classes_\"):\n564             # n_classes_ is a ndarray at this stage\n565             # all the supported type of target will have the same number of\n566             # classes in all outputs\n567             oob_pred_shape = (n_samples, self.n_classes_[0], n_outputs)\n568         else:\n569             # for regression, n_classes_ does not exist and we create an empty\n570             # axis to be consistent with the classification case and make\n571             # the array operations compatible with the 2 settings\n572             oob_pred_shape = (n_samples, 1, n_outputs)\n573 \n574         oob_pred = np.zeros(shape=oob_pred_shape, dtype=np.float64)\n575         n_oob_pred = np.zeros((n_samples, n_outputs), dtype=np.int64)\n576 \n577         n_samples_bootstrap = _get_n_samples_bootstrap(\n578             n_samples,\n579             self.max_samples,\n580         )\n581         for estimator in self.estimators_:\n582             unsampled_indices = _generate_unsampled_indices(\n583                 estimator.random_state,\n584                 n_samples,\n585                 n_samples_bootstrap,\n586             )\n587 \n588             y_pred = self._get_oob_predictions(estimator, X[unsampled_indices, :])\n589             oob_pred[unsampled_indices, ...] += y_pred\n590             n_oob_pred[unsampled_indices, :] += 1\n591 \n592         for k in range(n_outputs):\n593             if (n_oob_pred == 0).any():\n594                 warn(\n595                     \"Some inputs do not have OOB scores. This probably means \"\n596                     \"too few trees were used to compute any reliable OOB \"\n597                     \"estimates.\",\n598                     UserWarning,\n599                 )\n600                 n_oob_pred[n_oob_pred == 0] = 1\n601             oob_pred[..., k] /= n_oob_pred[..., [k]]\n602 \n603         return oob_pred\n604 \n605     def _validate_y_class_weight(self, y):\n606         # Default implementation\n607         return y, None\n608 \n609     def _validate_X_predict(self, X):\n610         \"\"\"\n611         Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n612         check_is_fitted(self)\n613         X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n614         if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):\n615             raise ValueError(\"No support for np.int64 index based sparse matrices\")\n616         return X\n617 \n618     @property\n619     def feature_importances_(self):\n620         \"\"\"\n621         The impurity-based feature importances.\n622 \n623         The higher, the more important the feature.\n624         The importance of a feature is computed as the (normalized)\n625         total reduction of the criterion brought by that feature.  It is also\n626         known as the Gini importance.\n627 \n628         Warning: impurity-based feature importances can be misleading for\n629         high cardinality features (many unique values). See\n630         :func:`sklearn.inspection.permutation_importance` as an alternative.\n631 \n632         Returns\n633         -------\n634         feature_importances_ : ndarray of shape (n_features,)\n635             The values of this array sum to 1, unless all trees are single node\n636             trees consisting of only the root node, in which case it will be an\n637             array of zeros.\n638         \"\"\"\n639         check_is_fitted(self)\n640 \n641         all_importances = Parallel(n_jobs=self.n_jobs, prefer=\"threads\")(\n642             delayed(getattr)(tree, \"feature_importances_\")\n643             for tree in self.estimators_\n644             if tree.tree_.node_count > 1\n645         )\n646 \n647         if not all_importances:\n648             return np.zeros(self.n_features_in_, dtype=np.float64)\n649 \n650         all_importances = np.mean(all_importances, axis=0, dtype=np.float64)\n651         return all_importances / np.sum(all_importances)\n652 \n653 \n654 def _accumulate_prediction(predict, X, out, lock):\n655     \"\"\"\n656     This is a utility function for joblib's Parallel.\n657 \n658     It can't go locally in ForestClassifier or ForestRegressor, because joblib\n659     complains that it cannot pickle it when placed there.\n660     \"\"\"\n661     prediction = predict(X, check_input=False)\n662     with lock:\n663         if len(out) == 1:\n664             out[0] += prediction\n665         else:\n666             for i in range(len(out)):\n667                 out[i] += prediction[i]\n668 \n669 \n670 class ForestClassifier(ClassifierMixin, BaseForest, metaclass=ABCMeta):\n671     \"\"\"\n672     Base class for forest of trees-based classifiers.\n673 \n674     Warning: This class should not be used directly. Use derived classes\n675     instead.\n676     \"\"\"\n677 \n678     @abstractmethod\n679     def __init__(\n680         self,\n681         estimator,\n682         n_estimators=100,\n683         *,\n684         estimator_params=tuple(),\n685         bootstrap=False,\n686         oob_score=False,\n687         n_jobs=None,\n688         random_state=None,\n689         verbose=0,\n690         warm_start=False,\n691         class_weight=None,\n692         max_samples=None,\n693         base_estimator=\"deprecated\",\n694     ):\n695         super().__init__(\n696             estimator=estimator,\n697             n_estimators=n_estimators,\n698             estimator_params=estimator_params,\n699             bootstrap=bootstrap,\n700             oob_score=oob_score,\n701             n_jobs=n_jobs,\n702             random_state=random_state,\n703             verbose=verbose,\n704             warm_start=warm_start,\n705             class_weight=class_weight,\n706             max_samples=max_samples,\n707             base_estimator=base_estimator,\n708         )\n709 \n710     @staticmethod\n711     def _get_oob_predictions(tree, X):\n712         \"\"\"Compute the OOB predictions for an individual tree.\n713 \n714         Parameters\n715         ----------\n716         tree : DecisionTreeClassifier object\n717             A single decision tree classifier.\n718         X : ndarray of shape (n_samples, n_features)\n719             The OOB samples.\n720 \n721         Returns\n722         -------\n723         y_pred : ndarray of shape (n_samples, n_classes, n_outputs)\n724             The OOB associated predictions.\n725         \"\"\"\n726         y_pred = tree.predict_proba(X, check_input=False)\n727         y_pred = np.array(y_pred, copy=False)\n728         if y_pred.ndim == 2:\n729             # binary and multiclass\n730             y_pred = y_pred[..., np.newaxis]\n731         else:\n732             # Roll the first `n_outputs` axis to the last axis. We will reshape\n733             # from a shape of (n_outputs, n_samples, n_classes) to a shape of\n734             # (n_samples, n_classes, n_outputs).\n735             y_pred = np.rollaxis(y_pred, axis=0, start=3)\n736         return y_pred\n737 \n738     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n739         \"\"\"Compute and set the OOB score and attributes.\n740 \n741         Parameters\n742         ----------\n743         X : array-like of shape (n_samples, n_features)\n744             The data matrix.\n745         y : ndarray of shape (n_samples, n_outputs)\n746             The target matrix.\n747         scoring_function : callable, default=None\n748             Scoring function for OOB score. Defaults to `accuracy_score`.\n749         \"\"\"\n750         self.oob_decision_function_ = super()._compute_oob_predictions(X, y)\n751         if self.oob_decision_function_.shape[-1] == 1:\n752             # drop the n_outputs axis if there is a single output\n753             self.oob_decision_function_ = self.oob_decision_function_.squeeze(axis=-1)\n754 \n755         if scoring_function is None:\n756             scoring_function = accuracy_score\n757 \n758         self.oob_score_ = scoring_function(\n759             y, np.argmax(self.oob_decision_function_, axis=1)\n760         )\n761 \n762     def _validate_y_class_weight(self, y):\n763         check_classification_targets(y)\n764 \n765         y = np.copy(y)\n766         expanded_class_weight = None\n767 \n768         if self.class_weight is not None:\n769             y_original = np.copy(y)\n770 \n771         self.classes_ = []\n772         self.n_classes_ = []\n773 \n774         y_store_unique_indices = np.zeros(y.shape, dtype=int)\n775         for k in range(self.n_outputs_):\n776             classes_k, y_store_unique_indices[:, k] = np.unique(\n777                 y[:, k], return_inverse=True\n778             )\n779             self.classes_.append(classes_k)\n780             self.n_classes_.append(classes_k.shape[0])\n781         y = y_store_unique_indices\n782 \n783         if self.class_weight is not None:\n784             valid_presets = (\"balanced\", \"balanced_subsample\")\n785             if isinstance(self.class_weight, str):\n786                 if self.class_weight not in valid_presets:\n787                     raise ValueError(\n788                         \"Valid presets for class_weight include \"\n789                         '\"balanced\" and \"balanced_subsample\".'\n790                         'Given \"%s\".'\n791                         % self.class_weight\n792                     )\n793                 if self.warm_start:\n794                     warn(\n795                         'class_weight presets \"balanced\" or '\n796                         '\"balanced_subsample\" are '\n797                         \"not recommended for warm_start if the fitted data \"\n798                         \"differs from the full dataset. In order to use \"\n799                         '\"balanced\" weights, use compute_class_weight '\n800                         '(\"balanced\", classes, y). In place of y you can use '\n801                         \"a large enough sample of the full training set \"\n802                         \"target to properly estimate the class frequency \"\n803                         \"distributions. Pass the resulting weights as the \"\n804                         \"class_weight parameter.\"\n805                     )\n806 \n807             if self.class_weight != \"balanced_subsample\" or not self.bootstrap:\n808                 if self.class_weight == \"balanced_subsample\":\n809                     class_weight = \"balanced\"\n810                 else:\n811                     class_weight = self.class_weight\n812                 expanded_class_weight = compute_sample_weight(class_weight, y_original)\n813 \n814         return y, expanded_class_weight\n815 \n816     def predict(self, X):\n817         \"\"\"\n818         Predict class for X.\n819 \n820         The predicted class of an input sample is a vote by the trees in\n821         the forest, weighted by their probability estimates. That is,\n822         the predicted class is the one with highest mean probability\n823         estimate across the trees.\n824 \n825         Parameters\n826         ----------\n827         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n828             The input samples. Internally, its dtype will be converted to\n829             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n830             converted into a sparse ``csr_matrix``.\n831 \n832         Returns\n833         -------\n834         y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n835             The predicted classes.\n836         \"\"\"\n837         proba = self.predict_proba(X)\n838 \n839         if self.n_outputs_ == 1:\n840             return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n841 \n842         else:\n843             n_samples = proba[0].shape[0]\n844             # all dtypes should be the same, so just take the first\n845             class_type = self.classes_[0].dtype\n846             predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n847 \n848             for k in range(self.n_outputs_):\n849                 predictions[:, k] = self.classes_[k].take(\n850                     np.argmax(proba[k], axis=1), axis=0\n851                 )\n852 \n853             return predictions\n854 \n855     def predict_proba(self, X):\n856         \"\"\"\n857         Predict class probabilities for X.\n858 \n859         The predicted class probabilities of an input sample are computed as\n860         the mean predicted class probabilities of the trees in the forest.\n861         The class probability of a single tree is the fraction of samples of\n862         the same class in a leaf.\n863 \n864         Parameters\n865         ----------\n866         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n867             The input samples. Internally, its dtype will be converted to\n868             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n869             converted into a sparse ``csr_matrix``.\n870 \n871         Returns\n872         -------\n873         p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n874             The class probabilities of the input samples. The order of the\n875             classes corresponds to that in the attribute :term:`classes_`.\n876         \"\"\"\n877         check_is_fitted(self)\n878         # Check data\n879         X = self._validate_X_predict(X)\n880 \n881         # Assign chunk of trees to jobs\n882         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n883 \n884         # avoid storing the output of every estimator by summing them here\n885         all_proba = [\n886             np.zeros((X.shape[0], j), dtype=np.float64)\n887             for j in np.atleast_1d(self.n_classes_)\n888         ]\n889         lock = threading.Lock()\n890         Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n891             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)\n892             for e in self.estimators_\n893         )\n894 \n895         for proba in all_proba:\n896             proba /= len(self.estimators_)\n897 \n898         if len(all_proba) == 1:\n899             return all_proba[0]\n900         else:\n901             return all_proba\n902 \n903     def predict_log_proba(self, X):\n904         \"\"\"\n905         Predict class log-probabilities for X.\n906 \n907         The predicted class log-probabilities of an input sample is computed as\n908         the log of the mean predicted class probabilities of the trees in the\n909         forest.\n910 \n911         Parameters\n912         ----------\n913         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n914             The input samples. Internally, its dtype will be converted to\n915             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n916             converted into a sparse ``csr_matrix``.\n917 \n918         Returns\n919         -------\n920         p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n921             The class probabilities of the input samples. The order of the\n922             classes corresponds to that in the attribute :term:`classes_`.\n923         \"\"\"\n924         proba = self.predict_proba(X)\n925 \n926         if self.n_outputs_ == 1:\n927             return np.log(proba)\n928 \n929         else:\n930             for k in range(self.n_outputs_):\n931                 proba[k] = np.log(proba[k])\n932 \n933             return proba\n934 \n935     def _more_tags(self):\n936         return {\"multilabel\": True}\n937 \n938 \n939 class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):\n940     \"\"\"\n941     Base class for forest of trees-based regressors.\n942 \n943     Warning: This class should not be used directly. Use derived classes\n944     instead.\n945     \"\"\"\n946 \n947     @abstractmethod\n948     def __init__(\n949         self,\n950         estimator,\n951         n_estimators=100,\n952         *,\n953         estimator_params=tuple(),\n954         bootstrap=False,\n955         oob_score=False,\n956         n_jobs=None,\n957         random_state=None,\n958         verbose=0,\n959         warm_start=False,\n960         max_samples=None,\n961         base_estimator=\"deprecated\",\n962     ):\n963         super().__init__(\n964             estimator,\n965             n_estimators=n_estimators,\n966             estimator_params=estimator_params,\n967             bootstrap=bootstrap,\n968             oob_score=oob_score,\n969             n_jobs=n_jobs,\n970             random_state=random_state,\n971             verbose=verbose,\n972             warm_start=warm_start,\n973             max_samples=max_samples,\n974             base_estimator=base_estimator,\n975         )\n976 \n977     def predict(self, X):\n978         \"\"\"\n979         Predict regression target for X.\n980 \n981         The predicted regression target of an input sample is computed as the\n982         mean predicted regression targets of the trees in the forest.\n983 \n984         Parameters\n985         ----------\n986         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n987             The input samples. Internally, its dtype will be converted to\n988             ``dtype=np.float32``. If a sparse matrix is provided, it will be\n989             converted into a sparse ``csr_matrix``.\n990 \n991         Returns\n992         -------\n993         y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n994             The predicted values.\n995         \"\"\"\n996         check_is_fitted(self)\n997         # Check data\n998         X = self._validate_X_predict(X)\n999 \n1000         # Assign chunk of trees to jobs\n1001         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n1002 \n1003         # avoid storing the output of every estimator by summing them here\n1004         if self.n_outputs_ > 1:\n1005             y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)\n1006         else:\n1007             y_hat = np.zeros((X.shape[0]), dtype=np.float64)\n1008 \n1009         # Parallel loop\n1010         lock = threading.Lock()\n1011         Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n1012             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)\n1013             for e in self.estimators_\n1014         )\n1015 \n1016         y_hat /= len(self.estimators_)\n1017 \n1018         return y_hat\n1019 \n1020     @staticmethod\n1021     def _get_oob_predictions(tree, X):\n1022         \"\"\"Compute the OOB predictions for an individual tree.\n1023 \n1024         Parameters\n1025         ----------\n1026         tree : DecisionTreeRegressor object\n1027             A single decision tree regressor.\n1028         X : ndarray of shape (n_samples, n_features)\n1029             The OOB samples.\n1030 \n1031         Returns\n1032         -------\n1033         y_pred : ndarray of shape (n_samples, 1, n_outputs)\n1034             The OOB associated predictions.\n1035         \"\"\"\n1036         y_pred = tree.predict(X, check_input=False)\n1037         if y_pred.ndim == 1:\n1038             # single output regression\n1039             y_pred = y_pred[:, np.newaxis, np.newaxis]\n1040         else:\n1041             # multioutput regression\n1042             y_pred = y_pred[:, np.newaxis, :]\n1043         return y_pred\n1044 \n1045     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n1046         \"\"\"Compute and set the OOB score and attributes.\n1047 \n1048         Parameters\n1049         ----------\n1050         X : array-like of shape (n_samples, n_features)\n1051             The data matrix.\n1052         y : ndarray of shape (n_samples, n_outputs)\n1053             The target matrix.\n1054         scoring_function : callable, default=None\n1055             Scoring function for OOB score. Defaults to `r2_score`.\n1056         \"\"\"\n1057         self.oob_prediction_ = super()._compute_oob_predictions(X, y).squeeze(axis=1)\n1058         if self.oob_prediction_.shape[-1] == 1:\n1059             # drop the n_outputs axis if there is a single output\n1060             self.oob_prediction_ = self.oob_prediction_.squeeze(axis=-1)\n1061 \n1062         if scoring_function is None:\n1063             scoring_function = r2_score\n1064 \n1065         self.oob_score_ = scoring_function(y, self.oob_prediction_)\n1066 \n1067     def _compute_partial_dependence_recursion(self, grid, target_features):\n1068         \"\"\"Fast partial dependence computation.\n1069 \n1070         Parameters\n1071         ----------\n1072         grid : ndarray of shape (n_samples, n_target_features)\n1073             The grid points on which the partial dependence should be\n1074             evaluated.\n1075         target_features : ndarray of shape (n_target_features)\n1076             The set of target features for which the partial dependence\n1077             should be evaluated.\n1078 \n1079         Returns\n1080         -------\n1081         averaged_predictions : ndarray of shape (n_samples,)\n1082             The value of the partial dependence function on each grid point.\n1083         \"\"\"\n1084         grid = np.asarray(grid, dtype=DTYPE, order=\"C\")\n1085         averaged_predictions = np.zeros(\n1086             shape=grid.shape[0], dtype=np.float64, order=\"C\"\n1087         )\n1088 \n1089         for tree in self.estimators_:\n1090             # Note: we don't sum in parallel because the GIL isn't released in\n1091             # the fast method.\n1092             tree.tree_.compute_partial_dependence(\n1093                 grid, target_features, averaged_predictions\n1094             )\n1095         # Average over the forest\n1096         averaged_predictions /= len(self.estimators_)\n1097 \n1098         return averaged_predictions\n1099 \n1100     def _more_tags(self):\n1101         return {\"multilabel\": True}\n1102 \n1103 \n1104 class RandomForestClassifier(ForestClassifier):\n1105     \"\"\"\n1106     A random forest classifier.\n1107 \n1108     A random forest is a meta estimator that fits a number of decision tree\n1109     classifiers on various sub-samples of the dataset and uses averaging to\n1110     improve the predictive accuracy and control over-fitting.\n1111     The sub-sample size is controlled with the `max_samples` parameter if\n1112     `bootstrap=True` (default), otherwise the whole dataset is used to build\n1113     each tree.\n1114 \n1115     Read more in the :ref:`User Guide <forest>`.\n1116 \n1117     Parameters\n1118     ----------\n1119     n_estimators : int, default=100\n1120         The number of trees in the forest.\n1121 \n1122         .. versionchanged:: 0.22\n1123            The default value of ``n_estimators`` changed from 10 to 100\n1124            in 0.22.\n1125 \n1126     criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n1127         The function to measure the quality of a split. Supported criteria are\n1128         \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n1129         Shannon information gain, see :ref:`tree_mathematical_formulation`.\n1130         Note: This parameter is tree-specific.\n1131 \n1132     max_depth : int, default=None\n1133         The maximum depth of the tree. If None, then nodes are expanded until\n1134         all leaves are pure or until all leaves contain less than\n1135         min_samples_split samples.\n1136 \n1137     min_samples_split : int or float, default=2\n1138         The minimum number of samples required to split an internal node:\n1139 \n1140         - If int, then consider `min_samples_split` as the minimum number.\n1141         - If float, then `min_samples_split` is a fraction and\n1142           `ceil(min_samples_split * n_samples)` are the minimum\n1143           number of samples for each split.\n1144 \n1145         .. versionchanged:: 0.18\n1146            Added float values for fractions.\n1147 \n1148     min_samples_leaf : int or float, default=1\n1149         The minimum number of samples required to be at a leaf node.\n1150         A split point at any depth will only be considered if it leaves at\n1151         least ``min_samples_leaf`` training samples in each of the left and\n1152         right branches.  This may have the effect of smoothing the model,\n1153         especially in regression.\n1154 \n1155         - If int, then consider `min_samples_leaf` as the minimum number.\n1156         - If float, then `min_samples_leaf` is a fraction and\n1157           `ceil(min_samples_leaf * n_samples)` are the minimum\n1158           number of samples for each node.\n1159 \n1160         .. versionchanged:: 0.18\n1161            Added float values for fractions.\n1162 \n1163     min_weight_fraction_leaf : float, default=0.0\n1164         The minimum weighted fraction of the sum total of weights (of all\n1165         the input samples) required to be at a leaf node. Samples have\n1166         equal weight when sample_weight is not provided.\n1167 \n1168     max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n1169         The number of features to consider when looking for the best split:\n1170 \n1171         - If int, then consider `max_features` features at each split.\n1172         - If float, then `max_features` is a fraction and\n1173           `max(1, int(max_features * n_features_in_))` features are considered at each\n1174           split.\n1175         - If \"auto\", then `max_features=sqrt(n_features)`.\n1176         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1177         - If \"log2\", then `max_features=log2(n_features)`.\n1178         - If None, then `max_features=n_features`.\n1179 \n1180         .. versionchanged:: 1.1\n1181             The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n1182 \n1183         .. deprecated:: 1.1\n1184             The `\"auto\"` option was deprecated in 1.1 and will be removed\n1185             in 1.3.\n1186 \n1187         Note: the search for a split does not stop until at least one\n1188         valid partition of the node samples is found, even if it requires to\n1189         effectively inspect more than ``max_features`` features.\n1190 \n1191     max_leaf_nodes : int, default=None\n1192         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1193         Best nodes are defined as relative reduction in impurity.\n1194         If None then unlimited number of leaf nodes.\n1195 \n1196     min_impurity_decrease : float, default=0.0\n1197         A node will be split if this split induces a decrease of the impurity\n1198         greater than or equal to this value.\n1199 \n1200         The weighted impurity decrease equation is the following::\n1201 \n1202             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1203                                 - N_t_L / N_t * left_impurity)\n1204 \n1205         where ``N`` is the total number of samples, ``N_t`` is the number of\n1206         samples at the current node, ``N_t_L`` is the number of samples in the\n1207         left child, and ``N_t_R`` is the number of samples in the right child.\n1208 \n1209         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1210         if ``sample_weight`` is passed.\n1211 \n1212         .. versionadded:: 0.19\n1213 \n1214     bootstrap : bool, default=True\n1215         Whether bootstrap samples are used when building trees. If False, the\n1216         whole dataset is used to build each tree.\n1217 \n1218     oob_score : bool or callable, default=False\n1219         Whether to use out-of-bag samples to estimate the generalization score.\n1220         By default, :func:`~sklearn.metrics.accuracy_score` is used.\n1221         Provide a callable with signature `metric(y_true, y_pred)` to use a\n1222         custom metric. Only available if `bootstrap=True`.\n1223 \n1224     n_jobs : int, default=None\n1225         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n1226         :meth:`decision_path` and :meth:`apply` are all parallelized over the\n1227         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n1228         context. ``-1`` means using all processors. See :term:`Glossary\n1229         <n_jobs>` for more details.\n1230 \n1231     random_state : int, RandomState instance or None, default=None\n1232         Controls both the randomness of the bootstrapping of the samples used\n1233         when building trees (if ``bootstrap=True``) and the sampling of the\n1234         features to consider when looking for the best split at each node\n1235         (if ``max_features < n_features``).\n1236         See :term:`Glossary <random_state>` for details.\n1237 \n1238     verbose : int, default=0\n1239         Controls the verbosity when fitting and predicting.\n1240 \n1241     warm_start : bool, default=False\n1242         When set to ``True``, reuse the solution of the previous call to fit\n1243         and add more estimators to the ensemble, otherwise, just fit a whole\n1244         new forest. See :term:`Glossary <warm_start>` and\n1245         :ref:`gradient_boosting_warm_start` for details.\n1246 \n1247     class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n1248             default=None\n1249         Weights associated with classes in the form ``{class_label: weight}``.\n1250         If not given, all classes are supposed to have weight one. For\n1251         multi-output problems, a list of dicts can be provided in the same\n1252         order as the columns of y.\n1253 \n1254         Note that for multioutput (including multilabel) weights should be\n1255         defined for each class of every column in its own dict. For example,\n1256         for four-class multilabel classification weights should be\n1257         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n1258         [{1:1}, {2:5}, {3:1}, {4:1}].\n1259 \n1260         The \"balanced\" mode uses the values of y to automatically adjust\n1261         weights inversely proportional to class frequencies in the input data\n1262         as ``n_samples / (n_classes * np.bincount(y))``\n1263 \n1264         The \"balanced_subsample\" mode is the same as \"balanced\" except that\n1265         weights are computed based on the bootstrap sample for every tree\n1266         grown.\n1267 \n1268         For multi-output, the weights of each column of y will be multiplied.\n1269 \n1270         Note that these weights will be multiplied with sample_weight (passed\n1271         through the fit method) if sample_weight is specified.\n1272 \n1273     ccp_alpha : non-negative float, default=0.0\n1274         Complexity parameter used for Minimal Cost-Complexity Pruning. The\n1275         subtree with the largest cost complexity that is smaller than\n1276         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n1277         :ref:`minimal_cost_complexity_pruning` for details.\n1278 \n1279         .. versionadded:: 0.22\n1280 \n1281     max_samples : int or float, default=None\n1282         If bootstrap is True, the number of samples to draw from X\n1283         to train each base estimator.\n1284 \n1285         - If None (default), then draw `X.shape[0]` samples.\n1286         - If int, then draw `max_samples` samples.\n1287         - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n1288           `max_samples` should be in the interval `(0.0, 1.0]`.\n1289 \n1290         .. versionadded:: 0.22\n1291 \n1292     Attributes\n1293     ----------\n1294     estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n1295         The child estimator template used to create the collection of fitted\n1296         sub-estimators.\n1297 \n1298         .. versionadded:: 1.2\n1299            `base_estimator_` was renamed to `estimator_`.\n1300 \n1301     base_estimator_ : DecisionTreeClassifier\n1302         The child estimator template used to create the collection of fitted\n1303         sub-estimators.\n1304 \n1305         .. deprecated:: 1.2\n1306             `base_estimator_` is deprecated and will be removed in 1.4.\n1307             Use `estimator_` instead.\n1308 \n1309     estimators_ : list of DecisionTreeClassifier\n1310         The collection of fitted sub-estimators.\n1311 \n1312     classes_ : ndarray of shape (n_classes,) or a list of such arrays\n1313         The classes labels (single output problem), or a list of arrays of\n1314         class labels (multi-output problem).\n1315 \n1316     n_classes_ : int or list\n1317         The number of classes (single output problem), or a list containing the\n1318         number of classes for each output (multi-output problem).\n1319 \n1320     n_features_in_ : int\n1321         Number of features seen during :term:`fit`.\n1322 \n1323         .. versionadded:: 0.24\n1324 \n1325     feature_names_in_ : ndarray of shape (`n_features_in_`,)\n1326         Names of features seen during :term:`fit`. Defined only when `X`\n1327         has feature names that are all strings.\n1328 \n1329         .. versionadded:: 1.0\n1330 \n1331     n_outputs_ : int\n1332         The number of outputs when ``fit`` is performed.\n1333 \n1334     feature_importances_ : ndarray of shape (n_features,)\n1335         The impurity-based feature importances.\n1336         The higher, the more important the feature.\n1337         The importance of a feature is computed as the (normalized)\n1338         total reduction of the criterion brought by that feature.  It is also\n1339         known as the Gini importance.\n1340 \n1341         Warning: impurity-based feature importances can be misleading for\n1342         high cardinality features (many unique values). See\n1343         :func:`sklearn.inspection.permutation_importance` as an alternative.\n1344 \n1345     oob_score_ : float\n1346         Score of the training dataset obtained using an out-of-bag estimate.\n1347         This attribute exists only when ``oob_score`` is True.\n1348 \n1349     oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\n1350             (n_samples, n_classes, n_outputs)\n1351         Decision function computed with out-of-bag estimate on the training\n1352         set. If n_estimators is small it might be possible that a data point\n1353         was never left out during the bootstrap. In this case,\n1354         `oob_decision_function_` might contain NaN. This attribute exists\n1355         only when ``oob_score`` is True.\n1356 \n1357     See Also\n1358     --------\n1359     sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n1360     sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n1361         tree classifiers.\n1362 \n1363     Notes\n1364     -----\n1365     The default values for the parameters controlling the size of the trees\n1366     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n1367     unpruned trees which can potentially be very large on some data sets. To\n1368     reduce memory consumption, the complexity and size of the trees should be\n1369     controlled by setting those parameter values.\n1370 \n1371     The features are always randomly permuted at each split. Therefore,\n1372     the best found split may vary, even with the same training data,\n1373     ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n1374     of the criterion is identical for several splits enumerated during the\n1375     search of the best split. To obtain a deterministic behaviour during\n1376     fitting, ``random_state`` has to be fixed.\n1377 \n1378     References\n1379     ----------\n1380     .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n1381 \n1382     Examples\n1383     --------\n1384     >>> from sklearn.ensemble import RandomForestClassifier\n1385     >>> from sklearn.datasets import make_classification\n1386     >>> X, y = make_classification(n_samples=1000, n_features=4,\n1387     ...                            n_informative=2, n_redundant=0,\n1388     ...                            random_state=0, shuffle=False)\n1389     >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n1390     >>> clf.fit(X, y)\n1391     RandomForestClassifier(...)\n1392     >>> print(clf.predict([[0, 0, 0, 0]]))\n1393     [1]\n1394     \"\"\"\n1395 \n1396     _parameter_constraints: dict = {\n1397         **ForestClassifier._parameter_constraints,\n1398         **DecisionTreeClassifier._parameter_constraints,\n1399         \"class_weight\": [\n1400             StrOptions({\"balanced_subsample\", \"balanced\"}),\n1401             dict,\n1402             list,\n1403             None,\n1404         ],\n1405     }\n1406     _parameter_constraints.pop(\"splitter\")\n1407 \n1408     def __init__(\n1409         self,\n1410         n_estimators=100,\n1411         *,\n1412         criterion=\"gini\",\n1413         max_depth=None,\n1414         min_samples_split=2,\n1415         min_samples_leaf=1,\n1416         min_weight_fraction_leaf=0.0,\n1417         max_features=\"sqrt\",\n1418         max_leaf_nodes=None,\n1419         min_impurity_decrease=0.0,\n1420         bootstrap=True,\n1421         oob_score=False,\n1422         n_jobs=None,\n1423         random_state=None,\n1424         verbose=0,\n1425         warm_start=False,\n1426         class_weight=None,\n1427         ccp_alpha=0.0,\n1428         max_samples=None,\n1429     ):\n1430         super().__init__(\n1431             estimator=DecisionTreeClassifier(),\n1432             n_estimators=n_estimators,\n1433             estimator_params=(\n1434                 \"criterion\",\n1435                 \"max_depth\",\n1436                 \"min_samples_split\",\n1437                 \"min_samples_leaf\",\n1438                 \"min_weight_fraction_leaf\",\n1439                 \"max_features\",\n1440                 \"max_leaf_nodes\",\n1441                 \"min_impurity_decrease\",\n1442                 \"random_state\",\n1443                 \"ccp_alpha\",\n1444             ),\n1445             bootstrap=bootstrap,\n1446             oob_score=oob_score,\n1447             n_jobs=n_jobs,\n1448             random_state=random_state,\n1449             verbose=verbose,\n1450             warm_start=warm_start,\n1451             class_weight=class_weight,\n1452             max_samples=max_samples,\n1453         )\n1454 \n1455         self.criterion = criterion\n1456         self.max_depth = max_depth\n1457         self.min_samples_split = min_samples_split\n1458         self.min_samples_leaf = min_samples_leaf\n1459         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1460         self.max_features = max_features\n1461         self.max_leaf_nodes = max_leaf_nodes\n1462         self.min_impurity_decrease = min_impurity_decrease\n1463         self.ccp_alpha = ccp_alpha\n1464 \n1465 \n1466 class RandomForestRegressor(ForestRegressor):\n1467     \"\"\"\n1468     A random forest regressor.\n1469 \n1470     A random forest is a meta estimator that fits a number of classifying\n1471     decision trees on various sub-samples of the dataset and uses averaging\n1472     to improve the predictive accuracy and control over-fitting.\n1473     The sub-sample size is controlled with the `max_samples` parameter if\n1474     `bootstrap=True` (default), otherwise the whole dataset is used to build\n1475     each tree.\n1476 \n1477     Read more in the :ref:`User Guide <forest>`.\n1478 \n1479     Parameters\n1480     ----------\n1481     n_estimators : int, default=100\n1482         The number of trees in the forest.\n1483 \n1484         .. versionchanged:: 0.22\n1485            The default value of ``n_estimators`` changed from 10 to 100\n1486            in 0.22.\n1487 \n1488     criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, \\\n1489             default=\"squared_error\"\n1490         The function to measure the quality of a split. Supported criteria\n1491         are \"squared_error\" for the mean squared error, which is equal to\n1492         variance reduction as feature selection criterion and minimizes the L2\n1493         loss using the mean of each terminal node, \"friedman_mse\", which uses\n1494         mean squared error with Friedman's improvement score for potential\n1495         splits, \"absolute_error\" for the mean absolute error, which minimizes\n1496         the L1 loss using the median of each terminal node, and \"poisson\" which\n1497         uses reduction in Poisson deviance to find splits.\n1498         Training using \"absolute_error\" is significantly slower\n1499         than when using \"squared_error\".\n1500 \n1501         .. versionadded:: 0.18\n1502            Mean Absolute Error (MAE) criterion.\n1503 \n1504         .. versionadded:: 1.0\n1505            Poisson criterion.\n1506 \n1507     max_depth : int, default=None\n1508         The maximum depth of the tree. If None, then nodes are expanded until\n1509         all leaves are pure or until all leaves contain less than\n1510         min_samples_split samples.\n1511 \n1512     min_samples_split : int or float, default=2\n1513         The minimum number of samples required to split an internal node:\n1514 \n1515         - If int, then consider `min_samples_split` as the minimum number.\n1516         - If float, then `min_samples_split` is a fraction and\n1517           `ceil(min_samples_split * n_samples)` are the minimum\n1518           number of samples for each split.\n1519 \n1520         .. versionchanged:: 0.18\n1521            Added float values for fractions.\n1522 \n1523     min_samples_leaf : int or float, default=1\n1524         The minimum number of samples required to be at a leaf node.\n1525         A split point at any depth will only be considered if it leaves at\n1526         least ``min_samples_leaf`` training samples in each of the left and\n1527         right branches.  This may have the effect of smoothing the model,\n1528         especially in regression.\n1529 \n1530         - If int, then consider `min_samples_leaf` as the minimum number.\n1531         - If float, then `min_samples_leaf` is a fraction and\n1532           `ceil(min_samples_leaf * n_samples)` are the minimum\n1533           number of samples for each node.\n1534 \n1535         .. versionchanged:: 0.18\n1536            Added float values for fractions.\n1537 \n1538     min_weight_fraction_leaf : float, default=0.0\n1539         The minimum weighted fraction of the sum total of weights (of all\n1540         the input samples) required to be at a leaf node. Samples have\n1541         equal weight when sample_weight is not provided.\n1542 \n1543     max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n1544         The number of features to consider when looking for the best split:\n1545 \n1546         - If int, then consider `max_features` features at each split.\n1547         - If float, then `max_features` is a fraction and\n1548           `max(1, int(max_features * n_features_in_))` features are considered at each\n1549           split.\n1550         - If \"auto\", then `max_features=n_features`.\n1551         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1552         - If \"log2\", then `max_features=log2(n_features)`.\n1553         - If None or 1.0, then `max_features=n_features`.\n1554 \n1555         .. note::\n1556             The default of 1.0 is equivalent to bagged trees and more\n1557             randomness can be achieved by setting smaller values, e.g. 0.3.\n1558 \n1559         .. versionchanged:: 1.1\n1560             The default of `max_features` changed from `\"auto\"` to 1.0.\n1561 \n1562         .. deprecated:: 1.1\n1563             The `\"auto\"` option was deprecated in 1.1 and will be removed\n1564             in 1.3.\n1565 \n1566         Note: the search for a split does not stop until at least one\n1567         valid partition of the node samples is found, even if it requires to\n1568         effectively inspect more than ``max_features`` features.\n1569 \n1570     max_leaf_nodes : int, default=None\n1571         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1572         Best nodes are defined as relative reduction in impurity.\n1573         If None then unlimited number of leaf nodes.\n1574 \n1575     min_impurity_decrease : float, default=0.0\n1576         A node will be split if this split induces a decrease of the impurity\n1577         greater than or equal to this value.\n1578 \n1579         The weighted impurity decrease equation is the following::\n1580 \n1581             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1582                                 - N_t_L / N_t * left_impurity)\n1583 \n1584         where ``N`` is the total number of samples, ``N_t`` is the number of\n1585         samples at the current node, ``N_t_L`` is the number of samples in the\n1586         left child, and ``N_t_R`` is the number of samples in the right child.\n1587 \n1588         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1589         if ``sample_weight`` is passed.\n1590 \n1591         .. versionadded:: 0.19\n1592 \n1593     bootstrap : bool, default=True\n1594         Whether bootstrap samples are used when building trees. If False, the\n1595         whole dataset is used to build each tree.\n1596 \n1597     oob_score : bool or callable, default=False\n1598         Whether to use out-of-bag samples to estimate the generalization score.\n1599         By default, :func:`~sklearn.metrics.r2_score` is used.\n1600         Provide a callable with signature `metric(y_true, y_pred)` to use a\n1601         custom metric. Only available if `bootstrap=True`.\n1602 \n1603     n_jobs : int, default=None\n1604         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n1605         :meth:`decision_path` and :meth:`apply` are all parallelized over the\n1606         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n1607         context. ``-1`` means using all processors. See :term:`Glossary\n1608         <n_jobs>` for more details.\n1609 \n1610     random_state : int, RandomState instance or None, default=None\n1611         Controls both the randomness of the bootstrapping of the samples used\n1612         when building trees (if ``bootstrap=True``) and the sampling of the\n1613         features to consider when looking for the best split at each node\n1614         (if ``max_features < n_features``).\n1615         See :term:`Glossary <random_state>` for details.\n1616 \n1617     verbose : int, default=0\n1618         Controls the verbosity when fitting and predicting.\n1619 \n1620     warm_start : bool, default=False\n1621         When set to ``True``, reuse the solution of the previous call to fit\n1622         and add more estimators to the ensemble, otherwise, just fit a whole\n1623         new forest. See :term:`Glossary <warm_start>` and\n1624         :ref:`gradient_boosting_warm_start` for details.\n1625 \n1626     ccp_alpha : non-negative float, default=0.0\n1627         Complexity parameter used for Minimal Cost-Complexity Pruning. The\n1628         subtree with the largest cost complexity that is smaller than\n1629         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n1630         :ref:`minimal_cost_complexity_pruning` for details.\n1631 \n1632         .. versionadded:: 0.22\n1633 \n1634     max_samples : int or float, default=None\n1635         If bootstrap is True, the number of samples to draw from X\n1636         to train each base estimator.\n1637 \n1638         - If None (default), then draw `X.shape[0]` samples.\n1639         - If int, then draw `max_samples` samples.\n1640         - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n1641           `max_samples` should be in the interval `(0.0, 1.0]`.\n1642 \n1643         .. versionadded:: 0.22\n1644 \n1645     Attributes\n1646     ----------\n1647     estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n1648         The child estimator template used to create the collection of fitted\n1649         sub-estimators.\n1650 \n1651         .. versionadded:: 1.2\n1652            `base_estimator_` was renamed to `estimator_`.\n1653 \n1654     base_estimator_ : DecisionTreeRegressor\n1655         The child estimator template used to create the collection of fitted\n1656         sub-estimators.\n1657 \n1658         .. deprecated:: 1.2\n1659             `base_estimator_` is deprecated and will be removed in 1.4.\n1660             Use `estimator_` instead.\n1661 \n1662     estimators_ : list of DecisionTreeRegressor\n1663         The collection of fitted sub-estimators.\n1664 \n1665     feature_importances_ : ndarray of shape (n_features,)\n1666         The impurity-based feature importances.\n1667         The higher, the more important the feature.\n1668         The importance of a feature is computed as the (normalized)\n1669         total reduction of the criterion brought by that feature.  It is also\n1670         known as the Gini importance.\n1671 \n1672         Warning: impurity-based feature importances can be misleading for\n1673         high cardinality features (many unique values). See\n1674         :func:`sklearn.inspection.permutation_importance` as an alternative.\n1675 \n1676     n_features_in_ : int\n1677         Number of features seen during :term:`fit`.\n1678 \n1679         .. versionadded:: 0.24\n1680 \n1681     feature_names_in_ : ndarray of shape (`n_features_in_`,)\n1682         Names of features seen during :term:`fit`. Defined only when `X`\n1683         has feature names that are all strings.\n1684 \n1685         .. versionadded:: 1.0\n1686 \n1687     n_outputs_ : int\n1688         The number of outputs when ``fit`` is performed.\n1689 \n1690     oob_score_ : float\n1691         Score of the training dataset obtained using an out-of-bag estimate.\n1692         This attribute exists only when ``oob_score`` is True.\n1693 \n1694     oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n1695         Prediction computed with out-of-bag estimate on the training set.\n1696         This attribute exists only when ``oob_score`` is True.\n1697 \n1698     See Also\n1699     --------\n1700     sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n1701     sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n1702         tree regressors.\n1703 \n1704     Notes\n1705     -----\n1706     The default values for the parameters controlling the size of the trees\n1707     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n1708     unpruned trees which can potentially be very large on some data sets. To\n1709     reduce memory consumption, the complexity and size of the trees should be\n1710     controlled by setting those parameter values.\n1711 \n1712     The features are always randomly permuted at each split. Therefore,\n1713     the best found split may vary, even with the same training data,\n1714     ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n1715     of the criterion is identical for several splits enumerated during the\n1716     search of the best split. To obtain a deterministic behaviour during\n1717     fitting, ``random_state`` has to be fixed.\n1718 \n1719     The default value ``max_features=\"auto\"`` uses ``n_features``\n1720     rather than ``n_features / 3``. The latter was originally suggested in\n1721     [1], whereas the former was more recently justified empirically in [2].\n1722 \n1723     References\n1724     ----------\n1725     .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n1726 \n1727     .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n1728            trees\", Machine Learning, 63(1), 3-42, 2006.\n1729 \n1730     Examples\n1731     --------\n1732     >>> from sklearn.ensemble import RandomForestRegressor\n1733     >>> from sklearn.datasets import make_regression\n1734     >>> X, y = make_regression(n_features=4, n_informative=2,\n1735     ...                        random_state=0, shuffle=False)\n1736     >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n1737     >>> regr.fit(X, y)\n1738     RandomForestRegressor(...)\n1739     >>> print(regr.predict([[0, 0, 0, 0]]))\n1740     [-8.32987858]\n1741     \"\"\"\n1742 \n1743     _parameter_constraints: dict = {\n1744         **ForestRegressor._parameter_constraints,\n1745         **DecisionTreeRegressor._parameter_constraints,\n1746     }\n1747     _parameter_constraints.pop(\"splitter\")\n1748 \n1749     def __init__(\n1750         self,\n1751         n_estimators=100,\n1752         *,\n1753         criterion=\"squared_error\",\n1754         max_depth=None,\n1755         min_samples_split=2,\n1756         min_samples_leaf=1,\n1757         min_weight_fraction_leaf=0.0,\n1758         max_features=1.0,\n1759         max_leaf_nodes=None,\n1760         min_impurity_decrease=0.0,\n1761         bootstrap=True,\n1762         oob_score=False,\n1763         n_jobs=None,\n1764         random_state=None,\n1765         verbose=0,\n1766         warm_start=False,\n1767         ccp_alpha=0.0,\n1768         max_samples=None,\n1769     ):\n1770         super().__init__(\n1771             estimator=DecisionTreeRegressor(),\n1772             n_estimators=n_estimators,\n1773             estimator_params=(\n1774                 \"criterion\",\n1775                 \"max_depth\",\n1776                 \"min_samples_split\",\n1777                 \"min_samples_leaf\",\n1778                 \"min_weight_fraction_leaf\",\n1779                 \"max_features\",\n1780                 \"max_leaf_nodes\",\n1781                 \"min_impurity_decrease\",\n1782                 \"random_state\",\n1783                 \"ccp_alpha\",\n1784             ),\n1785             bootstrap=bootstrap,\n1786             oob_score=oob_score,\n1787             n_jobs=n_jobs,\n1788             random_state=random_state,\n1789             verbose=verbose,\n1790             warm_start=warm_start,\n1791             max_samples=max_samples,\n1792         )\n1793 \n1794         self.criterion = criterion\n1795         self.max_depth = max_depth\n1796         self.min_samples_split = min_samples_split\n1797         self.min_samples_leaf = min_samples_leaf\n1798         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n1799         self.max_features = max_features\n1800         self.max_leaf_nodes = max_leaf_nodes\n1801         self.min_impurity_decrease = min_impurity_decrease\n1802         self.ccp_alpha = ccp_alpha\n1803 \n1804 \n1805 class ExtraTreesClassifier(ForestClassifier):\n1806     \"\"\"\n1807     An extra-trees classifier.\n1808 \n1809     This class implements a meta estimator that fits a number of\n1810     randomized decision trees (a.k.a. extra-trees) on various sub-samples\n1811     of the dataset and uses averaging to improve the predictive accuracy\n1812     and control over-fitting.\n1813 \n1814     Read more in the :ref:`User Guide <forest>`.\n1815 \n1816     Parameters\n1817     ----------\n1818     n_estimators : int, default=100\n1819         The number of trees in the forest.\n1820 \n1821         .. versionchanged:: 0.22\n1822            The default value of ``n_estimators`` changed from 10 to 100\n1823            in 0.22.\n1824 \n1825     criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n1826         The function to measure the quality of a split. Supported criteria are\n1827         \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n1828         Shannon information gain, see :ref:`tree_mathematical_formulation`.\n1829         Note: This parameter is tree-specific.\n1830 \n1831     max_depth : int, default=None\n1832         The maximum depth of the tree. If None, then nodes are expanded until\n1833         all leaves are pure or until all leaves contain less than\n1834         min_samples_split samples.\n1835 \n1836     min_samples_split : int or float, default=2\n1837         The minimum number of samples required to split an internal node:\n1838 \n1839         - If int, then consider `min_samples_split` as the minimum number.\n1840         - If float, then `min_samples_split` is a fraction and\n1841           `ceil(min_samples_split * n_samples)` are the minimum\n1842           number of samples for each split.\n1843 \n1844         .. versionchanged:: 0.18\n1845            Added float values for fractions.\n1846 \n1847     min_samples_leaf : int or float, default=1\n1848         The minimum number of samples required to be at a leaf node.\n1849         A split point at any depth will only be considered if it leaves at\n1850         least ``min_samples_leaf`` training samples in each of the left and\n1851         right branches.  This may have the effect of smoothing the model,\n1852         especially in regression.\n1853 \n1854         - If int, then consider `min_samples_leaf` as the minimum number.\n1855         - If float, then `min_samples_leaf` is a fraction and\n1856           `ceil(min_samples_leaf * n_samples)` are the minimum\n1857           number of samples for each node.\n1858 \n1859         .. versionchanged:: 0.18\n1860            Added float values for fractions.\n1861 \n1862     min_weight_fraction_leaf : float, default=0.0\n1863         The minimum weighted fraction of the sum total of weights (of all\n1864         the input samples) required to be at a leaf node. Samples have\n1865         equal weight when sample_weight is not provided.\n1866 \n1867     max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n1868         The number of features to consider when looking for the best split:\n1869 \n1870         - If int, then consider `max_features` features at each split.\n1871         - If float, then `max_features` is a fraction and\n1872           `max(1, int(max_features * n_features_in_))` features are considered at each\n1873           split.\n1874         - If \"auto\", then `max_features=sqrt(n_features)`.\n1875         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1876         - If \"log2\", then `max_features=log2(n_features)`.\n1877         - If None, then `max_features=n_features`.\n1878 \n1879         .. versionchanged:: 1.1\n1880             The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n1881 \n1882         .. deprecated:: 1.1\n1883             The `\"auto\"` option was deprecated in 1.1 and will be removed\n1884             in 1.3.\n1885 \n1886         Note: the search for a split does not stop until at least one\n1887         valid partition of the node samples is found, even if it requires to\n1888         effectively inspect more than ``max_features`` features.\n1889 \n1890     max_leaf_nodes : int, default=None\n1891         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1892         Best nodes are defined as relative reduction in impurity.\n1893         If None then unlimited number of leaf nodes.\n1894 \n1895     min_impurity_decrease : float, default=0.0\n1896         A node will be split if this split induces a decrease of the impurity\n1897         greater than or equal to this value.\n1898 \n1899         The weighted impurity decrease equation is the following::\n1900 \n1901             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1902                                 - N_t_L / N_t * left_impurity)\n1903 \n1904         where ``N`` is the total number of samples, ``N_t`` is the number of\n1905         samples at the current node, ``N_t_L`` is the number of samples in the\n1906         left child, and ``N_t_R`` is the number of samples in the right child.\n1907 \n1908         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1909         if ``sample_weight`` is passed.\n1910 \n1911         .. versionadded:: 0.19\n1912 \n1913     bootstrap : bool, default=False\n1914         Whether bootstrap samples are used when building trees. If False, the\n1915         whole dataset is used to build each tree.\n1916 \n1917     oob_score : bool or callable, default=False\n1918         Whether to use out-of-bag samples to estimate the generalization score.\n1919         By default, :func:`~sklearn.metrics.accuracy_score` is used.\n1920         Provide a callable with signature `metric(y_true, y_pred)` to use a\n1921         custom metric. Only available if `bootstrap=True`.\n1922 \n1923     n_jobs : int, default=None\n1924         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n1925         :meth:`decision_path` and :meth:`apply` are all parallelized over the\n1926         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n1927         context. ``-1`` means using all processors. See :term:`Glossary\n1928         <n_jobs>` for more details.\n1929 \n1930     random_state : int, RandomState instance or None, default=None\n1931         Controls 3 sources of randomness:\n1932 \n1933         - the bootstrapping of the samples used when building trees\n1934           (if ``bootstrap=True``)\n1935         - the sampling of the features to consider when looking for the best\n1936           split at each node (if ``max_features < n_features``)\n1937         - the draw of the splits for each of the `max_features`\n1938 \n1939         See :term:`Glossary <random_state>` for details.\n1940 \n1941     verbose : int, default=0\n1942         Controls the verbosity when fitting and predicting.\n1943 \n1944     warm_start : bool, default=False\n1945         When set to ``True``, reuse the solution of the previous call to fit\n1946         and add more estimators to the ensemble, otherwise, just fit a whole\n1947         new forest. See :term:`Glossary <warm_start>` and\n1948         :ref:`gradient_boosting_warm_start` for details.\n1949 \n1950     class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\\n1951             default=None\n1952         Weights associated with classes in the form ``{class_label: weight}``.\n1953         If not given, all classes are supposed to have weight one. For\n1954         multi-output problems, a list of dicts can be provided in the same\n1955         order as the columns of y.\n1956 \n1957         Note that for multioutput (including multilabel) weights should be\n1958         defined for each class of every column in its own dict. For example,\n1959         for four-class multilabel classification weights should be\n1960         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n1961         [{1:1}, {2:5}, {3:1}, {4:1}].\n1962 \n1963         The \"balanced\" mode uses the values of y to automatically adjust\n1964         weights inversely proportional to class frequencies in the input data\n1965         as ``n_samples / (n_classes * np.bincount(y))``\n1966 \n1967         The \"balanced_subsample\" mode is the same as \"balanced\" except that\n1968         weights are computed based on the bootstrap sample for every tree\n1969         grown.\n1970 \n1971         For multi-output, the weights of each column of y will be multiplied.\n1972 \n1973         Note that these weights will be multiplied with sample_weight (passed\n1974         through the fit method) if sample_weight is specified.\n1975 \n1976     ccp_alpha : non-negative float, default=0.0\n1977         Complexity parameter used for Minimal Cost-Complexity Pruning. The\n1978         subtree with the largest cost complexity that is smaller than\n1979         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n1980         :ref:`minimal_cost_complexity_pruning` for details.\n1981 \n1982         .. versionadded:: 0.22\n1983 \n1984     max_samples : int or float, default=None\n1985         If bootstrap is True, the number of samples to draw from X\n1986         to train each base estimator.\n1987 \n1988         - If None (default), then draw `X.shape[0]` samples.\n1989         - If int, then draw `max_samples` samples.\n1990         - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n1991           `max_samples` should be in the interval `(0.0, 1.0]`.\n1992 \n1993         .. versionadded:: 0.22\n1994 \n1995     Attributes\n1996     ----------\n1997     estimator_ : :class:`~sklearn.tree.ExtraTreesClassifier`\n1998         The child estimator template used to create the collection of fitted\n1999         sub-estimators.\n2000 \n2001         .. versionadded:: 1.2\n2002            `base_estimator_` was renamed to `estimator_`.\n2003 \n2004     base_estimator_ : ExtraTreesClassifier\n2005         The child estimator template used to create the collection of fitted\n2006         sub-estimators.\n2007 \n2008         .. deprecated:: 1.2\n2009             `base_estimator_` is deprecated and will be removed in 1.4.\n2010             Use `estimator_` instead.\n2011 \n2012     estimators_ : list of DecisionTreeClassifier\n2013         The collection of fitted sub-estimators.\n2014 \n2015     classes_ : ndarray of shape (n_classes,) or a list of such arrays\n2016         The classes labels (single output problem), or a list of arrays of\n2017         class labels (multi-output problem).\n2018 \n2019     n_classes_ : int or list\n2020         The number of classes (single output problem), or a list containing the\n2021         number of classes for each output (multi-output problem).\n2022 \n2023     feature_importances_ : ndarray of shape (n_features,)\n2024         The impurity-based feature importances.\n2025         The higher, the more important the feature.\n2026         The importance of a feature is computed as the (normalized)\n2027         total reduction of the criterion brought by that feature.  It is also\n2028         known as the Gini importance.\n2029 \n2030         Warning: impurity-based feature importances can be misleading for\n2031         high cardinality features (many unique values). See\n2032         :func:`sklearn.inspection.permutation_importance` as an alternative.\n2033 \n2034     n_features_in_ : int\n2035         Number of features seen during :term:`fit`.\n2036 \n2037         .. versionadded:: 0.24\n2038 \n2039     feature_names_in_ : ndarray of shape (`n_features_in_`,)\n2040         Names of features seen during :term:`fit`. Defined only when `X`\n2041         has feature names that are all strings.\n2042 \n2043         .. versionadded:: 1.0\n2044 \n2045     n_outputs_ : int\n2046         The number of outputs when ``fit`` is performed.\n2047 \n2048     oob_score_ : float\n2049         Score of the training dataset obtained using an out-of-bag estimate.\n2050         This attribute exists only when ``oob_score`` is True.\n2051 \n2052     oob_decision_function_ : ndarray of shape (n_samples, n_classes) or \\\n2053             (n_samples, n_classes, n_outputs)\n2054         Decision function computed with out-of-bag estimate on the training\n2055         set. If n_estimators is small it might be possible that a data point\n2056         was never left out during the bootstrap. In this case,\n2057         `oob_decision_function_` might contain NaN. This attribute exists\n2058         only when ``oob_score`` is True.\n2059 \n2060     See Also\n2061     --------\n2062     ExtraTreesRegressor : An extra-trees regressor with random splits.\n2063     RandomForestClassifier : A random forest classifier with optimal splits.\n2064     RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n2065 \n2066     Notes\n2067     -----\n2068     The default values for the parameters controlling the size of the trees\n2069     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n2070     unpruned trees which can potentially be very large on some data sets. To\n2071     reduce memory consumption, the complexity and size of the trees should be\n2072     controlled by setting those parameter values.\n2073 \n2074     References\n2075     ----------\n2076     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n2077            trees\", Machine Learning, 63(1), 3-42, 2006.\n2078 \n2079     Examples\n2080     --------\n2081     >>> from sklearn.ensemble import ExtraTreesClassifier\n2082     >>> from sklearn.datasets import make_classification\n2083     >>> X, y = make_classification(n_features=4, random_state=0)\n2084     >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n2085     >>> clf.fit(X, y)\n2086     ExtraTreesClassifier(random_state=0)\n2087     >>> clf.predict([[0, 0, 0, 0]])\n2088     array([1])\n2089     \"\"\"\n2090 \n2091     _parameter_constraints: dict = {\n2092         **ForestClassifier._parameter_constraints,\n2093         **DecisionTreeClassifier._parameter_constraints,\n2094         \"class_weight\": [\n2095             StrOptions({\"balanced_subsample\", \"balanced\"}),\n2096             dict,\n2097             list,\n2098             None,\n2099         ],\n2100     }\n2101     _parameter_constraints.pop(\"splitter\")\n2102 \n2103     def __init__(\n2104         self,\n2105         n_estimators=100,\n2106         *,\n2107         criterion=\"gini\",\n2108         max_depth=None,\n2109         min_samples_split=2,\n2110         min_samples_leaf=1,\n2111         min_weight_fraction_leaf=0.0,\n2112         max_features=\"sqrt\",\n2113         max_leaf_nodes=None,\n2114         min_impurity_decrease=0.0,\n2115         bootstrap=False,\n2116         oob_score=False,\n2117         n_jobs=None,\n2118         random_state=None,\n2119         verbose=0,\n2120         warm_start=False,\n2121         class_weight=None,\n2122         ccp_alpha=0.0,\n2123         max_samples=None,\n2124     ):\n2125         super().__init__(\n2126             estimator=ExtraTreeClassifier(),\n2127             n_estimators=n_estimators,\n2128             estimator_params=(\n2129                 \"criterion\",\n2130                 \"max_depth\",\n2131                 \"min_samples_split\",\n2132                 \"min_samples_leaf\",\n2133                 \"min_weight_fraction_leaf\",\n2134                 \"max_features\",\n2135                 \"max_leaf_nodes\",\n2136                 \"min_impurity_decrease\",\n2137                 \"random_state\",\n2138                 \"ccp_alpha\",\n2139             ),\n2140             bootstrap=bootstrap,\n2141             oob_score=oob_score,\n2142             n_jobs=n_jobs,\n2143             random_state=random_state,\n2144             verbose=verbose,\n2145             warm_start=warm_start,\n2146             class_weight=class_weight,\n2147             max_samples=max_samples,\n2148         )\n2149 \n2150         self.criterion = criterion\n2151         self.max_depth = max_depth\n2152         self.min_samples_split = min_samples_split\n2153         self.min_samples_leaf = min_samples_leaf\n2154         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n2155         self.max_features = max_features\n2156         self.max_leaf_nodes = max_leaf_nodes\n2157         self.min_impurity_decrease = min_impurity_decrease\n2158         self.ccp_alpha = ccp_alpha\n2159 \n2160 \n2161 class ExtraTreesRegressor(ForestRegressor):\n2162     \"\"\"\n2163     An extra-trees regressor.\n2164 \n2165     This class implements a meta estimator that fits a number of\n2166     randomized decision trees (a.k.a. extra-trees) on various sub-samples\n2167     of the dataset and uses averaging to improve the predictive accuracy\n2168     and control over-fitting.\n2169 \n2170     Read more in the :ref:`User Guide <forest>`.\n2171 \n2172     Parameters\n2173     ----------\n2174     n_estimators : int, default=100\n2175         The number of trees in the forest.\n2176 \n2177         .. versionchanged:: 0.22\n2178            The default value of ``n_estimators`` changed from 10 to 100\n2179            in 0.22.\n2180 \n2181     criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"}, \\\n2182             default=\"squared_error\"\n2183         The function to measure the quality of a split. Supported criteria\n2184         are \"squared_error\" for the mean squared error, which is equal to\n2185         variance reduction as feature selection criterion and minimizes the L2\n2186         loss using the mean of each terminal node, \"friedman_mse\", which uses\n2187         mean squared error with Friedman's improvement score for potential\n2188         splits, \"absolute_error\" for the mean absolute error, which minimizes\n2189         the L1 loss using the median of each terminal node, and \"poisson\" which\n2190         uses reduction in Poisson deviance to find splits.\n2191         Training using \"absolute_error\" is significantly slower\n2192         than when using \"squared_error\".\n2193 \n2194         .. versionadded:: 0.18\n2195            Mean Absolute Error (MAE) criterion.\n2196 \n2197     max_depth : int, default=None\n2198         The maximum depth of the tree. If None, then nodes are expanded until\n2199         all leaves are pure or until all leaves contain less than\n2200         min_samples_split samples.\n2201 \n2202     min_samples_split : int or float, default=2\n2203         The minimum number of samples required to split an internal node:\n2204 \n2205         - If int, then consider `min_samples_split` as the minimum number.\n2206         - If float, then `min_samples_split` is a fraction and\n2207           `ceil(min_samples_split * n_samples)` are the minimum\n2208           number of samples for each split.\n2209 \n2210         .. versionchanged:: 0.18\n2211            Added float values for fractions.\n2212 \n2213     min_samples_leaf : int or float, default=1\n2214         The minimum number of samples required to be at a leaf node.\n2215         A split point at any depth will only be considered if it leaves at\n2216         least ``min_samples_leaf`` training samples in each of the left and\n2217         right branches.  This may have the effect of smoothing the model,\n2218         especially in regression.\n2219 \n2220         - If int, then consider `min_samples_leaf` as the minimum number.\n2221         - If float, then `min_samples_leaf` is a fraction and\n2222           `ceil(min_samples_leaf * n_samples)` are the minimum\n2223           number of samples for each node.\n2224 \n2225         .. versionchanged:: 0.18\n2226            Added float values for fractions.\n2227 \n2228     min_weight_fraction_leaf : float, default=0.0\n2229         The minimum weighted fraction of the sum total of weights (of all\n2230         the input samples) required to be at a leaf node. Samples have\n2231         equal weight when sample_weight is not provided.\n2232 \n2233     max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n2234         The number of features to consider when looking for the best split:\n2235 \n2236         - If int, then consider `max_features` features at each split.\n2237         - If float, then `max_features` is a fraction and\n2238           `max(1, int(max_features * n_features_in_))` features are considered at each\n2239           split.\n2240         - If \"auto\", then `max_features=n_features`.\n2241         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n2242         - If \"log2\", then `max_features=log2(n_features)`.\n2243         - If None or 1.0, then `max_features=n_features`.\n2244 \n2245         .. note::\n2246             The default of 1.0 is equivalent to bagged trees and more\n2247             randomness can be achieved by setting smaller values, e.g. 0.3.\n2248 \n2249         .. versionchanged:: 1.1\n2250             The default of `max_features` changed from `\"auto\"` to 1.0.\n2251 \n2252         .. deprecated:: 1.1\n2253             The `\"auto\"` option was deprecated in 1.1 and will be removed\n2254             in 1.3.\n2255 \n2256         Note: the search for a split does not stop until at least one\n2257         valid partition of the node samples is found, even if it requires to\n2258         effectively inspect more than ``max_features`` features.\n2259 \n2260     max_leaf_nodes : int, default=None\n2261         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2262         Best nodes are defined as relative reduction in impurity.\n2263         If None then unlimited number of leaf nodes.\n2264 \n2265     min_impurity_decrease : float, default=0.0\n2266         A node will be split if this split induces a decrease of the impurity\n2267         greater than or equal to this value.\n2268 \n2269         The weighted impurity decrease equation is the following::\n2270 \n2271             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2272                                 - N_t_L / N_t * left_impurity)\n2273 \n2274         where ``N`` is the total number of samples, ``N_t`` is the number of\n2275         samples at the current node, ``N_t_L`` is the number of samples in the\n2276         left child, and ``N_t_R`` is the number of samples in the right child.\n2277 \n2278         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2279         if ``sample_weight`` is passed.\n2280 \n2281         .. versionadded:: 0.19\n2282 \n2283     bootstrap : bool, default=False\n2284         Whether bootstrap samples are used when building trees. If False, the\n2285         whole dataset is used to build each tree.\n2286 \n2287     oob_score : bool or callable, default=False\n2288         Whether to use out-of-bag samples to estimate the generalization score.\n2289         By default, :func:`~sklearn.metrics.r2_score` is used.\n2290         Provide a callable with signature `metric(y_true, y_pred)` to use a\n2291         custom metric. Only available if `bootstrap=True`.\n2292 \n2293     n_jobs : int, default=None\n2294         The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n2295         :meth:`decision_path` and :meth:`apply` are all parallelized over the\n2296         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n2297         context. ``-1`` means using all processors. See :term:`Glossary\n2298         <n_jobs>` for more details.\n2299 \n2300     random_state : int, RandomState instance or None, default=None\n2301         Controls 3 sources of randomness:\n2302 \n2303         - the bootstrapping of the samples used when building trees\n2304           (if ``bootstrap=True``)\n2305         - the sampling of the features to consider when looking for the best\n2306           split at each node (if ``max_features < n_features``)\n2307         - the draw of the splits for each of the `max_features`\n2308 \n2309         See :term:`Glossary <random_state>` for details.\n2310 \n2311     verbose : int, default=0\n2312         Controls the verbosity when fitting and predicting.\n2313 \n2314     warm_start : bool, default=False\n2315         When set to ``True``, reuse the solution of the previous call to fit\n2316         and add more estimators to the ensemble, otherwise, just fit a whole\n2317         new forest. See :term:`Glossary <warm_start>` and\n2318         :ref:`gradient_boosting_warm_start` for details.\n2319 \n2320     ccp_alpha : non-negative float, default=0.0\n2321         Complexity parameter used for Minimal Cost-Complexity Pruning. The\n2322         subtree with the largest cost complexity that is smaller than\n2323         ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n2324         :ref:`minimal_cost_complexity_pruning` for details.\n2325 \n2326         .. versionadded:: 0.22\n2327 \n2328     max_samples : int or float, default=None\n2329         If bootstrap is True, the number of samples to draw from X\n2330         to train each base estimator.\n2331 \n2332         - If None (default), then draw `X.shape[0]` samples.\n2333         - If int, then draw `max_samples` samples.\n2334         - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n2335           `max_samples` should be in the interval `(0.0, 1.0]`.\n2336 \n2337         .. versionadded:: 0.22\n2338 \n2339     Attributes\n2340     ----------\n2341     estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`\n2342         The child estimator template used to create the collection of fitted\n2343         sub-estimators.\n2344 \n2345         .. versionadded:: 1.2\n2346            `base_estimator_` was renamed to `estimator_`.\n2347 \n2348     base_estimator_ : ExtraTreeRegressor\n2349         The child estimator template used to create the collection of fitted\n2350         sub-estimators.\n2351 \n2352         .. deprecated:: 1.2\n2353             `base_estimator_` is deprecated and will be removed in 1.4.\n2354             Use `estimator_` instead.\n2355 \n2356     estimators_ : list of DecisionTreeRegressor\n2357         The collection of fitted sub-estimators.\n2358 \n2359     feature_importances_ : ndarray of shape (n_features,)\n2360         The impurity-based feature importances.\n2361         The higher, the more important the feature.\n2362         The importance of a feature is computed as the (normalized)\n2363         total reduction of the criterion brought by that feature.  It is also\n2364         known as the Gini importance.\n2365 \n2366         Warning: impurity-based feature importances can be misleading for\n2367         high cardinality features (many unique values). See\n2368         :func:`sklearn.inspection.permutation_importance` as an alternative.\n2369 \n2370     n_features_in_ : int\n2371         Number of features seen during :term:`fit`.\n2372 \n2373         .. versionadded:: 0.24\n2374 \n2375     feature_names_in_ : ndarray of shape (`n_features_in_`,)\n2376         Names of features seen during :term:`fit`. Defined only when `X`\n2377         has feature names that are all strings.\n2378 \n2379         .. versionadded:: 1.0\n2380 \n2381     n_outputs_ : int\n2382         The number of outputs.\n2383 \n2384     oob_score_ : float\n2385         Score of the training dataset obtained using an out-of-bag estimate.\n2386         This attribute exists only when ``oob_score`` is True.\n2387 \n2388     oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n2389         Prediction computed with out-of-bag estimate on the training set.\n2390         This attribute exists only when ``oob_score`` is True.\n2391 \n2392     See Also\n2393     --------\n2394     ExtraTreesClassifier : An extra-trees classifier with random splits.\n2395     RandomForestClassifier : A random forest classifier with optimal splits.\n2396     RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n2397 \n2398     Notes\n2399     -----\n2400     The default values for the parameters controlling the size of the trees\n2401     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n2402     unpruned trees which can potentially be very large on some data sets. To\n2403     reduce memory consumption, the complexity and size of the trees should be\n2404     controlled by setting those parameter values.\n2405 \n2406     References\n2407     ----------\n2408     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n2409            Machine Learning, 63(1), 3-42, 2006.\n2410 \n2411     Examples\n2412     --------\n2413     >>> from sklearn.datasets import load_diabetes\n2414     >>> from sklearn.model_selection import train_test_split\n2415     >>> from sklearn.ensemble import ExtraTreesRegressor\n2416     >>> X, y = load_diabetes(return_X_y=True)\n2417     >>> X_train, X_test, y_train, y_test = train_test_split(\n2418     ...     X, y, random_state=0)\n2419     >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n2420     ...    X_train, y_train)\n2421     >>> reg.score(X_test, y_test)\n2422     0.2727...\n2423     \"\"\"\n2424 \n2425     _parameter_constraints: dict = {\n2426         **ForestRegressor._parameter_constraints,\n2427         **DecisionTreeRegressor._parameter_constraints,\n2428     }\n2429     _parameter_constraints.pop(\"splitter\")\n2430 \n2431     def __init__(\n2432         self,\n2433         n_estimators=100,\n2434         *,\n2435         criterion=\"squared_error\",\n2436         max_depth=None,\n2437         min_samples_split=2,\n2438         min_samples_leaf=1,\n2439         min_weight_fraction_leaf=0.0,\n2440         max_features=1.0,\n2441         max_leaf_nodes=None,\n2442         min_impurity_decrease=0.0,\n2443         bootstrap=False,\n2444         oob_score=False,\n2445         n_jobs=None,\n2446         random_state=None,\n2447         verbose=0,\n2448         warm_start=False,\n2449         ccp_alpha=0.0,\n2450         max_samples=None,\n2451     ):\n2452         super().__init__(\n2453             estimator=ExtraTreeRegressor(),\n2454             n_estimators=n_estimators,\n2455             estimator_params=(\n2456                 \"criterion\",\n2457                 \"max_depth\",\n2458                 \"min_samples_split\",\n2459                 \"min_samples_leaf\",\n2460                 \"min_weight_fraction_leaf\",\n2461                 \"max_features\",\n2462                 \"max_leaf_nodes\",\n2463                 \"min_impurity_decrease\",\n2464                 \"random_state\",\n2465                 \"ccp_alpha\",\n2466             ),\n2467             bootstrap=bootstrap,\n2468             oob_score=oob_score,\n2469             n_jobs=n_jobs,\n2470             random_state=random_state,\n2471             verbose=verbose,\n2472             warm_start=warm_start,\n2473             max_samples=max_samples,\n2474         )\n2475 \n2476         self.criterion = criterion\n2477         self.max_depth = max_depth\n2478         self.min_samples_split = min_samples_split\n2479         self.min_samples_leaf = min_samples_leaf\n2480         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n2481         self.max_features = max_features\n2482         self.max_leaf_nodes = max_leaf_nodes\n2483         self.min_impurity_decrease = min_impurity_decrease\n2484         self.ccp_alpha = ccp_alpha\n2485 \n2486 \n2487 class RandomTreesEmbedding(TransformerMixin, BaseForest):\n2488     \"\"\"\n2489     An ensemble of totally random trees.\n2490 \n2491     An unsupervised transformation of a dataset to a high-dimensional\n2492     sparse representation. A datapoint is coded according to which leaf of\n2493     each tree it is sorted into. Using a one-hot encoding of the leaves,\n2494     this leads to a binary coding with as many ones as there are trees in\n2495     the forest.\n2496 \n2497     The dimensionality of the resulting representation is\n2498     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n2499     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n2500 \n2501     Read more in the :ref:`User Guide <random_trees_embedding>`.\n2502 \n2503     Parameters\n2504     ----------\n2505     n_estimators : int, default=100\n2506         Number of trees in the forest.\n2507 \n2508         .. versionchanged:: 0.22\n2509            The default value of ``n_estimators`` changed from 10 to 100\n2510            in 0.22.\n2511 \n2512     max_depth : int, default=5\n2513         The maximum depth of each tree. If None, then nodes are expanded until\n2514         all leaves are pure or until all leaves contain less than\n2515         min_samples_split samples.\n2516 \n2517     min_samples_split : int or float, default=2\n2518         The minimum number of samples required to split an internal node:\n2519 \n2520         - If int, then consider `min_samples_split` as the minimum number.\n2521         - If float, then `min_samples_split` is a fraction and\n2522           `ceil(min_samples_split * n_samples)` is the minimum\n2523           number of samples for each split.\n2524 \n2525         .. versionchanged:: 0.18\n2526            Added float values for fractions.\n2527 \n2528     min_samples_leaf : int or float, default=1\n2529         The minimum number of samples required to be at a leaf node.\n2530         A split point at any depth will only be considered if it leaves at\n2531         least ``min_samples_leaf`` training samples in each of the left and\n2532         right branches.  This may have the effect of smoothing the model,\n2533         especially in regression.\n2534 \n2535         - If int, then consider `min_samples_leaf` as the minimum number.\n2536         - If float, then `min_samples_leaf` is a fraction and\n2537           `ceil(min_samples_leaf * n_samples)` is the minimum\n2538           number of samples for each node.\n2539 \n2540         .. versionchanged:: 0.18\n2541            Added float values for fractions.\n2542 \n2543     min_weight_fraction_leaf : float, default=0.0\n2544         The minimum weighted fraction of the sum total of weights (of all\n2545         the input samples) required to be at a leaf node. Samples have\n2546         equal weight when sample_weight is not provided.\n2547 \n2548     max_leaf_nodes : int, default=None\n2549         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2550         Best nodes are defined as relative reduction in impurity.\n2551         If None then unlimited number of leaf nodes.\n2552 \n2553     min_impurity_decrease : float, default=0.0\n2554         A node will be split if this split induces a decrease of the impurity\n2555         greater than or equal to this value.\n2556 \n2557         The weighted impurity decrease equation is the following::\n2558 \n2559             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2560                                 - N_t_L / N_t * left_impurity)\n2561 \n2562         where ``N`` is the total number of samples, ``N_t`` is the number of\n2563         samples at the current node, ``N_t_L`` is the number of samples in the\n2564         left child, and ``N_t_R`` is the number of samples in the right child.\n2565 \n2566         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2567         if ``sample_weight`` is passed.\n2568 \n2569         .. versionadded:: 0.19\n2570 \n2571     sparse_output : bool, default=True\n2572         Whether or not to return a sparse CSR matrix, as default behavior,\n2573         or to return a dense array compatible with dense pipeline operators.\n2574 \n2575     n_jobs : int, default=None\n2576         The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n2577         :meth:`decision_path` and :meth:`apply` are all parallelized over the\n2578         trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n2579         context. ``-1`` means using all processors. See :term:`Glossary\n2580         <n_jobs>` for more details.\n2581 \n2582     random_state : int, RandomState instance or None, default=None\n2583         Controls the generation of the random `y` used to fit the trees\n2584         and the draw of the splits for each feature at the trees' nodes.\n2585         See :term:`Glossary <random_state>` for details.\n2586 \n2587     verbose : int, default=0\n2588         Controls the verbosity when fitting and predicting.\n2589 \n2590     warm_start : bool, default=False\n2591         When set to ``True``, reuse the solution of the previous call to fit\n2592         and add more estimators to the ensemble, otherwise, just fit a whole\n2593         new forest. See :term:`Glossary <warm_start>` and\n2594         :ref:`gradient_boosting_warm_start` for details.\n2595 \n2596     Attributes\n2597     ----------\n2598     estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n2599         The child estimator template used to create the collection of fitted\n2600         sub-estimators.\n2601 \n2602         .. versionadded:: 1.2\n2603            `base_estimator_` was renamed to `estimator_`.\n2604 \n2605     base_estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n2606         The child estimator template used to create the collection of fitted\n2607         sub-estimators.\n2608 \n2609         .. deprecated:: 1.2\n2610             `base_estimator_` is deprecated and will be removed in 1.4.\n2611             Use `estimator_` instead.\n2612 \n2613     estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances\n2614         The collection of fitted sub-estimators.\n2615 \n2616     feature_importances_ : ndarray of shape (n_features,)\n2617         The feature importances (the higher, the more important the feature).\n2618 \n2619     n_features_in_ : int\n2620         Number of features seen during :term:`fit`.\n2621 \n2622         .. versionadded:: 0.24\n2623 \n2624     feature_names_in_ : ndarray of shape (`n_features_in_`,)\n2625         Names of features seen during :term:`fit`. Defined only when `X`\n2626         has feature names that are all strings.\n2627 \n2628         .. versionadded:: 1.0\n2629 \n2630     n_outputs_ : int\n2631         The number of outputs when ``fit`` is performed.\n2632 \n2633     one_hot_encoder_ : OneHotEncoder instance\n2634         One-hot encoder used to create the sparse embedding.\n2635 \n2636     See Also\n2637     --------\n2638     ExtraTreesClassifier : An extra-trees classifier.\n2639     ExtraTreesRegressor : An extra-trees regressor.\n2640     RandomForestClassifier : A random forest classifier.\n2641     RandomForestRegressor : A random forest regressor.\n2642     sklearn.tree.ExtraTreeClassifier: An extremely randomized\n2643         tree classifier.\n2644     sklearn.tree.ExtraTreeRegressor : An extremely randomized\n2645         tree regressor.\n2646 \n2647     References\n2648     ----------\n2649     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n2650            Machine Learning, 63(1), 3-42, 2006.\n2651     .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n2652            visual codebooks using randomized clustering forests\"\n2653            NIPS 2007\n2654 \n2655     Examples\n2656     --------\n2657     >>> from sklearn.ensemble import RandomTreesEmbedding\n2658     >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n2659     >>> random_trees = RandomTreesEmbedding(\n2660     ...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n2661     >>> X_sparse_embedding = random_trees.transform(X)\n2662     >>> X_sparse_embedding.toarray()\n2663     array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n2664            [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n2665            [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n2666            [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n2667            [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])\n2668     \"\"\"\n2669 \n2670     _parameter_constraints: dict = {\n2671         \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n2672         \"n_jobs\": [Integral, None],\n2673         \"verbose\": [\"verbose\"],\n2674         \"warm_start\": [\"boolean\"],\n2675         **BaseDecisionTree._parameter_constraints,\n2676         \"sparse_output\": [\"boolean\"],\n2677     }\n2678     for param in (\"max_features\", \"ccp_alpha\", \"splitter\"):\n2679         _parameter_constraints.pop(param)\n2680 \n2681     criterion = \"squared_error\"\n2682     max_features = 1\n2683 \n2684     def __init__(\n2685         self,\n2686         n_estimators=100,\n2687         *,\n2688         max_depth=5,\n2689         min_samples_split=2,\n2690         min_samples_leaf=1,\n2691         min_weight_fraction_leaf=0.0,\n2692         max_leaf_nodes=None,\n2693         min_impurity_decrease=0.0,\n2694         sparse_output=True,\n2695         n_jobs=None,\n2696         random_state=None,\n2697         verbose=0,\n2698         warm_start=False,\n2699     ):\n2700         super().__init__(\n2701             estimator=ExtraTreeRegressor(),\n2702             n_estimators=n_estimators,\n2703             estimator_params=(\n2704                 \"criterion\",\n2705                 \"max_depth\",\n2706                 \"min_samples_split\",\n2707                 \"min_samples_leaf\",\n2708                 \"min_weight_fraction_leaf\",\n2709                 \"max_features\",\n2710                 \"max_leaf_nodes\",\n2711                 \"min_impurity_decrease\",\n2712                 \"random_state\",\n2713             ),\n2714             bootstrap=False,\n2715             oob_score=False,\n2716             n_jobs=n_jobs,\n2717             random_state=random_state,\n2718             verbose=verbose,\n2719             warm_start=warm_start,\n2720             max_samples=None,\n2721         )\n2722 \n2723         self.max_depth = max_depth\n2724         self.min_samples_split = min_samples_split\n2725         self.min_samples_leaf = min_samples_leaf\n2726         self.min_weight_fraction_leaf = min_weight_fraction_leaf\n2727         self.max_leaf_nodes = max_leaf_nodes\n2728         self.min_impurity_decrease = min_impurity_decrease\n2729         self.sparse_output = sparse_output\n2730 \n2731     def _set_oob_score_and_attributes(self, X, y, scoring_function=None):\n2732         raise NotImplementedError(\"OOB score not supported by tree embedding\")\n2733 \n2734     def fit(self, X, y=None, sample_weight=None):\n2735         \"\"\"\n2736         Fit estimator.\n2737 \n2738         Parameters\n2739         ----------\n2740         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n2741             The input samples. Use ``dtype=np.float32`` for maximum\n2742             efficiency. Sparse matrices are also supported, use sparse\n2743             ``csc_matrix`` for maximum efficiency.\n2744 \n2745         y : Ignored\n2746             Not used, present for API consistency by convention.\n2747 \n2748         sample_weight : array-like of shape (n_samples,), default=None\n2749             Sample weights. If None, then samples are equally weighted. Splits\n2750             that would create child nodes with net zero or negative weight are\n2751             ignored while searching for a split in each node. In the case of\n2752             classification, splits are also ignored if they would result in any\n2753             single class carrying a negative weight in either child node.\n2754 \n2755         Returns\n2756         -------\n2757         self : object\n2758             Returns the instance itself.\n2759         \"\"\"\n2760         # Parameters are validated in fit_transform\n2761         self.fit_transform(X, y, sample_weight=sample_weight)\n2762         return self\n2763 \n2764     def fit_transform(self, X, y=None, sample_weight=None):\n2765         \"\"\"\n2766         Fit estimator and transform dataset.\n2767 \n2768         Parameters\n2769         ----------\n2770         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n2771             Input data used to build forests. Use ``dtype=np.float32`` for\n2772             maximum efficiency.\n2773 \n2774         y : Ignored\n2775             Not used, present for API consistency by convention.\n2776 \n2777         sample_weight : array-like of shape (n_samples,), default=None\n2778             Sample weights. If None, then samples are equally weighted. Splits\n2779             that would create child nodes with net zero or negative weight are\n2780             ignored while searching for a split in each node. In the case of\n2781             classification, splits are also ignored if they would result in any\n2782             single class carrying a negative weight in either child node.\n2783 \n2784         Returns\n2785         -------\n2786         X_transformed : sparse matrix of shape (n_samples, n_out)\n2787             Transformed dataset.\n2788         \"\"\"\n2789         self._validate_params()\n2790 \n2791         rnd = check_random_state(self.random_state)\n2792         y = rnd.uniform(size=_num_samples(X))\n2793         super().fit(X, y, sample_weight=sample_weight)\n2794 \n2795         self.one_hot_encoder_ = OneHotEncoder(sparse_output=self.sparse_output)\n2796         output = self.one_hot_encoder_.fit_transform(self.apply(X))\n2797         self._n_features_out = output.shape[1]\n2798         return output\n2799 \n2800     def get_feature_names_out(self, input_features=None):\n2801         \"\"\"Get output feature names for transformation.\n2802 \n2803         Parameters\n2804         ----------\n2805         input_features : array-like of str or None, default=None\n2806             Only used to validate feature names with the names seen in :meth:`fit`.\n2807 \n2808         Returns\n2809         -------\n2810         feature_names_out : ndarray of str objects\n2811             Transformed feature names, in the format of\n2812             `randomtreesembedding_{tree}_{leaf}`, where `tree` is the tree used\n2813             to generate the leaf and `leaf` is the index of a leaf node\n2814             in that tree. Note that the node indexing scheme is used to\n2815             index both nodes with children (split nodes) and leaf nodes.\n2816             Only the latter can be present as output features.\n2817             As a consequence, there are missing indices in the output\n2818             feature names.\n2819         \"\"\"\n2820         check_is_fitted(self, \"_n_features_out\")\n2821         _check_feature_names_in(\n2822             self, input_features=input_features, generate_names=False\n2823         )\n2824 \n2825         feature_names = [\n2826             f\"randomtreesembedding_{tree}_{leaf}\"\n2827             for tree in range(self.n_estimators)\n2828             for leaf in self.one_hot_encoder_.categories_[tree]\n2829         ]\n2830         return np.asarray(feature_names, dtype=object)\n2831 \n2832     def transform(self, X):\n2833         \"\"\"\n2834         Transform dataset.\n2835 \n2836         Parameters\n2837         ----------\n2838         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n2839             Input data to be transformed. Use ``dtype=np.float32`` for maximum\n2840             efficiency. Sparse matrices are also supported, use sparse\n2841             ``csr_matrix`` for maximum efficiency.\n2842 \n2843         Returns\n2844         -------\n2845         X_transformed : sparse matrix of shape (n_samples, n_out)\n2846             Transformed dataset.\n2847         \"\"\"\n2848         check_is_fitted(self)\n2849         return self.one_hot_encoder_.transform(self.apply(X))\n",
        "intended_behavior": "Similar to the above, modifications should ensure that configurations pertinent to the fitting process (possibly including random state settings, number of jobs, etc.) are explicitly passed to each parallel execution context initiated via delayed. This may include augmenting the delayed call to ensure it carries over or applies the current configuration settings within each executed task, preventing misconfigurations when tasks run in threads/processes different from the main thread initializing them."
    }
]