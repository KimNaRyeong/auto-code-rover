[
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 1,
        "end": 3828,
        "class_name": null,
        "method_name": null,
        "code": "1 import datetime\n2 import functools\n3 from numbers import Number\n4 from typing import (\n5     TYPE_CHECKING,\n6     Any,\n7     Callable,\n8     Dict,\n9     Hashable,\n10     Iterable,\n11     List,\n12     Mapping,\n13     Optional,\n14     Sequence,\n15     Tuple,\n16     TypeVar,\n17     Union,\n18     cast,\n19 )\n20 \n21 import numpy as np\n22 import pandas as pd\n23 \n24 from ..plot.plot import _PlotMethods\n25 from . import (\n26     computation,\n27     dtypes,\n28     groupby,\n29     indexing,\n30     ops,\n31     pdcompat,\n32     resample,\n33     rolling,\n34     utils,\n35     weighted,\n36 )\n37 from .accessor_dt import CombinedDatetimelikeAccessor\n38 from .accessor_str import StringAccessor\n39 from .alignment import (\n40     _broadcast_helper,\n41     _get_broadcast_dims_map_common_coords,\n42     align,\n43     reindex_like_indexers,\n44 )\n45 from .common import AbstractArray, DataWithCoords\n46 from .coordinates import (\n47     DataArrayCoordinates,\n48     LevelCoordinatesSource,\n49     assert_coordinate_consistent,\n50     remap_label_indexers,\n51 )\n52 from .dataset import Dataset, split_indexes\n53 from .formatting import format_item\n54 from .indexes import Indexes, default_indexes, propagate_indexes\n55 from .indexing import is_fancy_indexer\n56 from .merge import PANDAS_TYPES, MergeError, _extract_indexes_from_coords\n57 from .options import OPTIONS\n58 from .utils import Default, ReprObject, _check_inplace, _default, either_dict_or_kwargs\n59 from .variable import (\n60     IndexVariable,\n61     Variable,\n62     as_compatible_data,\n63     as_variable,\n64     assert_unique_multiindex_level_names,\n65 )\n66 \n67 if TYPE_CHECKING:\n68     T_DSorDA = TypeVar(\"T_DSorDA\", \"DataArray\", Dataset)\n69 \n70     try:\n71         from dask.delayed import Delayed\n72     except ImportError:\n73         Delayed = None\n74     try:\n75         from cdms2 import Variable as cdms2_Variable\n76     except ImportError:\n77         cdms2_Variable = None\n78     try:\n79         from iris.cube import Cube as iris_Cube\n80     except ImportError:\n81         iris_Cube = None\n82 \n83 \n84 def _infer_coords_and_dims(\n85     shape, coords, dims\n86 ) -> \"Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]\":\n87     \"\"\"All the logic for creating a new DataArray\"\"\"\n88 \n89     if (\n90         coords is not None\n91         and not utils.is_dict_like(coords)\n92         and len(coords) != len(shape)\n93     ):\n94         raise ValueError(\n95             \"coords is not dict-like, but it has %s items, \"\n96             \"which does not match the %s dimensions of the \"\n97             \"data\" % (len(coords), len(shape))\n98         )\n99 \n100     if isinstance(dims, str):\n101         dims = (dims,)\n102 \n103     if dims is None:\n104         dims = [\"dim_%s\" % n for n in range(len(shape))]\n105         if coords is not None and len(coords) == len(shape):\n106             # try to infer dimensions from coords\n107             if utils.is_dict_like(coords):\n108                 # deprecated in GH993, removed in GH1539\n109                 raise ValueError(\n110                     \"inferring DataArray dimensions from \"\n111                     \"dictionary like ``coords`` is no longer \"\n112                     \"supported. Use an explicit list of \"\n113                     \"``dims`` instead.\"\n114                 )\n115             for n, (dim, coord) in enumerate(zip(dims, coords)):\n116                 coord = as_variable(coord, name=dims[n]).to_index_variable()\n117                 dims[n] = coord.name\n118         dims = tuple(dims)\n119     elif len(dims) != len(shape):\n120         raise ValueError(\n121             \"different number of dimensions on data \"\n122             \"and dims: %s vs %s\" % (len(shape), len(dims))\n123         )\n124     else:\n125         for d in dims:\n126             if not isinstance(d, str):\n127                 raise TypeError(\"dimension %s is not a string\" % d)\n128 \n129     new_coords: Dict[Any, Variable] = {}\n130 \n131     if utils.is_dict_like(coords):\n132         for k, v in coords.items():\n133             new_coords[k] = as_variable(v, name=k)\n134     elif coords is not None:\n135         for dim, coord in zip(dims, coords):\n136             var = as_variable(coord, name=dim)\n137             var.dims = (dim,)\n138             new_coords[dim] = var.to_index_variable()\n139 \n140     sizes = dict(zip(dims, shape))\n141     for k, v in new_coords.items():\n142         if any(d not in dims for d in v.dims):\n143             raise ValueError(\n144                 \"coordinate %s has dimensions %s, but these \"\n145                 \"are not a subset of the DataArray \"\n146                 \"dimensions %s\" % (k, v.dims, dims)\n147             )\n148 \n149         for d, s in zip(v.dims, v.shape):\n150             if s != sizes[d]:\n151                 raise ValueError(\n152                     \"conflicting sizes for dimension %r: \"\n153                     \"length %s on the data but length %s on \"\n154                     \"coordinate %r\" % (d, sizes[d], s, k)\n155                 )\n156 \n157         if k in sizes and v.shape != (sizes[k],):\n158             raise ValueError(\n159                 \"coordinate %r is a DataArray dimension, but \"\n160                 \"it has shape %r rather than expected shape %r \"\n161                 \"matching the dimension size\" % (k, v.shape, (sizes[k],))\n162             )\n163 \n164     assert_unique_multiindex_level_names(new_coords)\n165 \n166     return new_coords, dims\n167 \n168 \n169 def _check_data_shape(data, coords, dims):\n170     if data is dtypes.NA:\n171         data = np.nan\n172     if coords is not None and utils.is_scalar(data, include_0d=False):\n173         if utils.is_dict_like(coords):\n174             if dims is None:\n175                 return data\n176             else:\n177                 data_shape = tuple(\n178                     as_variable(coords[k], k).size if k in coords.keys() else 1\n179                     for k in dims\n180                 )\n181         else:\n182             data_shape = tuple(as_variable(coord, \"foo\").size for coord in coords)\n183         data = np.full(data_shape, data)\n184     return data\n185 \n186 \n187 class _LocIndexer:\n188     __slots__ = (\"data_array\",)\n189 \n190     def __init__(self, data_array: \"DataArray\"):\n191         self.data_array = data_array\n192 \n193     def __getitem__(self, key) -> \"DataArray\":\n194         if not utils.is_dict_like(key):\n195             # expand the indexer so we can handle Ellipsis\n196             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n197             key = dict(zip(self.data_array.dims, labels))\n198         return self.data_array.sel(**key)\n199 \n200     def __setitem__(self, key, value) -> None:\n201         if not utils.is_dict_like(key):\n202             # expand the indexer so we can handle Ellipsis\n203             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n204             key = dict(zip(self.data_array.dims, labels))\n205 \n206         pos_indexers, _ = remap_label_indexers(self.data_array, key)\n207         self.data_array[pos_indexers] = value\n208 \n209 \n210 # Used as the key corresponding to a DataArray's variable when converting\n211 # arbitrary DataArray objects to datasets\n212 _THIS_ARRAY = ReprObject(\"<this-array>\")\n213 \n214 \n215 class DataArray(AbstractArray, DataWithCoords):\n216     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n217 \n218     DataArray provides a wrapper around numpy ndarrays that uses labeled\n219     dimensions and coordinates to support metadata aware operations. The API is\n220     similar to that for the pandas Series or DataFrame, but DataArray objects\n221     can have any number of dimensions, and their contents have fixed data\n222     types.\n223 \n224     Additional features over raw numpy arrays:\n225 \n226     - Apply operations over dimensions by name: ``x.sum('time')``.\n227     - Select or assign values by integer location (like numpy): ``x[:10]``\n228       or by label (like pandas): ``x.loc['2014-01-01']`` or\n229       ``x.sel(time='2014-01-01')``.\n230     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n231       dimensions (known in numpy as \"broadcasting\") based on dimension names,\n232       regardless of their original order.\n233     - Keep track of arbitrary metadata in the form of a Python dictionary:\n234       ``x.attrs``\n235     - Convert to a pandas Series: ``x.to_series()``.\n236 \n237     Getting items from or doing mathematical operations with a DataArray\n238     always returns another DataArray.\n239     \"\"\"\n240 \n241     _cache: Dict[str, Any]\n242     _coords: Dict[Any, Variable]\n243     _indexes: Optional[Dict[Hashable, pd.Index]]\n244     _name: Optional[Hashable]\n245     _variable: Variable\n246 \n247     __slots__ = (\n248         \"_cache\",\n249         \"_coords\",\n250         \"_file_obj\",\n251         \"_indexes\",\n252         \"_name\",\n253         \"_variable\",\n254         \"__weakref__\",\n255     )\n256 \n257     _groupby_cls = groupby.DataArrayGroupBy\n258     _rolling_cls = rolling.DataArrayRolling\n259     _coarsen_cls = rolling.DataArrayCoarsen\n260     _resample_cls = resample.DataArrayResample\n261     _weighted_cls = weighted.DataArrayWeighted\n262 \n263     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)\n264 \n265     def __init__(\n266         self,\n267         data: Any = dtypes.NA,\n268         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n269         dims: Union[Hashable, Sequence[Hashable], None] = None,\n270         name: Hashable = None,\n271         attrs: Mapping = None,\n272         # internal parameters\n273         indexes: Dict[Hashable, pd.Index] = None,\n274         fastpath: bool = False,\n275     ):\n276         \"\"\"\n277         Parameters\n278         ----------\n279         data : array_like\n280             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n281             or castable to an ``ndarray``. If a self-described xarray or pandas\n282             object, attempts are made to use this array's metadata to fill in\n283             other unspecified arguments. A view of the array's data is used\n284             instead of a copy if possible.\n285         coords : sequence or dict of array_like objects, optional\n286             Coordinates (tick labels) to use for indexing along each dimension.\n287             The following notations are accepted:\n288 \n289             - mapping {dimension name: array-like}\n290             - sequence of tuples that are valid arguments for xarray.Variable()\n291               - (dims, data)\n292               - (dims, data, attrs)\n293               - (dims, data, attrs, encoding)\n294 \n295             Additionally, it is possible to define a coord whose name\n296             does not match the dimension name, or a coord based on multiple\n297             dimensions, with one of the following notations:\n298 \n299             - mapping {coord name: DataArray}\n300             - mapping {coord name: Variable}\n301             - mapping {coord name: (dimension name, array-like)}\n302             - mapping {coord name: (tuple of dimension names, array-like)}\n303 \n304         dims : hashable or sequence of hashable, optional\n305             Name(s) of the data dimension(s). Must be either a hashable (only\n306             for 1D data) or a sequence of hashables with length equal to the\n307             number of dimensions. If this argument is omitted, dimension names\n308             default to ``['dim_0', ... 'dim_n']``.\n309         name : str or None, optional\n310             Name of this array.\n311         attrs : dict_like or None, optional\n312             Attributes to assign to the new instance. By default, an empty\n313             attribute dictionary is initialized.\n314         \"\"\"\n315         if fastpath:\n316             variable = data\n317             assert dims is None\n318             assert attrs is None\n319         else:\n320             # try to fill in arguments from data if they weren't supplied\n321             if coords is None:\n322 \n323                 if isinstance(data, DataArray):\n324                     coords = data.coords\n325                 elif isinstance(data, pd.Series):\n326                     coords = [data.index]\n327                 elif isinstance(data, pd.DataFrame):\n328                     coords = [data.index, data.columns]\n329                 elif isinstance(data, (pd.Index, IndexVariable)):\n330                     coords = [data]\n331                 elif isinstance(data, pdcompat.Panel):\n332                     coords = [data.items, data.major_axis, data.minor_axis]\n333 \n334             if dims is None:\n335                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n336             if name is None:\n337                 name = getattr(data, \"name\", None)\n338             if attrs is None and not isinstance(data, PANDAS_TYPES):\n339                 attrs = getattr(data, \"attrs\", None)\n340 \n341             data = _check_data_shape(data, coords, dims)\n342             data = as_compatible_data(data)\n343             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n344             variable = Variable(dims, data, attrs, fastpath=True)\n345             indexes = dict(\n346                 _extract_indexes_from_coords(coords)\n347             )  # needed for to_dataset\n348 \n349         # These fully describe a DataArray\n350         self._variable = variable\n351         assert isinstance(coords, dict)\n352         self._coords = coords\n353         self._name = name\n354 \n355         # TODO(shoyer): document this argument, once it becomes part of the\n356         # public interface.\n357         self._indexes = indexes\n358 \n359         self._file_obj = None\n360 \n361     def _replace(\n362         self,\n363         variable: Variable = None,\n364         coords=None,\n365         name: Union[Hashable, None, Default] = _default,\n366         indexes=None,\n367     ) -> \"DataArray\":\n368         if variable is None:\n369             variable = self.variable\n370         if coords is None:\n371             coords = self._coords\n372         if name is _default:\n373             name = self.name\n374         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n375 \n376     def _replace_maybe_drop_dims(\n377         self, variable: Variable, name: Union[Hashable, None, Default] = _default\n378     ) -> \"DataArray\":\n379         if variable.dims == self.dims and variable.shape == self.shape:\n380             coords = self._coords.copy()\n381             indexes = self._indexes\n382         elif variable.dims == self.dims:\n383             # Shape has changed (e.g. from reduce(..., keepdims=True)\n384             new_sizes = dict(zip(self.dims, variable.shape))\n385             coords = {\n386                 k: v\n387                 for k, v in self._coords.items()\n388                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n389             }\n390             changed_dims = [\n391                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n392             ]\n393             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n394         else:\n395             allowed_dims = set(variable.dims)\n396             coords = {\n397                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n398             }\n399             indexes = propagate_indexes(\n400                 self._indexes, exclude=(set(self.dims) - allowed_dims)\n401             )\n402         return self._replace(variable, coords, name, indexes=indexes)\n403 \n404     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> \"DataArray\":\n405         if not len(indexes):\n406             return self\n407         coords = self._coords.copy()\n408         for name, idx in indexes.items():\n409             coords[name] = IndexVariable(name, idx)\n410         obj = self._replace(coords=coords)\n411 \n412         # switch from dimension to level names, if necessary\n413         dim_names: Dict[Any, str] = {}\n414         for dim, idx in indexes.items():\n415             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n416                 dim_names[dim] = idx.name\n417         if dim_names:\n418             obj = obj.rename(dim_names)\n419         return obj\n420 \n421     def _to_temp_dataset(self) -> Dataset:\n422         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n423 \n424     def _from_temp_dataset(\n425         self, dataset: Dataset, name: Hashable = _default\n426     ) -> \"DataArray\":\n427         variable = dataset._variables.pop(_THIS_ARRAY)\n428         coords = dataset._variables\n429         indexes = dataset._indexes\n430         return self._replace(variable, coords, name, indexes=indexes)\n431 \n432     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n433         \"\"\" splits dataarray along dimension 'dim' \"\"\"\n434 \n435         def subset(dim, label):\n436             array = self.loc[{dim: label}]\n437             array.attrs = {}\n438             return as_variable(array)\n439 \n440         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n441         variables.update({k: v for k, v in self._coords.items() if k != dim})\n442         indexes = propagate_indexes(self._indexes, exclude=dim)\n443         coord_names = set(self._coords) - set([dim])\n444         dataset = Dataset._construct_direct(\n445             variables, coord_names, indexes=indexes, attrs=self.attrs\n446         )\n447         return dataset\n448 \n449     def _to_dataset_whole(\n450         self, name: Hashable = None, shallow_copy: bool = True\n451     ) -> Dataset:\n452         if name is None:\n453             name = self.name\n454         if name is None:\n455             raise ValueError(\n456                 \"unable to convert unnamed DataArray to a \"\n457                 \"Dataset without providing an explicit name\"\n458             )\n459         if name in self.coords:\n460             raise ValueError(\n461                 \"cannot create a Dataset from a DataArray with \"\n462                 \"the same name as one of its coordinates\"\n463             )\n464         # use private APIs for speed: this is called by _to_temp_dataset(),\n465         # which is used in the guts of a lot of operations (e.g., reindex)\n466         variables = self._coords.copy()\n467         variables[name] = self.variable\n468         if shallow_copy:\n469             for k in variables:\n470                 variables[k] = variables[k].copy(deep=False)\n471         indexes = self._indexes\n472 \n473         coord_names = set(self._coords)\n474         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n475         return dataset\n476 \n477     def to_dataset(\n478         self,\n479         dim: Hashable = None,\n480         *,\n481         name: Hashable = None,\n482         promote_attrs: bool = False,\n483     ) -> Dataset:\n484         \"\"\"Convert a DataArray to a Dataset.\n485 \n486         Parameters\n487         ----------\n488         dim : hashable, optional\n489             Name of the dimension on this array along which to split this array\n490             into separate variables. If not provided, this array is converted\n491             into a Dataset of one variable.\n492         name : hashable, optional\n493             Name to substitute for this array's name. Only valid if ``dim`` is\n494             not provided.\n495         promote_attrs : bool, default False\n496             Set to True to shallow copy attrs of DataArray to returned Dataset.\n497 \n498         Returns\n499         -------\n500         dataset : Dataset\n501         \"\"\"\n502         if dim is not None and dim not in self.dims:\n503             raise TypeError(\n504                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n505             )\n506 \n507         if dim is not None:\n508             if name is not None:\n509                 raise TypeError(\"cannot supply both dim and name arguments\")\n510             result = self._to_dataset_split(dim)\n511         else:\n512             result = self._to_dataset_whole(name)\n513 \n514         if promote_attrs:\n515             result.attrs = dict(self.attrs)\n516 \n517         return result\n518 \n519     @property\n520     def name(self) -> Optional[Hashable]:\n521         \"\"\"The name of this array.\n522         \"\"\"\n523         return self._name\n524 \n525     @name.setter\n526     def name(self, value: Optional[Hashable]) -> None:\n527         self._name = value\n528 \n529     @property\n530     def variable(self) -> Variable:\n531         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n532         return self._variable\n533 \n534     @property\n535     def dtype(self) -> np.dtype:\n536         return self.variable.dtype\n537 \n538     @property\n539     def shape(self) -> Tuple[int, ...]:\n540         return self.variable.shape\n541 \n542     @property\n543     def size(self) -> int:\n544         return self.variable.size\n545 \n546     @property\n547     def nbytes(self) -> int:\n548         return self.variable.nbytes\n549 \n550     @property\n551     def ndim(self) -> int:\n552         return self.variable.ndim\n553 \n554     def __len__(self) -> int:\n555         return len(self.variable)\n556 \n557     @property\n558     def data(self) -> Any:\n559         \"\"\"The array's data as a dask or numpy array\n560         \"\"\"\n561         return self.variable.data\n562 \n563     @data.setter\n564     def data(self, value: Any) -> None:\n565         self.variable.data = value\n566 \n567     @property\n568     def values(self) -> np.ndarray:\n569         \"\"\"The array's data as a numpy.ndarray\"\"\"\n570         return self.variable.values\n571 \n572     @values.setter\n573     def values(self, value: Any) -> None:\n574         self.variable.values = value\n575 \n576     @property\n577     def _in_memory(self) -> bool:\n578         return self.variable._in_memory\n579 \n580     def to_index(self) -> pd.Index:\n581         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n582         arrays.\n583         \"\"\"\n584         return self.variable.to_index()\n585 \n586     @property\n587     def dims(self) -> Tuple[Hashable, ...]:\n588         \"\"\"Tuple of dimension names associated with this array.\n589 \n590         Note that the type of this property is inconsistent with\n591         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n592         consistently named properties.\n593         \"\"\"\n594         return self.variable.dims\n595 \n596     @dims.setter\n597     def dims(self, value):\n598         raise AttributeError(\n599             \"you cannot assign dims on a DataArray. Use \"\n600             \".rename() or .swap_dims() instead.\"\n601         )\n602 \n603     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n604         if utils.is_dict_like(key):\n605             return key\n606         else:\n607             key = indexing.expanded_indexer(key, self.ndim)\n608             return dict(zip(self.dims, key))\n609 \n610     @property\n611     def _level_coords(self) -> Dict[Hashable, Hashable]:\n612         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n613         coordinate name.\n614         \"\"\"\n615         level_coords: Dict[Hashable, Hashable] = {}\n616 \n617         for cname, var in self._coords.items():\n618             if var.ndim == 1 and isinstance(var, IndexVariable):\n619                 level_names = var.level_names\n620                 if level_names is not None:\n621                     (dim,) = var.dims\n622                     level_coords.update({lname: dim for lname in level_names})\n623         return level_coords\n624 \n625     def _getitem_coord(self, key):\n626         from .dataset import _get_virtual_variable\n627 \n628         try:\n629             var = self._coords[key]\n630         except KeyError:\n631             dim_sizes = dict(zip(self.dims, self.shape))\n632             _, key, var = _get_virtual_variable(\n633                 self._coords, key, self._level_coords, dim_sizes\n634             )\n635 \n636         return self._replace_maybe_drop_dims(var, name=key)\n637 \n638     def __getitem__(self, key: Any) -> \"DataArray\":\n639         if isinstance(key, str):\n640             return self._getitem_coord(key)\n641         else:\n642             # xarray-style array indexing\n643             return self.isel(indexers=self._item_key_to_dict(key))\n644 \n645     def __setitem__(self, key: Any, value: Any) -> None:\n646         if isinstance(key, str):\n647             self.coords[key] = value\n648         else:\n649             # Coordinates in key, value and self[key] should be consistent.\n650             # TODO Coordinate consistency in key is checked here, but it\n651             # causes unnecessary indexing. It should be optimized.\n652             obj = self[key]\n653             if isinstance(value, DataArray):\n654                 assert_coordinate_consistent(value, obj.coords.variables)\n655             # DataArray key -> Variable key\n656             key = {\n657                 k: v.variable if isinstance(v, DataArray) else v\n658                 for k, v in self._item_key_to_dict(key).items()\n659             }\n660             self.variable[key] = value\n661 \n662     def __delitem__(self, key: Any) -> None:\n663         del self.coords[key]\n664 \n665     @property\n666     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n667         \"\"\"List of places to look-up items for attribute-style access\n668         \"\"\"\n669         return self._item_sources + [self.attrs]\n670 \n671     @property\n672     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n673         \"\"\"List of places to look-up items for key-completion\n674         \"\"\"\n675         return [\n676             self.coords,\n677             {d: self.coords[d] for d in self.dims},\n678             LevelCoordinatesSource(self),\n679         ]\n680 \n681     def __contains__(self, key: Any) -> bool:\n682         return key in self.data\n683 \n684     @property\n685     def loc(self) -> _LocIndexer:\n686         \"\"\"Attribute for location based indexing like pandas.\n687         \"\"\"\n688         return _LocIndexer(self)\n689 \n690     @property\n691     def attrs(self) -> Dict[Hashable, Any]:\n692         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n693         return self.variable.attrs\n694 \n695     @attrs.setter\n696     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n697         # Disable type checking to work around mypy bug - see mypy#4167\n698         self.variable.attrs = value  # type: ignore\n699 \n700     @property\n701     def encoding(self) -> Dict[Hashable, Any]:\n702         \"\"\"Dictionary of format-specific settings for how this array should be\n703         serialized.\"\"\"\n704         return self.variable.encoding\n705 \n706     @encoding.setter\n707     def encoding(self, value: Mapping[Hashable, Any]) -> None:\n708         self.variable.encoding = value\n709 \n710     @property\n711     def indexes(self) -> Indexes:\n712         \"\"\"Mapping of pandas.Index objects used for label based indexing\n713         \"\"\"\n714         if self._indexes is None:\n715             self._indexes = default_indexes(self._coords, self.dims)\n716         return Indexes(self._indexes)\n717 \n718     @property\n719     def coords(self) -> DataArrayCoordinates:\n720         \"\"\"Dictionary-like container of coordinate arrays.\n721         \"\"\"\n722         return DataArrayCoordinates(self)\n723 \n724     def reset_coords(\n725         self,\n726         names: Union[Iterable[Hashable], Hashable, None] = None,\n727         drop: bool = False,\n728         inplace: bool = None,\n729     ) -> Union[None, \"DataArray\", Dataset]:\n730         \"\"\"Given names of coordinates, reset them to become variables.\n731 \n732         Parameters\n733         ----------\n734         names : hashable or iterable of hashables, optional\n735             Name(s) of non-index coordinates in this dataset to reset into\n736             variables. By default, all non-index coordinates are reset.\n737         drop : bool, optional\n738             If True, remove coordinates instead of converting them into\n739             variables.\n740 \n741         Returns\n742         -------\n743         Dataset, or DataArray if ``drop == True``\n744         \"\"\"\n745         _check_inplace(inplace)\n746         if names is None:\n747             names = set(self.coords) - set(self.dims)\n748         dataset = self.coords.to_dataset().reset_coords(names, drop)\n749         if drop:\n750             return self._replace(coords=dataset._variables)\n751         else:\n752             if self.name is None:\n753                 raise ValueError(\n754                     \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n755                 )\n756             dataset[self.name] = self.variable\n757             return dataset\n758 \n759     def __dask_tokenize__(self):\n760         from dask.base import normalize_token\n761 \n762         return normalize_token((type(self), self._variable, self._coords, self._name))\n763 \n764     def __dask_graph__(self):\n765         return self._to_temp_dataset().__dask_graph__()\n766 \n767     def __dask_keys__(self):\n768         return self._to_temp_dataset().__dask_keys__()\n769 \n770     def __dask_layers__(self):\n771         return self._to_temp_dataset().__dask_layers__()\n772 \n773     @property\n774     def __dask_optimize__(self):\n775         return self._to_temp_dataset().__dask_optimize__\n776 \n777     @property\n778     def __dask_scheduler__(self):\n779         return self._to_temp_dataset().__dask_scheduler__\n780 \n781     def __dask_postcompute__(self):\n782         func, args = self._to_temp_dataset().__dask_postcompute__()\n783         return self._dask_finalize, (func, args, self.name)\n784 \n785     def __dask_postpersist__(self):\n786         func, args = self._to_temp_dataset().__dask_postpersist__()\n787         return self._dask_finalize, (func, args, self.name)\n788 \n789     @staticmethod\n790     def _dask_finalize(results, func, args, name):\n791         ds = func(results, *args)\n792         variable = ds._variables.pop(_THIS_ARRAY)\n793         coords = ds._variables\n794         return DataArray(variable, coords, name=name, fastpath=True)\n795 \n796     def load(self, **kwargs) -> \"DataArray\":\n797         \"\"\"Manually trigger loading of this array's data from disk or a\n798         remote source into memory and return this array.\n799 \n800         Normally, it should not be necessary to call this method in user code,\n801         because all xarray functions should either work on deferred data or\n802         load data automatically. However, this method can be necessary when\n803         working with many file objects on disk.\n804 \n805         Parameters\n806         ----------\n807         **kwargs : dict\n808             Additional keyword arguments passed on to ``dask.array.compute``.\n809 \n810         See Also\n811         --------\n812         dask.array.compute\n813         \"\"\"\n814         ds = self._to_temp_dataset().load(**kwargs)\n815         new = self._from_temp_dataset(ds)\n816         self._variable = new._variable\n817         self._coords = new._coords\n818         return self\n819 \n820     def compute(self, **kwargs) -> \"DataArray\":\n821         \"\"\"Manually trigger loading of this array's data from disk or a\n822         remote source into memory and return a new array. The original is\n823         left unaltered.\n824 \n825         Normally, it should not be necessary to call this method in user code,\n826         because all xarray functions should either work on deferred data or\n827         load data automatically. However, this method can be necessary when\n828         working with many file objects on disk.\n829 \n830         Parameters\n831         ----------\n832         **kwargs : dict\n833             Additional keyword arguments passed on to ``dask.array.compute``.\n834 \n835         See Also\n836         --------\n837         dask.array.compute\n838         \"\"\"\n839         new = self.copy(deep=False)\n840         return new.load(**kwargs)\n841 \n842     def persist(self, **kwargs) -> \"DataArray\":\n843         \"\"\" Trigger computation in constituent dask arrays\n844 \n845         This keeps them as dask arrays but encourages them to keep data in\n846         memory.  This is particularly useful when on a distributed machine.\n847         When on a single machine consider using ``.compute()`` instead.\n848 \n849         Parameters\n850         ----------\n851         **kwargs : dict\n852             Additional keyword arguments passed on to ``dask.persist``.\n853 \n854         See Also\n855         --------\n856         dask.persist\n857         \"\"\"\n858         ds = self._to_temp_dataset().persist(**kwargs)\n859         return self._from_temp_dataset(ds)\n860 \n861     def copy(self, deep: bool = True, data: Any = None) -> \"DataArray\":\n862         \"\"\"Returns a copy of this array.\n863 \n864         If `deep=True`, a deep copy is made of the data array.\n865         Otherwise, a shallow copy is made, so each variable in the new\n866         array's dataset is also a variable in this array's dataset.\n867 \n868         Use `data` to create a new object with the same structure as\n869         original but entirely new data.\n870 \n871         Parameters\n872         ----------\n873         deep : bool, optional\n874             Whether the data array and its coordinates are loaded into memory\n875             and copied onto the new object. Default is True.\n876         data : array_like, optional\n877             Data to use in the new object. Must have same shape as original.\n878             When `data` is used, `deep` is ignored for all data variables,\n879             and only used for coords.\n880 \n881         Returns\n882         -------\n883         object : DataArray\n884             New object with dimensions, attributes, coordinates, name,\n885             encoding, and optionally data copied from original.\n886 \n887         Examples\n888         --------\n889 \n890         Shallow versus deep copy\n891 \n892         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n893         >>> array.copy()\n894         <xarray.DataArray (x: 3)>\n895         array([1, 2, 3])\n896         Coordinates:\n897         * x        (x) <U1 'a' 'b' 'c'\n898         >>> array_0 = array.copy(deep=False)\n899         >>> array_0[0] = 7\n900         >>> array_0\n901         <xarray.DataArray (x: 3)>\n902         array([7, 2, 3])\n903         Coordinates:\n904         * x        (x) <U1 'a' 'b' 'c'\n905         >>> array\n906         <xarray.DataArray (x: 3)>\n907         array([7, 2, 3])\n908         Coordinates:\n909         * x        (x) <U1 'a' 'b' 'c'\n910 \n911         Changing the data using the ``data`` argument maintains the\n912         structure of the original object, but with the new data. Original\n913         object is unaffected.\n914 \n915         >>> array.copy(data=[0.1, 0.2, 0.3])\n916         <xarray.DataArray (x: 3)>\n917         array([ 0.1,  0.2,  0.3])\n918         Coordinates:\n919         * x        (x) <U1 'a' 'b' 'c'\n920         >>> array\n921         <xarray.DataArray (x: 3)>\n922         array([1, 2, 3])\n923         Coordinates:\n924         * x        (x) <U1 'a' 'b' 'c'\n925 \n926         See Also\n927         --------\n928         pandas.DataFrame.copy\n929         \"\"\"\n930         variable = self.variable.copy(deep=deep, data=data)\n931         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n932         if self._indexes is None:\n933             indexes = self._indexes\n934         else:\n935             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n936         return self._replace(variable, coords, indexes=indexes)\n937 \n938     def __copy__(self) -> \"DataArray\":\n939         return self.copy(deep=False)\n940 \n941     def __deepcopy__(self, memo=None) -> \"DataArray\":\n942         # memo does nothing but is required for compatibility with\n943         # copy.deepcopy\n944         return self.copy(deep=True)\n945 \n946     # mutable objects should not be hashable\n947     # https://github.com/python/mypy/issues/4266\n948     __hash__ = None  # type: ignore\n949 \n950     @property\n951     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n952         \"\"\"Block dimensions for this array's data or None if it's not a dask\n953         array.\n954         \"\"\"\n955         return self.variable.chunks\n956 \n957     def chunk(\n958         self,\n959         chunks: Union[\n960             None,\n961             Number,\n962             Tuple[Number, ...],\n963             Tuple[Tuple[Number, ...], ...],\n964             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n965         ] = None,\n966         name_prefix: str = \"xarray-\",\n967         token: str = None,\n968         lock: bool = False,\n969     ) -> \"DataArray\":\n970         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n971 \n972         If this variable is a non-dask array, it will be converted to dask\n973         array. If it's a dask array, it will be rechunked to the given chunk\n974         sizes.\n975 \n976         If neither chunks is not provided for one or more dimensions, chunk\n977         sizes along that dimension will not be updated; non-dask arrays will be\n978         converted into dask arrays with a single block.\n979 \n980         Parameters\n981         ----------\n982         chunks : int, tuple or mapping, optional\n983             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n984             ``{'x': 5, 'y': 5}``.\n985         name_prefix : str, optional\n986             Prefix for the name of the new dask array.\n987         token : str, optional\n988             Token uniquely identifying this array.\n989         lock : optional\n990             Passed on to :py:func:`dask.array.from_array`, if the array is not\n991             already as dask array.\n992 \n993         Returns\n994         -------\n995         chunked : xarray.DataArray\n996         \"\"\"\n997         if isinstance(chunks, (tuple, list)):\n998             chunks = dict(zip(self.dims, chunks))\n999 \n1000         ds = self._to_temp_dataset().chunk(\n1001             chunks, name_prefix=name_prefix, token=token, lock=lock\n1002         )\n1003         return self._from_temp_dataset(ds)\n1004 \n1005     def isel(\n1006         self,\n1007         indexers: Mapping[Hashable, Any] = None,\n1008         drop: bool = False,\n1009         missing_dims: str = \"raise\",\n1010         **indexers_kwargs: Any,\n1011     ) -> \"DataArray\":\n1012         \"\"\"Return a new DataArray whose data is given by integer indexing\n1013         along the specified dimension(s).\n1014 \n1015         Parameters\n1016         ----------\n1017         indexers : dict, optional\n1018             A dict with keys matching dimensions and values given\n1019             by integers, slice objects or arrays.\n1020             indexer can be a integer, slice, array-like or DataArray.\n1021             If DataArrays are passed as indexers, xarray-style indexing will be\n1022             carried out. See :ref:`indexing` for the details.\n1023             One of indexers or indexers_kwargs must be provided.\n1024         drop : bool, optional\n1025             If ``drop=True``, drop coordinates variables indexed by integers\n1026             instead of making them scalar.\n1027         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1028             What to do if dimensions that should be selected from are not present in the\n1029             DataArray:\n1030             - \"exception\": raise an exception\n1031             - \"warning\": raise a warning, and ignore the missing dimensions\n1032             - \"ignore\": ignore the missing dimensions\n1033         **indexers_kwargs : {dim: indexer, ...}, optional\n1034             The keyword arguments form of ``indexers``.\n1035 \n1036         See Also\n1037         --------\n1038         Dataset.isel\n1039         DataArray.sel\n1040         \"\"\"\n1041 \n1042         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1043 \n1044         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1045             ds = self._to_temp_dataset()._isel_fancy(\n1046                 indexers, drop=drop, missing_dims=missing_dims\n1047             )\n1048             return self._from_temp_dataset(ds)\n1049 \n1050         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1051         # lists, or zero or one-dimensional np.ndarray's\n1052 \n1053         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1054 \n1055         coords = {}\n1056         for coord_name, coord_value in self._coords.items():\n1057             coord_indexers = {\n1058                 k: v for k, v in indexers.items() if k in coord_value.dims\n1059             }\n1060             if coord_indexers:\n1061                 coord_value = coord_value.isel(coord_indexers)\n1062                 if drop and coord_value.ndim == 0:\n1063                     continue\n1064             coords[coord_name] = coord_value\n1065 \n1066         return self._replace(variable=variable, coords=coords)\n1067 \n1068     def sel(\n1069         self,\n1070         indexers: Mapping[Hashable, Any] = None,\n1071         method: str = None,\n1072         tolerance=None,\n1073         drop: bool = False,\n1074         **indexers_kwargs: Any,\n1075     ) -> \"DataArray\":\n1076         \"\"\"Return a new DataArray whose data is given by selecting index\n1077         labels along the specified dimension(s).\n1078 \n1079         In contrast to `DataArray.isel`, indexers for this method should use\n1080         labels instead of integers.\n1081 \n1082         Under the hood, this method is powered by using pandas's powerful Index\n1083         objects. This makes label based indexing essentially just as fast as\n1084         using integer indexing.\n1085 \n1086         It also means this method uses pandas's (well documented) logic for\n1087         indexing. This means you can use string shortcuts for datetime indexes\n1088         (e.g., '2000-01' to select all values in January 2000). It also means\n1089         that slices are treated as inclusive of both the start and stop values,\n1090         unlike normal Python indexing.\n1091 \n1092         .. warning::\n1093 \n1094           Do not try to assign values when using any of the indexing methods\n1095           ``isel`` or ``sel``::\n1096 \n1097             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1098             # DO NOT do this\n1099             da.isel(x=[0, 1, 2])[1] = -1\n1100 \n1101           Assigning values with the chained indexing using ``.sel`` or\n1102           ``.isel`` fails silently.\n1103 \n1104         Parameters\n1105         ----------\n1106         indexers : dict, optional\n1107             A dict with keys matching dimensions and values given\n1108             by scalars, slices or arrays of tick labels. For dimensions with\n1109             multi-index, the indexer may also be a dict-like object with keys\n1110             matching index level names.\n1111             If DataArrays are passed as indexers, xarray-style indexing will be\n1112             carried out. See :ref:`indexing` for the details.\n1113             One of indexers or indexers_kwargs must be provided.\n1114         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1115             Method to use for inexact matches:\n1116 \n1117             * None (default): only exact matches\n1118             * pad / ffill: propagate last valid index value forward\n1119             * backfill / bfill: propagate next valid index value backward\n1120             * nearest: use nearest valid index value\n1121         tolerance : optional\n1122             Maximum distance between original and new labels for inexact\n1123             matches. The values of the index at the matching locations must\n1124             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1125         drop : bool, optional\n1126             If ``drop=True``, drop coordinates variables in `indexers` instead\n1127             of making them scalar.\n1128         **indexers_kwargs : {dim: indexer, ...}, optional\n1129             The keyword arguments form of ``indexers``.\n1130             One of indexers or indexers_kwargs must be provided.\n1131 \n1132         Returns\n1133         -------\n1134         obj : DataArray\n1135             A new DataArray with the same contents as this DataArray, except the\n1136             data and each dimension is indexed by the appropriate indexers.\n1137             If indexer DataArrays have coordinates that do not conflict with\n1138             this object, then these coordinates will be attached.\n1139             In general, each array's data will be a view of the array's data\n1140             in this DataArray, unless vectorized indexing was triggered by using\n1141             an array indexer, in which case the data will be a copy.\n1142 \n1143         See Also\n1144         --------\n1145         Dataset.sel\n1146         DataArray.isel\n1147 \n1148         \"\"\"\n1149         ds = self._to_temp_dataset().sel(\n1150             indexers=indexers,\n1151             drop=drop,\n1152             method=method,\n1153             tolerance=tolerance,\n1154             **indexers_kwargs,\n1155         )\n1156         return self._from_temp_dataset(ds)\n1157 \n1158     def head(\n1159         self,\n1160         indexers: Union[Mapping[Hashable, int], int] = None,\n1161         **indexers_kwargs: Any,\n1162     ) -> \"DataArray\":\n1163         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1164         values along the specified dimension(s). Default `n` = 5\n1165 \n1166         See Also\n1167         --------\n1168         Dataset.head\n1169         DataArray.tail\n1170         DataArray.thin\n1171         \"\"\"\n1172         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1173         return self._from_temp_dataset(ds)\n1174 \n1175     def tail(\n1176         self,\n1177         indexers: Union[Mapping[Hashable, int], int] = None,\n1178         **indexers_kwargs: Any,\n1179     ) -> \"DataArray\":\n1180         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1181         values along the specified dimension(s). Default `n` = 5\n1182 \n1183         See Also\n1184         --------\n1185         Dataset.tail\n1186         DataArray.head\n1187         DataArray.thin\n1188         \"\"\"\n1189         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1190         return self._from_temp_dataset(ds)\n1191 \n1192     def thin(\n1193         self,\n1194         indexers: Union[Mapping[Hashable, int], int] = None,\n1195         **indexers_kwargs: Any,\n1196     ) -> \"DataArray\":\n1197         \"\"\"Return a new DataArray whose data is given by each `n` value\n1198         along the specified dimension(s).\n1199 \n1200         See Also\n1201         --------\n1202         Dataset.thin\n1203         DataArray.head\n1204         DataArray.tail\n1205         \"\"\"\n1206         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1207         return self._from_temp_dataset(ds)\n1208 \n1209     def broadcast_like(\n1210         self, other: Union[\"DataArray\", Dataset], exclude: Iterable[Hashable] = None\n1211     ) -> \"DataArray\":\n1212         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1213 \n1214         This is equivalent to xr.broadcast(other, self)[1]\n1215 \n1216         xarray objects are broadcast against each other in arithmetic\n1217         operations, so this method is not be necessary for most uses.\n1218 \n1219         If no change is needed, the input data is returned to the output\n1220         without being copied.\n1221 \n1222         If new coords are added by the broadcast, their values are\n1223         NaN filled.\n1224 \n1225         Parameters\n1226         ----------\n1227         other : Dataset or DataArray\n1228             Object against which to broadcast this array.\n1229         exclude : iterable of hashable, optional\n1230             Dimensions that must not be broadcasted\n1231 \n1232         Returns\n1233         -------\n1234         new_da: xr.DataArray\n1235 \n1236         Examples\n1237         --------\n1238 \n1239         >>> arr1\n1240         <xarray.DataArray (x: 2, y: 3)>\n1241         array([[0.840235, 0.215216, 0.77917 ],\n1242                [0.726351, 0.543824, 0.875115]])\n1243         Coordinates:\n1244           * x        (x) <U1 'a' 'b'\n1245           * y        (y) <U1 'a' 'b' 'c'\n1246         >>> arr2\n1247         <xarray.DataArray (x: 3, y: 2)>\n1248         array([[0.612611, 0.125753],\n1249                [0.853181, 0.948818],\n1250                [0.180885, 0.33363 ]])\n1251         Coordinates:\n1252           * x        (x) <U1 'a' 'b' 'c'\n1253           * y        (y) <U1 'a' 'b'\n1254         >>> arr1.broadcast_like(arr2)\n1255         <xarray.DataArray (x: 3, y: 3)>\n1256         array([[0.840235, 0.215216, 0.77917 ],\n1257                [0.726351, 0.543824, 0.875115],\n1258                [     nan,      nan,      nan]])\n1259         Coordinates:\n1260           * x        (x) object 'a' 'b' 'c'\n1261           * y        (y) object 'a' 'b' 'c'\n1262         \"\"\"\n1263         if exclude is None:\n1264             exclude = set()\n1265         else:\n1266             exclude = set(exclude)\n1267         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1268 \n1269         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1270 \n1271         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n1272 \n1273     def reindex_like(\n1274         self,\n1275         other: Union[\"DataArray\", Dataset],\n1276         method: str = None,\n1277         tolerance=None,\n1278         copy: bool = True,\n1279         fill_value=dtypes.NA,\n1280     ) -> \"DataArray\":\n1281         \"\"\"Conform this object onto the indexes of another object, filling in\n1282         missing values with ``fill_value``. The default fill value is NaN.\n1283 \n1284         Parameters\n1285         ----------\n1286         other : Dataset or DataArray\n1287             Object with an 'indexes' attribute giving a mapping from dimension\n1288             names to pandas.Index objects, which provides coordinates upon\n1289             which to index the variables in this dataset. The indexes on this\n1290             other object need not be the same as the indexes on this\n1291             dataset. Any mis-matched index values will be filled in with\n1292             NaN, and any mis-matched dimension names will simply be ignored.\n1293         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1294             Method to use for filling index values from other not found on this\n1295             data array:\n1296 \n1297             * None (default): don't fill gaps\n1298             * pad / ffill: propagate last valid index value forward\n1299             * backfill / bfill: propagate next valid index value backward\n1300             * nearest: use nearest valid index value\n1301         tolerance : optional\n1302             Maximum distance between original and new labels for inexact\n1303             matches. The values of the index at the matching locations must\n1304             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1305         copy : bool, optional\n1306             If ``copy=True``, data in the return value is always copied. If\n1307             ``copy=False`` and reindexing is unnecessary, or can be performed\n1308             with only slice operations, then the output may share memory with\n1309             the input. In either case, a new xarray object is always returned.\n1310         fill_value : scalar, optional\n1311             Value to use for newly missing values\n1312 \n1313         Returns\n1314         -------\n1315         reindexed : DataArray\n1316             Another dataset array, with this array's data but coordinates from\n1317             the other object.\n1318 \n1319         See Also\n1320         --------\n1321         DataArray.reindex\n1322         align\n1323         \"\"\"\n1324         indexers = reindex_like_indexers(self, other)\n1325         return self.reindex(\n1326             indexers=indexers,\n1327             method=method,\n1328             tolerance=tolerance,\n1329             copy=copy,\n1330             fill_value=fill_value,\n1331         )\n1332 \n1333     def reindex(\n1334         self,\n1335         indexers: Mapping[Hashable, Any] = None,\n1336         method: str = None,\n1337         tolerance=None,\n1338         copy: bool = True,\n1339         fill_value=dtypes.NA,\n1340         **indexers_kwargs: Any,\n1341     ) -> \"DataArray\":\n1342         \"\"\"Conform this object onto the indexes of another object, filling in\n1343         missing values with ``fill_value``. The default fill value is NaN.\n1344 \n1345         Parameters\n1346         ----------\n1347         indexers : dict, optional\n1348             Dictionary with keys given by dimension names and values given by\n1349             arrays of coordinates tick labels. Any mis-matched coordinate\n1350             values will be filled in with NaN, and any mis-matched dimension\n1351             names will simply be ignored.\n1352             One of indexers or indexers_kwargs must be provided.\n1353         copy : bool, optional\n1354             If ``copy=True``, data in the return value is always copied. If\n1355             ``copy=False`` and reindexing is unnecessary, or can be performed\n1356             with only slice operations, then the output may share memory with\n1357             the input. In either case, a new xarray object is always returned.\n1358         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1359             Method to use for filling index values in ``indexers`` not found on\n1360             this data array:\n1361 \n1362             * None (default): don't fill gaps\n1363             * pad / ffill: propagate last valid index value forward\n1364             * backfill / bfill: propagate next valid index value backward\n1365             * nearest: use nearest valid index value\n1366         tolerance : optional\n1367             Maximum distance between original and new labels for inexact\n1368             matches. The values of the index at the matching locations must\n1369             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1370         fill_value : scalar, optional\n1371             Value to use for newly missing values\n1372         **indexers_kwargs : {dim: indexer, ...}, optional\n1373             The keyword arguments form of ``indexers``.\n1374             One of indexers or indexers_kwargs must be provided.\n1375 \n1376         Returns\n1377         -------\n1378         reindexed : DataArray\n1379             Another dataset array, with this array's data but replaced\n1380             coordinates.\n1381 \n1382         See Also\n1383         --------\n1384         DataArray.reindex_like\n1385         align\n1386         \"\"\"\n1387         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1388         ds = self._to_temp_dataset().reindex(\n1389             indexers=indexers,\n1390             method=method,\n1391             tolerance=tolerance,\n1392             copy=copy,\n1393             fill_value=fill_value,\n1394         )\n1395         return self._from_temp_dataset(ds)\n1396 \n1397     def interp(\n1398         self,\n1399         coords: Mapping[Hashable, Any] = None,\n1400         method: str = \"linear\",\n1401         assume_sorted: bool = False,\n1402         kwargs: Mapping[str, Any] = None,\n1403         **coords_kwargs: Any,\n1404     ) -> \"DataArray\":\n1405         \"\"\" Multidimensional interpolation of variables.\n1406 \n1407         coords : dict, optional\n1408             Mapping from dimension names to the new coordinates.\n1409             new coordinate can be an scalar, array-like or DataArray.\n1410             If DataArrays are passed as new coordates, their dimensions are\n1411             used for the broadcasting.\n1412         method: {'linear', 'nearest'} for multidimensional array,\n1413             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1414             for 1-dimensional array.\n1415         assume_sorted: boolean, optional\n1416             If False, values of x can be in any order and they are sorted\n1417             first. If True, x has to be an array of monotonically increasing\n1418             values.\n1419         kwargs: dictionary\n1420             Additional keyword arguments passed to scipy's interpolator. Valid\n1421             options and their behavior depend on if 1-dimensional or\n1422             multi-dimensional interpolation is used.\n1423         ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n1424             The keyword arguments form of ``coords``.\n1425             One of coords or coords_kwargs must be provided.\n1426 \n1427         Returns\n1428         -------\n1429         interpolated: xr.DataArray\n1430             New dataarray on the new coordinates.\n1431 \n1432         Notes\n1433         -----\n1434         scipy is required.\n1435 \n1436         See Also\n1437         --------\n1438         scipy.interpolate.interp1d\n1439         scipy.interpolate.interpn\n1440 \n1441         Examples\n1442         --------\n1443         >>> da = xr.DataArray([1, 3], [(\"x\", np.arange(2))])\n1444         >>> da.interp(x=0.5)\n1445         <xarray.DataArray ()>\n1446         array(2.0)\n1447         Coordinates:\n1448             x        float64 0.5\n1449         \"\"\"\n1450         if self.dtype.kind not in \"uifc\":\n1451             raise TypeError(\n1452                 \"interp only works for a numeric type array. \"\n1453                 \"Given {}.\".format(self.dtype)\n1454             )\n1455         ds = self._to_temp_dataset().interp(\n1456             coords,\n1457             method=method,\n1458             kwargs=kwargs,\n1459             assume_sorted=assume_sorted,\n1460             **coords_kwargs,\n1461         )\n1462         return self._from_temp_dataset(ds)\n1463 \n1464     def interp_like(\n1465         self,\n1466         other: Union[\"DataArray\", Dataset],\n1467         method: str = \"linear\",\n1468         assume_sorted: bool = False,\n1469         kwargs: Mapping[str, Any] = None,\n1470     ) -> \"DataArray\":\n1471         \"\"\"Interpolate this object onto the coordinates of another object,\n1472         filling out of range values with NaN.\n1473 \n1474         Parameters\n1475         ----------\n1476         other : Dataset or DataArray\n1477             Object with an 'indexes' attribute giving a mapping from dimension\n1478             names to an 1d array-like, which provides coordinates upon\n1479             which to index the variables in this dataset.\n1480         method: string, optional.\n1481             {'linear', 'nearest'} for multidimensional array,\n1482             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1483             for 1-dimensional array. 'linear' is used by default.\n1484         assume_sorted: boolean, optional\n1485             If False, values of coordinates that are interpolated over can be\n1486             in any order and they are sorted first. If True, interpolated\n1487             coordinates are assumed to be an array of monotonically increasing\n1488             values.\n1489         kwargs: dictionary, optional\n1490             Additional keyword passed to scipy's interpolator.\n1491 \n1492         Returns\n1493         -------\n1494         interpolated: xr.DataArray\n1495             Another dataarray by interpolating this dataarray's data along the\n1496             coordinates of the other object.\n1497 \n1498         Notes\n1499         -----\n1500         scipy is required.\n1501         If the dataarray has object-type coordinates, reindex is used for these\n1502         coordinates instead of the interpolation.\n1503 \n1504         See Also\n1505         --------\n1506         DataArray.interp\n1507         DataArray.reindex_like\n1508         \"\"\"\n1509         if self.dtype.kind not in \"uifc\":\n1510             raise TypeError(\n1511                 \"interp only works for a numeric type array. \"\n1512                 \"Given {}.\".format(self.dtype)\n1513             )\n1514         ds = self._to_temp_dataset().interp_like(\n1515             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1516         )\n1517         return self._from_temp_dataset(ds)\n1518 \n1519     def rename(\n1520         self,\n1521         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n1522         **names: Hashable,\n1523     ) -> \"DataArray\":\n1524         \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n1525 \n1526         Parameters\n1527         ----------\n1528         new_name_or_name_dict : str or dict-like, optional\n1529             If the argument is dict-like, it used as a mapping from old\n1530             names to new names for coordinates. Otherwise, use the argument\n1531             as the new name for this array.\n1532         **names: hashable, optional\n1533             The keyword arguments form of a mapping from old names to\n1534             new names for coordinates.\n1535             One of new_name_or_name_dict or names must be provided.\n1536 \n1537         Returns\n1538         -------\n1539         renamed : DataArray\n1540             Renamed array or array with renamed coordinates.\n1541 \n1542         See Also\n1543         --------\n1544         Dataset.rename\n1545         DataArray.swap_dims\n1546         \"\"\"\n1547         if names or utils.is_dict_like(new_name_or_name_dict):\n1548             new_name_or_name_dict = cast(\n1549                 Mapping[Hashable, Hashable], new_name_or_name_dict\n1550             )\n1551             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n1552             dataset = self._to_temp_dataset().rename(name_dict)\n1553             return self._from_temp_dataset(dataset)\n1554         else:\n1555             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n1556             return self._replace(name=new_name_or_name_dict)\n1557 \n1558     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> \"DataArray\":\n1559         \"\"\"Returns a new DataArray with swapped dimensions.\n1560 \n1561         Parameters\n1562         ----------\n1563         dims_dict : dict-like\n1564             Dictionary whose keys are current dimension names and whose values\n1565             are new names.\n1566 \n1567         Returns\n1568         -------\n1569         swapped : DataArray\n1570             DataArray with swapped dimensions.\n1571 \n1572         Examples\n1573         --------\n1574 \n1575         >>> arr = xr.DataArray(\n1576         ...     data=[0, 1], dims=\"x\", coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n1577         ... )\n1578         >>> arr\n1579         <xarray.DataArray (x: 2)>\n1580         array([0, 1])\n1581         Coordinates:\n1582           * x        (x) <U1 'a' 'b'\n1583             y        (x) int64 0 1\n1584 \n1585         >>> arr.swap_dims({\"x\": \"y\"})\n1586         <xarray.DataArray (y: 2)>\n1587         array([0, 1])\n1588         Coordinates:\n1589             x        (y) <U1 'a' 'b'\n1590           * y        (y) int64 0 1\n1591 \n1592         >>> arr.swap_dims({\"x\": \"z\"})\n1593         <xarray.DataArray (z: 2)>\n1594         array([0, 1])\n1595         Coordinates:\n1596             x        (z) <U1 'a' 'b'\n1597             y        (z) int64 0 1\n1598         Dimensions without coordinates: z\n1599 \n1600         See Also\n1601         --------\n1602 \n1603         DataArray.rename\n1604         Dataset.swap_dims\n1605         \"\"\"\n1606         ds = self._to_temp_dataset().swap_dims(dims_dict)\n1607         return self._from_temp_dataset(ds)\n1608 \n1609     def expand_dims(\n1610         self,\n1611         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n1612         axis=None,\n1613         **dim_kwargs: Any,\n1614     ) -> \"DataArray\":\n1615         \"\"\"Return a new object with an additional axis (or axes) inserted at\n1616         the corresponding position in the array shape. The new object is a\n1617         view into the underlying array, not a copy.\n1618 \n1619 \n1620         If dim is already a scalar coordinate, it will be promoted to a 1D\n1621         coordinate consisting of a single value.\n1622 \n1623         Parameters\n1624         ----------\n1625         dim : hashable, sequence of hashable, dict, or None\n1626             Dimensions to include on the new variable.\n1627             If provided as str or sequence of str, then dimensions are inserted\n1628             with length 1. If provided as a dict, then the keys are the new\n1629             dimensions and the values are either integers (giving the length of\n1630             the new dimensions) or sequence/ndarray (giving the coordinates of\n1631             the new dimensions).\n1632         axis : integer, list (or tuple) of integers, or None\n1633             Axis position(s) where new axis is to be inserted (position(s) on\n1634             the result array). If a list (or tuple) of integers is passed,\n1635             multiple axes are inserted. In this case, dim arguments should be\n1636             same length list. If axis=None is passed, all the axes will be\n1637             inserted to the start of the result array.\n1638         **dim_kwargs : int or sequence/ndarray\n1639             The keywords are arbitrary dimensions being inserted and the values\n1640             are either the lengths of the new dims (if int is given), or their\n1641             coordinates. Note, this is an alternative to passing a dict to the\n1642             dim kwarg and will only be used if dim is None.\n1643 \n1644         Returns\n1645         -------\n1646         expanded : same type as caller\n1647             This object, but with an additional dimension(s).\n1648         \"\"\"\n1649         if isinstance(dim, int):\n1650             raise TypeError(\"dim should be hashable or sequence/mapping of hashables\")\n1651         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n1652             if len(dim) != len(set(dim)):\n1653                 raise ValueError(\"dims should not contain duplicate values.\")\n1654             dim = dict.fromkeys(dim, 1)\n1655         elif dim is not None and not isinstance(dim, Mapping):\n1656             dim = {cast(Hashable, dim): 1}\n1657 \n1658         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n1659         ds = self._to_temp_dataset().expand_dims(dim, axis)\n1660         return self._from_temp_dataset(ds)\n1661 \n1662     def set_index(\n1663         self,\n1664         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n1665         append: bool = False,\n1666         inplace: bool = None,\n1667         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n1668     ) -> Optional[\"DataArray\"]:\n1669         \"\"\"Set DataArray (multi-)indexes using one or more existing\n1670         coordinates.\n1671 \n1672         Parameters\n1673         ----------\n1674         indexes : {dim: index, ...}\n1675             Mapping from names matching dimensions and values given\n1676             by (lists of) the names of existing coordinates or variables to set\n1677             as new (multi-)index.\n1678         append : bool, optional\n1679             If True, append the supplied index(es) to the existing index(es).\n1680             Otherwise replace the existing index(es) (default).\n1681         **indexes_kwargs: optional\n1682             The keyword arguments form of ``indexes``.\n1683             One of indexes or indexes_kwargs must be provided.\n1684 \n1685         Returns\n1686         -------\n1687         obj : DataArray\n1688             Another DataArray, with this data but replaced coordinates.\n1689 \n1690         Examples\n1691         --------\n1692         >>> arr = xr.DataArray(\n1693         ...     data=np.ones((2, 3)),\n1694         ...     dims=[\"x\", \"y\"],\n1695         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n1696         ... )\n1697         >>> arr\n1698         <xarray.DataArray (x: 2, y: 3)>\n1699         array([[1., 1., 1.],\n1700                [1., 1., 1.]])\n1701         Coordinates:\n1702           * x        (x) int64 0 1\n1703           * y        (y) int64 0 1 2\n1704             a        (x) int64 3 4\n1705         >>> arr.set_index(x=\"a\")\n1706         <xarray.DataArray (x: 2, y: 3)>\n1707         array([[1., 1., 1.],\n1708                [1., 1., 1.]])\n1709         Coordinates:\n1710           * x        (x) int64 3 4\n1711           * y        (y) int64 0 1 2\n1712 \n1713         See Also\n1714         --------\n1715         DataArray.reset_index\n1716         \"\"\"\n1717         ds = self._to_temp_dataset().set_index(\n1718             indexes, append=append, inplace=inplace, **indexes_kwargs\n1719         )\n1720         return self._from_temp_dataset(ds)\n1721 \n1722     def reset_index(\n1723         self,\n1724         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n1725         drop: bool = False,\n1726         inplace: bool = None,\n1727     ) -> Optional[\"DataArray\"]:\n1728         \"\"\"Reset the specified index(es) or multi-index level(s).\n1729 \n1730         Parameters\n1731         ----------\n1732         dims_or_levels : hashable or sequence of hashables\n1733             Name(s) of the dimension(s) and/or multi-index level(s) that will\n1734             be reset.\n1735         drop : bool, optional\n1736             If True, remove the specified indexes and/or multi-index levels\n1737             instead of extracting them as new coordinates (default: False).\n1738 \n1739         Returns\n1740         -------\n1741         obj : DataArray\n1742             Another dataarray, with this dataarray's data but replaced\n1743             coordinates.\n1744 \n1745         See Also\n1746         --------\n1747         DataArray.set_index\n1748         \"\"\"\n1749         _check_inplace(inplace)\n1750         coords, _ = split_indexes(\n1751             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n1752         )\n1753         return self._replace(coords=coords)\n1754 \n1755     def reorder_levels(\n1756         self,\n1757         dim_order: Mapping[Hashable, Sequence[int]] = None,\n1758         inplace: bool = None,\n1759         **dim_order_kwargs: Sequence[int],\n1760     ) -> \"DataArray\":\n1761         \"\"\"Rearrange index levels using input order.\n1762 \n1763         Parameters\n1764         ----------\n1765         dim_order : optional\n1766             Mapping from names matching dimensions and values given\n1767             by lists representing new level orders. Every given dimension\n1768             must have a multi-index.\n1769         **dim_order_kwargs: optional\n1770             The keyword arguments form of ``dim_order``.\n1771             One of dim_order or dim_order_kwargs must be provided.\n1772 \n1773         Returns\n1774         -------\n1775         obj : DataArray\n1776             Another dataarray, with this dataarray's data but replaced\n1777             coordinates.\n1778         \"\"\"\n1779         _check_inplace(inplace)\n1780         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n1781         replace_coords = {}\n1782         for dim, order in dim_order.items():\n1783             coord = self._coords[dim]\n1784             index = coord.to_index()\n1785             if not isinstance(index, pd.MultiIndex):\n1786                 raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n1787             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n1788         coords = self._coords.copy()\n1789         coords.update(replace_coords)\n1790         return self._replace(coords=coords)\n1791 \n1792     def stack(\n1793         self,\n1794         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n1795         **dimensions_kwargs: Sequence[Hashable],\n1796     ) -> \"DataArray\":\n1797         \"\"\"\n1798         Stack any number of existing dimensions into a single new dimension.\n1799 \n1800         New dimensions will be added at the end, and the corresponding\n1801         coordinate variables will be combined into a MultiIndex.\n1802 \n1803         Parameters\n1804         ----------\n1805         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n1806             Names of new dimensions, and the existing dimensions that they\n1807             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n1808             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n1809             all dimensions.\n1810         **dimensions_kwargs:\n1811             The keyword arguments form of ``dimensions``.\n1812             One of dimensions or dimensions_kwargs must be provided.\n1813 \n1814         Returns\n1815         -------\n1816         stacked : DataArray\n1817             DataArray with stacked data.\n1818 \n1819         Examples\n1820         --------\n1821 \n1822         >>> arr = xr.DataArray(\n1823         ...     np.arange(6).reshape(2, 3),\n1824         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1825         ... )\n1826         >>> arr\n1827         <xarray.DataArray (x: 2, y: 3)>\n1828         array([[0, 1, 2],\n1829                [3, 4, 5]])\n1830         Coordinates:\n1831           * x        (x) |S1 'a' 'b'\n1832           * y        (y) int64 0 1 2\n1833         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1834         >>> stacked.indexes[\"z\"]\n1835         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1836                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1837                    names=['x', 'y'])\n1838 \n1839         See Also\n1840         --------\n1841         DataArray.unstack\n1842         \"\"\"\n1843         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n1844         return self._from_temp_dataset(ds)\n1845 \n1846     def unstack(\n1847         self,\n1848         dim: Union[Hashable, Sequence[Hashable], None] = None,\n1849         fill_value: Any = dtypes.NA,\n1850         sparse: bool = False,\n1851     ) -> \"DataArray\":\n1852         \"\"\"\n1853         Unstack existing dimensions corresponding to MultiIndexes into\n1854         multiple new dimensions.\n1855 \n1856         New dimensions will be added at the end.\n1857 \n1858         Parameters\n1859         ----------\n1860         dim : hashable or sequence of hashable, optional\n1861             Dimension(s) over which to unstack. By default unstacks all\n1862             MultiIndexes.\n1863         fill_value: value to be filled. By default, np.nan\n1864         sparse: use sparse-array if True\n1865 \n1866         Returns\n1867         -------\n1868         unstacked : DataArray\n1869             Array with unstacked data.\n1870 \n1871         Examples\n1872         --------\n1873 \n1874         >>> arr = xr.DataArray(\n1875         ...     np.arange(6).reshape(2, 3),\n1876         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1877         ... )\n1878         >>> arr\n1879         <xarray.DataArray (x: 2, y: 3)>\n1880         array([[0, 1, 2],\n1881                [3, 4, 5]])\n1882         Coordinates:\n1883           * x        (x) |S1 'a' 'b'\n1884           * y        (y) int64 0 1 2\n1885         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1886         >>> stacked.indexes[\"z\"]\n1887         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1888                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1889                    names=['x', 'y'])\n1890         >>> roundtripped = stacked.unstack()\n1891         >>> arr.identical(roundtripped)\n1892         True\n1893 \n1894         See Also\n1895         --------\n1896         DataArray.stack\n1897         \"\"\"\n1898         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n1899         return self._from_temp_dataset(ds)\n1900 \n1901     def to_unstacked_dataset(self, dim, level=0):\n1902         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n1903         stacked coordinate.\n1904 \n1905         This is the inverse operation of Dataset.to_stacked_array.\n1906 \n1907         Parameters\n1908         ----------\n1909         dim : str\n1910             Name of existing dimension to unstack\n1911         level : int or str\n1912             The MultiIndex level to expand to a dataset along. Can either be\n1913             the integer index of the level or its name.\n1914         label : int, default 0\n1915             Label of the level to expand dataset along. Overrides the label\n1916             argument if given.\n1917 \n1918         Returns\n1919         -------\n1920         unstacked: Dataset\n1921 \n1922         Examples\n1923         --------\n1924         >>> import xarray as xr\n1925         >>> arr = xr.DataArray(\n1926         ...     np.arange(6).reshape(2, 3),\n1927         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1928         ... )\n1929         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n1930         >>> data\n1931         <xarray.Dataset>\n1932         Dimensions:  (x: 2, y: 3)\n1933         Coordinates:\n1934           * x        (x) <U1 'a' 'b'\n1935           * y        (y) int64 0 1 2\n1936         Data variables:\n1937             a        (x, y) int64 0 1 2 3 4 5\n1938             b        (x) int64 0 3\n1939         >>> stacked = data.to_stacked_array(\"z\", [\"y\"])\n1940         >>> stacked.indexes[\"z\"]\n1941         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1942                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n1943                 names=['variable', 'y'])\n1944         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n1945         >>> data.identical(roundtripped)\n1946         True\n1947 \n1948         See Also\n1949         --------\n1950         Dataset.to_stacked_array\n1951         \"\"\"\n1952 \n1953         idx = self.indexes[dim]\n1954         if not isinstance(idx, pd.MultiIndex):\n1955             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n1956 \n1957         level_number = idx._get_level_number(level)\n1958         variables = idx.levels[level_number]\n1959         variable_dim = idx.names[level_number]\n1960 \n1961         # pull variables out of datarray\n1962         data_dict = {}\n1963         for k in variables:\n1964             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n1965 \n1966         # unstacked dataset\n1967         return Dataset(data_dict)\n1968 \n1969     def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> \"DataArray\":\n1970         \"\"\"Return a new DataArray object with transposed dimensions.\n1971 \n1972         Parameters\n1973         ----------\n1974         *dims : hashable, optional\n1975             By default, reverse the dimensions. Otherwise, reorder the\n1976             dimensions to this order.\n1977         transpose_coords : boolean, default True\n1978             If True, also transpose the coordinates of this DataArray.\n1979 \n1980         Returns\n1981         -------\n1982         transposed : DataArray\n1983             The returned DataArray's array is transposed.\n1984 \n1985         Notes\n1986         -----\n1987         This operation returns a view of this array's data. It is\n1988         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n1989         -- the data will be fully loaded.\n1990 \n1991         See Also\n1992         --------\n1993         numpy.transpose\n1994         Dataset.transpose\n1995         \"\"\"\n1996         if dims:\n1997             dims = tuple(utils.infix_dims(dims, self.dims))\n1998         variable = self.variable.transpose(*dims)\n1999         if transpose_coords:\n2000             coords: Dict[Hashable, Variable] = {}\n2001             for name, coord in self.coords.items():\n2002                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2003                 coords[name] = coord.variable.transpose(*coord_dims)\n2004             return self._replace(variable, coords)\n2005         else:\n2006             return self._replace(variable)\n2007 \n2008     @property\n2009     def T(self) -> \"DataArray\":\n2010         return self.transpose()\n2011 \n2012     def drop_vars(\n2013         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n2014     ) -> \"DataArray\":\n2015         \"\"\"Drop variables from this DataArray.\n2016 \n2017         Parameters\n2018         ----------\n2019         names : hashable or iterable of hashables\n2020             Name(s) of variables to drop.\n2021         errors: {'raise', 'ignore'}, optional\n2022             If 'raise' (default), raises a ValueError error if any of the variable\n2023             passed are not in the dataset. If 'ignore', any given names that are in the\n2024             DataArray are dropped and no error is raised.\n2025 \n2026         Returns\n2027         -------\n2028         dropped : Dataset\n2029 \n2030         \"\"\"\n2031         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n2032         return self._from_temp_dataset(ds)\n2033 \n2034     def drop(\n2035         self,\n2036         labels: Mapping = None,\n2037         dim: Hashable = None,\n2038         *,\n2039         errors: str = \"raise\",\n2040         **labels_kwargs,\n2041     ) -> \"DataArray\":\n2042         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n2043 \n2044         Using either `drop_vars` or `drop_sel` is encouraged\n2045 \n2046         See Also\n2047         --------\n2048         DataArray.drop_vars\n2049         DataArray.drop_sel\n2050         \"\"\"\n2051         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n2052         return self._from_temp_dataset(ds)\n2053 \n2054     def drop_sel(\n2055         self,\n2056         labels: Mapping[Hashable, Any] = None,\n2057         *,\n2058         errors: str = \"raise\",\n2059         **labels_kwargs,\n2060     ) -> \"DataArray\":\n2061         \"\"\"Drop index labels from this DataArray.\n2062 \n2063         Parameters\n2064         ----------\n2065         labels : Mapping[Hashable, Any]\n2066             Index labels to drop\n2067         errors: {'raise', 'ignore'}, optional\n2068             If 'raise' (default), raises a ValueError error if\n2069             any of the index labels passed are not\n2070             in the dataset. If 'ignore', any given labels that are in the\n2071             dataset are dropped and no error is raised.\n2072         **labels_kwargs : {dim: label, ...}, optional\n2073             The keyword arguments form of ``dim`` and ``labels``\n2074 \n2075         Returns\n2076         -------\n2077         dropped : DataArray\n2078         \"\"\"\n2079         if labels_kwargs or isinstance(labels, dict):\n2080             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n2081 \n2082         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n2083         return self._from_temp_dataset(ds)\n2084 \n2085     def dropna(\n2086         self, dim: Hashable, how: str = \"any\", thresh: int = None\n2087     ) -> \"DataArray\":\n2088         \"\"\"Returns a new array with dropped labels for missing values along\n2089         the provided dimension.\n2090 \n2091         Parameters\n2092         ----------\n2093         dim : hashable\n2094             Dimension along which to drop missing values. Dropping along\n2095             multiple dimensions simultaneously is not yet supported.\n2096         how : {'any', 'all'}, optional\n2097             * any : if any NA values are present, drop that label\n2098             * all : if all values are NA, drop that label\n2099         thresh : int, default None\n2100             If supplied, require this many non-NA values.\n2101 \n2102         Returns\n2103         -------\n2104         DataArray\n2105         \"\"\"\n2106         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2107         return self._from_temp_dataset(ds)\n2108 \n2109     def fillna(self, value: Any) -> \"DataArray\":\n2110         \"\"\"Fill missing values in this object.\n2111 \n2112         This operation follows the normal broadcasting and alignment rules that\n2113         xarray uses for binary arithmetic, except the result is aligned to this\n2114         object (``join='left'``) instead of aligned to the intersection of\n2115         index coordinates (``join='inner'``).\n2116 \n2117         Parameters\n2118         ----------\n2119         value : scalar, ndarray or DataArray\n2120             Used to fill all matching missing values in this array. If the\n2121             argument is a DataArray, it is first aligned with (reindexed to)\n2122             this array.\n2123 \n2124         Returns\n2125         -------\n2126         DataArray\n2127         \"\"\"\n2128         if utils.is_dict_like(value):\n2129             raise TypeError(\n2130                 \"cannot provide fill value as a dictionary with \"\n2131                 \"fillna on a DataArray\"\n2132             )\n2133         out = ops.fillna(self, value)\n2134         return out\n2135 \n2136     def interpolate_na(\n2137         self,\n2138         dim: Hashable = None,\n2139         method: str = \"linear\",\n2140         limit: int = None,\n2141         use_coordinate: Union[bool, str] = True,\n2142         max_gap: Union[\n2143             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n2144         ] = None,\n2145         keep_attrs: bool = None,\n2146         **kwargs: Any,\n2147     ) -> \"DataArray\":\n2148         \"\"\"Fill in NaNs by interpolating according to different methods.\n2149 \n2150         Parameters\n2151         ----------\n2152         dim : str\n2153             Specifies the dimension along which to interpolate.\n2154         method : str, optional\n2155             String indicating which method to use for interpolation:\n2156 \n2157             - 'linear': linear interpolation (Default). Additional keyword\n2158               arguments are passed to :py:func:`numpy.interp`\n2159             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2160               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2161               ``method='polynomial'``, the ``order`` keyword argument must also be\n2162               provided.\n2163             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2164               respective :py:class:`scipy.interpolate` classes.\n2165 \n2166         use_coordinate : bool, str, default True\n2167             Specifies which index to use as the x values in the interpolation\n2168             formulated as `y = f(x)`. If False, values are treated as if\n2169             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2170             used. If ``use_coordinate`` is a string, it specifies the name of a\n2171             coordinate variariable to use as the index.\n2172         limit : int, default None\n2173             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2174             or None for no limit. This filling is done regardless of the size of\n2175             the gap in the data. To only interpolate over gaps less than a given length,\n2176             see ``max_gap``.\n2177         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n2178             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2179             Use None for no limit. When interpolating along a datetime64 dimension\n2180             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2181 \n2182             - a string that is valid input for pandas.to_timedelta\n2183             - a :py:class:`numpy.timedelta64` object\n2184             - a :py:class:`pandas.Timedelta` object\n2185             - a :py:class:`datetime.timedelta` object\n2186 \n2187             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2188             dimensions has not been implemented yet. Gap length is defined as the difference\n2189             between coordinate values at the first data point after a gap and the last value\n2190             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2191             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2192             For example, consider::\n2193 \n2194                 <xarray.DataArray (x: 9)>\n2195                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2196                 Coordinates:\n2197                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2198 \n2199             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2200         keep_attrs : bool, default True\n2201             If True, the dataarray's attributes (`attrs`) will be copied from\n2202             the original object to the new one.  If False, the new\n2203             object will be returned without attributes.\n2204         kwargs : dict, optional\n2205             parameters passed verbatim to the underlying interpolation function\n2206 \n2207         Returns\n2208         -------\n2209         interpolated: DataArray\n2210             Filled in DataArray.\n2211 \n2212         See also\n2213         --------\n2214         numpy.interp\n2215         scipy.interpolate\n2216         \"\"\"\n2217         from .missing import interp_na\n2218 \n2219         return interp_na(\n2220             self,\n2221             dim=dim,\n2222             method=method,\n2223             limit=limit,\n2224             use_coordinate=use_coordinate,\n2225             max_gap=max_gap,\n2226             keep_attrs=keep_attrs,\n2227             **kwargs,\n2228         )\n2229 \n2230     def ffill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2231         \"\"\"Fill NaN values by propogating values forward\n2232 \n2233         *Requires bottleneck.*\n2234 \n2235         Parameters\n2236         ----------\n2237         dim : hashable\n2238             Specifies the dimension along which to propagate values when\n2239             filling.\n2240         limit : int, default None\n2241             The maximum number of consecutive NaN values to forward fill. In\n2242             other words, if there is a gap with more than this number of\n2243             consecutive NaNs, it will only be partially filled. Must be greater\n2244             than 0 or None for no limit.\n2245 \n2246         Returns\n2247         -------\n2248         DataArray\n2249         \"\"\"\n2250         from .missing import ffill\n2251 \n2252         return ffill(self, dim, limit=limit)\n2253 \n2254     def bfill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2255         \"\"\"Fill NaN values by propogating values backward\n2256 \n2257         *Requires bottleneck.*\n2258 \n2259         Parameters\n2260         ----------\n2261         dim : str\n2262             Specifies the dimension along which to propagate values when\n2263             filling.\n2264         limit : int, default None\n2265             The maximum number of consecutive NaN values to backward fill. In\n2266             other words, if there is a gap with more than this number of\n2267             consecutive NaNs, it will only be partially filled. Must be greater\n2268             than 0 or None for no limit.\n2269 \n2270         Returns\n2271         -------\n2272         DataArray\n2273         \"\"\"\n2274         from .missing import bfill\n2275 \n2276         return bfill(self, dim, limit=limit)\n2277 \n2278     def combine_first(self, other: \"DataArray\") -> \"DataArray\":\n2279         \"\"\"Combine two DataArray objects, with union of coordinates.\n2280 \n2281         This operation follows the normal broadcasting and alignment rules of\n2282         ``join='outer'``.  Default to non-null values of array calling the\n2283         method.  Use np.nan to fill in vacant cells after alignment.\n2284 \n2285         Parameters\n2286         ----------\n2287         other : DataArray\n2288             Used to fill all matching missing values in this array.\n2289 \n2290         Returns\n2291         -------\n2292         DataArray\n2293         \"\"\"\n2294         return ops.fillna(self, other, join=\"outer\")\n2295 \n2296     def reduce(\n2297         self,\n2298         func: Callable[..., Any],\n2299         dim: Union[None, Hashable, Sequence[Hashable]] = None,\n2300         axis: Union[None, int, Sequence[int]] = None,\n2301         keep_attrs: bool = None,\n2302         keepdims: bool = False,\n2303         **kwargs: Any,\n2304     ) -> \"DataArray\":\n2305         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2306 \n2307         Parameters\n2308         ----------\n2309         func : function\n2310             Function which can be called in the form\n2311             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2312             np.ndarray over an integer valued axis.\n2313         dim : hashable or sequence of hashables, optional\n2314             Dimension(s) over which to apply `func`.\n2315         axis : int or sequence of int, optional\n2316             Axis(es) over which to repeatedly apply `func`. Only one of the\n2317             'dim' and 'axis' arguments can be supplied. If neither are\n2318             supplied, then the reduction is calculated over the flattened array\n2319             (by calling `f(x)` without an axis argument).\n2320         keep_attrs : bool, optional\n2321             If True, the variable's attributes (`attrs`) will be copied from\n2322             the original object to the new one.  If False (default), the new\n2323             object will be returned without attributes.\n2324         keepdims : bool, default False\n2325             If True, the dimensions which are reduced are left in the result\n2326             as dimensions of size one. Coordinates that use these dimensions\n2327             are removed.\n2328         **kwargs : dict\n2329             Additional keyword arguments passed on to `func`.\n2330 \n2331         Returns\n2332         -------\n2333         reduced : DataArray\n2334             DataArray with this object's array replaced with an array with\n2335             summarized data and the indicated dimension(s) removed.\n2336         \"\"\"\n2337 \n2338         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2339         return self._replace_maybe_drop_dims(var)\n2340 \n2341     def to_pandas(self) -> Union[\"DataArray\", pd.Series, pd.DataFrame]:\n2342         \"\"\"Convert this array into a pandas object with the same shape.\n2343 \n2344         The type of the returned object depends on the number of DataArray\n2345         dimensions:\n2346 \n2347         * 0D -> `xarray.DataArray`\n2348         * 1D -> `pandas.Series`\n2349         * 2D -> `pandas.DataFrame`\n2350 \n2351         Only works for arrays with 2 or fewer dimensions.\n2352 \n2353         The DataArray constructor performs the inverse transformation.\n2354         \"\"\"\n2355         # TODO: consolidate the info about pandas constructors and the\n2356         # attributes that correspond to their indexes into a separate module?\n2357         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2358         try:\n2359             constructor = constructors[self.ndim]\n2360         except KeyError:\n2361             raise ValueError(\n2362                 \"cannot convert arrays with %s dimensions into \"\n2363                 \"pandas objects\" % self.ndim\n2364             )\n2365         indexes = [self.get_index(dim) for dim in self.dims]\n2366         return constructor(self.values, *indexes)\n2367 \n2368     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n2369         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2370 \n2371         The DataFrame is indexed by the Cartesian product of index coordinates\n2372         (in the form of a :py:class:`pandas.MultiIndex`).\n2373 \n2374         Other coordinates are included as columns in the DataFrame.\n2375         \"\"\"\n2376         if name is None:\n2377             name = self.name\n2378         if name is None:\n2379             raise ValueError(\n2380                 \"cannot convert an unnamed DataArray to a \"\n2381                 \"DataFrame: use the ``name`` parameter\"\n2382             )\n2383 \n2384         dims = dict(zip(self.dims, self.shape))\n2385         # By using a unique name, we can convert a DataArray into a DataFrame\n2386         # even if it shares a name with one of its coordinates.\n2387         # I would normally use unique_name = object() but that results in a\n2388         # dataframe with columns in the wrong order, for reasons I have not\n2389         # been able to debug (possibly a pandas bug?).\n2390         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n2391         ds = self._to_dataset_whole(name=unique_name)\n2392         df = ds._to_dataframe(dims)\n2393         df.columns = [name if c == unique_name else c for c in df.columns]\n2394         return df\n2395 \n2396     def to_series(self) -> pd.Series:\n2397         \"\"\"Convert this array into a pandas.Series.\n2398 \n2399         The Series is indexed by the Cartesian product of index coordinates\n2400         (in the form of a :py:class:`pandas.MultiIndex`).\n2401         \"\"\"\n2402         index = self.coords.to_index()\n2403         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n2404 \n2405     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n2406         \"\"\"Convert this array into a numpy.ma.MaskedArray\n2407 \n2408         Parameters\n2409         ----------\n2410         copy : bool\n2411             If True (default) make a copy of the array in the result. If False,\n2412             a MaskedArray view of DataArray.values is returned.\n2413 \n2414         Returns\n2415         -------\n2416         result : MaskedArray\n2417             Masked where invalid values (nan or inf) occur.\n2418         \"\"\"\n2419         values = self.values  # only compute lazy arrays once\n2420         isnull = pd.isnull(values)\n2421         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n2422 \n2423     def to_netcdf(self, *args, **kwargs) -> Union[bytes, \"Delayed\", None]:\n2424         \"\"\"Write DataArray contents to a netCDF file.\n2425 \n2426         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n2427 \n2428         Notes\n2429         -----\n2430         Only xarray.Dataset objects can be written to netCDF files, so\n2431         the xarray.DataArray is converted to a xarray.Dataset object\n2432         containing a single variable. If the DataArray has no name, or if the\n2433         name is the same as a co-ordinate name, then it is given the name\n2434         '__xarray_dataarray_variable__'.\n2435         \"\"\"\n2436         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n2437 \n2438         if self.name is None:\n2439             # If no name is set then use a generic xarray name\n2440             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2441         elif self.name in self.coords or self.name in self.dims:\n2442             # The name is the same as one of the coords names, which netCDF\n2443             # doesn't support, so rename it but keep track of the old name\n2444             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2445             dataset.attrs[DATAARRAY_NAME] = self.name\n2446         else:\n2447             # No problems with the name - so we're fine!\n2448             dataset = self.to_dataset()\n2449 \n2450         return dataset.to_netcdf(*args, **kwargs)\n2451 \n2452     def to_dict(self, data: bool = True) -> dict:\n2453         \"\"\"\n2454         Convert this xarray.DataArray into a dictionary following xarray\n2455         naming conventions.\n2456 \n2457         Converts all variables and attributes to native Python objects.\n2458         Useful for converting to json. To avoid datetime incompatibility\n2459         use decode_times=False kwarg in xarrray.open_dataset.\n2460 \n2461         Parameters\n2462         ----------\n2463         data : bool, optional\n2464             Whether to include the actual data in the dictionary. When set to\n2465             False, returns just the schema.\n2466 \n2467         See also\n2468         --------\n2469         DataArray.from_dict\n2470         \"\"\"\n2471         d = self.variable.to_dict(data=data)\n2472         d.update({\"coords\": {}, \"name\": self.name})\n2473         for k in self.coords:\n2474             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n2475         return d\n2476 \n2477     @classmethod\n2478     def from_dict(cls, d: dict) -> \"DataArray\":\n2479         \"\"\"\n2480         Convert a dictionary into an xarray.DataArray\n2481 \n2482         Input dict can take several forms::\n2483 \n2484             d = {'dims': ('t'), 'data': x}\n2485 \n2486             d = {'coords': {'t': {'dims': 't', 'data': t,\n2487                                   'attrs': {'units':'s'}}},\n2488                  'attrs': {'title': 'air temperature'},\n2489                  'dims': 't',\n2490                  'data': x,\n2491                  'name': 'a'}\n2492 \n2493         where 't' is the name of the dimesion, 'a' is the name of the array,\n2494         and  x and t are lists, numpy.arrays, or pandas objects.\n2495 \n2496         Parameters\n2497         ----------\n2498         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n2499 \n2500         Returns\n2501         -------\n2502         obj : xarray.DataArray\n2503 \n2504         See also\n2505         --------\n2506         DataArray.to_dict\n2507         Dataset.from_dict\n2508         \"\"\"\n2509         coords = None\n2510         if \"coords\" in d:\n2511             try:\n2512                 coords = {\n2513                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n2514                     for k, v in d[\"coords\"].items()\n2515                 }\n2516             except KeyError as e:\n2517                 raise ValueError(\n2518                     \"cannot convert dict when coords are missing the key \"\n2519                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n2520                 )\n2521         try:\n2522             data = d[\"data\"]\n2523         except KeyError:\n2524             raise ValueError(\"cannot convert dict without the key 'data''\")\n2525         else:\n2526             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n2527         return obj\n2528 \n2529     @classmethod\n2530     def from_series(cls, series: pd.Series, sparse: bool = False) -> \"DataArray\":\n2531         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n2532 \n2533         If the series's index is a MultiIndex, it will be expanded into a\n2534         tensor product of one-dimensional coordinates (filling in missing\n2535         values with NaN). Thus this operation should be the inverse of the\n2536         `to_series` method.\n2537 \n2538         If sparse=True, creates a sparse array instead of a dense NumPy array.\n2539         Requires the pydata/sparse package.\n2540 \n2541         See also\n2542         --------\n2543         xarray.Dataset.from_dataframe\n2544         \"\"\"\n2545         temp_name = \"__temporary_name\"\n2546         df = pd.DataFrame({temp_name: series})\n2547         ds = Dataset.from_dataframe(df, sparse=sparse)\n2548         result = cast(DataArray, ds[temp_name])\n2549         result.name = series.name\n2550         return result\n2551 \n2552     def to_cdms2(self) -> \"cdms2_Variable\":\n2553         \"\"\"Convert this array into a cdms2.Variable\n2554         \"\"\"\n2555         from ..convert import to_cdms2\n2556 \n2557         return to_cdms2(self)\n2558 \n2559     @classmethod\n2560     def from_cdms2(cls, variable: \"cdms2_Variable\") -> \"DataArray\":\n2561         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\n2562         \"\"\"\n2563         from ..convert import from_cdms2\n2564 \n2565         return from_cdms2(variable)\n2566 \n2567     def to_iris(self) -> \"iris_Cube\":\n2568         \"\"\"Convert this array into a iris.cube.Cube\n2569         \"\"\"\n2570         from ..convert import to_iris\n2571 \n2572         return to_iris(self)\n2573 \n2574     @classmethod\n2575     def from_iris(cls, cube: \"iris_Cube\") -> \"DataArray\":\n2576         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\n2577         \"\"\"\n2578         from ..convert import from_iris\n2579 \n2580         return from_iris(cube)\n2581 \n2582     def _all_compat(self, other: \"DataArray\", compat_str: str) -> bool:\n2583         \"\"\"Helper function for equals, broadcast_equals, and identical\n2584         \"\"\"\n2585 \n2586         def compat(x, y):\n2587             return getattr(x.variable, compat_str)(y.variable)\n2588 \n2589         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n2590             self, other\n2591         )\n2592 \n2593     def broadcast_equals(self, other: \"DataArray\") -> bool:\n2594         \"\"\"Two DataArrays are broadcast equal if they are equal after\n2595         broadcasting them against each other such that they have the same\n2596         dimensions.\n2597 \n2598         See Also\n2599         --------\n2600         DataArray.equals\n2601         DataArray.identical\n2602         \"\"\"\n2603         try:\n2604             return self._all_compat(other, \"broadcast_equals\")\n2605         except (TypeError, AttributeError):\n2606             return False\n2607 \n2608     def equals(self, other: \"DataArray\") -> bool:\n2609         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n2610         values; otherwise False.\n2611 \n2612         DataArrays can still be equal (like pandas objects) if they have NaN\n2613         values in the same locations.\n2614 \n2615         This method is necessary because `v1 == v2` for ``DataArray``\n2616         does element-wise comparisons (like numpy.ndarrays).\n2617 \n2618         See Also\n2619         --------\n2620         DataArray.broadcast_equals\n2621         DataArray.identical\n2622         \"\"\"\n2623         try:\n2624             return self._all_compat(other, \"equals\")\n2625         except (TypeError, AttributeError):\n2626             return False\n2627 \n2628     def identical(self, other: \"DataArray\") -> bool:\n2629         \"\"\"Like equals, but also checks the array name and attributes, and\n2630         attributes on all coordinates.\n2631 \n2632         See Also\n2633         --------\n2634         DataArray.broadcast_equals\n2635         DataArray.equal\n2636         \"\"\"\n2637         try:\n2638             return self.name == other.name and self._all_compat(other, \"identical\")\n2639         except (TypeError, AttributeError):\n2640             return False\n2641 \n2642     def _result_name(self, other: Any = None) -> Optional[Hashable]:\n2643         # use the same naming heuristics as pandas:\n2644         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n2645         other_name = getattr(other, \"name\", _default)\n2646         if other_name is _default or other_name == self.name:\n2647             return self.name\n2648         else:\n2649             return None\n2650 \n2651     def __array_wrap__(self, obj, context=None) -> \"DataArray\":\n2652         new_var = self.variable.__array_wrap__(obj, context)\n2653         return self._replace(new_var)\n2654 \n2655     def __matmul__(self, obj):\n2656         return self.dot(obj)\n2657 \n2658     def __rmatmul__(self, other):\n2659         # currently somewhat duplicative, as only other DataArrays are\n2660         # compatible with matmul\n2661         return computation.dot(other, self)\n2662 \n2663     @staticmethod\n2664     def _unary_op(f: Callable[..., Any]) -> Callable[..., \"DataArray\"]:\n2665         @functools.wraps(f)\n2666         def func(self, *args, **kwargs):\n2667             with np.errstate(all=\"ignore\"):\n2668                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n2669 \n2670         return func\n2671 \n2672     @staticmethod\n2673     def _binary_op(\n2674         f: Callable[..., Any],\n2675         reflexive: bool = False,\n2676         join: str = None,  # see xarray.align\n2677         **ignored_kwargs,\n2678     ) -> Callable[..., \"DataArray\"]:\n2679         @functools.wraps(f)\n2680         def func(self, other):\n2681             if isinstance(other, (Dataset, groupby.GroupBy)):\n2682                 return NotImplemented\n2683             if isinstance(other, DataArray):\n2684                 align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n2685                 self, other = align(self, other, join=align_type, copy=False)\n2686             other_variable = getattr(other, \"variable\", other)\n2687             other_coords = getattr(other, \"coords\", None)\n2688 \n2689             variable = (\n2690                 f(self.variable, other_variable)\n2691                 if not reflexive\n2692                 else f(other_variable, self.variable)\n2693             )\n2694             coords, indexes = self.coords._merge_raw(other_coords)\n2695             name = self._result_name(other)\n2696 \n2697             return self._replace(variable, coords, name, indexes=indexes)\n2698 \n2699         return func\n2700 \n2701     @staticmethod\n2702     def _inplace_binary_op(f: Callable) -> Callable[..., \"DataArray\"]:\n2703         @functools.wraps(f)\n2704         def func(self, other):\n2705             if isinstance(other, groupby.GroupBy):\n2706                 raise TypeError(\n2707                     \"in-place operations between a DataArray and \"\n2708                     \"a grouped object are not permitted\"\n2709                 )\n2710             # n.b. we can't align other to self (with other.reindex_like(self))\n2711             # because `other` may be converted into floats, which would cause\n2712             # in-place arithmetic to fail unpredictably. Instead, we simply\n2713             # don't support automatic alignment with in-place arithmetic.\n2714             other_coords = getattr(other, \"coords\", None)\n2715             other_variable = getattr(other, \"variable\", other)\n2716             try:\n2717                 with self.coords._merge_inplace(other_coords):\n2718                     f(self.variable, other_variable)\n2719             except MergeError as exc:\n2720                 raise MergeError(\n2721                     \"Automatic alignment is not supported for in-place operations.\\n\"\n2722                     \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n2723                     \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n2724                 ) from exc\n2725             return self\n2726 \n2727         return func\n2728 \n2729     def _copy_attrs_from(self, other: Union[\"DataArray\", Dataset, Variable]) -> None:\n2730         self.attrs = other.attrs\n2731 \n2732     plot = utils.UncachedAccessor(_PlotMethods)\n2733 \n2734     def _title_for_slice(self, truncate: int = 50) -> str:\n2735         \"\"\"\n2736         If the dataarray has 1 dimensional coordinates or comes from a slice\n2737         we can show that info in the title\n2738 \n2739         Parameters\n2740         ----------\n2741         truncate : integer\n2742             maximum number of characters for title\n2743 \n2744         Returns\n2745         -------\n2746         title : string\n2747             Can be used for plot titles\n2748 \n2749         \"\"\"\n2750         one_dims = []\n2751         for dim, coord in self.coords.items():\n2752             if coord.size == 1:\n2753                 one_dims.append(\n2754                     \"{dim} = {v}\".format(dim=dim, v=format_item(coord.values))\n2755                 )\n2756 \n2757         title = \", \".join(one_dims)\n2758         if len(title) > truncate:\n2759             title = title[: (truncate - 3)] + \"...\"\n2760 \n2761         return title\n2762 \n2763     def diff(self, dim: Hashable, n: int = 1, label: Hashable = \"upper\") -> \"DataArray\":\n2764         \"\"\"Calculate the n-th order discrete difference along given axis.\n2765 \n2766         Parameters\n2767         ----------\n2768         dim : hashable\n2769             Dimension over which to calculate the finite difference.\n2770         n : int, optional\n2771             The number of times values are differenced.\n2772         label : hashable, optional\n2773             The new coordinate in dimension ``dim`` will have the\n2774             values of either the minuend's or subtrahend's coordinate\n2775             for values 'upper' and 'lower', respectively.  Other\n2776             values are not supported.\n2777 \n2778         Returns\n2779         -------\n2780         difference : same type as caller\n2781             The n-th order finite difference of this object.\n2782 \n2783         .. note::\n2784 \n2785             `n` matches numpy's behavior and is different from pandas' first\n2786             argument named `periods`.\n2787 \n2788 \n2789         Examples\n2790         --------\n2791         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n2792         >>> arr.diff(\"x\")\n2793         <xarray.DataArray (x: 3)>\n2794         array([0, 1, 0])\n2795         Coordinates:\n2796         * x        (x) int64 2 3 4\n2797         >>> arr.diff(\"x\", 2)\n2798         <xarray.DataArray (x: 2)>\n2799         array([ 1, -1])\n2800         Coordinates:\n2801         * x        (x) int64 3 4\n2802 \n2803         See Also\n2804         --------\n2805         DataArray.differentiate\n2806         \"\"\"\n2807         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n2808         return self._from_temp_dataset(ds)\n2809 \n2810     def shift(\n2811         self,\n2812         shifts: Mapping[Hashable, int] = None,\n2813         fill_value: Any = dtypes.NA,\n2814         **shifts_kwargs: int,\n2815     ) -> \"DataArray\":\n2816         \"\"\"Shift this array by an offset along one or more dimensions.\n2817 \n2818         Only the data is moved; coordinates stay in place. Values shifted from\n2819         beyond array bounds are replaced by NaN. This is consistent with the\n2820         behavior of ``shift`` in pandas.\n2821 \n2822         Parameters\n2823         ----------\n2824         shifts : Mapping with the form of {dim: offset}\n2825             Integer offset to shift along each of the given dimensions.\n2826             Positive offsets shift to the right; negative offsets shift to the\n2827             left.\n2828         fill_value: scalar, optional\n2829             Value to use for newly missing values\n2830         **shifts_kwargs:\n2831             The keyword arguments form of ``shifts``.\n2832             One of shifts or shifts_kwargs must be provided.\n2833 \n2834         Returns\n2835         -------\n2836         shifted : DataArray\n2837             DataArray with the same coordinates and attributes but shifted\n2838             data.\n2839 \n2840         See also\n2841         --------\n2842         roll\n2843 \n2844         Examples\n2845         --------\n2846 \n2847         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2848         >>> arr.shift(x=1)\n2849         <xarray.DataArray (x: 3)>\n2850         array([ nan,   5.,   6.])\n2851         Coordinates:\n2852           * x        (x) int64 0 1 2\n2853         \"\"\"\n2854         variable = self.variable.shift(\n2855             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n2856         )\n2857         return self._replace(variable=variable)\n2858 \n2859     def roll(\n2860         self,\n2861         shifts: Mapping[Hashable, int] = None,\n2862         roll_coords: bool = None,\n2863         **shifts_kwargs: int,\n2864     ) -> \"DataArray\":\n2865         \"\"\"Roll this array by an offset along one or more dimensions.\n2866 \n2867         Unlike shift, roll may rotate all variables, including coordinates\n2868         if specified. The direction of rotation is consistent with\n2869         :py:func:`numpy.roll`.\n2870 \n2871         Parameters\n2872         ----------\n2873         shifts : Mapping with the form of {dim: offset}\n2874             Integer offset to rotate each of the given dimensions.\n2875             Positive offsets roll to the right; negative offsets roll to the\n2876             left.\n2877         roll_coords : bool\n2878             Indicates whether to  roll the coordinates by the offset\n2879             The current default of roll_coords (None, equivalent to True) is\n2880             deprecated and will change to False in a future version.\n2881             Explicitly pass roll_coords to silence the warning.\n2882         **shifts_kwargs : The keyword arguments form of ``shifts``.\n2883             One of shifts or shifts_kwargs must be provided.\n2884 \n2885         Returns\n2886         -------\n2887         rolled : DataArray\n2888             DataArray with the same attributes but rolled data and coordinates.\n2889 \n2890         See also\n2891         --------\n2892         shift\n2893 \n2894         Examples\n2895         --------\n2896 \n2897         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2898         >>> arr.roll(x=1)\n2899         <xarray.DataArray (x: 3)>\n2900         array([7, 5, 6])\n2901         Coordinates:\n2902           * x        (x) int64 2 0 1\n2903         \"\"\"\n2904         ds = self._to_temp_dataset().roll(\n2905             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n2906         )\n2907         return self._from_temp_dataset(ds)\n2908 \n2909     @property\n2910     def real(self) -> \"DataArray\":\n2911         return self._replace(self.variable.real)\n2912 \n2913     @property\n2914     def imag(self) -> \"DataArray\":\n2915         return self._replace(self.variable.imag)\n2916 \n2917     def dot(\n2918         self, other: \"DataArray\", dims: Union[Hashable, Sequence[Hashable], None] = None\n2919     ) -> \"DataArray\":\n2920         \"\"\"Perform dot product of two DataArrays along their shared dims.\n2921 \n2922         Equivalent to taking taking tensordot over all shared dims.\n2923 \n2924         Parameters\n2925         ----------\n2926         other : DataArray\n2927             The other array with which the dot product is performed.\n2928         dims: '...', hashable or sequence of hashables, optional\n2929             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.\n2930             If not specified, then all the common dimensions are summed over.\n2931 \n2932         Returns\n2933         -------\n2934         result : DataArray\n2935             Array resulting from the dot product over all shared dimensions.\n2936 \n2937         See also\n2938         --------\n2939         dot\n2940         numpy.tensordot\n2941 \n2942         Examples\n2943         --------\n2944 \n2945         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n2946         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n2947         >>> dm_vals = np.arange(4)\n2948         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n2949 \n2950         >>> dm.dims\n2951         ('z')\n2952 \n2953         >>> da.dims\n2954         ('x', 'y', 'z')\n2955 \n2956         >>> dot_result = da.dot(dm)\n2957         >>> dot_result.dims\n2958         ('x', 'y')\n2959 \n2960         \"\"\"\n2961         if isinstance(other, Dataset):\n2962             raise NotImplementedError(\n2963                 \"dot products are not yet supported with Dataset objects.\"\n2964             )\n2965         if not isinstance(other, DataArray):\n2966             raise TypeError(\"dot only operates on DataArrays.\")\n2967 \n2968         return computation.dot(self, other, dims=dims)\n2969 \n2970     def sortby(\n2971         self,\n2972         variables: Union[Hashable, \"DataArray\", Sequence[Union[Hashable, \"DataArray\"]]],\n2973         ascending: bool = True,\n2974     ) -> \"DataArray\":\n2975         \"\"\"Sort object by labels or values (along an axis).\n2976 \n2977         Sorts the dataarray, either along specified dimensions,\n2978         or according to values of 1-D dataarrays that share dimension\n2979         with calling object.\n2980 \n2981         If the input variables are dataarrays, then the dataarrays are aligned\n2982         (via left-join) to the calling object prior to sorting by cell values.\n2983         NaNs are sorted to the end, following Numpy convention.\n2984 \n2985         If multiple sorts along the same dimension is\n2986         given, numpy's lexsort is performed along that dimension:\n2987         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n2988         and the FIRST key in the sequence is used as the primary sort key,\n2989         followed by the 2nd key, etc.\n2990 \n2991         Parameters\n2992         ----------\n2993         variables: hashable, DataArray, or sequence of either\n2994             1D DataArray objects or name(s) of 1D variable(s) in\n2995             coords whose values are used to sort this array.\n2996         ascending: boolean, optional\n2997             Whether to sort by ascending or descending order.\n2998 \n2999         Returns\n3000         -------\n3001         sorted: DataArray\n3002             A new dataarray where all the specified dims are sorted by dim\n3003             labels.\n3004 \n3005         Examples\n3006         --------\n3007 \n3008         >>> da = xr.DataArray(\n3009         ...     np.random.rand(5),\n3010         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n3011         ...     dims=\"time\",\n3012         ... )\n3013         >>> da\n3014         <xarray.DataArray (time: 5)>\n3015         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n3016         Coordinates:\n3017           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n3018 \n3019         >>> da.sortby(da)\n3020         <xarray.DataArray (time: 5)>\n3021         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n3022         Coordinates:\n3023           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n3024         \"\"\"\n3025         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n3026         return self._from_temp_dataset(ds)\n3027 \n3028     def quantile(\n3029         self,\n3030         q: Any,\n3031         dim: Union[Hashable, Sequence[Hashable], None] = None,\n3032         interpolation: str = \"linear\",\n3033         keep_attrs: bool = None,\n3034         skipna: bool = True,\n3035     ) -> \"DataArray\":\n3036         \"\"\"Compute the qth quantile of the data along the specified dimension.\n3037 \n3038         Returns the qth quantiles(s) of the array elements.\n3039 \n3040         Parameters\n3041         ----------\n3042         q : float in range of [0,1] or array-like of floats\n3043             Quantile to compute, which must be between 0 and 1 inclusive.\n3044         dim : hashable or sequence of hashable, optional\n3045             Dimension(s) over which to apply quantile.\n3046         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n3047             This optional parameter specifies the interpolation method to\n3048             use when the desired quantile lies between two data points\n3049             ``i < j``:\n3050 \n3051                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n3052                   the fractional part of the index surrounded by ``i`` and\n3053                   ``j``.\n3054                 - lower: ``i``.\n3055                 - higher: ``j``.\n3056                 - nearest: ``i`` or ``j``, whichever is nearest.\n3057                 - midpoint: ``(i + j) / 2``.\n3058         keep_attrs : bool, optional\n3059             If True, the dataset's attributes (`attrs`) will be copied from\n3060             the original object to the new one.  If False (default), the new\n3061             object will be returned without attributes.\n3062         skipna : bool, optional\n3063             Whether to skip missing values when aggregating.\n3064 \n3065         Returns\n3066         -------\n3067         quantiles : DataArray\n3068             If `q` is a single quantile, then the result\n3069             is a scalar. If multiple percentiles are given, first axis of\n3070             the result corresponds to the quantile and a quantile dimension\n3071             is added to the return array. The other dimensions are the\n3072             dimensions that remain after the reduction of the array.\n3073 \n3074         See Also\n3075         --------\n3076         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n3077 \n3078         Examples\n3079         --------\n3080 \n3081         >>> da = xr.DataArray(\n3082         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n3083         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n3084         ...     dims=(\"x\", \"y\"),\n3085         ... )\n3086         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3087         <xarray.DataArray ()>\n3088         array(0.7)\n3089         Coordinates:\n3090             quantile  float64 0.0\n3091         >>> da.quantile(0, dim=\"x\")\n3092         <xarray.DataArray (y: 4)>\n3093         array([0.7, 4.2, 2.6, 1.5])\n3094         Coordinates:\n3095           * y         (y) float64 1.0 1.5 2.0 2.5\n3096             quantile  float64 0.0\n3097         >>> da.quantile([0, 0.5, 1])\n3098         <xarray.DataArray (quantile: 3)>\n3099         array([0.7, 3.4, 9.4])\n3100         Coordinates:\n3101           * quantile  (quantile) float64 0.0 0.5 1.0\n3102         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3103         <xarray.DataArray (quantile: 3, y: 4)>\n3104         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3105                [3.6 , 5.75, 6.  , 1.7 ],\n3106                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3107         Coordinates:\n3108           * y         (y) float64 1.0 1.5 2.0 2.5\n3109           * quantile  (quantile) float64 0.0 0.5 1.0\n3110         \"\"\"\n3111 \n3112         ds = self._to_temp_dataset().quantile(\n3113             q,\n3114             dim=dim,\n3115             keep_attrs=keep_attrs,\n3116             interpolation=interpolation,\n3117             skipna=skipna,\n3118         )\n3119         return self._from_temp_dataset(ds)\n3120 \n3121     def rank(\n3122         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n3123     ) -> \"DataArray\":\n3124         \"\"\"Ranks the data.\n3125 \n3126         Equal values are assigned a rank that is the average of the ranks that\n3127         would have been otherwise assigned to all of the values within that\n3128         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n3129 \n3130         NaNs in the input array are returned as NaNs.\n3131 \n3132         The `bottleneck` library is required.\n3133 \n3134         Parameters\n3135         ----------\n3136         dim : hashable\n3137             Dimension over which to compute rank.\n3138         pct : bool, optional\n3139             If True, compute percentage ranks, otherwise compute integer ranks.\n3140         keep_attrs : bool, optional\n3141             If True, the dataset's attributes (`attrs`) will be copied from\n3142             the original object to the new one.  If False (default), the new\n3143             object will be returned without attributes.\n3144 \n3145         Returns\n3146         -------\n3147         ranked : DataArray\n3148             DataArray with the same coordinates and dtype 'float64'.\n3149 \n3150         Examples\n3151         --------\n3152 \n3153         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3154         >>> arr.rank(\"x\")\n3155         <xarray.DataArray (x: 3)>\n3156         array([ 1.,   2.,   3.])\n3157         Dimensions without coordinates: x\n3158         \"\"\"\n3159 \n3160         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n3161         return self._from_temp_dataset(ds)\n3162 \n3163     def differentiate(\n3164         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n3165     ) -> \"DataArray\":\n3166         \"\"\" Differentiate the array with the second order accurate central\n3167         differences.\n3168 \n3169         .. note::\n3170             This feature is limited to simple cartesian geometry, i.e. coord\n3171             must be one dimensional.\n3172 \n3173         Parameters\n3174         ----------\n3175         coord: hashable\n3176             The coordinate to be used to compute the gradient.\n3177         edge_order: 1 or 2. Default 1\n3178             N-th order accurate differences at the boundaries.\n3179         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n3180             'us', 'ns', 'ps', 'fs', 'as'}\n3181             Unit to compute gradient. Only valid for datetime coordinate.\n3182 \n3183         Returns\n3184         -------\n3185         differentiated: DataArray\n3186 \n3187         See also\n3188         --------\n3189         numpy.gradient: corresponding numpy function\n3190 \n3191         Examples\n3192         --------\n3193 \n3194         >>> da = xr.DataArray(\n3195         ...     np.arange(12).reshape(4, 3),\n3196         ...     dims=[\"x\", \"y\"],\n3197         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3198         ... )\n3199         >>> da\n3200         <xarray.DataArray (x: 4, y: 3)>\n3201         array([[ 0,  1,  2],\n3202                [ 3,  4,  5],\n3203                [ 6,  7,  8],\n3204                [ 9, 10, 11]])\n3205         Coordinates:\n3206           * x        (x) float64 0.0 0.1 1.1 1.2\n3207         Dimensions without coordinates: y\n3208         >>>\n3209         >>> da.differentiate(\"x\")\n3210         <xarray.DataArray (x: 4, y: 3)>\n3211         array([[30.      , 30.      , 30.      ],\n3212                [27.545455, 27.545455, 27.545455],\n3213                [27.545455, 27.545455, 27.545455],\n3214                [30.      , 30.      , 30.      ]])\n3215         Coordinates:\n3216           * x        (x) float64 0.0 0.1 1.1 1.2\n3217         Dimensions without coordinates: y\n3218         \"\"\"\n3219         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n3220         return self._from_temp_dataset(ds)\n3221 \n3222     def integrate(\n3223         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n3224     ) -> \"DataArray\":\n3225         \"\"\" integrate the array with the trapezoidal rule.\n3226 \n3227         .. note::\n3228             This feature is limited to simple cartesian geometry, i.e. dim\n3229             must be one dimensional.\n3230 \n3231         Parameters\n3232         ----------\n3233         dim: hashable, or a sequence of hashable\n3234             Coordinate(s) used for the integration.\n3235         datetime_unit: str, optional\n3236             Can be used to specify the unit if datetime coordinate is used.\n3237             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',\n3238             'fs', 'as'}\n3239 \n3240         Returns\n3241         -------\n3242         integrated: DataArray\n3243 \n3244         See also\n3245         --------\n3246         numpy.trapz: corresponding numpy function\n3247 \n3248         Examples\n3249         --------\n3250 \n3251         >>> da = xr.DataArray(\n3252         ...     np.arange(12).reshape(4, 3),\n3253         ...     dims=[\"x\", \"y\"],\n3254         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3255         ... )\n3256         >>> da\n3257         <xarray.DataArray (x: 4, y: 3)>\n3258         array([[ 0,  1,  2],\n3259                [ 3,  4,  5],\n3260                [ 6,  7,  8],\n3261                [ 9, 10, 11]])\n3262         Coordinates:\n3263           * x        (x) float64 0.0 0.1 1.1 1.2\n3264         Dimensions without coordinates: y\n3265         >>>\n3266         >>> da.integrate(\"x\")\n3267         <xarray.DataArray (y: 3)>\n3268         array([5.4, 6.6, 7.8])\n3269         Dimensions without coordinates: y\n3270         \"\"\"\n3271         ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n3272         return self._from_temp_dataset(ds)\n3273 \n3274     def unify_chunks(self) -> \"DataArray\":\n3275         \"\"\" Unify chunk size along all chunked dimensions of this DataArray.\n3276 \n3277         Returns\n3278         -------\n3279 \n3280         DataArray with consistent chunk sizes for all dask-array variables\n3281 \n3282         See Also\n3283         --------\n3284 \n3285         dask.array.core.unify_chunks\n3286         \"\"\"\n3287         ds = self._to_temp_dataset().unify_chunks()\n3288         return self._from_temp_dataset(ds)\n3289 \n3290     def map_blocks(\n3291         self,\n3292         func: \"Callable[..., T_DSorDA]\",\n3293         args: Sequence[Any] = (),\n3294         kwargs: Mapping[str, Any] = None,\n3295         template: Union[\"DataArray\", \"Dataset\"] = None,\n3296     ) -> \"T_DSorDA\":\n3297         \"\"\"\n3298         Apply a function to each block of this DataArray.\n3299 \n3300         .. warning::\n3301             This method is experimental and its signature may change.\n3302 \n3303         Parameters\n3304         ----------\n3305         func: callable\n3306             User-provided function that accepts a DataArray as its first\n3307             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n3308             corresponding to one chunk along each chunked dimension. ``func`` will be\n3309             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n3310 \n3311             This function must return either a single DataArray or a single Dataset.\n3312 \n3313             This function cannot add a new chunked dimension.\n3314 \n3315         obj: DataArray, Dataset\n3316             Passed to the function as its first argument, one block at a time.\n3317         args: Sequence\n3318             Passed to func after unpacking and subsetting any xarray objects by blocks.\n3319             xarray objects in args must be aligned with obj, otherwise an error is raised.\n3320         kwargs: Mapping\n3321             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n3322             subset to blocks. Passing dask collections in kwargs is not allowed.\n3323         template: (optional) DataArray, Dataset\n3324             xarray object representing the final result after compute is called. If not provided,\n3325             the function will be first run on mocked-up data, that looks like ``obj`` but\n3326             has sizes 0, to determine properties of the returned object such as dtype,\n3327             variable names, attributes, new dimensions and new indexes (if any).\n3328             ``template`` must be provided if the function changes the size of existing dimensions.\n3329             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n3330             ``attrs`` set by ``func`` will be ignored.\n3331 \n3332 \n3333         Returns\n3334         -------\n3335         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n3336         function.\n3337 \n3338         Notes\n3339         -----\n3340         This function is designed for when ``func`` needs to manipulate a whole xarray object\n3341         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n3342         recommended to use ``apply_ufunc``.\n3343 \n3344         If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n3345         equivalent to calling ``func(obj, *args, **kwargs)``.\n3346 \n3347         See Also\n3348         --------\n3349         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,\n3350         xarray.DataArray.map_blocks\n3351 \n3352         Examples\n3353         --------\n3354 \n3355         Calculate an anomaly from climatology using ``.groupby()``. Using\n3356         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n3357         its indices, and its methods like ``.groupby()``.\n3358 \n3359         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n3360         ...     gb = da.groupby(groupby_type)\n3361         ...     clim = gb.mean(dim=\"time\")\n3362         ...     return gb - clim\n3363         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n3364         >>> np.random.seed(123)\n3365         >>> array = xr.DataArray(\n3366         ...     np.random.rand(len(time)), dims=\"time\", coords=[time]\n3367         ... ).chunk()\n3368         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n3369         <xarray.DataArray (time: 24)>\n3370         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n3371                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n3372                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n3373                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n3374                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n3375         Coordinates:\n3376           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n3377 \n3378         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n3379         to the function being applied in ``xr.map_blocks()``:\n3380 \n3381         >>> array.map_blocks(\n3382         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array,\n3383         ... )\n3384         <xarray.DataArray (time: 24)>\n3385         array([ 0.15361741, -0.25671244, -0.31600032,  0.008463  ,  0.1766172 ,\n3386                -0.11974531,  0.43791243,  0.14197797, -0.06191987, -0.15073425,\n3387                -0.19967375,  0.18619794, -0.05100474, -0.42989909, -0.09153273,\n3388                 0.24841842, -0.30708526, -0.31412523,  0.04197439,  0.0422506 ,\n3389                 0.14482397,  0.35985481,  0.23487834,  0.12144652])\n3390         Coordinates:\n3391             * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n3392         \"\"\"\n3393         from .parallel import map_blocks\n3394 \n3395         return map_blocks(func, self, args, kwargs, template)\n3396 \n3397     def polyfit(\n3398         self,\n3399         dim: Hashable,\n3400         deg: int,\n3401         skipna: bool = None,\n3402         rcond: float = None,\n3403         w: Union[Hashable, Any] = None,\n3404         full: bool = False,\n3405         cov: bool = False,\n3406     ):\n3407         \"\"\"\n3408         Least squares polynomial fit.\n3409 \n3410         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n3411         invalid values when `skipna = True`.\n3412 \n3413         Parameters\n3414         ----------\n3415         dim : hashable\n3416             Coordinate along which to fit the polynomials.\n3417         deg : int\n3418             Degree of the fitting polynomial.\n3419         skipna : bool, optional\n3420             If True, removes all invalid values before fitting each 1D slices of the array.\n3421             Default is True if data is stored in a dask.array or if there is any\n3422             invalid values, False otherwise.\n3423         rcond : float, optional\n3424             Relative condition number to the fit.\n3425         w : Union[Hashable, Any], optional\n3426             Weights to apply to the y-coordinate of the sample points.\n3427             Can be an array-like object or the name of a coordinate in the dataset.\n3428         full : bool, optional\n3429             Whether to return the residuals, matrix rank and singular values in addition\n3430             to the coefficients.\n3431         cov : Union[bool, str], optional\n3432             Whether to return to the covariance matrix in addition to the coefficients.\n3433             The matrix is not scaled if `cov='unscaled'`.\n3434 \n3435         Returns\n3436         -------\n3437         polyfit_results : Dataset\n3438             A single dataset which contains:\n3439 \n3440             polyfit_coefficients\n3441                 The coefficients of the best fit.\n3442             polyfit_residuals\n3443                 The residuals of the least-square computation (only included if `full=True`)\n3444             [dim]_matrix_rank\n3445                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3446             [dim]_singular_value\n3447                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3448             polyfit_covariance\n3449                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n3450 \n3451         See also\n3452         --------\n3453         numpy.polyfit\n3454         \"\"\"\n3455         return self._to_temp_dataset().polyfit(\n3456             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n3457         )\n3458 \n3459     def pad(\n3460         self,\n3461         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n3462         mode: str = \"constant\",\n3463         stat_length: Union[\n3464             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3465         ] = None,\n3466         constant_values: Union[\n3467             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3468         ] = None,\n3469         end_values: Union[\n3470             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3471         ] = None,\n3472         reflect_type: str = None,\n3473         **pad_width_kwargs: Any,\n3474     ) -> \"DataArray\":\n3475         \"\"\"Pad this array along one or more dimensions.\n3476 \n3477         .. warning::\n3478             This function is experimental and its behaviour is likely to change\n3479             especially regarding padding of dimension coordinates (or IndexVariables).\n3480 \n3481         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n3482         coordinates will be padded with the same mode, otherwise coordinates\n3483         are padded using the \"constant\" mode with fill_value dtypes.NA.\n3484 \n3485         Parameters\n3486         ----------\n3487         pad_width : Mapping with the form of {dim: (pad_before, pad_after)}\n3488             Number of values padded along each dimension.\n3489             {dim: pad} is a shortcut for pad_before = pad_after = pad\n3490         mode : str\n3491             One of the following string values (taken from numpy docs)\n3492 \n3493             'constant' (default)\n3494                 Pads with a constant value.\n3495             'edge'\n3496                 Pads with the edge values of array.\n3497             'linear_ramp'\n3498                 Pads with the linear ramp between end_value and the\n3499                 array edge value.\n3500             'maximum'\n3501                 Pads with the maximum value of all or part of the\n3502                 vector along each axis.\n3503             'mean'\n3504                 Pads with the mean value of all or part of the\n3505                 vector along each axis.\n3506             'median'\n3507                 Pads with the median value of all or part of the\n3508                 vector along each axis.\n3509             'minimum'\n3510                 Pads with the minimum value of all or part of the\n3511                 vector along each axis.\n3512             'reflect'\n3513                 Pads with the reflection of the vector mirrored on\n3514                 the first and last values of the vector along each\n3515                 axis.\n3516             'symmetric'\n3517                 Pads with the reflection of the vector mirrored\n3518                 along the edge of the array.\n3519             'wrap'\n3520                 Pads with the wrap of the vector along the axis.\n3521                 The first values are used to pad the end and the\n3522                 end values are used to pad the beginning.\n3523         stat_length : int, tuple or mapping of the form {dim: tuple}\n3524             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n3525             values at edge of each axis used to calculate the statistic value.\n3526             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n3527             statistic lengths along each dimension.\n3528             ((before, after),) yields same before and after statistic lengths\n3529             for each dimension.\n3530             (stat_length,) or int is a shortcut for before = after = statistic\n3531             length for all axes.\n3532             Default is ``None``, to use the entire axis.\n3533         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n3534             Used in 'constant'.  The values to set the padded values for each\n3535             axis.\n3536             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3537             pad constants along each dimension.\n3538             ``((before, after),)`` yields same before and after constants for each\n3539             dimension.\n3540             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3541             all dimensions.\n3542             Default is 0.\n3543         end_values : scalar, tuple or mapping of the form {dim: tuple}\n3544             Used in 'linear_ramp'.  The values used for the ending value of the\n3545             linear_ramp and that will form the edge of the padded array.\n3546             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3547             end values along each dimension.\n3548             ``((before, after),)`` yields same before and after end values for each\n3549             axis.\n3550             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3551             all axes.\n3552             Default is 0.\n3553         reflect_type : {'even', 'odd'}, optional\n3554             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n3555             default with an unaltered reflection around the edge value.  For\n3556             the 'odd' style, the extended part of the array is created by\n3557             subtracting the reflected values from two times the edge value.\n3558         **pad_width_kwargs:\n3559             The keyword arguments form of ``pad_width``.\n3560             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n3561 \n3562         Returns\n3563         -------\n3564         padded : DataArray\n3565             DataArray with the padded coordinates and data.\n3566 \n3567         See also\n3568         --------\n3569         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n3570 \n3571         Notes\n3572         -----\n3573         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n3574         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n3575         specify ``constant_values=np.nan``\n3576 \n3577         Examples\n3578         --------\n3579 \n3580         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n3581         >>> arr.pad(x=(1, 2), constant_values=0)\n3582         <xarray.DataArray (x: 6)>\n3583         array([0, 5, 6, 7, 0, 0])\n3584         Coordinates:\n3585           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n3586 \n3587         >>> da = xr.DataArray(\n3588         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n3589         ...     dims=[\"x\", \"y\"],\n3590         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n3591         ... )\n3592         >>> da.pad(x=1)\n3593         <xarray.DataArray (x: 4, y: 4)>\n3594         array([[nan, nan, nan, nan],\n3595                [ 0.,  1.,  2.,  3.],\n3596                [10., 11., 12., 13.],\n3597                [nan, nan, nan, nan]])\n3598         Coordinates:\n3599           * x        (x) float64 nan 0.0 1.0 nan\n3600           * y        (y) int64 10 20 30 40\n3601             z        (x) float64 nan 100.0 200.0 nan\n3602         >>> da.pad(x=1, constant_values=np.nan)\n3603         <xarray.DataArray (x: 4, y: 4)>\n3604         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3605                 -9223372036854775808],\n3606                [                   0,                    1,                    2,\n3607                                    3],\n3608                [                  10,                   11,                   12,\n3609                                   13],\n3610                [-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3611                 -9223372036854775808]])\n3612         Coordinates:\n3613           * x        (x) float64 nan 0.0 1.0 nan\n3614           * y        (y) int64 10 20 30 40\n3615             z        (x) float64 nan 100.0 200.0 nan\n3616         \"\"\"\n3617         ds = self._to_temp_dataset().pad(\n3618             pad_width=pad_width,\n3619             mode=mode,\n3620             stat_length=stat_length,\n3621             constant_values=constant_values,\n3622             end_values=end_values,\n3623             reflect_type=reflect_type,\n3624             **pad_width_kwargs,\n3625         )\n3626         return self._from_temp_dataset(ds)\n3627 \n3628     def idxmin(\n3629         self,\n3630         dim: Hashable = None,\n3631         skipna: bool = None,\n3632         fill_value: Any = dtypes.NA,\n3633         keep_attrs: bool = None,\n3634     ) -> \"DataArray\":\n3635         \"\"\"Return the coordinate label of the minimum value along a dimension.\n3636 \n3637         Returns a new `DataArray` named after the dimension with the values of\n3638         the coordinate labels along that dimension corresponding to minimum\n3639         values along that dimension.\n3640 \n3641         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n3642         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n3643 \n3644         Parameters\n3645         ----------\n3646         dim : str, optional\n3647             Dimension over which to apply `idxmin`.  This is optional for 1D\n3648             arrays, but required for arrays with 2 or more dimensions.\n3649         skipna : bool or None, default None\n3650             If True, skip missing values (as marked by NaN). By default, only\n3651             skips missing values for ``float``, ``complex``, and ``object``\n3652             dtypes; other dtypes either do not have a sentinel missing value\n3653             (``int``) or ``skipna=True`` has not been implemented\n3654             (``datetime64`` or ``timedelta64``).\n3655         fill_value : Any, default NaN\n3656             Value to be filled in case all of the values along a dimension are\n3657             null.  By default this is NaN.  The fill value and result are\n3658             automatically converted to a compatible dtype if possible.\n3659             Ignored if ``skipna`` is False.\n3660         keep_attrs : bool, default False\n3661             If True, the attributes (``attrs``) will be copied from the\n3662             original object to the new one.  If False (default), the new object\n3663             will be returned without attributes.\n3664 \n3665         Returns\n3666         -------\n3667         reduced : DataArray\n3668             New `DataArray` object with `idxmin` applied to its data and the\n3669             indicated dimension removed.\n3670 \n3671         See also\n3672         --------\n3673         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n3674 \n3675         Examples\n3676         --------\n3677 \n3678         >>> array = xr.DataArray(\n3679         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n3680         ... )\n3681         >>> array.min()\n3682         <xarray.DataArray ()>\n3683         array(-2)\n3684         >>> array.argmin()\n3685         <xarray.DataArray ()>\n3686         array(4)\n3687         >>> array.idxmin()\n3688         <xarray.DataArray 'x' ()>\n3689         array('e', dtype='<U1')\n3690 \n3691         >>> array = xr.DataArray(\n3692         ...     [\n3693         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n3694         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3695         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n3696         ...     ],\n3697         ...     dims=[\"y\", \"x\"],\n3698         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n3699         ... )\n3700         >>> array.min(dim=\"x\")\n3701         <xarray.DataArray (y: 3)>\n3702         array([-2., -4.,  1.])\n3703         Coordinates:\n3704           * y        (y) int64 -1 0 1\n3705         >>> array.argmin(dim=\"x\")\n3706         <xarray.DataArray (y: 3)>\n3707         array([4, 0, 2])\n3708         Coordinates:\n3709           * y        (y) int64 -1 0 1\n3710         >>> array.idxmin(dim=\"x\")\n3711         <xarray.DataArray 'x' (y: 3)>\n3712         array([16.,  0.,  4.])\n3713         Coordinates:\n3714           * y        (y) int64 -1 0 1\n3715         \"\"\"\n3716         return computation._calc_idxminmax(\n3717             array=self,\n3718             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n3719             dim=dim,\n3720             skipna=skipna,\n3721             fill_value=fill_value,\n3722             keep_attrs=keep_attrs,\n3723         )\n3724 \n3725     def idxmax(\n3726         self,\n3727         dim: Hashable = None,\n3728         skipna: bool = None,\n3729         fill_value: Any = dtypes.NA,\n3730         keep_attrs: bool = None,\n3731     ) -> \"DataArray\":\n3732         \"\"\"Return the coordinate label of the maximum value along a dimension.\n3733 \n3734         Returns a new `DataArray` named after the dimension with the values of\n3735         the coordinate labels along that dimension corresponding to maximum\n3736         values along that dimension.\n3737 \n3738         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n3739         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n3740 \n3741         Parameters\n3742         ----------\n3743         dim : str, optional\n3744             Dimension over which to apply `idxmax`.  This is optional for 1D\n3745             arrays, but required for arrays with 2 or more dimensions.\n3746         skipna : bool or None, default None\n3747             If True, skip missing values (as marked by NaN). By default, only\n3748             skips missing values for ``float``, ``complex``, and ``object``\n3749             dtypes; other dtypes either do not have a sentinel missing value\n3750             (``int``) or ``skipna=True`` has not been implemented\n3751             (``datetime64`` or ``timedelta64``).\n3752         fill_value : Any, default NaN\n3753             Value to be filled in case all of the values along a dimension are\n3754             null.  By default this is NaN.  The fill value and result are\n3755             automatically converted to a compatible dtype if possible.\n3756             Ignored if ``skipna`` is False.\n3757         keep_attrs : bool, default False\n3758             If True, the attributes (``attrs``) will be copied from the\n3759             original object to the new one.  If False (default), the new object\n3760             will be returned without attributes.\n3761 \n3762         Returns\n3763         -------\n3764         reduced : DataArray\n3765             New `DataArray` object with `idxmax` applied to its data and the\n3766             indicated dimension removed.\n3767 \n3768         See also\n3769         --------\n3770         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n3771 \n3772         Examples\n3773         --------\n3774 \n3775         >>> array = xr.DataArray(\n3776         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n3777         ... )\n3778         >>> array.max()\n3779         <xarray.DataArray ()>\n3780         array(2)\n3781         >>> array.argmax()\n3782         <xarray.DataArray ()>\n3783         array(1)\n3784         >>> array.idxmax()\n3785         <xarray.DataArray 'x' ()>\n3786         array('b', dtype='<U1')\n3787 \n3788         >>> array = xr.DataArray(\n3789         ...     [\n3790         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n3791         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3792         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n3793         ...     ],\n3794         ...     dims=[\"y\", \"x\"],\n3795         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n3796         ... )\n3797         >>> array.max(dim=\"x\")\n3798         <xarray.DataArray (y: 3)>\n3799         array([2., 2., 1.])\n3800         Coordinates:\n3801           * y        (y) int64 -1 0 1\n3802         >>> array.argmax(dim=\"x\")\n3803         <xarray.DataArray (y: 3)>\n3804         array([0, 2, 2])\n3805         Coordinates:\n3806           * y        (y) int64 -1 0 1\n3807         >>> array.idxmax(dim=\"x\")\n3808         <xarray.DataArray 'x' (y: 3)>\n3809         array([0., 4., 4.])\n3810         Coordinates:\n3811           * y        (y) int64 -1 0 1\n3812         \"\"\"\n3813         return computation._calc_idxminmax(\n3814             array=self,\n3815             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n3816             dim=dim,\n3817             skipna=skipna,\n3818             fill_value=fill_value,\n3819             keep_attrs=keep_attrs,\n3820         )\n3821 \n3822     # this needs to be at the end, or mypy will confuse with `str`\n3823     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n3824     str = utils.UncachedAccessor(StringAccessor)\n3825 \n3826 \n3827 # priority most be higher than Variable to properly work with binary ufuncs\n3828 ops.inject_all_ops_and_reduce_methods(DataArray, priority=60)\n",
        "intended_behavior": "Creation or Modification of Representation Methods: Methods responsible for generating HTML representations (`_repr_html_`) should be added or located and designed or adjusted to ensure that data and attributes sections are expanded by default in the HTML representation."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 1,
        "end": 2424,
        "class_name": null,
        "method_name": null,
        "code": "1 import copy\n2 import functools\n3 import itertools\n4 import numbers\n5 import warnings\n6 from collections import defaultdict\n7 from datetime import timedelta\n8 from distutils.version import LooseVersion\n9 from typing import Any, Dict, Hashable, Mapping, Tuple, TypeVar, Union\n10 \n11 import numpy as np\n12 import pandas as pd\n13 \n14 import xarray as xr  # only for Dataset and DataArray\n15 \n16 from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\n17 from .indexing import (\n18     BasicIndexer,\n19     OuterIndexer,\n20     PandasIndexAdapter,\n21     VectorizedIndexer,\n22     as_indexable,\n23 )\n24 from .npcompat import IS_NEP18_ACTIVE\n25 from .options import _get_keep_attrs\n26 from .pycompat import dask_array_type, integer_types\n27 from .utils import (\n28     OrderedSet,\n29     _default,\n30     decode_numpy_dict_values,\n31     drop_dims_from_indexers,\n32     either_dict_or_kwargs,\n33     ensure_us_time_resolution,\n34     infix_dims,\n35 )\n36 \n37 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (\n38     indexing.ExplicitlyIndexed,\n39     pd.Index,\n40 ) + dask_array_type\n41 # https://github.com/python/mypy/issues/224\n42 BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\n43 \n44 VariableType = TypeVar(\"VariableType\", bound=\"Variable\")\n45 \"\"\"Type annotation to be used when methods of Variable return self or a copy of self.\n46 When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\n47 output as an instance of the subclass.\n48 \n49 Usage::\n50 \n51    class Variable:\n52        def f(self: VariableType, ...) -> VariableType:\n53            ...\n54 \"\"\"\n55 \n56 \n57 class MissingDimensionsError(ValueError):\n58     \"\"\"Error class used when we can't safely guess a dimension name.\n59     \"\"\"\n60 \n61     # inherits from ValueError for backward compatibility\n62     # TODO: move this to an xarray.exceptions module?\n63 \n64 \n65 def as_variable(obj, name=None) -> \"Union[Variable, IndexVariable]\":\n66     \"\"\"Convert an object into a Variable.\n67 \n68     Parameters\n69     ----------\n70     obj : object\n71         Object to convert into a Variable.\n72 \n73         - If the object is already a Variable, return a shallow copy.\n74         - Otherwise, if the object has 'dims' and 'data' attributes, convert\n75           it into a new Variable.\n76         - If all else fails, attempt to convert the object into a Variable by\n77           unpacking it into the arguments for creating a new Variable.\n78     name : str, optional\n79         If provided:\n80 \n81         - `obj` can be a 1D array, which is assumed to label coordinate values\n82           along a dimension of this given name.\n83         - Variables with name matching one of their dimensions are converted\n84           into `IndexVariable` objects.\n85 \n86     Returns\n87     -------\n88     var : Variable\n89         The newly created variable.\n90 \n91     \"\"\"\n92     from .dataarray import DataArray\n93 \n94     # TODO: consider extending this method to automatically handle Iris and\n95     if isinstance(obj, DataArray):\n96         # extract the primary Variable from DataArrays\n97         obj = obj.variable\n98 \n99     if isinstance(obj, Variable):\n100         obj = obj.copy(deep=False)\n101     elif isinstance(obj, tuple):\n102         try:\n103             obj = Variable(*obj)\n104         except (TypeError, ValueError) as error:\n105             # use .format() instead of % because it handles tuples consistently\n106             raise error.__class__(\n107                 \"Could not convert tuple of form \"\n108                 \"(dims, data[, attrs, encoding]): \"\n109                 \"{} to Variable.\".format(obj)\n110             )\n111     elif utils.is_scalar(obj):\n112         obj = Variable([], obj)\n113     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n114         obj = Variable(obj.name, obj)\n115     elif isinstance(obj, (set, dict)):\n116         raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n117     elif name is not None:\n118         data = as_compatible_data(obj)\n119         if data.ndim != 1:\n120             raise MissingDimensionsError(\n121                 \"cannot set variable %r with %r-dimensional data \"\n122                 \"without explicit dimension names. Pass a tuple of \"\n123                 \"(dims, data) instead.\" % (name, data.ndim)\n124             )\n125         obj = Variable(name, data, fastpath=True)\n126     else:\n127         raise TypeError(\n128             \"unable to convert object into a variable without an \"\n129             \"explicit list of dimensions: %r\" % obj\n130         )\n131 \n132     if name is not None and name in obj.dims:\n133         # convert the Variable into an Index\n134         if obj.ndim != 1:\n135             raise MissingDimensionsError(\n136                 \"%r has more than 1-dimension and the same name as one of its \"\n137                 \"dimensions %r. xarray disallows such variables because they \"\n138                 \"conflict with the coordinates used to label \"\n139                 \"dimensions.\" % (name, obj.dims)\n140             )\n141         obj = obj.to_index_variable()\n142 \n143     return obj\n144 \n145 \n146 def _maybe_wrap_data(data):\n147     \"\"\"\n148     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\n149     they can be indexed properly.\n150 \n151     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\n152     all pass through unmodified.\n153     \"\"\"\n154     if isinstance(data, pd.Index):\n155         return PandasIndexAdapter(data)\n156     return data\n157 \n158 \n159 def _possibly_convert_objects(values):\n160     \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into\n161     datetime64 and timedelta64, according to the pandas convention.\n162     \"\"\"\n163     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n164 \n165 \n166 def as_compatible_data(data, fastpath=False):\n167     \"\"\"Prepare and wrap data to put in a Variable.\n168 \n169     - If data does not have the necessary attributes, convert it to ndarray.\n170     - If data has dtype=datetime64, ensure that it has ns precision. If it's a\n171       pandas.Timestamp, convert it to datetime64.\n172     - If data is already a pandas or xarray object (other than an Index), just\n173       use the values.\n174 \n175     Finally, wrap it up with an adapter if necessary.\n176     \"\"\"\n177     if fastpath and getattr(data, \"ndim\", 0) > 0:\n178         # can't use fastpath (yet) for scalars\n179         return _maybe_wrap_data(data)\n180 \n181     if isinstance(data, Variable):\n182         return data.data\n183 \n184     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n185         return _maybe_wrap_data(data)\n186 \n187     if isinstance(data, tuple):\n188         data = utils.to_0d_object_array(data)\n189 \n190     if isinstance(data, pd.Timestamp):\n191         # TODO: convert, handle datetime objects, too\n192         data = np.datetime64(data.value, \"ns\")\n193 \n194     if isinstance(data, timedelta):\n195         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n196 \n197     # we don't want nested self-described arrays\n198     data = getattr(data, \"values\", data)\n199 \n200     if isinstance(data, np.ma.MaskedArray):\n201         mask = np.ma.getmaskarray(data)\n202         if mask.any():\n203             dtype, fill_value = dtypes.maybe_promote(data.dtype)\n204             data = np.asarray(data, dtype=dtype)\n205             data[mask] = fill_value\n206         else:\n207             data = np.asarray(data)\n208 \n209     if not isinstance(data, np.ndarray):\n210         if hasattr(data, \"__array_function__\"):\n211             if IS_NEP18_ACTIVE:\n212                 return data\n213             else:\n214                 raise TypeError(\n215                     \"Got an NumPy-like array type providing the \"\n216                     \"__array_function__ protocol but NEP18 is not enabled. \"\n217                     \"Check that numpy >= v1.16 and that the environment \"\n218                     'variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to '\n219                     '\"1\"'\n220                 )\n221 \n222     # validate whether the data is valid data types\n223     data = np.asarray(data)\n224 \n225     if isinstance(data, np.ndarray):\n226         if data.dtype.kind == \"O\":\n227             data = _possibly_convert_objects(data)\n228         elif data.dtype.kind == \"M\":\n229             data = np.asarray(data, \"datetime64[ns]\")\n230         elif data.dtype.kind == \"m\":\n231             data = np.asarray(data, \"timedelta64[ns]\")\n232 \n233     return _maybe_wrap_data(data)\n234 \n235 \n236 def _as_array_or_item(data):\n237     \"\"\"Return the given values as a numpy array, or as an individual item if\n238     it's a 0d datetime64 or timedelta64 array.\n239 \n240     Importantly, this function does not copy data if it is already an ndarray -\n241     otherwise, it will not be possible to update Variable values in place.\n242 \n243     This function mostly exists because 0-dimensional ndarrays with\n244     dtype=datetime64 are broken :(\n245     https://github.com/numpy/numpy/issues/4337\n246     https://github.com/numpy/numpy/issues/7619\n247 \n248     TODO: remove this (replace with np.asarray) once these issues are fixed\n249     \"\"\"\n250     data = np.asarray(data)\n251     if data.ndim == 0:\n252         if data.dtype.kind == \"M\":\n253             data = np.datetime64(data, \"ns\")\n254         elif data.dtype.kind == \"m\":\n255             data = np.timedelta64(data, \"ns\")\n256     return data\n257 \n258 \n259 class Variable(\n260     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n261 ):\n262     \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n263     which describe a single Array. A single Variable object is not fully\n264     described outside the context of its parent Dataset (if you want such a\n265     fully described object, use a DataArray instead).\n266 \n267     The main functional difference between Variables and numpy arrays is that\n268     numerical operations on Variables implement array broadcasting by dimension\n269     name. For example, adding an Variable with dimensions `('time',)` to\n270     another Variable with dimensions `('space',)` results in a new Variable\n271     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n272     like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n273     instead of an \"axis\".\n274 \n275     Variables are light-weight objects used as the building block for datasets.\n276     They are more primitive objects, so operations with them provide marginally\n277     higher performance than using DataArrays. However, manipulating data in the\n278     form of a Dataset or DataArray should almost always be preferred, because\n279     they can use more complete metadata in context of coordinate labels.\n280     \"\"\"\n281 \n282     __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n283 \n284     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n285         \"\"\"\n286         Parameters\n287         ----------\n288         dims : str or sequence of str\n289             Name(s) of the the data dimension(s). Must be either a string (only\n290             for 1D data) or a sequence of strings with length equal to the\n291             number of dimensions.\n292         data : array_like\n293             Data array which supports numpy-like data access.\n294         attrs : dict_like or None, optional\n295             Attributes to assign to the new variable. If None (default), an\n296             empty attribute dictionary is initialized.\n297         encoding : dict_like or None, optional\n298             Dictionary specifying how to encode this array's data into a\n299             serialized format like netCDF4. Currently used keys (for netCDF)\n300             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n301             Well-behaved code to serialize a Variable should ignore\n302             unrecognized encoding items.\n303         \"\"\"\n304         self._data = as_compatible_data(data, fastpath=fastpath)\n305         self._dims = self._parse_dimensions(dims)\n306         self._attrs = None\n307         self._encoding = None\n308         if attrs is not None:\n309             self.attrs = attrs\n310         if encoding is not None:\n311             self.encoding = encoding\n312 \n313     @property\n314     def dtype(self):\n315         return self._data.dtype\n316 \n317     @property\n318     def shape(self):\n319         return self._data.shape\n320 \n321     @property\n322     def nbytes(self):\n323         return self.size * self.dtype.itemsize\n324 \n325     @property\n326     def _in_memory(self):\n327         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n328             isinstance(self._data, indexing.MemoryCachedArray)\n329             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n330         )\n331 \n332     @property\n333     def data(self):\n334         if hasattr(self._data, \"__array_function__\") or isinstance(\n335             self._data, dask_array_type\n336         ):\n337             return self._data\n338         else:\n339             return self.values\n340 \n341     @data.setter\n342     def data(self, data):\n343         data = as_compatible_data(data)\n344         if data.shape != self.shape:\n345             raise ValueError(\n346                 f\"replacement data must match the Variable's shape. \"\n347                 f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n348             )\n349         self._data = data\n350 \n351     def load(self, **kwargs):\n352         \"\"\"Manually trigger loading of this variable's data from disk or a\n353         remote source into memory and return this variable.\n354 \n355         Normally, it should not be necessary to call this method in user code,\n356         because all xarray functions should either work on deferred data or\n357         load data automatically.\n358 \n359         Parameters\n360         ----------\n361         **kwargs : dict\n362             Additional keyword arguments passed on to ``dask.array.compute``.\n363 \n364         See Also\n365         --------\n366         dask.array.compute\n367         \"\"\"\n368         if isinstance(self._data, dask_array_type):\n369             self._data = as_compatible_data(self._data.compute(**kwargs))\n370         elif not hasattr(self._data, \"__array_function__\"):\n371             self._data = np.asarray(self._data)\n372         return self\n373 \n374     def compute(self, **kwargs):\n375         \"\"\"Manually trigger loading of this variable's data from disk or a\n376         remote source into memory and return a new variable. The original is\n377         left unaltered.\n378 \n379         Normally, it should not be necessary to call this method in user code,\n380         because all xarray functions should either work on deferred data or\n381         load data automatically.\n382 \n383         Parameters\n384         ----------\n385         **kwargs : dict\n386             Additional keyword arguments passed on to ``dask.array.compute``.\n387 \n388         See Also\n389         --------\n390         dask.array.compute\n391         \"\"\"\n392         new = self.copy(deep=False)\n393         return new.load(**kwargs)\n394 \n395     def __dask_tokenize__(self):\n396         # Use v.data, instead of v._data, in order to cope with the wrappers\n397         # around NetCDF and the like\n398         from dask.base import normalize_token\n399 \n400         return normalize_token((type(self), self._dims, self.data, self._attrs))\n401 \n402     def __dask_graph__(self):\n403         if isinstance(self._data, dask_array_type):\n404             return self._data.__dask_graph__()\n405         else:\n406             return None\n407 \n408     def __dask_keys__(self):\n409         return self._data.__dask_keys__()\n410 \n411     def __dask_layers__(self):\n412         return self._data.__dask_layers__()\n413 \n414     @property\n415     def __dask_optimize__(self):\n416         return self._data.__dask_optimize__\n417 \n418     @property\n419     def __dask_scheduler__(self):\n420         return self._data.__dask_scheduler__\n421 \n422     def __dask_postcompute__(self):\n423         array_func, array_args = self._data.__dask_postcompute__()\n424         return (\n425             self._dask_finalize,\n426             (array_func, array_args, self._dims, self._attrs, self._encoding),\n427         )\n428 \n429     def __dask_postpersist__(self):\n430         array_func, array_args = self._data.__dask_postpersist__()\n431         return (\n432             self._dask_finalize,\n433             (array_func, array_args, self._dims, self._attrs, self._encoding),\n434         )\n435 \n436     @staticmethod\n437     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n438         if isinstance(results, dict):  # persist case\n439             name = array_args[0]\n440             results = {k: v for k, v in results.items() if k[0] == name}\n441         data = array_func(results, *array_args)\n442         return Variable(dims, data, attrs=attrs, encoding=encoding)\n443 \n444     @property\n445     def values(self):\n446         \"\"\"The variable's data as a numpy.ndarray\"\"\"\n447         return _as_array_or_item(self._data)\n448 \n449     @values.setter\n450     def values(self, values):\n451         self.data = values\n452 \n453     def to_base_variable(self):\n454         \"\"\"Return this variable as a base xarray.Variable\"\"\"\n455         return Variable(\n456             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n457         )\n458 \n459     to_variable = utils.alias(to_base_variable, \"to_variable\")\n460 \n461     def to_index_variable(self):\n462         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n463         return IndexVariable(\n464             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n465         )\n466 \n467     to_coord = utils.alias(to_index_variable, \"to_coord\")\n468 \n469     def to_index(self):\n470         \"\"\"Convert this variable to a pandas.Index\"\"\"\n471         return self.to_index_variable().to_index()\n472 \n473     def to_dict(self, data=True):\n474         \"\"\"Dictionary representation of variable.\"\"\"\n475         item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n476         if data:\n477             item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n478         else:\n479             item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n480         return item\n481 \n482     @property\n483     def dims(self):\n484         \"\"\"Tuple of dimension names with which this variable is associated.\n485         \"\"\"\n486         return self._dims\n487 \n488     @dims.setter\n489     def dims(self, value):\n490         self._dims = self._parse_dimensions(value)\n491 \n492     def _parse_dimensions(self, dims):\n493         if isinstance(dims, str):\n494             dims = (dims,)\n495         dims = tuple(dims)\n496         if len(dims) != self.ndim:\n497             raise ValueError(\n498                 \"dimensions %s must have the same length as the \"\n499                 \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n500             )\n501         return dims\n502 \n503     def _item_key_to_tuple(self, key):\n504         if utils.is_dict_like(key):\n505             return tuple(key.get(dim, slice(None)) for dim in self.dims)\n506         else:\n507             return key\n508 \n509     def _broadcast_indexes(self, key):\n510         \"\"\"Prepare an indexing key for an indexing operation.\n511 \n512         Parameters\n513         -----------\n514         key: int, slice, array, dict or tuple of integer, slices and arrays\n515             Any valid input for indexing.\n516 \n517         Returns\n518         -------\n519         dims: tuple\n520             Dimension of the resultant variable.\n521         indexers: IndexingTuple subclass\n522             Tuple of integer, array-like, or slices to use when indexing\n523             self._data. The type of this argument indicates the type of\n524             indexing to perform, either basic, outer or vectorized.\n525         new_order : Optional[Sequence[int]]\n526             Optional reordering to do on the result of indexing. If not None,\n527             the first len(new_order) indexing should be moved to these\n528             positions.\n529         \"\"\"\n530         key = self._item_key_to_tuple(key)  # key is a tuple\n531         # key is a tuple of full size\n532         key = indexing.expanded_indexer(key, self.ndim)\n533         # Convert a scalar Variable to an integer\n534         key = tuple(\n535             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n536         )\n537         # Convert a 0d-array to an integer\n538         key = tuple(\n539             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n540         )\n541 \n542         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n543             return self._broadcast_indexes_basic(key)\n544 \n545         self._validate_indexers(key)\n546         # Detect it can be mapped as an outer indexer\n547         # If all key is unlabeled, or\n548         # key can be mapped as an OuterIndexer.\n549         if all(not isinstance(k, Variable) for k in key):\n550             return self._broadcast_indexes_outer(key)\n551 \n552         # If all key is 1-dimensional and there are no duplicate labels,\n553         # key can be mapped as an OuterIndexer.\n554         dims = []\n555         for k, d in zip(key, self.dims):\n556             if isinstance(k, Variable):\n557                 if len(k.dims) > 1:\n558                     return self._broadcast_indexes_vectorized(key)\n559                 dims.append(k.dims[0])\n560             elif not isinstance(k, integer_types):\n561                 dims.append(d)\n562         if len(set(dims)) == len(dims):\n563             return self._broadcast_indexes_outer(key)\n564 \n565         return self._broadcast_indexes_vectorized(key)\n566 \n567     def _broadcast_indexes_basic(self, key):\n568         dims = tuple(\n569             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n570         )\n571         return dims, BasicIndexer(key), None\n572 \n573     def _validate_indexers(self, key):\n574         \"\"\" Make sanity checks \"\"\"\n575         for dim, k in zip(self.dims, key):\n576             if isinstance(k, BASIC_INDEXING_TYPES):\n577                 pass\n578             else:\n579                 if not isinstance(k, Variable):\n580                     k = np.asarray(k)\n581                     if k.ndim > 1:\n582                         raise IndexError(\n583                             \"Unlabeled multi-dimensional array cannot be \"\n584                             \"used for indexing: {}\".format(k)\n585                         )\n586                 if k.dtype.kind == \"b\":\n587                     if self.shape[self.get_axis_num(dim)] != len(k):\n588                         raise IndexError(\n589                             \"Boolean array size {:d} is used to index array \"\n590                             \"with shape {:s}.\".format(len(k), str(self.shape))\n591                         )\n592                     if k.ndim > 1:\n593                         raise IndexError(\n594                             \"{}-dimensional boolean indexing is \"\n595                             \"not supported. \".format(k.ndim)\n596                         )\n597                     if getattr(k, \"dims\", (dim,)) != (dim,):\n598                         raise IndexError(\n599                             \"Boolean indexer should be unlabeled or on the \"\n600                             \"same dimension to the indexed array. Indexer is \"\n601                             \"on {:s} but the target dimension is {:s}.\".format(\n602                                 str(k.dims), dim\n603                             )\n604                         )\n605 \n606     def _broadcast_indexes_outer(self, key):\n607         dims = tuple(\n608             k.dims[0] if isinstance(k, Variable) else dim\n609             for k, dim in zip(key, self.dims)\n610             if not isinstance(k, integer_types)\n611         )\n612 \n613         new_key = []\n614         for k in key:\n615             if isinstance(k, Variable):\n616                 k = k.data\n617             if not isinstance(k, BASIC_INDEXING_TYPES):\n618                 k = np.asarray(k)\n619                 if k.size == 0:\n620                     # Slice by empty list; numpy could not infer the dtype\n621                     k = k.astype(int)\n622                 elif k.dtype.kind == \"b\":\n623                     (k,) = np.nonzero(k)\n624             new_key.append(k)\n625 \n626         return dims, OuterIndexer(tuple(new_key)), None\n627 \n628     def _nonzero(self):\n629         \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n630         # TODO we should replace dask's native nonzero\n631         # after https://github.com/dask/dask/issues/1076 is implemented.\n632         nonzeros = np.nonzero(self.data)\n633         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n634 \n635     def _broadcast_indexes_vectorized(self, key):\n636         variables = []\n637         out_dims_set = OrderedSet()\n638         for dim, value in zip(self.dims, key):\n639             if isinstance(value, slice):\n640                 out_dims_set.add(dim)\n641             else:\n642                 variable = (\n643                     value\n644                     if isinstance(value, Variable)\n645                     else as_variable(value, name=dim)\n646                 )\n647                 if variable.dtype.kind == \"b\":  # boolean indexing case\n648                     (variable,) = variable._nonzero()\n649 \n650                 variables.append(variable)\n651                 out_dims_set.update(variable.dims)\n652 \n653         variable_dims = set()\n654         for variable in variables:\n655             variable_dims.update(variable.dims)\n656 \n657         slices = []\n658         for i, (dim, value) in enumerate(zip(self.dims, key)):\n659             if isinstance(value, slice):\n660                 if dim in variable_dims:\n661                     # We only convert slice objects to variables if they share\n662                     # a dimension with at least one other variable. Otherwise,\n663                     # we can equivalently leave them as slices aknd transpose\n664                     # the result. This is significantly faster/more efficient\n665                     # for most array backends.\n666                     values = np.arange(*value.indices(self.sizes[dim]))\n667                     variables.insert(i - len(slices), Variable((dim,), values))\n668                 else:\n669                     slices.append((i, value))\n670 \n671         try:\n672             variables = _broadcast_compat_variables(*variables)\n673         except ValueError:\n674             raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n675 \n676         out_key = [variable.data for variable in variables]\n677         out_dims = tuple(out_dims_set)\n678         slice_positions = set()\n679         for i, value in slices:\n680             out_key.insert(i, value)\n681             new_position = out_dims.index(self.dims[i])\n682             slice_positions.add(new_position)\n683 \n684         if slice_positions:\n685             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n686         else:\n687             new_order = None\n688 \n689         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n690 \n691     def __getitem__(self: VariableType, key) -> VariableType:\n692         \"\"\"Return a new Variable object whose contents are consistent with\n693         getting the provided key from the underlying data.\n694 \n695         NB. __getitem__ and __setitem__ implement xarray-style indexing,\n696         where if keys are unlabeled arrays, we index the array orthogonally\n697         with them. If keys are labeled array (such as Variables), they are\n698         broadcasted with our usual scheme and then the array is indexed with\n699         the broadcasted key, like numpy's fancy indexing.\n700 \n701         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n702         array `x.values` directly.\n703         \"\"\"\n704         dims, indexer, new_order = self._broadcast_indexes(key)\n705         data = as_indexable(self._data)[indexer]\n706         if new_order:\n707             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n708         return self._finalize_indexing_result(dims, data)\n709 \n710     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n711         \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\n712         \"\"\"\n713         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n714 \n715     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n716         \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n717         # TODO(shoyer): expose this method in public API somewhere (isel?) and\n718         # use it for reindex.\n719         # TODO(shoyer): add a sanity check that all other integers are\n720         # non-negative\n721         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n722         # that is actually indexed rather than mapping it to the last value\n723         # along each axis.\n724 \n725         if fill_value is dtypes.NA:\n726             fill_value = dtypes.get_fill_value(self.dtype)\n727 \n728         dims, indexer, new_order = self._broadcast_indexes(key)\n729 \n730         if self.size:\n731             if isinstance(self._data, dask_array_type):\n732                 # dask's indexing is faster this way; also vindex does not\n733                 # support negative indices yet:\n734                 # https://github.com/dask/dask/pull/2967\n735                 actual_indexer = indexing.posify_mask_indexer(indexer)\n736             else:\n737                 actual_indexer = indexer\n738 \n739             data = as_indexable(self._data)[actual_indexer]\n740             mask = indexing.create_mask(indexer, self.shape, data)\n741             # we need to invert the mask in order to pass data first. This helps\n742             # pint to choose the correct unit\n743             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n744             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n745         else:\n746             # array cannot be indexed along dimensions of size 0, so just\n747             # build the mask directly instead.\n748             mask = indexing.create_mask(indexer, self.shape)\n749             data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n750 \n751         if new_order:\n752             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n753         return self._finalize_indexing_result(dims, data)\n754 \n755     def __setitem__(self, key, value):\n756         \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n757         orthogonal indexing.\n758 \n759         See __getitem__ for more details.\n760         \"\"\"\n761         dims, index_tuple, new_order = self._broadcast_indexes(key)\n762 \n763         if not isinstance(value, Variable):\n764             value = as_compatible_data(value)\n765             if value.ndim > len(dims):\n766                 raise ValueError(\n767                     \"shape mismatch: value array of shape %s could not be \"\n768                     \"broadcast to indexing result with %s dimensions\"\n769                     % (value.shape, len(dims))\n770                 )\n771             if value.ndim == 0:\n772                 value = Variable((), value)\n773             else:\n774                 value = Variable(dims[-value.ndim :], value)\n775         # broadcast to become assignable\n776         value = value.set_dims(dims).data\n777 \n778         if new_order:\n779             value = duck_array_ops.asarray(value)\n780             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n781             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n782 \n783         indexable = as_indexable(self._data)\n784         indexable[index_tuple] = value\n785 \n786     @property\n787     def attrs(self) -> Dict[Hashable, Any]:\n788         \"\"\"Dictionary of local attributes on this variable.\n789         \"\"\"\n790         if self._attrs is None:\n791             self._attrs = {}\n792         return self._attrs\n793 \n794     @attrs.setter\n795     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n796         self._attrs = dict(value)\n797 \n798     @property\n799     def encoding(self):\n800         \"\"\"Dictionary of encodings on this variable.\n801         \"\"\"\n802         if self._encoding is None:\n803             self._encoding = {}\n804         return self._encoding\n805 \n806     @encoding.setter\n807     def encoding(self, value):\n808         try:\n809             self._encoding = dict(value)\n810         except ValueError:\n811             raise ValueError(\"encoding must be castable to a dictionary\")\n812 \n813     def copy(self, deep=True, data=None):\n814         \"\"\"Returns a copy of this object.\n815 \n816         If `deep=True`, the data array is loaded into memory and copied onto\n817         the new object. Dimensions, attributes and encodings are always copied.\n818 \n819         Use `data` to create a new object with the same structure as\n820         original but entirely new data.\n821 \n822         Parameters\n823         ----------\n824         deep : bool, optional\n825             Whether the data array is loaded into memory and copied onto\n826             the new object. Default is True.\n827         data : array_like, optional\n828             Data to use in the new object. Must have same shape as original.\n829             When `data` is used, `deep` is ignored.\n830 \n831         Returns\n832         -------\n833         object : Variable\n834             New object with dimensions, attributes, encodings, and optionally\n835             data copied from original.\n836 \n837         Examples\n838         --------\n839 \n840         Shallow copy versus deep copy\n841 \n842         >>> var = xr.Variable(data=[1, 2, 3], dims=\"x\")\n843         >>> var.copy()\n844         <xarray.Variable (x: 3)>\n845         array([1, 2, 3])\n846         >>> var_0 = var.copy(deep=False)\n847         >>> var_0[0] = 7\n848         >>> var_0\n849         <xarray.Variable (x: 3)>\n850         array([7, 2, 3])\n851         >>> var\n852         <xarray.Variable (x: 3)>\n853         array([7, 2, 3])\n854 \n855         Changing the data using the ``data`` argument maintains the\n856         structure of the original object, but with the new data. Original\n857         object is unaffected.\n858 \n859         >>> var.copy(data=[0.1, 0.2, 0.3])\n860         <xarray.Variable (x: 3)>\n861         array([ 0.1,  0.2,  0.3])\n862         >>> var\n863         <xarray.Variable (x: 3)>\n864         array([7, 2, 3])\n865 \n866         See Also\n867         --------\n868         pandas.DataFrame.copy\n869         \"\"\"\n870         if data is None:\n871             data = self._data\n872 \n873             if isinstance(data, indexing.MemoryCachedArray):\n874                 # don't share caching between copies\n875                 data = indexing.MemoryCachedArray(data.array)\n876 \n877             if deep:\n878                 if hasattr(data, \"__array_function__\") or isinstance(\n879                     data, dask_array_type\n880                 ):\n881                     data = data.copy()\n882                 elif not isinstance(data, PandasIndexAdapter):\n883                     # pandas.Index is immutable\n884                     data = np.array(data)\n885         else:\n886             data = as_compatible_data(data)\n887             if self.shape != data.shape:\n888                 raise ValueError(\n889                     \"Data shape {} must match shape of object {}\".format(\n890                         data.shape, self.shape\n891                     )\n892                 )\n893 \n894         # note:\n895         # dims is already an immutable tuple\n896         # attributes and encoding will be copied when the new Array is created\n897         return self._replace(data=data)\n898 \n899     def _replace(\n900         self, dims=_default, data=_default, attrs=_default, encoding=_default\n901     ) -> \"Variable\":\n902         if dims is _default:\n903             dims = copy.copy(self._dims)\n904         if data is _default:\n905             data = copy.copy(self.data)\n906         if attrs is _default:\n907             attrs = copy.copy(self._attrs)\n908         if encoding is _default:\n909             encoding = copy.copy(self._encoding)\n910         return type(self)(dims, data, attrs, encoding, fastpath=True)\n911 \n912     def __copy__(self):\n913         return self.copy(deep=False)\n914 \n915     def __deepcopy__(self, memo=None):\n916         # memo does nothing but is required for compatibility with\n917         # copy.deepcopy\n918         return self.copy(deep=True)\n919 \n920     # mutable objects should not be hashable\n921     # https://github.com/python/mypy/issues/4266\n922     __hash__ = None  # type: ignore\n923 \n924     @property\n925     def chunks(self):\n926         \"\"\"Block dimensions for this array's data or None if it's not a dask\n927         array.\n928         \"\"\"\n929         return getattr(self._data, \"chunks\", None)\n930 \n931     _array_counter = itertools.count()\n932 \n933     def chunk(self, chunks=None, name=None, lock=False):\n934         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n935 \n936         If this variable is a non-dask array, it will be converted to dask\n937         array. If it's a dask array, it will be rechunked to the given chunk\n938         sizes.\n939 \n940         If neither chunks is not provided for one or more dimensions, chunk\n941         sizes along that dimension will not be updated; non-dask arrays will be\n942         converted into dask arrays with a single block.\n943 \n944         Parameters\n945         ----------\n946         chunks : int, tuple or dict, optional\n947             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n948             ``{'x': 5, 'y': 5}``.\n949         name : str, optional\n950             Used to generate the name for this array in the internal dask\n951             graph. Does not need not be unique.\n952         lock : optional\n953             Passed on to :py:func:`dask.array.from_array`, if the array is not\n954             already as dask array.\n955 \n956         Returns\n957         -------\n958         chunked : xarray.Variable\n959         \"\"\"\n960         import dask\n961         import dask.array as da\n962 \n963         if utils.is_dict_like(chunks):\n964             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n965 \n966         if chunks is None:\n967             chunks = self.chunks or self.shape\n968 \n969         data = self._data\n970         if isinstance(data, da.Array):\n971             data = data.rechunk(chunks)\n972         else:\n973             if isinstance(data, indexing.ExplicitlyIndexed):\n974                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n975                 # that can't handle general array indexing. For example, in netCDF4 you\n976                 # can do \"outer\" indexing along two dimensions independent, which works\n977                 # differently from how NumPy handles it.\n978                 # da.from_array works by using lazy indexing with a tuple of slices.\n979                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n980                 # different indexing types in an explicit way:\n981                 # https://github.com/dask/dask/issues/2883\n982                 data = indexing.ImplicitToExplicitIndexingAdapter(\n983                     data, indexing.OuterIndexer\n984                 )\n985                 if LooseVersion(dask.__version__) < \"2.0.0\":\n986                     kwargs = {}\n987                 else:\n988                     # All of our lazily loaded backend array classes should use NumPy\n989                     # array operations.\n990                     kwargs = {\"meta\": np.ndarray}\n991             else:\n992                 kwargs = {}\n993 \n994             if utils.is_dict_like(chunks):\n995                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n996 \n997             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n998 \n999         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n1000 \n1001     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n1002         \"\"\"\n1003         use sparse-array as backend.\n1004         \"\"\"\n1005         import sparse\n1006 \n1007         # TODO  what to do if dask-backended?\n1008         if fill_value is dtypes.NA:\n1009             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1010         else:\n1011             dtype = dtypes.result_type(self.dtype, fill_value)\n1012 \n1013         if sparse_format is _default:\n1014             sparse_format = \"coo\"\n1015         try:\n1016             as_sparse = getattr(sparse, \"as_{}\".format(sparse_format.lower()))\n1017         except AttributeError:\n1018             raise ValueError(\"{} is not a valid sparse format\".format(sparse_format))\n1019 \n1020         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n1021         return self._replace(data=data)\n1022 \n1023     def _to_dense(self):\n1024         \"\"\"\n1025         Change backend from sparse to np.array\n1026         \"\"\"\n1027         if hasattr(self._data, \"todense\"):\n1028             return self._replace(data=self._data.todense())\n1029         return self.copy(deep=False)\n1030 \n1031     def isel(\n1032         self: VariableType,\n1033         indexers: Mapping[Hashable, Any] = None,\n1034         missing_dims: str = \"raise\",\n1035         **indexers_kwargs: Any,\n1036     ) -> VariableType:\n1037         \"\"\"Return a new array indexed along the specified dimension(s).\n1038 \n1039         Parameters\n1040         ----------\n1041         **indexers : {dim: indexer, ...}\n1042             Keyword arguments with names matching dimensions and values given\n1043             by integers, slice objects or arrays.\n1044         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1045             What to do if dimensions that should be selected from are not present in the\n1046             DataArray:\n1047             - \"exception\": raise an exception\n1048             - \"warning\": raise a warning, and ignore the missing dimensions\n1049             - \"ignore\": ignore the missing dimensions\n1050 \n1051         Returns\n1052         -------\n1053         obj : Array object\n1054             A new Array with the selected data and dimensions. In general,\n1055             the new variable's data will be a view of this variable's data,\n1056             unless numpy fancy indexing was triggered by using an array\n1057             indexer, in which case the data will be a copy.\n1058         \"\"\"\n1059         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1060 \n1061         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n1062 \n1063         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n1064         return self[key]\n1065 \n1066     def squeeze(self, dim=None):\n1067         \"\"\"Return a new object with squeezed data.\n1068 \n1069         Parameters\n1070         ----------\n1071         dim : None or str or tuple of str, optional\n1072             Selects a subset of the length one dimensions. If a dimension is\n1073             selected with length greater than one, an error is raised. If\n1074             None, all length one dimensions are squeezed.\n1075 \n1076         Returns\n1077         -------\n1078         squeezed : same type as caller\n1079             This object, but with with all or a subset of the dimensions of\n1080             length 1 removed.\n1081 \n1082         See Also\n1083         --------\n1084         numpy.squeeze\n1085         \"\"\"\n1086         dims = common.get_squeeze_dims(self, dim)\n1087         return self.isel({d: 0 for d in dims})\n1088 \n1089     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n1090         axis = self.get_axis_num(dim)\n1091 \n1092         if count > 0:\n1093             keep = slice(None, -count)\n1094         elif count < 0:\n1095             keep = slice(-count, None)\n1096         else:\n1097             keep = slice(None)\n1098 \n1099         trimmed_data = self[(slice(None),) * axis + (keep,)].data\n1100 \n1101         if fill_value is dtypes.NA:\n1102             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1103         else:\n1104             dtype = self.dtype\n1105 \n1106         width = min(abs(count), self.shape[axis])\n1107         dim_pad = (width, 0) if count >= 0 else (0, width)\n1108         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n1109 \n1110         data = duck_array_ops.pad(\n1111             trimmed_data.astype(dtype),\n1112             pads,\n1113             mode=\"constant\",\n1114             constant_values=fill_value,\n1115         )\n1116 \n1117         if isinstance(data, dask_array_type):\n1118             # chunked data should come out with the same chunks; this makes\n1119             # it feasible to combine shifted and unshifted data\n1120             # TODO: remove this once dask.array automatically aligns chunks\n1121             data = data.rechunk(self.data.chunks)\n1122 \n1123         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1124 \n1125     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n1126         \"\"\"\n1127         Return a new Variable with shifted data.\n1128 \n1129         Parameters\n1130         ----------\n1131         shifts : mapping of the form {dim: offset}\n1132             Integer offset to shift along each of the given dimensions.\n1133             Positive offsets shift to the right; negative offsets shift to the\n1134             left.\n1135         fill_value: scalar, optional\n1136             Value to use for newly missing values\n1137         **shifts_kwargs:\n1138             The keyword arguments form of ``shifts``.\n1139             One of shifts or shifts_kwargs must be provided.\n1140 \n1141         Returns\n1142         -------\n1143         shifted : Variable\n1144             Variable with the same dimensions and attributes but shifted data.\n1145         \"\"\"\n1146         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n1147         result = self\n1148         for dim, count in shifts.items():\n1149             result = result._shift_one_dim(dim, count, fill_value=fill_value)\n1150         return result\n1151 \n1152     def _pad_options_dim_to_index(\n1153         self,\n1154         pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],\n1155         fill_with_shape=False,\n1156     ):\n1157         if fill_with_shape:\n1158             return [\n1159                 (n, n) if d not in pad_option else pad_option[d]\n1160                 for d, n in zip(self.dims, self.data.shape)\n1161             ]\n1162         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]\n1163 \n1164     def pad(\n1165         self,\n1166         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n1167         mode: str = \"constant\",\n1168         stat_length: Union[\n1169             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1170         ] = None,\n1171         constant_values: Union[\n1172             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1173         ] = None,\n1174         end_values: Union[\n1175             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1176         ] = None,\n1177         reflect_type: str = None,\n1178         **pad_width_kwargs: Any,\n1179     ):\n1180         \"\"\"\n1181         Return a new Variable with padded data.\n1182 \n1183         Parameters\n1184         ----------\n1185         pad_width: Mapping with the form of {dim: (pad_before, pad_after)}\n1186             Number of values padded along each dimension.\n1187             {dim: pad} is a shortcut for pad_before = pad_after = pad\n1188         mode: (str)\n1189             See numpy / Dask docs\n1190         stat_length : int, tuple or mapping of the form {dim: tuple}\n1191             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n1192             values at edge of each axis used to calculate the statistic value.\n1193         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n1194             Used in 'constant'.  The values to set the padded values for each\n1195             axis.\n1196         end_values : scalar, tuple or mapping of the form {dim: tuple}\n1197             Used in 'linear_ramp'.  The values used for the ending value of the\n1198             linear_ramp and that will form the edge of the padded array.\n1199         reflect_type : {'even', 'odd'}, optional\n1200             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n1201             default with an unaltered reflection around the edge value.  For\n1202             the 'odd' style, the extended part of the array is created by\n1203             subtracting the reflected values from two times the edge value.\n1204         **pad_width_kwargs:\n1205             One of pad_width or pad_width_kwargs must be provided.\n1206 \n1207         Returns\n1208         -------\n1209         padded : Variable\n1210             Variable with the same dimensions and attributes but padded data.\n1211         \"\"\"\n1212         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n1213 \n1214         # change default behaviour of pad with mode constant\n1215         if mode == \"constant\" and (\n1216             constant_values is None or constant_values is dtypes.NA\n1217         ):\n1218             dtype, constant_values = dtypes.maybe_promote(self.dtype)\n1219         else:\n1220             dtype = self.dtype\n1221 \n1222         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty\n1223         if isinstance(stat_length, dict):\n1224             stat_length = self._pad_options_dim_to_index(\n1225                 stat_length, fill_with_shape=True\n1226             )\n1227         if isinstance(constant_values, dict):\n1228             constant_values = self._pad_options_dim_to_index(constant_values)\n1229         if isinstance(end_values, dict):\n1230             end_values = self._pad_options_dim_to_index(end_values)\n1231 \n1232         # workaround for bug in Dask's default value of stat_length  https://github.com/dask/dask/issues/5303\n1233         if stat_length is None and mode in [\"maximum\", \"mean\", \"median\", \"minimum\"]:\n1234             stat_length = [(n, n) for n in self.data.shape]  # type: ignore\n1235 \n1236         # change integer values to a tuple of two of those values and change pad_width to index\n1237         for k, v in pad_width.items():\n1238             if isinstance(v, numbers.Number):\n1239                 pad_width[k] = (v, v)\n1240         pad_width_by_index = self._pad_options_dim_to_index(pad_width)\n1241 \n1242         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty\n1243         pad_option_kwargs = {}\n1244         if stat_length is not None:\n1245             pad_option_kwargs[\"stat_length\"] = stat_length\n1246         if constant_values is not None:\n1247             pad_option_kwargs[\"constant_values\"] = constant_values\n1248         if end_values is not None:\n1249             pad_option_kwargs[\"end_values\"] = end_values\n1250         if reflect_type is not None:\n1251             pad_option_kwargs[\"reflect_type\"] = reflect_type  # type: ignore\n1252 \n1253         array = duck_array_ops.pad(\n1254             self.data.astype(dtype, copy=False),\n1255             pad_width_by_index,\n1256             mode=mode,\n1257             **pad_option_kwargs,\n1258         )\n1259 \n1260         return type(self)(self.dims, array)\n1261 \n1262     def _roll_one_dim(self, dim, count):\n1263         axis = self.get_axis_num(dim)\n1264 \n1265         count %= self.shape[axis]\n1266         if count != 0:\n1267             indices = [slice(-count, None), slice(None, -count)]\n1268         else:\n1269             indices = [slice(None)]\n1270 \n1271         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n1272 \n1273         data = duck_array_ops.concatenate(arrays, axis)\n1274 \n1275         if isinstance(data, dask_array_type):\n1276             # chunked data should come out with the same chunks; this makes\n1277             # it feasible to combine shifted and unshifted data\n1278             # TODO: remove this once dask.array automatically aligns chunks\n1279             data = data.rechunk(self.data.chunks)\n1280 \n1281         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1282 \n1283     def roll(self, shifts=None, **shifts_kwargs):\n1284         \"\"\"\n1285         Return a new Variable with rolld data.\n1286 \n1287         Parameters\n1288         ----------\n1289         shifts : mapping of the form {dim: offset}\n1290             Integer offset to roll along each of the given dimensions.\n1291             Positive offsets roll to the right; negative offsets roll to the\n1292             left.\n1293         **shifts_kwargs:\n1294             The keyword arguments form of ``shifts``.\n1295             One of shifts or shifts_kwargs must be provided.\n1296 \n1297         Returns\n1298         -------\n1299         shifted : Variable\n1300             Variable with the same dimensions and attributes but rolled data.\n1301         \"\"\"\n1302         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n1303 \n1304         result = self\n1305         for dim, count in shifts.items():\n1306             result = result._roll_one_dim(dim, count)\n1307         return result\n1308 \n1309     def transpose(self, *dims) -> \"Variable\":\n1310         \"\"\"Return a new Variable object with transposed dimensions.\n1311 \n1312         Parameters\n1313         ----------\n1314         *dims : str, optional\n1315             By default, reverse the dimensions. Otherwise, reorder the\n1316             dimensions to this order.\n1317 \n1318         Returns\n1319         -------\n1320         transposed : Variable\n1321             The returned object has transposed data and dimensions with the\n1322             same attributes as the original.\n1323 \n1324         Notes\n1325         -----\n1326         This operation returns a view of this variable's data. It is\n1327         lazy for dask-backed Variables but not for numpy-backed Variables.\n1328 \n1329         See Also\n1330         --------\n1331         numpy.transpose\n1332         \"\"\"\n1333         if len(dims) == 0:\n1334             dims = self.dims[::-1]\n1335         dims = tuple(infix_dims(dims, self.dims))\n1336         axes = self.get_axis_num(dims)\n1337         if len(dims) < 2 or dims == self.dims:\n1338             # no need to transpose if only one dimension\n1339             # or dims are in same order\n1340             return self.copy(deep=False)\n1341 \n1342         data = as_indexable(self._data).transpose(axes)\n1343         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n1344 \n1345     @property\n1346     def T(self) -> \"Variable\":\n1347         return self.transpose()\n1348 \n1349     def set_dims(self, dims, shape=None):\n1350         \"\"\"Return a new variable with given set of dimensions.\n1351         This method might be used to attach new dimension(s) to variable.\n1352 \n1353         When possible, this operation does not copy this variable's data.\n1354 \n1355         Parameters\n1356         ----------\n1357         dims : str or sequence of str or dict\n1358             Dimensions to include on the new variable. If a dict, values are\n1359             used to provide the sizes of new dimensions; otherwise, new\n1360             dimensions are inserted with length 1.\n1361 \n1362         Returns\n1363         -------\n1364         Variable\n1365         \"\"\"\n1366         if isinstance(dims, str):\n1367             dims = [dims]\n1368 \n1369         if shape is None and utils.is_dict_like(dims):\n1370             shape = dims.values()\n1371 \n1372         missing_dims = set(self.dims) - set(dims)\n1373         if missing_dims:\n1374             raise ValueError(\n1375                 \"new dimensions %r must be a superset of \"\n1376                 \"existing dimensions %r\" % (dims, self.dims)\n1377             )\n1378 \n1379         self_dims = set(self.dims)\n1380         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n1381 \n1382         if self.dims == expanded_dims:\n1383             # don't use broadcast_to unless necessary so the result remains\n1384             # writeable if possible\n1385             expanded_data = self.data\n1386         elif shape is not None:\n1387             dims_map = dict(zip(dims, shape))\n1388             tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n1389             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n1390         else:\n1391             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n1392 \n1393         expanded_var = Variable(\n1394             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n1395         )\n1396         return expanded_var.transpose(*dims)\n1397 \n1398     def _stack_once(self, dims, new_dim):\n1399         if not set(dims) <= set(self.dims):\n1400             raise ValueError(\"invalid existing dimensions: %s\" % dims)\n1401 \n1402         if new_dim in self.dims:\n1403             raise ValueError(\n1404                 \"cannot create a new dimension with the same \"\n1405                 \"name as an existing dimension\"\n1406             )\n1407 \n1408         if len(dims) == 0:\n1409             # don't stack\n1410             return self.copy(deep=False)\n1411 \n1412         other_dims = [d for d in self.dims if d not in dims]\n1413         dim_order = other_dims + list(dims)\n1414         reordered = self.transpose(*dim_order)\n1415 \n1416         new_shape = reordered.shape[: len(other_dims)] + (-1,)\n1417         new_data = reordered.data.reshape(new_shape)\n1418         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n1419 \n1420         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1421 \n1422     def stack(self, dimensions=None, **dimensions_kwargs):\n1423         \"\"\"\n1424         Stack any number of existing dimensions into a single new dimension.\n1425 \n1426         New dimensions will be added at the end, and the order of the data\n1427         along each new dimension will be in contiguous (C) order.\n1428 \n1429         Parameters\n1430         ----------\n1431         dimensions : Mapping of form new_name=(dim1, dim2, ...)\n1432             Names of new dimensions, and the existing dimensions that they\n1433             replace.\n1434         **dimensions_kwargs:\n1435             The keyword arguments form of ``dimensions``.\n1436             One of dimensions or dimensions_kwargs must be provided.\n1437 \n1438         Returns\n1439         -------\n1440         stacked : Variable\n1441             Variable with the same attributes but stacked data.\n1442 \n1443         See also\n1444         --------\n1445         Variable.unstack\n1446         \"\"\"\n1447         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n1448         result = self\n1449         for new_dim, dims in dimensions.items():\n1450             result = result._stack_once(dims, new_dim)\n1451         return result\n1452 \n1453     def _unstack_once(self, dims, old_dim):\n1454         new_dim_names = tuple(dims.keys())\n1455         new_dim_sizes = tuple(dims.values())\n1456 \n1457         if old_dim not in self.dims:\n1458             raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n1459 \n1460         if set(new_dim_names).intersection(self.dims):\n1461             raise ValueError(\n1462                 \"cannot create a new dimension with the same \"\n1463                 \"name as an existing dimension\"\n1464             )\n1465 \n1466         if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n1467             raise ValueError(\n1468                 \"the product of the new dimension sizes must \"\n1469                 \"equal the size of the old dimension\"\n1470             )\n1471 \n1472         other_dims = [d for d in self.dims if d != old_dim]\n1473         dim_order = other_dims + [old_dim]\n1474         reordered = self.transpose(*dim_order)\n1475 \n1476         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n1477         new_data = reordered.data.reshape(new_shape)\n1478         new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n1479 \n1480         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1481 \n1482     def unstack(self, dimensions=None, **dimensions_kwargs):\n1483         \"\"\"\n1484         Unstack an existing dimension into multiple new dimensions.\n1485 \n1486         New dimensions will be added at the end, and the order of the data\n1487         along each new dimension will be in contiguous (C) order.\n1488 \n1489         Parameters\n1490         ----------\n1491         dimensions : mapping of the form old_dim={dim1: size1, ...}\n1492             Names of existing dimensions, and the new dimensions and sizes\n1493             that they map to.\n1494         **dimensions_kwargs:\n1495             The keyword arguments form of ``dimensions``.\n1496             One of dimensions or dimensions_kwargs must be provided.\n1497 \n1498         Returns\n1499         -------\n1500         unstacked : Variable\n1501             Variable with the same attributes but unstacked data.\n1502 \n1503         See also\n1504         --------\n1505         Variable.stack\n1506         \"\"\"\n1507         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n1508         result = self\n1509         for old_dim, dims in dimensions.items():\n1510             result = result._unstack_once(dims, old_dim)\n1511         return result\n1512 \n1513     def fillna(self, value):\n1514         return ops.fillna(self, value)\n1515 \n1516     def where(self, cond, other=dtypes.NA):\n1517         return ops.where_method(self, cond, other)\n1518 \n1519     def reduce(\n1520         self,\n1521         func,\n1522         dim=None,\n1523         axis=None,\n1524         keep_attrs=None,\n1525         keepdims=False,\n1526         allow_lazy=None,\n1527         **kwargs,\n1528     ):\n1529         \"\"\"Reduce this array by applying `func` along some dimension(s).\n1530 \n1531         Parameters\n1532         ----------\n1533         func : function\n1534             Function which can be called in the form\n1535             `func(x, axis=axis, **kwargs)` to return the result of reducing an\n1536             np.ndarray over an integer valued axis.\n1537         dim : str or sequence of str, optional\n1538             Dimension(s) over which to apply `func`.\n1539         axis : int or sequence of int, optional\n1540             Axis(es) over which to apply `func`. Only one of the 'dim'\n1541             and 'axis' arguments can be supplied. If neither are supplied, then\n1542             the reduction is calculated over the flattened array (by calling\n1543             `func(x)` without an axis argument).\n1544         keep_attrs : bool, optional\n1545             If True, the variable's attributes (`attrs`) will be copied from\n1546             the original object to the new one.  If False (default), the new\n1547             object will be returned without attributes.\n1548         keepdims : bool, default False\n1549             If True, the dimensions which are reduced are left in the result\n1550             as dimensions of size one\n1551         **kwargs : dict\n1552             Additional keyword arguments passed on to `func`.\n1553 \n1554         Returns\n1555         -------\n1556         reduced : Array\n1557             Array with summarized data and the indicated dimension(s)\n1558             removed.\n1559         \"\"\"\n1560         if dim == ...:\n1561             dim = None\n1562         if dim is not None and axis is not None:\n1563             raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n1564 \n1565         if dim is not None:\n1566             axis = self.get_axis_num(dim)\n1567 \n1568         if allow_lazy is not None:\n1569             warnings.warn(\n1570                 \"allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.\",\n1571                 DeprecationWarning,\n1572             )\n1573         else:\n1574             allow_lazy = True\n1575 \n1576         input_data = self.data if allow_lazy else self.values\n1577 \n1578         if axis is not None:\n1579             data = func(input_data, axis=axis, **kwargs)\n1580         else:\n1581             data = func(input_data, **kwargs)\n1582 \n1583         if getattr(data, \"shape\", ()) == self.shape:\n1584             dims = self.dims\n1585         else:\n1586             removed_axes = (\n1587                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n1588             )\n1589             if keepdims:\n1590                 # Insert np.newaxis for removed dims\n1591                 slices = tuple(\n1592                     np.newaxis if i in removed_axes else slice(None, None)\n1593                     for i in range(self.ndim)\n1594                 )\n1595                 if getattr(data, \"shape\", None) is None:\n1596                     # Reduce has produced a scalar value, not an array-like\n1597                     data = np.asanyarray(data)[slices]\n1598                 else:\n1599                     data = data[slices]\n1600                 dims = self.dims\n1601             else:\n1602                 dims = [\n1603                     adim for n, adim in enumerate(self.dims) if n not in removed_axes\n1604                 ]\n1605 \n1606         if keep_attrs is None:\n1607             keep_attrs = _get_keep_attrs(default=False)\n1608         attrs = self._attrs if keep_attrs else None\n1609 \n1610         return Variable(dims, data, attrs=attrs)\n1611 \n1612     @classmethod\n1613     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n1614         \"\"\"Concatenate variables along a new or existing dimension.\n1615 \n1616         Parameters\n1617         ----------\n1618         variables : iterable of Array\n1619             Arrays to stack together. Each variable is expected to have\n1620             matching dimensions and shape except for along the stacked\n1621             dimension.\n1622         dim : str or DataArray, optional\n1623             Name of the dimension to stack along. This can either be a new\n1624             dimension name, in which case it is added along axis=0, or an\n1625             existing dimension name, in which case the location of the\n1626             dimension is unchanged. Where to insert the new dimension is\n1627             determined by the first variable.\n1628         positions : None or list of integer arrays, optional\n1629             List of integer arrays which specifies the integer positions to\n1630             which to assign each dataset along the concatenated dimension.\n1631             If not supplied, objects are concatenated in the provided order.\n1632         shortcut : bool, optional\n1633             This option is used internally to speed-up groupby operations.\n1634             If `shortcut` is True, some checks of internal consistency between\n1635             arrays to concatenate are skipped.\n1636 \n1637         Returns\n1638         -------\n1639         stacked : Variable\n1640             Concatenated Variable formed by stacking all the supplied variables\n1641             along the given dimension.\n1642         \"\"\"\n1643         if not isinstance(dim, str):\n1644             (dim,) = dim.dims\n1645 \n1646         # can't do this lazily: we need to loop through variables at least\n1647         # twice\n1648         variables = list(variables)\n1649         first_var = variables[0]\n1650 \n1651         arrays = [v.data for v in variables]\n1652 \n1653         if dim in first_var.dims:\n1654             axis = first_var.get_axis_num(dim)\n1655             dims = first_var.dims\n1656             data = duck_array_ops.concatenate(arrays, axis=axis)\n1657             if positions is not None:\n1658                 # TODO: deprecate this option -- we don't need it for groupby\n1659                 # any more.\n1660                 indices = nputils.inverse_permutation(np.concatenate(positions))\n1661                 data = duck_array_ops.take(data, indices, axis=axis)\n1662         else:\n1663             axis = 0\n1664             dims = (dim,) + first_var.dims\n1665             data = duck_array_ops.stack(arrays, axis=axis)\n1666 \n1667         attrs = dict(first_var.attrs)\n1668         encoding = dict(first_var.encoding)\n1669         if not shortcut:\n1670             for var in variables:\n1671                 if var.dims != first_var.dims:\n1672                     raise ValueError(\n1673                         f\"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}\"\n1674                     )\n1675 \n1676         return cls(dims, data, attrs, encoding)\n1677 \n1678     def equals(self, other, equiv=duck_array_ops.array_equiv):\n1679         \"\"\"True if two Variables have the same dimensions and values;\n1680         otherwise False.\n1681 \n1682         Variables can still be equal (like pandas objects) if they have NaN\n1683         values in the same locations.\n1684 \n1685         This method is necessary because `v1 == v2` for Variables\n1686         does element-wise comparisons (like numpy.ndarrays).\n1687         \"\"\"\n1688         other = getattr(other, \"variable\", other)\n1689         try:\n1690             return self.dims == other.dims and (\n1691                 self._data is other._data or equiv(self.data, other.data)\n1692             )\n1693         except (TypeError, AttributeError):\n1694             return False\n1695 \n1696     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n1697         \"\"\"True if two Variables have the values after being broadcast against\n1698         each other; otherwise False.\n1699 \n1700         Variables can still be equal (like pandas objects) if they have NaN\n1701         values in the same locations.\n1702         \"\"\"\n1703         try:\n1704             self, other = broadcast_variables(self, other)\n1705         except (ValueError, AttributeError):\n1706             return False\n1707         return self.equals(other, equiv=equiv)\n1708 \n1709     def identical(self, other, equiv=duck_array_ops.array_equiv):\n1710         \"\"\"Like equals, but also checks attributes.\n1711         \"\"\"\n1712         try:\n1713             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n1714                 other, equiv=equiv\n1715             )\n1716         except (TypeError, AttributeError):\n1717             return False\n1718 \n1719     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n1720         \"\"\"True if the intersection of two Variable's non-null data is\n1721         equal; otherwise false.\n1722 \n1723         Variables can thus still be equal if there are locations where either,\n1724         or both, contain NaN values.\n1725         \"\"\"\n1726         return self.broadcast_equals(other, equiv=equiv)\n1727 \n1728     def quantile(\n1729         self, q, dim=None, interpolation=\"linear\", keep_attrs=None, skipna=True\n1730     ):\n1731         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1732 \n1733         Returns the qth quantiles(s) of the array elements.\n1734 \n1735         Parameters\n1736         ----------\n1737         q : float in range of [0,1] (or sequence of floats)\n1738             Quantile to compute, which must be between 0 and 1\n1739             inclusive.\n1740         dim : str or sequence of str, optional\n1741             Dimension(s) over which to apply quantile.\n1742         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n1743             This optional parameter specifies the interpolation method to\n1744             use when the desired quantile lies between two data points\n1745             ``i < j``:\n1746 \n1747                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1748                   the fractional part of the index surrounded by ``i`` and\n1749                   ``j``.\n1750                 * lower: ``i``.\n1751                 * higher: ``j``.\n1752                 * nearest: ``i`` or ``j``, whichever is nearest.\n1753                 * midpoint: ``(i + j) / 2``.\n1754 \n1755         keep_attrs : bool, optional\n1756             If True, the variable's attributes (`attrs`) will be copied from\n1757             the original object to the new one.  If False (default), the new\n1758             object will be returned without attributes.\n1759 \n1760         Returns\n1761         -------\n1762         quantiles : Variable\n1763             If `q` is a single quantile, then the result\n1764             is a scalar. If multiple percentiles are given, first axis of\n1765             the result corresponds to the quantile and a quantile dimension\n1766             is added to the return array. The other dimensions are the\n1767             dimensions that remain after the reduction of the array.\n1768 \n1769         See Also\n1770         --------\n1771         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,\n1772         DataArray.quantile\n1773         \"\"\"\n1774 \n1775         from .computation import apply_ufunc\n1776 \n1777         _quantile_func = np.nanquantile if skipna else np.quantile\n1778 \n1779         if keep_attrs is None:\n1780             keep_attrs = _get_keep_attrs(default=False)\n1781 \n1782         scalar = utils.is_scalar(q)\n1783         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1784 \n1785         if dim is None:\n1786             dim = self.dims\n1787 \n1788         if utils.is_scalar(dim):\n1789             dim = [dim]\n1790 \n1791         def _wrapper(npa, **kwargs):\n1792             # move quantile axis to end. required for apply_ufunc\n1793             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n1794 \n1795         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1796         result = apply_ufunc(\n1797             _wrapper,\n1798             self,\n1799             input_core_dims=[dim],\n1800             exclude_dims=set(dim),\n1801             output_core_dims=[[\"quantile\"]],\n1802             output_dtypes=[np.float64],\n1803             output_sizes={\"quantile\": len(q)},\n1804             dask=\"parallelized\",\n1805             kwargs={\"q\": q, \"axis\": axis, \"interpolation\": interpolation},\n1806         )\n1807 \n1808         # for backward compatibility\n1809         result = result.transpose(\"quantile\", ...)\n1810         if scalar:\n1811             result = result.squeeze(\"quantile\")\n1812         if keep_attrs:\n1813             result.attrs = self._attrs\n1814         return result\n1815 \n1816     def rank(self, dim, pct=False):\n1817         \"\"\"Ranks the data.\n1818 \n1819         Equal values are assigned a rank that is the average of the ranks that\n1820         would have been otherwise assigned to all of the values within that\n1821         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n1822 \n1823         NaNs in the input array are returned as NaNs.\n1824 \n1825         The `bottleneck` library is required.\n1826 \n1827         Parameters\n1828         ----------\n1829         dim : str\n1830             Dimension over which to compute rank.\n1831         pct : bool, optional\n1832             If True, compute percentage ranks, otherwise compute integer ranks.\n1833 \n1834         Returns\n1835         -------\n1836         ranked : Variable\n1837 \n1838         See Also\n1839         --------\n1840         Dataset.rank, DataArray.rank\n1841         \"\"\"\n1842         import bottleneck as bn\n1843 \n1844         data = self.data\n1845 \n1846         if isinstance(data, dask_array_type):\n1847             raise TypeError(\n1848                 \"rank does not work for arrays stored as dask \"\n1849                 \"arrays. Load the data via .compute() or .load() \"\n1850                 \"prior to calling this method.\"\n1851             )\n1852         elif not isinstance(data, np.ndarray):\n1853             raise TypeError(\n1854                 \"rank is not implemented for {} objects.\".format(type(data))\n1855             )\n1856 \n1857         axis = self.get_axis_num(dim)\n1858         func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n1859         ranked = func(data, axis=axis)\n1860         if pct:\n1861             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n1862             ranked /= count\n1863         return Variable(self.dims, ranked)\n1864 \n1865     def rolling_window(\n1866         self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n1867     ):\n1868         \"\"\"\n1869         Make a rolling_window along dim and add a new_dim to the last place.\n1870 \n1871         Parameters\n1872         ----------\n1873         dim: str\n1874             Dimension over which to compute rolling_window\n1875         window: int\n1876             Window size of the rolling\n1877         window_dim: str\n1878             New name of the window dimension.\n1879         center: boolean. default False.\n1880             If True, pad fill_value for both ends. Otherwise, pad in the head\n1881             of the axis.\n1882         fill_value:\n1883             value to be filled.\n1884 \n1885         Returns\n1886         -------\n1887         Variable that is a view of the original array with a added dimension of\n1888         size w.\n1889         The return dim: self.dims + (window_dim, )\n1890         The return shape: self.shape + (window, )\n1891 \n1892         Examples\n1893         --------\n1894         >>> v = Variable((\"a\", \"b\"), np.arange(8).reshape((2, 4)))\n1895         >>> v.rolling_window(x, \"b\", 3, \"window_dim\")\n1896         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1897         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n1898                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n1899 \n1900         >>> v.rolling_window(x, \"b\", 3, \"window_dim\", center=True)\n1901         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1902         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n1903                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n1904         \"\"\"\n1905         if fill_value is dtypes.NA:  # np.nan is passed\n1906             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1907             array = self.astype(dtype, copy=False).data\n1908         else:\n1909             dtype = self.dtype\n1910             array = self.data\n1911 \n1912         new_dims = self.dims + (window_dim,)\n1913         return Variable(\n1914             new_dims,\n1915             duck_array_ops.rolling_window(\n1916                 array,\n1917                 axis=self.get_axis_num(dim),\n1918                 window=window,\n1919                 center=center,\n1920                 fill_value=fill_value,\n1921             ),\n1922         )\n1923 \n1924     def coarsen(self, windows, func, boundary=\"exact\", side=\"left\", **kwargs):\n1925         \"\"\"\n1926         Apply reduction function.\n1927         \"\"\"\n1928         windows = {k: v for k, v in windows.items() if k in self.dims}\n1929         if not windows:\n1930             return self.copy()\n1931 \n1932         reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n1933         if isinstance(func, str):\n1934             name = func\n1935             func = getattr(duck_array_ops, name, None)\n1936             if func is None:\n1937                 raise NameError(f\"{name} is not a valid method.\")\n1938         return self._replace(data=func(reshaped, axis=axes, **kwargs))\n1939 \n1940     def _coarsen_reshape(self, windows, boundary, side):\n1941         \"\"\"\n1942         Construct a reshaped-array for coarsen\n1943         \"\"\"\n1944         if not utils.is_dict_like(boundary):\n1945             boundary = {d: boundary for d in windows.keys()}\n1946 \n1947         if not utils.is_dict_like(side):\n1948             side = {d: side for d in windows.keys()}\n1949 \n1950         # remove unrelated dimensions\n1951         boundary = {k: v for k, v in boundary.items() if k in windows}\n1952         side = {k: v for k, v in side.items() if k in windows}\n1953 \n1954         for d, window in windows.items():\n1955             if window <= 0:\n1956                 raise ValueError(f\"window must be > 0. Given {window}\")\n1957 \n1958         variable = self\n1959         for d, window in windows.items():\n1960             # trim or pad the object\n1961             size = variable.shape[self._get_axis_num(d)]\n1962             n = int(size / window)\n1963             if boundary[d] == \"exact\":\n1964                 if n * window != size:\n1965                     raise ValueError(\n1966                         \"Could not coarsen a dimension of size {} with \"\n1967                         \"window {}\".format(size, window)\n1968                     )\n1969             elif boundary[d] == \"trim\":\n1970                 if side[d] == \"left\":\n1971                     variable = variable.isel({d: slice(0, window * n)})\n1972                 else:\n1973                     excess = size - window * n\n1974                     variable = variable.isel({d: slice(excess, None)})\n1975             elif boundary[d] == \"pad\":  # pad\n1976                 pad = window * n - size\n1977                 if pad < 0:\n1978                     pad += window\n1979                 if side[d] == \"left\":\n1980                     pad_width = {d: (0, pad)}\n1981                 else:\n1982                     pad_width = {d: (pad, 0)}\n1983                 variable = variable.pad(pad_width, mode=\"constant\")\n1984             else:\n1985                 raise TypeError(\n1986                     \"{} is invalid for boundary. Valid option is 'exact', \"\n1987                     \"'trim' and 'pad'\".format(boundary[d])\n1988                 )\n1989 \n1990         shape = []\n1991         axes = []\n1992         axis_count = 0\n1993         for i, d in enumerate(variable.dims):\n1994             if d in windows:\n1995                 size = variable.shape[i]\n1996                 shape.append(int(size / windows[d]))\n1997                 shape.append(windows[d])\n1998                 axis_count += 1\n1999                 axes.append(i + axis_count)\n2000             else:\n2001                 shape.append(variable.shape[i])\n2002 \n2003         keep_attrs = _get_keep_attrs(default=False)\n2004         variable.attrs = variable._attrs if keep_attrs else {}\n2005 \n2006         return variable.data.reshape(shape), tuple(axes)\n2007 \n2008     @property\n2009     def real(self):\n2010         return type(self)(self.dims, self.data.real, self._attrs)\n2011 \n2012     @property\n2013     def imag(self):\n2014         return type(self)(self.dims, self.data.imag, self._attrs)\n2015 \n2016     def __array_wrap__(self, obj, context=None):\n2017         return Variable(self.dims, obj)\n2018 \n2019     @staticmethod\n2020     def _unary_op(f):\n2021         @functools.wraps(f)\n2022         def func(self, *args, **kwargs):\n2023             with np.errstate(all=\"ignore\"):\n2024                 return self.__array_wrap__(f(self.data, *args, **kwargs))\n2025 \n2026         return func\n2027 \n2028     @staticmethod\n2029     def _binary_op(f, reflexive=False, **ignored_kwargs):\n2030         @functools.wraps(f)\n2031         def func(self, other):\n2032             if isinstance(other, (xr.DataArray, xr.Dataset)):\n2033                 return NotImplemented\n2034             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2035             keep_attrs = _get_keep_attrs(default=False)\n2036             attrs = self._attrs if keep_attrs else None\n2037             with np.errstate(all=\"ignore\"):\n2038                 new_data = (\n2039                     f(self_data, other_data)\n2040                     if not reflexive\n2041                     else f(other_data, self_data)\n2042                 )\n2043             result = Variable(dims, new_data, attrs=attrs)\n2044             return result\n2045 \n2046         return func\n2047 \n2048     @staticmethod\n2049     def _inplace_binary_op(f):\n2050         @functools.wraps(f)\n2051         def func(self, other):\n2052             if isinstance(other, xr.Dataset):\n2053                 raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n2054             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2055             if dims != self.dims:\n2056                 raise ValueError(\"dimensions cannot change for in-place \" \"operations\")\n2057             with np.errstate(all=\"ignore\"):\n2058                 self.values = f(self_data, other_data)\n2059             return self\n2060 \n2061         return func\n2062 \n2063     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n2064         \"\"\" A (private) method to convert datetime array to numeric dtype\n2065         See duck_array_ops.datetime_to_numeric\n2066         \"\"\"\n2067         numeric_array = duck_array_ops.datetime_to_numeric(\n2068             self.data, offset, datetime_unit, dtype\n2069         )\n2070         return type(self)(self.dims, numeric_array, self._attrs)\n2071 \n2072 \n2073 ops.inject_all_ops_and_reduce_methods(Variable)\n2074 \n2075 \n2076 class IndexVariable(Variable):\n2077     \"\"\"Wrapper for accommodating a pandas.Index in an xarray.Variable.\n2078 \n2079     IndexVariable preserve loaded values in the form of a pandas.Index instead\n2080     of a NumPy array. Hence, their values are immutable and must always be one-\n2081     dimensional.\n2082 \n2083     They also have a name property, which is the name of their sole dimension\n2084     unless another name is given.\n2085     \"\"\"\n2086 \n2087     __slots__ = ()\n2088 \n2089     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n2090         super().__init__(dims, data, attrs, encoding, fastpath)\n2091         if self.ndim != 1:\n2092             raise ValueError(\"%s objects must be 1-dimensional\" % type(self).__name__)\n2093 \n2094         # Unlike in Variable, always eagerly load values into memory\n2095         if not isinstance(self._data, PandasIndexAdapter):\n2096             self._data = PandasIndexAdapter(self._data)\n2097 \n2098     def __dask_tokenize__(self):\n2099         from dask.base import normalize_token\n2100 \n2101         # Don't waste time converting pd.Index to np.ndarray\n2102         return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n2103 \n2104     def load(self):\n2105         # data is already loaded into memory for IndexVariable\n2106         return self\n2107 \n2108     # https://github.com/python/mypy/issues/1465\n2109     @Variable.data.setter  # type: ignore\n2110     def data(self, data):\n2111         raise ValueError(\n2112             f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n2113             f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n2114         )\n2115 \n2116     @Variable.values.setter  # type: ignore\n2117     def values(self, values):\n2118         raise ValueError(\n2119             f\"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n2120             f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n2121         )\n2122 \n2123     def chunk(self, chunks=None, name=None, lock=False):\n2124         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()\n2125         return self.copy(deep=False)\n2126 \n2127     def _as_sparse(self, sparse_format=_default, fill_value=_default):\n2128         # Dummy\n2129         return self.copy(deep=False)\n2130 \n2131     def _to_dense(self):\n2132         # Dummy\n2133         return self.copy(deep=False)\n2134 \n2135     def _finalize_indexing_result(self, dims, data):\n2136         if getattr(data, \"ndim\", 0) != 1:\n2137             # returns Variable rather than IndexVariable if multi-dimensional\n2138             return Variable(dims, data, self._attrs, self._encoding)\n2139         else:\n2140             return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n2141 \n2142     def __setitem__(self, key, value):\n2143         raise TypeError(\"%s values cannot be modified\" % type(self).__name__)\n2144 \n2145     @classmethod\n2146     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2147         \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n2148 \n2149         This exists because we want to avoid converting Index objects to NumPy\n2150         arrays, if possible.\n2151         \"\"\"\n2152         if not isinstance(dim, str):\n2153             (dim,) = dim.dims\n2154 \n2155         variables = list(variables)\n2156         first_var = variables[0]\n2157 \n2158         if any(not isinstance(v, cls) for v in variables):\n2159             raise TypeError(\n2160                 \"IndexVariable.concat requires that all input \"\n2161                 \"variables be IndexVariable objects\"\n2162             )\n2163 \n2164         indexes = [v._data.array for v in variables]\n2165 \n2166         if not indexes:\n2167             data = []\n2168         else:\n2169             data = indexes[0].append(indexes[1:])\n2170 \n2171             if positions is not None:\n2172                 indices = nputils.inverse_permutation(np.concatenate(positions))\n2173                 data = data.take(indices)\n2174 \n2175         attrs = dict(first_var.attrs)\n2176         if not shortcut:\n2177             for var in variables:\n2178                 if var.dims != first_var.dims:\n2179                     raise ValueError(\"inconsistent dimensions\")\n2180                 utils.remove_incompatible_items(attrs, var.attrs)\n2181 \n2182         return cls(first_var.dims, data, attrs)\n2183 \n2184     def copy(self, deep=True, data=None):\n2185         \"\"\"Returns a copy of this object.\n2186 \n2187         `deep` is ignored since data is stored in the form of\n2188         pandas.Index, which is already immutable. Dimensions, attributes\n2189         and encodings are always copied.\n2190 \n2191         Use `data` to create a new object with the same structure as\n2192         original but entirely new data.\n2193 \n2194         Parameters\n2195         ----------\n2196         deep : bool, optional\n2197             Deep is ignored when data is given. Whether the data array is\n2198             loaded into memory and copied onto the new object. Default is True.\n2199         data : array_like, optional\n2200             Data to use in the new object. Must have same shape as original.\n2201 \n2202         Returns\n2203         -------\n2204         object : Variable\n2205             New object with dimensions, attributes, encodings, and optionally\n2206             data copied from original.\n2207         \"\"\"\n2208         if data is None:\n2209             data = self._data.copy(deep=deep)\n2210         else:\n2211             data = as_compatible_data(data)\n2212             if self.shape != data.shape:\n2213                 raise ValueError(\n2214                     \"Data shape {} must match shape of object {}\".format(\n2215                         data.shape, self.shape\n2216                     )\n2217                 )\n2218         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n2219 \n2220     def equals(self, other, equiv=None):\n2221         # if equiv is specified, super up\n2222         if equiv is not None:\n2223             return super().equals(other, equiv)\n2224 \n2225         # otherwise use the native index equals, rather than looking at _data\n2226         other = getattr(other, \"variable\", other)\n2227         try:\n2228             return self.dims == other.dims and self._data_equals(other)\n2229         except (TypeError, AttributeError):\n2230             return False\n2231 \n2232     def _data_equals(self, other):\n2233         return self.to_index().equals(other.to_index())\n2234 \n2235     def to_index_variable(self):\n2236         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n2237         return self\n2238 \n2239     to_coord = utils.alias(to_index_variable, \"to_coord\")\n2240 \n2241     def to_index(self):\n2242         \"\"\"Convert this variable to a pandas.Index\"\"\"\n2243         # n.b. creating a new pandas.Index from an old pandas.Index is\n2244         # basically free as pandas.Index objects are immutable\n2245         assert self.ndim == 1\n2246         index = self._data.array\n2247         if isinstance(index, pd.MultiIndex):\n2248             # set default names for multi-index unnamed levels so that\n2249             # we can safely rename dimension / coordinate later\n2250             valid_level_names = [\n2251                 name or \"{}_level_{}\".format(self.dims[0], i)\n2252                 for i, name in enumerate(index.names)\n2253             ]\n2254             index = index.set_names(valid_level_names)\n2255         else:\n2256             index = index.set_names(self.name)\n2257         return index\n2258 \n2259     @property\n2260     def level_names(self):\n2261         \"\"\"Return MultiIndex level names or None if this IndexVariable has no\n2262         MultiIndex.\n2263         \"\"\"\n2264         index = self.to_index()\n2265         if isinstance(index, pd.MultiIndex):\n2266             return index.names\n2267         else:\n2268             return None\n2269 \n2270     def get_level_variable(self, level):\n2271         \"\"\"Return a new IndexVariable from a given MultiIndex level.\"\"\"\n2272         if self.level_names is None:\n2273             raise ValueError(\"IndexVariable %r has no MultiIndex\" % self.name)\n2274         index = self.to_index()\n2275         return type(self)(self.dims, index.get_level_values(level))\n2276 \n2277     @property\n2278     def name(self):\n2279         return self.dims[0]\n2280 \n2281     @name.setter\n2282     def name(self, value):\n2283         raise AttributeError(\"cannot modify name of IndexVariable in-place\")\n2284 \n2285 \n2286 # for backwards compatibility\n2287 Coordinate = utils.alias(IndexVariable, \"Coordinate\")\n2288 \n2289 \n2290 def _unified_dims(variables):\n2291     # validate dimensions\n2292     all_dims = {}\n2293     for var in variables:\n2294         var_dims = var.dims\n2295         if len(set(var_dims)) < len(var_dims):\n2296             raise ValueError(\n2297                 \"broadcasting cannot handle duplicate \"\n2298                 \"dimensions: %r\" % list(var_dims)\n2299             )\n2300         for d, s in zip(var_dims, var.shape):\n2301             if d not in all_dims:\n2302                 all_dims[d] = s\n2303             elif all_dims[d] != s:\n2304                 raise ValueError(\n2305                     \"operands cannot be broadcast together \"\n2306                     \"with mismatched lengths for dimension %r: %s\"\n2307                     % (d, (all_dims[d], s))\n2308                 )\n2309     return all_dims\n2310 \n2311 \n2312 def _broadcast_compat_variables(*variables):\n2313     \"\"\"Create broadcast compatible variables, with the same dimensions.\n2314 \n2315     Unlike the result of broadcast_variables(), some variables may have\n2316     dimensions of size 1 instead of the the size of the broadcast dimension.\n2317     \"\"\"\n2318     dims = tuple(_unified_dims(variables))\n2319     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)\n2320 \n2321 \n2322 def broadcast_variables(*variables):\n2323     \"\"\"Given any number of variables, return variables with matching dimensions\n2324     and broadcast data.\n2325 \n2326     The data on the returned variables will be a view of the data on the\n2327     corresponding original arrays, but dimensions will be reordered and\n2328     inserted so that both broadcast arrays have the same dimensions. The new\n2329     dimensions are sorted in order of appearance in the first variable's\n2330     dimensions followed by the second variable's dimensions.\n2331     \"\"\"\n2332     dims_map = _unified_dims(variables)\n2333     dims_tuple = tuple(dims_map)\n2334     return tuple(\n2335         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables\n2336     )\n2337 \n2338 \n2339 def _broadcast_compat_data(self, other):\n2340     if all(hasattr(other, attr) for attr in [\"dims\", \"data\", \"shape\", \"encoding\"]):\n2341         # `other` satisfies the necessary Variable API for broadcast_variables\n2342         new_self, new_other = _broadcast_compat_variables(self, other)\n2343         self_data = new_self.data\n2344         other_data = new_other.data\n2345         dims = new_self.dims\n2346     else:\n2347         # rely on numpy broadcasting rules\n2348         self_data = self.data\n2349         other_data = other\n2350         dims = self.dims\n2351     return self_data, other_data, dims\n2352 \n2353 \n2354 def concat(variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2355     \"\"\"Concatenate variables along a new or existing dimension.\n2356 \n2357     Parameters\n2358     ----------\n2359     variables : iterable of Array\n2360         Arrays to stack together. Each variable is expected to have\n2361         matching dimensions and shape except for along the stacked\n2362         dimension.\n2363     dim : str or DataArray, optional\n2364         Name of the dimension to stack along. This can either be a new\n2365         dimension name, in which case it is added along axis=0, or an\n2366         existing dimension name, in which case the location of the\n2367         dimension is unchanged. Where to insert the new dimension is\n2368         determined by the first variable.\n2369     positions : None or list of integer arrays, optional\n2370         List of integer arrays which specifies the integer positions to which\n2371         to assign each dataset along the concatenated dimension. If not\n2372         supplied, objects are concatenated in the provided order.\n2373     shortcut : bool, optional\n2374         This option is used internally to speed-up groupby operations.\n2375         If `shortcut` is True, some checks of internal consistency between\n2376         arrays to concatenate are skipped.\n2377 \n2378     Returns\n2379     -------\n2380     stacked : Variable\n2381         Concatenated Variable formed by stacking all the supplied variables\n2382         along the given dimension.\n2383     \"\"\"\n2384     variables = list(variables)\n2385     if all(isinstance(v, IndexVariable) for v in variables):\n2386         return IndexVariable.concat(variables, dim, positions, shortcut)\n2387     else:\n2388         return Variable.concat(variables, dim, positions, shortcut)\n2389 \n2390 \n2391 def assert_unique_multiindex_level_names(variables):\n2392     \"\"\"Check for uniqueness of MultiIndex level names in all given\n2393     variables.\n2394 \n2395     Not public API. Used for checking consistency of DataArray and Dataset\n2396     objects.\n2397     \"\"\"\n2398     level_names = defaultdict(list)\n2399     all_level_names = set()\n2400     for var_name, var in variables.items():\n2401         if isinstance(var._data, PandasIndexAdapter):\n2402             idx_level_names = var.to_index_variable().level_names\n2403             if idx_level_names is not None:\n2404                 for n in idx_level_names:\n2405                     level_names[n].append(f\"{n!r} ({var_name})\")\n2406             if idx_level_names:\n2407                 all_level_names.update(idx_level_names)\n2408 \n2409     for k, v in level_names.items():\n2410         if k in variables:\n2411             v.append(\"(%s)\" % k)\n2412 \n2413     duplicate_names = [v for v in level_names.values() if len(v) > 1]\n2414     if duplicate_names:\n2415         conflict_str = \"\\n\".join(\", \".join(v) for v in duplicate_names)\n2416         raise ValueError(\"conflicting MultiIndex level name(s):\\n%s\" % conflict_str)\n2417     # Check confliction between level names and dimensions GH:2299\n2418     for k, v in variables.items():\n2419         for d in v.dims:\n2420             if d in all_level_names:\n2421                 raise ValueError(\n2422                     \"conflicting level / dimension names. {} \"\n2423                     \"already exists as a level name.\".format(d)\n2424                 )\n",
        "intended_behavior": "Creation or Modification of Representation Methods: If not already present, methods responsible for generating HTML representations (`_repr_html_`) should be added or located. These methods need to ensure data and attributes sections are expanded by default, modifying how collapsible sections are initialized in the HTML."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 1,
        "end": 3828,
        "class_name": null,
        "method_name": null,
        "code": "1 import datetime\n2 import functools\n3 from numbers import Number\n4 from typing import (\n5     TYPE_CHECKING,\n6     Any,\n7     Callable,\n8     Dict,\n9     Hashable,\n10     Iterable,\n11     List,\n12     Mapping,\n13     Optional,\n14     Sequence,\n15     Tuple,\n16     TypeVar,\n17     Union,\n18     cast,\n19 )\n20 \n21 import numpy as np\n22 import pandas as pd\n23 \n24 from ..plot.plot import _PlotMethods\n25 from . import (\n26     computation,\n27     dtypes,\n28     groupby,\n29     indexing,\n30     ops,\n31     pdcompat,\n32     resample,\n33     rolling,\n34     utils,\n35     weighted,\n36 )\n37 from .accessor_dt import CombinedDatetimelikeAccessor\n38 from .accessor_str import StringAccessor\n39 from .alignment import (\n40     _broadcast_helper,\n41     _get_broadcast_dims_map_common_coords,\n42     align,\n43     reindex_like_indexers,\n44 )\n45 from .common import AbstractArray, DataWithCoords\n46 from .coordinates import (\n47     DataArrayCoordinates,\n48     LevelCoordinatesSource,\n49     assert_coordinate_consistent,\n50     remap_label_indexers,\n51 )\n52 from .dataset import Dataset, split_indexes\n53 from .formatting import format_item\n54 from .indexes import Indexes, default_indexes, propagate_indexes\n55 from .indexing import is_fancy_indexer\n56 from .merge import PANDAS_TYPES, MergeError, _extract_indexes_from_coords\n57 from .options import OPTIONS\n58 from .utils import Default, ReprObject, _check_inplace, _default, either_dict_or_kwargs\n59 from .variable import (\n60     IndexVariable,\n61     Variable,\n62     as_compatible_data,\n63     as_variable,\n64     assert_unique_multiindex_level_names,\n65 )\n66 \n67 if TYPE_CHECKING:\n68     T_DSorDA = TypeVar(\"T_DSorDA\", \"DataArray\", Dataset)\n69 \n70     try:\n71         from dask.delayed import Delayed\n72     except ImportError:\n73         Delayed = None\n74     try:\n75         from cdms2 import Variable as cdms2_Variable\n76     except ImportError:\n77         cdms2_Variable = None\n78     try:\n79         from iris.cube import Cube as iris_Cube\n80     except ImportError:\n81         iris_Cube = None\n82 \n83 \n84 def _infer_coords_and_dims(\n85     shape, coords, dims\n86 ) -> \"Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]\":\n87     \"\"\"All the logic for creating a new DataArray\"\"\"\n88 \n89     if (\n90         coords is not None\n91         and not utils.is_dict_like(coords)\n92         and len(coords) != len(shape)\n93     ):\n94         raise ValueError(\n95             \"coords is not dict-like, but it has %s items, \"\n96             \"which does not match the %s dimensions of the \"\n97             \"data\" % (len(coords), len(shape))\n98         )\n99 \n100     if isinstance(dims, str):\n101         dims = (dims,)\n102 \n103     if dims is None:\n104         dims = [\"dim_%s\" % n for n in range(len(shape))]\n105         if coords is not None and len(coords) == len(shape):\n106             # try to infer dimensions from coords\n107             if utils.is_dict_like(coords):\n108                 # deprecated in GH993, removed in GH1539\n109                 raise ValueError(\n110                     \"inferring DataArray dimensions from \"\n111                     \"dictionary like ``coords`` is no longer \"\n112                     \"supported. Use an explicit list of \"\n113                     \"``dims`` instead.\"\n114                 )\n115             for n, (dim, coord) in enumerate(zip(dims, coords)):\n116                 coord = as_variable(coord, name=dims[n]).to_index_variable()\n117                 dims[n] = coord.name\n118         dims = tuple(dims)\n119     elif len(dims) != len(shape):\n120         raise ValueError(\n121             \"different number of dimensions on data \"\n122             \"and dims: %s vs %s\" % (len(shape), len(dims))\n123         )\n124     else:\n125         for d in dims:\n126             if not isinstance(d, str):\n127                 raise TypeError(\"dimension %s is not a string\" % d)\n128 \n129     new_coords: Dict[Any, Variable] = {}\n130 \n131     if utils.is_dict_like(coords):\n132         for k, v in coords.items():\n133             new_coords[k] = as_variable(v, name=k)\n134     elif coords is not None:\n135         for dim, coord in zip(dims, coords):\n136             var = as_variable(coord, name=dim)\n137             var.dims = (dim,)\n138             new_coords[dim] = var.to_index_variable()\n139 \n140     sizes = dict(zip(dims, shape))\n141     for k, v in new_coords.items():\n142         if any(d not in dims for d in v.dims):\n143             raise ValueError(\n144                 \"coordinate %s has dimensions %s, but these \"\n145                 \"are not a subset of the DataArray \"\n146                 \"dimensions %s\" % (k, v.dims, dims)\n147             )\n148 \n149         for d, s in zip(v.dims, v.shape):\n150             if s != sizes[d]:\n151                 raise ValueError(\n152                     \"conflicting sizes for dimension %r: \"\n153                     \"length %s on the data but length %s on \"\n154                     \"coordinate %r\" % (d, sizes[d], s, k)\n155                 )\n156 \n157         if k in sizes and v.shape != (sizes[k],):\n158             raise ValueError(\n159                 \"coordinate %r is a DataArray dimension, but \"\n160                 \"it has shape %r rather than expected shape %r \"\n161                 \"matching the dimension size\" % (k, v.shape, (sizes[k],))\n162             )\n163 \n164     assert_unique_multiindex_level_names(new_coords)\n165 \n166     return new_coords, dims\n167 \n168 \n169 def _check_data_shape(data, coords, dims):\n170     if data is dtypes.NA:\n171         data = np.nan\n172     if coords is not None and utils.is_scalar(data, include_0d=False):\n173         if utils.is_dict_like(coords):\n174             if dims is None:\n175                 return data\n176             else:\n177                 data_shape = tuple(\n178                     as_variable(coords[k], k).size if k in coords.keys() else 1\n179                     for k in dims\n180                 )\n181         else:\n182             data_shape = tuple(as_variable(coord, \"foo\").size for coord in coords)\n183         data = np.full(data_shape, data)\n184     return data\n185 \n186 \n187 class _LocIndexer:\n188     __slots__ = (\"data_array\",)\n189 \n190     def __init__(self, data_array: \"DataArray\"):\n191         self.data_array = data_array\n192 \n193     def __getitem__(self, key) -> \"DataArray\":\n194         if not utils.is_dict_like(key):\n195             # expand the indexer so we can handle Ellipsis\n196             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n197             key = dict(zip(self.data_array.dims, labels))\n198         return self.data_array.sel(**key)\n199 \n200     def __setitem__(self, key, value) -> None:\n201         if not utils.is_dict_like(key):\n202             # expand the indexer so we can handle Ellipsis\n203             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n204             key = dict(zip(self.data_array.dims, labels))\n205 \n206         pos_indexers, _ = remap_label_indexers(self.data_array, key)\n207         self.data_array[pos_indexers] = value\n208 \n209 \n210 # Used as the key corresponding to a DataArray's variable when converting\n211 # arbitrary DataArray objects to datasets\n212 _THIS_ARRAY = ReprObject(\"<this-array>\")\n213 \n214 \n215 class DataArray(AbstractArray, DataWithCoords):\n216     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n217 \n218     DataArray provides a wrapper around numpy ndarrays that uses labeled\n219     dimensions and coordinates to support metadata aware operations. The API is\n220     similar to that for the pandas Series or DataFrame, but DataArray objects\n221     can have any number of dimensions, and their contents have fixed data\n222     types.\n223 \n224     Additional features over raw numpy arrays:\n225 \n226     - Apply operations over dimensions by name: ``x.sum('time')``.\n227     - Select or assign values by integer location (like numpy): ``x[:10]``\n228       or by label (like pandas): ``x.loc['2014-01-01']`` or\n229       ``x.sel(time='2014-01-01')``.\n230     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n231       dimensions (known in numpy as \"broadcasting\") based on dimension names,\n232       regardless of their original order.\n233     - Keep track of arbitrary metadata in the form of a Python dictionary:\n234       ``x.attrs``\n235     - Convert to a pandas Series: ``x.to_series()``.\n236 \n237     Getting items from or doing mathematical operations with a DataArray\n238     always returns another DataArray.\n239     \"\"\"\n240 \n241     _cache: Dict[str, Any]\n242     _coords: Dict[Any, Variable]\n243     _indexes: Optional[Dict[Hashable, pd.Index]]\n244     _name: Optional[Hashable]\n245     _variable: Variable\n246 \n247     __slots__ = (\n248         \"_cache\",\n249         \"_coords\",\n250         \"_file_obj\",\n251         \"_indexes\",\n252         \"_name\",\n253         \"_variable\",\n254         \"__weakref__\",\n255     )\n256 \n257     _groupby_cls = groupby.DataArrayGroupBy\n258     _rolling_cls = rolling.DataArrayRolling\n259     _coarsen_cls = rolling.DataArrayCoarsen\n260     _resample_cls = resample.DataArrayResample\n261     _weighted_cls = weighted.DataArrayWeighted\n262 \n263     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)\n264 \n265     def __init__(\n266         self,\n267         data: Any = dtypes.NA,\n268         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n269         dims: Union[Hashable, Sequence[Hashable], None] = None,\n270         name: Hashable = None,\n271         attrs: Mapping = None,\n272         # internal parameters\n273         indexes: Dict[Hashable, pd.Index] = None,\n274         fastpath: bool = False,\n275     ):\n276         \"\"\"\n277         Parameters\n278         ----------\n279         data : array_like\n280             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n281             or castable to an ``ndarray``. If a self-described xarray or pandas\n282             object, attempts are made to use this array's metadata to fill in\n283             other unspecified arguments. A view of the array's data is used\n284             instead of a copy if possible.\n285         coords : sequence or dict of array_like objects, optional\n286             Coordinates (tick labels) to use for indexing along each dimension.\n287             The following notations are accepted:\n288 \n289             - mapping {dimension name: array-like}\n290             - sequence of tuples that are valid arguments for xarray.Variable()\n291               - (dims, data)\n292               - (dims, data, attrs)\n293               - (dims, data, attrs, encoding)\n294 \n295             Additionally, it is possible to define a coord whose name\n296             does not match the dimension name, or a coord based on multiple\n297             dimensions, with one of the following notations:\n298 \n299             - mapping {coord name: DataArray}\n300             - mapping {coord name: Variable}\n301             - mapping {coord name: (dimension name, array-like)}\n302             - mapping {coord name: (tuple of dimension names, array-like)}\n303 \n304         dims : hashable or sequence of hashable, optional\n305             Name(s) of the data dimension(s). Must be either a hashable (only\n306             for 1D data) or a sequence of hashables with length equal to the\n307             number of dimensions. If this argument is omitted, dimension names\n308             default to ``['dim_0', ... 'dim_n']``.\n309         name : str or None, optional\n310             Name of this array.\n311         attrs : dict_like or None, optional\n312             Attributes to assign to the new instance. By default, an empty\n313             attribute dictionary is initialized.\n314         \"\"\"\n315         if fastpath:\n316             variable = data\n317             assert dims is None\n318             assert attrs is None\n319         else:\n320             # try to fill in arguments from data if they weren't supplied\n321             if coords is None:\n322 \n323                 if isinstance(data, DataArray):\n324                     coords = data.coords\n325                 elif isinstance(data, pd.Series):\n326                     coords = [data.index]\n327                 elif isinstance(data, pd.DataFrame):\n328                     coords = [data.index, data.columns]\n329                 elif isinstance(data, (pd.Index, IndexVariable)):\n330                     coords = [data]\n331                 elif isinstance(data, pdcompat.Panel):\n332                     coords = [data.items, data.major_axis, data.minor_axis]\n333 \n334             if dims is None:\n335                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n336             if name is None:\n337                 name = getattr(data, \"name\", None)\n338             if attrs is None and not isinstance(data, PANDAS_TYPES):\n339                 attrs = getattr(data, \"attrs\", None)\n340 \n341             data = _check_data_shape(data, coords, dims)\n342             data = as_compatible_data(data)\n343             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n344             variable = Variable(dims, data, attrs, fastpath=True)\n345             indexes = dict(\n346                 _extract_indexes_from_coords(coords)\n347             )  # needed for to_dataset\n348 \n349         # These fully describe a DataArray\n350         self._variable = variable\n351         assert isinstance(coords, dict)\n352         self._coords = coords\n353         self._name = name\n354 \n355         # TODO(shoyer): document this argument, once it becomes part of the\n356         # public interface.\n357         self._indexes = indexes\n358 \n359         self._file_obj = None\n360 \n361     def _replace(\n362         self,\n363         variable: Variable = None,\n364         coords=None,\n365         name: Union[Hashable, None, Default] = _default,\n366         indexes=None,\n367     ) -> \"DataArray\":\n368         if variable is None:\n369             variable = self.variable\n370         if coords is None:\n371             coords = self._coords\n372         if name is _default:\n373             name = self.name\n374         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n375 \n376     def _replace_maybe_drop_dims(\n377         self, variable: Variable, name: Union[Hashable, None, Default] = _default\n378     ) -> \"DataArray\":\n379         if variable.dims == self.dims and variable.shape == self.shape:\n380             coords = self._coords.copy()\n381             indexes = self._indexes\n382         elif variable.dims == self.dims:\n383             # Shape has changed (e.g. from reduce(..., keepdims=True)\n384             new_sizes = dict(zip(self.dims, variable.shape))\n385             coords = {\n386                 k: v\n387                 for k, v in self._coords.items()\n388                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n389             }\n390             changed_dims = [\n391                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n392             ]\n393             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n394         else:\n395             allowed_dims = set(variable.dims)\n396             coords = {\n397                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n398             }\n399             indexes = propagate_indexes(\n400                 self._indexes, exclude=(set(self.dims) - allowed_dims)\n401             )\n402         return self._replace(variable, coords, name, indexes=indexes)\n403 \n404     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> \"DataArray\":\n405         if not len(indexes):\n406             return self\n407         coords = self._coords.copy()\n408         for name, idx in indexes.items():\n409             coords[name] = IndexVariable(name, idx)\n410         obj = self._replace(coords=coords)\n411 \n412         # switch from dimension to level names, if necessary\n413         dim_names: Dict[Any, str] = {}\n414         for dim, idx in indexes.items():\n415             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n416                 dim_names[dim] = idx.name\n417         if dim_names:\n418             obj = obj.rename(dim_names)\n419         return obj\n420 \n421     def _to_temp_dataset(self) -> Dataset:\n422         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n423 \n424     def _from_temp_dataset(\n425         self, dataset: Dataset, name: Hashable = _default\n426     ) -> \"DataArray\":\n427         variable = dataset._variables.pop(_THIS_ARRAY)\n428         coords = dataset._variables\n429         indexes = dataset._indexes\n430         return self._replace(variable, coords, name, indexes=indexes)\n431 \n432     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n433         \"\"\" splits dataarray along dimension 'dim' \"\"\"\n434 \n435         def subset(dim, label):\n436             array = self.loc[{dim: label}]\n437             array.attrs = {}\n438             return as_variable(array)\n439 \n440         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n441         variables.update({k: v for k, v in self._coords.items() if k != dim})\n442         indexes = propagate_indexes(self._indexes, exclude=dim)\n443         coord_names = set(self._coords) - set([dim])\n444         dataset = Dataset._construct_direct(\n445             variables, coord_names, indexes=indexes, attrs=self.attrs\n446         )\n447         return dataset\n448 \n449     def _to_dataset_whole(\n450         self, name: Hashable = None, shallow_copy: bool = True\n451     ) -> Dataset:\n452         if name is None:\n453             name = self.name\n454         if name is None:\n455             raise ValueError(\n456                 \"unable to convert unnamed DataArray to a \"\n457                 \"Dataset without providing an explicit name\"\n458             )\n459         if name in self.coords:\n460             raise ValueError(\n461                 \"cannot create a Dataset from a DataArray with \"\n462                 \"the same name as one of its coordinates\"\n463             )\n464         # use private APIs for speed: this is called by _to_temp_dataset(),\n465         # which is used in the guts of a lot of operations (e.g., reindex)\n466         variables = self._coords.copy()\n467         variables[name] = self.variable\n468         if shallow_copy:\n469             for k in variables:\n470                 variables[k] = variables[k].copy(deep=False)\n471         indexes = self._indexes\n472 \n473         coord_names = set(self._coords)\n474         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n475         return dataset\n476 \n477     def to_dataset(\n478         self,\n479         dim: Hashable = None,\n480         *,\n481         name: Hashable = None,\n482         promote_attrs: bool = False,\n483     ) -> Dataset:\n484         \"\"\"Convert a DataArray to a Dataset.\n485 \n486         Parameters\n487         ----------\n488         dim : hashable, optional\n489             Name of the dimension on this array along which to split this array\n490             into separate variables. If not provided, this array is converted\n491             into a Dataset of one variable.\n492         name : hashable, optional\n493             Name to substitute for this array's name. Only valid if ``dim`` is\n494             not provided.\n495         promote_attrs : bool, default False\n496             Set to True to shallow copy attrs of DataArray to returned Dataset.\n497 \n498         Returns\n499         -------\n500         dataset : Dataset\n501         \"\"\"\n502         if dim is not None and dim not in self.dims:\n503             raise TypeError(\n504                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n505             )\n506 \n507         if dim is not None:\n508             if name is not None:\n509                 raise TypeError(\"cannot supply both dim and name arguments\")\n510             result = self._to_dataset_split(dim)\n511         else:\n512             result = self._to_dataset_whole(name)\n513 \n514         if promote_attrs:\n515             result.attrs = dict(self.attrs)\n516 \n517         return result\n518 \n519     @property\n520     def name(self) -> Optional[Hashable]:\n521         \"\"\"The name of this array.\n522         \"\"\"\n523         return self._name\n524 \n525     @name.setter\n526     def name(self, value: Optional[Hashable]) -> None:\n527         self._name = value\n528 \n529     @property\n530     def variable(self) -> Variable:\n531         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n532         return self._variable\n533 \n534     @property\n535     def dtype(self) -> np.dtype:\n536         return self.variable.dtype\n537 \n538     @property\n539     def shape(self) -> Tuple[int, ...]:\n540         return self.variable.shape\n541 \n542     @property\n543     def size(self) -> int:\n544         return self.variable.size\n545 \n546     @property\n547     def nbytes(self) -> int:\n548         return self.variable.nbytes\n549 \n550     @property\n551     def ndim(self) -> int:\n552         return self.variable.ndim\n553 \n554     def __len__(self) -> int:\n555         return len(self.variable)\n556 \n557     @property\n558     def data(self) -> Any:\n559         \"\"\"The array's data as a dask or numpy array\n560         \"\"\"\n561         return self.variable.data\n562 \n563     @data.setter\n564     def data(self, value: Any) -> None:\n565         self.variable.data = value\n566 \n567     @property\n568     def values(self) -> np.ndarray:\n569         \"\"\"The array's data as a numpy.ndarray\"\"\"\n570         return self.variable.values\n571 \n572     @values.setter\n573     def values(self, value: Any) -> None:\n574         self.variable.values = value\n575 \n576     @property\n577     def _in_memory(self) -> bool:\n578         return self.variable._in_memory\n579 \n580     def to_index(self) -> pd.Index:\n581         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n582         arrays.\n583         \"\"\"\n584         return self.variable.to_index()\n585 \n586     @property\n587     def dims(self) -> Tuple[Hashable, ...]:\n588         \"\"\"Tuple of dimension names associated with this array.\n589 \n590         Note that the type of this property is inconsistent with\n591         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n592         consistently named properties.\n593         \"\"\"\n594         return self.variable.dims\n595 \n596     @dims.setter\n597     def dims(self, value):\n598         raise AttributeError(\n599             \"you cannot assign dims on a DataArray. Use \"\n600             \".rename() or .swap_dims() instead.\"\n601         )\n602 \n603     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n604         if utils.is_dict_like(key):\n605             return key\n606         else:\n607             key = indexing.expanded_indexer(key, self.ndim)\n608             return dict(zip(self.dims, key))\n609 \n610     @property\n611     def _level_coords(self) -> Dict[Hashable, Hashable]:\n612         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n613         coordinate name.\n614         \"\"\"\n615         level_coords: Dict[Hashable, Hashable] = {}\n616 \n617         for cname, var in self._coords.items():\n618             if var.ndim == 1 and isinstance(var, IndexVariable):\n619                 level_names = var.level_names\n620                 if level_names is not None:\n621                     (dim,) = var.dims\n622                     level_coords.update({lname: dim for lname in level_names})\n623         return level_coords\n624 \n625     def _getitem_coord(self, key):\n626         from .dataset import _get_virtual_variable\n627 \n628         try:\n629             var = self._coords[key]\n630         except KeyError:\n631             dim_sizes = dict(zip(self.dims, self.shape))\n632             _, key, var = _get_virtual_variable(\n633                 self._coords, key, self._level_coords, dim_sizes\n634             )\n635 \n636         return self._replace_maybe_drop_dims(var, name=key)\n637 \n638     def __getitem__(self, key: Any) -> \"DataArray\":\n639         if isinstance(key, str):\n640             return self._getitem_coord(key)\n641         else:\n642             # xarray-style array indexing\n643             return self.isel(indexers=self._item_key_to_dict(key))\n644 \n645     def __setitem__(self, key: Any, value: Any) -> None:\n646         if isinstance(key, str):\n647             self.coords[key] = value\n648         else:\n649             # Coordinates in key, value and self[key] should be consistent.\n650             # TODO Coordinate consistency in key is checked here, but it\n651             # causes unnecessary indexing. It should be optimized.\n652             obj = self[key]\n653             if isinstance(value, DataArray):\n654                 assert_coordinate_consistent(value, obj.coords.variables)\n655             # DataArray key -> Variable key\n656             key = {\n657                 k: v.variable if isinstance(v, DataArray) else v\n658                 for k, v in self._item_key_to_dict(key).items()\n659             }\n660             self.variable[key] = value\n661 \n662     def __delitem__(self, key: Any) -> None:\n663         del self.coords[key]\n664 \n665     @property\n666     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n667         \"\"\"List of places to look-up items for attribute-style access\n668         \"\"\"\n669         return self._item_sources + [self.attrs]\n670 \n671     @property\n672     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n673         \"\"\"List of places to look-up items for key-completion\n674         \"\"\"\n675         return [\n676             self.coords,\n677             {d: self.coords[d] for d in self.dims},\n678             LevelCoordinatesSource(self),\n679         ]\n680 \n681     def __contains__(self, key: Any) -> bool:\n682         return key in self.data\n683 \n684     @property\n685     def loc(self) -> _LocIndexer:\n686         \"\"\"Attribute for location based indexing like pandas.\n687         \"\"\"\n688         return _LocIndexer(self)\n689 \n690     @property\n691     def attrs(self) -> Dict[Hashable, Any]:\n692         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n693         return self.variable.attrs\n694 \n695     @attrs.setter\n696     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n697         # Disable type checking to work around mypy bug - see mypy#4167\n698         self.variable.attrs = value  # type: ignore\n699 \n700     @property\n701     def encoding(self) -> Dict[Hashable, Any]:\n702         \"\"\"Dictionary of format-specific settings for how this array should be\n703         serialized.\"\"\"\n704         return self.variable.encoding\n705 \n706     @encoding.setter\n707     def encoding(self, value: Mapping[Hashable, Any]) -> None:\n708         self.variable.encoding = value\n709 \n710     @property\n711     def indexes(self) -> Indexes:\n712         \"\"\"Mapping of pandas.Index objects used for label based indexing\n713         \"\"\"\n714         if self._indexes is None:\n715             self._indexes = default_indexes(self._coords, self.dims)\n716         return Indexes(self._indexes)\n717 \n718     @property\n719     def coords(self) -> DataArrayCoordinates:\n720         \"\"\"Dictionary-like container of coordinate arrays.\n721         \"\"\"\n722         return DataArrayCoordinates(self)\n723 \n724     def reset_coords(\n725         self,\n726         names: Union[Iterable[Hashable], Hashable, None] = None,\n727         drop: bool = False,\n728         inplace: bool = None,\n729     ) -> Union[None, \"DataArray\", Dataset]:\n730         \"\"\"Given names of coordinates, reset them to become variables.\n731 \n732         Parameters\n733         ----------\n734         names : hashable or iterable of hashables, optional\n735             Name(s) of non-index coordinates in this dataset to reset into\n736             variables. By default, all non-index coordinates are reset.\n737         drop : bool, optional\n738             If True, remove coordinates instead of converting them into\n739             variables.\n740 \n741         Returns\n742         -------\n743         Dataset, or DataArray if ``drop == True``\n744         \"\"\"\n745         _check_inplace(inplace)\n746         if names is None:\n747             names = set(self.coords) - set(self.dims)\n748         dataset = self.coords.to_dataset().reset_coords(names, drop)\n749         if drop:\n750             return self._replace(coords=dataset._variables)\n751         else:\n752             if self.name is None:\n753                 raise ValueError(\n754                     \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n755                 )\n756             dataset[self.name] = self.variable\n757             return dataset\n758 \n759     def __dask_tokenize__(self):\n760         from dask.base import normalize_token\n761 \n762         return normalize_token((type(self), self._variable, self._coords, self._name))\n763 \n764     def __dask_graph__(self):\n765         return self._to_temp_dataset().__dask_graph__()\n766 \n767     def __dask_keys__(self):\n768         return self._to_temp_dataset().__dask_keys__()\n769 \n770     def __dask_layers__(self):\n771         return self._to_temp_dataset().__dask_layers__()\n772 \n773     @property\n774     def __dask_optimize__(self):\n775         return self._to_temp_dataset().__dask_optimize__\n776 \n777     @property\n778     def __dask_scheduler__(self):\n779         return self._to_temp_dataset().__dask_scheduler__\n780 \n781     def __dask_postcompute__(self):\n782         func, args = self._to_temp_dataset().__dask_postcompute__()\n783         return self._dask_finalize, (func, args, self.name)\n784 \n785     def __dask_postpersist__(self):\n786         func, args = self._to_temp_dataset().__dask_postpersist__()\n787         return self._dask_finalize, (func, args, self.name)\n788 \n789     @staticmethod\n790     def _dask_finalize(results, func, args, name):\n791         ds = func(results, *args)\n792         variable = ds._variables.pop(_THIS_ARRAY)\n793         coords = ds._variables\n794         return DataArray(variable, coords, name=name, fastpath=True)\n795 \n796     def load(self, **kwargs) -> \"DataArray\":\n797         \"\"\"Manually trigger loading of this array's data from disk or a\n798         remote source into memory and return this array.\n799 \n800         Normally, it should not be necessary to call this method in user code,\n801         because all xarray functions should either work on deferred data or\n802         load data automatically. However, this method can be necessary when\n803         working with many file objects on disk.\n804 \n805         Parameters\n806         ----------\n807         **kwargs : dict\n808             Additional keyword arguments passed on to ``dask.array.compute``.\n809 \n810         See Also\n811         --------\n812         dask.array.compute\n813         \"\"\"\n814         ds = self._to_temp_dataset().load(**kwargs)\n815         new = self._from_temp_dataset(ds)\n816         self._variable = new._variable\n817         self._coords = new._coords\n818         return self\n819 \n820     def compute(self, **kwargs) -> \"DataArray\":\n821         \"\"\"Manually trigger loading of this array's data from disk or a\n822         remote source into memory and return a new array. The original is\n823         left unaltered.\n824 \n825         Normally, it should not be necessary to call this method in user code,\n826         because all xarray functions should either work on deferred data or\n827         load data automatically. However, this method can be necessary when\n828         working with many file objects on disk.\n829 \n830         Parameters\n831         ----------\n832         **kwargs : dict\n833             Additional keyword arguments passed on to ``dask.array.compute``.\n834 \n835         See Also\n836         --------\n837         dask.array.compute\n838         \"\"\"\n839         new = self.copy(deep=False)\n840         return new.load(**kwargs)\n841 \n842     def persist(self, **kwargs) -> \"DataArray\":\n843         \"\"\" Trigger computation in constituent dask arrays\n844 \n845         This keeps them as dask arrays but encourages them to keep data in\n846         memory.  This is particularly useful when on a distributed machine.\n847         When on a single machine consider using ``.compute()`` instead.\n848 \n849         Parameters\n850         ----------\n851         **kwargs : dict\n852             Additional keyword arguments passed on to ``dask.persist``.\n853 \n854         See Also\n855         --------\n856         dask.persist\n857         \"\"\"\n858         ds = self._to_temp_dataset().persist(**kwargs)\n859         return self._from_temp_dataset(ds)\n860 \n861     def copy(self, deep: bool = True, data: Any = None) -> \"DataArray\":\n862         \"\"\"Returns a copy of this array.\n863 \n864         If `deep=True`, a deep copy is made of the data array.\n865         Otherwise, a shallow copy is made, so each variable in the new\n866         array's dataset is also a variable in this array's dataset.\n867 \n868         Use `data` to create a new object with the same structure as\n869         original but entirely new data.\n870 \n871         Parameters\n872         ----------\n873         deep : bool, optional\n874             Whether the data array and its coordinates are loaded into memory\n875             and copied onto the new object. Default is True.\n876         data : array_like, optional\n877             Data to use in the new object. Must have same shape as original.\n878             When `data` is used, `deep` is ignored for all data variables,\n879             and only used for coords.\n880 \n881         Returns\n882         -------\n883         object : DataArray\n884             New object with dimensions, attributes, coordinates, name,\n885             encoding, and optionally data copied from original.\n886 \n887         Examples\n888         --------\n889 \n890         Shallow versus deep copy\n891 \n892         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n893         >>> array.copy()\n894         <xarray.DataArray (x: 3)>\n895         array([1, 2, 3])\n896         Coordinates:\n897         * x        (x) <U1 'a' 'b' 'c'\n898         >>> array_0 = array.copy(deep=False)\n899         >>> array_0[0] = 7\n900         >>> array_0\n901         <xarray.DataArray (x: 3)>\n902         array([7, 2, 3])\n903         Coordinates:\n904         * x        (x) <U1 'a' 'b' 'c'\n905         >>> array\n906         <xarray.DataArray (x: 3)>\n907         array([7, 2, 3])\n908         Coordinates:\n909         * x        (x) <U1 'a' 'b' 'c'\n910 \n911         Changing the data using the ``data`` argument maintains the\n912         structure of the original object, but with the new data. Original\n913         object is unaffected.\n914 \n915         >>> array.copy(data=[0.1, 0.2, 0.3])\n916         <xarray.DataArray (x: 3)>\n917         array([ 0.1,  0.2,  0.3])\n918         Coordinates:\n919         * x        (x) <U1 'a' 'b' 'c'\n920         >>> array\n921         <xarray.DataArray (x: 3)>\n922         array([1, 2, 3])\n923         Coordinates:\n924         * x        (x) <U1 'a' 'b' 'c'\n925 \n926         See Also\n927         --------\n928         pandas.DataFrame.copy\n929         \"\"\"\n930         variable = self.variable.copy(deep=deep, data=data)\n931         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n932         if self._indexes is None:\n933             indexes = self._indexes\n934         else:\n935             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n936         return self._replace(variable, coords, indexes=indexes)\n937 \n938     def __copy__(self) -> \"DataArray\":\n939         return self.copy(deep=False)\n940 \n941     def __deepcopy__(self, memo=None) -> \"DataArray\":\n942         # memo does nothing but is required for compatibility with\n943         # copy.deepcopy\n944         return self.copy(deep=True)\n945 \n946     # mutable objects should not be hashable\n947     # https://github.com/python/mypy/issues/4266\n948     __hash__ = None  # type: ignore\n949 \n950     @property\n951     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n952         \"\"\"Block dimensions for this array's data or None if it's not a dask\n953         array.\n954         \"\"\"\n955         return self.variable.chunks\n956 \n957     def chunk(\n958         self,\n959         chunks: Union[\n960             None,\n961             Number,\n962             Tuple[Number, ...],\n963             Tuple[Tuple[Number, ...], ...],\n964             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n965         ] = None,\n966         name_prefix: str = \"xarray-\",\n967         token: str = None,\n968         lock: bool = False,\n969     ) -> \"DataArray\":\n970         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n971 \n972         If this variable is a non-dask array, it will be converted to dask\n973         array. If it's a dask array, it will be rechunked to the given chunk\n974         sizes.\n975 \n976         If neither chunks is not provided for one or more dimensions, chunk\n977         sizes along that dimension will not be updated; non-dask arrays will be\n978         converted into dask arrays with a single block.\n979 \n980         Parameters\n981         ----------\n982         chunks : int, tuple or mapping, optional\n983             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n984             ``{'x': 5, 'y': 5}``.\n985         name_prefix : str, optional\n986             Prefix for the name of the new dask array.\n987         token : str, optional\n988             Token uniquely identifying this array.\n989         lock : optional\n990             Passed on to :py:func:`dask.array.from_array`, if the array is not\n991             already as dask array.\n992 \n993         Returns\n994         -------\n995         chunked : xarray.DataArray\n996         \"\"\"\n997         if isinstance(chunks, (tuple, list)):\n998             chunks = dict(zip(self.dims, chunks))\n999 \n1000         ds = self._to_temp_dataset().chunk(\n1001             chunks, name_prefix=name_prefix, token=token, lock=lock\n1002         )\n1003         return self._from_temp_dataset(ds)\n1004 \n1005     def isel(\n1006         self,\n1007         indexers: Mapping[Hashable, Any] = None,\n1008         drop: bool = False,\n1009         missing_dims: str = \"raise\",\n1010         **indexers_kwargs: Any,\n1011     ) -> \"DataArray\":\n1012         \"\"\"Return a new DataArray whose data is given by integer indexing\n1013         along the specified dimension(s).\n1014 \n1015         Parameters\n1016         ----------\n1017         indexers : dict, optional\n1018             A dict with keys matching dimensions and values given\n1019             by integers, slice objects or arrays.\n1020             indexer can be a integer, slice, array-like or DataArray.\n1021             If DataArrays are passed as indexers, xarray-style indexing will be\n1022             carried out. See :ref:`indexing` for the details.\n1023             One of indexers or indexers_kwargs must be provided.\n1024         drop : bool, optional\n1025             If ``drop=True``, drop coordinates variables indexed by integers\n1026             instead of making them scalar.\n1027         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1028             What to do if dimensions that should be selected from are not present in the\n1029             DataArray:\n1030             - \"exception\": raise an exception\n1031             - \"warning\": raise a warning, and ignore the missing dimensions\n1032             - \"ignore\": ignore the missing dimensions\n1033         **indexers_kwargs : {dim: indexer, ...}, optional\n1034             The keyword arguments form of ``indexers``.\n1035 \n1036         See Also\n1037         --------\n1038         Dataset.isel\n1039         DataArray.sel\n1040         \"\"\"\n1041 \n1042         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1043 \n1044         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1045             ds = self._to_temp_dataset()._isel_fancy(\n1046                 indexers, drop=drop, missing_dims=missing_dims\n1047             )\n1048             return self._from_temp_dataset(ds)\n1049 \n1050         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1051         # lists, or zero or one-dimensional np.ndarray's\n1052 \n1053         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1054 \n1055         coords = {}\n1056         for coord_name, coord_value in self._coords.items():\n1057             coord_indexers = {\n1058                 k: v for k, v in indexers.items() if k in coord_value.dims\n1059             }\n1060             if coord_indexers:\n1061                 coord_value = coord_value.isel(coord_indexers)\n1062                 if drop and coord_value.ndim == 0:\n1063                     continue\n1064             coords[coord_name] = coord_value\n1065 \n1066         return self._replace(variable=variable, coords=coords)\n1067 \n1068     def sel(\n1069         self,\n1070         indexers: Mapping[Hashable, Any] = None,\n1071         method: str = None,\n1072         tolerance=None,\n1073         drop: bool = False,\n1074         **indexers_kwargs: Any,\n1075     ) -> \"DataArray\":\n1076         \"\"\"Return a new DataArray whose data is given by selecting index\n1077         labels along the specified dimension(s).\n1078 \n1079         In contrast to `DataArray.isel`, indexers for this method should use\n1080         labels instead of integers.\n1081 \n1082         Under the hood, this method is powered by using pandas's powerful Index\n1083         objects. This makes label based indexing essentially just as fast as\n1084         using integer indexing.\n1085 \n1086         It also means this method uses pandas's (well documented) logic for\n1087         indexing. This means you can use string shortcuts for datetime indexes\n1088         (e.g., '2000-01' to select all values in January 2000). It also means\n1089         that slices are treated as inclusive of both the start and stop values,\n1090         unlike normal Python indexing.\n1091 \n1092         .. warning::\n1093 \n1094           Do not try to assign values when using any of the indexing methods\n1095           ``isel`` or ``sel``::\n1096 \n1097             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1098             # DO NOT do this\n1099             da.isel(x=[0, 1, 2])[1] = -1\n1100 \n1101           Assigning values with the chained indexing using ``.sel`` or\n1102           ``.isel`` fails silently.\n1103 \n1104         Parameters\n1105         ----------\n1106         indexers : dict, optional\n1107             A dict with keys matching dimensions and values given\n1108             by scalars, slices or arrays of tick labels. For dimensions with\n1109             multi-index, the indexer may also be a dict-like object with keys\n1110             matching index level names.\n1111             If DataArrays are passed as indexers, xarray-style indexing will be\n1112             carried out. See :ref:`indexing` for the details.\n1113             One of indexers or indexers_kwargs must be provided.\n1114         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1115             Method to use for inexact matches:\n1116 \n1117             * None (default): only exact matches\n1118             * pad / ffill: propagate last valid index value forward\n1119             * backfill / bfill: propagate next valid index value backward\n1120             * nearest: use nearest valid index value\n1121         tolerance : optional\n1122             Maximum distance between original and new labels for inexact\n1123             matches. The values of the index at the matching locations must\n1124             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1125         drop : bool, optional\n1126             If ``drop=True``, drop coordinates variables in `indexers` instead\n1127             of making them scalar.\n1128         **indexers_kwargs : {dim: indexer, ...}, optional\n1129             The keyword arguments form of ``indexers``.\n1130             One of indexers or indexers_kwargs must be provided.\n1131 \n1132         Returns\n1133         -------\n1134         obj : DataArray\n1135             A new DataArray with the same contents as this DataArray, except the\n1136             data and each dimension is indexed by the appropriate indexers.\n1137             If indexer DataArrays have coordinates that do not conflict with\n1138             this object, then these coordinates will be attached.\n1139             In general, each array's data will be a view of the array's data\n1140             in this DataArray, unless vectorized indexing was triggered by using\n1141             an array indexer, in which case the data will be a copy.\n1142 \n1143         See Also\n1144         --------\n1145         Dataset.sel\n1146         DataArray.isel\n1147 \n1148         \"\"\"\n1149         ds = self._to_temp_dataset().sel(\n1150             indexers=indexers,\n1151             drop=drop,\n1152             method=method,\n1153             tolerance=tolerance,\n1154             **indexers_kwargs,\n1155         )\n1156         return self._from_temp_dataset(ds)\n1157 \n1158     def head(\n1159         self,\n1160         indexers: Union[Mapping[Hashable, int], int] = None,\n1161         **indexers_kwargs: Any,\n1162     ) -> \"DataArray\":\n1163         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1164         values along the specified dimension(s). Default `n` = 5\n1165 \n1166         See Also\n1167         --------\n1168         Dataset.head\n1169         DataArray.tail\n1170         DataArray.thin\n1171         \"\"\"\n1172         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1173         return self._from_temp_dataset(ds)\n1174 \n1175     def tail(\n1176         self,\n1177         indexers: Union[Mapping[Hashable, int], int] = None,\n1178         **indexers_kwargs: Any,\n1179     ) -> \"DataArray\":\n1180         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1181         values along the specified dimension(s). Default `n` = 5\n1182 \n1183         See Also\n1184         --------\n1185         Dataset.tail\n1186         DataArray.head\n1187         DataArray.thin\n1188         \"\"\"\n1189         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1190         return self._from_temp_dataset(ds)\n1191 \n1192     def thin(\n1193         self,\n1194         indexers: Union[Mapping[Hashable, int], int] = None,\n1195         **indexers_kwargs: Any,\n1196     ) -> \"DataArray\":\n1197         \"\"\"Return a new DataArray whose data is given by each `n` value\n1198         along the specified dimension(s).\n1199 \n1200         See Also\n1201         --------\n1202         Dataset.thin\n1203         DataArray.head\n1204         DataArray.tail\n1205         \"\"\"\n1206         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1207         return self._from_temp_dataset(ds)\n1208 \n1209     def broadcast_like(\n1210         self, other: Union[\"DataArray\", Dataset], exclude: Iterable[Hashable] = None\n1211     ) -> \"DataArray\":\n1212         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1213 \n1214         This is equivalent to xr.broadcast(other, self)[1]\n1215 \n1216         xarray objects are broadcast against each other in arithmetic\n1217         operations, so this method is not be necessary for most uses.\n1218 \n1219         If no change is needed, the input data is returned to the output\n1220         without being copied.\n1221 \n1222         If new coords are added by the broadcast, their values are\n1223         NaN filled.\n1224 \n1225         Parameters\n1226         ----------\n1227         other : Dataset or DataArray\n1228             Object against which to broadcast this array.\n1229         exclude : iterable of hashable, optional\n1230             Dimensions that must not be broadcasted\n1231 \n1232         Returns\n1233         -------\n1234         new_da: xr.DataArray\n1235 \n1236         Examples\n1237         --------\n1238 \n1239         >>> arr1\n1240         <xarray.DataArray (x: 2, y: 3)>\n1241         array([[0.840235, 0.215216, 0.77917 ],\n1242                [0.726351, 0.543824, 0.875115]])\n1243         Coordinates:\n1244           * x        (x) <U1 'a' 'b'\n1245           * y        (y) <U1 'a' 'b' 'c'\n1246         >>> arr2\n1247         <xarray.DataArray (x: 3, y: 2)>\n1248         array([[0.612611, 0.125753],\n1249                [0.853181, 0.948818],\n1250                [0.180885, 0.33363 ]])\n1251         Coordinates:\n1252           * x        (x) <U1 'a' 'b' 'c'\n1253           * y        (y) <U1 'a' 'b'\n1254         >>> arr1.broadcast_like(arr2)\n1255         <xarray.DataArray (x: 3, y: 3)>\n1256         array([[0.840235, 0.215216, 0.77917 ],\n1257                [0.726351, 0.543824, 0.875115],\n1258                [     nan,      nan,      nan]])\n1259         Coordinates:\n1260           * x        (x) object 'a' 'b' 'c'\n1261           * y        (y) object 'a' 'b' 'c'\n1262         \"\"\"\n1263         if exclude is None:\n1264             exclude = set()\n1265         else:\n1266             exclude = set(exclude)\n1267         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1268 \n1269         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1270 \n1271         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n1272 \n1273     def reindex_like(\n1274         self,\n1275         other: Union[\"DataArray\", Dataset],\n1276         method: str = None,\n1277         tolerance=None,\n1278         copy: bool = True,\n1279         fill_value=dtypes.NA,\n1280     ) -> \"DataArray\":\n1281         \"\"\"Conform this object onto the indexes of another object, filling in\n1282         missing values with ``fill_value``. The default fill value is NaN.\n1283 \n1284         Parameters\n1285         ----------\n1286         other : Dataset or DataArray\n1287             Object with an 'indexes' attribute giving a mapping from dimension\n1288             names to pandas.Index objects, which provides coordinates upon\n1289             which to index the variables in this dataset. The indexes on this\n1290             other object need not be the same as the indexes on this\n1291             dataset. Any mis-matched index values will be filled in with\n1292             NaN, and any mis-matched dimension names will simply be ignored.\n1293         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1294             Method to use for filling index values from other not found on this\n1295             data array:\n1296 \n1297             * None (default): don't fill gaps\n1298             * pad / ffill: propagate last valid index value forward\n1299             * backfill / bfill: propagate next valid index value backward\n1300             * nearest: use nearest valid index value\n1301         tolerance : optional\n1302             Maximum distance between original and new labels for inexact\n1303             matches. The values of the index at the matching locations must\n1304             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1305         copy : bool, optional\n1306             If ``copy=True``, data in the return value is always copied. If\n1307             ``copy=False`` and reindexing is unnecessary, or can be performed\n1308             with only slice operations, then the output may share memory with\n1309             the input. In either case, a new xarray object is always returned.\n1310         fill_value : scalar, optional\n1311             Value to use for newly missing values\n1312 \n1313         Returns\n1314         -------\n1315         reindexed : DataArray\n1316             Another dataset array, with this array's data but coordinates from\n1317             the other object.\n1318 \n1319         See Also\n1320         --------\n1321         DataArray.reindex\n1322         align\n1323         \"\"\"\n1324         indexers = reindex_like_indexers(self, other)\n1325         return self.reindex(\n1326             indexers=indexers,\n1327             method=method,\n1328             tolerance=tolerance,\n1329             copy=copy,\n1330             fill_value=fill_value,\n1331         )\n1332 \n1333     def reindex(\n1334         self,\n1335         indexers: Mapping[Hashable, Any] = None,\n1336         method: str = None,\n1337         tolerance=None,\n1338         copy: bool = True,\n1339         fill_value=dtypes.NA,\n1340         **indexers_kwargs: Any,\n1341     ) -> \"DataArray\":\n1342         \"\"\"Conform this object onto the indexes of another object, filling in\n1343         missing values with ``fill_value``. The default fill value is NaN.\n1344 \n1345         Parameters\n1346         ----------\n1347         indexers : dict, optional\n1348             Dictionary with keys given by dimension names and values given by\n1349             arrays of coordinates tick labels. Any mis-matched coordinate\n1350             values will be filled in with NaN, and any mis-matched dimension\n1351             names will simply be ignored.\n1352             One of indexers or indexers_kwargs must be provided.\n1353         copy : bool, optional\n1354             If ``copy=True``, data in the return value is always copied. If\n1355             ``copy=False`` and reindexing is unnecessary, or can be performed\n1356             with only slice operations, then the output may share memory with\n1357             the input. In either case, a new xarray object is always returned.\n1358         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1359             Method to use for filling index values in ``indexers`` not found on\n1360             this data array:\n1361 \n1362             * None (default): don't fill gaps\n1363             * pad / ffill: propagate last valid index value forward\n1364             * backfill / bfill: propagate next valid index value backward\n1365             * nearest: use nearest valid index value\n1366         tolerance : optional\n1367             Maximum distance between original and new labels for inexact\n1368             matches. The values of the index at the matching locations must\n1369             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1370         fill_value : scalar, optional\n1371             Value to use for newly missing values\n1372         **indexers_kwargs : {dim: indexer, ...}, optional\n1373             The keyword arguments form of ``indexers``.\n1374             One of indexers or indexers_kwargs must be provided.\n1375 \n1376         Returns\n1377         -------\n1378         reindexed : DataArray\n1379             Another dataset array, with this array's data but replaced\n1380             coordinates.\n1381 \n1382         See Also\n1383         --------\n1384         DataArray.reindex_like\n1385         align\n1386         \"\"\"\n1387         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1388         ds = self._to_temp_dataset().reindex(\n1389             indexers=indexers,\n1390             method=method,\n1391             tolerance=tolerance,\n1392             copy=copy,\n1393             fill_value=fill_value,\n1394         )\n1395         return self._from_temp_dataset(ds)\n1396 \n1397     def interp(\n1398         self,\n1399         coords: Mapping[Hashable, Any] = None,\n1400         method: str = \"linear\",\n1401         assume_sorted: bool = False,\n1402         kwargs: Mapping[str, Any] = None,\n1403         **coords_kwargs: Any,\n1404     ) -> \"DataArray\":\n1405         \"\"\" Multidimensional interpolation of variables.\n1406 \n1407         coords : dict, optional\n1408             Mapping from dimension names to the new coordinates.\n1409             new coordinate can be an scalar, array-like or DataArray.\n1410             If DataArrays are passed as new coordates, their dimensions are\n1411             used for the broadcasting.\n1412         method: {'linear', 'nearest'} for multidimensional array,\n1413             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1414             for 1-dimensional array.\n1415         assume_sorted: boolean, optional\n1416             If False, values of x can be in any order and they are sorted\n1417             first. If True, x has to be an array of monotonically increasing\n1418             values.\n1419         kwargs: dictionary\n1420             Additional keyword arguments passed to scipy's interpolator. Valid\n1421             options and their behavior depend on if 1-dimensional or\n1422             multi-dimensional interpolation is used.\n1423         ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n1424             The keyword arguments form of ``coords``.\n1425             One of coords or coords_kwargs must be provided.\n1426 \n1427         Returns\n1428         -------\n1429         interpolated: xr.DataArray\n1430             New dataarray on the new coordinates.\n1431 \n1432         Notes\n1433         -----\n1434         scipy is required.\n1435 \n1436         See Also\n1437         --------\n1438         scipy.interpolate.interp1d\n1439         scipy.interpolate.interpn\n1440 \n1441         Examples\n1442         --------\n1443         >>> da = xr.DataArray([1, 3], [(\"x\", np.arange(2))])\n1444         >>> da.interp(x=0.5)\n1445         <xarray.DataArray ()>\n1446         array(2.0)\n1447         Coordinates:\n1448             x        float64 0.5\n1449         \"\"\"\n1450         if self.dtype.kind not in \"uifc\":\n1451             raise TypeError(\n1452                 \"interp only works for a numeric type array. \"\n1453                 \"Given {}.\".format(self.dtype)\n1454             )\n1455         ds = self._to_temp_dataset().interp(\n1456             coords,\n1457             method=method,\n1458             kwargs=kwargs,\n1459             assume_sorted=assume_sorted,\n1460             **coords_kwargs,\n1461         )\n1462         return self._from_temp_dataset(ds)\n1463 \n1464     def interp_like(\n1465         self,\n1466         other: Union[\"DataArray\", Dataset],\n1467         method: str = \"linear\",\n1468         assume_sorted: bool = False,\n1469         kwargs: Mapping[str, Any] = None,\n1470     ) -> \"DataArray\":\n1471         \"\"\"Interpolate this object onto the coordinates of another object,\n1472         filling out of range values with NaN.\n1473 \n1474         Parameters\n1475         ----------\n1476         other : Dataset or DataArray\n1477             Object with an 'indexes' attribute giving a mapping from dimension\n1478             names to an 1d array-like, which provides coordinates upon\n1479             which to index the variables in this dataset.\n1480         method: string, optional.\n1481             {'linear', 'nearest'} for multidimensional array,\n1482             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1483             for 1-dimensional array. 'linear' is used by default.\n1484         assume_sorted: boolean, optional\n1485             If False, values of coordinates that are interpolated over can be\n1486             in any order and they are sorted first. If True, interpolated\n1487             coordinates are assumed to be an array of monotonically increasing\n1488             values.\n1489         kwargs: dictionary, optional\n1490             Additional keyword passed to scipy's interpolator.\n1491 \n1492         Returns\n1493         -------\n1494         interpolated: xr.DataArray\n1495             Another dataarray by interpolating this dataarray's data along the\n1496             coordinates of the other object.\n1497 \n1498         Notes\n1499         -----\n1500         scipy is required.\n1501         If the dataarray has object-type coordinates, reindex is used for these\n1502         coordinates instead of the interpolation.\n1503 \n1504         See Also\n1505         --------\n1506         DataArray.interp\n1507         DataArray.reindex_like\n1508         \"\"\"\n1509         if self.dtype.kind not in \"uifc\":\n1510             raise TypeError(\n1511                 \"interp only works for a numeric type array. \"\n1512                 \"Given {}.\".format(self.dtype)\n1513             )\n1514         ds = self._to_temp_dataset().interp_like(\n1515             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1516         )\n1517         return self._from_temp_dataset(ds)\n1518 \n1519     def rename(\n1520         self,\n1521         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n1522         **names: Hashable,\n1523     ) -> \"DataArray\":\n1524         \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n1525 \n1526         Parameters\n1527         ----------\n1528         new_name_or_name_dict : str or dict-like, optional\n1529             If the argument is dict-like, it used as a mapping from old\n1530             names to new names for coordinates. Otherwise, use the argument\n1531             as the new name for this array.\n1532         **names: hashable, optional\n1533             The keyword arguments form of a mapping from old names to\n1534             new names for coordinates.\n1535             One of new_name_or_name_dict or names must be provided.\n1536 \n1537         Returns\n1538         -------\n1539         renamed : DataArray\n1540             Renamed array or array with renamed coordinates.\n1541 \n1542         See Also\n1543         --------\n1544         Dataset.rename\n1545         DataArray.swap_dims\n1546         \"\"\"\n1547         if names or utils.is_dict_like(new_name_or_name_dict):\n1548             new_name_or_name_dict = cast(\n1549                 Mapping[Hashable, Hashable], new_name_or_name_dict\n1550             )\n1551             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n1552             dataset = self._to_temp_dataset().rename(name_dict)\n1553             return self._from_temp_dataset(dataset)\n1554         else:\n1555             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n1556             return self._replace(name=new_name_or_name_dict)\n1557 \n1558     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> \"DataArray\":\n1559         \"\"\"Returns a new DataArray with swapped dimensions.\n1560 \n1561         Parameters\n1562         ----------\n1563         dims_dict : dict-like\n1564             Dictionary whose keys are current dimension names and whose values\n1565             are new names.\n1566 \n1567         Returns\n1568         -------\n1569         swapped : DataArray\n1570             DataArray with swapped dimensions.\n1571 \n1572         Examples\n1573         --------\n1574 \n1575         >>> arr = xr.DataArray(\n1576         ...     data=[0, 1], dims=\"x\", coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n1577         ... )\n1578         >>> arr\n1579         <xarray.DataArray (x: 2)>\n1580         array([0, 1])\n1581         Coordinates:\n1582           * x        (x) <U1 'a' 'b'\n1583             y        (x) int64 0 1\n1584 \n1585         >>> arr.swap_dims({\"x\": \"y\"})\n1586         <xarray.DataArray (y: 2)>\n1587         array([0, 1])\n1588         Coordinates:\n1589             x        (y) <U1 'a' 'b'\n1590           * y        (y) int64 0 1\n1591 \n1592         >>> arr.swap_dims({\"x\": \"z\"})\n1593         <xarray.DataArray (z: 2)>\n1594         array([0, 1])\n1595         Coordinates:\n1596             x        (z) <U1 'a' 'b'\n1597             y        (z) int64 0 1\n1598         Dimensions without coordinates: z\n1599 \n1600         See Also\n1601         --------\n1602 \n1603         DataArray.rename\n1604         Dataset.swap_dims\n1605         \"\"\"\n1606         ds = self._to_temp_dataset().swap_dims(dims_dict)\n1607         return self._from_temp_dataset(ds)\n1608 \n1609     def expand_dims(\n1610         self,\n1611         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n1612         axis=None,\n1613         **dim_kwargs: Any,\n1614     ) -> \"DataArray\":\n1615         \"\"\"Return a new object with an additional axis (or axes) inserted at\n1616         the corresponding position in the array shape. The new object is a\n1617         view into the underlying array, not a copy.\n1618 \n1619 \n1620         If dim is already a scalar coordinate, it will be promoted to a 1D\n1621         coordinate consisting of a single value.\n1622 \n1623         Parameters\n1624         ----------\n1625         dim : hashable, sequence of hashable, dict, or None\n1626             Dimensions to include on the new variable.\n1627             If provided as str or sequence of str, then dimensions are inserted\n1628             with length 1. If provided as a dict, then the keys are the new\n1629             dimensions and the values are either integers (giving the length of\n1630             the new dimensions) or sequence/ndarray (giving the coordinates of\n1631             the new dimensions).\n1632         axis : integer, list (or tuple) of integers, or None\n1633             Axis position(s) where new axis is to be inserted (position(s) on\n1634             the result array). If a list (or tuple) of integers is passed,\n1635             multiple axes are inserted. In this case, dim arguments should be\n1636             same length list. If axis=None is passed, all the axes will be\n1637             inserted to the start of the result array.\n1638         **dim_kwargs : int or sequence/ndarray\n1639             The keywords are arbitrary dimensions being inserted and the values\n1640             are either the lengths of the new dims (if int is given), or their\n1641             coordinates. Note, this is an alternative to passing a dict to the\n1642             dim kwarg and will only be used if dim is None.\n1643 \n1644         Returns\n1645         -------\n1646         expanded : same type as caller\n1647             This object, but with an additional dimension(s).\n1648         \"\"\"\n1649         if isinstance(dim, int):\n1650             raise TypeError(\"dim should be hashable or sequence/mapping of hashables\")\n1651         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n1652             if len(dim) != len(set(dim)):\n1653                 raise ValueError(\"dims should not contain duplicate values.\")\n1654             dim = dict.fromkeys(dim, 1)\n1655         elif dim is not None and not isinstance(dim, Mapping):\n1656             dim = {cast(Hashable, dim): 1}\n1657 \n1658         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n1659         ds = self._to_temp_dataset().expand_dims(dim, axis)\n1660         return self._from_temp_dataset(ds)\n1661 \n1662     def set_index(\n1663         self,\n1664         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n1665         append: bool = False,\n1666         inplace: bool = None,\n1667         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n1668     ) -> Optional[\"DataArray\"]:\n1669         \"\"\"Set DataArray (multi-)indexes using one or more existing\n1670         coordinates.\n1671 \n1672         Parameters\n1673         ----------\n1674         indexes : {dim: index, ...}\n1675             Mapping from names matching dimensions and values given\n1676             by (lists of) the names of existing coordinates or variables to set\n1677             as new (multi-)index.\n1678         append : bool, optional\n1679             If True, append the supplied index(es) to the existing index(es).\n1680             Otherwise replace the existing index(es) (default).\n1681         **indexes_kwargs: optional\n1682             The keyword arguments form of ``indexes``.\n1683             One of indexes or indexes_kwargs must be provided.\n1684 \n1685         Returns\n1686         -------\n1687         obj : DataArray\n1688             Another DataArray, with this data but replaced coordinates.\n1689 \n1690         Examples\n1691         --------\n1692         >>> arr = xr.DataArray(\n1693         ...     data=np.ones((2, 3)),\n1694         ...     dims=[\"x\", \"y\"],\n1695         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n1696         ... )\n1697         >>> arr\n1698         <xarray.DataArray (x: 2, y: 3)>\n1699         array([[1., 1., 1.],\n1700                [1., 1., 1.]])\n1701         Coordinates:\n1702           * x        (x) int64 0 1\n1703           * y        (y) int64 0 1 2\n1704             a        (x) int64 3 4\n1705         >>> arr.set_index(x=\"a\")\n1706         <xarray.DataArray (x: 2, y: 3)>\n1707         array([[1., 1., 1.],\n1708                [1., 1., 1.]])\n1709         Coordinates:\n1710           * x        (x) int64 3 4\n1711           * y        (y) int64 0 1 2\n1712 \n1713         See Also\n1714         --------\n1715         DataArray.reset_index\n1716         \"\"\"\n1717         ds = self._to_temp_dataset().set_index(\n1718             indexes, append=append, inplace=inplace, **indexes_kwargs\n1719         )\n1720         return self._from_temp_dataset(ds)\n1721 \n1722     def reset_index(\n1723         self,\n1724         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n1725         drop: bool = False,\n1726         inplace: bool = None,\n1727     ) -> Optional[\"DataArray\"]:\n1728         \"\"\"Reset the specified index(es) or multi-index level(s).\n1729 \n1730         Parameters\n1731         ----------\n1732         dims_or_levels : hashable or sequence of hashables\n1733             Name(s) of the dimension(s) and/or multi-index level(s) that will\n1734             be reset.\n1735         drop : bool, optional\n1736             If True, remove the specified indexes and/or multi-index levels\n1737             instead of extracting them as new coordinates (default: False).\n1738 \n1739         Returns\n1740         -------\n1741         obj : DataArray\n1742             Another dataarray, with this dataarray's data but replaced\n1743             coordinates.\n1744 \n1745         See Also\n1746         --------\n1747         DataArray.set_index\n1748         \"\"\"\n1749         _check_inplace(inplace)\n1750         coords, _ = split_indexes(\n1751             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n1752         )\n1753         return self._replace(coords=coords)\n1754 \n1755     def reorder_levels(\n1756         self,\n1757         dim_order: Mapping[Hashable, Sequence[int]] = None,\n1758         inplace: bool = None,\n1759         **dim_order_kwargs: Sequence[int],\n1760     ) -> \"DataArray\":\n1761         \"\"\"Rearrange index levels using input order.\n1762 \n1763         Parameters\n1764         ----------\n1765         dim_order : optional\n1766             Mapping from names matching dimensions and values given\n1767             by lists representing new level orders. Every given dimension\n1768             must have a multi-index.\n1769         **dim_order_kwargs: optional\n1770             The keyword arguments form of ``dim_order``.\n1771             One of dim_order or dim_order_kwargs must be provided.\n1772 \n1773         Returns\n1774         -------\n1775         obj : DataArray\n1776             Another dataarray, with this dataarray's data but replaced\n1777             coordinates.\n1778         \"\"\"\n1779         _check_inplace(inplace)\n1780         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n1781         replace_coords = {}\n1782         for dim, order in dim_order.items():\n1783             coord = self._coords[dim]\n1784             index = coord.to_index()\n1785             if not isinstance(index, pd.MultiIndex):\n1786                 raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n1787             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n1788         coords = self._coords.copy()\n1789         coords.update(replace_coords)\n1790         return self._replace(coords=coords)\n1791 \n1792     def stack(\n1793         self,\n1794         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n1795         **dimensions_kwargs: Sequence[Hashable],\n1796     ) -> \"DataArray\":\n1797         \"\"\"\n1798         Stack any number of existing dimensions into a single new dimension.\n1799 \n1800         New dimensions will be added at the end, and the corresponding\n1801         coordinate variables will be combined into a MultiIndex.\n1802 \n1803         Parameters\n1804         ----------\n1805         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n1806             Names of new dimensions, and the existing dimensions that they\n1807             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n1808             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n1809             all dimensions.\n1810         **dimensions_kwargs:\n1811             The keyword arguments form of ``dimensions``.\n1812             One of dimensions or dimensions_kwargs must be provided.\n1813 \n1814         Returns\n1815         -------\n1816         stacked : DataArray\n1817             DataArray with stacked data.\n1818 \n1819         Examples\n1820         --------\n1821 \n1822         >>> arr = xr.DataArray(\n1823         ...     np.arange(6).reshape(2, 3),\n1824         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1825         ... )\n1826         >>> arr\n1827         <xarray.DataArray (x: 2, y: 3)>\n1828         array([[0, 1, 2],\n1829                [3, 4, 5]])\n1830         Coordinates:\n1831           * x        (x) |S1 'a' 'b'\n1832           * y        (y) int64 0 1 2\n1833         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1834         >>> stacked.indexes[\"z\"]\n1835         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1836                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1837                    names=['x', 'y'])\n1838 \n1839         See Also\n1840         --------\n1841         DataArray.unstack\n1842         \"\"\"\n1843         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n1844         return self._from_temp_dataset(ds)\n1845 \n1846     def unstack(\n1847         self,\n1848         dim: Union[Hashable, Sequence[Hashable], None] = None,\n1849         fill_value: Any = dtypes.NA,\n1850         sparse: bool = False,\n1851     ) -> \"DataArray\":\n1852         \"\"\"\n1853         Unstack existing dimensions corresponding to MultiIndexes into\n1854         multiple new dimensions.\n1855 \n1856         New dimensions will be added at the end.\n1857 \n1858         Parameters\n1859         ----------\n1860         dim : hashable or sequence of hashable, optional\n1861             Dimension(s) over which to unstack. By default unstacks all\n1862             MultiIndexes.\n1863         fill_value: value to be filled. By default, np.nan\n1864         sparse: use sparse-array if True\n1865 \n1866         Returns\n1867         -------\n1868         unstacked : DataArray\n1869             Array with unstacked data.\n1870 \n1871         Examples\n1872         --------\n1873 \n1874         >>> arr = xr.DataArray(\n1875         ...     np.arange(6).reshape(2, 3),\n1876         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1877         ... )\n1878         >>> arr\n1879         <xarray.DataArray (x: 2, y: 3)>\n1880         array([[0, 1, 2],\n1881                [3, 4, 5]])\n1882         Coordinates:\n1883           * x        (x) |S1 'a' 'b'\n1884           * y        (y) int64 0 1 2\n1885         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1886         >>> stacked.indexes[\"z\"]\n1887         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1888                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1889                    names=['x', 'y'])\n1890         >>> roundtripped = stacked.unstack()\n1891         >>> arr.identical(roundtripped)\n1892         True\n1893 \n1894         See Also\n1895         --------\n1896         DataArray.stack\n1897         \"\"\"\n1898         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n1899         return self._from_temp_dataset(ds)\n1900 \n1901     def to_unstacked_dataset(self, dim, level=0):\n1902         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n1903         stacked coordinate.\n1904 \n1905         This is the inverse operation of Dataset.to_stacked_array.\n1906 \n1907         Parameters\n1908         ----------\n1909         dim : str\n1910             Name of existing dimension to unstack\n1911         level : int or str\n1912             The MultiIndex level to expand to a dataset along. Can either be\n1913             the integer index of the level or its name.\n1914         label : int, default 0\n1915             Label of the level to expand dataset along. Overrides the label\n1916             argument if given.\n1917 \n1918         Returns\n1919         -------\n1920         unstacked: Dataset\n1921 \n1922         Examples\n1923         --------\n1924         >>> import xarray as xr\n1925         >>> arr = xr.DataArray(\n1926         ...     np.arange(6).reshape(2, 3),\n1927         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1928         ... )\n1929         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n1930         >>> data\n1931         <xarray.Dataset>\n1932         Dimensions:  (x: 2, y: 3)\n1933         Coordinates:\n1934           * x        (x) <U1 'a' 'b'\n1935           * y        (y) int64 0 1 2\n1936         Data variables:\n1937             a        (x, y) int64 0 1 2 3 4 5\n1938             b        (x) int64 0 3\n1939         >>> stacked = data.to_stacked_array(\"z\", [\"y\"])\n1940         >>> stacked.indexes[\"z\"]\n1941         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1942                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n1943                 names=['variable', 'y'])\n1944         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n1945         >>> data.identical(roundtripped)\n1946         True\n1947 \n1948         See Also\n1949         --------\n1950         Dataset.to_stacked_array\n1951         \"\"\"\n1952 \n1953         idx = self.indexes[dim]\n1954         if not isinstance(idx, pd.MultiIndex):\n1955             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n1956 \n1957         level_number = idx._get_level_number(level)\n1958         variables = idx.levels[level_number]\n1959         variable_dim = idx.names[level_number]\n1960 \n1961         # pull variables out of datarray\n1962         data_dict = {}\n1963         for k in variables:\n1964             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n1965 \n1966         # unstacked dataset\n1967         return Dataset(data_dict)\n1968 \n1969     def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> \"DataArray\":\n1970         \"\"\"Return a new DataArray object with transposed dimensions.\n1971 \n1972         Parameters\n1973         ----------\n1974         *dims : hashable, optional\n1975             By default, reverse the dimensions. Otherwise, reorder the\n1976             dimensions to this order.\n1977         transpose_coords : boolean, default True\n1978             If True, also transpose the coordinates of this DataArray.\n1979 \n1980         Returns\n1981         -------\n1982         transposed : DataArray\n1983             The returned DataArray's array is transposed.\n1984 \n1985         Notes\n1986         -----\n1987         This operation returns a view of this array's data. It is\n1988         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n1989         -- the data will be fully loaded.\n1990 \n1991         See Also\n1992         --------\n1993         numpy.transpose\n1994         Dataset.transpose\n1995         \"\"\"\n1996         if dims:\n1997             dims = tuple(utils.infix_dims(dims, self.dims))\n1998         variable = self.variable.transpose(*dims)\n1999         if transpose_coords:\n2000             coords: Dict[Hashable, Variable] = {}\n2001             for name, coord in self.coords.items():\n2002                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2003                 coords[name] = coord.variable.transpose(*coord_dims)\n2004             return self._replace(variable, coords)\n2005         else:\n2006             return self._replace(variable)\n2007 \n2008     @property\n2009     def T(self) -> \"DataArray\":\n2010         return self.transpose()\n2011 \n2012     def drop_vars(\n2013         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n2014     ) -> \"DataArray\":\n2015         \"\"\"Drop variables from this DataArray.\n2016 \n2017         Parameters\n2018         ----------\n2019         names : hashable or iterable of hashables\n2020             Name(s) of variables to drop.\n2021         errors: {'raise', 'ignore'}, optional\n2022             If 'raise' (default), raises a ValueError error if any of the variable\n2023             passed are not in the dataset. If 'ignore', any given names that are in the\n2024             DataArray are dropped and no error is raised.\n2025 \n2026         Returns\n2027         -------\n2028         dropped : Dataset\n2029 \n2030         \"\"\"\n2031         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n2032         return self._from_temp_dataset(ds)\n2033 \n2034     def drop(\n2035         self,\n2036         labels: Mapping = None,\n2037         dim: Hashable = None,\n2038         *,\n2039         errors: str = \"raise\",\n2040         **labels_kwargs,\n2041     ) -> \"DataArray\":\n2042         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n2043 \n2044         Using either `drop_vars` or `drop_sel` is encouraged\n2045 \n2046         See Also\n2047         --------\n2048         DataArray.drop_vars\n2049         DataArray.drop_sel\n2050         \"\"\"\n2051         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n2052         return self._from_temp_dataset(ds)\n2053 \n2054     def drop_sel(\n2055         self,\n2056         labels: Mapping[Hashable, Any] = None,\n2057         *,\n2058         errors: str = \"raise\",\n2059         **labels_kwargs,\n2060     ) -> \"DataArray\":\n2061         \"\"\"Drop index labels from this DataArray.\n2062 \n2063         Parameters\n2064         ----------\n2065         labels : Mapping[Hashable, Any]\n2066             Index labels to drop\n2067         errors: {'raise', 'ignore'}, optional\n2068             If 'raise' (default), raises a ValueError error if\n2069             any of the index labels passed are not\n2070             in the dataset. If 'ignore', any given labels that are in the\n2071             dataset are dropped and no error is raised.\n2072         **labels_kwargs : {dim: label, ...}, optional\n2073             The keyword arguments form of ``dim`` and ``labels``\n2074 \n2075         Returns\n2076         -------\n2077         dropped : DataArray\n2078         \"\"\"\n2079         if labels_kwargs or isinstance(labels, dict):\n2080             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n2081 \n2082         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n2083         return self._from_temp_dataset(ds)\n2084 \n2085     def dropna(\n2086         self, dim: Hashable, how: str = \"any\", thresh: int = None\n2087     ) -> \"DataArray\":\n2088         \"\"\"Returns a new array with dropped labels for missing values along\n2089         the provided dimension.\n2090 \n2091         Parameters\n2092         ----------\n2093         dim : hashable\n2094             Dimension along which to drop missing values. Dropping along\n2095             multiple dimensions simultaneously is not yet supported.\n2096         how : {'any', 'all'}, optional\n2097             * any : if any NA values are present, drop that label\n2098             * all : if all values are NA, drop that label\n2099         thresh : int, default None\n2100             If supplied, require this many non-NA values.\n2101 \n2102         Returns\n2103         -------\n2104         DataArray\n2105         \"\"\"\n2106         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2107         return self._from_temp_dataset(ds)\n2108 \n2109     def fillna(self, value: Any) -> \"DataArray\":\n2110         \"\"\"Fill missing values in this object.\n2111 \n2112         This operation follows the normal broadcasting and alignment rules that\n2113         xarray uses for binary arithmetic, except the result is aligned to this\n2114         object (``join='left'``) instead of aligned to the intersection of\n2115         index coordinates (``join='inner'``).\n2116 \n2117         Parameters\n2118         ----------\n2119         value : scalar, ndarray or DataArray\n2120             Used to fill all matching missing values in this array. If the\n2121             argument is a DataArray, it is first aligned with (reindexed to)\n2122             this array.\n2123 \n2124         Returns\n2125         -------\n2126         DataArray\n2127         \"\"\"\n2128         if utils.is_dict_like(value):\n2129             raise TypeError(\n2130                 \"cannot provide fill value as a dictionary with \"\n2131                 \"fillna on a DataArray\"\n2132             )\n2133         out = ops.fillna(self, value)\n2134         return out\n2135 \n2136     def interpolate_na(\n2137         self,\n2138         dim: Hashable = None,\n2139         method: str = \"linear\",\n2140         limit: int = None,\n2141         use_coordinate: Union[bool, str] = True,\n2142         max_gap: Union[\n2143             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n2144         ] = None,\n2145         keep_attrs: bool = None,\n2146         **kwargs: Any,\n2147     ) -> \"DataArray\":\n2148         \"\"\"Fill in NaNs by interpolating according to different methods.\n2149 \n2150         Parameters\n2151         ----------\n2152         dim : str\n2153             Specifies the dimension along which to interpolate.\n2154         method : str, optional\n2155             String indicating which method to use for interpolation:\n2156 \n2157             - 'linear': linear interpolation (Default). Additional keyword\n2158               arguments are passed to :py:func:`numpy.interp`\n2159             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2160               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2161               ``method='polynomial'``, the ``order`` keyword argument must also be\n2162               provided.\n2163             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2164               respective :py:class:`scipy.interpolate` classes.\n2165 \n2166         use_coordinate : bool, str, default True\n2167             Specifies which index to use as the x values in the interpolation\n2168             formulated as `y = f(x)`. If False, values are treated as if\n2169             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2170             used. If ``use_coordinate`` is a string, it specifies the name of a\n2171             coordinate variariable to use as the index.\n2172         limit : int, default None\n2173             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2174             or None for no limit. This filling is done regardless of the size of\n2175             the gap in the data. To only interpolate over gaps less than a given length,\n2176             see ``max_gap``.\n2177         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n2178             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2179             Use None for no limit. When interpolating along a datetime64 dimension\n2180             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2181 \n2182             - a string that is valid input for pandas.to_timedelta\n2183             - a :py:class:`numpy.timedelta64` object\n2184             - a :py:class:`pandas.Timedelta` object\n2185             - a :py:class:`datetime.timedelta` object\n2186 \n2187             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2188             dimensions has not been implemented yet. Gap length is defined as the difference\n2189             between coordinate values at the first data point after a gap and the last value\n2190             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2191             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2192             For example, consider::\n2193 \n2194                 <xarray.DataArray (x: 9)>\n2195                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2196                 Coordinates:\n2197                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2198 \n2199             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2200         keep_attrs : bool, default True\n2201             If True, the dataarray's attributes (`attrs`) will be copied from\n2202             the original object to the new one.  If False, the new\n2203             object will be returned without attributes.\n2204         kwargs : dict, optional\n2205             parameters passed verbatim to the underlying interpolation function\n2206 \n2207         Returns\n2208         -------\n2209         interpolated: DataArray\n2210             Filled in DataArray.\n2211 \n2212         See also\n2213         --------\n2214         numpy.interp\n2215         scipy.interpolate\n2216         \"\"\"\n2217         from .missing import interp_na\n2218 \n2219         return interp_na(\n2220             self,\n2221             dim=dim,\n2222             method=method,\n2223             limit=limit,\n2224             use_coordinate=use_coordinate,\n2225             max_gap=max_gap,\n2226             keep_attrs=keep_attrs,\n2227             **kwargs,\n2228         )\n2229 \n2230     def ffill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2231         \"\"\"Fill NaN values by propogating values forward\n2232 \n2233         *Requires bottleneck.*\n2234 \n2235         Parameters\n2236         ----------\n2237         dim : hashable\n2238             Specifies the dimension along which to propagate values when\n2239             filling.\n2240         limit : int, default None\n2241             The maximum number of consecutive NaN values to forward fill. In\n2242             other words, if there is a gap with more than this number of\n2243             consecutive NaNs, it will only be partially filled. Must be greater\n2244             than 0 or None for no limit.\n2245 \n2246         Returns\n2247         -------\n2248         DataArray\n2249         \"\"\"\n2250         from .missing import ffill\n2251 \n2252         return ffill(self, dim, limit=limit)\n2253 \n2254     def bfill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2255         \"\"\"Fill NaN values by propogating values backward\n2256 \n2257         *Requires bottleneck.*\n2258 \n2259         Parameters\n2260         ----------\n2261         dim : str\n2262             Specifies the dimension along which to propagate values when\n2263             filling.\n2264         limit : int, default None\n2265             The maximum number of consecutive NaN values to backward fill. In\n2266             other words, if there is a gap with more than this number of\n2267             consecutive NaNs, it will only be partially filled. Must be greater\n2268             than 0 or None for no limit.\n2269 \n2270         Returns\n2271         -------\n2272         DataArray\n2273         \"\"\"\n2274         from .missing import bfill\n2275 \n2276         return bfill(self, dim, limit=limit)\n2277 \n2278     def combine_first(self, other: \"DataArray\") -> \"DataArray\":\n2279         \"\"\"Combine two DataArray objects, with union of coordinates.\n2280 \n2281         This operation follows the normal broadcasting and alignment rules of\n2282         ``join='outer'``.  Default to non-null values of array calling the\n2283         method.  Use np.nan to fill in vacant cells after alignment.\n2284 \n2285         Parameters\n2286         ----------\n2287         other : DataArray\n2288             Used to fill all matching missing values in this array.\n2289 \n2290         Returns\n2291         -------\n2292         DataArray\n2293         \"\"\"\n2294         return ops.fillna(self, other, join=\"outer\")\n2295 \n2296     def reduce(\n2297         self,\n2298         func: Callable[..., Any],\n2299         dim: Union[None, Hashable, Sequence[Hashable]] = None,\n2300         axis: Union[None, int, Sequence[int]] = None,\n2301         keep_attrs: bool = None,\n2302         keepdims: bool = False,\n2303         **kwargs: Any,\n2304     ) -> \"DataArray\":\n2305         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2306 \n2307         Parameters\n2308         ----------\n2309         func : function\n2310             Function which can be called in the form\n2311             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2312             np.ndarray over an integer valued axis.\n2313         dim : hashable or sequence of hashables, optional\n2314             Dimension(s) over which to apply `func`.\n2315         axis : int or sequence of int, optional\n2316             Axis(es) over which to repeatedly apply `func`. Only one of the\n2317             'dim' and 'axis' arguments can be supplied. If neither are\n2318             supplied, then the reduction is calculated over the flattened array\n2319             (by calling `f(x)` without an axis argument).\n2320         keep_attrs : bool, optional\n2321             If True, the variable's attributes (`attrs`) will be copied from\n2322             the original object to the new one.  If False (default), the new\n2323             object will be returned without attributes.\n2324         keepdims : bool, default False\n2325             If True, the dimensions which are reduced are left in the result\n2326             as dimensions of size one. Coordinates that use these dimensions\n2327             are removed.\n2328         **kwargs : dict\n2329             Additional keyword arguments passed on to `func`.\n2330 \n2331         Returns\n2332         -------\n2333         reduced : DataArray\n2334             DataArray with this object's array replaced with an array with\n2335             summarized data and the indicated dimension(s) removed.\n2336         \"\"\"\n2337 \n2338         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2339         return self._replace_maybe_drop_dims(var)\n2340 \n2341     def to_pandas(self) -> Union[\"DataArray\", pd.Series, pd.DataFrame]:\n2342         \"\"\"Convert this array into a pandas object with the same shape.\n2343 \n2344         The type of the returned object depends on the number of DataArray\n2345         dimensions:\n2346 \n2347         * 0D -> `xarray.DataArray`\n2348         * 1D -> `pandas.Series`\n2349         * 2D -> `pandas.DataFrame`\n2350 \n2351         Only works for arrays with 2 or fewer dimensions.\n2352 \n2353         The DataArray constructor performs the inverse transformation.\n2354         \"\"\"\n2355         # TODO: consolidate the info about pandas constructors and the\n2356         # attributes that correspond to their indexes into a separate module?\n2357         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2358         try:\n2359             constructor = constructors[self.ndim]\n2360         except KeyError:\n2361             raise ValueError(\n2362                 \"cannot convert arrays with %s dimensions into \"\n2363                 \"pandas objects\" % self.ndim\n2364             )\n2365         indexes = [self.get_index(dim) for dim in self.dims]\n2366         return constructor(self.values, *indexes)\n2367 \n2368     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n2369         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2370 \n2371         The DataFrame is indexed by the Cartesian product of index coordinates\n2372         (in the form of a :py:class:`pandas.MultiIndex`).\n2373 \n2374         Other coordinates are included as columns in the DataFrame.\n2375         \"\"\"\n2376         if name is None:\n2377             name = self.name\n2378         if name is None:\n2379             raise ValueError(\n2380                 \"cannot convert an unnamed DataArray to a \"\n2381                 \"DataFrame: use the ``name`` parameter\"\n2382             )\n2383 \n2384         dims = dict(zip(self.dims, self.shape))\n2385         # By using a unique name, we can convert a DataArray into a DataFrame\n2386         # even if it shares a name with one of its coordinates.\n2387         # I would normally use unique_name = object() but that results in a\n2388         # dataframe with columns in the wrong order, for reasons I have not\n2389         # been able to debug (possibly a pandas bug?).\n2390         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n2391         ds = self._to_dataset_whole(name=unique_name)\n2392         df = ds._to_dataframe(dims)\n2393         df.columns = [name if c == unique_name else c for c in df.columns]\n2394         return df\n2395 \n2396     def to_series(self) -> pd.Series:\n2397         \"\"\"Convert this array into a pandas.Series.\n2398 \n2399         The Series is indexed by the Cartesian product of index coordinates\n2400         (in the form of a :py:class:`pandas.MultiIndex`).\n2401         \"\"\"\n2402         index = self.coords.to_index()\n2403         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n2404 \n2405     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n2406         \"\"\"Convert this array into a numpy.ma.MaskedArray\n2407 \n2408         Parameters\n2409         ----------\n2410         copy : bool\n2411             If True (default) make a copy of the array in the result. If False,\n2412             a MaskedArray view of DataArray.values is returned.\n2413 \n2414         Returns\n2415         -------\n2416         result : MaskedArray\n2417             Masked where invalid values (nan or inf) occur.\n2418         \"\"\"\n2419         values = self.values  # only compute lazy arrays once\n2420         isnull = pd.isnull(values)\n2421         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n2422 \n2423     def to_netcdf(self, *args, **kwargs) -> Union[bytes, \"Delayed\", None]:\n2424         \"\"\"Write DataArray contents to a netCDF file.\n2425 \n2426         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n2427 \n2428         Notes\n2429         -----\n2430         Only xarray.Dataset objects can be written to netCDF files, so\n2431         the xarray.DataArray is converted to a xarray.Dataset object\n2432         containing a single variable. If the DataArray has no name, or if the\n2433         name is the same as a co-ordinate name, then it is given the name\n2434         '__xarray_dataarray_variable__'.\n2435         \"\"\"\n2436         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n2437 \n2438         if self.name is None:\n2439             # If no name is set then use a generic xarray name\n2440             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2441         elif self.name in self.coords or self.name in self.dims:\n2442             # The name is the same as one of the coords names, which netCDF\n2443             # doesn't support, so rename it but keep track of the old name\n2444             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2445             dataset.attrs[DATAARRAY_NAME] = self.name\n2446         else:\n2447             # No problems with the name - so we're fine!\n2448             dataset = self.to_dataset()\n2449 \n2450         return dataset.to_netcdf(*args, **kwargs)\n2451 \n2452     def to_dict(self, data: bool = True) -> dict:\n2453         \"\"\"\n2454         Convert this xarray.DataArray into a dictionary following xarray\n2455         naming conventions.\n2456 \n2457         Converts all variables and attributes to native Python objects.\n2458         Useful for converting to json. To avoid datetime incompatibility\n2459         use decode_times=False kwarg in xarrray.open_dataset.\n2460 \n2461         Parameters\n2462         ----------\n2463         data : bool, optional\n2464             Whether to include the actual data in the dictionary. When set to\n2465             False, returns just the schema.\n2466 \n2467         See also\n2468         --------\n2469         DataArray.from_dict\n2470         \"\"\"\n2471         d = self.variable.to_dict(data=data)\n2472         d.update({\"coords\": {}, \"name\": self.name})\n2473         for k in self.coords:\n2474             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n2475         return d\n2476 \n2477     @classmethod\n2478     def from_dict(cls, d: dict) -> \"DataArray\":\n2479         \"\"\"\n2480         Convert a dictionary into an xarray.DataArray\n2481 \n2482         Input dict can take several forms::\n2483 \n2484             d = {'dims': ('t'), 'data': x}\n2485 \n2486             d = {'coords': {'t': {'dims': 't', 'data': t,\n2487                                   'attrs': {'units':'s'}}},\n2488                  'attrs': {'title': 'air temperature'},\n2489                  'dims': 't',\n2490                  'data': x,\n2491                  'name': 'a'}\n2492 \n2493         where 't' is the name of the dimesion, 'a' is the name of the array,\n2494         and  x and t are lists, numpy.arrays, or pandas objects.\n2495 \n2496         Parameters\n2497         ----------\n2498         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n2499 \n2500         Returns\n2501         -------\n2502         obj : xarray.DataArray\n2503 \n2504         See also\n2505         --------\n2506         DataArray.to_dict\n2507         Dataset.from_dict\n2508         \"\"\"\n2509         coords = None\n2510         if \"coords\" in d:\n2511             try:\n2512                 coords = {\n2513                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n2514                     for k, v in d[\"coords\"].items()\n2515                 }\n2516             except KeyError as e:\n2517                 raise ValueError(\n2518                     \"cannot convert dict when coords are missing the key \"\n2519                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n2520                 )\n2521         try:\n2522             data = d[\"data\"]\n2523         except KeyError:\n2524             raise ValueError(\"cannot convert dict without the key 'data''\")\n2525         else:\n2526             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n2527         return obj\n2528 \n2529     @classmethod\n2530     def from_series(cls, series: pd.Series, sparse: bool = False) -> \"DataArray\":\n2531         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n2532 \n2533         If the series's index is a MultiIndex, it will be expanded into a\n2534         tensor product of one-dimensional coordinates (filling in missing\n2535         values with NaN). Thus this operation should be the inverse of the\n2536         `to_series` method.\n2537 \n2538         If sparse=True, creates a sparse array instead of a dense NumPy array.\n2539         Requires the pydata/sparse package.\n2540 \n2541         See also\n2542         --------\n2543         xarray.Dataset.from_dataframe\n2544         \"\"\"\n2545         temp_name = \"__temporary_name\"\n2546         df = pd.DataFrame({temp_name: series})\n2547         ds = Dataset.from_dataframe(df, sparse=sparse)\n2548         result = cast(DataArray, ds[temp_name])\n2549         result.name = series.name\n2550         return result\n2551 \n2552     def to_cdms2(self) -> \"cdms2_Variable\":\n2553         \"\"\"Convert this array into a cdms2.Variable\n2554         \"\"\"\n2555         from ..convert import to_cdms2\n2556 \n2557         return to_cdms2(self)\n2558 \n2559     @classmethod\n2560     def from_cdms2(cls, variable: \"cdms2_Variable\") -> \"DataArray\":\n2561         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\n2562         \"\"\"\n2563         from ..convert import from_cdms2\n2564 \n2565         return from_cdms2(variable)\n2566 \n2567     def to_iris(self) -> \"iris_Cube\":\n2568         \"\"\"Convert this array into a iris.cube.Cube\n2569         \"\"\"\n2570         from ..convert import to_iris\n2571 \n2572         return to_iris(self)\n2573 \n2574     @classmethod\n2575     def from_iris(cls, cube: \"iris_Cube\") -> \"DataArray\":\n2576         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\n2577         \"\"\"\n2578         from ..convert import from_iris\n2579 \n2580         return from_iris(cube)\n2581 \n2582     def _all_compat(self, other: \"DataArray\", compat_str: str) -> bool:\n2583         \"\"\"Helper function for equals, broadcast_equals, and identical\n2584         \"\"\"\n2585 \n2586         def compat(x, y):\n2587             return getattr(x.variable, compat_str)(y.variable)\n2588 \n2589         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n2590             self, other\n2591         )\n2592 \n2593     def broadcast_equals(self, other: \"DataArray\") -> bool:\n2594         \"\"\"Two DataArrays are broadcast equal if they are equal after\n2595         broadcasting them against each other such that they have the same\n2596         dimensions.\n2597 \n2598         See Also\n2599         --------\n2600         DataArray.equals\n2601         DataArray.identical\n2602         \"\"\"\n2603         try:\n2604             return self._all_compat(other, \"broadcast_equals\")\n2605         except (TypeError, AttributeError):\n2606             return False\n2607 \n2608     def equals(self, other: \"DataArray\") -> bool:\n2609         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n2610         values; otherwise False.\n2611 \n2612         DataArrays can still be equal (like pandas objects) if they have NaN\n2613         values in the same locations.\n2614 \n2615         This method is necessary because `v1 == v2` for ``DataArray``\n2616         does element-wise comparisons (like numpy.ndarrays).\n2617 \n2618         See Also\n2619         --------\n2620         DataArray.broadcast_equals\n2621         DataArray.identical\n2622         \"\"\"\n2623         try:\n2624             return self._all_compat(other, \"equals\")\n2625         except (TypeError, AttributeError):\n2626             return False\n2627 \n2628     def identical(self, other: \"DataArray\") -> bool:\n2629         \"\"\"Like equals, but also checks the array name and attributes, and\n2630         attributes on all coordinates.\n2631 \n2632         See Also\n2633         --------\n2634         DataArray.broadcast_equals\n2635         DataArray.equal\n2636         \"\"\"\n2637         try:\n2638             return self.name == other.name and self._all_compat(other, \"identical\")\n2639         except (TypeError, AttributeError):\n2640             return False\n2641 \n2642     def _result_name(self, other: Any = None) -> Optional[Hashable]:\n2643         # use the same naming heuristics as pandas:\n2644         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n2645         other_name = getattr(other, \"name\", _default)\n2646         if other_name is _default or other_name == self.name:\n2647             return self.name\n2648         else:\n2649             return None\n2650 \n2651     def __array_wrap__(self, obj, context=None) -> \"DataArray\":\n2652         new_var = self.variable.__array_wrap__(obj, context)\n2653         return self._replace(new_var)\n2654 \n2655     def __matmul__(self, obj):\n2656         return self.dot(obj)\n2657 \n2658     def __rmatmul__(self, other):\n2659         # currently somewhat duplicative, as only other DataArrays are\n2660         # compatible with matmul\n2661         return computation.dot(other, self)\n2662 \n2663     @staticmethod\n2664     def _unary_op(f: Callable[..., Any]) -> Callable[..., \"DataArray\"]:\n2665         @functools.wraps(f)\n2666         def func(self, *args, **kwargs):\n2667             with np.errstate(all=\"ignore\"):\n2668                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n2669 \n2670         return func\n2671 \n2672     @staticmethod\n2673     def _binary_op(\n2674         f: Callable[..., Any],\n2675         reflexive: bool = False,\n2676         join: str = None,  # see xarray.align\n2677         **ignored_kwargs,\n2678     ) -> Callable[..., \"DataArray\"]:\n2679         @functools.wraps(f)\n2680         def func(self, other):\n2681             if isinstance(other, (Dataset, groupby.GroupBy)):\n2682                 return NotImplemented\n2683             if isinstance(other, DataArray):\n2684                 align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n2685                 self, other = align(self, other, join=align_type, copy=False)\n2686             other_variable = getattr(other, \"variable\", other)\n2687             other_coords = getattr(other, \"coords\", None)\n2688 \n2689             variable = (\n2690                 f(self.variable, other_variable)\n2691                 if not reflexive\n2692                 else f(other_variable, self.variable)\n2693             )\n2694             coords, indexes = self.coords._merge_raw(other_coords)\n2695             name = self._result_name(other)\n2696 \n2697             return self._replace(variable, coords, name, indexes=indexes)\n2698 \n2699         return func\n2700 \n2701     @staticmethod\n2702     def _inplace_binary_op(f: Callable) -> Callable[..., \"DataArray\"]:\n2703         @functools.wraps(f)\n2704         def func(self, other):\n2705             if isinstance(other, groupby.GroupBy):\n2706                 raise TypeError(\n2707                     \"in-place operations between a DataArray and \"\n2708                     \"a grouped object are not permitted\"\n2709                 )\n2710             # n.b. we can't align other to self (with other.reindex_like(self))\n2711             # because `other` may be converted into floats, which would cause\n2712             # in-place arithmetic to fail unpredictably. Instead, we simply\n2713             # don't support automatic alignment with in-place arithmetic.\n2714             other_coords = getattr(other, \"coords\", None)\n2715             other_variable = getattr(other, \"variable\", other)\n2716             try:\n2717                 with self.coords._merge_inplace(other_coords):\n2718                     f(self.variable, other_variable)\n2719             except MergeError as exc:\n2720                 raise MergeError(\n2721                     \"Automatic alignment is not supported for in-place operations.\\n\"\n2722                     \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n2723                     \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n2724                 ) from exc\n2725             return self\n2726 \n2727         return func\n2728 \n2729     def _copy_attrs_from(self, other: Union[\"DataArray\", Dataset, Variable]) -> None:\n2730         self.attrs = other.attrs\n2731 \n2732     plot = utils.UncachedAccessor(_PlotMethods)\n2733 \n2734     def _title_for_slice(self, truncate: int = 50) -> str:\n2735         \"\"\"\n2736         If the dataarray has 1 dimensional coordinates or comes from a slice\n2737         we can show that info in the title\n2738 \n2739         Parameters\n2740         ----------\n2741         truncate : integer\n2742             maximum number of characters for title\n2743 \n2744         Returns\n2745         -------\n2746         title : string\n2747             Can be used for plot titles\n2748 \n2749         \"\"\"\n2750         one_dims = []\n2751         for dim, coord in self.coords.items():\n2752             if coord.size == 1:\n2753                 one_dims.append(\n2754                     \"{dim} = {v}\".format(dim=dim, v=format_item(coord.values))\n2755                 )\n2756 \n2757         title = \", \".join(one_dims)\n2758         if len(title) > truncate:\n2759             title = title[: (truncate - 3)] + \"...\"\n2760 \n2761         return title\n2762 \n2763     def diff(self, dim: Hashable, n: int = 1, label: Hashable = \"upper\") -> \"DataArray\":\n2764         \"\"\"Calculate the n-th order discrete difference along given axis.\n2765 \n2766         Parameters\n2767         ----------\n2768         dim : hashable\n2769             Dimension over which to calculate the finite difference.\n2770         n : int, optional\n2771             The number of times values are differenced.\n2772         label : hashable, optional\n2773             The new coordinate in dimension ``dim`` will have the\n2774             values of either the minuend's or subtrahend's coordinate\n2775             for values 'upper' and 'lower', respectively.  Other\n2776             values are not supported.\n2777 \n2778         Returns\n2779         -------\n2780         difference : same type as caller\n2781             The n-th order finite difference of this object.\n2782 \n2783         .. note::\n2784 \n2785             `n` matches numpy's behavior and is different from pandas' first\n2786             argument named `periods`.\n2787 \n2788 \n2789         Examples\n2790         --------\n2791         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n2792         >>> arr.diff(\"x\")\n2793         <xarray.DataArray (x: 3)>\n2794         array([0, 1, 0])\n2795         Coordinates:\n2796         * x        (x) int64 2 3 4\n2797         >>> arr.diff(\"x\", 2)\n2798         <xarray.DataArray (x: 2)>\n2799         array([ 1, -1])\n2800         Coordinates:\n2801         * x        (x) int64 3 4\n2802 \n2803         See Also\n2804         --------\n2805         DataArray.differentiate\n2806         \"\"\"\n2807         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n2808         return self._from_temp_dataset(ds)\n2809 \n2810     def shift(\n2811         self,\n2812         shifts: Mapping[Hashable, int] = None,\n2813         fill_value: Any = dtypes.NA,\n2814         **shifts_kwargs: int,\n2815     ) -> \"DataArray\":\n2816         \"\"\"Shift this array by an offset along one or more dimensions.\n2817 \n2818         Only the data is moved; coordinates stay in place. Values shifted from\n2819         beyond array bounds are replaced by NaN. This is consistent with the\n2820         behavior of ``shift`` in pandas.\n2821 \n2822         Parameters\n2823         ----------\n2824         shifts : Mapping with the form of {dim: offset}\n2825             Integer offset to shift along each of the given dimensions.\n2826             Positive offsets shift to the right; negative offsets shift to the\n2827             left.\n2828         fill_value: scalar, optional\n2829             Value to use for newly missing values\n2830         **shifts_kwargs:\n2831             The keyword arguments form of ``shifts``.\n2832             One of shifts or shifts_kwargs must be provided.\n2833 \n2834         Returns\n2835         -------\n2836         shifted : DataArray\n2837             DataArray with the same coordinates and attributes but shifted\n2838             data.\n2839 \n2840         See also\n2841         --------\n2842         roll\n2843 \n2844         Examples\n2845         --------\n2846 \n2847         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2848         >>> arr.shift(x=1)\n2849         <xarray.DataArray (x: 3)>\n2850         array([ nan,   5.,   6.])\n2851         Coordinates:\n2852           * x        (x) int64 0 1 2\n2853         \"\"\"\n2854         variable = self.variable.shift(\n2855             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n2856         )\n2857         return self._replace(variable=variable)\n2858 \n2859     def roll(\n2860         self,\n2861         shifts: Mapping[Hashable, int] = None,\n2862         roll_coords: bool = None,\n2863         **shifts_kwargs: int,\n2864     ) -> \"DataArray\":\n2865         \"\"\"Roll this array by an offset along one or more dimensions.\n2866 \n2867         Unlike shift, roll may rotate all variables, including coordinates\n2868         if specified. The direction of rotation is consistent with\n2869         :py:func:`numpy.roll`.\n2870 \n2871         Parameters\n2872         ----------\n2873         shifts : Mapping with the form of {dim: offset}\n2874             Integer offset to rotate each of the given dimensions.\n2875             Positive offsets roll to the right; negative offsets roll to the\n2876             left.\n2877         roll_coords : bool\n2878             Indicates whether to  roll the coordinates by the offset\n2879             The current default of roll_coords (None, equivalent to True) is\n2880             deprecated and will change to False in a future version.\n2881             Explicitly pass roll_coords to silence the warning.\n2882         **shifts_kwargs : The keyword arguments form of ``shifts``.\n2883             One of shifts or shifts_kwargs must be provided.\n2884 \n2885         Returns\n2886         -------\n2887         rolled : DataArray\n2888             DataArray with the same attributes but rolled data and coordinates.\n2889 \n2890         See also\n2891         --------\n2892         shift\n2893 \n2894         Examples\n2895         --------\n2896 \n2897         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2898         >>> arr.roll(x=1)\n2899         <xarray.DataArray (x: 3)>\n2900         array([7, 5, 6])\n2901         Coordinates:\n2902           * x        (x) int64 2 0 1\n2903         \"\"\"\n2904         ds = self._to_temp_dataset().roll(\n2905             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n2906         )\n2907         return self._from_temp_dataset(ds)\n2908 \n2909     @property\n2910     def real(self) -> \"DataArray\":\n2911         return self._replace(self.variable.real)\n2912 \n2913     @property\n2914     def imag(self) -> \"DataArray\":\n2915         return self._replace(self.variable.imag)\n2916 \n2917     def dot(\n2918         self, other: \"DataArray\", dims: Union[Hashable, Sequence[Hashable], None] = None\n2919     ) -> \"DataArray\":\n2920         \"\"\"Perform dot product of two DataArrays along their shared dims.\n2921 \n2922         Equivalent to taking taking tensordot over all shared dims.\n2923 \n2924         Parameters\n2925         ----------\n2926         other : DataArray\n2927             The other array with which the dot product is performed.\n2928         dims: '...', hashable or sequence of hashables, optional\n2929             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.\n2930             If not specified, then all the common dimensions are summed over.\n2931 \n2932         Returns\n2933         -------\n2934         result : DataArray\n2935             Array resulting from the dot product over all shared dimensions.\n2936 \n2937         See also\n2938         --------\n2939         dot\n2940         numpy.tensordot\n2941 \n2942         Examples\n2943         --------\n2944 \n2945         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n2946         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n2947         >>> dm_vals = np.arange(4)\n2948         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n2949 \n2950         >>> dm.dims\n2951         ('z')\n2952 \n2953         >>> da.dims\n2954         ('x', 'y', 'z')\n2955 \n2956         >>> dot_result = da.dot(dm)\n2957         >>> dot_result.dims\n2958         ('x', 'y')\n2959 \n2960         \"\"\"\n2961         if isinstance(other, Dataset):\n2962             raise NotImplementedError(\n2963                 \"dot products are not yet supported with Dataset objects.\"\n2964             )\n2965         if not isinstance(other, DataArray):\n2966             raise TypeError(\"dot only operates on DataArrays.\")\n2967 \n2968         return computation.dot(self, other, dims=dims)\n2969 \n2970     def sortby(\n2971         self,\n2972         variables: Union[Hashable, \"DataArray\", Sequence[Union[Hashable, \"DataArray\"]]],\n2973         ascending: bool = True,\n2974     ) -> \"DataArray\":\n2975         \"\"\"Sort object by labels or values (along an axis).\n2976 \n2977         Sorts the dataarray, either along specified dimensions,\n2978         or according to values of 1-D dataarrays that share dimension\n2979         with calling object.\n2980 \n2981         If the input variables are dataarrays, then the dataarrays are aligned\n2982         (via left-join) to the calling object prior to sorting by cell values.\n2983         NaNs are sorted to the end, following Numpy convention.\n2984 \n2985         If multiple sorts along the same dimension is\n2986         given, numpy's lexsort is performed along that dimension:\n2987         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n2988         and the FIRST key in the sequence is used as the primary sort key,\n2989         followed by the 2nd key, etc.\n2990 \n2991         Parameters\n2992         ----------\n2993         variables: hashable, DataArray, or sequence of either\n2994             1D DataArray objects or name(s) of 1D variable(s) in\n2995             coords whose values are used to sort this array.\n2996         ascending: boolean, optional\n2997             Whether to sort by ascending or descending order.\n2998 \n2999         Returns\n3000         -------\n3001         sorted: DataArray\n3002             A new dataarray where all the specified dims are sorted by dim\n3003             labels.\n3004 \n3005         Examples\n3006         --------\n3007 \n3008         >>> da = xr.DataArray(\n3009         ...     np.random.rand(5),\n3010         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n3011         ...     dims=\"time\",\n3012         ... )\n3013         >>> da\n3014         <xarray.DataArray (time: 5)>\n3015         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n3016         Coordinates:\n3017           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n3018 \n3019         >>> da.sortby(da)\n3020         <xarray.DataArray (time: 5)>\n3021         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n3022         Coordinates:\n3023           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n3024         \"\"\"\n3025         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n3026         return self._from_temp_dataset(ds)\n3027 \n3028     def quantile(\n3029         self,\n3030         q: Any,\n3031         dim: Union[Hashable, Sequence[Hashable], None] = None,\n3032         interpolation: str = \"linear\",\n3033         keep_attrs: bool = None,\n3034         skipna: bool = True,\n3035     ) -> \"DataArray\":\n3036         \"\"\"Compute the qth quantile of the data along the specified dimension.\n3037 \n3038         Returns the qth quantiles(s) of the array elements.\n3039 \n3040         Parameters\n3041         ----------\n3042         q : float in range of [0,1] or array-like of floats\n3043             Quantile to compute, which must be between 0 and 1 inclusive.\n3044         dim : hashable or sequence of hashable, optional\n3045             Dimension(s) over which to apply quantile.\n3046         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n3047             This optional parameter specifies the interpolation method to\n3048             use when the desired quantile lies between two data points\n3049             ``i < j``:\n3050 \n3051                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n3052                   the fractional part of the index surrounded by ``i`` and\n3053                   ``j``.\n3054                 - lower: ``i``.\n3055                 - higher: ``j``.\n3056                 - nearest: ``i`` or ``j``, whichever is nearest.\n3057                 - midpoint: ``(i + j) / 2``.\n3058         keep_attrs : bool, optional\n3059             If True, the dataset's attributes (`attrs`) will be copied from\n3060             the original object to the new one.  If False (default), the new\n3061             object will be returned without attributes.\n3062         skipna : bool, optional\n3063             Whether to skip missing values when aggregating.\n3064 \n3065         Returns\n3066         -------\n3067         quantiles : DataArray\n3068             If `q` is a single quantile, then the result\n3069             is a scalar. If multiple percentiles are given, first axis of\n3070             the result corresponds to the quantile and a quantile dimension\n3071             is added to the return array. The other dimensions are the\n3072             dimensions that remain after the reduction of the array.\n3073 \n3074         See Also\n3075         --------\n3076         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n3077 \n3078         Examples\n3079         --------\n3080 \n3081         >>> da = xr.DataArray(\n3082         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n3083         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n3084         ...     dims=(\"x\", \"y\"),\n3085         ... )\n3086         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3087         <xarray.DataArray ()>\n3088         array(0.7)\n3089         Coordinates:\n3090             quantile  float64 0.0\n3091         >>> da.quantile(0, dim=\"x\")\n3092         <xarray.DataArray (y: 4)>\n3093         array([0.7, 4.2, 2.6, 1.5])\n3094         Coordinates:\n3095           * y         (y) float64 1.0 1.5 2.0 2.5\n3096             quantile  float64 0.0\n3097         >>> da.quantile([0, 0.5, 1])\n3098         <xarray.DataArray (quantile: 3)>\n3099         array([0.7, 3.4, 9.4])\n3100         Coordinates:\n3101           * quantile  (quantile) float64 0.0 0.5 1.0\n3102         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3103         <xarray.DataArray (quantile: 3, y: 4)>\n3104         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3105                [3.6 , 5.75, 6.  , 1.7 ],\n3106                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3107         Coordinates:\n3108           * y         (y) float64 1.0 1.5 2.0 2.5\n3109           * quantile  (quantile) float64 0.0 0.5 1.0\n3110         \"\"\"\n3111 \n3112         ds = self._to_temp_dataset().quantile(\n3113             q,\n3114             dim=dim,\n3115             keep_attrs=keep_attrs,\n3116             interpolation=interpolation,\n3117             skipna=skipna,\n3118         )\n3119         return self._from_temp_dataset(ds)\n3120 \n3121     def rank(\n3122         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n3123     ) -> \"DataArray\":\n3124         \"\"\"Ranks the data.\n3125 \n3126         Equal values are assigned a rank that is the average of the ranks that\n3127         would have been otherwise assigned to all of the values within that\n3128         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n3129 \n3130         NaNs in the input array are returned as NaNs.\n3131 \n3132         The `bottleneck` library is required.\n3133 \n3134         Parameters\n3135         ----------\n3136         dim : hashable\n3137             Dimension over which to compute rank.\n3138         pct : bool, optional\n3139             If True, compute percentage ranks, otherwise compute integer ranks.\n3140         keep_attrs : bool, optional\n3141             If True, the dataset's attributes (`attrs`) will be copied from\n3142             the original object to the new one.  If False (default), the new\n3143             object will be returned without attributes.\n3144 \n3145         Returns\n3146         -------\n3147         ranked : DataArray\n3148             DataArray with the same coordinates and dtype 'float64'.\n3149 \n3150         Examples\n3151         --------\n3152 \n3153         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3154         >>> arr.rank(\"x\")\n3155         <xarray.DataArray (x: 3)>\n3156         array([ 1.,   2.,   3.])\n3157         Dimensions without coordinates: x\n3158         \"\"\"\n3159 \n3160         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n3161         return self._from_temp_dataset(ds)\n3162 \n3163     def differentiate(\n3164         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n3165     ) -> \"DataArray\":\n3166         \"\"\" Differentiate the array with the second order accurate central\n3167         differences.\n3168 \n3169         .. note::\n3170             This feature is limited to simple cartesian geometry, i.e. coord\n3171             must be one dimensional.\n3172 \n3173         Parameters\n3174         ----------\n3175         coord: hashable\n3176             The coordinate to be used to compute the gradient.\n3177         edge_order: 1 or 2. Default 1\n3178             N-th order accurate differences at the boundaries.\n3179         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n3180             'us', 'ns', 'ps', 'fs', 'as'}\n3181             Unit to compute gradient. Only valid for datetime coordinate.\n3182 \n3183         Returns\n3184         -------\n3185         differentiated: DataArray\n3186 \n3187         See also\n3188         --------\n3189         numpy.gradient: corresponding numpy function\n3190 \n3191         Examples\n3192         --------\n3193 \n3194         >>> da = xr.DataArray(\n3195         ...     np.arange(12).reshape(4, 3),\n3196         ...     dims=[\"x\", \"y\"],\n3197         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3198         ... )\n3199         >>> da\n3200         <xarray.DataArray (x: 4, y: 3)>\n3201         array([[ 0,  1,  2],\n3202                [ 3,  4,  5],\n3203                [ 6,  7,  8],\n3204                [ 9, 10, 11]])\n3205         Coordinates:\n3206           * x        (x) float64 0.0 0.1 1.1 1.2\n3207         Dimensions without coordinates: y\n3208         >>>\n3209         >>> da.differentiate(\"x\")\n3210         <xarray.DataArray (x: 4, y: 3)>\n3211         array([[30.      , 30.      , 30.      ],\n3212                [27.545455, 27.545455, 27.545455],\n3213                [27.545455, 27.545455, 27.545455],\n3214                [30.      , 30.      , 30.      ]])\n3215         Coordinates:\n3216           * x        (x) float64 0.0 0.1 1.1 1.2\n3217         Dimensions without coordinates: y\n3218         \"\"\"\n3219         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n3220         return self._from_temp_dataset(ds)\n3221 \n3222     def integrate(\n3223         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n3224     ) -> \"DataArray\":\n3225         \"\"\" integrate the array with the trapezoidal rule.\n3226 \n3227         .. note::\n3228             This feature is limited to simple cartesian geometry, i.e. dim\n3229             must be one dimensional.\n3230 \n3231         Parameters\n3232         ----------\n3233         dim: hashable, or a sequence of hashable\n3234             Coordinate(s) used for the integration.\n3235         datetime_unit: str, optional\n3236             Can be used to specify the unit if datetime coordinate is used.\n3237             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',\n3238             'fs', 'as'}\n3239 \n3240         Returns\n3241         -------\n3242         integrated: DataArray\n3243 \n3244         See also\n3245         --------\n3246         numpy.trapz: corresponding numpy function\n3247 \n3248         Examples\n3249         --------\n3250 \n3251         >>> da = xr.DataArray(\n3252         ...     np.arange(12).reshape(4, 3),\n3253         ...     dims=[\"x\", \"y\"],\n3254         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3255         ... )\n3256         >>> da\n3257         <xarray.DataArray (x: 4, y: 3)>\n3258         array([[ 0,  1,  2],\n3259                [ 3,  4,  5],\n3260                [ 6,  7,  8],\n3261                [ 9, 10, 11]])\n3262         Coordinates:\n3263           * x        (x) float64 0.0 0.1 1.1 1.2\n3264         Dimensions without coordinates: y\n3265         >>>\n3266         >>> da.integrate(\"x\")\n3267         <xarray.DataArray (y: 3)>\n3268         array([5.4, 6.6, 7.8])\n3269         Dimensions without coordinates: y\n3270         \"\"\"\n3271         ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n3272         return self._from_temp_dataset(ds)\n3273 \n3274     def unify_chunks(self) -> \"DataArray\":\n3275         \"\"\" Unify chunk size along all chunked dimensions of this DataArray.\n3276 \n3277         Returns\n3278         -------\n3279 \n3280         DataArray with consistent chunk sizes for all dask-array variables\n3281 \n3282         See Also\n3283         --------\n3284 \n3285         dask.array.core.unify_chunks\n3286         \"\"\"\n3287         ds = self._to_temp_dataset().unify_chunks()\n3288         return self._from_temp_dataset(ds)\n3289 \n3290     def map_blocks(\n3291         self,\n3292         func: \"Callable[..., T_DSorDA]\",\n3293         args: Sequence[Any] = (),\n3294         kwargs: Mapping[str, Any] = None,\n3295         template: Union[\"DataArray\", \"Dataset\"] = None,\n3296     ) -> \"T_DSorDA\":\n3297         \"\"\"\n3298         Apply a function to each block of this DataArray.\n3299 \n3300         .. warning::\n3301             This method is experimental and its signature may change.\n3302 \n3303         Parameters\n3304         ----------\n3305         func: callable\n3306             User-provided function that accepts a DataArray as its first\n3307             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n3308             corresponding to one chunk along each chunked dimension. ``func`` will be\n3309             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n3310 \n3311             This function must return either a single DataArray or a single Dataset.\n3312 \n3313             This function cannot add a new chunked dimension.\n3314 \n3315         obj: DataArray, Dataset\n3316             Passed to the function as its first argument, one block at a time.\n3317         args: Sequence\n3318             Passed to func after unpacking and subsetting any xarray objects by blocks.\n3319             xarray objects in args must be aligned with obj, otherwise an error is raised.\n3320         kwargs: Mapping\n3321             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n3322             subset to blocks. Passing dask collections in kwargs is not allowed.\n3323         template: (optional) DataArray, Dataset\n3324             xarray object representing the final result after compute is called. If not provided,\n3325             the function will be first run on mocked-up data, that looks like ``obj`` but\n3326             has sizes 0, to determine properties of the returned object such as dtype,\n3327             variable names, attributes, new dimensions and new indexes (if any).\n3328             ``template`` must be provided if the function changes the size of existing dimensions.\n3329             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n3330             ``attrs`` set by ``func`` will be ignored.\n3331 \n3332 \n3333         Returns\n3334         -------\n3335         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n3336         function.\n3337 \n3338         Notes\n3339         -----\n3340         This function is designed for when ``func`` needs to manipulate a whole xarray object\n3341         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n3342         recommended to use ``apply_ufunc``.\n3343 \n3344         If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n3345         equivalent to calling ``func(obj, *args, **kwargs)``.\n3346 \n3347         See Also\n3348         --------\n3349         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,\n3350         xarray.DataArray.map_blocks\n3351 \n3352         Examples\n3353         --------\n3354 \n3355         Calculate an anomaly from climatology using ``.groupby()``. Using\n3356         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n3357         its indices, and its methods like ``.groupby()``.\n3358 \n3359         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n3360         ...     gb = da.groupby(groupby_type)\n3361         ...     clim = gb.mean(dim=\"time\")\n3362         ...     return gb - clim\n3363         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n3364         >>> np.random.seed(123)\n3365         >>> array = xr.DataArray(\n3366         ...     np.random.rand(len(time)), dims=\"time\", coords=[time]\n3367         ... ).chunk()\n3368         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n3369         <xarray.DataArray (time: 24)>\n3370         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n3371                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n3372                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n3373                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n3374                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n3375         Coordinates:\n3376           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n3377 \n3378         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n3379         to the function being applied in ``xr.map_blocks()``:\n3380 \n3381         >>> array.map_blocks(\n3382         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array,\n3383         ... )\n3384         <xarray.DataArray (time: 24)>\n3385         array([ 0.15361741, -0.25671244, -0.31600032,  0.008463  ,  0.1766172 ,\n3386                -0.11974531,  0.43791243,  0.14197797, -0.06191987, -0.15073425,\n3387                -0.19967375,  0.18619794, -0.05100474, -0.42989909, -0.09153273,\n3388                 0.24841842, -0.30708526, -0.31412523,  0.04197439,  0.0422506 ,\n3389                 0.14482397,  0.35985481,  0.23487834,  0.12144652])\n3390         Coordinates:\n3391             * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n3392         \"\"\"\n3393         from .parallel import map_blocks\n3394 \n3395         return map_blocks(func, self, args, kwargs, template)\n3396 \n3397     def polyfit(\n3398         self,\n3399         dim: Hashable,\n3400         deg: int,\n3401         skipna: bool = None,\n3402         rcond: float = None,\n3403         w: Union[Hashable, Any] = None,\n3404         full: bool = False,\n3405         cov: bool = False,\n3406     ):\n3407         \"\"\"\n3408         Least squares polynomial fit.\n3409 \n3410         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n3411         invalid values when `skipna = True`.\n3412 \n3413         Parameters\n3414         ----------\n3415         dim : hashable\n3416             Coordinate along which to fit the polynomials.\n3417         deg : int\n3418             Degree of the fitting polynomial.\n3419         skipna : bool, optional\n3420             If True, removes all invalid values before fitting each 1D slices of the array.\n3421             Default is True if data is stored in a dask.array or if there is any\n3422             invalid values, False otherwise.\n3423         rcond : float, optional\n3424             Relative condition number to the fit.\n3425         w : Union[Hashable, Any], optional\n3426             Weights to apply to the y-coordinate of the sample points.\n3427             Can be an array-like object or the name of a coordinate in the dataset.\n3428         full : bool, optional\n3429             Whether to return the residuals, matrix rank and singular values in addition\n3430             to the coefficients.\n3431         cov : Union[bool, str], optional\n3432             Whether to return to the covariance matrix in addition to the coefficients.\n3433             The matrix is not scaled if `cov='unscaled'`.\n3434 \n3435         Returns\n3436         -------\n3437         polyfit_results : Dataset\n3438             A single dataset which contains:\n3439 \n3440             polyfit_coefficients\n3441                 The coefficients of the best fit.\n3442             polyfit_residuals\n3443                 The residuals of the least-square computation (only included if `full=True`)\n3444             [dim]_matrix_rank\n3445                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3446             [dim]_singular_value\n3447                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3448             polyfit_covariance\n3449                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n3450 \n3451         See also\n3452         --------\n3453         numpy.polyfit\n3454         \"\"\"\n3455         return self._to_temp_dataset().polyfit(\n3456             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n3457         )\n3458 \n3459     def pad(\n3460         self,\n3461         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n3462         mode: str = \"constant\",\n3463         stat_length: Union[\n3464             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3465         ] = None,\n3466         constant_values: Union[\n3467             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3468         ] = None,\n3469         end_values: Union[\n3470             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3471         ] = None,\n3472         reflect_type: str = None,\n3473         **pad_width_kwargs: Any,\n3474     ) -> \"DataArray\":\n3475         \"\"\"Pad this array along one or more dimensions.\n3476 \n3477         .. warning::\n3478             This function is experimental and its behaviour is likely to change\n3479             especially regarding padding of dimension coordinates (or IndexVariables).\n3480 \n3481         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n3482         coordinates will be padded with the same mode, otherwise coordinates\n3483         are padded using the \"constant\" mode with fill_value dtypes.NA.\n3484 \n3485         Parameters\n3486         ----------\n3487         pad_width : Mapping with the form of {dim: (pad_before, pad_after)}\n3488             Number of values padded along each dimension.\n3489             {dim: pad} is a shortcut for pad_before = pad_after = pad\n3490         mode : str\n3491             One of the following string values (taken from numpy docs)\n3492 \n3493             'constant' (default)\n3494                 Pads with a constant value.\n3495             'edge'\n3496                 Pads with the edge values of array.\n3497             'linear_ramp'\n3498                 Pads with the linear ramp between end_value and the\n3499                 array edge value.\n3500             'maximum'\n3501                 Pads with the maximum value of all or part of the\n3502                 vector along each axis.\n3503             'mean'\n3504                 Pads with the mean value of all or part of the\n3505                 vector along each axis.\n3506             'median'\n3507                 Pads with the median value of all or part of the\n3508                 vector along each axis.\n3509             'minimum'\n3510                 Pads with the minimum value of all or part of the\n3511                 vector along each axis.\n3512             'reflect'\n3513                 Pads with the reflection of the vector mirrored on\n3514                 the first and last values of the vector along each\n3515                 axis.\n3516             'symmetric'\n3517                 Pads with the reflection of the vector mirrored\n3518                 along the edge of the array.\n3519             'wrap'\n3520                 Pads with the wrap of the vector along the axis.\n3521                 The first values are used to pad the end and the\n3522                 end values are used to pad the beginning.\n3523         stat_length : int, tuple or mapping of the form {dim: tuple}\n3524             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n3525             values at edge of each axis used to calculate the statistic value.\n3526             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n3527             statistic lengths along each dimension.\n3528             ((before, after),) yields same before and after statistic lengths\n3529             for each dimension.\n3530             (stat_length,) or int is a shortcut for before = after = statistic\n3531             length for all axes.\n3532             Default is ``None``, to use the entire axis.\n3533         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n3534             Used in 'constant'.  The values to set the padded values for each\n3535             axis.\n3536             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3537             pad constants along each dimension.\n3538             ``((before, after),)`` yields same before and after constants for each\n3539             dimension.\n3540             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3541             all dimensions.\n3542             Default is 0.\n3543         end_values : scalar, tuple or mapping of the form {dim: tuple}\n3544             Used in 'linear_ramp'.  The values used for the ending value of the\n3545             linear_ramp and that will form the edge of the padded array.\n3546             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3547             end values along each dimension.\n3548             ``((before, after),)`` yields same before and after end values for each\n3549             axis.\n3550             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3551             all axes.\n3552             Default is 0.\n3553         reflect_type : {'even', 'odd'}, optional\n3554             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n3555             default with an unaltered reflection around the edge value.  For\n3556             the 'odd' style, the extended part of the array is created by\n3557             subtracting the reflected values from two times the edge value.\n3558         **pad_width_kwargs:\n3559             The keyword arguments form of ``pad_width``.\n3560             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n3561 \n3562         Returns\n3563         -------\n3564         padded : DataArray\n3565             DataArray with the padded coordinates and data.\n3566 \n3567         See also\n3568         --------\n3569         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n3570 \n3571         Notes\n3572         -----\n3573         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n3574         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n3575         specify ``constant_values=np.nan``\n3576 \n3577         Examples\n3578         --------\n3579 \n3580         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n3581         >>> arr.pad(x=(1, 2), constant_values=0)\n3582         <xarray.DataArray (x: 6)>\n3583         array([0, 5, 6, 7, 0, 0])\n3584         Coordinates:\n3585           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n3586 \n3587         >>> da = xr.DataArray(\n3588         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n3589         ...     dims=[\"x\", \"y\"],\n3590         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n3591         ... )\n3592         >>> da.pad(x=1)\n3593         <xarray.DataArray (x: 4, y: 4)>\n3594         array([[nan, nan, nan, nan],\n3595                [ 0.,  1.,  2.,  3.],\n3596                [10., 11., 12., 13.],\n3597                [nan, nan, nan, nan]])\n3598         Coordinates:\n3599           * x        (x) float64 nan 0.0 1.0 nan\n3600           * y        (y) int64 10 20 30 40\n3601             z        (x) float64 nan 100.0 200.0 nan\n3602         >>> da.pad(x=1, constant_values=np.nan)\n3603         <xarray.DataArray (x: 4, y: 4)>\n3604         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3605                 -9223372036854775808],\n3606                [                   0,                    1,                    2,\n3607                                    3],\n3608                [                  10,                   11,                   12,\n3609                                   13],\n3610                [-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3611                 -9223372036854775808]])\n3612         Coordinates:\n3613           * x        (x) float64 nan 0.0 1.0 nan\n3614           * y        (y) int64 10 20 30 40\n3615             z        (x) float64 nan 100.0 200.0 nan\n3616         \"\"\"\n3617         ds = self._to_temp_dataset().pad(\n3618             pad_width=pad_width,\n3619             mode=mode,\n3620             stat_length=stat_length,\n3621             constant_values=constant_values,\n3622             end_values=end_values,\n3623             reflect_type=reflect_type,\n3624             **pad_width_kwargs,\n3625         )\n3626         return self._from_temp_dataset(ds)\n3627 \n3628     def idxmin(\n3629         self,\n3630         dim: Hashable = None,\n3631         skipna: bool = None,\n3632         fill_value: Any = dtypes.NA,\n3633         keep_attrs: bool = None,\n3634     ) -> \"DataArray\":\n3635         \"\"\"Return the coordinate label of the minimum value along a dimension.\n3636 \n3637         Returns a new `DataArray` named after the dimension with the values of\n3638         the coordinate labels along that dimension corresponding to minimum\n3639         values along that dimension.\n3640 \n3641         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n3642         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n3643 \n3644         Parameters\n3645         ----------\n3646         dim : str, optional\n3647             Dimension over which to apply `idxmin`.  This is optional for 1D\n3648             arrays, but required for arrays with 2 or more dimensions.\n3649         skipna : bool or None, default None\n3650             If True, skip missing values (as marked by NaN). By default, only\n3651             skips missing values for ``float``, ``complex``, and ``object``\n3652             dtypes; other dtypes either do not have a sentinel missing value\n3653             (``int``) or ``skipna=True`` has not been implemented\n3654             (``datetime64`` or ``timedelta64``).\n3655         fill_value : Any, default NaN\n3656             Value to be filled in case all of the values along a dimension are\n3657             null.  By default this is NaN.  The fill value and result are\n3658             automatically converted to a compatible dtype if possible.\n3659             Ignored if ``skipna`` is False.\n3660         keep_attrs : bool, default False\n3661             If True, the attributes (``attrs``) will be copied from the\n3662             original object to the new one.  If False (default), the new object\n3663             will be returned without attributes.\n3664 \n3665         Returns\n3666         -------\n3667         reduced : DataArray\n3668             New `DataArray` object with `idxmin` applied to its data and the\n3669             indicated dimension removed.\n3670 \n3671         See also\n3672         --------\n3673         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n3674 \n3675         Examples\n3676         --------\n3677 \n3678         >>> array = xr.DataArray(\n3679         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n3680         ... )\n3681         >>> array.min()\n3682         <xarray.DataArray ()>\n3683         array(-2)\n3684         >>> array.argmin()\n3685         <xarray.DataArray ()>\n3686         array(4)\n3687         >>> array.idxmin()\n3688         <xarray.DataArray 'x' ()>\n3689         array('e', dtype='<U1')\n3690 \n3691         >>> array = xr.DataArray(\n3692         ...     [\n3693         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n3694         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3695         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n3696         ...     ],\n3697         ...     dims=[\"y\", \"x\"],\n3698         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n3699         ... )\n3700         >>> array.min(dim=\"x\")\n3701         <xarray.DataArray (y: 3)>\n3702         array([-2., -4.,  1.])\n3703         Coordinates:\n3704           * y        (y) int64 -1 0 1\n3705         >>> array.argmin(dim=\"x\")\n3706         <xarray.DataArray (y: 3)>\n3707         array([4, 0, 2])\n3708         Coordinates:\n3709           * y        (y) int64 -1 0 1\n3710         >>> array.idxmin(dim=\"x\")\n3711         <xarray.DataArray 'x' (y: 3)>\n3712         array([16.,  0.,  4.])\n3713         Coordinates:\n3714           * y        (y) int64 -1 0 1\n3715         \"\"\"\n3716         return computation._calc_idxminmax(\n3717             array=self,\n3718             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n3719             dim=dim,\n3720             skipna=skipna,\n3721             fill_value=fill_value,\n3722             keep_attrs=keep_attrs,\n3723         )\n3724 \n3725     def idxmax(\n3726         self,\n3727         dim: Hashable = None,\n3728         skipna: bool = None,\n3729         fill_value: Any = dtypes.NA,\n3730         keep_attrs: bool = None,\n3731     ) -> \"DataArray\":\n3732         \"\"\"Return the coordinate label of the maximum value along a dimension.\n3733 \n3734         Returns a new `DataArray` named after the dimension with the values of\n3735         the coordinate labels along that dimension corresponding to maximum\n3736         values along that dimension.\n3737 \n3738         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n3739         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n3740 \n3741         Parameters\n3742         ----------\n3743         dim : str, optional\n3744             Dimension over which to apply `idxmax`.  This is optional for 1D\n3745             arrays, but required for arrays with 2 or more dimensions.\n3746         skipna : bool or None, default None\n3747             If True, skip missing values (as marked by NaN). By default, only\n3748             skips missing values for ``float``, ``complex``, and ``object``\n3749             dtypes; other dtypes either do not have a sentinel missing value\n3750             (``int``) or ``skipna=True`` has not been implemented\n3751             (``datetime64`` or ``timedelta64``).\n3752         fill_value : Any, default NaN\n3753             Value to be filled in case all of the values along a dimension are\n3754             null.  By default this is NaN.  The fill value and result are\n3755             automatically converted to a compatible dtype if possible.\n3756             Ignored if ``skipna`` is False.\n3757         keep_attrs : bool, default False\n3758             If True, the attributes (``attrs``) will be copied from the\n3759             original object to the new one.  If False (default), the new object\n3760             will be returned without attributes.\n3761 \n3762         Returns\n3763         -------\n3764         reduced : DataArray\n3765             New `DataArray` object with `idxmax` applied to its data and the\n3766             indicated dimension removed.\n3767 \n3768         See also\n3769         --------\n3770         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n3771 \n3772         Examples\n3773         --------\n3774 \n3775         >>> array = xr.DataArray(\n3776         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n3777         ... )\n3778         >>> array.max()\n3779         <xarray.DataArray ()>\n3780         array(2)\n3781         >>> array.argmax()\n3782         <xarray.DataArray ()>\n3783         array(1)\n3784         >>> array.idxmax()\n3785         <xarray.DataArray 'x' ()>\n3786         array('b', dtype='<U1')\n3787 \n3788         >>> array = xr.DataArray(\n3789         ...     [\n3790         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n3791         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3792         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n3793         ...     ],\n3794         ...     dims=[\"y\", \"x\"],\n3795         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n3796         ... )\n3797         >>> array.max(dim=\"x\")\n3798         <xarray.DataArray (y: 3)>\n3799         array([2., 2., 1.])\n3800         Coordinates:\n3801           * y        (y) int64 -1 0 1\n3802         >>> array.argmax(dim=\"x\")\n3803         <xarray.DataArray (y: 3)>\n3804         array([0, 2, 2])\n3805         Coordinates:\n3806           * y        (y) int64 -1 0 1\n3807         >>> array.idxmax(dim=\"x\")\n3808         <xarray.DataArray 'x' (y: 3)>\n3809         array([0., 4., 4.])\n3810         Coordinates:\n3811           * y        (y) int64 -1 0 1\n3812         \"\"\"\n3813         return computation._calc_idxminmax(\n3814             array=self,\n3815             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n3816             dim=dim,\n3817             skipna=skipna,\n3818             fill_value=fill_value,\n3819             keep_attrs=keep_attrs,\n3820         )\n3821 \n3822     # this needs to be at the end, or mypy will confuse with `str`\n3823     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n3824     str = utils.UncachedAccessor(StringAccessor)\n3825 \n3826 \n3827 # priority most be higher than Variable to properly work with binary ufuncs\n3828 ops.inject_all_ops_and_reduce_methods(DataArray, priority=60)\n",
        "intended_behavior": "Preservation of Existing Functionality: Enhancements or modifications should keep the existing functionality of the methods, ensuring backward compatibility and not altering expected aspects of the representation. The expand/collapse toggle functionality should remain."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 1,
        "end": 2424,
        "class_name": null,
        "method_name": null,
        "code": "1 import copy\n2 import functools\n3 import itertools\n4 import numbers\n5 import warnings\n6 from collections import defaultdict\n7 from datetime import timedelta\n8 from distutils.version import LooseVersion\n9 from typing import Any, Dict, Hashable, Mapping, Tuple, TypeVar, Union\n10 \n11 import numpy as np\n12 import pandas as pd\n13 \n14 import xarray as xr  # only for Dataset and DataArray\n15 \n16 from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\n17 from .indexing import (\n18     BasicIndexer,\n19     OuterIndexer,\n20     PandasIndexAdapter,\n21     VectorizedIndexer,\n22     as_indexable,\n23 )\n24 from .npcompat import IS_NEP18_ACTIVE\n25 from .options import _get_keep_attrs\n26 from .pycompat import dask_array_type, integer_types\n27 from .utils import (\n28     OrderedSet,\n29     _default,\n30     decode_numpy_dict_values,\n31     drop_dims_from_indexers,\n32     either_dict_or_kwargs,\n33     ensure_us_time_resolution,\n34     infix_dims,\n35 )\n36 \n37 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (\n38     indexing.ExplicitlyIndexed,\n39     pd.Index,\n40 ) + dask_array_type\n41 # https://github.com/python/mypy/issues/224\n42 BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\n43 \n44 VariableType = TypeVar(\"VariableType\", bound=\"Variable\")\n45 \"\"\"Type annotation to be used when methods of Variable return self or a copy of self.\n46 When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\n47 output as an instance of the subclass.\n48 \n49 Usage::\n50 \n51    class Variable:\n52        def f(self: VariableType, ...) -> VariableType:\n53            ...\n54 \"\"\"\n55 \n56 \n57 class MissingDimensionsError(ValueError):\n58     \"\"\"Error class used when we can't safely guess a dimension name.\n59     \"\"\"\n60 \n61     # inherits from ValueError for backward compatibility\n62     # TODO: move this to an xarray.exceptions module?\n63 \n64 \n65 def as_variable(obj, name=None) -> \"Union[Variable, IndexVariable]\":\n66     \"\"\"Convert an object into a Variable.\n67 \n68     Parameters\n69     ----------\n70     obj : object\n71         Object to convert into a Variable.\n72 \n73         - If the object is already a Variable, return a shallow copy.\n74         - Otherwise, if the object has 'dims' and 'data' attributes, convert\n75           it into a new Variable.\n76         - If all else fails, attempt to convert the object into a Variable by\n77           unpacking it into the arguments for creating a new Variable.\n78     name : str, optional\n79         If provided:\n80 \n81         - `obj` can be a 1D array, which is assumed to label coordinate values\n82           along a dimension of this given name.\n83         - Variables with name matching one of their dimensions are converted\n84           into `IndexVariable` objects.\n85 \n86     Returns\n87     -------\n88     var : Variable\n89         The newly created variable.\n90 \n91     \"\"\"\n92     from .dataarray import DataArray\n93 \n94     # TODO: consider extending this method to automatically handle Iris and\n95     if isinstance(obj, DataArray):\n96         # extract the primary Variable from DataArrays\n97         obj = obj.variable\n98 \n99     if isinstance(obj, Variable):\n100         obj = obj.copy(deep=False)\n101     elif isinstance(obj, tuple):\n102         try:\n103             obj = Variable(*obj)\n104         except (TypeError, ValueError) as error:\n105             # use .format() instead of % because it handles tuples consistently\n106             raise error.__class__(\n107                 \"Could not convert tuple of form \"\n108                 \"(dims, data[, attrs, encoding]): \"\n109                 \"{} to Variable.\".format(obj)\n110             )\n111     elif utils.is_scalar(obj):\n112         obj = Variable([], obj)\n113     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n114         obj = Variable(obj.name, obj)\n115     elif isinstance(obj, (set, dict)):\n116         raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n117     elif name is not None:\n118         data = as_compatible_data(obj)\n119         if data.ndim != 1:\n120             raise MissingDimensionsError(\n121                 \"cannot set variable %r with %r-dimensional data \"\n122                 \"without explicit dimension names. Pass a tuple of \"\n123                 \"(dims, data) instead.\" % (name, data.ndim)\n124             )\n125         obj = Variable(name, data, fastpath=True)\n126     else:\n127         raise TypeError(\n128             \"unable to convert object into a variable without an \"\n129             \"explicit list of dimensions: %r\" % obj\n130         )\n131 \n132     if name is not None and name in obj.dims:\n133         # convert the Variable into an Index\n134         if obj.ndim != 1:\n135             raise MissingDimensionsError(\n136                 \"%r has more than 1-dimension and the same name as one of its \"\n137                 \"dimensions %r. xarray disallows such variables because they \"\n138                 \"conflict with the coordinates used to label \"\n139                 \"dimensions.\" % (name, obj.dims)\n140             )\n141         obj = obj.to_index_variable()\n142 \n143     return obj\n144 \n145 \n146 def _maybe_wrap_data(data):\n147     \"\"\"\n148     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\n149     they can be indexed properly.\n150 \n151     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\n152     all pass through unmodified.\n153     \"\"\"\n154     if isinstance(data, pd.Index):\n155         return PandasIndexAdapter(data)\n156     return data\n157 \n158 \n159 def _possibly_convert_objects(values):\n160     \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into\n161     datetime64 and timedelta64, according to the pandas convention.\n162     \"\"\"\n163     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n164 \n165 \n166 def as_compatible_data(data, fastpath=False):\n167     \"\"\"Prepare and wrap data to put in a Variable.\n168 \n169     - If data does not have the necessary attributes, convert it to ndarray.\n170     - If data has dtype=datetime64, ensure that it has ns precision. If it's a\n171       pandas.Timestamp, convert it to datetime64.\n172     - If data is already a pandas or xarray object (other than an Index), just\n173       use the values.\n174 \n175     Finally, wrap it up with an adapter if necessary.\n176     \"\"\"\n177     if fastpath and getattr(data, \"ndim\", 0) > 0:\n178         # can't use fastpath (yet) for scalars\n179         return _maybe_wrap_data(data)\n180 \n181     if isinstance(data, Variable):\n182         return data.data\n183 \n184     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n185         return _maybe_wrap_data(data)\n186 \n187     if isinstance(data, tuple):\n188         data = utils.to_0d_object_array(data)\n189 \n190     if isinstance(data, pd.Timestamp):\n191         # TODO: convert, handle datetime objects, too\n192         data = np.datetime64(data.value, \"ns\")\n193 \n194     if isinstance(data, timedelta):\n195         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n196 \n197     # we don't want nested self-described arrays\n198     data = getattr(data, \"values\", data)\n199 \n200     if isinstance(data, np.ma.MaskedArray):\n201         mask = np.ma.getmaskarray(data)\n202         if mask.any():\n203             dtype, fill_value = dtypes.maybe_promote(data.dtype)\n204             data = np.asarray(data, dtype=dtype)\n205             data[mask] = fill_value\n206         else:\n207             data = np.asarray(data)\n208 \n209     if not isinstance(data, np.ndarray):\n210         if hasattr(data, \"__array_function__\"):\n211             if IS_NEP18_ACTIVE:\n212                 return data\n213             else:\n214                 raise TypeError(\n215                     \"Got an NumPy-like array type providing the \"\n216                     \"__array_function__ protocol but NEP18 is not enabled. \"\n217                     \"Check that numpy >= v1.16 and that the environment \"\n218                     'variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to '\n219                     '\"1\"'\n220                 )\n221 \n222     # validate whether the data is valid data types\n223     data = np.asarray(data)\n224 \n225     if isinstance(data, np.ndarray):\n226         if data.dtype.kind == \"O\":\n227             data = _possibly_convert_objects(data)\n228         elif data.dtype.kind == \"M\":\n229             data = np.asarray(data, \"datetime64[ns]\")\n230         elif data.dtype.kind == \"m\":\n231             data = np.asarray(data, \"timedelta64[ns]\")\n232 \n233     return _maybe_wrap_data(data)\n234 \n235 \n236 def _as_array_or_item(data):\n237     \"\"\"Return the given values as a numpy array, or as an individual item if\n238     it's a 0d datetime64 or timedelta64 array.\n239 \n240     Importantly, this function does not copy data if it is already an ndarray -\n241     otherwise, it will not be possible to update Variable values in place.\n242 \n243     This function mostly exists because 0-dimensional ndarrays with\n244     dtype=datetime64 are broken :(\n245     https://github.com/numpy/numpy/issues/4337\n246     https://github.com/numpy/numpy/issues/7619\n247 \n248     TODO: remove this (replace with np.asarray) once these issues are fixed\n249     \"\"\"\n250     data = np.asarray(data)\n251     if data.ndim == 0:\n252         if data.dtype.kind == \"M\":\n253             data = np.datetime64(data, \"ns\")\n254         elif data.dtype.kind == \"m\":\n255             data = np.timedelta64(data, \"ns\")\n256     return data\n257 \n258 \n259 class Variable(\n260     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n261 ):\n262     \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n263     which describe a single Array. A single Variable object is not fully\n264     described outside the context of its parent Dataset (if you want such a\n265     fully described object, use a DataArray instead).\n266 \n267     The main functional difference between Variables and numpy arrays is that\n268     numerical operations on Variables implement array broadcasting by dimension\n269     name. For example, adding an Variable with dimensions `('time',)` to\n270     another Variable with dimensions `('space',)` results in a new Variable\n271     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n272     like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n273     instead of an \"axis\".\n274 \n275     Variables are light-weight objects used as the building block for datasets.\n276     They are more primitive objects, so operations with them provide marginally\n277     higher performance than using DataArrays. However, manipulating data in the\n278     form of a Dataset or DataArray should almost always be preferred, because\n279     they can use more complete metadata in context of coordinate labels.\n280     \"\"\"\n281 \n282     __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n283 \n284     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n285         \"\"\"\n286         Parameters\n287         ----------\n288         dims : str or sequence of str\n289             Name(s) of the the data dimension(s). Must be either a string (only\n290             for 1D data) or a sequence of strings with length equal to the\n291             number of dimensions.\n292         data : array_like\n293             Data array which supports numpy-like data access.\n294         attrs : dict_like or None, optional\n295             Attributes to assign to the new variable. If None (default), an\n296             empty attribute dictionary is initialized.\n297         encoding : dict_like or None, optional\n298             Dictionary specifying how to encode this array's data into a\n299             serialized format like netCDF4. Currently used keys (for netCDF)\n300             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n301             Well-behaved code to serialize a Variable should ignore\n302             unrecognized encoding items.\n303         \"\"\"\n304         self._data = as_compatible_data(data, fastpath=fastpath)\n305         self._dims = self._parse_dimensions(dims)\n306         self._attrs = None\n307         self._encoding = None\n308         if attrs is not None:\n309             self.attrs = attrs\n310         if encoding is not None:\n311             self.encoding = encoding\n312 \n313     @property\n314     def dtype(self):\n315         return self._data.dtype\n316 \n317     @property\n318     def shape(self):\n319         return self._data.shape\n320 \n321     @property\n322     def nbytes(self):\n323         return self.size * self.dtype.itemsize\n324 \n325     @property\n326     def _in_memory(self):\n327         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n328             isinstance(self._data, indexing.MemoryCachedArray)\n329             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n330         )\n331 \n332     @property\n333     def data(self):\n334         if hasattr(self._data, \"__array_function__\") or isinstance(\n335             self._data, dask_array_type\n336         ):\n337             return self._data\n338         else:\n339             return self.values\n340 \n341     @data.setter\n342     def data(self, data):\n343         data = as_compatible_data(data)\n344         if data.shape != self.shape:\n345             raise ValueError(\n346                 f\"replacement data must match the Variable's shape. \"\n347                 f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n348             )\n349         self._data = data\n350 \n351     def load(self, **kwargs):\n352         \"\"\"Manually trigger loading of this variable's data from disk or a\n353         remote source into memory and return this variable.\n354 \n355         Normally, it should not be necessary to call this method in user code,\n356         because all xarray functions should either work on deferred data or\n357         load data automatically.\n358 \n359         Parameters\n360         ----------\n361         **kwargs : dict\n362             Additional keyword arguments passed on to ``dask.array.compute``.\n363 \n364         See Also\n365         --------\n366         dask.array.compute\n367         \"\"\"\n368         if isinstance(self._data, dask_array_type):\n369             self._data = as_compatible_data(self._data.compute(**kwargs))\n370         elif not hasattr(self._data, \"__array_function__\"):\n371             self._data = np.asarray(self._data)\n372         return self\n373 \n374     def compute(self, **kwargs):\n375         \"\"\"Manually trigger loading of this variable's data from disk or a\n376         remote source into memory and return a new variable. The original is\n377         left unaltered.\n378 \n379         Normally, it should not be necessary to call this method in user code,\n380         because all xarray functions should either work on deferred data or\n381         load data automatically.\n382 \n383         Parameters\n384         ----------\n385         **kwargs : dict\n386             Additional keyword arguments passed on to ``dask.array.compute``.\n387 \n388         See Also\n389         --------\n390         dask.array.compute\n391         \"\"\"\n392         new = self.copy(deep=False)\n393         return new.load(**kwargs)\n394 \n395     def __dask_tokenize__(self):\n396         # Use v.data, instead of v._data, in order to cope with the wrappers\n397         # around NetCDF and the like\n398         from dask.base import normalize_token\n399 \n400         return normalize_token((type(self), self._dims, self.data, self._attrs))\n401 \n402     def __dask_graph__(self):\n403         if isinstance(self._data, dask_array_type):\n404             return self._data.__dask_graph__()\n405         else:\n406             return None\n407 \n408     def __dask_keys__(self):\n409         return self._data.__dask_keys__()\n410 \n411     def __dask_layers__(self):\n412         return self._data.__dask_layers__()\n413 \n414     @property\n415     def __dask_optimize__(self):\n416         return self._data.__dask_optimize__\n417 \n418     @property\n419     def __dask_scheduler__(self):\n420         return self._data.__dask_scheduler__\n421 \n422     def __dask_postcompute__(self):\n423         array_func, array_args = self._data.__dask_postcompute__()\n424         return (\n425             self._dask_finalize,\n426             (array_func, array_args, self._dims, self._attrs, self._encoding),\n427         )\n428 \n429     def __dask_postpersist__(self):\n430         array_func, array_args = self._data.__dask_postpersist__()\n431         return (\n432             self._dask_finalize,\n433             (array_func, array_args, self._dims, self._attrs, self._encoding),\n434         )\n435 \n436     @staticmethod\n437     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n438         if isinstance(results, dict):  # persist case\n439             name = array_args[0]\n440             results = {k: v for k, v in results.items() if k[0] == name}\n441         data = array_func(results, *array_args)\n442         return Variable(dims, data, attrs=attrs, encoding=encoding)\n443 \n444     @property\n445     def values(self):\n446         \"\"\"The variable's data as a numpy.ndarray\"\"\"\n447         return _as_array_or_item(self._data)\n448 \n449     @values.setter\n450     def values(self, values):\n451         self.data = values\n452 \n453     def to_base_variable(self):\n454         \"\"\"Return this variable as a base xarray.Variable\"\"\"\n455         return Variable(\n456             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n457         )\n458 \n459     to_variable = utils.alias(to_base_variable, \"to_variable\")\n460 \n461     def to_index_variable(self):\n462         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n463         return IndexVariable(\n464             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n465         )\n466 \n467     to_coord = utils.alias(to_index_variable, \"to_coord\")\n468 \n469     def to_index(self):\n470         \"\"\"Convert this variable to a pandas.Index\"\"\"\n471         return self.to_index_variable().to_index()\n472 \n473     def to_dict(self, data=True):\n474         \"\"\"Dictionary representation of variable.\"\"\"\n475         item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n476         if data:\n477             item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n478         else:\n479             item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n480         return item\n481 \n482     @property\n483     def dims(self):\n484         \"\"\"Tuple of dimension names with which this variable is associated.\n485         \"\"\"\n486         return self._dims\n487 \n488     @dims.setter\n489     def dims(self, value):\n490         self._dims = self._parse_dimensions(value)\n491 \n492     def _parse_dimensions(self, dims):\n493         if isinstance(dims, str):\n494             dims = (dims,)\n495         dims = tuple(dims)\n496         if len(dims) != self.ndim:\n497             raise ValueError(\n498                 \"dimensions %s must have the same length as the \"\n499                 \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n500             )\n501         return dims\n502 \n503     def _item_key_to_tuple(self, key):\n504         if utils.is_dict_like(key):\n505             return tuple(key.get(dim, slice(None)) for dim in self.dims)\n506         else:\n507             return key\n508 \n509     def _broadcast_indexes(self, key):\n510         \"\"\"Prepare an indexing key for an indexing operation.\n511 \n512         Parameters\n513         -----------\n514         key: int, slice, array, dict or tuple of integer, slices and arrays\n515             Any valid input for indexing.\n516 \n517         Returns\n518         -------\n519         dims: tuple\n520             Dimension of the resultant variable.\n521         indexers: IndexingTuple subclass\n522             Tuple of integer, array-like, or slices to use when indexing\n523             self._data. The type of this argument indicates the type of\n524             indexing to perform, either basic, outer or vectorized.\n525         new_order : Optional[Sequence[int]]\n526             Optional reordering to do on the result of indexing. If not None,\n527             the first len(new_order) indexing should be moved to these\n528             positions.\n529         \"\"\"\n530         key = self._item_key_to_tuple(key)  # key is a tuple\n531         # key is a tuple of full size\n532         key = indexing.expanded_indexer(key, self.ndim)\n533         # Convert a scalar Variable to an integer\n534         key = tuple(\n535             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n536         )\n537         # Convert a 0d-array to an integer\n538         key = tuple(\n539             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n540         )\n541 \n542         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n543             return self._broadcast_indexes_basic(key)\n544 \n545         self._validate_indexers(key)\n546         # Detect it can be mapped as an outer indexer\n547         # If all key is unlabeled, or\n548         # key can be mapped as an OuterIndexer.\n549         if all(not isinstance(k, Variable) for k in key):\n550             return self._broadcast_indexes_outer(key)\n551 \n552         # If all key is 1-dimensional and there are no duplicate labels,\n553         # key can be mapped as an OuterIndexer.\n554         dims = []\n555         for k, d in zip(key, self.dims):\n556             if isinstance(k, Variable):\n557                 if len(k.dims) > 1:\n558                     return self._broadcast_indexes_vectorized(key)\n559                 dims.append(k.dims[0])\n560             elif not isinstance(k, integer_types):\n561                 dims.append(d)\n562         if len(set(dims)) == len(dims):\n563             return self._broadcast_indexes_outer(key)\n564 \n565         return self._broadcast_indexes_vectorized(key)\n566 \n567     def _broadcast_indexes_basic(self, key):\n568         dims = tuple(\n569             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n570         )\n571         return dims, BasicIndexer(key), None\n572 \n573     def _validate_indexers(self, key):\n574         \"\"\" Make sanity checks \"\"\"\n575         for dim, k in zip(self.dims, key):\n576             if isinstance(k, BASIC_INDEXING_TYPES):\n577                 pass\n578             else:\n579                 if not isinstance(k, Variable):\n580                     k = np.asarray(k)\n581                     if k.ndim > 1:\n582                         raise IndexError(\n583                             \"Unlabeled multi-dimensional array cannot be \"\n584                             \"used for indexing: {}\".format(k)\n585                         )\n586                 if k.dtype.kind == \"b\":\n587                     if self.shape[self.get_axis_num(dim)] != len(k):\n588                         raise IndexError(\n589                             \"Boolean array size {:d} is used to index array \"\n590                             \"with shape {:s}.\".format(len(k), str(self.shape))\n591                         )\n592                     if k.ndim > 1:\n593                         raise IndexError(\n594                             \"{}-dimensional boolean indexing is \"\n595                             \"not supported. \".format(k.ndim)\n596                         )\n597                     if getattr(k, \"dims\", (dim,)) != (dim,):\n598                         raise IndexError(\n599                             \"Boolean indexer should be unlabeled or on the \"\n600                             \"same dimension to the indexed array. Indexer is \"\n601                             \"on {:s} but the target dimension is {:s}.\".format(\n602                                 str(k.dims), dim\n603                             )\n604                         )\n605 \n606     def _broadcast_indexes_outer(self, key):\n607         dims = tuple(\n608             k.dims[0] if isinstance(k, Variable) else dim\n609             for k, dim in zip(key, self.dims)\n610             if not isinstance(k, integer_types)\n611         )\n612 \n613         new_key = []\n614         for k in key:\n615             if isinstance(k, Variable):\n616                 k = k.data\n617             if not isinstance(k, BASIC_INDEXING_TYPES):\n618                 k = np.asarray(k)\n619                 if k.size == 0:\n620                     # Slice by empty list; numpy could not infer the dtype\n621                     k = k.astype(int)\n622                 elif k.dtype.kind == \"b\":\n623                     (k,) = np.nonzero(k)\n624             new_key.append(k)\n625 \n626         return dims, OuterIndexer(tuple(new_key)), None\n627 \n628     def _nonzero(self):\n629         \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n630         # TODO we should replace dask's native nonzero\n631         # after https://github.com/dask/dask/issues/1076 is implemented.\n632         nonzeros = np.nonzero(self.data)\n633         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n634 \n635     def _broadcast_indexes_vectorized(self, key):\n636         variables = []\n637         out_dims_set = OrderedSet()\n638         for dim, value in zip(self.dims, key):\n639             if isinstance(value, slice):\n640                 out_dims_set.add(dim)\n641             else:\n642                 variable = (\n643                     value\n644                     if isinstance(value, Variable)\n645                     else as_variable(value, name=dim)\n646                 )\n647                 if variable.dtype.kind == \"b\":  # boolean indexing case\n648                     (variable,) = variable._nonzero()\n649 \n650                 variables.append(variable)\n651                 out_dims_set.update(variable.dims)\n652 \n653         variable_dims = set()\n654         for variable in variables:\n655             variable_dims.update(variable.dims)\n656 \n657         slices = []\n658         for i, (dim, value) in enumerate(zip(self.dims, key)):\n659             if isinstance(value, slice):\n660                 if dim in variable_dims:\n661                     # We only convert slice objects to variables if they share\n662                     # a dimension with at least one other variable. Otherwise,\n663                     # we can equivalently leave them as slices aknd transpose\n664                     # the result. This is significantly faster/more efficient\n665                     # for most array backends.\n666                     values = np.arange(*value.indices(self.sizes[dim]))\n667                     variables.insert(i - len(slices), Variable((dim,), values))\n668                 else:\n669                     slices.append((i, value))\n670 \n671         try:\n672             variables = _broadcast_compat_variables(*variables)\n673         except ValueError:\n674             raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n675 \n676         out_key = [variable.data for variable in variables]\n677         out_dims = tuple(out_dims_set)\n678         slice_positions = set()\n679         for i, value in slices:\n680             out_key.insert(i, value)\n681             new_position = out_dims.index(self.dims[i])\n682             slice_positions.add(new_position)\n683 \n684         if slice_positions:\n685             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n686         else:\n687             new_order = None\n688 \n689         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n690 \n691     def __getitem__(self: VariableType, key) -> VariableType:\n692         \"\"\"Return a new Variable object whose contents are consistent with\n693         getting the provided key from the underlying data.\n694 \n695         NB. __getitem__ and __setitem__ implement xarray-style indexing,\n696         where if keys are unlabeled arrays, we index the array orthogonally\n697         with them. If keys are labeled array (such as Variables), they are\n698         broadcasted with our usual scheme and then the array is indexed with\n699         the broadcasted key, like numpy's fancy indexing.\n700 \n701         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n702         array `x.values` directly.\n703         \"\"\"\n704         dims, indexer, new_order = self._broadcast_indexes(key)\n705         data = as_indexable(self._data)[indexer]\n706         if new_order:\n707             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n708         return self._finalize_indexing_result(dims, data)\n709 \n710     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n711         \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\n712         \"\"\"\n713         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n714 \n715     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n716         \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n717         # TODO(shoyer): expose this method in public API somewhere (isel?) and\n718         # use it for reindex.\n719         # TODO(shoyer): add a sanity check that all other integers are\n720         # non-negative\n721         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n722         # that is actually indexed rather than mapping it to the last value\n723         # along each axis.\n724 \n725         if fill_value is dtypes.NA:\n726             fill_value = dtypes.get_fill_value(self.dtype)\n727 \n728         dims, indexer, new_order = self._broadcast_indexes(key)\n729 \n730         if self.size:\n731             if isinstance(self._data, dask_array_type):\n732                 # dask's indexing is faster this way; also vindex does not\n733                 # support negative indices yet:\n734                 # https://github.com/dask/dask/pull/2967\n735                 actual_indexer = indexing.posify_mask_indexer(indexer)\n736             else:\n737                 actual_indexer = indexer\n738 \n739             data = as_indexable(self._data)[actual_indexer]\n740             mask = indexing.create_mask(indexer, self.shape, data)\n741             # we need to invert the mask in order to pass data first. This helps\n742             # pint to choose the correct unit\n743             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n744             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n745         else:\n746             # array cannot be indexed along dimensions of size 0, so just\n747             # build the mask directly instead.\n748             mask = indexing.create_mask(indexer, self.shape)\n749             data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n750 \n751         if new_order:\n752             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n753         return self._finalize_indexing_result(dims, data)\n754 \n755     def __setitem__(self, key, value):\n756         \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n757         orthogonal indexing.\n758 \n759         See __getitem__ for more details.\n760         \"\"\"\n761         dims, index_tuple, new_order = self._broadcast_indexes(key)\n762 \n763         if not isinstance(value, Variable):\n764             value = as_compatible_data(value)\n765             if value.ndim > len(dims):\n766                 raise ValueError(\n767                     \"shape mismatch: value array of shape %s could not be \"\n768                     \"broadcast to indexing result with %s dimensions\"\n769                     % (value.shape, len(dims))\n770                 )\n771             if value.ndim == 0:\n772                 value = Variable((), value)\n773             else:\n774                 value = Variable(dims[-value.ndim :], value)\n775         # broadcast to become assignable\n776         value = value.set_dims(dims).data\n777 \n778         if new_order:\n779             value = duck_array_ops.asarray(value)\n780             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n781             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n782 \n783         indexable = as_indexable(self._data)\n784         indexable[index_tuple] = value\n785 \n786     @property\n787     def attrs(self) -> Dict[Hashable, Any]:\n788         \"\"\"Dictionary of local attributes on this variable.\n789         \"\"\"\n790         if self._attrs is None:\n791             self._attrs = {}\n792         return self._attrs\n793 \n794     @attrs.setter\n795     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n796         self._attrs = dict(value)\n797 \n798     @property\n799     def encoding(self):\n800         \"\"\"Dictionary of encodings on this variable.\n801         \"\"\"\n802         if self._encoding is None:\n803             self._encoding = {}\n804         return self._encoding\n805 \n806     @encoding.setter\n807     def encoding(self, value):\n808         try:\n809             self._encoding = dict(value)\n810         except ValueError:\n811             raise ValueError(\"encoding must be castable to a dictionary\")\n812 \n813     def copy(self, deep=True, data=None):\n814         \"\"\"Returns a copy of this object.\n815 \n816         If `deep=True`, the data array is loaded into memory and copied onto\n817         the new object. Dimensions, attributes and encodings are always copied.\n818 \n819         Use `data` to create a new object with the same structure as\n820         original but entirely new data.\n821 \n822         Parameters\n823         ----------\n824         deep : bool, optional\n825             Whether the data array is loaded into memory and copied onto\n826             the new object. Default is True.\n827         data : array_like, optional\n828             Data to use in the new object. Must have same shape as original.\n829             When `data` is used, `deep` is ignored.\n830 \n831         Returns\n832         -------\n833         object : Variable\n834             New object with dimensions, attributes, encodings, and optionally\n835             data copied from original.\n836 \n837         Examples\n838         --------\n839 \n840         Shallow copy versus deep copy\n841 \n842         >>> var = xr.Variable(data=[1, 2, 3], dims=\"x\")\n843         >>> var.copy()\n844         <xarray.Variable (x: 3)>\n845         array([1, 2, 3])\n846         >>> var_0 = var.copy(deep=False)\n847         >>> var_0[0] = 7\n848         >>> var_0\n849         <xarray.Variable (x: 3)>\n850         array([7, 2, 3])\n851         >>> var\n852         <xarray.Variable (x: 3)>\n853         array([7, 2, 3])\n854 \n855         Changing the data using the ``data`` argument maintains the\n856         structure of the original object, but with the new data. Original\n857         object is unaffected.\n858 \n859         >>> var.copy(data=[0.1, 0.2, 0.3])\n860         <xarray.Variable (x: 3)>\n861         array([ 0.1,  0.2,  0.3])\n862         >>> var\n863         <xarray.Variable (x: 3)>\n864         array([7, 2, 3])\n865 \n866         See Also\n867         --------\n868         pandas.DataFrame.copy\n869         \"\"\"\n870         if data is None:\n871             data = self._data\n872 \n873             if isinstance(data, indexing.MemoryCachedArray):\n874                 # don't share caching between copies\n875                 data = indexing.MemoryCachedArray(data.array)\n876 \n877             if deep:\n878                 if hasattr(data, \"__array_function__\") or isinstance(\n879                     data, dask_array_type\n880                 ):\n881                     data = data.copy()\n882                 elif not isinstance(data, PandasIndexAdapter):\n883                     # pandas.Index is immutable\n884                     data = np.array(data)\n885         else:\n886             data = as_compatible_data(data)\n887             if self.shape != data.shape:\n888                 raise ValueError(\n889                     \"Data shape {} must match shape of object {}\".format(\n890                         data.shape, self.shape\n891                     )\n892                 )\n893 \n894         # note:\n895         # dims is already an immutable tuple\n896         # attributes and encoding will be copied when the new Array is created\n897         return self._replace(data=data)\n898 \n899     def _replace(\n900         self, dims=_default, data=_default, attrs=_default, encoding=_default\n901     ) -> \"Variable\":\n902         if dims is _default:\n903             dims = copy.copy(self._dims)\n904         if data is _default:\n905             data = copy.copy(self.data)\n906         if attrs is _default:\n907             attrs = copy.copy(self._attrs)\n908         if encoding is _default:\n909             encoding = copy.copy(self._encoding)\n910         return type(self)(dims, data, attrs, encoding, fastpath=True)\n911 \n912     def __copy__(self):\n913         return self.copy(deep=False)\n914 \n915     def __deepcopy__(self, memo=None):\n916         # memo does nothing but is required for compatibility with\n917         # copy.deepcopy\n918         return self.copy(deep=True)\n919 \n920     # mutable objects should not be hashable\n921     # https://github.com/python/mypy/issues/4266\n922     __hash__ = None  # type: ignore\n923 \n924     @property\n925     def chunks(self):\n926         \"\"\"Block dimensions for this array's data or None if it's not a dask\n927         array.\n928         \"\"\"\n929         return getattr(self._data, \"chunks\", None)\n930 \n931     _array_counter = itertools.count()\n932 \n933     def chunk(self, chunks=None, name=None, lock=False):\n934         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n935 \n936         If this variable is a non-dask array, it will be converted to dask\n937         array. If it's a dask array, it will be rechunked to the given chunk\n938         sizes.\n939 \n940         If neither chunks is not provided for one or more dimensions, chunk\n941         sizes along that dimension will not be updated; non-dask arrays will be\n942         converted into dask arrays with a single block.\n943 \n944         Parameters\n945         ----------\n946         chunks : int, tuple or dict, optional\n947             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n948             ``{'x': 5, 'y': 5}``.\n949         name : str, optional\n950             Used to generate the name for this array in the internal dask\n951             graph. Does not need not be unique.\n952         lock : optional\n953             Passed on to :py:func:`dask.array.from_array`, if the array is not\n954             already as dask array.\n955 \n956         Returns\n957         -------\n958         chunked : xarray.Variable\n959         \"\"\"\n960         import dask\n961         import dask.array as da\n962 \n963         if utils.is_dict_like(chunks):\n964             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n965 \n966         if chunks is None:\n967             chunks = self.chunks or self.shape\n968 \n969         data = self._data\n970         if isinstance(data, da.Array):\n971             data = data.rechunk(chunks)\n972         else:\n973             if isinstance(data, indexing.ExplicitlyIndexed):\n974                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n975                 # that can't handle general array indexing. For example, in netCDF4 you\n976                 # can do \"outer\" indexing along two dimensions independent, which works\n977                 # differently from how NumPy handles it.\n978                 # da.from_array works by using lazy indexing with a tuple of slices.\n979                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n980                 # different indexing types in an explicit way:\n981                 # https://github.com/dask/dask/issues/2883\n982                 data = indexing.ImplicitToExplicitIndexingAdapter(\n983                     data, indexing.OuterIndexer\n984                 )\n985                 if LooseVersion(dask.__version__) < \"2.0.0\":\n986                     kwargs = {}\n987                 else:\n988                     # All of our lazily loaded backend array classes should use NumPy\n989                     # array operations.\n990                     kwargs = {\"meta\": np.ndarray}\n991             else:\n992                 kwargs = {}\n993 \n994             if utils.is_dict_like(chunks):\n995                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n996 \n997             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n998 \n999         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n1000 \n1001     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n1002         \"\"\"\n1003         use sparse-array as backend.\n1004         \"\"\"\n1005         import sparse\n1006 \n1007         # TODO  what to do if dask-backended?\n1008         if fill_value is dtypes.NA:\n1009             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1010         else:\n1011             dtype = dtypes.result_type(self.dtype, fill_value)\n1012 \n1013         if sparse_format is _default:\n1014             sparse_format = \"coo\"\n1015         try:\n1016             as_sparse = getattr(sparse, \"as_{}\".format(sparse_format.lower()))\n1017         except AttributeError:\n1018             raise ValueError(\"{} is not a valid sparse format\".format(sparse_format))\n1019 \n1020         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n1021         return self._replace(data=data)\n1022 \n1023     def _to_dense(self):\n1024         \"\"\"\n1025         Change backend from sparse to np.array\n1026         \"\"\"\n1027         if hasattr(self._data, \"todense\"):\n1028             return self._replace(data=self._data.todense())\n1029         return self.copy(deep=False)\n1030 \n1031     def isel(\n1032         self: VariableType,\n1033         indexers: Mapping[Hashable, Any] = None,\n1034         missing_dims: str = \"raise\",\n1035         **indexers_kwargs: Any,\n1036     ) -> VariableType:\n1037         \"\"\"Return a new array indexed along the specified dimension(s).\n1038 \n1039         Parameters\n1040         ----------\n1041         **indexers : {dim: indexer, ...}\n1042             Keyword arguments with names matching dimensions and values given\n1043             by integers, slice objects or arrays.\n1044         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1045             What to do if dimensions that should be selected from are not present in the\n1046             DataArray:\n1047             - \"exception\": raise an exception\n1048             - \"warning\": raise a warning, and ignore the missing dimensions\n1049             - \"ignore\": ignore the missing dimensions\n1050 \n1051         Returns\n1052         -------\n1053         obj : Array object\n1054             A new Array with the selected data and dimensions. In general,\n1055             the new variable's data will be a view of this variable's data,\n1056             unless numpy fancy indexing was triggered by using an array\n1057             indexer, in which case the data will be a copy.\n1058         \"\"\"\n1059         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1060 \n1061         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n1062 \n1063         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n1064         return self[key]\n1065 \n1066     def squeeze(self, dim=None):\n1067         \"\"\"Return a new object with squeezed data.\n1068 \n1069         Parameters\n1070         ----------\n1071         dim : None or str or tuple of str, optional\n1072             Selects a subset of the length one dimensions. If a dimension is\n1073             selected with length greater than one, an error is raised. If\n1074             None, all length one dimensions are squeezed.\n1075 \n1076         Returns\n1077         -------\n1078         squeezed : same type as caller\n1079             This object, but with with all or a subset of the dimensions of\n1080             length 1 removed.\n1081 \n1082         See Also\n1083         --------\n1084         numpy.squeeze\n1085         \"\"\"\n1086         dims = common.get_squeeze_dims(self, dim)\n1087         return self.isel({d: 0 for d in dims})\n1088 \n1089     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n1090         axis = self.get_axis_num(dim)\n1091 \n1092         if count > 0:\n1093             keep = slice(None, -count)\n1094         elif count < 0:\n1095             keep = slice(-count, None)\n1096         else:\n1097             keep = slice(None)\n1098 \n1099         trimmed_data = self[(slice(None),) * axis + (keep,)].data\n1100 \n1101         if fill_value is dtypes.NA:\n1102             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1103         else:\n1104             dtype = self.dtype\n1105 \n1106         width = min(abs(count), self.shape[axis])\n1107         dim_pad = (width, 0) if count >= 0 else (0, width)\n1108         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n1109 \n1110         data = duck_array_ops.pad(\n1111             trimmed_data.astype(dtype),\n1112             pads,\n1113             mode=\"constant\",\n1114             constant_values=fill_value,\n1115         )\n1116 \n1117         if isinstance(data, dask_array_type):\n1118             # chunked data should come out with the same chunks; this makes\n1119             # it feasible to combine shifted and unshifted data\n1120             # TODO: remove this once dask.array automatically aligns chunks\n1121             data = data.rechunk(self.data.chunks)\n1122 \n1123         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1124 \n1125     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n1126         \"\"\"\n1127         Return a new Variable with shifted data.\n1128 \n1129         Parameters\n1130         ----------\n1131         shifts : mapping of the form {dim: offset}\n1132             Integer offset to shift along each of the given dimensions.\n1133             Positive offsets shift to the right; negative offsets shift to the\n1134             left.\n1135         fill_value: scalar, optional\n1136             Value to use for newly missing values\n1137         **shifts_kwargs:\n1138             The keyword arguments form of ``shifts``.\n1139             One of shifts or shifts_kwargs must be provided.\n1140 \n1141         Returns\n1142         -------\n1143         shifted : Variable\n1144             Variable with the same dimensions and attributes but shifted data.\n1145         \"\"\"\n1146         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n1147         result = self\n1148         for dim, count in shifts.items():\n1149             result = result._shift_one_dim(dim, count, fill_value=fill_value)\n1150         return result\n1151 \n1152     def _pad_options_dim_to_index(\n1153         self,\n1154         pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],\n1155         fill_with_shape=False,\n1156     ):\n1157         if fill_with_shape:\n1158             return [\n1159                 (n, n) if d not in pad_option else pad_option[d]\n1160                 for d, n in zip(self.dims, self.data.shape)\n1161             ]\n1162         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]\n1163 \n1164     def pad(\n1165         self,\n1166         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n1167         mode: str = \"constant\",\n1168         stat_length: Union[\n1169             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1170         ] = None,\n1171         constant_values: Union[\n1172             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1173         ] = None,\n1174         end_values: Union[\n1175             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1176         ] = None,\n1177         reflect_type: str = None,\n1178         **pad_width_kwargs: Any,\n1179     ):\n1180         \"\"\"\n1181         Return a new Variable with padded data.\n1182 \n1183         Parameters\n1184         ----------\n1185         pad_width: Mapping with the form of {dim: (pad_before, pad_after)}\n1186             Number of values padded along each dimension.\n1187             {dim: pad} is a shortcut for pad_before = pad_after = pad\n1188         mode: (str)\n1189             See numpy / Dask docs\n1190         stat_length : int, tuple or mapping of the form {dim: tuple}\n1191             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n1192             values at edge of each axis used to calculate the statistic value.\n1193         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n1194             Used in 'constant'.  The values to set the padded values for each\n1195             axis.\n1196         end_values : scalar, tuple or mapping of the form {dim: tuple}\n1197             Used in 'linear_ramp'.  The values used for the ending value of the\n1198             linear_ramp and that will form the edge of the padded array.\n1199         reflect_type : {'even', 'odd'}, optional\n1200             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n1201             default with an unaltered reflection around the edge value.  For\n1202             the 'odd' style, the extended part of the array is created by\n1203             subtracting the reflected values from two times the edge value.\n1204         **pad_width_kwargs:\n1205             One of pad_width or pad_width_kwargs must be provided.\n1206 \n1207         Returns\n1208         -------\n1209         padded : Variable\n1210             Variable with the same dimensions and attributes but padded data.\n1211         \"\"\"\n1212         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n1213 \n1214         # change default behaviour of pad with mode constant\n1215         if mode == \"constant\" and (\n1216             constant_values is None or constant_values is dtypes.NA\n1217         ):\n1218             dtype, constant_values = dtypes.maybe_promote(self.dtype)\n1219         else:\n1220             dtype = self.dtype\n1221 \n1222         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty\n1223         if isinstance(stat_length, dict):\n1224             stat_length = self._pad_options_dim_to_index(\n1225                 stat_length, fill_with_shape=True\n1226             )\n1227         if isinstance(constant_values, dict):\n1228             constant_values = self._pad_options_dim_to_index(constant_values)\n1229         if isinstance(end_values, dict):\n1230             end_values = self._pad_options_dim_to_index(end_values)\n1231 \n1232         # workaround for bug in Dask's default value of stat_length  https://github.com/dask/dask/issues/5303\n1233         if stat_length is None and mode in [\"maximum\", \"mean\", \"median\", \"minimum\"]:\n1234             stat_length = [(n, n) for n in self.data.shape]  # type: ignore\n1235 \n1236         # change integer values to a tuple of two of those values and change pad_width to index\n1237         for k, v in pad_width.items():\n1238             if isinstance(v, numbers.Number):\n1239                 pad_width[k] = (v, v)\n1240         pad_width_by_index = self._pad_options_dim_to_index(pad_width)\n1241 \n1242         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty\n1243         pad_option_kwargs = {}\n1244         if stat_length is not None:\n1245             pad_option_kwargs[\"stat_length\"] = stat_length\n1246         if constant_values is not None:\n1247             pad_option_kwargs[\"constant_values\"] = constant_values\n1248         if end_values is not None:\n1249             pad_option_kwargs[\"end_values\"] = end_values\n1250         if reflect_type is not None:\n1251             pad_option_kwargs[\"reflect_type\"] = reflect_type  # type: ignore\n1252 \n1253         array = duck_array_ops.pad(\n1254             self.data.astype(dtype, copy=False),\n1255             pad_width_by_index,\n1256             mode=mode,\n1257             **pad_option_kwargs,\n1258         )\n1259 \n1260         return type(self)(self.dims, array)\n1261 \n1262     def _roll_one_dim(self, dim, count):\n1263         axis = self.get_axis_num(dim)\n1264 \n1265         count %= self.shape[axis]\n1266         if count != 0:\n1267             indices = [slice(-count, None), slice(None, -count)]\n1268         else:\n1269             indices = [slice(None)]\n1270 \n1271         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n1272 \n1273         data = duck_array_ops.concatenate(arrays, axis)\n1274 \n1275         if isinstance(data, dask_array_type):\n1276             # chunked data should come out with the same chunks; this makes\n1277             # it feasible to combine shifted and unshifted data\n1278             # TODO: remove this once dask.array automatically aligns chunks\n1279             data = data.rechunk(self.data.chunks)\n1280 \n1281         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1282 \n1283     def roll(self, shifts=None, **shifts_kwargs):\n1284         \"\"\"\n1285         Return a new Variable with rolld data.\n1286 \n1287         Parameters\n1288         ----------\n1289         shifts : mapping of the form {dim: offset}\n1290             Integer offset to roll along each of the given dimensions.\n1291             Positive offsets roll to the right; negative offsets roll to the\n1292             left.\n1293         **shifts_kwargs:\n1294             The keyword arguments form of ``shifts``.\n1295             One of shifts or shifts_kwargs must be provided.\n1296 \n1297         Returns\n1298         -------\n1299         shifted : Variable\n1300             Variable with the same dimensions and attributes but rolled data.\n1301         \"\"\"\n1302         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n1303 \n1304         result = self\n1305         for dim, count in shifts.items():\n1306             result = result._roll_one_dim(dim, count)\n1307         return result\n1308 \n1309     def transpose(self, *dims) -> \"Variable\":\n1310         \"\"\"Return a new Variable object with transposed dimensions.\n1311 \n1312         Parameters\n1313         ----------\n1314         *dims : str, optional\n1315             By default, reverse the dimensions. Otherwise, reorder the\n1316             dimensions to this order.\n1317 \n1318         Returns\n1319         -------\n1320         transposed : Variable\n1321             The returned object has transposed data and dimensions with the\n1322             same attributes as the original.\n1323 \n1324         Notes\n1325         -----\n1326         This operation returns a view of this variable's data. It is\n1327         lazy for dask-backed Variables but not for numpy-backed Variables.\n1328 \n1329         See Also\n1330         --------\n1331         numpy.transpose\n1332         \"\"\"\n1333         if len(dims) == 0:\n1334             dims = self.dims[::-1]\n1335         dims = tuple(infix_dims(dims, self.dims))\n1336         axes = self.get_axis_num(dims)\n1337         if len(dims) < 2 or dims == self.dims:\n1338             # no need to transpose if only one dimension\n1339             # or dims are in same order\n1340             return self.copy(deep=False)\n1341 \n1342         data = as_indexable(self._data).transpose(axes)\n1343         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n1344 \n1345     @property\n1346     def T(self) -> \"Variable\":\n1347         return self.transpose()\n1348 \n1349     def set_dims(self, dims, shape=None):\n1350         \"\"\"Return a new variable with given set of dimensions.\n1351         This method might be used to attach new dimension(s) to variable.\n1352 \n1353         When possible, this operation does not copy this variable's data.\n1354 \n1355         Parameters\n1356         ----------\n1357         dims : str or sequence of str or dict\n1358             Dimensions to include on the new variable. If a dict, values are\n1359             used to provide the sizes of new dimensions; otherwise, new\n1360             dimensions are inserted with length 1.\n1361 \n1362         Returns\n1363         -------\n1364         Variable\n1365         \"\"\"\n1366         if isinstance(dims, str):\n1367             dims = [dims]\n1368 \n1369         if shape is None and utils.is_dict_like(dims):\n1370             shape = dims.values()\n1371 \n1372         missing_dims = set(self.dims) - set(dims)\n1373         if missing_dims:\n1374             raise ValueError(\n1375                 \"new dimensions %r must be a superset of \"\n1376                 \"existing dimensions %r\" % (dims, self.dims)\n1377             )\n1378 \n1379         self_dims = set(self.dims)\n1380         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n1381 \n1382         if self.dims == expanded_dims:\n1383             # don't use broadcast_to unless necessary so the result remains\n1384             # writeable if possible\n1385             expanded_data = self.data\n1386         elif shape is not None:\n1387             dims_map = dict(zip(dims, shape))\n1388             tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n1389             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n1390         else:\n1391             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n1392 \n1393         expanded_var = Variable(\n1394             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n1395         )\n1396         return expanded_var.transpose(*dims)\n1397 \n1398     def _stack_once(self, dims, new_dim):\n1399         if not set(dims) <= set(self.dims):\n1400             raise ValueError(\"invalid existing dimensions: %s\" % dims)\n1401 \n1402         if new_dim in self.dims:\n1403             raise ValueError(\n1404                 \"cannot create a new dimension with the same \"\n1405                 \"name as an existing dimension\"\n1406             )\n1407 \n1408         if len(dims) == 0:\n1409             # don't stack\n1410             return self.copy(deep=False)\n1411 \n1412         other_dims = [d for d in self.dims if d not in dims]\n1413         dim_order = other_dims + list(dims)\n1414         reordered = self.transpose(*dim_order)\n1415 \n1416         new_shape = reordered.shape[: len(other_dims)] + (-1,)\n1417         new_data = reordered.data.reshape(new_shape)\n1418         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n1419 \n1420         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1421 \n1422     def stack(self, dimensions=None, **dimensions_kwargs):\n1423         \"\"\"\n1424         Stack any number of existing dimensions into a single new dimension.\n1425 \n1426         New dimensions will be added at the end, and the order of the data\n1427         along each new dimension will be in contiguous (C) order.\n1428 \n1429         Parameters\n1430         ----------\n1431         dimensions : Mapping of form new_name=(dim1, dim2, ...)\n1432             Names of new dimensions, and the existing dimensions that they\n1433             replace.\n1434         **dimensions_kwargs:\n1435             The keyword arguments form of ``dimensions``.\n1436             One of dimensions or dimensions_kwargs must be provided.\n1437 \n1438         Returns\n1439         -------\n1440         stacked : Variable\n1441             Variable with the same attributes but stacked data.\n1442 \n1443         See also\n1444         --------\n1445         Variable.unstack\n1446         \"\"\"\n1447         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n1448         result = self\n1449         for new_dim, dims in dimensions.items():\n1450             result = result._stack_once(dims, new_dim)\n1451         return result\n1452 \n1453     def _unstack_once(self, dims, old_dim):\n1454         new_dim_names = tuple(dims.keys())\n1455         new_dim_sizes = tuple(dims.values())\n1456 \n1457         if old_dim not in self.dims:\n1458             raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n1459 \n1460         if set(new_dim_names).intersection(self.dims):\n1461             raise ValueError(\n1462                 \"cannot create a new dimension with the same \"\n1463                 \"name as an existing dimension\"\n1464             )\n1465 \n1466         if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n1467             raise ValueError(\n1468                 \"the product of the new dimension sizes must \"\n1469                 \"equal the size of the old dimension\"\n1470             )\n1471 \n1472         other_dims = [d for d in self.dims if d != old_dim]\n1473         dim_order = other_dims + [old_dim]\n1474         reordered = self.transpose(*dim_order)\n1475 \n1476         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n1477         new_data = reordered.data.reshape(new_shape)\n1478         new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n1479 \n1480         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1481 \n1482     def unstack(self, dimensions=None, **dimensions_kwargs):\n1483         \"\"\"\n1484         Unstack an existing dimension into multiple new dimensions.\n1485 \n1486         New dimensions will be added at the end, and the order of the data\n1487         along each new dimension will be in contiguous (C) order.\n1488 \n1489         Parameters\n1490         ----------\n1491         dimensions : mapping of the form old_dim={dim1: size1, ...}\n1492             Names of existing dimensions, and the new dimensions and sizes\n1493             that they map to.\n1494         **dimensions_kwargs:\n1495             The keyword arguments form of ``dimensions``.\n1496             One of dimensions or dimensions_kwargs must be provided.\n1497 \n1498         Returns\n1499         -------\n1500         unstacked : Variable\n1501             Variable with the same attributes but unstacked data.\n1502 \n1503         See also\n1504         --------\n1505         Variable.stack\n1506         \"\"\"\n1507         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n1508         result = self\n1509         for old_dim, dims in dimensions.items():\n1510             result = result._unstack_once(dims, old_dim)\n1511         return result\n1512 \n1513     def fillna(self, value):\n1514         return ops.fillna(self, value)\n1515 \n1516     def where(self, cond, other=dtypes.NA):\n1517         return ops.where_method(self, cond, other)\n1518 \n1519     def reduce(\n1520         self,\n1521         func,\n1522         dim=None,\n1523         axis=None,\n1524         keep_attrs=None,\n1525         keepdims=False,\n1526         allow_lazy=None,\n1527         **kwargs,\n1528     ):\n1529         \"\"\"Reduce this array by applying `func` along some dimension(s).\n1530 \n1531         Parameters\n1532         ----------\n1533         func : function\n1534             Function which can be called in the form\n1535             `func(x, axis=axis, **kwargs)` to return the result of reducing an\n1536             np.ndarray over an integer valued axis.\n1537         dim : str or sequence of str, optional\n1538             Dimension(s) over which to apply `func`.\n1539         axis : int or sequence of int, optional\n1540             Axis(es) over which to apply `func`. Only one of the 'dim'\n1541             and 'axis' arguments can be supplied. If neither are supplied, then\n1542             the reduction is calculated over the flattened array (by calling\n1543             `func(x)` without an axis argument).\n1544         keep_attrs : bool, optional\n1545             If True, the variable's attributes (`attrs`) will be copied from\n1546             the original object to the new one.  If False (default), the new\n1547             object will be returned without attributes.\n1548         keepdims : bool, default False\n1549             If True, the dimensions which are reduced are left in the result\n1550             as dimensions of size one\n1551         **kwargs : dict\n1552             Additional keyword arguments passed on to `func`.\n1553 \n1554         Returns\n1555         -------\n1556         reduced : Array\n1557             Array with summarized data and the indicated dimension(s)\n1558             removed.\n1559         \"\"\"\n1560         if dim == ...:\n1561             dim = None\n1562         if dim is not None and axis is not None:\n1563             raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n1564 \n1565         if dim is not None:\n1566             axis = self.get_axis_num(dim)\n1567 \n1568         if allow_lazy is not None:\n1569             warnings.warn(\n1570                 \"allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.\",\n1571                 DeprecationWarning,\n1572             )\n1573         else:\n1574             allow_lazy = True\n1575 \n1576         input_data = self.data if allow_lazy else self.values\n1577 \n1578         if axis is not None:\n1579             data = func(input_data, axis=axis, **kwargs)\n1580         else:\n1581             data = func(input_data, **kwargs)\n1582 \n1583         if getattr(data, \"shape\", ()) == self.shape:\n1584             dims = self.dims\n1585         else:\n1586             removed_axes = (\n1587                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n1588             )\n1589             if keepdims:\n1590                 # Insert np.newaxis for removed dims\n1591                 slices = tuple(\n1592                     np.newaxis if i in removed_axes else slice(None, None)\n1593                     for i in range(self.ndim)\n1594                 )\n1595                 if getattr(data, \"shape\", None) is None:\n1596                     # Reduce has produced a scalar value, not an array-like\n1597                     data = np.asanyarray(data)[slices]\n1598                 else:\n1599                     data = data[slices]\n1600                 dims = self.dims\n1601             else:\n1602                 dims = [\n1603                     adim for n, adim in enumerate(self.dims) if n not in removed_axes\n1604                 ]\n1605 \n1606         if keep_attrs is None:\n1607             keep_attrs = _get_keep_attrs(default=False)\n1608         attrs = self._attrs if keep_attrs else None\n1609 \n1610         return Variable(dims, data, attrs=attrs)\n1611 \n1612     @classmethod\n1613     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n1614         \"\"\"Concatenate variables along a new or existing dimension.\n1615 \n1616         Parameters\n1617         ----------\n1618         variables : iterable of Array\n1619             Arrays to stack together. Each variable is expected to have\n1620             matching dimensions and shape except for along the stacked\n1621             dimension.\n1622         dim : str or DataArray, optional\n1623             Name of the dimension to stack along. This can either be a new\n1624             dimension name, in which case it is added along axis=0, or an\n1625             existing dimension name, in which case the location of the\n1626             dimension is unchanged. Where to insert the new dimension is\n1627             determined by the first variable.\n1628         positions : None or list of integer arrays, optional\n1629             List of integer arrays which specifies the integer positions to\n1630             which to assign each dataset along the concatenated dimension.\n1631             If not supplied, objects are concatenated in the provided order.\n1632         shortcut : bool, optional\n1633             This option is used internally to speed-up groupby operations.\n1634             If `shortcut` is True, some checks of internal consistency between\n1635             arrays to concatenate are skipped.\n1636 \n1637         Returns\n1638         -------\n1639         stacked : Variable\n1640             Concatenated Variable formed by stacking all the supplied variables\n1641             along the given dimension.\n1642         \"\"\"\n1643         if not isinstance(dim, str):\n1644             (dim,) = dim.dims\n1645 \n1646         # can't do this lazily: we need to loop through variables at least\n1647         # twice\n1648         variables = list(variables)\n1649         first_var = variables[0]\n1650 \n1651         arrays = [v.data for v in variables]\n1652 \n1653         if dim in first_var.dims:\n1654             axis = first_var.get_axis_num(dim)\n1655             dims = first_var.dims\n1656             data = duck_array_ops.concatenate(arrays, axis=axis)\n1657             if positions is not None:\n1658                 # TODO: deprecate this option -- we don't need it for groupby\n1659                 # any more.\n1660                 indices = nputils.inverse_permutation(np.concatenate(positions))\n1661                 data = duck_array_ops.take(data, indices, axis=axis)\n1662         else:\n1663             axis = 0\n1664             dims = (dim,) + first_var.dims\n1665             data = duck_array_ops.stack(arrays, axis=axis)\n1666 \n1667         attrs = dict(first_var.attrs)\n1668         encoding = dict(first_var.encoding)\n1669         if not shortcut:\n1670             for var in variables:\n1671                 if var.dims != first_var.dims:\n1672                     raise ValueError(\n1673                         f\"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}\"\n1674                     )\n1675 \n1676         return cls(dims, data, attrs, encoding)\n1677 \n1678     def equals(self, other, equiv=duck_array_ops.array_equiv):\n1679         \"\"\"True if two Variables have the same dimensions and values;\n1680         otherwise False.\n1681 \n1682         Variables can still be equal (like pandas objects) if they have NaN\n1683         values in the same locations.\n1684 \n1685         This method is necessary because `v1 == v2` for Variables\n1686         does element-wise comparisons (like numpy.ndarrays).\n1687         \"\"\"\n1688         other = getattr(other, \"variable\", other)\n1689         try:\n1690             return self.dims == other.dims and (\n1691                 self._data is other._data or equiv(self.data, other.data)\n1692             )\n1693         except (TypeError, AttributeError):\n1694             return False\n1695 \n1696     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n1697         \"\"\"True if two Variables have the values after being broadcast against\n1698         each other; otherwise False.\n1699 \n1700         Variables can still be equal (like pandas objects) if they have NaN\n1701         values in the same locations.\n1702         \"\"\"\n1703         try:\n1704             self, other = broadcast_variables(self, other)\n1705         except (ValueError, AttributeError):\n1706             return False\n1707         return self.equals(other, equiv=equiv)\n1708 \n1709     def identical(self, other, equiv=duck_array_ops.array_equiv):\n1710         \"\"\"Like equals, but also checks attributes.\n1711         \"\"\"\n1712         try:\n1713             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n1714                 other, equiv=equiv\n1715             )\n1716         except (TypeError, AttributeError):\n1717             return False\n1718 \n1719     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n1720         \"\"\"True if the intersection of two Variable's non-null data is\n1721         equal; otherwise false.\n1722 \n1723         Variables can thus still be equal if there are locations where either,\n1724         or both, contain NaN values.\n1725         \"\"\"\n1726         return self.broadcast_equals(other, equiv=equiv)\n1727 \n1728     def quantile(\n1729         self, q, dim=None, interpolation=\"linear\", keep_attrs=None, skipna=True\n1730     ):\n1731         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1732 \n1733         Returns the qth quantiles(s) of the array elements.\n1734 \n1735         Parameters\n1736         ----------\n1737         q : float in range of [0,1] (or sequence of floats)\n1738             Quantile to compute, which must be between 0 and 1\n1739             inclusive.\n1740         dim : str or sequence of str, optional\n1741             Dimension(s) over which to apply quantile.\n1742         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n1743             This optional parameter specifies the interpolation method to\n1744             use when the desired quantile lies between two data points\n1745             ``i < j``:\n1746 \n1747                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1748                   the fractional part of the index surrounded by ``i`` and\n1749                   ``j``.\n1750                 * lower: ``i``.\n1751                 * higher: ``j``.\n1752                 * nearest: ``i`` or ``j``, whichever is nearest.\n1753                 * midpoint: ``(i + j) / 2``.\n1754 \n1755         keep_attrs : bool, optional\n1756             If True, the variable's attributes (`attrs`) will be copied from\n1757             the original object to the new one.  If False (default), the new\n1758             object will be returned without attributes.\n1759 \n1760         Returns\n1761         -------\n1762         quantiles : Variable\n1763             If `q` is a single quantile, then the result\n1764             is a scalar. If multiple percentiles are given, first axis of\n1765             the result corresponds to the quantile and a quantile dimension\n1766             is added to the return array. The other dimensions are the\n1767             dimensions that remain after the reduction of the array.\n1768 \n1769         See Also\n1770         --------\n1771         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,\n1772         DataArray.quantile\n1773         \"\"\"\n1774 \n1775         from .computation import apply_ufunc\n1776 \n1777         _quantile_func = np.nanquantile if skipna else np.quantile\n1778 \n1779         if keep_attrs is None:\n1780             keep_attrs = _get_keep_attrs(default=False)\n1781 \n1782         scalar = utils.is_scalar(q)\n1783         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1784 \n1785         if dim is None:\n1786             dim = self.dims\n1787 \n1788         if utils.is_scalar(dim):\n1789             dim = [dim]\n1790 \n1791         def _wrapper(npa, **kwargs):\n1792             # move quantile axis to end. required for apply_ufunc\n1793             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n1794 \n1795         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1796         result = apply_ufunc(\n1797             _wrapper,\n1798             self,\n1799             input_core_dims=[dim],\n1800             exclude_dims=set(dim),\n1801             output_core_dims=[[\"quantile\"]],\n1802             output_dtypes=[np.float64],\n1803             output_sizes={\"quantile\": len(q)},\n1804             dask=\"parallelized\",\n1805             kwargs={\"q\": q, \"axis\": axis, \"interpolation\": interpolation},\n1806         )\n1807 \n1808         # for backward compatibility\n1809         result = result.transpose(\"quantile\", ...)\n1810         if scalar:\n1811             result = result.squeeze(\"quantile\")\n1812         if keep_attrs:\n1813             result.attrs = self._attrs\n1814         return result\n1815 \n1816     def rank(self, dim, pct=False):\n1817         \"\"\"Ranks the data.\n1818 \n1819         Equal values are assigned a rank that is the average of the ranks that\n1820         would have been otherwise assigned to all of the values within that\n1821         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n1822 \n1823         NaNs in the input array are returned as NaNs.\n1824 \n1825         The `bottleneck` library is required.\n1826 \n1827         Parameters\n1828         ----------\n1829         dim : str\n1830             Dimension over which to compute rank.\n1831         pct : bool, optional\n1832             If True, compute percentage ranks, otherwise compute integer ranks.\n1833 \n1834         Returns\n1835         -------\n1836         ranked : Variable\n1837 \n1838         See Also\n1839         --------\n1840         Dataset.rank, DataArray.rank\n1841         \"\"\"\n1842         import bottleneck as bn\n1843 \n1844         data = self.data\n1845 \n1846         if isinstance(data, dask_array_type):\n1847             raise TypeError(\n1848                 \"rank does not work for arrays stored as dask \"\n1849                 \"arrays. Load the data via .compute() or .load() \"\n1850                 \"prior to calling this method.\"\n1851             )\n1852         elif not isinstance(data, np.ndarray):\n1853             raise TypeError(\n1854                 \"rank is not implemented for {} objects.\".format(type(data))\n1855             )\n1856 \n1857         axis = self.get_axis_num(dim)\n1858         func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n1859         ranked = func(data, axis=axis)\n1860         if pct:\n1861             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n1862             ranked /= count\n1863         return Variable(self.dims, ranked)\n1864 \n1865     def rolling_window(\n1866         self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n1867     ):\n1868         \"\"\"\n1869         Make a rolling_window along dim and add a new_dim to the last place.\n1870 \n1871         Parameters\n1872         ----------\n1873         dim: str\n1874             Dimension over which to compute rolling_window\n1875         window: int\n1876             Window size of the rolling\n1877         window_dim: str\n1878             New name of the window dimension.\n1879         center: boolean. default False.\n1880             If True, pad fill_value for both ends. Otherwise, pad in the head\n1881             of the axis.\n1882         fill_value:\n1883             value to be filled.\n1884 \n1885         Returns\n1886         -------\n1887         Variable that is a view of the original array with a added dimension of\n1888         size w.\n1889         The return dim: self.dims + (window_dim, )\n1890         The return shape: self.shape + (window, )\n1891 \n1892         Examples\n1893         --------\n1894         >>> v = Variable((\"a\", \"b\"), np.arange(8).reshape((2, 4)))\n1895         >>> v.rolling_window(x, \"b\", 3, \"window_dim\")\n1896         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1897         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n1898                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n1899 \n1900         >>> v.rolling_window(x, \"b\", 3, \"window_dim\", center=True)\n1901         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1902         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n1903                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n1904         \"\"\"\n1905         if fill_value is dtypes.NA:  # np.nan is passed\n1906             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1907             array = self.astype(dtype, copy=False).data\n1908         else:\n1909             dtype = self.dtype\n1910             array = self.data\n1911 \n1912         new_dims = self.dims + (window_dim,)\n1913         return Variable(\n1914             new_dims,\n1915             duck_array_ops.rolling_window(\n1916                 array,\n1917                 axis=self.get_axis_num(dim),\n1918                 window=window,\n1919                 center=center,\n1920                 fill_value=fill_value,\n1921             ),\n1922         )\n1923 \n1924     def coarsen(self, windows, func, boundary=\"exact\", side=\"left\", **kwargs):\n1925         \"\"\"\n1926         Apply reduction function.\n1927         \"\"\"\n1928         windows = {k: v for k, v in windows.items() if k in self.dims}\n1929         if not windows:\n1930             return self.copy()\n1931 \n1932         reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n1933         if isinstance(func, str):\n1934             name = func\n1935             func = getattr(duck_array_ops, name, None)\n1936             if func is None:\n1937                 raise NameError(f\"{name} is not a valid method.\")\n1938         return self._replace(data=func(reshaped, axis=axes, **kwargs))\n1939 \n1940     def _coarsen_reshape(self, windows, boundary, side):\n1941         \"\"\"\n1942         Construct a reshaped-array for coarsen\n1943         \"\"\"\n1944         if not utils.is_dict_like(boundary):\n1945             boundary = {d: boundary for d in windows.keys()}\n1946 \n1947         if not utils.is_dict_like(side):\n1948             side = {d: side for d in windows.keys()}\n1949 \n1950         # remove unrelated dimensions\n1951         boundary = {k: v for k, v in boundary.items() if k in windows}\n1952         side = {k: v for k, v in side.items() if k in windows}\n1953 \n1954         for d, window in windows.items():\n1955             if window <= 0:\n1956                 raise ValueError(f\"window must be > 0. Given {window}\")\n1957 \n1958         variable = self\n1959         for d, window in windows.items():\n1960             # trim or pad the object\n1961             size = variable.shape[self._get_axis_num(d)]\n1962             n = int(size / window)\n1963             if boundary[d] == \"exact\":\n1964                 if n * window != size:\n1965                     raise ValueError(\n1966                         \"Could not coarsen a dimension of size {} with \"\n1967                         \"window {}\".format(size, window)\n1968                     )\n1969             elif boundary[d] == \"trim\":\n1970                 if side[d] == \"left\":\n1971                     variable = variable.isel({d: slice(0, window * n)})\n1972                 else:\n1973                     excess = size - window * n\n1974                     variable = variable.isel({d: slice(excess, None)})\n1975             elif boundary[d] == \"pad\":  # pad\n1976                 pad = window * n - size\n1977                 if pad < 0:\n1978                     pad += window\n1979                 if side[d] == \"left\":\n1980                     pad_width = {d: (0, pad)}\n1981                 else:\n1982                     pad_width = {d: (pad, 0)}\n1983                 variable = variable.pad(pad_width, mode=\"constant\")\n1984             else:\n1985                 raise TypeError(\n1986                     \"{} is invalid for boundary. Valid option is 'exact', \"\n1987                     \"'trim' and 'pad'\".format(boundary[d])\n1988                 )\n1989 \n1990         shape = []\n1991         axes = []\n1992         axis_count = 0\n1993         for i, d in enumerate(variable.dims):\n1994             if d in windows:\n1995                 size = variable.shape[i]\n1996                 shape.append(int(size / windows[d]))\n1997                 shape.append(windows[d])\n1998                 axis_count += 1\n1999                 axes.append(i + axis_count)\n2000             else:\n2001                 shape.append(variable.shape[i])\n2002 \n2003         keep_attrs = _get_keep_attrs(default=False)\n2004         variable.attrs = variable._attrs if keep_attrs else {}\n2005 \n2006         return variable.data.reshape(shape), tuple(axes)\n2007 \n2008     @property\n2009     def real(self):\n2010         return type(self)(self.dims, self.data.real, self._attrs)\n2011 \n2012     @property\n2013     def imag(self):\n2014         return type(self)(self.dims, self.data.imag, self._attrs)\n2015 \n2016     def __array_wrap__(self, obj, context=None):\n2017         return Variable(self.dims, obj)\n2018 \n2019     @staticmethod\n2020     def _unary_op(f):\n2021         @functools.wraps(f)\n2022         def func(self, *args, **kwargs):\n2023             with np.errstate(all=\"ignore\"):\n2024                 return self.__array_wrap__(f(self.data, *args, **kwargs))\n2025 \n2026         return func\n2027 \n2028     @staticmethod\n2029     def _binary_op(f, reflexive=False, **ignored_kwargs):\n2030         @functools.wraps(f)\n2031         def func(self, other):\n2032             if isinstance(other, (xr.DataArray, xr.Dataset)):\n2033                 return NotImplemented\n2034             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2035             keep_attrs = _get_keep_attrs(default=False)\n2036             attrs = self._attrs if keep_attrs else None\n2037             with np.errstate(all=\"ignore\"):\n2038                 new_data = (\n2039                     f(self_data, other_data)\n2040                     if not reflexive\n2041                     else f(other_data, self_data)\n2042                 )\n2043             result = Variable(dims, new_data, attrs=attrs)\n2044             return result\n2045 \n2046         return func\n2047 \n2048     @staticmethod\n2049     def _inplace_binary_op(f):\n2050         @functools.wraps(f)\n2051         def func(self, other):\n2052             if isinstance(other, xr.Dataset):\n2053                 raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n2054             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2055             if dims != self.dims:\n2056                 raise ValueError(\"dimensions cannot change for in-place \" \"operations\")\n2057             with np.errstate(all=\"ignore\"):\n2058                 self.values = f(self_data, other_data)\n2059             return self\n2060 \n2061         return func\n2062 \n2063     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n2064         \"\"\" A (private) method to convert datetime array to numeric dtype\n2065         See duck_array_ops.datetime_to_numeric\n2066         \"\"\"\n2067         numeric_array = duck_array_ops.datetime_to_numeric(\n2068             self.data, offset, datetime_unit, dtype\n2069         )\n2070         return type(self)(self.dims, numeric_array, self._attrs)\n2071 \n2072 \n2073 ops.inject_all_ops_and_reduce_methods(Variable)\n2074 \n2075 \n2076 class IndexVariable(Variable):\n2077     \"\"\"Wrapper for accommodating a pandas.Index in an xarray.Variable.\n2078 \n2079     IndexVariable preserve loaded values in the form of a pandas.Index instead\n2080     of a NumPy array. Hence, their values are immutable and must always be one-\n2081     dimensional.\n2082 \n2083     They also have a name property, which is the name of their sole dimension\n2084     unless another name is given.\n2085     \"\"\"\n2086 \n2087     __slots__ = ()\n2088 \n2089     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n2090         super().__init__(dims, data, attrs, encoding, fastpath)\n2091         if self.ndim != 1:\n2092             raise ValueError(\"%s objects must be 1-dimensional\" % type(self).__name__)\n2093 \n2094         # Unlike in Variable, always eagerly load values into memory\n2095         if not isinstance(self._data, PandasIndexAdapter):\n2096             self._data = PandasIndexAdapter(self._data)\n2097 \n2098     def __dask_tokenize__(self):\n2099         from dask.base import normalize_token\n2100 \n2101         # Don't waste time converting pd.Index to np.ndarray\n2102         return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n2103 \n2104     def load(self):\n2105         # data is already loaded into memory for IndexVariable\n2106         return self\n2107 \n2108     # https://github.com/python/mypy/issues/1465\n2109     @Variable.data.setter  # type: ignore\n2110     def data(self, data):\n2111         raise ValueError(\n2112             f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n2113             f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n2114         )\n2115 \n2116     @Variable.values.setter  # type: ignore\n2117     def values(self, values):\n2118         raise ValueError(\n2119             f\"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n2120             f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n2121         )\n2122 \n2123     def chunk(self, chunks=None, name=None, lock=False):\n2124         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()\n2125         return self.copy(deep=False)\n2126 \n2127     def _as_sparse(self, sparse_format=_default, fill_value=_default):\n2128         # Dummy\n2129         return self.copy(deep=False)\n2130 \n2131     def _to_dense(self):\n2132         # Dummy\n2133         return self.copy(deep=False)\n2134 \n2135     def _finalize_indexing_result(self, dims, data):\n2136         if getattr(data, \"ndim\", 0) != 1:\n2137             # returns Variable rather than IndexVariable if multi-dimensional\n2138             return Variable(dims, data, self._attrs, self._encoding)\n2139         else:\n2140             return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n2141 \n2142     def __setitem__(self, key, value):\n2143         raise TypeError(\"%s values cannot be modified\" % type(self).__name__)\n2144 \n2145     @classmethod\n2146     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2147         \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n2148 \n2149         This exists because we want to avoid converting Index objects to NumPy\n2150         arrays, if possible.\n2151         \"\"\"\n2152         if not isinstance(dim, str):\n2153             (dim,) = dim.dims\n2154 \n2155         variables = list(variables)\n2156         first_var = variables[0]\n2157 \n2158         if any(not isinstance(v, cls) for v in variables):\n2159             raise TypeError(\n2160                 \"IndexVariable.concat requires that all input \"\n2161                 \"variables be IndexVariable objects\"\n2162             )\n2163 \n2164         indexes = [v._data.array for v in variables]\n2165 \n2166         if not indexes:\n2167             data = []\n2168         else:\n2169             data = indexes[0].append(indexes[1:])\n2170 \n2171             if positions is not None:\n2172                 indices = nputils.inverse_permutation(np.concatenate(positions))\n2173                 data = data.take(indices)\n2174 \n2175         attrs = dict(first_var.attrs)\n2176         if not shortcut:\n2177             for var in variables:\n2178                 if var.dims != first_var.dims:\n2179                     raise ValueError(\"inconsistent dimensions\")\n2180                 utils.remove_incompatible_items(attrs, var.attrs)\n2181 \n2182         return cls(first_var.dims, data, attrs)\n2183 \n2184     def copy(self, deep=True, data=None):\n2185         \"\"\"Returns a copy of this object.\n2186 \n2187         `deep` is ignored since data is stored in the form of\n2188         pandas.Index, which is already immutable. Dimensions, attributes\n2189         and encodings are always copied.\n2190 \n2191         Use `data` to create a new object with the same structure as\n2192         original but entirely new data.\n2193 \n2194         Parameters\n2195         ----------\n2196         deep : bool, optional\n2197             Deep is ignored when data is given. Whether the data array is\n2198             loaded into memory and copied onto the new object. Default is True.\n2199         data : array_like, optional\n2200             Data to use in the new object. Must have same shape as original.\n2201 \n2202         Returns\n2203         -------\n2204         object : Variable\n2205             New object with dimensions, attributes, encodings, and optionally\n2206             data copied from original.\n2207         \"\"\"\n2208         if data is None:\n2209             data = self._data.copy(deep=deep)\n2210         else:\n2211             data = as_compatible_data(data)\n2212             if self.shape != data.shape:\n2213                 raise ValueError(\n2214                     \"Data shape {} must match shape of object {}\".format(\n2215                         data.shape, self.shape\n2216                     )\n2217                 )\n2218         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n2219 \n2220     def equals(self, other, equiv=None):\n2221         # if equiv is specified, super up\n2222         if equiv is not None:\n2223             return super().equals(other, equiv)\n2224 \n2225         # otherwise use the native index equals, rather than looking at _data\n2226         other = getattr(other, \"variable\", other)\n2227         try:\n2228             return self.dims == other.dims and self._data_equals(other)\n2229         except (TypeError, AttributeError):\n2230             return False\n2231 \n2232     def _data_equals(self, other):\n2233         return self.to_index().equals(other.to_index())\n2234 \n2235     def to_index_variable(self):\n2236         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n2237         return self\n2238 \n2239     to_coord = utils.alias(to_index_variable, \"to_coord\")\n2240 \n2241     def to_index(self):\n2242         \"\"\"Convert this variable to a pandas.Index\"\"\"\n2243         # n.b. creating a new pandas.Index from an old pandas.Index is\n2244         # basically free as pandas.Index objects are immutable\n2245         assert self.ndim == 1\n2246         index = self._data.array\n2247         if isinstance(index, pd.MultiIndex):\n2248             # set default names for multi-index unnamed levels so that\n2249             # we can safely rename dimension / coordinate later\n2250             valid_level_names = [\n2251                 name or \"{}_level_{}\".format(self.dims[0], i)\n2252                 for i, name in enumerate(index.names)\n2253             ]\n2254             index = index.set_names(valid_level_names)\n2255         else:\n2256             index = index.set_names(self.name)\n2257         return index\n2258 \n2259     @property\n2260     def level_names(self):\n2261         \"\"\"Return MultiIndex level names or None if this IndexVariable has no\n2262         MultiIndex.\n2263         \"\"\"\n2264         index = self.to_index()\n2265         if isinstance(index, pd.MultiIndex):\n2266             return index.names\n2267         else:\n2268             return None\n2269 \n2270     def get_level_variable(self, level):\n2271         \"\"\"Return a new IndexVariable from a given MultiIndex level.\"\"\"\n2272         if self.level_names is None:\n2273             raise ValueError(\"IndexVariable %r has no MultiIndex\" % self.name)\n2274         index = self.to_index()\n2275         return type(self)(self.dims, index.get_level_values(level))\n2276 \n2277     @property\n2278     def name(self):\n2279         return self.dims[0]\n2280 \n2281     @name.setter\n2282     def name(self, value):\n2283         raise AttributeError(\"cannot modify name of IndexVariable in-place\")\n2284 \n2285 \n2286 # for backwards compatibility\n2287 Coordinate = utils.alias(IndexVariable, \"Coordinate\")\n2288 \n2289 \n2290 def _unified_dims(variables):\n2291     # validate dimensions\n2292     all_dims = {}\n2293     for var in variables:\n2294         var_dims = var.dims\n2295         if len(set(var_dims)) < len(var_dims):\n2296             raise ValueError(\n2297                 \"broadcasting cannot handle duplicate \"\n2298                 \"dimensions: %r\" % list(var_dims)\n2299             )\n2300         for d, s in zip(var_dims, var.shape):\n2301             if d not in all_dims:\n2302                 all_dims[d] = s\n2303             elif all_dims[d] != s:\n2304                 raise ValueError(\n2305                     \"operands cannot be broadcast together \"\n2306                     \"with mismatched lengths for dimension %r: %s\"\n2307                     % (d, (all_dims[d], s))\n2308                 )\n2309     return all_dims\n2310 \n2311 \n2312 def _broadcast_compat_variables(*variables):\n2313     \"\"\"Create broadcast compatible variables, with the same dimensions.\n2314 \n2315     Unlike the result of broadcast_variables(), some variables may have\n2316     dimensions of size 1 instead of the the size of the broadcast dimension.\n2317     \"\"\"\n2318     dims = tuple(_unified_dims(variables))\n2319     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)\n2320 \n2321 \n2322 def broadcast_variables(*variables):\n2323     \"\"\"Given any number of variables, return variables with matching dimensions\n2324     and broadcast data.\n2325 \n2326     The data on the returned variables will be a view of the data on the\n2327     corresponding original arrays, but dimensions will be reordered and\n2328     inserted so that both broadcast arrays have the same dimensions. The new\n2329     dimensions are sorted in order of appearance in the first variable's\n2330     dimensions followed by the second variable's dimensions.\n2331     \"\"\"\n2332     dims_map = _unified_dims(variables)\n2333     dims_tuple = tuple(dims_map)\n2334     return tuple(\n2335         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables\n2336     )\n2337 \n2338 \n2339 def _broadcast_compat_data(self, other):\n2340     if all(hasattr(other, attr) for attr in [\"dims\", \"data\", \"shape\", \"encoding\"]):\n2341         # `other` satisfies the necessary Variable API for broadcast_variables\n2342         new_self, new_other = _broadcast_compat_variables(self, other)\n2343         self_data = new_self.data\n2344         other_data = new_other.data\n2345         dims = new_self.dims\n2346     else:\n2347         # rely on numpy broadcasting rules\n2348         self_data = self.data\n2349         other_data = other\n2350         dims = self.dims\n2351     return self_data, other_data, dims\n2352 \n2353 \n2354 def concat(variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2355     \"\"\"Concatenate variables along a new or existing dimension.\n2356 \n2357     Parameters\n2358     ----------\n2359     variables : iterable of Array\n2360         Arrays to stack together. Each variable is expected to have\n2361         matching dimensions and shape except for along the stacked\n2362         dimension.\n2363     dim : str or DataArray, optional\n2364         Name of the dimension to stack along. This can either be a new\n2365         dimension name, in which case it is added along axis=0, or an\n2366         existing dimension name, in which case the location of the\n2367         dimension is unchanged. Where to insert the new dimension is\n2368         determined by the first variable.\n2369     positions : None or list of integer arrays, optional\n2370         List of integer arrays which specifies the integer positions to which\n2371         to assign each dataset along the concatenated dimension. If not\n2372         supplied, objects are concatenated in the provided order.\n2373     shortcut : bool, optional\n2374         This option is used internally to speed-up groupby operations.\n2375         If `shortcut` is True, some checks of internal consistency between\n2376         arrays to concatenate are skipped.\n2377 \n2378     Returns\n2379     -------\n2380     stacked : Variable\n2381         Concatenated Variable formed by stacking all the supplied variables\n2382         along the given dimension.\n2383     \"\"\"\n2384     variables = list(variables)\n2385     if all(isinstance(v, IndexVariable) for v in variables):\n2386         return IndexVariable.concat(variables, dim, positions, shortcut)\n2387     else:\n2388         return Variable.concat(variables, dim, positions, shortcut)\n2389 \n2390 \n2391 def assert_unique_multiindex_level_names(variables):\n2392     \"\"\"Check for uniqueness of MultiIndex level names in all given\n2393     variables.\n2394 \n2395     Not public API. Used for checking consistency of DataArray and Dataset\n2396     objects.\n2397     \"\"\"\n2398     level_names = defaultdict(list)\n2399     all_level_names = set()\n2400     for var_name, var in variables.items():\n2401         if isinstance(var._data, PandasIndexAdapter):\n2402             idx_level_names = var.to_index_variable().level_names\n2403             if idx_level_names is not None:\n2404                 for n in idx_level_names:\n2405                     level_names[n].append(f\"{n!r} ({var_name})\")\n2406             if idx_level_names:\n2407                 all_level_names.update(idx_level_names)\n2408 \n2409     for k, v in level_names.items():\n2410         if k in variables:\n2411             v.append(\"(%s)\" % k)\n2412 \n2413     duplicate_names = [v for v in level_names.values() if len(v) > 1]\n2414     if duplicate_names:\n2415         conflict_str = \"\\n\".join(\", \".join(v) for v in duplicate_names)\n2416         raise ValueError(\"conflicting MultiIndex level name(s):\\n%s\" % conflict_str)\n2417     # Check confliction between level names and dimensions GH:2299\n2418     for k, v in variables.items():\n2419         for d in v.dims:\n2420             if d in all_level_names:\n2421                 raise ValueError(\n2422                     \"conflicting level / dimension names. {} \"\n2423                     \"already exists as a level name.\".format(d)\n2424                 )\n",
        "intended_behavior": "Preservation of Existing Functionality: Any enhancements should maintain the current method functionalities, ensuring that the toggle functionality (expand/collapse) remains intact for user interaction."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/dataarray.py",
        "start": 1,
        "end": 3828,
        "class_name": null,
        "method_name": null,
        "code": "1 import datetime\n2 import functools\n3 from numbers import Number\n4 from typing import (\n5     TYPE_CHECKING,\n6     Any,\n7     Callable,\n8     Dict,\n9     Hashable,\n10     Iterable,\n11     List,\n12     Mapping,\n13     Optional,\n14     Sequence,\n15     Tuple,\n16     TypeVar,\n17     Union,\n18     cast,\n19 )\n20 \n21 import numpy as np\n22 import pandas as pd\n23 \n24 from ..plot.plot import _PlotMethods\n25 from . import (\n26     computation,\n27     dtypes,\n28     groupby,\n29     indexing,\n30     ops,\n31     pdcompat,\n32     resample,\n33     rolling,\n34     utils,\n35     weighted,\n36 )\n37 from .accessor_dt import CombinedDatetimelikeAccessor\n38 from .accessor_str import StringAccessor\n39 from .alignment import (\n40     _broadcast_helper,\n41     _get_broadcast_dims_map_common_coords,\n42     align,\n43     reindex_like_indexers,\n44 )\n45 from .common import AbstractArray, DataWithCoords\n46 from .coordinates import (\n47     DataArrayCoordinates,\n48     LevelCoordinatesSource,\n49     assert_coordinate_consistent,\n50     remap_label_indexers,\n51 )\n52 from .dataset import Dataset, split_indexes\n53 from .formatting import format_item\n54 from .indexes import Indexes, default_indexes, propagate_indexes\n55 from .indexing import is_fancy_indexer\n56 from .merge import PANDAS_TYPES, MergeError, _extract_indexes_from_coords\n57 from .options import OPTIONS\n58 from .utils import Default, ReprObject, _check_inplace, _default, either_dict_or_kwargs\n59 from .variable import (\n60     IndexVariable,\n61     Variable,\n62     as_compatible_data,\n63     as_variable,\n64     assert_unique_multiindex_level_names,\n65 )\n66 \n67 if TYPE_CHECKING:\n68     T_DSorDA = TypeVar(\"T_DSorDA\", \"DataArray\", Dataset)\n69 \n70     try:\n71         from dask.delayed import Delayed\n72     except ImportError:\n73         Delayed = None\n74     try:\n75         from cdms2 import Variable as cdms2_Variable\n76     except ImportError:\n77         cdms2_Variable = None\n78     try:\n79         from iris.cube import Cube as iris_Cube\n80     except ImportError:\n81         iris_Cube = None\n82 \n83 \n84 def _infer_coords_and_dims(\n85     shape, coords, dims\n86 ) -> \"Tuple[Dict[Any, Variable], Tuple[Hashable, ...]]\":\n87     \"\"\"All the logic for creating a new DataArray\"\"\"\n88 \n89     if (\n90         coords is not None\n91         and not utils.is_dict_like(coords)\n92         and len(coords) != len(shape)\n93     ):\n94         raise ValueError(\n95             \"coords is not dict-like, but it has %s items, \"\n96             \"which does not match the %s dimensions of the \"\n97             \"data\" % (len(coords), len(shape))\n98         )\n99 \n100     if isinstance(dims, str):\n101         dims = (dims,)\n102 \n103     if dims is None:\n104         dims = [\"dim_%s\" % n for n in range(len(shape))]\n105         if coords is not None and len(coords) == len(shape):\n106             # try to infer dimensions from coords\n107             if utils.is_dict_like(coords):\n108                 # deprecated in GH993, removed in GH1539\n109                 raise ValueError(\n110                     \"inferring DataArray dimensions from \"\n111                     \"dictionary like ``coords`` is no longer \"\n112                     \"supported. Use an explicit list of \"\n113                     \"``dims`` instead.\"\n114                 )\n115             for n, (dim, coord) in enumerate(zip(dims, coords)):\n116                 coord = as_variable(coord, name=dims[n]).to_index_variable()\n117                 dims[n] = coord.name\n118         dims = tuple(dims)\n119     elif len(dims) != len(shape):\n120         raise ValueError(\n121             \"different number of dimensions on data \"\n122             \"and dims: %s vs %s\" % (len(shape), len(dims))\n123         )\n124     else:\n125         for d in dims:\n126             if not isinstance(d, str):\n127                 raise TypeError(\"dimension %s is not a string\" % d)\n128 \n129     new_coords: Dict[Any, Variable] = {}\n130 \n131     if utils.is_dict_like(coords):\n132         for k, v in coords.items():\n133             new_coords[k] = as_variable(v, name=k)\n134     elif coords is not None:\n135         for dim, coord in zip(dims, coords):\n136             var = as_variable(coord, name=dim)\n137             var.dims = (dim,)\n138             new_coords[dim] = var.to_index_variable()\n139 \n140     sizes = dict(zip(dims, shape))\n141     for k, v in new_coords.items():\n142         if any(d not in dims for d in v.dims):\n143             raise ValueError(\n144                 \"coordinate %s has dimensions %s, but these \"\n145                 \"are not a subset of the DataArray \"\n146                 \"dimensions %s\" % (k, v.dims, dims)\n147             )\n148 \n149         for d, s in zip(v.dims, v.shape):\n150             if s != sizes[d]:\n151                 raise ValueError(\n152                     \"conflicting sizes for dimension %r: \"\n153                     \"length %s on the data but length %s on \"\n154                     \"coordinate %r\" % (d, sizes[d], s, k)\n155                 )\n156 \n157         if k in sizes and v.shape != (sizes[k],):\n158             raise ValueError(\n159                 \"coordinate %r is a DataArray dimension, but \"\n160                 \"it has shape %r rather than expected shape %r \"\n161                 \"matching the dimension size\" % (k, v.shape, (sizes[k],))\n162             )\n163 \n164     assert_unique_multiindex_level_names(new_coords)\n165 \n166     return new_coords, dims\n167 \n168 \n169 def _check_data_shape(data, coords, dims):\n170     if data is dtypes.NA:\n171         data = np.nan\n172     if coords is not None and utils.is_scalar(data, include_0d=False):\n173         if utils.is_dict_like(coords):\n174             if dims is None:\n175                 return data\n176             else:\n177                 data_shape = tuple(\n178                     as_variable(coords[k], k).size if k in coords.keys() else 1\n179                     for k in dims\n180                 )\n181         else:\n182             data_shape = tuple(as_variable(coord, \"foo\").size for coord in coords)\n183         data = np.full(data_shape, data)\n184     return data\n185 \n186 \n187 class _LocIndexer:\n188     __slots__ = (\"data_array\",)\n189 \n190     def __init__(self, data_array: \"DataArray\"):\n191         self.data_array = data_array\n192 \n193     def __getitem__(self, key) -> \"DataArray\":\n194         if not utils.is_dict_like(key):\n195             # expand the indexer so we can handle Ellipsis\n196             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n197             key = dict(zip(self.data_array.dims, labels))\n198         return self.data_array.sel(**key)\n199 \n200     def __setitem__(self, key, value) -> None:\n201         if not utils.is_dict_like(key):\n202             # expand the indexer so we can handle Ellipsis\n203             labels = indexing.expanded_indexer(key, self.data_array.ndim)\n204             key = dict(zip(self.data_array.dims, labels))\n205 \n206         pos_indexers, _ = remap_label_indexers(self.data_array, key)\n207         self.data_array[pos_indexers] = value\n208 \n209 \n210 # Used as the key corresponding to a DataArray's variable when converting\n211 # arbitrary DataArray objects to datasets\n212 _THIS_ARRAY = ReprObject(\"<this-array>\")\n213 \n214 \n215 class DataArray(AbstractArray, DataWithCoords):\n216     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n217 \n218     DataArray provides a wrapper around numpy ndarrays that uses labeled\n219     dimensions and coordinates to support metadata aware operations. The API is\n220     similar to that for the pandas Series or DataFrame, but DataArray objects\n221     can have any number of dimensions, and their contents have fixed data\n222     types.\n223 \n224     Additional features over raw numpy arrays:\n225 \n226     - Apply operations over dimensions by name: ``x.sum('time')``.\n227     - Select or assign values by integer location (like numpy): ``x[:10]``\n228       or by label (like pandas): ``x.loc['2014-01-01']`` or\n229       ``x.sel(time='2014-01-01')``.\n230     - Mathematical operations (e.g., ``x - y``) vectorize across multiple\n231       dimensions (known in numpy as \"broadcasting\") based on dimension names,\n232       regardless of their original order.\n233     - Keep track of arbitrary metadata in the form of a Python dictionary:\n234       ``x.attrs``\n235     - Convert to a pandas Series: ``x.to_series()``.\n236 \n237     Getting items from or doing mathematical operations with a DataArray\n238     always returns another DataArray.\n239     \"\"\"\n240 \n241     _cache: Dict[str, Any]\n242     _coords: Dict[Any, Variable]\n243     _indexes: Optional[Dict[Hashable, pd.Index]]\n244     _name: Optional[Hashable]\n245     _variable: Variable\n246 \n247     __slots__ = (\n248         \"_cache\",\n249         \"_coords\",\n250         \"_file_obj\",\n251         \"_indexes\",\n252         \"_name\",\n253         \"_variable\",\n254         \"__weakref__\",\n255     )\n256 \n257     _groupby_cls = groupby.DataArrayGroupBy\n258     _rolling_cls = rolling.DataArrayRolling\n259     _coarsen_cls = rolling.DataArrayCoarsen\n260     _resample_cls = resample.DataArrayResample\n261     _weighted_cls = weighted.DataArrayWeighted\n262 \n263     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)\n264 \n265     def __init__(\n266         self,\n267         data: Any = dtypes.NA,\n268         coords: Union[Sequence[Tuple], Mapping[Hashable, Any], None] = None,\n269         dims: Union[Hashable, Sequence[Hashable], None] = None,\n270         name: Hashable = None,\n271         attrs: Mapping = None,\n272         # internal parameters\n273         indexes: Dict[Hashable, pd.Index] = None,\n274         fastpath: bool = False,\n275     ):\n276         \"\"\"\n277         Parameters\n278         ----------\n279         data : array_like\n280             Values for this array. Must be an ``numpy.ndarray``, ndarray like,\n281             or castable to an ``ndarray``. If a self-described xarray or pandas\n282             object, attempts are made to use this array's metadata to fill in\n283             other unspecified arguments. A view of the array's data is used\n284             instead of a copy if possible.\n285         coords : sequence or dict of array_like objects, optional\n286             Coordinates (tick labels) to use for indexing along each dimension.\n287             The following notations are accepted:\n288 \n289             - mapping {dimension name: array-like}\n290             - sequence of tuples that are valid arguments for xarray.Variable()\n291               - (dims, data)\n292               - (dims, data, attrs)\n293               - (dims, data, attrs, encoding)\n294 \n295             Additionally, it is possible to define a coord whose name\n296             does not match the dimension name, or a coord based on multiple\n297             dimensions, with one of the following notations:\n298 \n299             - mapping {coord name: DataArray}\n300             - mapping {coord name: Variable}\n301             - mapping {coord name: (dimension name, array-like)}\n302             - mapping {coord name: (tuple of dimension names, array-like)}\n303 \n304         dims : hashable or sequence of hashable, optional\n305             Name(s) of the data dimension(s). Must be either a hashable (only\n306             for 1D data) or a sequence of hashables with length equal to the\n307             number of dimensions. If this argument is omitted, dimension names\n308             default to ``['dim_0', ... 'dim_n']``.\n309         name : str or None, optional\n310             Name of this array.\n311         attrs : dict_like or None, optional\n312             Attributes to assign to the new instance. By default, an empty\n313             attribute dictionary is initialized.\n314         \"\"\"\n315         if fastpath:\n316             variable = data\n317             assert dims is None\n318             assert attrs is None\n319         else:\n320             # try to fill in arguments from data if they weren't supplied\n321             if coords is None:\n322 \n323                 if isinstance(data, DataArray):\n324                     coords = data.coords\n325                 elif isinstance(data, pd.Series):\n326                     coords = [data.index]\n327                 elif isinstance(data, pd.DataFrame):\n328                     coords = [data.index, data.columns]\n329                 elif isinstance(data, (pd.Index, IndexVariable)):\n330                     coords = [data]\n331                 elif isinstance(data, pdcompat.Panel):\n332                     coords = [data.items, data.major_axis, data.minor_axis]\n333 \n334             if dims is None:\n335                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n336             if name is None:\n337                 name = getattr(data, \"name\", None)\n338             if attrs is None and not isinstance(data, PANDAS_TYPES):\n339                 attrs = getattr(data, \"attrs\", None)\n340 \n341             data = _check_data_shape(data, coords, dims)\n342             data = as_compatible_data(data)\n343             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n344             variable = Variable(dims, data, attrs, fastpath=True)\n345             indexes = dict(\n346                 _extract_indexes_from_coords(coords)\n347             )  # needed for to_dataset\n348 \n349         # These fully describe a DataArray\n350         self._variable = variable\n351         assert isinstance(coords, dict)\n352         self._coords = coords\n353         self._name = name\n354 \n355         # TODO(shoyer): document this argument, once it becomes part of the\n356         # public interface.\n357         self._indexes = indexes\n358 \n359         self._file_obj = None\n360 \n361     def _replace(\n362         self,\n363         variable: Variable = None,\n364         coords=None,\n365         name: Union[Hashable, None, Default] = _default,\n366         indexes=None,\n367     ) -> \"DataArray\":\n368         if variable is None:\n369             variable = self.variable\n370         if coords is None:\n371             coords = self._coords\n372         if name is _default:\n373             name = self.name\n374         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)\n375 \n376     def _replace_maybe_drop_dims(\n377         self, variable: Variable, name: Union[Hashable, None, Default] = _default\n378     ) -> \"DataArray\":\n379         if variable.dims == self.dims and variable.shape == self.shape:\n380             coords = self._coords.copy()\n381             indexes = self._indexes\n382         elif variable.dims == self.dims:\n383             # Shape has changed (e.g. from reduce(..., keepdims=True)\n384             new_sizes = dict(zip(self.dims, variable.shape))\n385             coords = {\n386                 k: v\n387                 for k, v in self._coords.items()\n388                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n389             }\n390             changed_dims = [\n391                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]\n392             ]\n393             indexes = propagate_indexes(self._indexes, exclude=changed_dims)\n394         else:\n395             allowed_dims = set(variable.dims)\n396             coords = {\n397                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n398             }\n399             indexes = propagate_indexes(\n400                 self._indexes, exclude=(set(self.dims) - allowed_dims)\n401             )\n402         return self._replace(variable, coords, name, indexes=indexes)\n403 \n404     def _overwrite_indexes(self, indexes: Mapping[Hashable, Any]) -> \"DataArray\":\n405         if not len(indexes):\n406             return self\n407         coords = self._coords.copy()\n408         for name, idx in indexes.items():\n409             coords[name] = IndexVariable(name, idx)\n410         obj = self._replace(coords=coords)\n411 \n412         # switch from dimension to level names, if necessary\n413         dim_names: Dict[Any, str] = {}\n414         for dim, idx in indexes.items():\n415             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:\n416                 dim_names[dim] = idx.name\n417         if dim_names:\n418             obj = obj.rename(dim_names)\n419         return obj\n420 \n421     def _to_temp_dataset(self) -> Dataset:\n422         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n423 \n424     def _from_temp_dataset(\n425         self, dataset: Dataset, name: Hashable = _default\n426     ) -> \"DataArray\":\n427         variable = dataset._variables.pop(_THIS_ARRAY)\n428         coords = dataset._variables\n429         indexes = dataset._indexes\n430         return self._replace(variable, coords, name, indexes=indexes)\n431 \n432     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n433         \"\"\" splits dataarray along dimension 'dim' \"\"\"\n434 \n435         def subset(dim, label):\n436             array = self.loc[{dim: label}]\n437             array.attrs = {}\n438             return as_variable(array)\n439 \n440         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n441         variables.update({k: v for k, v in self._coords.items() if k != dim})\n442         indexes = propagate_indexes(self._indexes, exclude=dim)\n443         coord_names = set(self._coords) - set([dim])\n444         dataset = Dataset._construct_direct(\n445             variables, coord_names, indexes=indexes, attrs=self.attrs\n446         )\n447         return dataset\n448 \n449     def _to_dataset_whole(\n450         self, name: Hashable = None, shallow_copy: bool = True\n451     ) -> Dataset:\n452         if name is None:\n453             name = self.name\n454         if name is None:\n455             raise ValueError(\n456                 \"unable to convert unnamed DataArray to a \"\n457                 \"Dataset without providing an explicit name\"\n458             )\n459         if name in self.coords:\n460             raise ValueError(\n461                 \"cannot create a Dataset from a DataArray with \"\n462                 \"the same name as one of its coordinates\"\n463             )\n464         # use private APIs for speed: this is called by _to_temp_dataset(),\n465         # which is used in the guts of a lot of operations (e.g., reindex)\n466         variables = self._coords.copy()\n467         variables[name] = self.variable\n468         if shallow_copy:\n469             for k in variables:\n470                 variables[k] = variables[k].copy(deep=False)\n471         indexes = self._indexes\n472 \n473         coord_names = set(self._coords)\n474         dataset = Dataset._construct_direct(variables, coord_names, indexes=indexes)\n475         return dataset\n476 \n477     def to_dataset(\n478         self,\n479         dim: Hashable = None,\n480         *,\n481         name: Hashable = None,\n482         promote_attrs: bool = False,\n483     ) -> Dataset:\n484         \"\"\"Convert a DataArray to a Dataset.\n485 \n486         Parameters\n487         ----------\n488         dim : hashable, optional\n489             Name of the dimension on this array along which to split this array\n490             into separate variables. If not provided, this array is converted\n491             into a Dataset of one variable.\n492         name : hashable, optional\n493             Name to substitute for this array's name. Only valid if ``dim`` is\n494             not provided.\n495         promote_attrs : bool, default False\n496             Set to True to shallow copy attrs of DataArray to returned Dataset.\n497 \n498         Returns\n499         -------\n500         dataset : Dataset\n501         \"\"\"\n502         if dim is not None and dim not in self.dims:\n503             raise TypeError(\n504                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n505             )\n506 \n507         if dim is not None:\n508             if name is not None:\n509                 raise TypeError(\"cannot supply both dim and name arguments\")\n510             result = self._to_dataset_split(dim)\n511         else:\n512             result = self._to_dataset_whole(name)\n513 \n514         if promote_attrs:\n515             result.attrs = dict(self.attrs)\n516 \n517         return result\n518 \n519     @property\n520     def name(self) -> Optional[Hashable]:\n521         \"\"\"The name of this array.\n522         \"\"\"\n523         return self._name\n524 \n525     @name.setter\n526     def name(self, value: Optional[Hashable]) -> None:\n527         self._name = value\n528 \n529     @property\n530     def variable(self) -> Variable:\n531         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n532         return self._variable\n533 \n534     @property\n535     def dtype(self) -> np.dtype:\n536         return self.variable.dtype\n537 \n538     @property\n539     def shape(self) -> Tuple[int, ...]:\n540         return self.variable.shape\n541 \n542     @property\n543     def size(self) -> int:\n544         return self.variable.size\n545 \n546     @property\n547     def nbytes(self) -> int:\n548         return self.variable.nbytes\n549 \n550     @property\n551     def ndim(self) -> int:\n552         return self.variable.ndim\n553 \n554     def __len__(self) -> int:\n555         return len(self.variable)\n556 \n557     @property\n558     def data(self) -> Any:\n559         \"\"\"The array's data as a dask or numpy array\n560         \"\"\"\n561         return self.variable.data\n562 \n563     @data.setter\n564     def data(self, value: Any) -> None:\n565         self.variable.data = value\n566 \n567     @property\n568     def values(self) -> np.ndarray:\n569         \"\"\"The array's data as a numpy.ndarray\"\"\"\n570         return self.variable.values\n571 \n572     @values.setter\n573     def values(self, value: Any) -> None:\n574         self.variable.values = value\n575 \n576     @property\n577     def _in_memory(self) -> bool:\n578         return self.variable._in_memory\n579 \n580     def to_index(self) -> pd.Index:\n581         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n582         arrays.\n583         \"\"\"\n584         return self.variable.to_index()\n585 \n586     @property\n587     def dims(self) -> Tuple[Hashable, ...]:\n588         \"\"\"Tuple of dimension names associated with this array.\n589 \n590         Note that the type of this property is inconsistent with\n591         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n592         consistently named properties.\n593         \"\"\"\n594         return self.variable.dims\n595 \n596     @dims.setter\n597     def dims(self, value):\n598         raise AttributeError(\n599             \"you cannot assign dims on a DataArray. Use \"\n600             \".rename() or .swap_dims() instead.\"\n601         )\n602 \n603     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n604         if utils.is_dict_like(key):\n605             return key\n606         else:\n607             key = indexing.expanded_indexer(key, self.ndim)\n608             return dict(zip(self.dims, key))\n609 \n610     @property\n611     def _level_coords(self) -> Dict[Hashable, Hashable]:\n612         \"\"\"Return a mapping of all MultiIndex levels and their corresponding\n613         coordinate name.\n614         \"\"\"\n615         level_coords: Dict[Hashable, Hashable] = {}\n616 \n617         for cname, var in self._coords.items():\n618             if var.ndim == 1 and isinstance(var, IndexVariable):\n619                 level_names = var.level_names\n620                 if level_names is not None:\n621                     (dim,) = var.dims\n622                     level_coords.update({lname: dim for lname in level_names})\n623         return level_coords\n624 \n625     def _getitem_coord(self, key):\n626         from .dataset import _get_virtual_variable\n627 \n628         try:\n629             var = self._coords[key]\n630         except KeyError:\n631             dim_sizes = dict(zip(self.dims, self.shape))\n632             _, key, var = _get_virtual_variable(\n633                 self._coords, key, self._level_coords, dim_sizes\n634             )\n635 \n636         return self._replace_maybe_drop_dims(var, name=key)\n637 \n638     def __getitem__(self, key: Any) -> \"DataArray\":\n639         if isinstance(key, str):\n640             return self._getitem_coord(key)\n641         else:\n642             # xarray-style array indexing\n643             return self.isel(indexers=self._item_key_to_dict(key))\n644 \n645     def __setitem__(self, key: Any, value: Any) -> None:\n646         if isinstance(key, str):\n647             self.coords[key] = value\n648         else:\n649             # Coordinates in key, value and self[key] should be consistent.\n650             # TODO Coordinate consistency in key is checked here, but it\n651             # causes unnecessary indexing. It should be optimized.\n652             obj = self[key]\n653             if isinstance(value, DataArray):\n654                 assert_coordinate_consistent(value, obj.coords.variables)\n655             # DataArray key -> Variable key\n656             key = {\n657                 k: v.variable if isinstance(v, DataArray) else v\n658                 for k, v in self._item_key_to_dict(key).items()\n659             }\n660             self.variable[key] = value\n661 \n662     def __delitem__(self, key: Any) -> None:\n663         del self.coords[key]\n664 \n665     @property\n666     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:\n667         \"\"\"List of places to look-up items for attribute-style access\n668         \"\"\"\n669         return self._item_sources + [self.attrs]\n670 \n671     @property\n672     def _item_sources(self) -> List[Mapping[Hashable, Any]]:\n673         \"\"\"List of places to look-up items for key-completion\n674         \"\"\"\n675         return [\n676             self.coords,\n677             {d: self.coords[d] for d in self.dims},\n678             LevelCoordinatesSource(self),\n679         ]\n680 \n681     def __contains__(self, key: Any) -> bool:\n682         return key in self.data\n683 \n684     @property\n685     def loc(self) -> _LocIndexer:\n686         \"\"\"Attribute for location based indexing like pandas.\n687         \"\"\"\n688         return _LocIndexer(self)\n689 \n690     @property\n691     def attrs(self) -> Dict[Hashable, Any]:\n692         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n693         return self.variable.attrs\n694 \n695     @attrs.setter\n696     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n697         # Disable type checking to work around mypy bug - see mypy#4167\n698         self.variable.attrs = value  # type: ignore\n699 \n700     @property\n701     def encoding(self) -> Dict[Hashable, Any]:\n702         \"\"\"Dictionary of format-specific settings for how this array should be\n703         serialized.\"\"\"\n704         return self.variable.encoding\n705 \n706     @encoding.setter\n707     def encoding(self, value: Mapping[Hashable, Any]) -> None:\n708         self.variable.encoding = value\n709 \n710     @property\n711     def indexes(self) -> Indexes:\n712         \"\"\"Mapping of pandas.Index objects used for label based indexing\n713         \"\"\"\n714         if self._indexes is None:\n715             self._indexes = default_indexes(self._coords, self.dims)\n716         return Indexes(self._indexes)\n717 \n718     @property\n719     def coords(self) -> DataArrayCoordinates:\n720         \"\"\"Dictionary-like container of coordinate arrays.\n721         \"\"\"\n722         return DataArrayCoordinates(self)\n723 \n724     def reset_coords(\n725         self,\n726         names: Union[Iterable[Hashable], Hashable, None] = None,\n727         drop: bool = False,\n728         inplace: bool = None,\n729     ) -> Union[None, \"DataArray\", Dataset]:\n730         \"\"\"Given names of coordinates, reset them to become variables.\n731 \n732         Parameters\n733         ----------\n734         names : hashable or iterable of hashables, optional\n735             Name(s) of non-index coordinates in this dataset to reset into\n736             variables. By default, all non-index coordinates are reset.\n737         drop : bool, optional\n738             If True, remove coordinates instead of converting them into\n739             variables.\n740 \n741         Returns\n742         -------\n743         Dataset, or DataArray if ``drop == True``\n744         \"\"\"\n745         _check_inplace(inplace)\n746         if names is None:\n747             names = set(self.coords) - set(self.dims)\n748         dataset = self.coords.to_dataset().reset_coords(names, drop)\n749         if drop:\n750             return self._replace(coords=dataset._variables)\n751         else:\n752             if self.name is None:\n753                 raise ValueError(\n754                     \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n755                 )\n756             dataset[self.name] = self.variable\n757             return dataset\n758 \n759     def __dask_tokenize__(self):\n760         from dask.base import normalize_token\n761 \n762         return normalize_token((type(self), self._variable, self._coords, self._name))\n763 \n764     def __dask_graph__(self):\n765         return self._to_temp_dataset().__dask_graph__()\n766 \n767     def __dask_keys__(self):\n768         return self._to_temp_dataset().__dask_keys__()\n769 \n770     def __dask_layers__(self):\n771         return self._to_temp_dataset().__dask_layers__()\n772 \n773     @property\n774     def __dask_optimize__(self):\n775         return self._to_temp_dataset().__dask_optimize__\n776 \n777     @property\n778     def __dask_scheduler__(self):\n779         return self._to_temp_dataset().__dask_scheduler__\n780 \n781     def __dask_postcompute__(self):\n782         func, args = self._to_temp_dataset().__dask_postcompute__()\n783         return self._dask_finalize, (func, args, self.name)\n784 \n785     def __dask_postpersist__(self):\n786         func, args = self._to_temp_dataset().__dask_postpersist__()\n787         return self._dask_finalize, (func, args, self.name)\n788 \n789     @staticmethod\n790     def _dask_finalize(results, func, args, name):\n791         ds = func(results, *args)\n792         variable = ds._variables.pop(_THIS_ARRAY)\n793         coords = ds._variables\n794         return DataArray(variable, coords, name=name, fastpath=True)\n795 \n796     def load(self, **kwargs) -> \"DataArray\":\n797         \"\"\"Manually trigger loading of this array's data from disk or a\n798         remote source into memory and return this array.\n799 \n800         Normally, it should not be necessary to call this method in user code,\n801         because all xarray functions should either work on deferred data or\n802         load data automatically. However, this method can be necessary when\n803         working with many file objects on disk.\n804 \n805         Parameters\n806         ----------\n807         **kwargs : dict\n808             Additional keyword arguments passed on to ``dask.array.compute``.\n809 \n810         See Also\n811         --------\n812         dask.array.compute\n813         \"\"\"\n814         ds = self._to_temp_dataset().load(**kwargs)\n815         new = self._from_temp_dataset(ds)\n816         self._variable = new._variable\n817         self._coords = new._coords\n818         return self\n819 \n820     def compute(self, **kwargs) -> \"DataArray\":\n821         \"\"\"Manually trigger loading of this array's data from disk or a\n822         remote source into memory and return a new array. The original is\n823         left unaltered.\n824 \n825         Normally, it should not be necessary to call this method in user code,\n826         because all xarray functions should either work on deferred data or\n827         load data automatically. However, this method can be necessary when\n828         working with many file objects on disk.\n829 \n830         Parameters\n831         ----------\n832         **kwargs : dict\n833             Additional keyword arguments passed on to ``dask.array.compute``.\n834 \n835         See Also\n836         --------\n837         dask.array.compute\n838         \"\"\"\n839         new = self.copy(deep=False)\n840         return new.load(**kwargs)\n841 \n842     def persist(self, **kwargs) -> \"DataArray\":\n843         \"\"\" Trigger computation in constituent dask arrays\n844 \n845         This keeps them as dask arrays but encourages them to keep data in\n846         memory.  This is particularly useful when on a distributed machine.\n847         When on a single machine consider using ``.compute()`` instead.\n848 \n849         Parameters\n850         ----------\n851         **kwargs : dict\n852             Additional keyword arguments passed on to ``dask.persist``.\n853 \n854         See Also\n855         --------\n856         dask.persist\n857         \"\"\"\n858         ds = self._to_temp_dataset().persist(**kwargs)\n859         return self._from_temp_dataset(ds)\n860 \n861     def copy(self, deep: bool = True, data: Any = None) -> \"DataArray\":\n862         \"\"\"Returns a copy of this array.\n863 \n864         If `deep=True`, a deep copy is made of the data array.\n865         Otherwise, a shallow copy is made, so each variable in the new\n866         array's dataset is also a variable in this array's dataset.\n867 \n868         Use `data` to create a new object with the same structure as\n869         original but entirely new data.\n870 \n871         Parameters\n872         ----------\n873         deep : bool, optional\n874             Whether the data array and its coordinates are loaded into memory\n875             and copied onto the new object. Default is True.\n876         data : array_like, optional\n877             Data to use in the new object. Must have same shape as original.\n878             When `data` is used, `deep` is ignored for all data variables,\n879             and only used for coords.\n880 \n881         Returns\n882         -------\n883         object : DataArray\n884             New object with dimensions, attributes, coordinates, name,\n885             encoding, and optionally data copied from original.\n886 \n887         Examples\n888         --------\n889 \n890         Shallow versus deep copy\n891 \n892         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n893         >>> array.copy()\n894         <xarray.DataArray (x: 3)>\n895         array([1, 2, 3])\n896         Coordinates:\n897         * x        (x) <U1 'a' 'b' 'c'\n898         >>> array_0 = array.copy(deep=False)\n899         >>> array_0[0] = 7\n900         >>> array_0\n901         <xarray.DataArray (x: 3)>\n902         array([7, 2, 3])\n903         Coordinates:\n904         * x        (x) <U1 'a' 'b' 'c'\n905         >>> array\n906         <xarray.DataArray (x: 3)>\n907         array([7, 2, 3])\n908         Coordinates:\n909         * x        (x) <U1 'a' 'b' 'c'\n910 \n911         Changing the data using the ``data`` argument maintains the\n912         structure of the original object, but with the new data. Original\n913         object is unaffected.\n914 \n915         >>> array.copy(data=[0.1, 0.2, 0.3])\n916         <xarray.DataArray (x: 3)>\n917         array([ 0.1,  0.2,  0.3])\n918         Coordinates:\n919         * x        (x) <U1 'a' 'b' 'c'\n920         >>> array\n921         <xarray.DataArray (x: 3)>\n922         array([1, 2, 3])\n923         Coordinates:\n924         * x        (x) <U1 'a' 'b' 'c'\n925 \n926         See Also\n927         --------\n928         pandas.DataFrame.copy\n929         \"\"\"\n930         variable = self.variable.copy(deep=deep, data=data)\n931         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}\n932         if self._indexes is None:\n933             indexes = self._indexes\n934         else:\n935             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}\n936         return self._replace(variable, coords, indexes=indexes)\n937 \n938     def __copy__(self) -> \"DataArray\":\n939         return self.copy(deep=False)\n940 \n941     def __deepcopy__(self, memo=None) -> \"DataArray\":\n942         # memo does nothing but is required for compatibility with\n943         # copy.deepcopy\n944         return self.copy(deep=True)\n945 \n946     # mutable objects should not be hashable\n947     # https://github.com/python/mypy/issues/4266\n948     __hash__ = None  # type: ignore\n949 \n950     @property\n951     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:\n952         \"\"\"Block dimensions for this array's data or None if it's not a dask\n953         array.\n954         \"\"\"\n955         return self.variable.chunks\n956 \n957     def chunk(\n958         self,\n959         chunks: Union[\n960             None,\n961             Number,\n962             Tuple[Number, ...],\n963             Tuple[Tuple[Number, ...], ...],\n964             Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]],\n965         ] = None,\n966         name_prefix: str = \"xarray-\",\n967         token: str = None,\n968         lock: bool = False,\n969     ) -> \"DataArray\":\n970         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n971 \n972         If this variable is a non-dask array, it will be converted to dask\n973         array. If it's a dask array, it will be rechunked to the given chunk\n974         sizes.\n975 \n976         If neither chunks is not provided for one or more dimensions, chunk\n977         sizes along that dimension will not be updated; non-dask arrays will be\n978         converted into dask arrays with a single block.\n979 \n980         Parameters\n981         ----------\n982         chunks : int, tuple or mapping, optional\n983             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n984             ``{'x': 5, 'y': 5}``.\n985         name_prefix : str, optional\n986             Prefix for the name of the new dask array.\n987         token : str, optional\n988             Token uniquely identifying this array.\n989         lock : optional\n990             Passed on to :py:func:`dask.array.from_array`, if the array is not\n991             already as dask array.\n992 \n993         Returns\n994         -------\n995         chunked : xarray.DataArray\n996         \"\"\"\n997         if isinstance(chunks, (tuple, list)):\n998             chunks = dict(zip(self.dims, chunks))\n999 \n1000         ds = self._to_temp_dataset().chunk(\n1001             chunks, name_prefix=name_prefix, token=token, lock=lock\n1002         )\n1003         return self._from_temp_dataset(ds)\n1004 \n1005     def isel(\n1006         self,\n1007         indexers: Mapping[Hashable, Any] = None,\n1008         drop: bool = False,\n1009         missing_dims: str = \"raise\",\n1010         **indexers_kwargs: Any,\n1011     ) -> \"DataArray\":\n1012         \"\"\"Return a new DataArray whose data is given by integer indexing\n1013         along the specified dimension(s).\n1014 \n1015         Parameters\n1016         ----------\n1017         indexers : dict, optional\n1018             A dict with keys matching dimensions and values given\n1019             by integers, slice objects or arrays.\n1020             indexer can be a integer, slice, array-like or DataArray.\n1021             If DataArrays are passed as indexers, xarray-style indexing will be\n1022             carried out. See :ref:`indexing` for the details.\n1023             One of indexers or indexers_kwargs must be provided.\n1024         drop : bool, optional\n1025             If ``drop=True``, drop coordinates variables indexed by integers\n1026             instead of making them scalar.\n1027         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1028             What to do if dimensions that should be selected from are not present in the\n1029             DataArray:\n1030             - \"exception\": raise an exception\n1031             - \"warning\": raise a warning, and ignore the missing dimensions\n1032             - \"ignore\": ignore the missing dimensions\n1033         **indexers_kwargs : {dim: indexer, ...}, optional\n1034             The keyword arguments form of ``indexers``.\n1035 \n1036         See Also\n1037         --------\n1038         Dataset.isel\n1039         DataArray.sel\n1040         \"\"\"\n1041 \n1042         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1043 \n1044         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1045             ds = self._to_temp_dataset()._isel_fancy(\n1046                 indexers, drop=drop, missing_dims=missing_dims\n1047             )\n1048             return self._from_temp_dataset(ds)\n1049 \n1050         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1051         # lists, or zero or one-dimensional np.ndarray's\n1052 \n1053         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1054 \n1055         coords = {}\n1056         for coord_name, coord_value in self._coords.items():\n1057             coord_indexers = {\n1058                 k: v for k, v in indexers.items() if k in coord_value.dims\n1059             }\n1060             if coord_indexers:\n1061                 coord_value = coord_value.isel(coord_indexers)\n1062                 if drop and coord_value.ndim == 0:\n1063                     continue\n1064             coords[coord_name] = coord_value\n1065 \n1066         return self._replace(variable=variable, coords=coords)\n1067 \n1068     def sel(\n1069         self,\n1070         indexers: Mapping[Hashable, Any] = None,\n1071         method: str = None,\n1072         tolerance=None,\n1073         drop: bool = False,\n1074         **indexers_kwargs: Any,\n1075     ) -> \"DataArray\":\n1076         \"\"\"Return a new DataArray whose data is given by selecting index\n1077         labels along the specified dimension(s).\n1078 \n1079         In contrast to `DataArray.isel`, indexers for this method should use\n1080         labels instead of integers.\n1081 \n1082         Under the hood, this method is powered by using pandas's powerful Index\n1083         objects. This makes label based indexing essentially just as fast as\n1084         using integer indexing.\n1085 \n1086         It also means this method uses pandas's (well documented) logic for\n1087         indexing. This means you can use string shortcuts for datetime indexes\n1088         (e.g., '2000-01' to select all values in January 2000). It also means\n1089         that slices are treated as inclusive of both the start and stop values,\n1090         unlike normal Python indexing.\n1091 \n1092         .. warning::\n1093 \n1094           Do not try to assign values when using any of the indexing methods\n1095           ``isel`` or ``sel``::\n1096 \n1097             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1098             # DO NOT do this\n1099             da.isel(x=[0, 1, 2])[1] = -1\n1100 \n1101           Assigning values with the chained indexing using ``.sel`` or\n1102           ``.isel`` fails silently.\n1103 \n1104         Parameters\n1105         ----------\n1106         indexers : dict, optional\n1107             A dict with keys matching dimensions and values given\n1108             by scalars, slices or arrays of tick labels. For dimensions with\n1109             multi-index, the indexer may also be a dict-like object with keys\n1110             matching index level names.\n1111             If DataArrays are passed as indexers, xarray-style indexing will be\n1112             carried out. See :ref:`indexing` for the details.\n1113             One of indexers or indexers_kwargs must be provided.\n1114         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1115             Method to use for inexact matches:\n1116 \n1117             * None (default): only exact matches\n1118             * pad / ffill: propagate last valid index value forward\n1119             * backfill / bfill: propagate next valid index value backward\n1120             * nearest: use nearest valid index value\n1121         tolerance : optional\n1122             Maximum distance between original and new labels for inexact\n1123             matches. The values of the index at the matching locations must\n1124             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1125         drop : bool, optional\n1126             If ``drop=True``, drop coordinates variables in `indexers` instead\n1127             of making them scalar.\n1128         **indexers_kwargs : {dim: indexer, ...}, optional\n1129             The keyword arguments form of ``indexers``.\n1130             One of indexers or indexers_kwargs must be provided.\n1131 \n1132         Returns\n1133         -------\n1134         obj : DataArray\n1135             A new DataArray with the same contents as this DataArray, except the\n1136             data and each dimension is indexed by the appropriate indexers.\n1137             If indexer DataArrays have coordinates that do not conflict with\n1138             this object, then these coordinates will be attached.\n1139             In general, each array's data will be a view of the array's data\n1140             in this DataArray, unless vectorized indexing was triggered by using\n1141             an array indexer, in which case the data will be a copy.\n1142 \n1143         See Also\n1144         --------\n1145         Dataset.sel\n1146         DataArray.isel\n1147 \n1148         \"\"\"\n1149         ds = self._to_temp_dataset().sel(\n1150             indexers=indexers,\n1151             drop=drop,\n1152             method=method,\n1153             tolerance=tolerance,\n1154             **indexers_kwargs,\n1155         )\n1156         return self._from_temp_dataset(ds)\n1157 \n1158     def head(\n1159         self,\n1160         indexers: Union[Mapping[Hashable, int], int] = None,\n1161         **indexers_kwargs: Any,\n1162     ) -> \"DataArray\":\n1163         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1164         values along the specified dimension(s). Default `n` = 5\n1165 \n1166         See Also\n1167         --------\n1168         Dataset.head\n1169         DataArray.tail\n1170         DataArray.thin\n1171         \"\"\"\n1172         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1173         return self._from_temp_dataset(ds)\n1174 \n1175     def tail(\n1176         self,\n1177         indexers: Union[Mapping[Hashable, int], int] = None,\n1178         **indexers_kwargs: Any,\n1179     ) -> \"DataArray\":\n1180         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1181         values along the specified dimension(s). Default `n` = 5\n1182 \n1183         See Also\n1184         --------\n1185         Dataset.tail\n1186         DataArray.head\n1187         DataArray.thin\n1188         \"\"\"\n1189         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1190         return self._from_temp_dataset(ds)\n1191 \n1192     def thin(\n1193         self,\n1194         indexers: Union[Mapping[Hashable, int], int] = None,\n1195         **indexers_kwargs: Any,\n1196     ) -> \"DataArray\":\n1197         \"\"\"Return a new DataArray whose data is given by each `n` value\n1198         along the specified dimension(s).\n1199 \n1200         See Also\n1201         --------\n1202         Dataset.thin\n1203         DataArray.head\n1204         DataArray.tail\n1205         \"\"\"\n1206         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1207         return self._from_temp_dataset(ds)\n1208 \n1209     def broadcast_like(\n1210         self, other: Union[\"DataArray\", Dataset], exclude: Iterable[Hashable] = None\n1211     ) -> \"DataArray\":\n1212         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1213 \n1214         This is equivalent to xr.broadcast(other, self)[1]\n1215 \n1216         xarray objects are broadcast against each other in arithmetic\n1217         operations, so this method is not be necessary for most uses.\n1218 \n1219         If no change is needed, the input data is returned to the output\n1220         without being copied.\n1221 \n1222         If new coords are added by the broadcast, their values are\n1223         NaN filled.\n1224 \n1225         Parameters\n1226         ----------\n1227         other : Dataset or DataArray\n1228             Object against which to broadcast this array.\n1229         exclude : iterable of hashable, optional\n1230             Dimensions that must not be broadcasted\n1231 \n1232         Returns\n1233         -------\n1234         new_da: xr.DataArray\n1235 \n1236         Examples\n1237         --------\n1238 \n1239         >>> arr1\n1240         <xarray.DataArray (x: 2, y: 3)>\n1241         array([[0.840235, 0.215216, 0.77917 ],\n1242                [0.726351, 0.543824, 0.875115]])\n1243         Coordinates:\n1244           * x        (x) <U1 'a' 'b'\n1245           * y        (y) <U1 'a' 'b' 'c'\n1246         >>> arr2\n1247         <xarray.DataArray (x: 3, y: 2)>\n1248         array([[0.612611, 0.125753],\n1249                [0.853181, 0.948818],\n1250                [0.180885, 0.33363 ]])\n1251         Coordinates:\n1252           * x        (x) <U1 'a' 'b' 'c'\n1253           * y        (y) <U1 'a' 'b'\n1254         >>> arr1.broadcast_like(arr2)\n1255         <xarray.DataArray (x: 3, y: 3)>\n1256         array([[0.840235, 0.215216, 0.77917 ],\n1257                [0.726351, 0.543824, 0.875115],\n1258                [     nan,      nan,      nan]])\n1259         Coordinates:\n1260           * x        (x) object 'a' 'b' 'c'\n1261           * y        (y) object 'a' 'b' 'c'\n1262         \"\"\"\n1263         if exclude is None:\n1264             exclude = set()\n1265         else:\n1266             exclude = set(exclude)\n1267         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1268 \n1269         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1270 \n1271         return _broadcast_helper(args[1], exclude, dims_map, common_coords)\n1272 \n1273     def reindex_like(\n1274         self,\n1275         other: Union[\"DataArray\", Dataset],\n1276         method: str = None,\n1277         tolerance=None,\n1278         copy: bool = True,\n1279         fill_value=dtypes.NA,\n1280     ) -> \"DataArray\":\n1281         \"\"\"Conform this object onto the indexes of another object, filling in\n1282         missing values with ``fill_value``. The default fill value is NaN.\n1283 \n1284         Parameters\n1285         ----------\n1286         other : Dataset or DataArray\n1287             Object with an 'indexes' attribute giving a mapping from dimension\n1288             names to pandas.Index objects, which provides coordinates upon\n1289             which to index the variables in this dataset. The indexes on this\n1290             other object need not be the same as the indexes on this\n1291             dataset. Any mis-matched index values will be filled in with\n1292             NaN, and any mis-matched dimension names will simply be ignored.\n1293         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1294             Method to use for filling index values from other not found on this\n1295             data array:\n1296 \n1297             * None (default): don't fill gaps\n1298             * pad / ffill: propagate last valid index value forward\n1299             * backfill / bfill: propagate next valid index value backward\n1300             * nearest: use nearest valid index value\n1301         tolerance : optional\n1302             Maximum distance between original and new labels for inexact\n1303             matches. The values of the index at the matching locations must\n1304             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1305         copy : bool, optional\n1306             If ``copy=True``, data in the return value is always copied. If\n1307             ``copy=False`` and reindexing is unnecessary, or can be performed\n1308             with only slice operations, then the output may share memory with\n1309             the input. In either case, a new xarray object is always returned.\n1310         fill_value : scalar, optional\n1311             Value to use for newly missing values\n1312 \n1313         Returns\n1314         -------\n1315         reindexed : DataArray\n1316             Another dataset array, with this array's data but coordinates from\n1317             the other object.\n1318 \n1319         See Also\n1320         --------\n1321         DataArray.reindex\n1322         align\n1323         \"\"\"\n1324         indexers = reindex_like_indexers(self, other)\n1325         return self.reindex(\n1326             indexers=indexers,\n1327             method=method,\n1328             tolerance=tolerance,\n1329             copy=copy,\n1330             fill_value=fill_value,\n1331         )\n1332 \n1333     def reindex(\n1334         self,\n1335         indexers: Mapping[Hashable, Any] = None,\n1336         method: str = None,\n1337         tolerance=None,\n1338         copy: bool = True,\n1339         fill_value=dtypes.NA,\n1340         **indexers_kwargs: Any,\n1341     ) -> \"DataArray\":\n1342         \"\"\"Conform this object onto the indexes of another object, filling in\n1343         missing values with ``fill_value``. The default fill value is NaN.\n1344 \n1345         Parameters\n1346         ----------\n1347         indexers : dict, optional\n1348             Dictionary with keys given by dimension names and values given by\n1349             arrays of coordinates tick labels. Any mis-matched coordinate\n1350             values will be filled in with NaN, and any mis-matched dimension\n1351             names will simply be ignored.\n1352             One of indexers or indexers_kwargs must be provided.\n1353         copy : bool, optional\n1354             If ``copy=True``, data in the return value is always copied. If\n1355             ``copy=False`` and reindexing is unnecessary, or can be performed\n1356             with only slice operations, then the output may share memory with\n1357             the input. In either case, a new xarray object is always returned.\n1358         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1359             Method to use for filling index values in ``indexers`` not found on\n1360             this data array:\n1361 \n1362             * None (default): don't fill gaps\n1363             * pad / ffill: propagate last valid index value forward\n1364             * backfill / bfill: propagate next valid index value backward\n1365             * nearest: use nearest valid index value\n1366         tolerance : optional\n1367             Maximum distance between original and new labels for inexact\n1368             matches. The values of the index at the matching locations must\n1369             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1370         fill_value : scalar, optional\n1371             Value to use for newly missing values\n1372         **indexers_kwargs : {dim: indexer, ...}, optional\n1373             The keyword arguments form of ``indexers``.\n1374             One of indexers or indexers_kwargs must be provided.\n1375 \n1376         Returns\n1377         -------\n1378         reindexed : DataArray\n1379             Another dataset array, with this array's data but replaced\n1380             coordinates.\n1381 \n1382         See Also\n1383         --------\n1384         DataArray.reindex_like\n1385         align\n1386         \"\"\"\n1387         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1388         ds = self._to_temp_dataset().reindex(\n1389             indexers=indexers,\n1390             method=method,\n1391             tolerance=tolerance,\n1392             copy=copy,\n1393             fill_value=fill_value,\n1394         )\n1395         return self._from_temp_dataset(ds)\n1396 \n1397     def interp(\n1398         self,\n1399         coords: Mapping[Hashable, Any] = None,\n1400         method: str = \"linear\",\n1401         assume_sorted: bool = False,\n1402         kwargs: Mapping[str, Any] = None,\n1403         **coords_kwargs: Any,\n1404     ) -> \"DataArray\":\n1405         \"\"\" Multidimensional interpolation of variables.\n1406 \n1407         coords : dict, optional\n1408             Mapping from dimension names to the new coordinates.\n1409             new coordinate can be an scalar, array-like or DataArray.\n1410             If DataArrays are passed as new coordates, their dimensions are\n1411             used for the broadcasting.\n1412         method: {'linear', 'nearest'} for multidimensional array,\n1413             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1414             for 1-dimensional array.\n1415         assume_sorted: boolean, optional\n1416             If False, values of x can be in any order and they are sorted\n1417             first. If True, x has to be an array of monotonically increasing\n1418             values.\n1419         kwargs: dictionary\n1420             Additional keyword arguments passed to scipy's interpolator. Valid\n1421             options and their behavior depend on if 1-dimensional or\n1422             multi-dimensional interpolation is used.\n1423         ``**coords_kwargs`` : {dim: coordinate, ...}, optional\n1424             The keyword arguments form of ``coords``.\n1425             One of coords or coords_kwargs must be provided.\n1426 \n1427         Returns\n1428         -------\n1429         interpolated: xr.DataArray\n1430             New dataarray on the new coordinates.\n1431 \n1432         Notes\n1433         -----\n1434         scipy is required.\n1435 \n1436         See Also\n1437         --------\n1438         scipy.interpolate.interp1d\n1439         scipy.interpolate.interpn\n1440 \n1441         Examples\n1442         --------\n1443         >>> da = xr.DataArray([1, 3], [(\"x\", np.arange(2))])\n1444         >>> da.interp(x=0.5)\n1445         <xarray.DataArray ()>\n1446         array(2.0)\n1447         Coordinates:\n1448             x        float64 0.5\n1449         \"\"\"\n1450         if self.dtype.kind not in \"uifc\":\n1451             raise TypeError(\n1452                 \"interp only works for a numeric type array. \"\n1453                 \"Given {}.\".format(self.dtype)\n1454             )\n1455         ds = self._to_temp_dataset().interp(\n1456             coords,\n1457             method=method,\n1458             kwargs=kwargs,\n1459             assume_sorted=assume_sorted,\n1460             **coords_kwargs,\n1461         )\n1462         return self._from_temp_dataset(ds)\n1463 \n1464     def interp_like(\n1465         self,\n1466         other: Union[\"DataArray\", Dataset],\n1467         method: str = \"linear\",\n1468         assume_sorted: bool = False,\n1469         kwargs: Mapping[str, Any] = None,\n1470     ) -> \"DataArray\":\n1471         \"\"\"Interpolate this object onto the coordinates of another object,\n1472         filling out of range values with NaN.\n1473 \n1474         Parameters\n1475         ----------\n1476         other : Dataset or DataArray\n1477             Object with an 'indexes' attribute giving a mapping from dimension\n1478             names to an 1d array-like, which provides coordinates upon\n1479             which to index the variables in this dataset.\n1480         method: string, optional.\n1481             {'linear', 'nearest'} for multidimensional array,\n1482             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}\n1483             for 1-dimensional array. 'linear' is used by default.\n1484         assume_sorted: boolean, optional\n1485             If False, values of coordinates that are interpolated over can be\n1486             in any order and they are sorted first. If True, interpolated\n1487             coordinates are assumed to be an array of monotonically increasing\n1488             values.\n1489         kwargs: dictionary, optional\n1490             Additional keyword passed to scipy's interpolator.\n1491 \n1492         Returns\n1493         -------\n1494         interpolated: xr.DataArray\n1495             Another dataarray by interpolating this dataarray's data along the\n1496             coordinates of the other object.\n1497 \n1498         Notes\n1499         -----\n1500         scipy is required.\n1501         If the dataarray has object-type coordinates, reindex is used for these\n1502         coordinates instead of the interpolation.\n1503 \n1504         See Also\n1505         --------\n1506         DataArray.interp\n1507         DataArray.reindex_like\n1508         \"\"\"\n1509         if self.dtype.kind not in \"uifc\":\n1510             raise TypeError(\n1511                 \"interp only works for a numeric type array. \"\n1512                 \"Given {}.\".format(self.dtype)\n1513             )\n1514         ds = self._to_temp_dataset().interp_like(\n1515             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1516         )\n1517         return self._from_temp_dataset(ds)\n1518 \n1519     def rename(\n1520         self,\n1521         new_name_or_name_dict: Union[Hashable, Mapping[Hashable, Hashable]] = None,\n1522         **names: Hashable,\n1523     ) -> \"DataArray\":\n1524         \"\"\"Returns a new DataArray with renamed coordinates or a new name.\n1525 \n1526         Parameters\n1527         ----------\n1528         new_name_or_name_dict : str or dict-like, optional\n1529             If the argument is dict-like, it used as a mapping from old\n1530             names to new names for coordinates. Otherwise, use the argument\n1531             as the new name for this array.\n1532         **names: hashable, optional\n1533             The keyword arguments form of a mapping from old names to\n1534             new names for coordinates.\n1535             One of new_name_or_name_dict or names must be provided.\n1536 \n1537         Returns\n1538         -------\n1539         renamed : DataArray\n1540             Renamed array or array with renamed coordinates.\n1541 \n1542         See Also\n1543         --------\n1544         Dataset.rename\n1545         DataArray.swap_dims\n1546         \"\"\"\n1547         if names or utils.is_dict_like(new_name_or_name_dict):\n1548             new_name_or_name_dict = cast(\n1549                 Mapping[Hashable, Hashable], new_name_or_name_dict\n1550             )\n1551             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n1552             dataset = self._to_temp_dataset().rename(name_dict)\n1553             return self._from_temp_dataset(dataset)\n1554         else:\n1555             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)\n1556             return self._replace(name=new_name_or_name_dict)\n1557 \n1558     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> \"DataArray\":\n1559         \"\"\"Returns a new DataArray with swapped dimensions.\n1560 \n1561         Parameters\n1562         ----------\n1563         dims_dict : dict-like\n1564             Dictionary whose keys are current dimension names and whose values\n1565             are new names.\n1566 \n1567         Returns\n1568         -------\n1569         swapped : DataArray\n1570             DataArray with swapped dimensions.\n1571 \n1572         Examples\n1573         --------\n1574 \n1575         >>> arr = xr.DataArray(\n1576         ...     data=[0, 1], dims=\"x\", coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n1577         ... )\n1578         >>> arr\n1579         <xarray.DataArray (x: 2)>\n1580         array([0, 1])\n1581         Coordinates:\n1582           * x        (x) <U1 'a' 'b'\n1583             y        (x) int64 0 1\n1584 \n1585         >>> arr.swap_dims({\"x\": \"y\"})\n1586         <xarray.DataArray (y: 2)>\n1587         array([0, 1])\n1588         Coordinates:\n1589             x        (y) <U1 'a' 'b'\n1590           * y        (y) int64 0 1\n1591 \n1592         >>> arr.swap_dims({\"x\": \"z\"})\n1593         <xarray.DataArray (z: 2)>\n1594         array([0, 1])\n1595         Coordinates:\n1596             x        (z) <U1 'a' 'b'\n1597             y        (z) int64 0 1\n1598         Dimensions without coordinates: z\n1599 \n1600         See Also\n1601         --------\n1602 \n1603         DataArray.rename\n1604         Dataset.swap_dims\n1605         \"\"\"\n1606         ds = self._to_temp_dataset().swap_dims(dims_dict)\n1607         return self._from_temp_dataset(ds)\n1608 \n1609     def expand_dims(\n1610         self,\n1611         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,\n1612         axis=None,\n1613         **dim_kwargs: Any,\n1614     ) -> \"DataArray\":\n1615         \"\"\"Return a new object with an additional axis (or axes) inserted at\n1616         the corresponding position in the array shape. The new object is a\n1617         view into the underlying array, not a copy.\n1618 \n1619 \n1620         If dim is already a scalar coordinate, it will be promoted to a 1D\n1621         coordinate consisting of a single value.\n1622 \n1623         Parameters\n1624         ----------\n1625         dim : hashable, sequence of hashable, dict, or None\n1626             Dimensions to include on the new variable.\n1627             If provided as str or sequence of str, then dimensions are inserted\n1628             with length 1. If provided as a dict, then the keys are the new\n1629             dimensions and the values are either integers (giving the length of\n1630             the new dimensions) or sequence/ndarray (giving the coordinates of\n1631             the new dimensions).\n1632         axis : integer, list (or tuple) of integers, or None\n1633             Axis position(s) where new axis is to be inserted (position(s) on\n1634             the result array). If a list (or tuple) of integers is passed,\n1635             multiple axes are inserted. In this case, dim arguments should be\n1636             same length list. If axis=None is passed, all the axes will be\n1637             inserted to the start of the result array.\n1638         **dim_kwargs : int or sequence/ndarray\n1639             The keywords are arbitrary dimensions being inserted and the values\n1640             are either the lengths of the new dims (if int is given), or their\n1641             coordinates. Note, this is an alternative to passing a dict to the\n1642             dim kwarg and will only be used if dim is None.\n1643 \n1644         Returns\n1645         -------\n1646         expanded : same type as caller\n1647             This object, but with an additional dimension(s).\n1648         \"\"\"\n1649         if isinstance(dim, int):\n1650             raise TypeError(\"dim should be hashable or sequence/mapping of hashables\")\n1651         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n1652             if len(dim) != len(set(dim)):\n1653                 raise ValueError(\"dims should not contain duplicate values.\")\n1654             dim = dict.fromkeys(dim, 1)\n1655         elif dim is not None and not isinstance(dim, Mapping):\n1656             dim = {cast(Hashable, dim): 1}\n1657 \n1658         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n1659         ds = self._to_temp_dataset().expand_dims(dim, axis)\n1660         return self._from_temp_dataset(ds)\n1661 \n1662     def set_index(\n1663         self,\n1664         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,\n1665         append: bool = False,\n1666         inplace: bool = None,\n1667         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],\n1668     ) -> Optional[\"DataArray\"]:\n1669         \"\"\"Set DataArray (multi-)indexes using one or more existing\n1670         coordinates.\n1671 \n1672         Parameters\n1673         ----------\n1674         indexes : {dim: index, ...}\n1675             Mapping from names matching dimensions and values given\n1676             by (lists of) the names of existing coordinates or variables to set\n1677             as new (multi-)index.\n1678         append : bool, optional\n1679             If True, append the supplied index(es) to the existing index(es).\n1680             Otherwise replace the existing index(es) (default).\n1681         **indexes_kwargs: optional\n1682             The keyword arguments form of ``indexes``.\n1683             One of indexes or indexes_kwargs must be provided.\n1684 \n1685         Returns\n1686         -------\n1687         obj : DataArray\n1688             Another DataArray, with this data but replaced coordinates.\n1689 \n1690         Examples\n1691         --------\n1692         >>> arr = xr.DataArray(\n1693         ...     data=np.ones((2, 3)),\n1694         ...     dims=[\"x\", \"y\"],\n1695         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n1696         ... )\n1697         >>> arr\n1698         <xarray.DataArray (x: 2, y: 3)>\n1699         array([[1., 1., 1.],\n1700                [1., 1., 1.]])\n1701         Coordinates:\n1702           * x        (x) int64 0 1\n1703           * y        (y) int64 0 1 2\n1704             a        (x) int64 3 4\n1705         >>> arr.set_index(x=\"a\")\n1706         <xarray.DataArray (x: 2, y: 3)>\n1707         array([[1., 1., 1.],\n1708                [1., 1., 1.]])\n1709         Coordinates:\n1710           * x        (x) int64 3 4\n1711           * y        (y) int64 0 1 2\n1712 \n1713         See Also\n1714         --------\n1715         DataArray.reset_index\n1716         \"\"\"\n1717         ds = self._to_temp_dataset().set_index(\n1718             indexes, append=append, inplace=inplace, **indexes_kwargs\n1719         )\n1720         return self._from_temp_dataset(ds)\n1721 \n1722     def reset_index(\n1723         self,\n1724         dims_or_levels: Union[Hashable, Sequence[Hashable]],\n1725         drop: bool = False,\n1726         inplace: bool = None,\n1727     ) -> Optional[\"DataArray\"]:\n1728         \"\"\"Reset the specified index(es) or multi-index level(s).\n1729 \n1730         Parameters\n1731         ----------\n1732         dims_or_levels : hashable or sequence of hashables\n1733             Name(s) of the dimension(s) and/or multi-index level(s) that will\n1734             be reset.\n1735         drop : bool, optional\n1736             If True, remove the specified indexes and/or multi-index levels\n1737             instead of extracting them as new coordinates (default: False).\n1738 \n1739         Returns\n1740         -------\n1741         obj : DataArray\n1742             Another dataarray, with this dataarray's data but replaced\n1743             coordinates.\n1744 \n1745         See Also\n1746         --------\n1747         DataArray.set_index\n1748         \"\"\"\n1749         _check_inplace(inplace)\n1750         coords, _ = split_indexes(\n1751             dims_or_levels, self._coords, set(), self._level_coords, drop=drop\n1752         )\n1753         return self._replace(coords=coords)\n1754 \n1755     def reorder_levels(\n1756         self,\n1757         dim_order: Mapping[Hashable, Sequence[int]] = None,\n1758         inplace: bool = None,\n1759         **dim_order_kwargs: Sequence[int],\n1760     ) -> \"DataArray\":\n1761         \"\"\"Rearrange index levels using input order.\n1762 \n1763         Parameters\n1764         ----------\n1765         dim_order : optional\n1766             Mapping from names matching dimensions and values given\n1767             by lists representing new level orders. Every given dimension\n1768             must have a multi-index.\n1769         **dim_order_kwargs: optional\n1770             The keyword arguments form of ``dim_order``.\n1771             One of dim_order or dim_order_kwargs must be provided.\n1772 \n1773         Returns\n1774         -------\n1775         obj : DataArray\n1776             Another dataarray, with this dataarray's data but replaced\n1777             coordinates.\n1778         \"\"\"\n1779         _check_inplace(inplace)\n1780         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n1781         replace_coords = {}\n1782         for dim, order in dim_order.items():\n1783             coord = self._coords[dim]\n1784             index = coord.to_index()\n1785             if not isinstance(index, pd.MultiIndex):\n1786                 raise ValueError(\"coordinate %r has no MultiIndex\" % dim)\n1787             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))\n1788         coords = self._coords.copy()\n1789         coords.update(replace_coords)\n1790         return self._replace(coords=coords)\n1791 \n1792     def stack(\n1793         self,\n1794         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,\n1795         **dimensions_kwargs: Sequence[Hashable],\n1796     ) -> \"DataArray\":\n1797         \"\"\"\n1798         Stack any number of existing dimensions into a single new dimension.\n1799 \n1800         New dimensions will be added at the end, and the corresponding\n1801         coordinate variables will be combined into a MultiIndex.\n1802 \n1803         Parameters\n1804         ----------\n1805         dimensions : Mapping of the form new_name=(dim1, dim2, ...)\n1806             Names of new dimensions, and the existing dimensions that they\n1807             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n1808             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n1809             all dimensions.\n1810         **dimensions_kwargs:\n1811             The keyword arguments form of ``dimensions``.\n1812             One of dimensions or dimensions_kwargs must be provided.\n1813 \n1814         Returns\n1815         -------\n1816         stacked : DataArray\n1817             DataArray with stacked data.\n1818 \n1819         Examples\n1820         --------\n1821 \n1822         >>> arr = xr.DataArray(\n1823         ...     np.arange(6).reshape(2, 3),\n1824         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1825         ... )\n1826         >>> arr\n1827         <xarray.DataArray (x: 2, y: 3)>\n1828         array([[0, 1, 2],\n1829                [3, 4, 5]])\n1830         Coordinates:\n1831           * x        (x) |S1 'a' 'b'\n1832           * y        (y) int64 0 1 2\n1833         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1834         >>> stacked.indexes[\"z\"]\n1835         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1836                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1837                    names=['x', 'y'])\n1838 \n1839         See Also\n1840         --------\n1841         DataArray.unstack\n1842         \"\"\"\n1843         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)\n1844         return self._from_temp_dataset(ds)\n1845 \n1846     def unstack(\n1847         self,\n1848         dim: Union[Hashable, Sequence[Hashable], None] = None,\n1849         fill_value: Any = dtypes.NA,\n1850         sparse: bool = False,\n1851     ) -> \"DataArray\":\n1852         \"\"\"\n1853         Unstack existing dimensions corresponding to MultiIndexes into\n1854         multiple new dimensions.\n1855 \n1856         New dimensions will be added at the end.\n1857 \n1858         Parameters\n1859         ----------\n1860         dim : hashable or sequence of hashable, optional\n1861             Dimension(s) over which to unstack. By default unstacks all\n1862             MultiIndexes.\n1863         fill_value: value to be filled. By default, np.nan\n1864         sparse: use sparse-array if True\n1865 \n1866         Returns\n1867         -------\n1868         unstacked : DataArray\n1869             Array with unstacked data.\n1870 \n1871         Examples\n1872         --------\n1873 \n1874         >>> arr = xr.DataArray(\n1875         ...     np.arange(6).reshape(2, 3),\n1876         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1877         ... )\n1878         >>> arr\n1879         <xarray.DataArray (x: 2, y: 3)>\n1880         array([[0, 1, 2],\n1881                [3, 4, 5]])\n1882         Coordinates:\n1883           * x        (x) |S1 'a' 'b'\n1884           * y        (y) int64 0 1 2\n1885         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n1886         >>> stacked.indexes[\"z\"]\n1887         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1888                    codes=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]],\n1889                    names=['x', 'y'])\n1890         >>> roundtripped = stacked.unstack()\n1891         >>> arr.identical(roundtripped)\n1892         True\n1893 \n1894         See Also\n1895         --------\n1896         DataArray.stack\n1897         \"\"\"\n1898         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n1899         return self._from_temp_dataset(ds)\n1900 \n1901     def to_unstacked_dataset(self, dim, level=0):\n1902         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n1903         stacked coordinate.\n1904 \n1905         This is the inverse operation of Dataset.to_stacked_array.\n1906 \n1907         Parameters\n1908         ----------\n1909         dim : str\n1910             Name of existing dimension to unstack\n1911         level : int or str\n1912             The MultiIndex level to expand to a dataset along. Can either be\n1913             the integer index of the level or its name.\n1914         label : int, default 0\n1915             Label of the level to expand dataset along. Overrides the label\n1916             argument if given.\n1917 \n1918         Returns\n1919         -------\n1920         unstacked: Dataset\n1921 \n1922         Examples\n1923         --------\n1924         >>> import xarray as xr\n1925         >>> arr = xr.DataArray(\n1926         ...     np.arange(6).reshape(2, 3),\n1927         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n1928         ... )\n1929         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n1930         >>> data\n1931         <xarray.Dataset>\n1932         Dimensions:  (x: 2, y: 3)\n1933         Coordinates:\n1934           * x        (x) <U1 'a' 'b'\n1935           * y        (y) int64 0 1 2\n1936         Data variables:\n1937             a        (x, y) int64 0 1 2 3 4 5\n1938             b        (x) int64 0 3\n1939         >>> stacked = data.to_stacked_array(\"z\", [\"y\"])\n1940         >>> stacked.indexes[\"z\"]\n1941         MultiIndex(levels=[['a', 'b'], [0, 1, 2]],\n1942                 labels=[[0, 0, 0, 1], [0, 1, 2, -1]],\n1943                 names=['variable', 'y'])\n1944         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n1945         >>> data.identical(roundtripped)\n1946         True\n1947 \n1948         See Also\n1949         --------\n1950         Dataset.to_stacked_array\n1951         \"\"\"\n1952 \n1953         idx = self.indexes[dim]\n1954         if not isinstance(idx, pd.MultiIndex):\n1955             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n1956 \n1957         level_number = idx._get_level_number(level)\n1958         variables = idx.levels[level_number]\n1959         variable_dim = idx.names[level_number]\n1960 \n1961         # pull variables out of datarray\n1962         data_dict = {}\n1963         for k in variables:\n1964             data_dict[k] = self.sel({variable_dim: k}).squeeze(drop=True)\n1965 \n1966         # unstacked dataset\n1967         return Dataset(data_dict)\n1968 \n1969     def transpose(self, *dims: Hashable, transpose_coords: bool = True) -> \"DataArray\":\n1970         \"\"\"Return a new DataArray object with transposed dimensions.\n1971 \n1972         Parameters\n1973         ----------\n1974         *dims : hashable, optional\n1975             By default, reverse the dimensions. Otherwise, reorder the\n1976             dimensions to this order.\n1977         transpose_coords : boolean, default True\n1978             If True, also transpose the coordinates of this DataArray.\n1979 \n1980         Returns\n1981         -------\n1982         transposed : DataArray\n1983             The returned DataArray's array is transposed.\n1984 \n1985         Notes\n1986         -----\n1987         This operation returns a view of this array's data. It is\n1988         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n1989         -- the data will be fully loaded.\n1990 \n1991         See Also\n1992         --------\n1993         numpy.transpose\n1994         Dataset.transpose\n1995         \"\"\"\n1996         if dims:\n1997             dims = tuple(utils.infix_dims(dims, self.dims))\n1998         variable = self.variable.transpose(*dims)\n1999         if transpose_coords:\n2000             coords: Dict[Hashable, Variable] = {}\n2001             for name, coord in self.coords.items():\n2002                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2003                 coords[name] = coord.variable.transpose(*coord_dims)\n2004             return self._replace(variable, coords)\n2005         else:\n2006             return self._replace(variable)\n2007 \n2008     @property\n2009     def T(self) -> \"DataArray\":\n2010         return self.transpose()\n2011 \n2012     def drop_vars(\n2013         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = \"raise\"\n2014     ) -> \"DataArray\":\n2015         \"\"\"Drop variables from this DataArray.\n2016 \n2017         Parameters\n2018         ----------\n2019         names : hashable or iterable of hashables\n2020             Name(s) of variables to drop.\n2021         errors: {'raise', 'ignore'}, optional\n2022             If 'raise' (default), raises a ValueError error if any of the variable\n2023             passed are not in the dataset. If 'ignore', any given names that are in the\n2024             DataArray are dropped and no error is raised.\n2025 \n2026         Returns\n2027         -------\n2028         dropped : Dataset\n2029 \n2030         \"\"\"\n2031         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n2032         return self._from_temp_dataset(ds)\n2033 \n2034     def drop(\n2035         self,\n2036         labels: Mapping = None,\n2037         dim: Hashable = None,\n2038         *,\n2039         errors: str = \"raise\",\n2040         **labels_kwargs,\n2041     ) -> \"DataArray\":\n2042         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n2043 \n2044         Using either `drop_vars` or `drop_sel` is encouraged\n2045 \n2046         See Also\n2047         --------\n2048         DataArray.drop_vars\n2049         DataArray.drop_sel\n2050         \"\"\"\n2051         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)\n2052         return self._from_temp_dataset(ds)\n2053 \n2054     def drop_sel(\n2055         self,\n2056         labels: Mapping[Hashable, Any] = None,\n2057         *,\n2058         errors: str = \"raise\",\n2059         **labels_kwargs,\n2060     ) -> \"DataArray\":\n2061         \"\"\"Drop index labels from this DataArray.\n2062 \n2063         Parameters\n2064         ----------\n2065         labels : Mapping[Hashable, Any]\n2066             Index labels to drop\n2067         errors: {'raise', 'ignore'}, optional\n2068             If 'raise' (default), raises a ValueError error if\n2069             any of the index labels passed are not\n2070             in the dataset. If 'ignore', any given labels that are in the\n2071             dataset are dropped and no error is raised.\n2072         **labels_kwargs : {dim: label, ...}, optional\n2073             The keyword arguments form of ``dim`` and ``labels``\n2074 \n2075         Returns\n2076         -------\n2077         dropped : DataArray\n2078         \"\"\"\n2079         if labels_kwargs or isinstance(labels, dict):\n2080             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n2081 \n2082         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n2083         return self._from_temp_dataset(ds)\n2084 \n2085     def dropna(\n2086         self, dim: Hashable, how: str = \"any\", thresh: int = None\n2087     ) -> \"DataArray\":\n2088         \"\"\"Returns a new array with dropped labels for missing values along\n2089         the provided dimension.\n2090 \n2091         Parameters\n2092         ----------\n2093         dim : hashable\n2094             Dimension along which to drop missing values. Dropping along\n2095             multiple dimensions simultaneously is not yet supported.\n2096         how : {'any', 'all'}, optional\n2097             * any : if any NA values are present, drop that label\n2098             * all : if all values are NA, drop that label\n2099         thresh : int, default None\n2100             If supplied, require this many non-NA values.\n2101 \n2102         Returns\n2103         -------\n2104         DataArray\n2105         \"\"\"\n2106         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2107         return self._from_temp_dataset(ds)\n2108 \n2109     def fillna(self, value: Any) -> \"DataArray\":\n2110         \"\"\"Fill missing values in this object.\n2111 \n2112         This operation follows the normal broadcasting and alignment rules that\n2113         xarray uses for binary arithmetic, except the result is aligned to this\n2114         object (``join='left'``) instead of aligned to the intersection of\n2115         index coordinates (``join='inner'``).\n2116 \n2117         Parameters\n2118         ----------\n2119         value : scalar, ndarray or DataArray\n2120             Used to fill all matching missing values in this array. If the\n2121             argument is a DataArray, it is first aligned with (reindexed to)\n2122             this array.\n2123 \n2124         Returns\n2125         -------\n2126         DataArray\n2127         \"\"\"\n2128         if utils.is_dict_like(value):\n2129             raise TypeError(\n2130                 \"cannot provide fill value as a dictionary with \"\n2131                 \"fillna on a DataArray\"\n2132             )\n2133         out = ops.fillna(self, value)\n2134         return out\n2135 \n2136     def interpolate_na(\n2137         self,\n2138         dim: Hashable = None,\n2139         method: str = \"linear\",\n2140         limit: int = None,\n2141         use_coordinate: Union[bool, str] = True,\n2142         max_gap: Union[\n2143             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta\n2144         ] = None,\n2145         keep_attrs: bool = None,\n2146         **kwargs: Any,\n2147     ) -> \"DataArray\":\n2148         \"\"\"Fill in NaNs by interpolating according to different methods.\n2149 \n2150         Parameters\n2151         ----------\n2152         dim : str\n2153             Specifies the dimension along which to interpolate.\n2154         method : str, optional\n2155             String indicating which method to use for interpolation:\n2156 \n2157             - 'linear': linear interpolation (Default). Additional keyword\n2158               arguments are passed to :py:func:`numpy.interp`\n2159             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2160               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2161               ``method='polynomial'``, the ``order`` keyword argument must also be\n2162               provided.\n2163             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2164               respective :py:class:`scipy.interpolate` classes.\n2165 \n2166         use_coordinate : bool, str, default True\n2167             Specifies which index to use as the x values in the interpolation\n2168             formulated as `y = f(x)`. If False, values are treated as if\n2169             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2170             used. If ``use_coordinate`` is a string, it specifies the name of a\n2171             coordinate variariable to use as the index.\n2172         limit : int, default None\n2173             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2174             or None for no limit. This filling is done regardless of the size of\n2175             the gap in the data. To only interpolate over gaps less than a given length,\n2176             see ``max_gap``.\n2177         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.\n2178             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2179             Use None for no limit. When interpolating along a datetime64 dimension\n2180             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2181 \n2182             - a string that is valid input for pandas.to_timedelta\n2183             - a :py:class:`numpy.timedelta64` object\n2184             - a :py:class:`pandas.Timedelta` object\n2185             - a :py:class:`datetime.timedelta` object\n2186 \n2187             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2188             dimensions has not been implemented yet. Gap length is defined as the difference\n2189             between coordinate values at the first data point after a gap and the last value\n2190             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2191             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2192             For example, consider::\n2193 \n2194                 <xarray.DataArray (x: 9)>\n2195                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2196                 Coordinates:\n2197                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2198 \n2199             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2200         keep_attrs : bool, default True\n2201             If True, the dataarray's attributes (`attrs`) will be copied from\n2202             the original object to the new one.  If False, the new\n2203             object will be returned without attributes.\n2204         kwargs : dict, optional\n2205             parameters passed verbatim to the underlying interpolation function\n2206 \n2207         Returns\n2208         -------\n2209         interpolated: DataArray\n2210             Filled in DataArray.\n2211 \n2212         See also\n2213         --------\n2214         numpy.interp\n2215         scipy.interpolate\n2216         \"\"\"\n2217         from .missing import interp_na\n2218 \n2219         return interp_na(\n2220             self,\n2221             dim=dim,\n2222             method=method,\n2223             limit=limit,\n2224             use_coordinate=use_coordinate,\n2225             max_gap=max_gap,\n2226             keep_attrs=keep_attrs,\n2227             **kwargs,\n2228         )\n2229 \n2230     def ffill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2231         \"\"\"Fill NaN values by propogating values forward\n2232 \n2233         *Requires bottleneck.*\n2234 \n2235         Parameters\n2236         ----------\n2237         dim : hashable\n2238             Specifies the dimension along which to propagate values when\n2239             filling.\n2240         limit : int, default None\n2241             The maximum number of consecutive NaN values to forward fill. In\n2242             other words, if there is a gap with more than this number of\n2243             consecutive NaNs, it will only be partially filled. Must be greater\n2244             than 0 or None for no limit.\n2245 \n2246         Returns\n2247         -------\n2248         DataArray\n2249         \"\"\"\n2250         from .missing import ffill\n2251 \n2252         return ffill(self, dim, limit=limit)\n2253 \n2254     def bfill(self, dim: Hashable, limit: int = None) -> \"DataArray\":\n2255         \"\"\"Fill NaN values by propogating values backward\n2256 \n2257         *Requires bottleneck.*\n2258 \n2259         Parameters\n2260         ----------\n2261         dim : str\n2262             Specifies the dimension along which to propagate values when\n2263             filling.\n2264         limit : int, default None\n2265             The maximum number of consecutive NaN values to backward fill. In\n2266             other words, if there is a gap with more than this number of\n2267             consecutive NaNs, it will only be partially filled. Must be greater\n2268             than 0 or None for no limit.\n2269 \n2270         Returns\n2271         -------\n2272         DataArray\n2273         \"\"\"\n2274         from .missing import bfill\n2275 \n2276         return bfill(self, dim, limit=limit)\n2277 \n2278     def combine_first(self, other: \"DataArray\") -> \"DataArray\":\n2279         \"\"\"Combine two DataArray objects, with union of coordinates.\n2280 \n2281         This operation follows the normal broadcasting and alignment rules of\n2282         ``join='outer'``.  Default to non-null values of array calling the\n2283         method.  Use np.nan to fill in vacant cells after alignment.\n2284 \n2285         Parameters\n2286         ----------\n2287         other : DataArray\n2288             Used to fill all matching missing values in this array.\n2289 \n2290         Returns\n2291         -------\n2292         DataArray\n2293         \"\"\"\n2294         return ops.fillna(self, other, join=\"outer\")\n2295 \n2296     def reduce(\n2297         self,\n2298         func: Callable[..., Any],\n2299         dim: Union[None, Hashable, Sequence[Hashable]] = None,\n2300         axis: Union[None, int, Sequence[int]] = None,\n2301         keep_attrs: bool = None,\n2302         keepdims: bool = False,\n2303         **kwargs: Any,\n2304     ) -> \"DataArray\":\n2305         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2306 \n2307         Parameters\n2308         ----------\n2309         func : function\n2310             Function which can be called in the form\n2311             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2312             np.ndarray over an integer valued axis.\n2313         dim : hashable or sequence of hashables, optional\n2314             Dimension(s) over which to apply `func`.\n2315         axis : int or sequence of int, optional\n2316             Axis(es) over which to repeatedly apply `func`. Only one of the\n2317             'dim' and 'axis' arguments can be supplied. If neither are\n2318             supplied, then the reduction is calculated over the flattened array\n2319             (by calling `f(x)` without an axis argument).\n2320         keep_attrs : bool, optional\n2321             If True, the variable's attributes (`attrs`) will be copied from\n2322             the original object to the new one.  If False (default), the new\n2323             object will be returned without attributes.\n2324         keepdims : bool, default False\n2325             If True, the dimensions which are reduced are left in the result\n2326             as dimensions of size one. Coordinates that use these dimensions\n2327             are removed.\n2328         **kwargs : dict\n2329             Additional keyword arguments passed on to `func`.\n2330 \n2331         Returns\n2332         -------\n2333         reduced : DataArray\n2334             DataArray with this object's array replaced with an array with\n2335             summarized data and the indicated dimension(s) removed.\n2336         \"\"\"\n2337 \n2338         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2339         return self._replace_maybe_drop_dims(var)\n2340 \n2341     def to_pandas(self) -> Union[\"DataArray\", pd.Series, pd.DataFrame]:\n2342         \"\"\"Convert this array into a pandas object with the same shape.\n2343 \n2344         The type of the returned object depends on the number of DataArray\n2345         dimensions:\n2346 \n2347         * 0D -> `xarray.DataArray`\n2348         * 1D -> `pandas.Series`\n2349         * 2D -> `pandas.DataFrame`\n2350 \n2351         Only works for arrays with 2 or fewer dimensions.\n2352 \n2353         The DataArray constructor performs the inverse transformation.\n2354         \"\"\"\n2355         # TODO: consolidate the info about pandas constructors and the\n2356         # attributes that correspond to their indexes into a separate module?\n2357         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2358         try:\n2359             constructor = constructors[self.ndim]\n2360         except KeyError:\n2361             raise ValueError(\n2362                 \"cannot convert arrays with %s dimensions into \"\n2363                 \"pandas objects\" % self.ndim\n2364             )\n2365         indexes = [self.get_index(dim) for dim in self.dims]\n2366         return constructor(self.values, *indexes)\n2367 \n2368     def to_dataframe(self, name: Hashable = None) -> pd.DataFrame:\n2369         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2370 \n2371         The DataFrame is indexed by the Cartesian product of index coordinates\n2372         (in the form of a :py:class:`pandas.MultiIndex`).\n2373 \n2374         Other coordinates are included as columns in the DataFrame.\n2375         \"\"\"\n2376         if name is None:\n2377             name = self.name\n2378         if name is None:\n2379             raise ValueError(\n2380                 \"cannot convert an unnamed DataArray to a \"\n2381                 \"DataFrame: use the ``name`` parameter\"\n2382             )\n2383 \n2384         dims = dict(zip(self.dims, self.shape))\n2385         # By using a unique name, we can convert a DataArray into a DataFrame\n2386         # even if it shares a name with one of its coordinates.\n2387         # I would normally use unique_name = object() but that results in a\n2388         # dataframe with columns in the wrong order, for reasons I have not\n2389         # been able to debug (possibly a pandas bug?).\n2390         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n2391         ds = self._to_dataset_whole(name=unique_name)\n2392         df = ds._to_dataframe(dims)\n2393         df.columns = [name if c == unique_name else c for c in df.columns]\n2394         return df\n2395 \n2396     def to_series(self) -> pd.Series:\n2397         \"\"\"Convert this array into a pandas.Series.\n2398 \n2399         The Series is indexed by the Cartesian product of index coordinates\n2400         (in the form of a :py:class:`pandas.MultiIndex`).\n2401         \"\"\"\n2402         index = self.coords.to_index()\n2403         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n2404 \n2405     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n2406         \"\"\"Convert this array into a numpy.ma.MaskedArray\n2407 \n2408         Parameters\n2409         ----------\n2410         copy : bool\n2411             If True (default) make a copy of the array in the result. If False,\n2412             a MaskedArray view of DataArray.values is returned.\n2413 \n2414         Returns\n2415         -------\n2416         result : MaskedArray\n2417             Masked where invalid values (nan or inf) occur.\n2418         \"\"\"\n2419         values = self.values  # only compute lazy arrays once\n2420         isnull = pd.isnull(values)\n2421         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n2422 \n2423     def to_netcdf(self, *args, **kwargs) -> Union[bytes, \"Delayed\", None]:\n2424         \"\"\"Write DataArray contents to a netCDF file.\n2425 \n2426         All parameters are passed directly to `xarray.Dataset.to_netcdf`.\n2427 \n2428         Notes\n2429         -----\n2430         Only xarray.Dataset objects can be written to netCDF files, so\n2431         the xarray.DataArray is converted to a xarray.Dataset object\n2432         containing a single variable. If the DataArray has no name, or if the\n2433         name is the same as a co-ordinate name, then it is given the name\n2434         '__xarray_dataarray_variable__'.\n2435         \"\"\"\n2436         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE\n2437 \n2438         if self.name is None:\n2439             # If no name is set then use a generic xarray name\n2440             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2441         elif self.name in self.coords or self.name in self.dims:\n2442             # The name is the same as one of the coords names, which netCDF\n2443             # doesn't support, so rename it but keep track of the old name\n2444             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n2445             dataset.attrs[DATAARRAY_NAME] = self.name\n2446         else:\n2447             # No problems with the name - so we're fine!\n2448             dataset = self.to_dataset()\n2449 \n2450         return dataset.to_netcdf(*args, **kwargs)\n2451 \n2452     def to_dict(self, data: bool = True) -> dict:\n2453         \"\"\"\n2454         Convert this xarray.DataArray into a dictionary following xarray\n2455         naming conventions.\n2456 \n2457         Converts all variables and attributes to native Python objects.\n2458         Useful for converting to json. To avoid datetime incompatibility\n2459         use decode_times=False kwarg in xarrray.open_dataset.\n2460 \n2461         Parameters\n2462         ----------\n2463         data : bool, optional\n2464             Whether to include the actual data in the dictionary. When set to\n2465             False, returns just the schema.\n2466 \n2467         See also\n2468         --------\n2469         DataArray.from_dict\n2470         \"\"\"\n2471         d = self.variable.to_dict(data=data)\n2472         d.update({\"coords\": {}, \"name\": self.name})\n2473         for k in self.coords:\n2474             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n2475         return d\n2476 \n2477     @classmethod\n2478     def from_dict(cls, d: dict) -> \"DataArray\":\n2479         \"\"\"\n2480         Convert a dictionary into an xarray.DataArray\n2481 \n2482         Input dict can take several forms::\n2483 \n2484             d = {'dims': ('t'), 'data': x}\n2485 \n2486             d = {'coords': {'t': {'dims': 't', 'data': t,\n2487                                   'attrs': {'units':'s'}}},\n2488                  'attrs': {'title': 'air temperature'},\n2489                  'dims': 't',\n2490                  'data': x,\n2491                  'name': 'a'}\n2492 \n2493         where 't' is the name of the dimesion, 'a' is the name of the array,\n2494         and  x and t are lists, numpy.arrays, or pandas objects.\n2495 \n2496         Parameters\n2497         ----------\n2498         d : dict, with a minimum structure of {'dims': [..], 'data': [..]}\n2499 \n2500         Returns\n2501         -------\n2502         obj : xarray.DataArray\n2503 \n2504         See also\n2505         --------\n2506         DataArray.to_dict\n2507         Dataset.from_dict\n2508         \"\"\"\n2509         coords = None\n2510         if \"coords\" in d:\n2511             try:\n2512                 coords = {\n2513                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n2514                     for k, v in d[\"coords\"].items()\n2515                 }\n2516             except KeyError as e:\n2517                 raise ValueError(\n2518                     \"cannot convert dict when coords are missing the key \"\n2519                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n2520                 )\n2521         try:\n2522             data = d[\"data\"]\n2523         except KeyError:\n2524             raise ValueError(\"cannot convert dict without the key 'data''\")\n2525         else:\n2526             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n2527         return obj\n2528 \n2529     @classmethod\n2530     def from_series(cls, series: pd.Series, sparse: bool = False) -> \"DataArray\":\n2531         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n2532 \n2533         If the series's index is a MultiIndex, it will be expanded into a\n2534         tensor product of one-dimensional coordinates (filling in missing\n2535         values with NaN). Thus this operation should be the inverse of the\n2536         `to_series` method.\n2537 \n2538         If sparse=True, creates a sparse array instead of a dense NumPy array.\n2539         Requires the pydata/sparse package.\n2540 \n2541         See also\n2542         --------\n2543         xarray.Dataset.from_dataframe\n2544         \"\"\"\n2545         temp_name = \"__temporary_name\"\n2546         df = pd.DataFrame({temp_name: series})\n2547         ds = Dataset.from_dataframe(df, sparse=sparse)\n2548         result = cast(DataArray, ds[temp_name])\n2549         result.name = series.name\n2550         return result\n2551 \n2552     def to_cdms2(self) -> \"cdms2_Variable\":\n2553         \"\"\"Convert this array into a cdms2.Variable\n2554         \"\"\"\n2555         from ..convert import to_cdms2\n2556 \n2557         return to_cdms2(self)\n2558 \n2559     @classmethod\n2560     def from_cdms2(cls, variable: \"cdms2_Variable\") -> \"DataArray\":\n2561         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\n2562         \"\"\"\n2563         from ..convert import from_cdms2\n2564 \n2565         return from_cdms2(variable)\n2566 \n2567     def to_iris(self) -> \"iris_Cube\":\n2568         \"\"\"Convert this array into a iris.cube.Cube\n2569         \"\"\"\n2570         from ..convert import to_iris\n2571 \n2572         return to_iris(self)\n2573 \n2574     @classmethod\n2575     def from_iris(cls, cube: \"iris_Cube\") -> \"DataArray\":\n2576         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\n2577         \"\"\"\n2578         from ..convert import from_iris\n2579 \n2580         return from_iris(cube)\n2581 \n2582     def _all_compat(self, other: \"DataArray\", compat_str: str) -> bool:\n2583         \"\"\"Helper function for equals, broadcast_equals, and identical\n2584         \"\"\"\n2585 \n2586         def compat(x, y):\n2587             return getattr(x.variable, compat_str)(y.variable)\n2588 \n2589         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n2590             self, other\n2591         )\n2592 \n2593     def broadcast_equals(self, other: \"DataArray\") -> bool:\n2594         \"\"\"Two DataArrays are broadcast equal if they are equal after\n2595         broadcasting them against each other such that they have the same\n2596         dimensions.\n2597 \n2598         See Also\n2599         --------\n2600         DataArray.equals\n2601         DataArray.identical\n2602         \"\"\"\n2603         try:\n2604             return self._all_compat(other, \"broadcast_equals\")\n2605         except (TypeError, AttributeError):\n2606             return False\n2607 \n2608     def equals(self, other: \"DataArray\") -> bool:\n2609         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n2610         values; otherwise False.\n2611 \n2612         DataArrays can still be equal (like pandas objects) if they have NaN\n2613         values in the same locations.\n2614 \n2615         This method is necessary because `v1 == v2` for ``DataArray``\n2616         does element-wise comparisons (like numpy.ndarrays).\n2617 \n2618         See Also\n2619         --------\n2620         DataArray.broadcast_equals\n2621         DataArray.identical\n2622         \"\"\"\n2623         try:\n2624             return self._all_compat(other, \"equals\")\n2625         except (TypeError, AttributeError):\n2626             return False\n2627 \n2628     def identical(self, other: \"DataArray\") -> bool:\n2629         \"\"\"Like equals, but also checks the array name and attributes, and\n2630         attributes on all coordinates.\n2631 \n2632         See Also\n2633         --------\n2634         DataArray.broadcast_equals\n2635         DataArray.equal\n2636         \"\"\"\n2637         try:\n2638             return self.name == other.name and self._all_compat(other, \"identical\")\n2639         except (TypeError, AttributeError):\n2640             return False\n2641 \n2642     def _result_name(self, other: Any = None) -> Optional[Hashable]:\n2643         # use the same naming heuristics as pandas:\n2644         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n2645         other_name = getattr(other, \"name\", _default)\n2646         if other_name is _default or other_name == self.name:\n2647             return self.name\n2648         else:\n2649             return None\n2650 \n2651     def __array_wrap__(self, obj, context=None) -> \"DataArray\":\n2652         new_var = self.variable.__array_wrap__(obj, context)\n2653         return self._replace(new_var)\n2654 \n2655     def __matmul__(self, obj):\n2656         return self.dot(obj)\n2657 \n2658     def __rmatmul__(self, other):\n2659         # currently somewhat duplicative, as only other DataArrays are\n2660         # compatible with matmul\n2661         return computation.dot(other, self)\n2662 \n2663     @staticmethod\n2664     def _unary_op(f: Callable[..., Any]) -> Callable[..., \"DataArray\"]:\n2665         @functools.wraps(f)\n2666         def func(self, *args, **kwargs):\n2667             with np.errstate(all=\"ignore\"):\n2668                 return self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n2669 \n2670         return func\n2671 \n2672     @staticmethod\n2673     def _binary_op(\n2674         f: Callable[..., Any],\n2675         reflexive: bool = False,\n2676         join: str = None,  # see xarray.align\n2677         **ignored_kwargs,\n2678     ) -> Callable[..., \"DataArray\"]:\n2679         @functools.wraps(f)\n2680         def func(self, other):\n2681             if isinstance(other, (Dataset, groupby.GroupBy)):\n2682                 return NotImplemented\n2683             if isinstance(other, DataArray):\n2684                 align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n2685                 self, other = align(self, other, join=align_type, copy=False)\n2686             other_variable = getattr(other, \"variable\", other)\n2687             other_coords = getattr(other, \"coords\", None)\n2688 \n2689             variable = (\n2690                 f(self.variable, other_variable)\n2691                 if not reflexive\n2692                 else f(other_variable, self.variable)\n2693             )\n2694             coords, indexes = self.coords._merge_raw(other_coords)\n2695             name = self._result_name(other)\n2696 \n2697             return self._replace(variable, coords, name, indexes=indexes)\n2698 \n2699         return func\n2700 \n2701     @staticmethod\n2702     def _inplace_binary_op(f: Callable) -> Callable[..., \"DataArray\"]:\n2703         @functools.wraps(f)\n2704         def func(self, other):\n2705             if isinstance(other, groupby.GroupBy):\n2706                 raise TypeError(\n2707                     \"in-place operations between a DataArray and \"\n2708                     \"a grouped object are not permitted\"\n2709                 )\n2710             # n.b. we can't align other to self (with other.reindex_like(self))\n2711             # because `other` may be converted into floats, which would cause\n2712             # in-place arithmetic to fail unpredictably. Instead, we simply\n2713             # don't support automatic alignment with in-place arithmetic.\n2714             other_coords = getattr(other, \"coords\", None)\n2715             other_variable = getattr(other, \"variable\", other)\n2716             try:\n2717                 with self.coords._merge_inplace(other_coords):\n2718                     f(self.variable, other_variable)\n2719             except MergeError as exc:\n2720                 raise MergeError(\n2721                     \"Automatic alignment is not supported for in-place operations.\\n\"\n2722                     \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n2723                     \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n2724                 ) from exc\n2725             return self\n2726 \n2727         return func\n2728 \n2729     def _copy_attrs_from(self, other: Union[\"DataArray\", Dataset, Variable]) -> None:\n2730         self.attrs = other.attrs\n2731 \n2732     plot = utils.UncachedAccessor(_PlotMethods)\n2733 \n2734     def _title_for_slice(self, truncate: int = 50) -> str:\n2735         \"\"\"\n2736         If the dataarray has 1 dimensional coordinates or comes from a slice\n2737         we can show that info in the title\n2738 \n2739         Parameters\n2740         ----------\n2741         truncate : integer\n2742             maximum number of characters for title\n2743 \n2744         Returns\n2745         -------\n2746         title : string\n2747             Can be used for plot titles\n2748 \n2749         \"\"\"\n2750         one_dims = []\n2751         for dim, coord in self.coords.items():\n2752             if coord.size == 1:\n2753                 one_dims.append(\n2754                     \"{dim} = {v}\".format(dim=dim, v=format_item(coord.values))\n2755                 )\n2756 \n2757         title = \", \".join(one_dims)\n2758         if len(title) > truncate:\n2759             title = title[: (truncate - 3)] + \"...\"\n2760 \n2761         return title\n2762 \n2763     def diff(self, dim: Hashable, n: int = 1, label: Hashable = \"upper\") -> \"DataArray\":\n2764         \"\"\"Calculate the n-th order discrete difference along given axis.\n2765 \n2766         Parameters\n2767         ----------\n2768         dim : hashable\n2769             Dimension over which to calculate the finite difference.\n2770         n : int, optional\n2771             The number of times values are differenced.\n2772         label : hashable, optional\n2773             The new coordinate in dimension ``dim`` will have the\n2774             values of either the minuend's or subtrahend's coordinate\n2775             for values 'upper' and 'lower', respectively.  Other\n2776             values are not supported.\n2777 \n2778         Returns\n2779         -------\n2780         difference : same type as caller\n2781             The n-th order finite difference of this object.\n2782 \n2783         .. note::\n2784 \n2785             `n` matches numpy's behavior and is different from pandas' first\n2786             argument named `periods`.\n2787 \n2788 \n2789         Examples\n2790         --------\n2791         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n2792         >>> arr.diff(\"x\")\n2793         <xarray.DataArray (x: 3)>\n2794         array([0, 1, 0])\n2795         Coordinates:\n2796         * x        (x) int64 2 3 4\n2797         >>> arr.diff(\"x\", 2)\n2798         <xarray.DataArray (x: 2)>\n2799         array([ 1, -1])\n2800         Coordinates:\n2801         * x        (x) int64 3 4\n2802 \n2803         See Also\n2804         --------\n2805         DataArray.differentiate\n2806         \"\"\"\n2807         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n2808         return self._from_temp_dataset(ds)\n2809 \n2810     def shift(\n2811         self,\n2812         shifts: Mapping[Hashable, int] = None,\n2813         fill_value: Any = dtypes.NA,\n2814         **shifts_kwargs: int,\n2815     ) -> \"DataArray\":\n2816         \"\"\"Shift this array by an offset along one or more dimensions.\n2817 \n2818         Only the data is moved; coordinates stay in place. Values shifted from\n2819         beyond array bounds are replaced by NaN. This is consistent with the\n2820         behavior of ``shift`` in pandas.\n2821 \n2822         Parameters\n2823         ----------\n2824         shifts : Mapping with the form of {dim: offset}\n2825             Integer offset to shift along each of the given dimensions.\n2826             Positive offsets shift to the right; negative offsets shift to the\n2827             left.\n2828         fill_value: scalar, optional\n2829             Value to use for newly missing values\n2830         **shifts_kwargs:\n2831             The keyword arguments form of ``shifts``.\n2832             One of shifts or shifts_kwargs must be provided.\n2833 \n2834         Returns\n2835         -------\n2836         shifted : DataArray\n2837             DataArray with the same coordinates and attributes but shifted\n2838             data.\n2839 \n2840         See also\n2841         --------\n2842         roll\n2843 \n2844         Examples\n2845         --------\n2846 \n2847         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2848         >>> arr.shift(x=1)\n2849         <xarray.DataArray (x: 3)>\n2850         array([ nan,   5.,   6.])\n2851         Coordinates:\n2852           * x        (x) int64 0 1 2\n2853         \"\"\"\n2854         variable = self.variable.shift(\n2855             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n2856         )\n2857         return self._replace(variable=variable)\n2858 \n2859     def roll(\n2860         self,\n2861         shifts: Mapping[Hashable, int] = None,\n2862         roll_coords: bool = None,\n2863         **shifts_kwargs: int,\n2864     ) -> \"DataArray\":\n2865         \"\"\"Roll this array by an offset along one or more dimensions.\n2866 \n2867         Unlike shift, roll may rotate all variables, including coordinates\n2868         if specified. The direction of rotation is consistent with\n2869         :py:func:`numpy.roll`.\n2870 \n2871         Parameters\n2872         ----------\n2873         shifts : Mapping with the form of {dim: offset}\n2874             Integer offset to rotate each of the given dimensions.\n2875             Positive offsets roll to the right; negative offsets roll to the\n2876             left.\n2877         roll_coords : bool\n2878             Indicates whether to  roll the coordinates by the offset\n2879             The current default of roll_coords (None, equivalent to True) is\n2880             deprecated and will change to False in a future version.\n2881             Explicitly pass roll_coords to silence the warning.\n2882         **shifts_kwargs : The keyword arguments form of ``shifts``.\n2883             One of shifts or shifts_kwargs must be provided.\n2884 \n2885         Returns\n2886         -------\n2887         rolled : DataArray\n2888             DataArray with the same attributes but rolled data and coordinates.\n2889 \n2890         See also\n2891         --------\n2892         shift\n2893 \n2894         Examples\n2895         --------\n2896 \n2897         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n2898         >>> arr.roll(x=1)\n2899         <xarray.DataArray (x: 3)>\n2900         array([7, 5, 6])\n2901         Coordinates:\n2902           * x        (x) int64 2 0 1\n2903         \"\"\"\n2904         ds = self._to_temp_dataset().roll(\n2905             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n2906         )\n2907         return self._from_temp_dataset(ds)\n2908 \n2909     @property\n2910     def real(self) -> \"DataArray\":\n2911         return self._replace(self.variable.real)\n2912 \n2913     @property\n2914     def imag(self) -> \"DataArray\":\n2915         return self._replace(self.variable.imag)\n2916 \n2917     def dot(\n2918         self, other: \"DataArray\", dims: Union[Hashable, Sequence[Hashable], None] = None\n2919     ) -> \"DataArray\":\n2920         \"\"\"Perform dot product of two DataArrays along their shared dims.\n2921 \n2922         Equivalent to taking taking tensordot over all shared dims.\n2923 \n2924         Parameters\n2925         ----------\n2926         other : DataArray\n2927             The other array with which the dot product is performed.\n2928         dims: '...', hashable or sequence of hashables, optional\n2929             Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.\n2930             If not specified, then all the common dimensions are summed over.\n2931 \n2932         Returns\n2933         -------\n2934         result : DataArray\n2935             Array resulting from the dot product over all shared dimensions.\n2936 \n2937         See also\n2938         --------\n2939         dot\n2940         numpy.tensordot\n2941 \n2942         Examples\n2943         --------\n2944 \n2945         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n2946         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n2947         >>> dm_vals = np.arange(4)\n2948         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n2949 \n2950         >>> dm.dims\n2951         ('z')\n2952 \n2953         >>> da.dims\n2954         ('x', 'y', 'z')\n2955 \n2956         >>> dot_result = da.dot(dm)\n2957         >>> dot_result.dims\n2958         ('x', 'y')\n2959 \n2960         \"\"\"\n2961         if isinstance(other, Dataset):\n2962             raise NotImplementedError(\n2963                 \"dot products are not yet supported with Dataset objects.\"\n2964             )\n2965         if not isinstance(other, DataArray):\n2966             raise TypeError(\"dot only operates on DataArrays.\")\n2967 \n2968         return computation.dot(self, other, dims=dims)\n2969 \n2970     def sortby(\n2971         self,\n2972         variables: Union[Hashable, \"DataArray\", Sequence[Union[Hashable, \"DataArray\"]]],\n2973         ascending: bool = True,\n2974     ) -> \"DataArray\":\n2975         \"\"\"Sort object by labels or values (along an axis).\n2976 \n2977         Sorts the dataarray, either along specified dimensions,\n2978         or according to values of 1-D dataarrays that share dimension\n2979         with calling object.\n2980 \n2981         If the input variables are dataarrays, then the dataarrays are aligned\n2982         (via left-join) to the calling object prior to sorting by cell values.\n2983         NaNs are sorted to the end, following Numpy convention.\n2984 \n2985         If multiple sorts along the same dimension is\n2986         given, numpy's lexsort is performed along that dimension:\n2987         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html\n2988         and the FIRST key in the sequence is used as the primary sort key,\n2989         followed by the 2nd key, etc.\n2990 \n2991         Parameters\n2992         ----------\n2993         variables: hashable, DataArray, or sequence of either\n2994             1D DataArray objects or name(s) of 1D variable(s) in\n2995             coords whose values are used to sort this array.\n2996         ascending: boolean, optional\n2997             Whether to sort by ascending or descending order.\n2998 \n2999         Returns\n3000         -------\n3001         sorted: DataArray\n3002             A new dataarray where all the specified dims are sorted by dim\n3003             labels.\n3004 \n3005         Examples\n3006         --------\n3007 \n3008         >>> da = xr.DataArray(\n3009         ...     np.random.rand(5),\n3010         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n3011         ...     dims=\"time\",\n3012         ... )\n3013         >>> da\n3014         <xarray.DataArray (time: 5)>\n3015         array([ 0.965471,  0.615637,  0.26532 ,  0.270962,  0.552878])\n3016         Coordinates:\n3017           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03 ...\n3018 \n3019         >>> da.sortby(da)\n3020         <xarray.DataArray (time: 5)>\n3021         array([ 0.26532 ,  0.270962,  0.552878,  0.615637,  0.965471])\n3022         Coordinates:\n3023           * time     (time) datetime64[ns] 2000-01-03 2000-01-04 2000-01-05 ...\n3024         \"\"\"\n3025         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n3026         return self._from_temp_dataset(ds)\n3027 \n3028     def quantile(\n3029         self,\n3030         q: Any,\n3031         dim: Union[Hashable, Sequence[Hashable], None] = None,\n3032         interpolation: str = \"linear\",\n3033         keep_attrs: bool = None,\n3034         skipna: bool = True,\n3035     ) -> \"DataArray\":\n3036         \"\"\"Compute the qth quantile of the data along the specified dimension.\n3037 \n3038         Returns the qth quantiles(s) of the array elements.\n3039 \n3040         Parameters\n3041         ----------\n3042         q : float in range of [0,1] or array-like of floats\n3043             Quantile to compute, which must be between 0 and 1 inclusive.\n3044         dim : hashable or sequence of hashable, optional\n3045             Dimension(s) over which to apply quantile.\n3046         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n3047             This optional parameter specifies the interpolation method to\n3048             use when the desired quantile lies between two data points\n3049             ``i < j``:\n3050 \n3051                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is\n3052                   the fractional part of the index surrounded by ``i`` and\n3053                   ``j``.\n3054                 - lower: ``i``.\n3055                 - higher: ``j``.\n3056                 - nearest: ``i`` or ``j``, whichever is nearest.\n3057                 - midpoint: ``(i + j) / 2``.\n3058         keep_attrs : bool, optional\n3059             If True, the dataset's attributes (`attrs`) will be copied from\n3060             the original object to the new one.  If False (default), the new\n3061             object will be returned without attributes.\n3062         skipna : bool, optional\n3063             Whether to skip missing values when aggregating.\n3064 \n3065         Returns\n3066         -------\n3067         quantiles : DataArray\n3068             If `q` is a single quantile, then the result\n3069             is a scalar. If multiple percentiles are given, first axis of\n3070             the result corresponds to the quantile and a quantile dimension\n3071             is added to the return array. The other dimensions are the\n3072             dimensions that remain after the reduction of the array.\n3073 \n3074         See Also\n3075         --------\n3076         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n3077 \n3078         Examples\n3079         --------\n3080 \n3081         >>> da = xr.DataArray(\n3082         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n3083         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n3084         ...     dims=(\"x\", \"y\"),\n3085         ... )\n3086         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3087         <xarray.DataArray ()>\n3088         array(0.7)\n3089         Coordinates:\n3090             quantile  float64 0.0\n3091         >>> da.quantile(0, dim=\"x\")\n3092         <xarray.DataArray (y: 4)>\n3093         array([0.7, 4.2, 2.6, 1.5])\n3094         Coordinates:\n3095           * y         (y) float64 1.0 1.5 2.0 2.5\n3096             quantile  float64 0.0\n3097         >>> da.quantile([0, 0.5, 1])\n3098         <xarray.DataArray (quantile: 3)>\n3099         array([0.7, 3.4, 9.4])\n3100         Coordinates:\n3101           * quantile  (quantile) float64 0.0 0.5 1.0\n3102         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3103         <xarray.DataArray (quantile: 3, y: 4)>\n3104         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3105                [3.6 , 5.75, 6.  , 1.7 ],\n3106                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3107         Coordinates:\n3108           * y         (y) float64 1.0 1.5 2.0 2.5\n3109           * quantile  (quantile) float64 0.0 0.5 1.0\n3110         \"\"\"\n3111 \n3112         ds = self._to_temp_dataset().quantile(\n3113             q,\n3114             dim=dim,\n3115             keep_attrs=keep_attrs,\n3116             interpolation=interpolation,\n3117             skipna=skipna,\n3118         )\n3119         return self._from_temp_dataset(ds)\n3120 \n3121     def rank(\n3122         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None\n3123     ) -> \"DataArray\":\n3124         \"\"\"Ranks the data.\n3125 \n3126         Equal values are assigned a rank that is the average of the ranks that\n3127         would have been otherwise assigned to all of the values within that\n3128         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n3129 \n3130         NaNs in the input array are returned as NaNs.\n3131 \n3132         The `bottleneck` library is required.\n3133 \n3134         Parameters\n3135         ----------\n3136         dim : hashable\n3137             Dimension over which to compute rank.\n3138         pct : bool, optional\n3139             If True, compute percentage ranks, otherwise compute integer ranks.\n3140         keep_attrs : bool, optional\n3141             If True, the dataset's attributes (`attrs`) will be copied from\n3142             the original object to the new one.  If False (default), the new\n3143             object will be returned without attributes.\n3144 \n3145         Returns\n3146         -------\n3147         ranked : DataArray\n3148             DataArray with the same coordinates and dtype 'float64'.\n3149 \n3150         Examples\n3151         --------\n3152 \n3153         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3154         >>> arr.rank(\"x\")\n3155         <xarray.DataArray (x: 3)>\n3156         array([ 1.,   2.,   3.])\n3157         Dimensions without coordinates: x\n3158         \"\"\"\n3159 \n3160         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n3161         return self._from_temp_dataset(ds)\n3162 \n3163     def differentiate(\n3164         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None\n3165     ) -> \"DataArray\":\n3166         \"\"\" Differentiate the array with the second order accurate central\n3167         differences.\n3168 \n3169         .. note::\n3170             This feature is limited to simple cartesian geometry, i.e. coord\n3171             must be one dimensional.\n3172 \n3173         Parameters\n3174         ----------\n3175         coord: hashable\n3176             The coordinate to be used to compute the gradient.\n3177         edge_order: 1 or 2. Default 1\n3178             N-th order accurate differences at the boundaries.\n3179         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',\n3180             'us', 'ns', 'ps', 'fs', 'as'}\n3181             Unit to compute gradient. Only valid for datetime coordinate.\n3182 \n3183         Returns\n3184         -------\n3185         differentiated: DataArray\n3186 \n3187         See also\n3188         --------\n3189         numpy.gradient: corresponding numpy function\n3190 \n3191         Examples\n3192         --------\n3193 \n3194         >>> da = xr.DataArray(\n3195         ...     np.arange(12).reshape(4, 3),\n3196         ...     dims=[\"x\", \"y\"],\n3197         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3198         ... )\n3199         >>> da\n3200         <xarray.DataArray (x: 4, y: 3)>\n3201         array([[ 0,  1,  2],\n3202                [ 3,  4,  5],\n3203                [ 6,  7,  8],\n3204                [ 9, 10, 11]])\n3205         Coordinates:\n3206           * x        (x) float64 0.0 0.1 1.1 1.2\n3207         Dimensions without coordinates: y\n3208         >>>\n3209         >>> da.differentiate(\"x\")\n3210         <xarray.DataArray (x: 4, y: 3)>\n3211         array([[30.      , 30.      , 30.      ],\n3212                [27.545455, 27.545455, 27.545455],\n3213                [27.545455, 27.545455, 27.545455],\n3214                [30.      , 30.      , 30.      ]])\n3215         Coordinates:\n3216           * x        (x) float64 0.0 0.1 1.1 1.2\n3217         Dimensions without coordinates: y\n3218         \"\"\"\n3219         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n3220         return self._from_temp_dataset(ds)\n3221 \n3222     def integrate(\n3223         self, dim: Union[Hashable, Sequence[Hashable]], datetime_unit: str = None\n3224     ) -> \"DataArray\":\n3225         \"\"\" integrate the array with the trapezoidal rule.\n3226 \n3227         .. note::\n3228             This feature is limited to simple cartesian geometry, i.e. dim\n3229             must be one dimensional.\n3230 \n3231         Parameters\n3232         ----------\n3233         dim: hashable, or a sequence of hashable\n3234             Coordinate(s) used for the integration.\n3235         datetime_unit: str, optional\n3236             Can be used to specify the unit if datetime coordinate is used.\n3237             One of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps',\n3238             'fs', 'as'}\n3239 \n3240         Returns\n3241         -------\n3242         integrated: DataArray\n3243 \n3244         See also\n3245         --------\n3246         numpy.trapz: corresponding numpy function\n3247 \n3248         Examples\n3249         --------\n3250 \n3251         >>> da = xr.DataArray(\n3252         ...     np.arange(12).reshape(4, 3),\n3253         ...     dims=[\"x\", \"y\"],\n3254         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n3255         ... )\n3256         >>> da\n3257         <xarray.DataArray (x: 4, y: 3)>\n3258         array([[ 0,  1,  2],\n3259                [ 3,  4,  5],\n3260                [ 6,  7,  8],\n3261                [ 9, 10, 11]])\n3262         Coordinates:\n3263           * x        (x) float64 0.0 0.1 1.1 1.2\n3264         Dimensions without coordinates: y\n3265         >>>\n3266         >>> da.integrate(\"x\")\n3267         <xarray.DataArray (y: 3)>\n3268         array([5.4, 6.6, 7.8])\n3269         Dimensions without coordinates: y\n3270         \"\"\"\n3271         ds = self._to_temp_dataset().integrate(dim, datetime_unit)\n3272         return self._from_temp_dataset(ds)\n3273 \n3274     def unify_chunks(self) -> \"DataArray\":\n3275         \"\"\" Unify chunk size along all chunked dimensions of this DataArray.\n3276 \n3277         Returns\n3278         -------\n3279 \n3280         DataArray with consistent chunk sizes for all dask-array variables\n3281 \n3282         See Also\n3283         --------\n3284 \n3285         dask.array.core.unify_chunks\n3286         \"\"\"\n3287         ds = self._to_temp_dataset().unify_chunks()\n3288         return self._from_temp_dataset(ds)\n3289 \n3290     def map_blocks(\n3291         self,\n3292         func: \"Callable[..., T_DSorDA]\",\n3293         args: Sequence[Any] = (),\n3294         kwargs: Mapping[str, Any] = None,\n3295         template: Union[\"DataArray\", \"Dataset\"] = None,\n3296     ) -> \"T_DSorDA\":\n3297         \"\"\"\n3298         Apply a function to each block of this DataArray.\n3299 \n3300         .. warning::\n3301             This method is experimental and its signature may change.\n3302 \n3303         Parameters\n3304         ----------\n3305         func: callable\n3306             User-provided function that accepts a DataArray as its first\n3307             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n3308             corresponding to one chunk along each chunked dimension. ``func`` will be\n3309             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n3310 \n3311             This function must return either a single DataArray or a single Dataset.\n3312 \n3313             This function cannot add a new chunked dimension.\n3314 \n3315         obj: DataArray, Dataset\n3316             Passed to the function as its first argument, one block at a time.\n3317         args: Sequence\n3318             Passed to func after unpacking and subsetting any xarray objects by blocks.\n3319             xarray objects in args must be aligned with obj, otherwise an error is raised.\n3320         kwargs: Mapping\n3321             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n3322             subset to blocks. Passing dask collections in kwargs is not allowed.\n3323         template: (optional) DataArray, Dataset\n3324             xarray object representing the final result after compute is called. If not provided,\n3325             the function will be first run on mocked-up data, that looks like ``obj`` but\n3326             has sizes 0, to determine properties of the returned object such as dtype,\n3327             variable names, attributes, new dimensions and new indexes (if any).\n3328             ``template`` must be provided if the function changes the size of existing dimensions.\n3329             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n3330             ``attrs`` set by ``func`` will be ignored.\n3331 \n3332 \n3333         Returns\n3334         -------\n3335         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n3336         function.\n3337 \n3338         Notes\n3339         -----\n3340         This function is designed for when ``func`` needs to manipulate a whole xarray object\n3341         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is\n3342         recommended to use ``apply_ufunc``.\n3343 \n3344         If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n3345         equivalent to calling ``func(obj, *args, **kwargs)``.\n3346 \n3347         See Also\n3348         --------\n3349         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,\n3350         xarray.DataArray.map_blocks\n3351 \n3352         Examples\n3353         --------\n3354 \n3355         Calculate an anomaly from climatology using ``.groupby()``. Using\n3356         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n3357         its indices, and its methods like ``.groupby()``.\n3358 \n3359         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n3360         ...     gb = da.groupby(groupby_type)\n3361         ...     clim = gb.mean(dim=\"time\")\n3362         ...     return gb - clim\n3363         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n3364         >>> np.random.seed(123)\n3365         >>> array = xr.DataArray(\n3366         ...     np.random.rand(len(time)), dims=\"time\", coords=[time]\n3367         ... ).chunk()\n3368         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n3369         <xarray.DataArray (time: 24)>\n3370         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n3371                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n3372                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n3373                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n3374                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n3375         Coordinates:\n3376           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n3377 \n3378         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n3379         to the function being applied in ``xr.map_blocks()``:\n3380 \n3381         >>> array.map_blocks(\n3382         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array,\n3383         ... )\n3384         <xarray.DataArray (time: 24)>\n3385         array([ 0.15361741, -0.25671244, -0.31600032,  0.008463  ,  0.1766172 ,\n3386                -0.11974531,  0.43791243,  0.14197797, -0.06191987, -0.15073425,\n3387                -0.19967375,  0.18619794, -0.05100474, -0.42989909, -0.09153273,\n3388                 0.24841842, -0.30708526, -0.31412523,  0.04197439,  0.0422506 ,\n3389                 0.14482397,  0.35985481,  0.23487834,  0.12144652])\n3390         Coordinates:\n3391             * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n3392         \"\"\"\n3393         from .parallel import map_blocks\n3394 \n3395         return map_blocks(func, self, args, kwargs, template)\n3396 \n3397     def polyfit(\n3398         self,\n3399         dim: Hashable,\n3400         deg: int,\n3401         skipna: bool = None,\n3402         rcond: float = None,\n3403         w: Union[Hashable, Any] = None,\n3404         full: bool = False,\n3405         cov: bool = False,\n3406     ):\n3407         \"\"\"\n3408         Least squares polynomial fit.\n3409 \n3410         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n3411         invalid values when `skipna = True`.\n3412 \n3413         Parameters\n3414         ----------\n3415         dim : hashable\n3416             Coordinate along which to fit the polynomials.\n3417         deg : int\n3418             Degree of the fitting polynomial.\n3419         skipna : bool, optional\n3420             If True, removes all invalid values before fitting each 1D slices of the array.\n3421             Default is True if data is stored in a dask.array or if there is any\n3422             invalid values, False otherwise.\n3423         rcond : float, optional\n3424             Relative condition number to the fit.\n3425         w : Union[Hashable, Any], optional\n3426             Weights to apply to the y-coordinate of the sample points.\n3427             Can be an array-like object or the name of a coordinate in the dataset.\n3428         full : bool, optional\n3429             Whether to return the residuals, matrix rank and singular values in addition\n3430             to the coefficients.\n3431         cov : Union[bool, str], optional\n3432             Whether to return to the covariance matrix in addition to the coefficients.\n3433             The matrix is not scaled if `cov='unscaled'`.\n3434 \n3435         Returns\n3436         -------\n3437         polyfit_results : Dataset\n3438             A single dataset which contains:\n3439 \n3440             polyfit_coefficients\n3441                 The coefficients of the best fit.\n3442             polyfit_residuals\n3443                 The residuals of the least-square computation (only included if `full=True`)\n3444             [dim]_matrix_rank\n3445                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3446             [dim]_singular_value\n3447                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n3448             polyfit_covariance\n3449                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n3450 \n3451         See also\n3452         --------\n3453         numpy.polyfit\n3454         \"\"\"\n3455         return self._to_temp_dataset().polyfit(\n3456             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n3457         )\n3458 \n3459     def pad(\n3460         self,\n3461         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n3462         mode: str = \"constant\",\n3463         stat_length: Union[\n3464             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3465         ] = None,\n3466         constant_values: Union[\n3467             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3468         ] = None,\n3469         end_values: Union[\n3470             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n3471         ] = None,\n3472         reflect_type: str = None,\n3473         **pad_width_kwargs: Any,\n3474     ) -> \"DataArray\":\n3475         \"\"\"Pad this array along one or more dimensions.\n3476 \n3477         .. warning::\n3478             This function is experimental and its behaviour is likely to change\n3479             especially regarding padding of dimension coordinates (or IndexVariables).\n3480 \n3481         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n3482         coordinates will be padded with the same mode, otherwise coordinates\n3483         are padded using the \"constant\" mode with fill_value dtypes.NA.\n3484 \n3485         Parameters\n3486         ----------\n3487         pad_width : Mapping with the form of {dim: (pad_before, pad_after)}\n3488             Number of values padded along each dimension.\n3489             {dim: pad} is a shortcut for pad_before = pad_after = pad\n3490         mode : str\n3491             One of the following string values (taken from numpy docs)\n3492 \n3493             'constant' (default)\n3494                 Pads with a constant value.\n3495             'edge'\n3496                 Pads with the edge values of array.\n3497             'linear_ramp'\n3498                 Pads with the linear ramp between end_value and the\n3499                 array edge value.\n3500             'maximum'\n3501                 Pads with the maximum value of all or part of the\n3502                 vector along each axis.\n3503             'mean'\n3504                 Pads with the mean value of all or part of the\n3505                 vector along each axis.\n3506             'median'\n3507                 Pads with the median value of all or part of the\n3508                 vector along each axis.\n3509             'minimum'\n3510                 Pads with the minimum value of all or part of the\n3511                 vector along each axis.\n3512             'reflect'\n3513                 Pads with the reflection of the vector mirrored on\n3514                 the first and last values of the vector along each\n3515                 axis.\n3516             'symmetric'\n3517                 Pads with the reflection of the vector mirrored\n3518                 along the edge of the array.\n3519             'wrap'\n3520                 Pads with the wrap of the vector along the axis.\n3521                 The first values are used to pad the end and the\n3522                 end values are used to pad the beginning.\n3523         stat_length : int, tuple or mapping of the form {dim: tuple}\n3524             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n3525             values at edge of each axis used to calculate the statistic value.\n3526             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n3527             statistic lengths along each dimension.\n3528             ((before, after),) yields same before and after statistic lengths\n3529             for each dimension.\n3530             (stat_length,) or int is a shortcut for before = after = statistic\n3531             length for all axes.\n3532             Default is ``None``, to use the entire axis.\n3533         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n3534             Used in 'constant'.  The values to set the padded values for each\n3535             axis.\n3536             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3537             pad constants along each dimension.\n3538             ``((before, after),)`` yields same before and after constants for each\n3539             dimension.\n3540             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3541             all dimensions.\n3542             Default is 0.\n3543         end_values : scalar, tuple or mapping of the form {dim: tuple}\n3544             Used in 'linear_ramp'.  The values used for the ending value of the\n3545             linear_ramp and that will form the edge of the padded array.\n3546             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n3547             end values along each dimension.\n3548             ``((before, after),)`` yields same before and after end values for each\n3549             axis.\n3550             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n3551             all axes.\n3552             Default is 0.\n3553         reflect_type : {'even', 'odd'}, optional\n3554             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n3555             default with an unaltered reflection around the edge value.  For\n3556             the 'odd' style, the extended part of the array is created by\n3557             subtracting the reflected values from two times the edge value.\n3558         **pad_width_kwargs:\n3559             The keyword arguments form of ``pad_width``.\n3560             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n3561 \n3562         Returns\n3563         -------\n3564         padded : DataArray\n3565             DataArray with the padded coordinates and data.\n3566 \n3567         See also\n3568         --------\n3569         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n3570 \n3571         Notes\n3572         -----\n3573         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n3574         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n3575         specify ``constant_values=np.nan``\n3576 \n3577         Examples\n3578         --------\n3579 \n3580         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n3581         >>> arr.pad(x=(1, 2), constant_values=0)\n3582         <xarray.DataArray (x: 6)>\n3583         array([0, 5, 6, 7, 0, 0])\n3584         Coordinates:\n3585           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n3586 \n3587         >>> da = xr.DataArray(\n3588         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n3589         ...     dims=[\"x\", \"y\"],\n3590         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n3591         ... )\n3592         >>> da.pad(x=1)\n3593         <xarray.DataArray (x: 4, y: 4)>\n3594         array([[nan, nan, nan, nan],\n3595                [ 0.,  1.,  2.,  3.],\n3596                [10., 11., 12., 13.],\n3597                [nan, nan, nan, nan]])\n3598         Coordinates:\n3599           * x        (x) float64 nan 0.0 1.0 nan\n3600           * y        (y) int64 10 20 30 40\n3601             z        (x) float64 nan 100.0 200.0 nan\n3602         >>> da.pad(x=1, constant_values=np.nan)\n3603         <xarray.DataArray (x: 4, y: 4)>\n3604         array([[-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3605                 -9223372036854775808],\n3606                [                   0,                    1,                    2,\n3607                                    3],\n3608                [                  10,                   11,                   12,\n3609                                   13],\n3610                [-9223372036854775808, -9223372036854775808, -9223372036854775808,\n3611                 -9223372036854775808]])\n3612         Coordinates:\n3613           * x        (x) float64 nan 0.0 1.0 nan\n3614           * y        (y) int64 10 20 30 40\n3615             z        (x) float64 nan 100.0 200.0 nan\n3616         \"\"\"\n3617         ds = self._to_temp_dataset().pad(\n3618             pad_width=pad_width,\n3619             mode=mode,\n3620             stat_length=stat_length,\n3621             constant_values=constant_values,\n3622             end_values=end_values,\n3623             reflect_type=reflect_type,\n3624             **pad_width_kwargs,\n3625         )\n3626         return self._from_temp_dataset(ds)\n3627 \n3628     def idxmin(\n3629         self,\n3630         dim: Hashable = None,\n3631         skipna: bool = None,\n3632         fill_value: Any = dtypes.NA,\n3633         keep_attrs: bool = None,\n3634     ) -> \"DataArray\":\n3635         \"\"\"Return the coordinate label of the minimum value along a dimension.\n3636 \n3637         Returns a new `DataArray` named after the dimension with the values of\n3638         the coordinate labels along that dimension corresponding to minimum\n3639         values along that dimension.\n3640 \n3641         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n3642         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n3643 \n3644         Parameters\n3645         ----------\n3646         dim : str, optional\n3647             Dimension over which to apply `idxmin`.  This is optional for 1D\n3648             arrays, but required for arrays with 2 or more dimensions.\n3649         skipna : bool or None, default None\n3650             If True, skip missing values (as marked by NaN). By default, only\n3651             skips missing values for ``float``, ``complex``, and ``object``\n3652             dtypes; other dtypes either do not have a sentinel missing value\n3653             (``int``) or ``skipna=True`` has not been implemented\n3654             (``datetime64`` or ``timedelta64``).\n3655         fill_value : Any, default NaN\n3656             Value to be filled in case all of the values along a dimension are\n3657             null.  By default this is NaN.  The fill value and result are\n3658             automatically converted to a compatible dtype if possible.\n3659             Ignored if ``skipna`` is False.\n3660         keep_attrs : bool, default False\n3661             If True, the attributes (``attrs``) will be copied from the\n3662             original object to the new one.  If False (default), the new object\n3663             will be returned without attributes.\n3664 \n3665         Returns\n3666         -------\n3667         reduced : DataArray\n3668             New `DataArray` object with `idxmin` applied to its data and the\n3669             indicated dimension removed.\n3670 \n3671         See also\n3672         --------\n3673         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n3674 \n3675         Examples\n3676         --------\n3677 \n3678         >>> array = xr.DataArray(\n3679         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n3680         ... )\n3681         >>> array.min()\n3682         <xarray.DataArray ()>\n3683         array(-2)\n3684         >>> array.argmin()\n3685         <xarray.DataArray ()>\n3686         array(4)\n3687         >>> array.idxmin()\n3688         <xarray.DataArray 'x' ()>\n3689         array('e', dtype='<U1')\n3690 \n3691         >>> array = xr.DataArray(\n3692         ...     [\n3693         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n3694         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3695         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n3696         ...     ],\n3697         ...     dims=[\"y\", \"x\"],\n3698         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n3699         ... )\n3700         >>> array.min(dim=\"x\")\n3701         <xarray.DataArray (y: 3)>\n3702         array([-2., -4.,  1.])\n3703         Coordinates:\n3704           * y        (y) int64 -1 0 1\n3705         >>> array.argmin(dim=\"x\")\n3706         <xarray.DataArray (y: 3)>\n3707         array([4, 0, 2])\n3708         Coordinates:\n3709           * y        (y) int64 -1 0 1\n3710         >>> array.idxmin(dim=\"x\")\n3711         <xarray.DataArray 'x' (y: 3)>\n3712         array([16.,  0.,  4.])\n3713         Coordinates:\n3714           * y        (y) int64 -1 0 1\n3715         \"\"\"\n3716         return computation._calc_idxminmax(\n3717             array=self,\n3718             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n3719             dim=dim,\n3720             skipna=skipna,\n3721             fill_value=fill_value,\n3722             keep_attrs=keep_attrs,\n3723         )\n3724 \n3725     def idxmax(\n3726         self,\n3727         dim: Hashable = None,\n3728         skipna: bool = None,\n3729         fill_value: Any = dtypes.NA,\n3730         keep_attrs: bool = None,\n3731     ) -> \"DataArray\":\n3732         \"\"\"Return the coordinate label of the maximum value along a dimension.\n3733 \n3734         Returns a new `DataArray` named after the dimension with the values of\n3735         the coordinate labels along that dimension corresponding to maximum\n3736         values along that dimension.\n3737 \n3738         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n3739         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n3740 \n3741         Parameters\n3742         ----------\n3743         dim : str, optional\n3744             Dimension over which to apply `idxmax`.  This is optional for 1D\n3745             arrays, but required for arrays with 2 or more dimensions.\n3746         skipna : bool or None, default None\n3747             If True, skip missing values (as marked by NaN). By default, only\n3748             skips missing values for ``float``, ``complex``, and ``object``\n3749             dtypes; other dtypes either do not have a sentinel missing value\n3750             (``int``) or ``skipna=True`` has not been implemented\n3751             (``datetime64`` or ``timedelta64``).\n3752         fill_value : Any, default NaN\n3753             Value to be filled in case all of the values along a dimension are\n3754             null.  By default this is NaN.  The fill value and result are\n3755             automatically converted to a compatible dtype if possible.\n3756             Ignored if ``skipna`` is False.\n3757         keep_attrs : bool, default False\n3758             If True, the attributes (``attrs``) will be copied from the\n3759             original object to the new one.  If False (default), the new object\n3760             will be returned without attributes.\n3761 \n3762         Returns\n3763         -------\n3764         reduced : DataArray\n3765             New `DataArray` object with `idxmax` applied to its data and the\n3766             indicated dimension removed.\n3767 \n3768         See also\n3769         --------\n3770         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n3771 \n3772         Examples\n3773         --------\n3774 \n3775         >>> array = xr.DataArray(\n3776         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n3777         ... )\n3778         >>> array.max()\n3779         <xarray.DataArray ()>\n3780         array(2)\n3781         >>> array.argmax()\n3782         <xarray.DataArray ()>\n3783         array(1)\n3784         >>> array.idxmax()\n3785         <xarray.DataArray 'x' ()>\n3786         array('b', dtype='<U1')\n3787 \n3788         >>> array = xr.DataArray(\n3789         ...     [\n3790         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n3791         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n3792         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n3793         ...     ],\n3794         ...     dims=[\"y\", \"x\"],\n3795         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n3796         ... )\n3797         >>> array.max(dim=\"x\")\n3798         <xarray.DataArray (y: 3)>\n3799         array([2., 2., 1.])\n3800         Coordinates:\n3801           * y        (y) int64 -1 0 1\n3802         >>> array.argmax(dim=\"x\")\n3803         <xarray.DataArray (y: 3)>\n3804         array([0, 2, 2])\n3805         Coordinates:\n3806           * y        (y) int64 -1 0 1\n3807         >>> array.idxmax(dim=\"x\")\n3808         <xarray.DataArray 'x' (y: 3)>\n3809         array([0., 4., 4.])\n3810         Coordinates:\n3811           * y        (y) int64 -1 0 1\n3812         \"\"\"\n3813         return computation._calc_idxminmax(\n3814             array=self,\n3815             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n3816             dim=dim,\n3817             skipna=skipna,\n3818             fill_value=fill_value,\n3819             keep_attrs=keep_attrs,\n3820         )\n3821 \n3822     # this needs to be at the end, or mypy will confuse with `str`\n3823     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n3824     str = utils.UncachedAccessor(StringAccessor)\n3825 \n3826 \n3827 # priority most be higher than Variable to properly work with binary ufuncs\n3828 ops.inject_all_ops_and_reduce_methods(DataArray, priority=60)\n",
        "intended_behavior": "Consistency with Text Representation: The default behavior of the HTML representation should mirror the text representation, showing data and attributes sections openly for a unified user experience across console and notebook environments."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 1,
        "end": 2424,
        "class_name": null,
        "method_name": null,
        "code": "1 import copy\n2 import functools\n3 import itertools\n4 import numbers\n5 import warnings\n6 from collections import defaultdict\n7 from datetime import timedelta\n8 from distutils.version import LooseVersion\n9 from typing import Any, Dict, Hashable, Mapping, Tuple, TypeVar, Union\n10 \n11 import numpy as np\n12 import pandas as pd\n13 \n14 import xarray as xr  # only for Dataset and DataArray\n15 \n16 from . import arithmetic, common, dtypes, duck_array_ops, indexing, nputils, ops, utils\n17 from .indexing import (\n18     BasicIndexer,\n19     OuterIndexer,\n20     PandasIndexAdapter,\n21     VectorizedIndexer,\n22     as_indexable,\n23 )\n24 from .npcompat import IS_NEP18_ACTIVE\n25 from .options import _get_keep_attrs\n26 from .pycompat import dask_array_type, integer_types\n27 from .utils import (\n28     OrderedSet,\n29     _default,\n30     decode_numpy_dict_values,\n31     drop_dims_from_indexers,\n32     either_dict_or_kwargs,\n33     ensure_us_time_resolution,\n34     infix_dims,\n35 )\n36 \n37 NON_NUMPY_SUPPORTED_ARRAY_TYPES = (\n38     indexing.ExplicitlyIndexed,\n39     pd.Index,\n40 ) + dask_array_type\n41 # https://github.com/python/mypy/issues/224\n42 BASIC_INDEXING_TYPES = integer_types + (slice,)  # type: ignore\n43 \n44 VariableType = TypeVar(\"VariableType\", bound=\"Variable\")\n45 \"\"\"Type annotation to be used when methods of Variable return self or a copy of self.\n46 When called from an instance of a subclass, e.g. IndexVariable, mypy identifies the\n47 output as an instance of the subclass.\n48 \n49 Usage::\n50 \n51    class Variable:\n52        def f(self: VariableType, ...) -> VariableType:\n53            ...\n54 \"\"\"\n55 \n56 \n57 class MissingDimensionsError(ValueError):\n58     \"\"\"Error class used when we can't safely guess a dimension name.\n59     \"\"\"\n60 \n61     # inherits from ValueError for backward compatibility\n62     # TODO: move this to an xarray.exceptions module?\n63 \n64 \n65 def as_variable(obj, name=None) -> \"Union[Variable, IndexVariable]\":\n66     \"\"\"Convert an object into a Variable.\n67 \n68     Parameters\n69     ----------\n70     obj : object\n71         Object to convert into a Variable.\n72 \n73         - If the object is already a Variable, return a shallow copy.\n74         - Otherwise, if the object has 'dims' and 'data' attributes, convert\n75           it into a new Variable.\n76         - If all else fails, attempt to convert the object into a Variable by\n77           unpacking it into the arguments for creating a new Variable.\n78     name : str, optional\n79         If provided:\n80 \n81         - `obj` can be a 1D array, which is assumed to label coordinate values\n82           along a dimension of this given name.\n83         - Variables with name matching one of their dimensions are converted\n84           into `IndexVariable` objects.\n85 \n86     Returns\n87     -------\n88     var : Variable\n89         The newly created variable.\n90 \n91     \"\"\"\n92     from .dataarray import DataArray\n93 \n94     # TODO: consider extending this method to automatically handle Iris and\n95     if isinstance(obj, DataArray):\n96         # extract the primary Variable from DataArrays\n97         obj = obj.variable\n98 \n99     if isinstance(obj, Variable):\n100         obj = obj.copy(deep=False)\n101     elif isinstance(obj, tuple):\n102         try:\n103             obj = Variable(*obj)\n104         except (TypeError, ValueError) as error:\n105             # use .format() instead of % because it handles tuples consistently\n106             raise error.__class__(\n107                 \"Could not convert tuple of form \"\n108                 \"(dims, data[, attrs, encoding]): \"\n109                 \"{} to Variable.\".format(obj)\n110             )\n111     elif utils.is_scalar(obj):\n112         obj = Variable([], obj)\n113     elif isinstance(obj, (pd.Index, IndexVariable)) and obj.name is not None:\n114         obj = Variable(obj.name, obj)\n115     elif isinstance(obj, (set, dict)):\n116         raise TypeError(\"variable {!r} has invalid type {!r}\".format(name, type(obj)))\n117     elif name is not None:\n118         data = as_compatible_data(obj)\n119         if data.ndim != 1:\n120             raise MissingDimensionsError(\n121                 \"cannot set variable %r with %r-dimensional data \"\n122                 \"without explicit dimension names. Pass a tuple of \"\n123                 \"(dims, data) instead.\" % (name, data.ndim)\n124             )\n125         obj = Variable(name, data, fastpath=True)\n126     else:\n127         raise TypeError(\n128             \"unable to convert object into a variable without an \"\n129             \"explicit list of dimensions: %r\" % obj\n130         )\n131 \n132     if name is not None and name in obj.dims:\n133         # convert the Variable into an Index\n134         if obj.ndim != 1:\n135             raise MissingDimensionsError(\n136                 \"%r has more than 1-dimension and the same name as one of its \"\n137                 \"dimensions %r. xarray disallows such variables because they \"\n138                 \"conflict with the coordinates used to label \"\n139                 \"dimensions.\" % (name, obj.dims)\n140             )\n141         obj = obj.to_index_variable()\n142 \n143     return obj\n144 \n145 \n146 def _maybe_wrap_data(data):\n147     \"\"\"\n148     Put pandas.Index and numpy.ndarray arguments in adapter objects to ensure\n149     they can be indexed properly.\n150 \n151     NumpyArrayAdapter, PandasIndexAdapter and LazilyOuterIndexedArray should\n152     all pass through unmodified.\n153     \"\"\"\n154     if isinstance(data, pd.Index):\n155         return PandasIndexAdapter(data)\n156     return data\n157 \n158 \n159 def _possibly_convert_objects(values):\n160     \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into\n161     datetime64 and timedelta64, according to the pandas convention.\n162     \"\"\"\n163     return np.asarray(pd.Series(values.ravel())).reshape(values.shape)\n164 \n165 \n166 def as_compatible_data(data, fastpath=False):\n167     \"\"\"Prepare and wrap data to put in a Variable.\n168 \n169     - If data does not have the necessary attributes, convert it to ndarray.\n170     - If data has dtype=datetime64, ensure that it has ns precision. If it's a\n171       pandas.Timestamp, convert it to datetime64.\n172     - If data is already a pandas or xarray object (other than an Index), just\n173       use the values.\n174 \n175     Finally, wrap it up with an adapter if necessary.\n176     \"\"\"\n177     if fastpath and getattr(data, \"ndim\", 0) > 0:\n178         # can't use fastpath (yet) for scalars\n179         return _maybe_wrap_data(data)\n180 \n181     if isinstance(data, Variable):\n182         return data.data\n183 \n184     if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):\n185         return _maybe_wrap_data(data)\n186 \n187     if isinstance(data, tuple):\n188         data = utils.to_0d_object_array(data)\n189 \n190     if isinstance(data, pd.Timestamp):\n191         # TODO: convert, handle datetime objects, too\n192         data = np.datetime64(data.value, \"ns\")\n193 \n194     if isinstance(data, timedelta):\n195         data = np.timedelta64(getattr(data, \"value\", data), \"ns\")\n196 \n197     # we don't want nested self-described arrays\n198     data = getattr(data, \"values\", data)\n199 \n200     if isinstance(data, np.ma.MaskedArray):\n201         mask = np.ma.getmaskarray(data)\n202         if mask.any():\n203             dtype, fill_value = dtypes.maybe_promote(data.dtype)\n204             data = np.asarray(data, dtype=dtype)\n205             data[mask] = fill_value\n206         else:\n207             data = np.asarray(data)\n208 \n209     if not isinstance(data, np.ndarray):\n210         if hasattr(data, \"__array_function__\"):\n211             if IS_NEP18_ACTIVE:\n212                 return data\n213             else:\n214                 raise TypeError(\n215                     \"Got an NumPy-like array type providing the \"\n216                     \"__array_function__ protocol but NEP18 is not enabled. \"\n217                     \"Check that numpy >= v1.16 and that the environment \"\n218                     'variable \"NUMPY_EXPERIMENTAL_ARRAY_FUNCTION\" is set to '\n219                     '\"1\"'\n220                 )\n221 \n222     # validate whether the data is valid data types\n223     data = np.asarray(data)\n224 \n225     if isinstance(data, np.ndarray):\n226         if data.dtype.kind == \"O\":\n227             data = _possibly_convert_objects(data)\n228         elif data.dtype.kind == \"M\":\n229             data = np.asarray(data, \"datetime64[ns]\")\n230         elif data.dtype.kind == \"m\":\n231             data = np.asarray(data, \"timedelta64[ns]\")\n232 \n233     return _maybe_wrap_data(data)\n234 \n235 \n236 def _as_array_or_item(data):\n237     \"\"\"Return the given values as a numpy array, or as an individual item if\n238     it's a 0d datetime64 or timedelta64 array.\n239 \n240     Importantly, this function does not copy data if it is already an ndarray -\n241     otherwise, it will not be possible to update Variable values in place.\n242 \n243     This function mostly exists because 0-dimensional ndarrays with\n244     dtype=datetime64 are broken :(\n245     https://github.com/numpy/numpy/issues/4337\n246     https://github.com/numpy/numpy/issues/7619\n247 \n248     TODO: remove this (replace with np.asarray) once these issues are fixed\n249     \"\"\"\n250     data = np.asarray(data)\n251     if data.ndim == 0:\n252         if data.dtype.kind == \"M\":\n253             data = np.datetime64(data, \"ns\")\n254         elif data.dtype.kind == \"m\":\n255             data = np.timedelta64(data, \"ns\")\n256     return data\n257 \n258 \n259 class Variable(\n260     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n261 ):\n262     \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n263     which describe a single Array. A single Variable object is not fully\n264     described outside the context of its parent Dataset (if you want such a\n265     fully described object, use a DataArray instead).\n266 \n267     The main functional difference between Variables and numpy arrays is that\n268     numerical operations on Variables implement array broadcasting by dimension\n269     name. For example, adding an Variable with dimensions `('time',)` to\n270     another Variable with dimensions `('space',)` results in a new Variable\n271     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n272     like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n273     instead of an \"axis\".\n274 \n275     Variables are light-weight objects used as the building block for datasets.\n276     They are more primitive objects, so operations with them provide marginally\n277     higher performance than using DataArrays. However, manipulating data in the\n278     form of a Dataset or DataArray should almost always be preferred, because\n279     they can use more complete metadata in context of coordinate labels.\n280     \"\"\"\n281 \n282     __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n283 \n284     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n285         \"\"\"\n286         Parameters\n287         ----------\n288         dims : str or sequence of str\n289             Name(s) of the the data dimension(s). Must be either a string (only\n290             for 1D data) or a sequence of strings with length equal to the\n291             number of dimensions.\n292         data : array_like\n293             Data array which supports numpy-like data access.\n294         attrs : dict_like or None, optional\n295             Attributes to assign to the new variable. If None (default), an\n296             empty attribute dictionary is initialized.\n297         encoding : dict_like or None, optional\n298             Dictionary specifying how to encode this array's data into a\n299             serialized format like netCDF4. Currently used keys (for netCDF)\n300             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n301             Well-behaved code to serialize a Variable should ignore\n302             unrecognized encoding items.\n303         \"\"\"\n304         self._data = as_compatible_data(data, fastpath=fastpath)\n305         self._dims = self._parse_dimensions(dims)\n306         self._attrs = None\n307         self._encoding = None\n308         if attrs is not None:\n309             self.attrs = attrs\n310         if encoding is not None:\n311             self.encoding = encoding\n312 \n313     @property\n314     def dtype(self):\n315         return self._data.dtype\n316 \n317     @property\n318     def shape(self):\n319         return self._data.shape\n320 \n321     @property\n322     def nbytes(self):\n323         return self.size * self.dtype.itemsize\n324 \n325     @property\n326     def _in_memory(self):\n327         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n328             isinstance(self._data, indexing.MemoryCachedArray)\n329             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n330         )\n331 \n332     @property\n333     def data(self):\n334         if hasattr(self._data, \"__array_function__\") or isinstance(\n335             self._data, dask_array_type\n336         ):\n337             return self._data\n338         else:\n339             return self.values\n340 \n341     @data.setter\n342     def data(self, data):\n343         data = as_compatible_data(data)\n344         if data.shape != self.shape:\n345             raise ValueError(\n346                 f\"replacement data must match the Variable's shape. \"\n347                 f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n348             )\n349         self._data = data\n350 \n351     def load(self, **kwargs):\n352         \"\"\"Manually trigger loading of this variable's data from disk or a\n353         remote source into memory and return this variable.\n354 \n355         Normally, it should not be necessary to call this method in user code,\n356         because all xarray functions should either work on deferred data or\n357         load data automatically.\n358 \n359         Parameters\n360         ----------\n361         **kwargs : dict\n362             Additional keyword arguments passed on to ``dask.array.compute``.\n363 \n364         See Also\n365         --------\n366         dask.array.compute\n367         \"\"\"\n368         if isinstance(self._data, dask_array_type):\n369             self._data = as_compatible_data(self._data.compute(**kwargs))\n370         elif not hasattr(self._data, \"__array_function__\"):\n371             self._data = np.asarray(self._data)\n372         return self\n373 \n374     def compute(self, **kwargs):\n375         \"\"\"Manually trigger loading of this variable's data from disk or a\n376         remote source into memory and return a new variable. The original is\n377         left unaltered.\n378 \n379         Normally, it should not be necessary to call this method in user code,\n380         because all xarray functions should either work on deferred data or\n381         load data automatically.\n382 \n383         Parameters\n384         ----------\n385         **kwargs : dict\n386             Additional keyword arguments passed on to ``dask.array.compute``.\n387 \n388         See Also\n389         --------\n390         dask.array.compute\n391         \"\"\"\n392         new = self.copy(deep=False)\n393         return new.load(**kwargs)\n394 \n395     def __dask_tokenize__(self):\n396         # Use v.data, instead of v._data, in order to cope with the wrappers\n397         # around NetCDF and the like\n398         from dask.base import normalize_token\n399 \n400         return normalize_token((type(self), self._dims, self.data, self._attrs))\n401 \n402     def __dask_graph__(self):\n403         if isinstance(self._data, dask_array_type):\n404             return self._data.__dask_graph__()\n405         else:\n406             return None\n407 \n408     def __dask_keys__(self):\n409         return self._data.__dask_keys__()\n410 \n411     def __dask_layers__(self):\n412         return self._data.__dask_layers__()\n413 \n414     @property\n415     def __dask_optimize__(self):\n416         return self._data.__dask_optimize__\n417 \n418     @property\n419     def __dask_scheduler__(self):\n420         return self._data.__dask_scheduler__\n421 \n422     def __dask_postcompute__(self):\n423         array_func, array_args = self._data.__dask_postcompute__()\n424         return (\n425             self._dask_finalize,\n426             (array_func, array_args, self._dims, self._attrs, self._encoding),\n427         )\n428 \n429     def __dask_postpersist__(self):\n430         array_func, array_args = self._data.__dask_postpersist__()\n431         return (\n432             self._dask_finalize,\n433             (array_func, array_args, self._dims, self._attrs, self._encoding),\n434         )\n435 \n436     @staticmethod\n437     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n438         if isinstance(results, dict):  # persist case\n439             name = array_args[0]\n440             results = {k: v for k, v in results.items() if k[0] == name}\n441         data = array_func(results, *array_args)\n442         return Variable(dims, data, attrs=attrs, encoding=encoding)\n443 \n444     @property\n445     def values(self):\n446         \"\"\"The variable's data as a numpy.ndarray\"\"\"\n447         return _as_array_or_item(self._data)\n448 \n449     @values.setter\n450     def values(self, values):\n451         self.data = values\n452 \n453     def to_base_variable(self):\n454         \"\"\"Return this variable as a base xarray.Variable\"\"\"\n455         return Variable(\n456             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n457         )\n458 \n459     to_variable = utils.alias(to_base_variable, \"to_variable\")\n460 \n461     def to_index_variable(self):\n462         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n463         return IndexVariable(\n464             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n465         )\n466 \n467     to_coord = utils.alias(to_index_variable, \"to_coord\")\n468 \n469     def to_index(self):\n470         \"\"\"Convert this variable to a pandas.Index\"\"\"\n471         return self.to_index_variable().to_index()\n472 \n473     def to_dict(self, data=True):\n474         \"\"\"Dictionary representation of variable.\"\"\"\n475         item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n476         if data:\n477             item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n478         else:\n479             item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n480         return item\n481 \n482     @property\n483     def dims(self):\n484         \"\"\"Tuple of dimension names with which this variable is associated.\n485         \"\"\"\n486         return self._dims\n487 \n488     @dims.setter\n489     def dims(self, value):\n490         self._dims = self._parse_dimensions(value)\n491 \n492     def _parse_dimensions(self, dims):\n493         if isinstance(dims, str):\n494             dims = (dims,)\n495         dims = tuple(dims)\n496         if len(dims) != self.ndim:\n497             raise ValueError(\n498                 \"dimensions %s must have the same length as the \"\n499                 \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n500             )\n501         return dims\n502 \n503     def _item_key_to_tuple(self, key):\n504         if utils.is_dict_like(key):\n505             return tuple(key.get(dim, slice(None)) for dim in self.dims)\n506         else:\n507             return key\n508 \n509     def _broadcast_indexes(self, key):\n510         \"\"\"Prepare an indexing key for an indexing operation.\n511 \n512         Parameters\n513         -----------\n514         key: int, slice, array, dict or tuple of integer, slices and arrays\n515             Any valid input for indexing.\n516 \n517         Returns\n518         -------\n519         dims: tuple\n520             Dimension of the resultant variable.\n521         indexers: IndexingTuple subclass\n522             Tuple of integer, array-like, or slices to use when indexing\n523             self._data. The type of this argument indicates the type of\n524             indexing to perform, either basic, outer or vectorized.\n525         new_order : Optional[Sequence[int]]\n526             Optional reordering to do on the result of indexing. If not None,\n527             the first len(new_order) indexing should be moved to these\n528             positions.\n529         \"\"\"\n530         key = self._item_key_to_tuple(key)  # key is a tuple\n531         # key is a tuple of full size\n532         key = indexing.expanded_indexer(key, self.ndim)\n533         # Convert a scalar Variable to an integer\n534         key = tuple(\n535             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n536         )\n537         # Convert a 0d-array to an integer\n538         key = tuple(\n539             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n540         )\n541 \n542         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n543             return self._broadcast_indexes_basic(key)\n544 \n545         self._validate_indexers(key)\n546         # Detect it can be mapped as an outer indexer\n547         # If all key is unlabeled, or\n548         # key can be mapped as an OuterIndexer.\n549         if all(not isinstance(k, Variable) for k in key):\n550             return self._broadcast_indexes_outer(key)\n551 \n552         # If all key is 1-dimensional and there are no duplicate labels,\n553         # key can be mapped as an OuterIndexer.\n554         dims = []\n555         for k, d in zip(key, self.dims):\n556             if isinstance(k, Variable):\n557                 if len(k.dims) > 1:\n558                     return self._broadcast_indexes_vectorized(key)\n559                 dims.append(k.dims[0])\n560             elif not isinstance(k, integer_types):\n561                 dims.append(d)\n562         if len(set(dims)) == len(dims):\n563             return self._broadcast_indexes_outer(key)\n564 \n565         return self._broadcast_indexes_vectorized(key)\n566 \n567     def _broadcast_indexes_basic(self, key):\n568         dims = tuple(\n569             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n570         )\n571         return dims, BasicIndexer(key), None\n572 \n573     def _validate_indexers(self, key):\n574         \"\"\" Make sanity checks \"\"\"\n575         for dim, k in zip(self.dims, key):\n576             if isinstance(k, BASIC_INDEXING_TYPES):\n577                 pass\n578             else:\n579                 if not isinstance(k, Variable):\n580                     k = np.asarray(k)\n581                     if k.ndim > 1:\n582                         raise IndexError(\n583                             \"Unlabeled multi-dimensional array cannot be \"\n584                             \"used for indexing: {}\".format(k)\n585                         )\n586                 if k.dtype.kind == \"b\":\n587                     if self.shape[self.get_axis_num(dim)] != len(k):\n588                         raise IndexError(\n589                             \"Boolean array size {:d} is used to index array \"\n590                             \"with shape {:s}.\".format(len(k), str(self.shape))\n591                         )\n592                     if k.ndim > 1:\n593                         raise IndexError(\n594                             \"{}-dimensional boolean indexing is \"\n595                             \"not supported. \".format(k.ndim)\n596                         )\n597                     if getattr(k, \"dims\", (dim,)) != (dim,):\n598                         raise IndexError(\n599                             \"Boolean indexer should be unlabeled or on the \"\n600                             \"same dimension to the indexed array. Indexer is \"\n601                             \"on {:s} but the target dimension is {:s}.\".format(\n602                                 str(k.dims), dim\n603                             )\n604                         )\n605 \n606     def _broadcast_indexes_outer(self, key):\n607         dims = tuple(\n608             k.dims[0] if isinstance(k, Variable) else dim\n609             for k, dim in zip(key, self.dims)\n610             if not isinstance(k, integer_types)\n611         )\n612 \n613         new_key = []\n614         for k in key:\n615             if isinstance(k, Variable):\n616                 k = k.data\n617             if not isinstance(k, BASIC_INDEXING_TYPES):\n618                 k = np.asarray(k)\n619                 if k.size == 0:\n620                     # Slice by empty list; numpy could not infer the dtype\n621                     k = k.astype(int)\n622                 elif k.dtype.kind == \"b\":\n623                     (k,) = np.nonzero(k)\n624             new_key.append(k)\n625 \n626         return dims, OuterIndexer(tuple(new_key)), None\n627 \n628     def _nonzero(self):\n629         \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n630         # TODO we should replace dask's native nonzero\n631         # after https://github.com/dask/dask/issues/1076 is implemented.\n632         nonzeros = np.nonzero(self.data)\n633         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n634 \n635     def _broadcast_indexes_vectorized(self, key):\n636         variables = []\n637         out_dims_set = OrderedSet()\n638         for dim, value in zip(self.dims, key):\n639             if isinstance(value, slice):\n640                 out_dims_set.add(dim)\n641             else:\n642                 variable = (\n643                     value\n644                     if isinstance(value, Variable)\n645                     else as_variable(value, name=dim)\n646                 )\n647                 if variable.dtype.kind == \"b\":  # boolean indexing case\n648                     (variable,) = variable._nonzero()\n649 \n650                 variables.append(variable)\n651                 out_dims_set.update(variable.dims)\n652 \n653         variable_dims = set()\n654         for variable in variables:\n655             variable_dims.update(variable.dims)\n656 \n657         slices = []\n658         for i, (dim, value) in enumerate(zip(self.dims, key)):\n659             if isinstance(value, slice):\n660                 if dim in variable_dims:\n661                     # We only convert slice objects to variables if they share\n662                     # a dimension with at least one other variable. Otherwise,\n663                     # we can equivalently leave them as slices aknd transpose\n664                     # the result. This is significantly faster/more efficient\n665                     # for most array backends.\n666                     values = np.arange(*value.indices(self.sizes[dim]))\n667                     variables.insert(i - len(slices), Variable((dim,), values))\n668                 else:\n669                     slices.append((i, value))\n670 \n671         try:\n672             variables = _broadcast_compat_variables(*variables)\n673         except ValueError:\n674             raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n675 \n676         out_key = [variable.data for variable in variables]\n677         out_dims = tuple(out_dims_set)\n678         slice_positions = set()\n679         for i, value in slices:\n680             out_key.insert(i, value)\n681             new_position = out_dims.index(self.dims[i])\n682             slice_positions.add(new_position)\n683 \n684         if slice_positions:\n685             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n686         else:\n687             new_order = None\n688 \n689         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n690 \n691     def __getitem__(self: VariableType, key) -> VariableType:\n692         \"\"\"Return a new Variable object whose contents are consistent with\n693         getting the provided key from the underlying data.\n694 \n695         NB. __getitem__ and __setitem__ implement xarray-style indexing,\n696         where if keys are unlabeled arrays, we index the array orthogonally\n697         with them. If keys are labeled array (such as Variables), they are\n698         broadcasted with our usual scheme and then the array is indexed with\n699         the broadcasted key, like numpy's fancy indexing.\n700 \n701         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n702         array `x.values` directly.\n703         \"\"\"\n704         dims, indexer, new_order = self._broadcast_indexes(key)\n705         data = as_indexable(self._data)[indexer]\n706         if new_order:\n707             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n708         return self._finalize_indexing_result(dims, data)\n709 \n710     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n711         \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\n712         \"\"\"\n713         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n714 \n715     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n716         \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n717         # TODO(shoyer): expose this method in public API somewhere (isel?) and\n718         # use it for reindex.\n719         # TODO(shoyer): add a sanity check that all other integers are\n720         # non-negative\n721         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n722         # that is actually indexed rather than mapping it to the last value\n723         # along each axis.\n724 \n725         if fill_value is dtypes.NA:\n726             fill_value = dtypes.get_fill_value(self.dtype)\n727 \n728         dims, indexer, new_order = self._broadcast_indexes(key)\n729 \n730         if self.size:\n731             if isinstance(self._data, dask_array_type):\n732                 # dask's indexing is faster this way; also vindex does not\n733                 # support negative indices yet:\n734                 # https://github.com/dask/dask/pull/2967\n735                 actual_indexer = indexing.posify_mask_indexer(indexer)\n736             else:\n737                 actual_indexer = indexer\n738 \n739             data = as_indexable(self._data)[actual_indexer]\n740             mask = indexing.create_mask(indexer, self.shape, data)\n741             # we need to invert the mask in order to pass data first. This helps\n742             # pint to choose the correct unit\n743             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n744             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n745         else:\n746             # array cannot be indexed along dimensions of size 0, so just\n747             # build the mask directly instead.\n748             mask = indexing.create_mask(indexer, self.shape)\n749             data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n750 \n751         if new_order:\n752             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n753         return self._finalize_indexing_result(dims, data)\n754 \n755     def __setitem__(self, key, value):\n756         \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n757         orthogonal indexing.\n758 \n759         See __getitem__ for more details.\n760         \"\"\"\n761         dims, index_tuple, new_order = self._broadcast_indexes(key)\n762 \n763         if not isinstance(value, Variable):\n764             value = as_compatible_data(value)\n765             if value.ndim > len(dims):\n766                 raise ValueError(\n767                     \"shape mismatch: value array of shape %s could not be \"\n768                     \"broadcast to indexing result with %s dimensions\"\n769                     % (value.shape, len(dims))\n770                 )\n771             if value.ndim == 0:\n772                 value = Variable((), value)\n773             else:\n774                 value = Variable(dims[-value.ndim :], value)\n775         # broadcast to become assignable\n776         value = value.set_dims(dims).data\n777 \n778         if new_order:\n779             value = duck_array_ops.asarray(value)\n780             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n781             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n782 \n783         indexable = as_indexable(self._data)\n784         indexable[index_tuple] = value\n785 \n786     @property\n787     def attrs(self) -> Dict[Hashable, Any]:\n788         \"\"\"Dictionary of local attributes on this variable.\n789         \"\"\"\n790         if self._attrs is None:\n791             self._attrs = {}\n792         return self._attrs\n793 \n794     @attrs.setter\n795     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n796         self._attrs = dict(value)\n797 \n798     @property\n799     def encoding(self):\n800         \"\"\"Dictionary of encodings on this variable.\n801         \"\"\"\n802         if self._encoding is None:\n803             self._encoding = {}\n804         return self._encoding\n805 \n806     @encoding.setter\n807     def encoding(self, value):\n808         try:\n809             self._encoding = dict(value)\n810         except ValueError:\n811             raise ValueError(\"encoding must be castable to a dictionary\")\n812 \n813     def copy(self, deep=True, data=None):\n814         \"\"\"Returns a copy of this object.\n815 \n816         If `deep=True`, the data array is loaded into memory and copied onto\n817         the new object. Dimensions, attributes and encodings are always copied.\n818 \n819         Use `data` to create a new object with the same structure as\n820         original but entirely new data.\n821 \n822         Parameters\n823         ----------\n824         deep : bool, optional\n825             Whether the data array is loaded into memory and copied onto\n826             the new object. Default is True.\n827         data : array_like, optional\n828             Data to use in the new object. Must have same shape as original.\n829             When `data` is used, `deep` is ignored.\n830 \n831         Returns\n832         -------\n833         object : Variable\n834             New object with dimensions, attributes, encodings, and optionally\n835             data copied from original.\n836 \n837         Examples\n838         --------\n839 \n840         Shallow copy versus deep copy\n841 \n842         >>> var = xr.Variable(data=[1, 2, 3], dims=\"x\")\n843         >>> var.copy()\n844         <xarray.Variable (x: 3)>\n845         array([1, 2, 3])\n846         >>> var_0 = var.copy(deep=False)\n847         >>> var_0[0] = 7\n848         >>> var_0\n849         <xarray.Variable (x: 3)>\n850         array([7, 2, 3])\n851         >>> var\n852         <xarray.Variable (x: 3)>\n853         array([7, 2, 3])\n854 \n855         Changing the data using the ``data`` argument maintains the\n856         structure of the original object, but with the new data. Original\n857         object is unaffected.\n858 \n859         >>> var.copy(data=[0.1, 0.2, 0.3])\n860         <xarray.Variable (x: 3)>\n861         array([ 0.1,  0.2,  0.3])\n862         >>> var\n863         <xarray.Variable (x: 3)>\n864         array([7, 2, 3])\n865 \n866         See Also\n867         --------\n868         pandas.DataFrame.copy\n869         \"\"\"\n870         if data is None:\n871             data = self._data\n872 \n873             if isinstance(data, indexing.MemoryCachedArray):\n874                 # don't share caching between copies\n875                 data = indexing.MemoryCachedArray(data.array)\n876 \n877             if deep:\n878                 if hasattr(data, \"__array_function__\") or isinstance(\n879                     data, dask_array_type\n880                 ):\n881                     data = data.copy()\n882                 elif not isinstance(data, PandasIndexAdapter):\n883                     # pandas.Index is immutable\n884                     data = np.array(data)\n885         else:\n886             data = as_compatible_data(data)\n887             if self.shape != data.shape:\n888                 raise ValueError(\n889                     \"Data shape {} must match shape of object {}\".format(\n890                         data.shape, self.shape\n891                     )\n892                 )\n893 \n894         # note:\n895         # dims is already an immutable tuple\n896         # attributes and encoding will be copied when the new Array is created\n897         return self._replace(data=data)\n898 \n899     def _replace(\n900         self, dims=_default, data=_default, attrs=_default, encoding=_default\n901     ) -> \"Variable\":\n902         if dims is _default:\n903             dims = copy.copy(self._dims)\n904         if data is _default:\n905             data = copy.copy(self.data)\n906         if attrs is _default:\n907             attrs = copy.copy(self._attrs)\n908         if encoding is _default:\n909             encoding = copy.copy(self._encoding)\n910         return type(self)(dims, data, attrs, encoding, fastpath=True)\n911 \n912     def __copy__(self):\n913         return self.copy(deep=False)\n914 \n915     def __deepcopy__(self, memo=None):\n916         # memo does nothing but is required for compatibility with\n917         # copy.deepcopy\n918         return self.copy(deep=True)\n919 \n920     # mutable objects should not be hashable\n921     # https://github.com/python/mypy/issues/4266\n922     __hash__ = None  # type: ignore\n923 \n924     @property\n925     def chunks(self):\n926         \"\"\"Block dimensions for this array's data or None if it's not a dask\n927         array.\n928         \"\"\"\n929         return getattr(self._data, \"chunks\", None)\n930 \n931     _array_counter = itertools.count()\n932 \n933     def chunk(self, chunks=None, name=None, lock=False):\n934         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n935 \n936         If this variable is a non-dask array, it will be converted to dask\n937         array. If it's a dask array, it will be rechunked to the given chunk\n938         sizes.\n939 \n940         If neither chunks is not provided for one or more dimensions, chunk\n941         sizes along that dimension will not be updated; non-dask arrays will be\n942         converted into dask arrays with a single block.\n943 \n944         Parameters\n945         ----------\n946         chunks : int, tuple or dict, optional\n947             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n948             ``{'x': 5, 'y': 5}``.\n949         name : str, optional\n950             Used to generate the name for this array in the internal dask\n951             graph. Does not need not be unique.\n952         lock : optional\n953             Passed on to :py:func:`dask.array.from_array`, if the array is not\n954             already as dask array.\n955 \n956         Returns\n957         -------\n958         chunked : xarray.Variable\n959         \"\"\"\n960         import dask\n961         import dask.array as da\n962 \n963         if utils.is_dict_like(chunks):\n964             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n965 \n966         if chunks is None:\n967             chunks = self.chunks or self.shape\n968 \n969         data = self._data\n970         if isinstance(data, da.Array):\n971             data = data.rechunk(chunks)\n972         else:\n973             if isinstance(data, indexing.ExplicitlyIndexed):\n974                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n975                 # that can't handle general array indexing. For example, in netCDF4 you\n976                 # can do \"outer\" indexing along two dimensions independent, which works\n977                 # differently from how NumPy handles it.\n978                 # da.from_array works by using lazy indexing with a tuple of slices.\n979                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n980                 # different indexing types in an explicit way:\n981                 # https://github.com/dask/dask/issues/2883\n982                 data = indexing.ImplicitToExplicitIndexingAdapter(\n983                     data, indexing.OuterIndexer\n984                 )\n985                 if LooseVersion(dask.__version__) < \"2.0.0\":\n986                     kwargs = {}\n987                 else:\n988                     # All of our lazily loaded backend array classes should use NumPy\n989                     # array operations.\n990                     kwargs = {\"meta\": np.ndarray}\n991             else:\n992                 kwargs = {}\n993 \n994             if utils.is_dict_like(chunks):\n995                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n996 \n997             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n998 \n999         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n1000 \n1001     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n1002         \"\"\"\n1003         use sparse-array as backend.\n1004         \"\"\"\n1005         import sparse\n1006 \n1007         # TODO  what to do if dask-backended?\n1008         if fill_value is dtypes.NA:\n1009             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1010         else:\n1011             dtype = dtypes.result_type(self.dtype, fill_value)\n1012 \n1013         if sparse_format is _default:\n1014             sparse_format = \"coo\"\n1015         try:\n1016             as_sparse = getattr(sparse, \"as_{}\".format(sparse_format.lower()))\n1017         except AttributeError:\n1018             raise ValueError(\"{} is not a valid sparse format\".format(sparse_format))\n1019 \n1020         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n1021         return self._replace(data=data)\n1022 \n1023     def _to_dense(self):\n1024         \"\"\"\n1025         Change backend from sparse to np.array\n1026         \"\"\"\n1027         if hasattr(self._data, \"todense\"):\n1028             return self._replace(data=self._data.todense())\n1029         return self.copy(deep=False)\n1030 \n1031     def isel(\n1032         self: VariableType,\n1033         indexers: Mapping[Hashable, Any] = None,\n1034         missing_dims: str = \"raise\",\n1035         **indexers_kwargs: Any,\n1036     ) -> VariableType:\n1037         \"\"\"Return a new array indexed along the specified dimension(s).\n1038 \n1039         Parameters\n1040         ----------\n1041         **indexers : {dim: indexer, ...}\n1042             Keyword arguments with names matching dimensions and values given\n1043             by integers, slice objects or arrays.\n1044         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default \"raise\"\n1045             What to do if dimensions that should be selected from are not present in the\n1046             DataArray:\n1047             - \"exception\": raise an exception\n1048             - \"warning\": raise a warning, and ignore the missing dimensions\n1049             - \"ignore\": ignore the missing dimensions\n1050 \n1051         Returns\n1052         -------\n1053         obj : Array object\n1054             A new Array with the selected data and dimensions. In general,\n1055             the new variable's data will be a view of this variable's data,\n1056             unless numpy fancy indexing was triggered by using an array\n1057             indexer, in which case the data will be a copy.\n1058         \"\"\"\n1059         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1060 \n1061         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n1062 \n1063         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n1064         return self[key]\n1065 \n1066     def squeeze(self, dim=None):\n1067         \"\"\"Return a new object with squeezed data.\n1068 \n1069         Parameters\n1070         ----------\n1071         dim : None or str or tuple of str, optional\n1072             Selects a subset of the length one dimensions. If a dimension is\n1073             selected with length greater than one, an error is raised. If\n1074             None, all length one dimensions are squeezed.\n1075 \n1076         Returns\n1077         -------\n1078         squeezed : same type as caller\n1079             This object, but with with all or a subset of the dimensions of\n1080             length 1 removed.\n1081 \n1082         See Also\n1083         --------\n1084         numpy.squeeze\n1085         \"\"\"\n1086         dims = common.get_squeeze_dims(self, dim)\n1087         return self.isel({d: 0 for d in dims})\n1088 \n1089     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n1090         axis = self.get_axis_num(dim)\n1091 \n1092         if count > 0:\n1093             keep = slice(None, -count)\n1094         elif count < 0:\n1095             keep = slice(-count, None)\n1096         else:\n1097             keep = slice(None)\n1098 \n1099         trimmed_data = self[(slice(None),) * axis + (keep,)].data\n1100 \n1101         if fill_value is dtypes.NA:\n1102             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1103         else:\n1104             dtype = self.dtype\n1105 \n1106         width = min(abs(count), self.shape[axis])\n1107         dim_pad = (width, 0) if count >= 0 else (0, width)\n1108         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n1109 \n1110         data = duck_array_ops.pad(\n1111             trimmed_data.astype(dtype),\n1112             pads,\n1113             mode=\"constant\",\n1114             constant_values=fill_value,\n1115         )\n1116 \n1117         if isinstance(data, dask_array_type):\n1118             # chunked data should come out with the same chunks; this makes\n1119             # it feasible to combine shifted and unshifted data\n1120             # TODO: remove this once dask.array automatically aligns chunks\n1121             data = data.rechunk(self.data.chunks)\n1122 \n1123         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1124 \n1125     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n1126         \"\"\"\n1127         Return a new Variable with shifted data.\n1128 \n1129         Parameters\n1130         ----------\n1131         shifts : mapping of the form {dim: offset}\n1132             Integer offset to shift along each of the given dimensions.\n1133             Positive offsets shift to the right; negative offsets shift to the\n1134             left.\n1135         fill_value: scalar, optional\n1136             Value to use for newly missing values\n1137         **shifts_kwargs:\n1138             The keyword arguments form of ``shifts``.\n1139             One of shifts or shifts_kwargs must be provided.\n1140 \n1141         Returns\n1142         -------\n1143         shifted : Variable\n1144             Variable with the same dimensions and attributes but shifted data.\n1145         \"\"\"\n1146         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n1147         result = self\n1148         for dim, count in shifts.items():\n1149             result = result._shift_one_dim(dim, count, fill_value=fill_value)\n1150         return result\n1151 \n1152     def _pad_options_dim_to_index(\n1153         self,\n1154         pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],\n1155         fill_with_shape=False,\n1156     ):\n1157         if fill_with_shape:\n1158             return [\n1159                 (n, n) if d not in pad_option else pad_option[d]\n1160                 for d, n in zip(self.dims, self.data.shape)\n1161             ]\n1162         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]\n1163 \n1164     def pad(\n1165         self,\n1166         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n1167         mode: str = \"constant\",\n1168         stat_length: Union[\n1169             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1170         ] = None,\n1171         constant_values: Union[\n1172             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1173         ] = None,\n1174         end_values: Union[\n1175             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1176         ] = None,\n1177         reflect_type: str = None,\n1178         **pad_width_kwargs: Any,\n1179     ):\n1180         \"\"\"\n1181         Return a new Variable with padded data.\n1182 \n1183         Parameters\n1184         ----------\n1185         pad_width: Mapping with the form of {dim: (pad_before, pad_after)}\n1186             Number of values padded along each dimension.\n1187             {dim: pad} is a shortcut for pad_before = pad_after = pad\n1188         mode: (str)\n1189             See numpy / Dask docs\n1190         stat_length : int, tuple or mapping of the form {dim: tuple}\n1191             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n1192             values at edge of each axis used to calculate the statistic value.\n1193         constant_values : scalar, tuple or mapping of the form {dim: tuple}\n1194             Used in 'constant'.  The values to set the padded values for each\n1195             axis.\n1196         end_values : scalar, tuple or mapping of the form {dim: tuple}\n1197             Used in 'linear_ramp'.  The values used for the ending value of the\n1198             linear_ramp and that will form the edge of the padded array.\n1199         reflect_type : {'even', 'odd'}, optional\n1200             Used in 'reflect', and 'symmetric'.  The 'even' style is the\n1201             default with an unaltered reflection around the edge value.  For\n1202             the 'odd' style, the extended part of the array is created by\n1203             subtracting the reflected values from two times the edge value.\n1204         **pad_width_kwargs:\n1205             One of pad_width or pad_width_kwargs must be provided.\n1206 \n1207         Returns\n1208         -------\n1209         padded : Variable\n1210             Variable with the same dimensions and attributes but padded data.\n1211         \"\"\"\n1212         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n1213 \n1214         # change default behaviour of pad with mode constant\n1215         if mode == \"constant\" and (\n1216             constant_values is None or constant_values is dtypes.NA\n1217         ):\n1218             dtype, constant_values = dtypes.maybe_promote(self.dtype)\n1219         else:\n1220             dtype = self.dtype\n1221 \n1222         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty\n1223         if isinstance(stat_length, dict):\n1224             stat_length = self._pad_options_dim_to_index(\n1225                 stat_length, fill_with_shape=True\n1226             )\n1227         if isinstance(constant_values, dict):\n1228             constant_values = self._pad_options_dim_to_index(constant_values)\n1229         if isinstance(end_values, dict):\n1230             end_values = self._pad_options_dim_to_index(end_values)\n1231 \n1232         # workaround for bug in Dask's default value of stat_length  https://github.com/dask/dask/issues/5303\n1233         if stat_length is None and mode in [\"maximum\", \"mean\", \"median\", \"minimum\"]:\n1234             stat_length = [(n, n) for n in self.data.shape]  # type: ignore\n1235 \n1236         # change integer values to a tuple of two of those values and change pad_width to index\n1237         for k, v in pad_width.items():\n1238             if isinstance(v, numbers.Number):\n1239                 pad_width[k] = (v, v)\n1240         pad_width_by_index = self._pad_options_dim_to_index(pad_width)\n1241 \n1242         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty\n1243         pad_option_kwargs = {}\n1244         if stat_length is not None:\n1245             pad_option_kwargs[\"stat_length\"] = stat_length\n1246         if constant_values is not None:\n1247             pad_option_kwargs[\"constant_values\"] = constant_values\n1248         if end_values is not None:\n1249             pad_option_kwargs[\"end_values\"] = end_values\n1250         if reflect_type is not None:\n1251             pad_option_kwargs[\"reflect_type\"] = reflect_type  # type: ignore\n1252 \n1253         array = duck_array_ops.pad(\n1254             self.data.astype(dtype, copy=False),\n1255             pad_width_by_index,\n1256             mode=mode,\n1257             **pad_option_kwargs,\n1258         )\n1259 \n1260         return type(self)(self.dims, array)\n1261 \n1262     def _roll_one_dim(self, dim, count):\n1263         axis = self.get_axis_num(dim)\n1264 \n1265         count %= self.shape[axis]\n1266         if count != 0:\n1267             indices = [slice(-count, None), slice(None, -count)]\n1268         else:\n1269             indices = [slice(None)]\n1270 \n1271         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n1272 \n1273         data = duck_array_ops.concatenate(arrays, axis)\n1274 \n1275         if isinstance(data, dask_array_type):\n1276             # chunked data should come out with the same chunks; this makes\n1277             # it feasible to combine shifted and unshifted data\n1278             # TODO: remove this once dask.array automatically aligns chunks\n1279             data = data.rechunk(self.data.chunks)\n1280 \n1281         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1282 \n1283     def roll(self, shifts=None, **shifts_kwargs):\n1284         \"\"\"\n1285         Return a new Variable with rolld data.\n1286 \n1287         Parameters\n1288         ----------\n1289         shifts : mapping of the form {dim: offset}\n1290             Integer offset to roll along each of the given dimensions.\n1291             Positive offsets roll to the right; negative offsets roll to the\n1292             left.\n1293         **shifts_kwargs:\n1294             The keyword arguments form of ``shifts``.\n1295             One of shifts or shifts_kwargs must be provided.\n1296 \n1297         Returns\n1298         -------\n1299         shifted : Variable\n1300             Variable with the same dimensions and attributes but rolled data.\n1301         \"\"\"\n1302         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n1303 \n1304         result = self\n1305         for dim, count in shifts.items():\n1306             result = result._roll_one_dim(dim, count)\n1307         return result\n1308 \n1309     def transpose(self, *dims) -> \"Variable\":\n1310         \"\"\"Return a new Variable object with transposed dimensions.\n1311 \n1312         Parameters\n1313         ----------\n1314         *dims : str, optional\n1315             By default, reverse the dimensions. Otherwise, reorder the\n1316             dimensions to this order.\n1317 \n1318         Returns\n1319         -------\n1320         transposed : Variable\n1321             The returned object has transposed data and dimensions with the\n1322             same attributes as the original.\n1323 \n1324         Notes\n1325         -----\n1326         This operation returns a view of this variable's data. It is\n1327         lazy for dask-backed Variables but not for numpy-backed Variables.\n1328 \n1329         See Also\n1330         --------\n1331         numpy.transpose\n1332         \"\"\"\n1333         if len(dims) == 0:\n1334             dims = self.dims[::-1]\n1335         dims = tuple(infix_dims(dims, self.dims))\n1336         axes = self.get_axis_num(dims)\n1337         if len(dims) < 2 or dims == self.dims:\n1338             # no need to transpose if only one dimension\n1339             # or dims are in same order\n1340             return self.copy(deep=False)\n1341 \n1342         data = as_indexable(self._data).transpose(axes)\n1343         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n1344 \n1345     @property\n1346     def T(self) -> \"Variable\":\n1347         return self.transpose()\n1348 \n1349     def set_dims(self, dims, shape=None):\n1350         \"\"\"Return a new variable with given set of dimensions.\n1351         This method might be used to attach new dimension(s) to variable.\n1352 \n1353         When possible, this operation does not copy this variable's data.\n1354 \n1355         Parameters\n1356         ----------\n1357         dims : str or sequence of str or dict\n1358             Dimensions to include on the new variable. If a dict, values are\n1359             used to provide the sizes of new dimensions; otherwise, new\n1360             dimensions are inserted with length 1.\n1361 \n1362         Returns\n1363         -------\n1364         Variable\n1365         \"\"\"\n1366         if isinstance(dims, str):\n1367             dims = [dims]\n1368 \n1369         if shape is None and utils.is_dict_like(dims):\n1370             shape = dims.values()\n1371 \n1372         missing_dims = set(self.dims) - set(dims)\n1373         if missing_dims:\n1374             raise ValueError(\n1375                 \"new dimensions %r must be a superset of \"\n1376                 \"existing dimensions %r\" % (dims, self.dims)\n1377             )\n1378 \n1379         self_dims = set(self.dims)\n1380         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n1381 \n1382         if self.dims == expanded_dims:\n1383             # don't use broadcast_to unless necessary so the result remains\n1384             # writeable if possible\n1385             expanded_data = self.data\n1386         elif shape is not None:\n1387             dims_map = dict(zip(dims, shape))\n1388             tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n1389             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n1390         else:\n1391             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n1392 \n1393         expanded_var = Variable(\n1394             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n1395         )\n1396         return expanded_var.transpose(*dims)\n1397 \n1398     def _stack_once(self, dims, new_dim):\n1399         if not set(dims) <= set(self.dims):\n1400             raise ValueError(\"invalid existing dimensions: %s\" % dims)\n1401 \n1402         if new_dim in self.dims:\n1403             raise ValueError(\n1404                 \"cannot create a new dimension with the same \"\n1405                 \"name as an existing dimension\"\n1406             )\n1407 \n1408         if len(dims) == 0:\n1409             # don't stack\n1410             return self.copy(deep=False)\n1411 \n1412         other_dims = [d for d in self.dims if d not in dims]\n1413         dim_order = other_dims + list(dims)\n1414         reordered = self.transpose(*dim_order)\n1415 \n1416         new_shape = reordered.shape[: len(other_dims)] + (-1,)\n1417         new_data = reordered.data.reshape(new_shape)\n1418         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n1419 \n1420         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1421 \n1422     def stack(self, dimensions=None, **dimensions_kwargs):\n1423         \"\"\"\n1424         Stack any number of existing dimensions into a single new dimension.\n1425 \n1426         New dimensions will be added at the end, and the order of the data\n1427         along each new dimension will be in contiguous (C) order.\n1428 \n1429         Parameters\n1430         ----------\n1431         dimensions : Mapping of form new_name=(dim1, dim2, ...)\n1432             Names of new dimensions, and the existing dimensions that they\n1433             replace.\n1434         **dimensions_kwargs:\n1435             The keyword arguments form of ``dimensions``.\n1436             One of dimensions or dimensions_kwargs must be provided.\n1437 \n1438         Returns\n1439         -------\n1440         stacked : Variable\n1441             Variable with the same attributes but stacked data.\n1442 \n1443         See also\n1444         --------\n1445         Variable.unstack\n1446         \"\"\"\n1447         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n1448         result = self\n1449         for new_dim, dims in dimensions.items():\n1450             result = result._stack_once(dims, new_dim)\n1451         return result\n1452 \n1453     def _unstack_once(self, dims, old_dim):\n1454         new_dim_names = tuple(dims.keys())\n1455         new_dim_sizes = tuple(dims.values())\n1456 \n1457         if old_dim not in self.dims:\n1458             raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n1459 \n1460         if set(new_dim_names).intersection(self.dims):\n1461             raise ValueError(\n1462                 \"cannot create a new dimension with the same \"\n1463                 \"name as an existing dimension\"\n1464             )\n1465 \n1466         if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n1467             raise ValueError(\n1468                 \"the product of the new dimension sizes must \"\n1469                 \"equal the size of the old dimension\"\n1470             )\n1471 \n1472         other_dims = [d for d in self.dims if d != old_dim]\n1473         dim_order = other_dims + [old_dim]\n1474         reordered = self.transpose(*dim_order)\n1475 \n1476         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n1477         new_data = reordered.data.reshape(new_shape)\n1478         new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n1479 \n1480         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1481 \n1482     def unstack(self, dimensions=None, **dimensions_kwargs):\n1483         \"\"\"\n1484         Unstack an existing dimension into multiple new dimensions.\n1485 \n1486         New dimensions will be added at the end, and the order of the data\n1487         along each new dimension will be in contiguous (C) order.\n1488 \n1489         Parameters\n1490         ----------\n1491         dimensions : mapping of the form old_dim={dim1: size1, ...}\n1492             Names of existing dimensions, and the new dimensions and sizes\n1493             that they map to.\n1494         **dimensions_kwargs:\n1495             The keyword arguments form of ``dimensions``.\n1496             One of dimensions or dimensions_kwargs must be provided.\n1497 \n1498         Returns\n1499         -------\n1500         unstacked : Variable\n1501             Variable with the same attributes but unstacked data.\n1502 \n1503         See also\n1504         --------\n1505         Variable.stack\n1506         \"\"\"\n1507         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n1508         result = self\n1509         for old_dim, dims in dimensions.items():\n1510             result = result._unstack_once(dims, old_dim)\n1511         return result\n1512 \n1513     def fillna(self, value):\n1514         return ops.fillna(self, value)\n1515 \n1516     def where(self, cond, other=dtypes.NA):\n1517         return ops.where_method(self, cond, other)\n1518 \n1519     def reduce(\n1520         self,\n1521         func,\n1522         dim=None,\n1523         axis=None,\n1524         keep_attrs=None,\n1525         keepdims=False,\n1526         allow_lazy=None,\n1527         **kwargs,\n1528     ):\n1529         \"\"\"Reduce this array by applying `func` along some dimension(s).\n1530 \n1531         Parameters\n1532         ----------\n1533         func : function\n1534             Function which can be called in the form\n1535             `func(x, axis=axis, **kwargs)` to return the result of reducing an\n1536             np.ndarray over an integer valued axis.\n1537         dim : str or sequence of str, optional\n1538             Dimension(s) over which to apply `func`.\n1539         axis : int or sequence of int, optional\n1540             Axis(es) over which to apply `func`. Only one of the 'dim'\n1541             and 'axis' arguments can be supplied. If neither are supplied, then\n1542             the reduction is calculated over the flattened array (by calling\n1543             `func(x)` without an axis argument).\n1544         keep_attrs : bool, optional\n1545             If True, the variable's attributes (`attrs`) will be copied from\n1546             the original object to the new one.  If False (default), the new\n1547             object will be returned without attributes.\n1548         keepdims : bool, default False\n1549             If True, the dimensions which are reduced are left in the result\n1550             as dimensions of size one\n1551         **kwargs : dict\n1552             Additional keyword arguments passed on to `func`.\n1553 \n1554         Returns\n1555         -------\n1556         reduced : Array\n1557             Array with summarized data and the indicated dimension(s)\n1558             removed.\n1559         \"\"\"\n1560         if dim == ...:\n1561             dim = None\n1562         if dim is not None and axis is not None:\n1563             raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n1564 \n1565         if dim is not None:\n1566             axis = self.get_axis_num(dim)\n1567 \n1568         if allow_lazy is not None:\n1569             warnings.warn(\n1570                 \"allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.\",\n1571                 DeprecationWarning,\n1572             )\n1573         else:\n1574             allow_lazy = True\n1575 \n1576         input_data = self.data if allow_lazy else self.values\n1577 \n1578         if axis is not None:\n1579             data = func(input_data, axis=axis, **kwargs)\n1580         else:\n1581             data = func(input_data, **kwargs)\n1582 \n1583         if getattr(data, \"shape\", ()) == self.shape:\n1584             dims = self.dims\n1585         else:\n1586             removed_axes = (\n1587                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n1588             )\n1589             if keepdims:\n1590                 # Insert np.newaxis for removed dims\n1591                 slices = tuple(\n1592                     np.newaxis if i in removed_axes else slice(None, None)\n1593                     for i in range(self.ndim)\n1594                 )\n1595                 if getattr(data, \"shape\", None) is None:\n1596                     # Reduce has produced a scalar value, not an array-like\n1597                     data = np.asanyarray(data)[slices]\n1598                 else:\n1599                     data = data[slices]\n1600                 dims = self.dims\n1601             else:\n1602                 dims = [\n1603                     adim for n, adim in enumerate(self.dims) if n not in removed_axes\n1604                 ]\n1605 \n1606         if keep_attrs is None:\n1607             keep_attrs = _get_keep_attrs(default=False)\n1608         attrs = self._attrs if keep_attrs else None\n1609 \n1610         return Variable(dims, data, attrs=attrs)\n1611 \n1612     @classmethod\n1613     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n1614         \"\"\"Concatenate variables along a new or existing dimension.\n1615 \n1616         Parameters\n1617         ----------\n1618         variables : iterable of Array\n1619             Arrays to stack together. Each variable is expected to have\n1620             matching dimensions and shape except for along the stacked\n1621             dimension.\n1622         dim : str or DataArray, optional\n1623             Name of the dimension to stack along. This can either be a new\n1624             dimension name, in which case it is added along axis=0, or an\n1625             existing dimension name, in which case the location of the\n1626             dimension is unchanged. Where to insert the new dimension is\n1627             determined by the first variable.\n1628         positions : None or list of integer arrays, optional\n1629             List of integer arrays which specifies the integer positions to\n1630             which to assign each dataset along the concatenated dimension.\n1631             If not supplied, objects are concatenated in the provided order.\n1632         shortcut : bool, optional\n1633             This option is used internally to speed-up groupby operations.\n1634             If `shortcut` is True, some checks of internal consistency between\n1635             arrays to concatenate are skipped.\n1636 \n1637         Returns\n1638         -------\n1639         stacked : Variable\n1640             Concatenated Variable formed by stacking all the supplied variables\n1641             along the given dimension.\n1642         \"\"\"\n1643         if not isinstance(dim, str):\n1644             (dim,) = dim.dims\n1645 \n1646         # can't do this lazily: we need to loop through variables at least\n1647         # twice\n1648         variables = list(variables)\n1649         first_var = variables[0]\n1650 \n1651         arrays = [v.data for v in variables]\n1652 \n1653         if dim in first_var.dims:\n1654             axis = first_var.get_axis_num(dim)\n1655             dims = first_var.dims\n1656             data = duck_array_ops.concatenate(arrays, axis=axis)\n1657             if positions is not None:\n1658                 # TODO: deprecate this option -- we don't need it for groupby\n1659                 # any more.\n1660                 indices = nputils.inverse_permutation(np.concatenate(positions))\n1661                 data = duck_array_ops.take(data, indices, axis=axis)\n1662         else:\n1663             axis = 0\n1664             dims = (dim,) + first_var.dims\n1665             data = duck_array_ops.stack(arrays, axis=axis)\n1666 \n1667         attrs = dict(first_var.attrs)\n1668         encoding = dict(first_var.encoding)\n1669         if not shortcut:\n1670             for var in variables:\n1671                 if var.dims != first_var.dims:\n1672                     raise ValueError(\n1673                         f\"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}\"\n1674                     )\n1675 \n1676         return cls(dims, data, attrs, encoding)\n1677 \n1678     def equals(self, other, equiv=duck_array_ops.array_equiv):\n1679         \"\"\"True if two Variables have the same dimensions and values;\n1680         otherwise False.\n1681 \n1682         Variables can still be equal (like pandas objects) if they have NaN\n1683         values in the same locations.\n1684 \n1685         This method is necessary because `v1 == v2` for Variables\n1686         does element-wise comparisons (like numpy.ndarrays).\n1687         \"\"\"\n1688         other = getattr(other, \"variable\", other)\n1689         try:\n1690             return self.dims == other.dims and (\n1691                 self._data is other._data or equiv(self.data, other.data)\n1692             )\n1693         except (TypeError, AttributeError):\n1694             return False\n1695 \n1696     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n1697         \"\"\"True if two Variables have the values after being broadcast against\n1698         each other; otherwise False.\n1699 \n1700         Variables can still be equal (like pandas objects) if they have NaN\n1701         values in the same locations.\n1702         \"\"\"\n1703         try:\n1704             self, other = broadcast_variables(self, other)\n1705         except (ValueError, AttributeError):\n1706             return False\n1707         return self.equals(other, equiv=equiv)\n1708 \n1709     def identical(self, other, equiv=duck_array_ops.array_equiv):\n1710         \"\"\"Like equals, but also checks attributes.\n1711         \"\"\"\n1712         try:\n1713             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n1714                 other, equiv=equiv\n1715             )\n1716         except (TypeError, AttributeError):\n1717             return False\n1718 \n1719     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n1720         \"\"\"True if the intersection of two Variable's non-null data is\n1721         equal; otherwise false.\n1722 \n1723         Variables can thus still be equal if there are locations where either,\n1724         or both, contain NaN values.\n1725         \"\"\"\n1726         return self.broadcast_equals(other, equiv=equiv)\n1727 \n1728     def quantile(\n1729         self, q, dim=None, interpolation=\"linear\", keep_attrs=None, skipna=True\n1730     ):\n1731         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1732 \n1733         Returns the qth quantiles(s) of the array elements.\n1734 \n1735         Parameters\n1736         ----------\n1737         q : float in range of [0,1] (or sequence of floats)\n1738             Quantile to compute, which must be between 0 and 1\n1739             inclusive.\n1740         dim : str or sequence of str, optional\n1741             Dimension(s) over which to apply quantile.\n1742         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n1743             This optional parameter specifies the interpolation method to\n1744             use when the desired quantile lies between two data points\n1745             ``i < j``:\n1746 \n1747                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1748                   the fractional part of the index surrounded by ``i`` and\n1749                   ``j``.\n1750                 * lower: ``i``.\n1751                 * higher: ``j``.\n1752                 * nearest: ``i`` or ``j``, whichever is nearest.\n1753                 * midpoint: ``(i + j) / 2``.\n1754 \n1755         keep_attrs : bool, optional\n1756             If True, the variable's attributes (`attrs`) will be copied from\n1757             the original object to the new one.  If False (default), the new\n1758             object will be returned without attributes.\n1759 \n1760         Returns\n1761         -------\n1762         quantiles : Variable\n1763             If `q` is a single quantile, then the result\n1764             is a scalar. If multiple percentiles are given, first axis of\n1765             the result corresponds to the quantile and a quantile dimension\n1766             is added to the return array. The other dimensions are the\n1767             dimensions that remain after the reduction of the array.\n1768 \n1769         See Also\n1770         --------\n1771         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,\n1772         DataArray.quantile\n1773         \"\"\"\n1774 \n1775         from .computation import apply_ufunc\n1776 \n1777         _quantile_func = np.nanquantile if skipna else np.quantile\n1778 \n1779         if keep_attrs is None:\n1780             keep_attrs = _get_keep_attrs(default=False)\n1781 \n1782         scalar = utils.is_scalar(q)\n1783         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1784 \n1785         if dim is None:\n1786             dim = self.dims\n1787 \n1788         if utils.is_scalar(dim):\n1789             dim = [dim]\n1790 \n1791         def _wrapper(npa, **kwargs):\n1792             # move quantile axis to end. required for apply_ufunc\n1793             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n1794 \n1795         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1796         result = apply_ufunc(\n1797             _wrapper,\n1798             self,\n1799             input_core_dims=[dim],\n1800             exclude_dims=set(dim),\n1801             output_core_dims=[[\"quantile\"]],\n1802             output_dtypes=[np.float64],\n1803             output_sizes={\"quantile\": len(q)},\n1804             dask=\"parallelized\",\n1805             kwargs={\"q\": q, \"axis\": axis, \"interpolation\": interpolation},\n1806         )\n1807 \n1808         # for backward compatibility\n1809         result = result.transpose(\"quantile\", ...)\n1810         if scalar:\n1811             result = result.squeeze(\"quantile\")\n1812         if keep_attrs:\n1813             result.attrs = self._attrs\n1814         return result\n1815 \n1816     def rank(self, dim, pct=False):\n1817         \"\"\"Ranks the data.\n1818 \n1819         Equal values are assigned a rank that is the average of the ranks that\n1820         would have been otherwise assigned to all of the values within that\n1821         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n1822 \n1823         NaNs in the input array are returned as NaNs.\n1824 \n1825         The `bottleneck` library is required.\n1826 \n1827         Parameters\n1828         ----------\n1829         dim : str\n1830             Dimension over which to compute rank.\n1831         pct : bool, optional\n1832             If True, compute percentage ranks, otherwise compute integer ranks.\n1833 \n1834         Returns\n1835         -------\n1836         ranked : Variable\n1837 \n1838         See Also\n1839         --------\n1840         Dataset.rank, DataArray.rank\n1841         \"\"\"\n1842         import bottleneck as bn\n1843 \n1844         data = self.data\n1845 \n1846         if isinstance(data, dask_array_type):\n1847             raise TypeError(\n1848                 \"rank does not work for arrays stored as dask \"\n1849                 \"arrays. Load the data via .compute() or .load() \"\n1850                 \"prior to calling this method.\"\n1851             )\n1852         elif not isinstance(data, np.ndarray):\n1853             raise TypeError(\n1854                 \"rank is not implemented for {} objects.\".format(type(data))\n1855             )\n1856 \n1857         axis = self.get_axis_num(dim)\n1858         func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n1859         ranked = func(data, axis=axis)\n1860         if pct:\n1861             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n1862             ranked /= count\n1863         return Variable(self.dims, ranked)\n1864 \n1865     def rolling_window(\n1866         self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n1867     ):\n1868         \"\"\"\n1869         Make a rolling_window along dim and add a new_dim to the last place.\n1870 \n1871         Parameters\n1872         ----------\n1873         dim: str\n1874             Dimension over which to compute rolling_window\n1875         window: int\n1876             Window size of the rolling\n1877         window_dim: str\n1878             New name of the window dimension.\n1879         center: boolean. default False.\n1880             If True, pad fill_value for both ends. Otherwise, pad in the head\n1881             of the axis.\n1882         fill_value:\n1883             value to be filled.\n1884 \n1885         Returns\n1886         -------\n1887         Variable that is a view of the original array with a added dimension of\n1888         size w.\n1889         The return dim: self.dims + (window_dim, )\n1890         The return shape: self.shape + (window, )\n1891 \n1892         Examples\n1893         --------\n1894         >>> v = Variable((\"a\", \"b\"), np.arange(8).reshape((2, 4)))\n1895         >>> v.rolling_window(x, \"b\", 3, \"window_dim\")\n1896         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1897         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],\n1898                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])\n1899 \n1900         >>> v.rolling_window(x, \"b\", 3, \"window_dim\", center=True)\n1901         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1902         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],\n1903                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])\n1904         \"\"\"\n1905         if fill_value is dtypes.NA:  # np.nan is passed\n1906             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1907             array = self.astype(dtype, copy=False).data\n1908         else:\n1909             dtype = self.dtype\n1910             array = self.data\n1911 \n1912         new_dims = self.dims + (window_dim,)\n1913         return Variable(\n1914             new_dims,\n1915             duck_array_ops.rolling_window(\n1916                 array,\n1917                 axis=self.get_axis_num(dim),\n1918                 window=window,\n1919                 center=center,\n1920                 fill_value=fill_value,\n1921             ),\n1922         )\n1923 \n1924     def coarsen(self, windows, func, boundary=\"exact\", side=\"left\", **kwargs):\n1925         \"\"\"\n1926         Apply reduction function.\n1927         \"\"\"\n1928         windows = {k: v for k, v in windows.items() if k in self.dims}\n1929         if not windows:\n1930             return self.copy()\n1931 \n1932         reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n1933         if isinstance(func, str):\n1934             name = func\n1935             func = getattr(duck_array_ops, name, None)\n1936             if func is None:\n1937                 raise NameError(f\"{name} is not a valid method.\")\n1938         return self._replace(data=func(reshaped, axis=axes, **kwargs))\n1939 \n1940     def _coarsen_reshape(self, windows, boundary, side):\n1941         \"\"\"\n1942         Construct a reshaped-array for coarsen\n1943         \"\"\"\n1944         if not utils.is_dict_like(boundary):\n1945             boundary = {d: boundary for d in windows.keys()}\n1946 \n1947         if not utils.is_dict_like(side):\n1948             side = {d: side for d in windows.keys()}\n1949 \n1950         # remove unrelated dimensions\n1951         boundary = {k: v for k, v in boundary.items() if k in windows}\n1952         side = {k: v for k, v in side.items() if k in windows}\n1953 \n1954         for d, window in windows.items():\n1955             if window <= 0:\n1956                 raise ValueError(f\"window must be > 0. Given {window}\")\n1957 \n1958         variable = self\n1959         for d, window in windows.items():\n1960             # trim or pad the object\n1961             size = variable.shape[self._get_axis_num(d)]\n1962             n = int(size / window)\n1963             if boundary[d] == \"exact\":\n1964                 if n * window != size:\n1965                     raise ValueError(\n1966                         \"Could not coarsen a dimension of size {} with \"\n1967                         \"window {}\".format(size, window)\n1968                     )\n1969             elif boundary[d] == \"trim\":\n1970                 if side[d] == \"left\":\n1971                     variable = variable.isel({d: slice(0, window * n)})\n1972                 else:\n1973                     excess = size - window * n\n1974                     variable = variable.isel({d: slice(excess, None)})\n1975             elif boundary[d] == \"pad\":  # pad\n1976                 pad = window * n - size\n1977                 if pad < 0:\n1978                     pad += window\n1979                 if side[d] == \"left\":\n1980                     pad_width = {d: (0, pad)}\n1981                 else:\n1982                     pad_width = {d: (pad, 0)}\n1983                 variable = variable.pad(pad_width, mode=\"constant\")\n1984             else:\n1985                 raise TypeError(\n1986                     \"{} is invalid for boundary. Valid option is 'exact', \"\n1987                     \"'trim' and 'pad'\".format(boundary[d])\n1988                 )\n1989 \n1990         shape = []\n1991         axes = []\n1992         axis_count = 0\n1993         for i, d in enumerate(variable.dims):\n1994             if d in windows:\n1995                 size = variable.shape[i]\n1996                 shape.append(int(size / windows[d]))\n1997                 shape.append(windows[d])\n1998                 axis_count += 1\n1999                 axes.append(i + axis_count)\n2000             else:\n2001                 shape.append(variable.shape[i])\n2002 \n2003         keep_attrs = _get_keep_attrs(default=False)\n2004         variable.attrs = variable._attrs if keep_attrs else {}\n2005 \n2006         return variable.data.reshape(shape), tuple(axes)\n2007 \n2008     @property\n2009     def real(self):\n2010         return type(self)(self.dims, self.data.real, self._attrs)\n2011 \n2012     @property\n2013     def imag(self):\n2014         return type(self)(self.dims, self.data.imag, self._attrs)\n2015 \n2016     def __array_wrap__(self, obj, context=None):\n2017         return Variable(self.dims, obj)\n2018 \n2019     @staticmethod\n2020     def _unary_op(f):\n2021         @functools.wraps(f)\n2022         def func(self, *args, **kwargs):\n2023             with np.errstate(all=\"ignore\"):\n2024                 return self.__array_wrap__(f(self.data, *args, **kwargs))\n2025 \n2026         return func\n2027 \n2028     @staticmethod\n2029     def _binary_op(f, reflexive=False, **ignored_kwargs):\n2030         @functools.wraps(f)\n2031         def func(self, other):\n2032             if isinstance(other, (xr.DataArray, xr.Dataset)):\n2033                 return NotImplemented\n2034             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2035             keep_attrs = _get_keep_attrs(default=False)\n2036             attrs = self._attrs if keep_attrs else None\n2037             with np.errstate(all=\"ignore\"):\n2038                 new_data = (\n2039                     f(self_data, other_data)\n2040                     if not reflexive\n2041                     else f(other_data, self_data)\n2042                 )\n2043             result = Variable(dims, new_data, attrs=attrs)\n2044             return result\n2045 \n2046         return func\n2047 \n2048     @staticmethod\n2049     def _inplace_binary_op(f):\n2050         @functools.wraps(f)\n2051         def func(self, other):\n2052             if isinstance(other, xr.Dataset):\n2053                 raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n2054             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2055             if dims != self.dims:\n2056                 raise ValueError(\"dimensions cannot change for in-place \" \"operations\")\n2057             with np.errstate(all=\"ignore\"):\n2058                 self.values = f(self_data, other_data)\n2059             return self\n2060 \n2061         return func\n2062 \n2063     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n2064         \"\"\" A (private) method to convert datetime array to numeric dtype\n2065         See duck_array_ops.datetime_to_numeric\n2066         \"\"\"\n2067         numeric_array = duck_array_ops.datetime_to_numeric(\n2068             self.data, offset, datetime_unit, dtype\n2069         )\n2070         return type(self)(self.dims, numeric_array, self._attrs)\n2071 \n2072 \n2073 ops.inject_all_ops_and_reduce_methods(Variable)\n2074 \n2075 \n2076 class IndexVariable(Variable):\n2077     \"\"\"Wrapper for accommodating a pandas.Index in an xarray.Variable.\n2078 \n2079     IndexVariable preserve loaded values in the form of a pandas.Index instead\n2080     of a NumPy array. Hence, their values are immutable and must always be one-\n2081     dimensional.\n2082 \n2083     They also have a name property, which is the name of their sole dimension\n2084     unless another name is given.\n2085     \"\"\"\n2086 \n2087     __slots__ = ()\n2088 \n2089     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n2090         super().__init__(dims, data, attrs, encoding, fastpath)\n2091         if self.ndim != 1:\n2092             raise ValueError(\"%s objects must be 1-dimensional\" % type(self).__name__)\n2093 \n2094         # Unlike in Variable, always eagerly load values into memory\n2095         if not isinstance(self._data, PandasIndexAdapter):\n2096             self._data = PandasIndexAdapter(self._data)\n2097 \n2098     def __dask_tokenize__(self):\n2099         from dask.base import normalize_token\n2100 \n2101         # Don't waste time converting pd.Index to np.ndarray\n2102         return normalize_token((type(self), self._dims, self._data.array, self._attrs))\n2103 \n2104     def load(self):\n2105         # data is already loaded into memory for IndexVariable\n2106         return self\n2107 \n2108     # https://github.com/python/mypy/issues/1465\n2109     @Variable.data.setter  # type: ignore\n2110     def data(self, data):\n2111         raise ValueError(\n2112             f\"Cannot assign to the .data attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n2113             f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n2114         )\n2115 \n2116     @Variable.values.setter  # type: ignore\n2117     def values(self, values):\n2118         raise ValueError(\n2119             f\"Cannot assign to the .values attribute of dimension coordinate a.k.a IndexVariable {self.name!r}. \"\n2120             f\"Please use DataArray.assign_coords, Dataset.assign_coords or Dataset.assign as appropriate.\"\n2121         )\n2122 \n2123     def chunk(self, chunks=None, name=None, lock=False):\n2124         # Dummy - do not chunk. This method is invoked e.g. by Dataset.chunk()\n2125         return self.copy(deep=False)\n2126 \n2127     def _as_sparse(self, sparse_format=_default, fill_value=_default):\n2128         # Dummy\n2129         return self.copy(deep=False)\n2130 \n2131     def _to_dense(self):\n2132         # Dummy\n2133         return self.copy(deep=False)\n2134 \n2135     def _finalize_indexing_result(self, dims, data):\n2136         if getattr(data, \"ndim\", 0) != 1:\n2137             # returns Variable rather than IndexVariable if multi-dimensional\n2138             return Variable(dims, data, self._attrs, self._encoding)\n2139         else:\n2140             return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n2141 \n2142     def __setitem__(self, key, value):\n2143         raise TypeError(\"%s values cannot be modified\" % type(self).__name__)\n2144 \n2145     @classmethod\n2146     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2147         \"\"\"Specialized version of Variable.concat for IndexVariable objects.\n2148 \n2149         This exists because we want to avoid converting Index objects to NumPy\n2150         arrays, if possible.\n2151         \"\"\"\n2152         if not isinstance(dim, str):\n2153             (dim,) = dim.dims\n2154 \n2155         variables = list(variables)\n2156         first_var = variables[0]\n2157 \n2158         if any(not isinstance(v, cls) for v in variables):\n2159             raise TypeError(\n2160                 \"IndexVariable.concat requires that all input \"\n2161                 \"variables be IndexVariable objects\"\n2162             )\n2163 \n2164         indexes = [v._data.array for v in variables]\n2165 \n2166         if not indexes:\n2167             data = []\n2168         else:\n2169             data = indexes[0].append(indexes[1:])\n2170 \n2171             if positions is not None:\n2172                 indices = nputils.inverse_permutation(np.concatenate(positions))\n2173                 data = data.take(indices)\n2174 \n2175         attrs = dict(first_var.attrs)\n2176         if not shortcut:\n2177             for var in variables:\n2178                 if var.dims != first_var.dims:\n2179                     raise ValueError(\"inconsistent dimensions\")\n2180                 utils.remove_incompatible_items(attrs, var.attrs)\n2181 \n2182         return cls(first_var.dims, data, attrs)\n2183 \n2184     def copy(self, deep=True, data=None):\n2185         \"\"\"Returns a copy of this object.\n2186 \n2187         `deep` is ignored since data is stored in the form of\n2188         pandas.Index, which is already immutable. Dimensions, attributes\n2189         and encodings are always copied.\n2190 \n2191         Use `data` to create a new object with the same structure as\n2192         original but entirely new data.\n2193 \n2194         Parameters\n2195         ----------\n2196         deep : bool, optional\n2197             Deep is ignored when data is given. Whether the data array is\n2198             loaded into memory and copied onto the new object. Default is True.\n2199         data : array_like, optional\n2200             Data to use in the new object. Must have same shape as original.\n2201 \n2202         Returns\n2203         -------\n2204         object : Variable\n2205             New object with dimensions, attributes, encodings, and optionally\n2206             data copied from original.\n2207         \"\"\"\n2208         if data is None:\n2209             data = self._data.copy(deep=deep)\n2210         else:\n2211             data = as_compatible_data(data)\n2212             if self.shape != data.shape:\n2213                 raise ValueError(\n2214                     \"Data shape {} must match shape of object {}\".format(\n2215                         data.shape, self.shape\n2216                     )\n2217                 )\n2218         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n2219 \n2220     def equals(self, other, equiv=None):\n2221         # if equiv is specified, super up\n2222         if equiv is not None:\n2223             return super().equals(other, equiv)\n2224 \n2225         # otherwise use the native index equals, rather than looking at _data\n2226         other = getattr(other, \"variable\", other)\n2227         try:\n2228             return self.dims == other.dims and self._data_equals(other)\n2229         except (TypeError, AttributeError):\n2230             return False\n2231 \n2232     def _data_equals(self, other):\n2233         return self.to_index().equals(other.to_index())\n2234 \n2235     def to_index_variable(self):\n2236         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n2237         return self\n2238 \n2239     to_coord = utils.alias(to_index_variable, \"to_coord\")\n2240 \n2241     def to_index(self):\n2242         \"\"\"Convert this variable to a pandas.Index\"\"\"\n2243         # n.b. creating a new pandas.Index from an old pandas.Index is\n2244         # basically free as pandas.Index objects are immutable\n2245         assert self.ndim == 1\n2246         index = self._data.array\n2247         if isinstance(index, pd.MultiIndex):\n2248             # set default names for multi-index unnamed levels so that\n2249             # we can safely rename dimension / coordinate later\n2250             valid_level_names = [\n2251                 name or \"{}_level_{}\".format(self.dims[0], i)\n2252                 for i, name in enumerate(index.names)\n2253             ]\n2254             index = index.set_names(valid_level_names)\n2255         else:\n2256             index = index.set_names(self.name)\n2257         return index\n2258 \n2259     @property\n2260     def level_names(self):\n2261         \"\"\"Return MultiIndex level names or None if this IndexVariable has no\n2262         MultiIndex.\n2263         \"\"\"\n2264         index = self.to_index()\n2265         if isinstance(index, pd.MultiIndex):\n2266             return index.names\n2267         else:\n2268             return None\n2269 \n2270     def get_level_variable(self, level):\n2271         \"\"\"Return a new IndexVariable from a given MultiIndex level.\"\"\"\n2272         if self.level_names is None:\n2273             raise ValueError(\"IndexVariable %r has no MultiIndex\" % self.name)\n2274         index = self.to_index()\n2275         return type(self)(self.dims, index.get_level_values(level))\n2276 \n2277     @property\n2278     def name(self):\n2279         return self.dims[0]\n2280 \n2281     @name.setter\n2282     def name(self, value):\n2283         raise AttributeError(\"cannot modify name of IndexVariable in-place\")\n2284 \n2285 \n2286 # for backwards compatibility\n2287 Coordinate = utils.alias(IndexVariable, \"Coordinate\")\n2288 \n2289 \n2290 def _unified_dims(variables):\n2291     # validate dimensions\n2292     all_dims = {}\n2293     for var in variables:\n2294         var_dims = var.dims\n2295         if len(set(var_dims)) < len(var_dims):\n2296             raise ValueError(\n2297                 \"broadcasting cannot handle duplicate \"\n2298                 \"dimensions: %r\" % list(var_dims)\n2299             )\n2300         for d, s in zip(var_dims, var.shape):\n2301             if d not in all_dims:\n2302                 all_dims[d] = s\n2303             elif all_dims[d] != s:\n2304                 raise ValueError(\n2305                     \"operands cannot be broadcast together \"\n2306                     \"with mismatched lengths for dimension %r: %s\"\n2307                     % (d, (all_dims[d], s))\n2308                 )\n2309     return all_dims\n2310 \n2311 \n2312 def _broadcast_compat_variables(*variables):\n2313     \"\"\"Create broadcast compatible variables, with the same dimensions.\n2314 \n2315     Unlike the result of broadcast_variables(), some variables may have\n2316     dimensions of size 1 instead of the the size of the broadcast dimension.\n2317     \"\"\"\n2318     dims = tuple(_unified_dims(variables))\n2319     return tuple(var.set_dims(dims) if var.dims != dims else var for var in variables)\n2320 \n2321 \n2322 def broadcast_variables(*variables):\n2323     \"\"\"Given any number of variables, return variables with matching dimensions\n2324     and broadcast data.\n2325 \n2326     The data on the returned variables will be a view of the data on the\n2327     corresponding original arrays, but dimensions will be reordered and\n2328     inserted so that both broadcast arrays have the same dimensions. The new\n2329     dimensions are sorted in order of appearance in the first variable's\n2330     dimensions followed by the second variable's dimensions.\n2331     \"\"\"\n2332     dims_map = _unified_dims(variables)\n2333     dims_tuple = tuple(dims_map)\n2334     return tuple(\n2335         var.set_dims(dims_map) if var.dims != dims_tuple else var for var in variables\n2336     )\n2337 \n2338 \n2339 def _broadcast_compat_data(self, other):\n2340     if all(hasattr(other, attr) for attr in [\"dims\", \"data\", \"shape\", \"encoding\"]):\n2341         # `other` satisfies the necessary Variable API for broadcast_variables\n2342         new_self, new_other = _broadcast_compat_variables(self, other)\n2343         self_data = new_self.data\n2344         other_data = new_other.data\n2345         dims = new_self.dims\n2346     else:\n2347         # rely on numpy broadcasting rules\n2348         self_data = self.data\n2349         other_data = other\n2350         dims = self.dims\n2351     return self_data, other_data, dims\n2352 \n2353 \n2354 def concat(variables, dim=\"concat_dim\", positions=None, shortcut=False):\n2355     \"\"\"Concatenate variables along a new or existing dimension.\n2356 \n2357     Parameters\n2358     ----------\n2359     variables : iterable of Array\n2360         Arrays to stack together. Each variable is expected to have\n2361         matching dimensions and shape except for along the stacked\n2362         dimension.\n2363     dim : str or DataArray, optional\n2364         Name of the dimension to stack along. This can either be a new\n2365         dimension name, in which case it is added along axis=0, or an\n2366         existing dimension name, in which case the location of the\n2367         dimension is unchanged. Where to insert the new dimension is\n2368         determined by the first variable.\n2369     positions : None or list of integer arrays, optional\n2370         List of integer arrays which specifies the integer positions to which\n2371         to assign each dataset along the concatenated dimension. If not\n2372         supplied, objects are concatenated in the provided order.\n2373     shortcut : bool, optional\n2374         This option is used internally to speed-up groupby operations.\n2375         If `shortcut` is True, some checks of internal consistency between\n2376         arrays to concatenate are skipped.\n2377 \n2378     Returns\n2379     -------\n2380     stacked : Variable\n2381         Concatenated Variable formed by stacking all the supplied variables\n2382         along the given dimension.\n2383     \"\"\"\n2384     variables = list(variables)\n2385     if all(isinstance(v, IndexVariable) for v in variables):\n2386         return IndexVariable.concat(variables, dim, positions, shortcut)\n2387     else:\n2388         return Variable.concat(variables, dim, positions, shortcut)\n2389 \n2390 \n2391 def assert_unique_multiindex_level_names(variables):\n2392     \"\"\"Check for uniqueness of MultiIndex level names in all given\n2393     variables.\n2394 \n2395     Not public API. Used for checking consistency of DataArray and Dataset\n2396     objects.\n2397     \"\"\"\n2398     level_names = defaultdict(list)\n2399     all_level_names = set()\n2400     for var_name, var in variables.items():\n2401         if isinstance(var._data, PandasIndexAdapter):\n2402             idx_level_names = var.to_index_variable().level_names\n2403             if idx_level_names is not None:\n2404                 for n in idx_level_names:\n2405                     level_names[n].append(f\"{n!r} ({var_name})\")\n2406             if idx_level_names:\n2407                 all_level_names.update(idx_level_names)\n2408 \n2409     for k, v in level_names.items():\n2410         if k in variables:\n2411             v.append(\"(%s)\" % k)\n2412 \n2413     duplicate_names = [v for v in level_names.values() if len(v) > 1]\n2414     if duplicate_names:\n2415         conflict_str = \"\\n\".join(\", \".join(v) for v in duplicate_names)\n2416         raise ValueError(\"conflicting MultiIndex level name(s):\\n%s\" % conflict_str)\n2417     # Check confliction between level names and dimensions GH:2299\n2418     for k, v in variables.items():\n2419         for d in v.dims:\n2420             if d in all_level_names:\n2421                 raise ValueError(\n2422                     \"conflicting level / dimension names. {} \"\n2423                     \"already exists as a level name.\".format(d)\n2424                 )\n",
        "intended_behavior": "Consistency with Text Representation: The HTML representation's default behavior should reflect the text representation's practice of openly showing data and attributes sections, ensuring a consistent user experience."
    }
]