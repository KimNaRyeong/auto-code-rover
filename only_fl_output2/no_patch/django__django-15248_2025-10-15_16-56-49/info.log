2025-10-15 16:56:49.651 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 16:56:52.588 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=197, output_tokens=12, cost=0.002330
2025-10-15 16:56:52.589 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 16:56:52.593 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 16:56:59.302 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=504, output_tokens=235, cost=0.012090
2025-10-15 16:56:59.309 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 16:57:01.659 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=589, output_tokens=70, cost=0.007990
2025-10-15 16:57:01.661 | DEBUG    | Extracted a valid json.
2025-10-15 16:57:01.661 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Signal\")",
        "search_class(\"DeleteSignal\")",
        "search_method(\"delete\")",
        "search_method(\"queryset.delete\")",
        "search_code(\"delete source\")",
        "search_code(\"origin of delete\")"
    ],
    "bug_locations": []
}
2025-10-15 16:57:01.732 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 16:57:01.733 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 16:57:18.845 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1535, output_tokens=705, cost=0.036500
2025-10-15 16:57:18.859 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 16:57:26.266 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2456, output_tokens=280, cost=0.032960
2025-10-15 16:57:26.274 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 16:57:34.040 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=634, output_tokens=279, cost=0.014710
2025-10-15 16:57:34.041 | DEBUG    | Extracted a valid json.
2025-10-15 16:57:34.041 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_file(\"delete\", \"django/db/models/base.py\")",
        "search_method_in_file(\"delete\", \"django/db/models/query.py\")",
        "search_method_in_class(\"delete\", \"QuerySet\")"
    ],
    "bug_locations": [
        {
            "file": "django/db/models/base.py",
            "class": "Model",
            "method": "delete",
            "intended_behavior": "This method should be enhanced to include information about the deletion's origin specifically when it's called directly on a model instance. It should preserve its existing functionality of deleting the instance from the database. Additionally, when emitting the pre-delete and post-delete signals, it should now also include context about this being a direct model instance deletion."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "delete",
            "intended_behavior": "This method is responsible for batch deletion of objects identified by the queryset. To address the issue, it should be modified to emit pre-delete and post-delete signals with extra context indicating the deletion's origin as a queryset operation. This modification must preserve the method's initial behavior of efficiently deleting multiple instances while providing the additional context needed for signal listeners to distinguish the source of the deletion."
        }
    ]
}
2025-10-15 16:57:34.084 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 16:57:34.085 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 16:57:50.838 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4228, output_tokens=616, cost=0.060760
2025-10-15 16:57:50.851 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 16:57:59.377 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5060, output_tokens=363, cost=0.061490
2025-10-15 16:57:59.385 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 16:58:09.604 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=717, output_tokens=302, cost=0.016230
2025-10-15 16:58:09.605 | DEBUG    | Extracted a valid json.
2025-10-15 16:58:09.605 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/base.py",
            "class": "Model",
            "method": "delete",
            "intended_behavior": "This method should be extended to include an optional parameter or context that allows for specifying the deletion's origin. When emitting pre-delete or post-delete signals, this method should ensure that the signal's data includes whether the deletion was initiated directly through a model instance. Signal receivers can then use this context to understand the deletion's origin. The implementation must ensure this functionality does not break or change the existing API in a way that would require changes from end-users unless absolutely necessary. This means adding parameters with default values could be a preferred approach, maintaining backwards compatibility."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "delete",
            "intended_behavior": "Similarly, this method needs to be adapted to include context information about the deletion's origin as coming from a queryset operation. Given that `QuerySet.delete()` can affect numerous records, including a way to specify and pass along this information to the delete signals is crucial. Modifications to this method must ensure that any signals sent out as part of the deletion process carry this additional context. As with model instance deletions, it's essential to implement these changes in a way that doesn't disrupt existing usages of the method."
        }
    ]
}
2025-10-15 16:58:09.625 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/base.py</file>
<class>Model</class>
<method>delete</method>
<code>
983     def delete(self, using=None, keep_parents=False):
984         if self.pk is None:
985             raise ValueError(
986                 "%s object can't be deleted because its %s attribute is set "
987                 "to None." % (self._meta.object_name, self._meta.pk.attname)
988             )
989         using = using or router.db_for_write(self.__class__, instance=self)
990         collector = Collector(using=using)
991         collector.collect([self], keep_parents=keep_parents)
992         return collector.delete()

</code><intended_behavior>This method should be extended to include an optional parameter or context that allows for specifying the deletion's origin. When emitting pre-delete or post-delete signals, this method should ensure that the signal's data includes whether the deletion was initiated directly through a model instance. Signal receivers can then use this context to understand the deletion's origin. The implementation must ensure this functionality does not break or change the existing API in a way that would require changes from end-users unless absolutely necessary. This means adding parameters with default values could be a preferred approach, maintaining backwards compatibility.</intended_behavior>, <file>django/db/models/base.py</file>
<class>Model</class>
<method>None</method>
<code>
424 class Model(metaclass=ModelBase):
425 
426     def __init__(self, *args, **kwargs):
427         # Alias some things as locals to avoid repeat global lookups
428         cls = self.__class__
429         opts = self._meta
430         _setattr = setattr
431         _DEFERRED = DEFERRED
432         if opts.abstract:
433             raise TypeError('Abstract models cannot be instantiated.')
434 
435         pre_init.send(sender=cls, args=args, kwargs=kwargs)
436 
437         # Set up the storage for instance state
438         self._state = ModelState()
439 
440         # There is a rather weird disparity here; if kwargs, it's set, then args
441         # overrides it. It should be one or the other; don't duplicate the work
442         # The reason for the kwargs check is that standard iterator passes in by
443         # args, and instantiation for iteration is 33% faster.
444         if len(args) > len(opts.concrete_fields):
445             # Daft, but matches old exception sans the err msg.
446             raise IndexError("Number of args exceeds number of fields")
447 
448         if not kwargs:
449             fields_iter = iter(opts.concrete_fields)
450             # The ordering of the zip calls matter - zip throws StopIteration
451             # when an iter throws it. So if the first iter throws it, the second
452             # is *not* consumed. We rely on this, so don't change the order
453             # without changing the logic.
454             for val, field in zip(args, fields_iter):
455                 if val is _DEFERRED:
456                     continue
457                 _setattr(self, field.attname, val)
458         else:
459             # Slower, kwargs-ready version.
460             fields_iter = iter(opts.fields)
461             for val, field in zip(args, fields_iter):
462                 if val is _DEFERRED:
463                     continue
464                 _setattr(self, field.attname, val)
465                 if kwargs.pop(field.name, NOT_PROVIDED) is not NOT_PROVIDED:
466                     raise TypeError(
467                         f"{cls.__qualname__}() got both positional and "
468                         f"keyword arguments for field '{field.name}'."
469                     )
470 
471         # Now we're left with the unprocessed fields that *must* come from
472         # keywords, or default.
473 
474         for field in fields_iter:
475             is_related_object = False
476             # Virtual field
477             if field.attname not in kwargs and field.column is None:
478                 continue
479             if kwargs:
480                 if isinstance(field.remote_field, ForeignObjectRel):
481                     try:
482                         # Assume object instance was passed in.
483                         rel_obj = kwargs.pop(field.name)
484                         is_related_object = True
485                     except KeyError:
486                         try:
487                             # Object instance wasn't passed in -- must be an ID.
488                             val = kwargs.pop(field.attname)
489                         except KeyError:
490                             val = field.get_default()
491                 else:
492                     try:
493                         val = kwargs.pop(field.attname)
494                     except KeyError:
495                         # This is done with an exception rather than the
496                         # default argument on pop because we don't want
497                         # get_default() to be evaluated, and then not used.
498                         # Refs #12057.
499                         val = field.get_default()
500             else:
501                 val = field.get_default()
502 
503             if is_related_object:
504                 # If we are passed a related instance, set it using the
505                 # field.name instead of field.attname (e.g. "user" instead of
506                 # "user_id") so that the object gets properly cached (and type
507                 # checked) by the RelatedObjectDescriptor.
508                 if rel_obj is not _DEFERRED:
509                     _setattr(self, field.name, rel_obj)
510             else:
511                 if val is not _DEFERRED:
512                     _setattr(self, field.attname, val)
513 
514         if kwargs:
515             property_names = opts._property_names
516             for prop in tuple(kwargs):
517                 try:
518                     # Any remaining kwargs must correspond to properties or
519                     # virtual fields.
520                     if prop in property_names or opts.get_field(prop):
521                         if kwargs[prop] is not _DEFERRED:
522                             _setattr(self, prop, kwargs[prop])
523                         del kwargs[prop]
524                 except (AttributeError, FieldDoesNotExist):
525                     pass
526             for kwarg in kwargs:
527                 raise TypeError("%s() got an unexpected keyword argument '%s'" % (cls.__name__, kwarg))
528         super().__init__()
529         post_init.send(sender=cls, instance=self)
530 
531     @classmethod
532     def from_db(cls, db, field_names, values):
533         if len(values) != len(cls._meta.concrete_fields):
534             values_iter = iter(values)
535             values = [
536                 next(values_iter) if f.attname in field_names else DEFERRED
537                 for f in cls._meta.concrete_fields
538             ]
539         new = cls(*values)
540         new._state.adding = False
541         new._state.db = db
542         return new
543 
544     def __repr__(self):
545         return '<%s: %s>' % (self.__class__.__name__, self)
546 
547     def __str__(self):
548         return '%s object (%s)' % (self.__class__.__name__, self.pk)
549 
550     def __eq__(self, other):
551         if not isinstance(other, Model):
552             return NotImplemented
553         if self._meta.concrete_model != other._meta.concrete_model:
554             return False
555         my_pk = self.pk
556         if my_pk is None:
557             return self is other
558         return my_pk == other.pk
559 
560     def __hash__(self):
561         if self.pk is None:
562             raise TypeError("Model instances without primary key value are unhashable")
563         return hash(self.pk)
564 
565     def __reduce__(self):
566         data = self.__getstate__()
567         data[DJANGO_VERSION_PICKLE_KEY] = django.__version__
568         class_id = self._meta.app_label, self._meta.object_name
569         return model_unpickle, (class_id,), data
570 
571     def __getstate__(self):
572         """Hook to allow choosing the attributes to pickle."""
573         state = self.__dict__.copy()
574         state['_state'] = copy.copy(state['_state'])
575         # memoryview cannot be pickled, so cast it to bytes and store
576         # separately.
577         _memoryview_attrs = []
578         for attr, value in state.items():
579             if isinstance(value, memoryview):
580                 _memoryview_attrs.append((attr, bytes(value)))
581         if _memoryview_attrs:
582             state['_memoryview_attrs'] = _memoryview_attrs
583             for attr, value in _memoryview_attrs:
584                 state.pop(attr)
585         return state
586 
587     def __setstate__(self, state):
588         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
589         if pickled_version:
590             if pickled_version != django.__version__:
591                 warnings.warn(
592                     "Pickled model instance's Django version %s does not "
593                     "match the current version %s."
594                     % (pickled_version, django.__version__),
595                     RuntimeWarning,
596                     stacklevel=2,
597                 )
598         else:
599             warnings.warn(
600                 "Pickled model instance's Django version is not specified.",
601                 RuntimeWarning,
602                 stacklevel=2,
603             )
604         if '_memoryview_attrs' in state:
605             for attr, value in state.pop('_memoryview_attrs'):
606                 state[attr] = memoryview(value)
607         self.__dict__.update(state)
608 
609     def _get_pk_val(self, meta=None):
610         meta = meta or self._meta
611         return getattr(self, meta.pk.attname)
612 
613     def _set_pk_val(self, value):
614         for parent_link in self._meta.parents.values():
615             if parent_link and parent_link != self._meta.pk:
616                 setattr(self, parent_link.target_field.attname, value)
617         return setattr(self, self._meta.pk.attname, value)
618 
619     pk = property(_get_pk_val, _set_pk_val)
620 
621     def get_deferred_fields(self):
622         """
623         Return a set containing names of deferred fields for this instance.
624         """
625         return {
626             f.attname for f in self._meta.concrete_fields
627             if f.attname not in self.__dict__
628         }
629 
630     def refresh_from_db(self, using=None, fields=None):
631         """
632         Reload field values from the database.
633 
634         By default, the reloading happens from the database this instance was
635         loaded from, or by the read router if this instance wasn't loaded from
636         any database. The using parameter will override the default.
637 
638         Fields can be used to specify which fields to reload. The fields
639         should be an iterable of field attnames. If fields is None, then
640         all non-deferred fields are reloaded.
641 
642         When accessing deferred fields of an instance, the deferred loading
643         of the field will call this method.
644         """
645         if fields is None:
646             self._prefetched_objects_cache = {}
647         else:
648             prefetched_objects_cache = getattr(self, '_prefetched_objects_cache', ())
649             for field in fields:
650                 if field in prefetched_objects_cache:
651                     del prefetched_objects_cache[field]
652                     fields.remove(field)
653             if not fields:
654                 return
655             if any(LOOKUP_SEP in f for f in fields):
656                 raise ValueError(
657                     'Found "%s" in fields argument. Relations and transforms '
658                     'are not allowed in fields.' % LOOKUP_SEP)
659 
660         hints = {'instance': self}
661         db_instance_qs = self.__class__._base_manager.db_manager(using, hints=hints).filter(pk=self.pk)
662 
663         # Use provided fields, if not set then reload all non-deferred fields.
664         deferred_fields = self.get_deferred_fields()
665         if fields is not None:
666             fields = list(fields)
667             db_instance_qs = db_instance_qs.only(*fields)
668         elif deferred_fields:
669             fields = [f.attname for f in self._meta.concrete_fields
670                       if f.attname not in deferred_fields]
671             db_instance_qs = db_instance_qs.only(*fields)
672 
673         db_instance = db_instance_qs.get()
674         non_loaded_fields = db_instance.get_deferred_fields()
675         for field in self._meta.concrete_fields:
676             if field.attname in non_loaded_fields:
677                 # This field wasn't refreshed - skip ahead.
678                 continue
679             setattr(self, field.attname, getattr(db_instance, field.attname))
680             # Clear cached foreign keys.
681             if field.is_relation and field.is_cached(self):
682                 field.delete_cached_value(self)
683 
684         # Clear cached relations.
685         for field in self._meta.related_objects:
686             if field.is_cached(self):
687                 field.delete_cached_value(self)
688 
689         self._state.db = db_instance._state.db
690 
691     def serializable_value(self, field_name):
692         """
693         Return the value of the field name for this instance. If the field is
694         a foreign key, return the id value instead of the object. If there's
695         no Field object with this name on the model, return the model
696         attribute's value.
697 
698         Used to serialize a field's value (in the serializer, or form output,
699         for example). Normally, you would just access the attribute directly
700         and not use this method.
701         """
702         try:
703             field = self._meta.get_field(field_name)
704         except FieldDoesNotExist:
705             return getattr(self, field_name)
706         return getattr(self, field.attname)
707 
708     def save(self, force_insert=False, force_update=False, using=None,
709              update_fields=None):
710         """
711         Save the current instance. Override this in a subclass if you want to
712         control the saving process.
713 
714         The 'force_insert' and 'force_update' parameters can be used to insist
715         that the "save" must be an SQL insert or update (or equivalent for
716         non-SQL backends), respectively. Normally, they should not be set.
717         """
718         self._prepare_related_fields_for_save(operation_name='save')
719 
720         using = using or router.db_for_write(self.__class__, instance=self)
721         if force_insert and (force_update or update_fields):
722             raise ValueError("Cannot force both insert and updating in model saving.")
723 
724         deferred_fields = self.get_deferred_fields()
725         if update_fields is not None:
726             # If update_fields is empty, skip the save. We do also check for
727             # no-op saves later on for inheritance cases. This bailout is
728             # still needed for skipping signal sending.
729             if not update_fields:
730                 return
731 
732             update_fields = frozenset(update_fields)
733             field_names = set()
734 
735             for field in self._meta.concrete_fields:
736                 if not field.primary_key:
737                     field_names.add(field.name)
738 
739                     if field.name != field.attname:
740                         field_names.add(field.attname)
741 
742             non_model_fields = update_fields.difference(field_names)
743 
744             if non_model_fields:
745                 raise ValueError(
746                     'The following fields do not exist in this model, are m2m '
747                     'fields, or are non-concrete fields: %s'
748                     % ', '.join(non_model_fields)
749                 )
750 
751         # If saving to the same database, and this model is deferred, then
752         # automatically do an "update_fields" save on the loaded fields.
753         elif not force_insert and deferred_fields and using == self._state.db:
754             field_names = set()
755             for field in self._meta.concrete_fields:
756                 if not field.primary_key and not hasattr(field, 'through'):
757                     field_names.add(field.attname)
758             loaded_fields = field_names.difference(deferred_fields)
759             if loaded_fields:
760                 update_fields = frozenset(loaded_fields)
761 
762         self.save_base(using=using, force_insert=force_insert,
763                        force_update=force_update, update_fields=update_fields)
764     save.alters_data = True
765 
766     def save_base(self, raw=False, force_insert=False,
767                   force_update=False, using=None, update_fields=None):
768         """
769         Handle the parts of saving which should be done only once per save,
770         yet need to be done in raw saves, too. This includes some sanity
771         checks and signal sending.
772 
773         The 'raw' argument is telling save_base not to save any parent
774         models and not to do any changes to the values before save. This
775         is used by fixture loading.
776         """
777         using = using or router.db_for_write(self.__class__, instance=self)
778         assert not (force_insert and (force_update or update_fields))
779         assert update_fields is None or update_fields
780         cls = origin = self.__class__
781         # Skip proxies, but keep the origin as the proxy model.
782         if cls._meta.proxy:
783             cls = cls._meta.concrete_model
784         meta = cls._meta
785         if not meta.auto_created:
786             pre_save.send(
787                 sender=origin, instance=self, raw=raw, using=using,
788                 update_fields=update_fields,
789             )
790         # A transaction isn't needed if one query is issued.
791         if meta.parents:
792             context_manager = transaction.atomic(using=using, savepoint=False)
793         else:
794             context_manager = transaction.mark_for_rollback_on_error(using=using)
795         with context_manager:
796             parent_inserted = False
797             if not raw:
798                 parent_inserted = self._save_parents(cls, using, update_fields)
799             updated = self._save_table(
800                 raw, cls, force_insert or parent_inserted,
801                 force_update, using, update_fields,
802             )
803         # Store the database on which the object was saved
804         self._state.db = using
805         # Once saved, this is no longer a to-be-added instance.
806         self._state.adding = False
807 
808         # Signal that the save is complete
809         if not meta.auto_created:
810             post_save.send(
811                 sender=origin, instance=self, created=(not updated),
812                 update_fields=update_fields, raw=raw, using=using,
813             )
814 
815     save_base.alters_data = True
816 
817     def _save_parents(self, cls, using, update_fields):
818         """Save all the parents of cls using values from self."""
819         meta = cls._meta
820         inserted = False
821         for parent, field in meta.parents.items():
822             # Make sure the link fields are synced between parent and self.
823             if (field and getattr(self, parent._meta.pk.attname) is None and
824                     getattr(self, field.attname) is not None):
825                 setattr(self, parent._meta.pk.attname, getattr(self, field.attname))
826             parent_inserted = self._save_parents(cls=parent, using=using, update_fields=update_fields)
827             updated = self._save_table(
828                 cls=parent, using=using, update_fields=update_fields,
829                 force_insert=parent_inserted,
830             )
831             if not updated:
832                 inserted = True
833             # Set the parent's PK value to self.
834             if field:
835                 setattr(self, field.attname, self._get_pk_val(parent._meta))
836                 # Since we didn't have an instance of the parent handy set
837                 # attname directly, bypassing the descriptor. Invalidate
838                 # the related object cache, in case it's been accidentally
839                 # populated. A fresh instance will be re-built from the
840                 # database if necessary.
841                 if field.is_cached(self):
842                     field.delete_cached_value(self)
843         return inserted
844 
845     def _save_table(self, raw=False, cls=None, force_insert=False,
846                     force_update=False, using=None, update_fields=None):
847         """
848         Do the heavy-lifting involved in saving. Update or insert the data
849         for a single table.
850         """
851         meta = cls._meta
852         non_pks = [f for f in meta.local_concrete_fields if not f.primary_key]
853 
854         if update_fields:
855             non_pks = [f for f in non_pks
856                        if f.name in update_fields or f.attname in update_fields]
857 
858         pk_val = self._get_pk_val(meta)
859         if pk_val is None:
860             pk_val = meta.pk.get_pk_value_on_save(self)
861             setattr(self, meta.pk.attname, pk_val)
862         pk_set = pk_val is not None
863         if not pk_set and (force_update or update_fields):
864             raise ValueError("Cannot force an update in save() with no primary key.")
865         updated = False
866         # Skip an UPDATE when adding an instance and primary key has a default.
867         if (
868             not raw and
869             not force_insert and
870             self._state.adding and
871             meta.pk.default and
872             meta.pk.default is not NOT_PROVIDED
873         ):
874             force_insert = True
875         # If possible, try an UPDATE. If that doesn't update anything, do an INSERT.
876         if pk_set and not force_insert:
877             base_qs = cls._base_manager.using(using)
878             values = [(f, None, (getattr(self, f.attname) if raw else f.pre_save(self, False)))
879                       for f in non_pks]
880             forced_update = update_fields or force_update
881             updated = self._do_update(base_qs, using, pk_val, values, update_fields,
882                                       forced_update)
883             if force_update and not updated:
884                 raise DatabaseError("Forced update did not affect any rows.")
885             if update_fields and not updated:
886                 raise DatabaseError("Save with update_fields did not affect any rows.")
887         if not updated:
888             if meta.order_with_respect_to:
889                 # If this is a model with an order_with_respect_to
890                 # autopopulate the _order field
891                 field = meta.order_with_respect_to
892                 filter_args = field.get_filter_kwargs_for_object(self)
893                 self._order = cls._base_manager.using(using).filter(**filter_args).aggregate(
894                     _order__max=Coalesce(
895                         ExpressionWrapper(Max('_order') + Value(1), output_field=IntegerField()),
896                         Value(0),
897                     ),
898                 )['_order__max']
899             fields = meta.local_concrete_fields
900             if not pk_set:
901                 fields = [f for f in fields if f is not meta.auto_field]
902 
903             returning_fields = meta.db_returning_fields
904             results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
905             if results:
906                 for value, field in zip(results[0], returning_fields):
907                     setattr(self, field.attname, value)
908         return updated
909 
910     def _do_update(self, base_qs, using, pk_val, values, update_fields, forced_update):
911         """
912         Try to update the model. Return True if the model was updated (if an
913         update query was done and a matching row was found in the DB).
914         """
915         filtered = base_qs.filter(pk=pk_val)
916         if not values:
917             # We can end up here when saving a model in inheritance chain where
918             # update_fields doesn't target any field in current model. In that
919             # case we just say the update succeeded. Another case ending up here
920             # is a model with just PK - in that case check that the PK still
921             # exists.
922             return update_fields is not None or filtered.exists()
923         if self._meta.select_on_save and not forced_update:
924             return (
925                 filtered.exists() and
926                 # It may happen that the object is deleted from the DB right after
927                 # this check, causing the subsequent UPDATE to return zero matching
928                 # rows. The same result can occur in some rare cases when the
929                 # database returns zero despite the UPDATE being executed
930                 # successfully (a row is matched and updated). In order to
931                 # distinguish these two cases, the object's existence in the
932                 # database is again checked for if the UPDATE query returns 0.
933                 (filtered._update(values) > 0 or filtered.exists())
934             )
935         return filtered._update(values) > 0
936 
937     def _do_insert(self, manager, using, fields, returning_fields, raw):
938         """
939         Do an INSERT. If returning_fields is defined then this method should
940         return the newly created data for the model.
941         """
942         return manager._insert(
943             [self], fields=fields, returning_fields=returning_fields,
944             using=using, raw=raw,
945         )
946 
947     def _prepare_related_fields_for_save(self, operation_name, fields=None):
948         # Ensure that a model instance without a PK hasn't been assigned to
949         # a ForeignKey or OneToOneField on this model. If the field is
950         # nullable, allowing the save would result in silent data loss.
951         for field in self._meta.concrete_fields:
952             if fields and field not in fields:
953                 continue
954             # If the related field isn't cached, then an instance hasn't been
955             # assigned and there's no need to worry about this check.
956             if field.is_relation and field.is_cached(self):
957                 obj = getattr(self, field.name, None)
958                 if not obj:
959                     continue
960                 # A pk may have been assigned manually to a model instance not
961                 # saved to the database (or auto-generated in a case like
962                 # UUIDField), but we allow the save to proceed and rely on the
963                 # database to raise an IntegrityError if applicable. If
964                 # constraints aren't supported by the database, there's the
965                 # unavoidable risk of data corruption.
966                 if obj.pk is None:
967                     # Remove the object from a related instance cache.
968                     if not field.remote_field.multiple:
969                         field.remote_field.delete_cached_value(obj)
970                     raise ValueError(
971                         "%s() prohibited to prevent data loss due to unsaved "
972                         "related object '%s'." % (operation_name, field.name)
973                     )
974                 elif getattr(self, field.attname) in field.empty_values:
975                     # Use pk from related object if it has been saved after
976                     # an assignment.
977                     setattr(self, field.attname, obj.pk)
978                 # If the relationship's pk/to_field was changed, clear the
979                 # cached relationship.
980                 if getattr(obj, field.target_field.attname) != getattr(self, field.attname):
981                     field.delete_cached_value(self)
982 
983     def delete(self, using=None, keep_parents=False):
984         if self.pk is None:
985             raise ValueError(
986                 "%s object can't be deleted because its %s attribute is set "
987                 "to None." % (self._meta.object_name, self._meta.pk.attname)
988             )
989         using = using or router.db_for_write(self.__class__, instance=self)
990         collector = Collector(using=using)
991         collector.collect([self], keep_parents=keep_parents)
992         return collector.delete()
993 
994     delete.alters_data = True
995 
996     def _get_FIELD_display(self, field):
997         value = getattr(self, field.attname)
998         choices_dict = dict(make_hashable(field.flatchoices))
999         # force_str() to coerce lazy strings.
1000         return force_str(choices_dict.get(make_hashable(value), value), strings_only=True)
1001 
1002     def _get_next_or_previous_by_FIELD(self, field, is_next, **kwargs):
1003         if not self.pk:
1004             raise ValueError("get_next/get_previous cannot be used on unsaved objects.")
1005         op = 'gt' if is_next else 'lt'
1006         order = '' if is_next else '-'
1007         param = getattr(self, field.attname)
1008         q = Q((field.name, param), (f'pk__{op}', self.pk), _connector=Q.AND)
1009         q = Q(q, (f'{field.name}__{op}', param), _connector=Q.OR)
1010         qs = self.__class__._default_manager.using(self._state.db).filter(**kwargs).filter(q).order_by(
1011             '%s%s' % (order, field.name), '%spk' % order
1012         )
1013         try:
1014             return qs[0]
1015         except IndexError:
1016             raise self.DoesNotExist("%s matching query does not exist." % self.__class__._meta.object_name)
1017 
1018     def _get_next_or_previous_in_order(self, is_next):
1019         cachename = "__%s_order_cache" % is_next
1020         if not hasattr(self, cachename):
1021             op = 'gt' if is_next else 'lt'
1022             order = '_order' if is_next else '-_order'
1023             order_field = self._meta.order_with_respect_to
1024             filter_args = order_field.get_filter_kwargs_for_object(self)
1025             obj = self.__class__._default_manager.filter(**filter_args).filter(**{
1026                 '_order__%s' % op: self.__class__._default_manager.values('_order').filter(**{
1027                     self._meta.pk.name: self.pk
1028                 })
1029             }).order_by(order)[:1].get()
1030             setattr(self, cachename, obj)
1031         return getattr(self, cachename)
1032 
1033     def prepare_database_save(self, field):
1034         if self.pk is None:
1035             raise ValueError("Unsaved model instance %r cannot be used in an ORM query." % self)
1036         return getattr(self, field.remote_field.get_related_field().attname)
1037 
1038     def clean(self):
1039         """
1040         Hook for doing any extra model-wide validation after clean() has been
1041         called on every field by self.clean_fields. Any ValidationError raised
1042         by this method will not be associated with a particular field; it will
1043         have a special-case association with the field defined by NON_FIELD_ERRORS.
1044         """
1045         pass
1046 
1047     def validate_unique(self, exclude=None):
1048         """
1049         Check unique constraints on the model and raise ValidationError if any
1050         failed.
1051         """
1052         unique_checks, date_checks = self._get_unique_checks(exclude=exclude)
1053 
1054         errors = self._perform_unique_checks(unique_checks)
1055         date_errors = self._perform_date_checks(date_checks)
1056 
1057         for k, v in date_errors.items():
1058             errors.setdefault(k, []).extend(v)
1059 
1060         if errors:
1061             raise ValidationError(errors)
1062 
1063     def _get_unique_checks(self, exclude=None):
1064         """
1065         Return a list of checks to perform. Since validate_unique() could be
1066         called from a ModelForm, some fields may have been excluded; we can't
1067         perform a unique check on a model that is missing fields involved
1068         in that check. Fields that did not validate should also be excluded,
1069         but they need to be passed in via the exclude argument.
1070         """
1071         if exclude is None:
1072             exclude = []
1073         unique_checks = []
1074 
1075         unique_togethers = [(self.__class__, self._meta.unique_together)]
1076         constraints = [(self.__class__, self._meta.total_unique_constraints)]
1077         for parent_class in self._meta.get_parent_list():
1078             if parent_class._meta.unique_together:
1079                 unique_togethers.append((parent_class, parent_class._meta.unique_together))
1080             if parent_class._meta.total_unique_constraints:
1081                 constraints.append(
1082                     (parent_class, parent_class._meta.total_unique_constraints)
1083                 )
1084 
1085         for model_class, unique_together in unique_togethers:
1086             for check in unique_together:
1087                 if not any(name in exclude for name in check):
1088                     # Add the check if the field isn't excluded.
1089                     unique_checks.append((model_class, tuple(check)))
1090 
1091         for model_class, model_constraints in constraints:
1092             for constraint in model_constraints:
1093                 if not any(name in exclude for name in constraint.fields):
1094                     unique_checks.append((model_class, constraint.fields))
1095 
1096         # These are checks for the unique_for_<date/year/month>.
1097         date_checks = []
1098 
1099         # Gather a list of checks for fields declared as unique and add them to
1100         # the list of checks.
1101 
1102         fields_with_class = [(self.__class__, self._meta.local_fields)]
1103         for parent_class in self._meta.get_parent_list():
1104             fields_with_class.append((parent_class, parent_class._meta.local_fields))
1105 
1106         for model_class, fields in fields_with_class:
1107             for f in fields:
1108                 name = f.name
1109                 if name in exclude:
1110                     continue
1111                 if f.unique:
1112                     unique_checks.append((model_class, (name,)))
1113                 if f.unique_for_date and f.unique_for_date not in exclude:
1114                     date_checks.append((model_class, 'date', name, f.unique_for_date))
1115                 if f.unique_for_year and f.unique_for_year not in exclude:
1116                     date_checks.append((model_class, 'year', name, f.unique_for_year))
1117                 if f.unique_for_month and f.unique_for_month not in exclude:
1118                     date_checks.append((model_class, 'month', name, f.unique_for_month))
1119         return unique_checks, date_checks
1120 
1121     def _perform_unique_checks(self, unique_checks):
1122         errors = {}
1123 
1124         for model_class, unique_check in unique_checks:
1125             # Try to look up an existing object with the same values as this
1126             # object's values for all the unique field.
1127 
1128             lookup_kwargs = {}
1129             for field_name in unique_check:
1130                 f = self._meta.get_field(field_name)
1131                 lookup_value = getattr(self, f.attname)
1132                 # TODO: Handle multiple backends with different feature flags.
1133                 if (lookup_value is None or
1134                         (lookup_value == '' and connection.features.interprets_empty_strings_as_nulls)):
1135                     # no value, skip the lookup
1136                     continue
1137                 if f.primary_key and not self._state.adding:
1138                     # no need to check for unique primary key when editing
1139                     continue
1140                 lookup_kwargs[str(field_name)] = lookup_value
1141 
1142             # some fields were skipped, no reason to do the check
1143             if len(unique_check) != len(lookup_kwargs):
1144                 continue
1145 
1146             qs = model_class._default_manager.filter(**lookup_kwargs)
1147 
1148             # Exclude the current object from the query if we are editing an
1149             # instance (as opposed to creating a new one)
1150             # Note that we need to use the pk as defined by model_class, not
1151             # self.pk. These can be different fields because model inheritance
1152             # allows single model to have effectively multiple primary keys.
1153             # Refs #17615.
1154             model_class_pk = self._get_pk_val(model_class._meta)
1155             if not self._state.adding and model_class_pk is not None:
1156                 qs = qs.exclude(pk=model_class_pk)
1157             if qs.exists():
1158                 if len(unique_check) == 1:
1159                     key = unique_check[0]
1160                 else:
1161                     key = NON_FIELD_ERRORS
1162                 errors.setdefault(key, []).append(self.unique_error_message(model_class, unique_check))
1163 
1164         return errors
1165 
1166     def _perform_date_checks(self, date_checks):
1167         errors = {}
1168         for model_class, lookup_type, field, unique_for in date_checks:
1169             lookup_kwargs = {}
1170             # there's a ticket to add a date lookup, we can remove this special
1171             # case if that makes it's way in
1172             date = getattr(self, unique_for)
1173             if date is None:
1174                 continue
1175             if lookup_type == 'date':
1176                 lookup_kwargs['%s__day' % unique_for] = date.day
1177                 lookup_kwargs['%s__month' % unique_for] = date.month
1178                 lookup_kwargs['%s__year' % unique_for] = date.year
1179             else:
1180                 lookup_kwargs['%s__%s' % (unique_for, lookup_type)] = getattr(date, lookup_type)
1181             lookup_kwargs[field] = getattr(self, field)
1182 
1183             qs = model_class._default_manager.filter(**lookup_kwargs)
1184             # Exclude the current object from the query if we are editing an
1185             # instance (as opposed to creating a new one)
1186             if not self._state.adding and self.pk is not None:
1187                 qs = qs.exclude(pk=self.pk)
1188 
1189             if qs.exists():
1190                 errors.setdefault(field, []).append(
1191                     self.date_error_message(lookup_type, field, unique_for)
1192                 )
1193         return errors
1194 
1195     def date_error_message(self, lookup_type, field_name, unique_for):
1196         opts = self._meta
1197         field = opts.get_field(field_name)
1198         return ValidationError(
1199             message=field.error_messages['unique_for_date'],
1200             code='unique_for_date',
1201             params={
1202                 'model': self,
1203                 'model_name': capfirst(opts.verbose_name),
1204                 'lookup_type': lookup_type,
1205                 'field': field_name,
1206                 'field_label': capfirst(field.verbose_name),
1207                 'date_field': unique_for,
1208                 'date_field_label': capfirst(opts.get_field(unique_for).verbose_name),
1209             }
1210         )
1211 
1212     def unique_error_message(self, model_class, unique_check):
1213         opts = model_class._meta
1214 
1215         params = {
1216             'model': self,
1217             'model_class': model_class,
1218             'model_name': capfirst(opts.verbose_name),
1219             'unique_check': unique_check,
1220         }
1221 
1222         # A unique field
1223         if len(unique_check) == 1:
1224             field = opts.get_field(unique_check[0])
1225             params['field_label'] = capfirst(field.verbose_name)
1226             return ValidationError(
1227                 message=field.error_messages['unique'],
1228                 code='unique',
1229                 params=params,
1230             )
1231 
1232         # unique_together
1233         else:
1234             field_labels = [capfirst(opts.get_field(f).verbose_name) for f in unique_check]
1235             params['field_labels'] = get_text_list(field_labels, _('and'))
1236             return ValidationError(
1237                 message=_("%(model_name)s with this %(field_labels)s already exists."),
1238                 code='unique_together',
1239                 params=params,
1240             )
1241 
1242     def full_clean(self, exclude=None, validate_unique=True):
1243         """
1244         Call clean_fields(), clean(), and validate_unique() on the model.
1245         Raise a ValidationError for any errors that occur.
1246         """
1247         errors = {}
1248         if exclude is None:
1249             exclude = []
1250         else:
1251             exclude = list(exclude)
1252 
1253         try:
1254             self.clean_fields(exclude=exclude)
1255         except ValidationError as e:
1256             errors = e.update_error_dict(errors)
1257 
1258         # Form.clean() is run even if other validation fails, so do the
1259         # same with Model.clean() for consistency.
1260         try:
1261             self.clean()
1262         except ValidationError as e:
1263             errors = e.update_error_dict(errors)
1264 
1265         # Run unique checks, but only for fields that passed validation.
1266         if validate_unique:
1267             for name in errors:
1268                 if name != NON_FIELD_ERRORS and name not in exclude:
1269                     exclude.append(name)
1270             try:
1271                 self.validate_unique(exclude=exclude)
1272             except ValidationError as e:
1273                 errors = e.update_error_dict(errors)
1274 
1275         if errors:
1276             raise ValidationError(errors)
1277 
1278     def clean_fields(self, exclude=None):
1279         """
1280         Clean all fields and raise a ValidationError containing a dict
1281         of all validation errors if any occur.
1282         """
1283         if exclude is None:
1284             exclude = []
1285 
1286         errors = {}
1287         for f in self._meta.fields:
1288             if f.name in exclude:
1289                 continue
1290             # Skip validation for empty fields with blank=True. The developer
1291             # is responsible for making sure they have a valid value.
1292             raw_value = getattr(self, f.attname)
1293             if f.blank and raw_value in f.empty_values:
1294                 continue
1295             try:
1296                 setattr(self, f.attname, f.clean(raw_value, self))
1297             except ValidationError as e:
1298                 errors[f.name] = e.error_list
1299 
1300         if errors:
1301             raise ValidationError(errors)
1302 
1303     @classmethod
1304     def check(cls, **kwargs):
1305         errors = [*cls._check_swappable(), *cls._check_model(), *cls._check_managers(**kwargs)]
1306         if not cls._meta.swapped:
1307             databases = kwargs.get('databases') or []
1308             errors += [
1309                 *cls._check_fields(**kwargs),
1310                 *cls._check_m2m_through_same_relationship(),
1311                 *cls._check_long_column_names(databases),
1312             ]
1313             clash_errors = (
1314                 *cls._check_id_field(),
1315                 *cls._check_field_name_clashes(),
1316                 *cls._check_model_name_db_lookup_clashes(),
1317                 *cls._check_property_name_related_field_accessor_clashes(),
1318                 *cls._check_single_primary_key(),
1319             )
1320             errors.extend(clash_errors)
1321             # If there are field name clashes, hide consequent column name
1322             # clashes.
1323             if not clash_errors:
1324                 errors.extend(cls._check_column_name_clashes())
1325             errors += [
1326                 *cls._check_index_together(),
1327                 *cls._check_unique_together(),
1328                 *cls._check_indexes(databases),
1329                 *cls._check_ordering(),
1330                 *cls._check_constraints(databases),
1331                 *cls._check_default_pk(),
1332             ]
1333 
1334         return errors
1335 
1336     @classmethod
1337     def _check_default_pk(cls):
1338         if (
1339             not cls._meta.abstract and
1340             cls._meta.pk.auto_created and
1341             # Inherited PKs are checked in parents models.
1342             not (
1343                 isinstance(cls._meta.pk, OneToOneField) and
1344                 cls._meta.pk.remote_field.parent_link
1345             ) and
1346             not settings.is_overridden('DEFAULT_AUTO_FIELD') and
1347             cls._meta.app_config and
1348             not cls._meta.app_config._is_default_auto_field_overridden
1349         ):
1350             return [
1351                 checks.Warning(
1352                     f"Auto-created primary key used when not defining a "
1353                     f"primary key type, by default "
1354                     f"'{settings.DEFAULT_AUTO_FIELD}'.",
1355                     hint=(
1356                         f"Configure the DEFAULT_AUTO_FIELD setting or the "
1357                         f"{cls._meta.app_config.__class__.__qualname__}."
1358                         f"default_auto_field attribute to point to a subclass "
1359                         f"of AutoField, e.g. 'django.db.models.BigAutoField'."
1360                     ),
1361                     obj=cls,
1362                     id='models.W042',
1363                 ),
1364             ]
1365         return []
1366 
1367     @classmethod
1368     def _check_swappable(cls):
1369         """Check if the swapped model exists."""
1370         errors = []
1371         if cls._meta.swapped:
1372             try:
1373                 apps.get_model(cls._meta.swapped)
1374             except ValueError:
1375                 errors.append(
1376                     checks.Error(
1377                         "'%s' is not of the form 'app_label.app_name'." % cls._meta.swappable,
1378                         id='models.E001',
1379                     )
1380                 )
1381             except LookupError:
1382                 app_label, model_name = cls._meta.swapped.split('.')
1383                 errors.append(
1384                     checks.Error(
1385                         "'%s' references '%s.%s', which has not been "
1386                         "installed, or is abstract." % (
1387                             cls._meta.swappable, app_label, model_name
1388                         ),
1389                         id='models.E002',
1390                     )
1391                 )
1392         return errors
1393 
1394     @classmethod
1395     def _check_model(cls):
1396         errors = []
1397         if cls._meta.proxy:
1398             if cls._meta.local_fields or cls._meta.local_many_to_many:
1399                 errors.append(
1400                     checks.Error(
1401                         "Proxy model '%s' contains model fields." % cls.__name__,
1402                         id='models.E017',
1403                     )
1404                 )
1405         return errors
1406 
1407     @classmethod
1408     def _check_managers(cls, **kwargs):
1409         """Perform all manager checks."""
1410         errors = []
1411         for manager in cls._meta.managers:
1412             errors.extend(manager.check(**kwargs))
1413         return errors
1414 
1415     @classmethod
1416     def _check_fields(cls, **kwargs):
1417         """Perform all field checks."""
1418         errors = []
1419         for field in cls._meta.local_fields:
1420             errors.extend(field.check(**kwargs))
1421         for field in cls._meta.local_many_to_many:
1422             errors.extend(field.check(from_model=cls, **kwargs))
1423         return errors
1424 
1425     @classmethod
1426     def _check_m2m_through_same_relationship(cls):
1427         """ Check if no relationship model is used by more than one m2m field.
1428         """
1429 
1430         errors = []
1431         seen_intermediary_signatures = []
1432 
1433         fields = cls._meta.local_many_to_many
1434 
1435         # Skip when the target model wasn't found.
1436         fields = (f for f in fields if isinstance(f.remote_field.model, ModelBase))
1437 
1438         # Skip when the relationship model wasn't found.
1439         fields = (f for f in fields if isinstance(f.remote_field.through, ModelBase))
1440 
1441         for f in fields:
1442             signature = (f.remote_field.model, cls, f.remote_field.through, f.remote_field.through_fields)
1443             if signature in seen_intermediary_signatures:
1444                 errors.append(
1445                     checks.Error(
1446                         "The model has two identical many-to-many relations "
1447                         "through the intermediate model '%s'." %
1448                         f.remote_field.through._meta.label,
1449                         obj=cls,
1450                         id='models.E003',
1451                     )
1452                 )
1453             else:
1454                 seen_intermediary_signatures.append(signature)
1455         return errors
1456 
1457     @classmethod
1458     def _check_id_field(cls):
1459         """Check if `id` field is a primary key."""
1460         fields = [f for f in cls._meta.local_fields if f.name == 'id' and f != cls._meta.pk]
1461         # fields is empty or consists of the invalid "id" field
1462         if fields and not fields[0].primary_key and cls._meta.pk.name == 'id':
1463             return [
1464                 checks.Error(
1465                     "'id' can only be used as a field name if the field also "
1466                     "sets 'primary_key=True'.",
1467                     obj=cls,
1468                     id='models.E004',
1469                 )
1470             ]
1471         else:
1472             return []
1473 
1474     @classmethod
1475     def _check_field_name_clashes(cls):
1476         """Forbid field shadowing in multi-table inheritance."""
1477         errors = []
1478         used_fields = {}  # name or attname -> field
1479 
1480         # Check that multi-inheritance doesn't cause field name shadowing.
1481         for parent in cls._meta.get_parent_list():
1482             for f in parent._meta.local_fields:
1483                 clash = used_fields.get(f.name) or used_fields.get(f.attname) or None
1484                 if clash:
1485                     errors.append(
1486                         checks.Error(
1487                             "The field '%s' from parent model "
1488                             "'%s' clashes with the field '%s' "
1489                             "from parent model '%s'." % (
1490                                 clash.name, clash.model._meta,
1491                                 f.name, f.model._meta
1492                             ),
1493                             obj=cls,
1494                             id='models.E005',
1495                         )
1496                     )
1497                 used_fields[f.name] = f
1498                 used_fields[f.attname] = f
1499 
1500         # Check that fields defined in the model don't clash with fields from
1501         # parents, including auto-generated fields like multi-table inheritance
1502         # child accessors.
1503         for parent in cls._meta.get_parent_list():
1504             for f in parent._meta.get_fields():
1505                 if f not in used_fields:
1506                     used_fields[f.name] = f
1507 
1508         for f in cls._meta.local_fields:
1509             clash = used_fields.get(f.name) or used_fields.get(f.attname) or None
1510             # Note that we may detect clash between user-defined non-unique
1511             # field "id" and automatically added unique field "id", both
1512             # defined at the same model. This special case is considered in
1513             # _check_id_field and here we ignore it.
1514             id_conflict = f.name == "id" and clash and clash.name == "id" and clash.model == cls
1515             if clash and not id_conflict:
1516                 errors.append(
1517                     checks.Error(
1518                         "The field '%s' clashes with the field '%s' "
1519                         "from model '%s'." % (
1520                             f.name, clash.name, clash.model._meta
1521                         ),
1522                         obj=f,
1523                         id='models.E006',
1524                     )
1525                 )
1526             used_fields[f.name] = f
1527             used_fields[f.attname] = f
1528 
1529         return errors
1530 
1531     @classmethod
1532     def _check_column_name_clashes(cls):
1533         # Store a list of column names which have already been used by other fields.
1534         used_column_names = []
1535         errors = []
1536 
1537         for f in cls._meta.local_fields:
1538             _, column_name = f.get_attname_column()
1539 
1540             # Ensure the column name is not already in use.
1541             if column_name and column_name in used_column_names:
1542                 errors.append(
1543                     checks.Error(
1544                         "Field '%s' has column name '%s' that is used by "
1545                         "another field." % (f.name, column_name),
1546                         hint="Specify a 'db_column' for the field.",
1547                         obj=cls,
1548                         id='models.E007'
1549                     )
1550                 )
1551             else:
1552                 used_column_names.append(column_name)
1553 
1554         return errors
1555 
1556     @classmethod
1557     def _check_model_name_db_lookup_clashes(cls):
1558         errors = []
1559         model_name = cls.__name__
1560         if model_name.startswith('_') or model_name.endswith('_'):
1561             errors.append(
1562                 checks.Error(
1563                     "The model name '%s' cannot start or end with an underscore "
1564                     "as it collides with the query lookup syntax." % model_name,
1565                     obj=cls,
1566                     id='models.E023'
1567                 )
1568             )
1569         elif LOOKUP_SEP in model_name:
1570             errors.append(
1571                 checks.Error(
1572                     "The model name '%s' cannot contain double underscores as "
1573                     "it collides with the query lookup syntax." % model_name,
1574                     obj=cls,
1575                     id='models.E024'
1576                 )
1577             )
1578         return errors
1579 
1580     @classmethod
1581     def _check_property_name_related_field_accessor_clashes(cls):
1582         errors = []
1583         property_names = cls._meta._property_names
1584         related_field_accessors = (
1585             f.get_attname() for f in cls._meta._get_fields(reverse=False)
1586             if f.is_relation and f.related_model is not None
1587         )
1588         for accessor in related_field_accessors:
1589             if accessor in property_names:
1590                 errors.append(
1591                     checks.Error(
1592                         "The property '%s' clashes with a related field "
1593                         "accessor." % accessor,
1594                         obj=cls,
1595                         id='models.E025',
1596                     )
1597                 )
1598         return errors
1599 
1600     @classmethod
1601     def _check_single_primary_key(cls):
1602         errors = []
1603         if sum(1 for f in cls._meta.local_fields if f.primary_key) > 1:
1604             errors.append(
1605                 checks.Error(
1606                     "The model cannot have more than one field with "
1607                     "'primary_key=True'.",
1608                     obj=cls,
1609                     id='models.E026',
1610                 )
1611             )
1612         return errors
1613 
1614     @classmethod
1615     def _check_index_together(cls):
1616         """Check the value of "index_together" option."""
1617         if not isinstance(cls._meta.index_together, (tuple, list)):
1618             return [
1619                 checks.Error(
1620                     "'index_together' must be a list or tuple.",
1621                     obj=cls,
1622                     id='models.E008',
1623                 )
1624             ]
1625 
1626         elif any(not isinstance(fields, (tuple, list)) for fields in cls._meta.index_together):
1627             return [
1628                 checks.Error(
1629                     "All 'index_together' elements must be lists or tuples.",
1630                     obj=cls,
1631                     id='models.E009',
1632                 )
1633             ]
1634 
1635         else:
1636             errors = []
1637             for fields in cls._meta.index_together:
1638                 errors.extend(cls._check_local_fields(fields, "index_together"))
1639             return errors
1640 
1641     @classmethod
1642     def _check_unique_together(cls):
1643         """Check the value of "unique_together" option."""
1644         if not isinstance(cls._meta.unique_together, (tuple, list)):
1645             return [
1646                 checks.Error(
1647                     "'unique_together' must be a list or tuple.",
1648                     obj=cls,
1649                     id='models.E010',
1650                 )
1651             ]
1652 
1653         elif any(not isinstance(fields, (tuple, list)) for fields in cls._meta.unique_together):
1654             return [
1655                 checks.Error(
1656                     "All 'unique_together' elements must be lists or tuples.",
1657                     obj=cls,
1658                     id='models.E011',
1659                 )
1660             ]
1661 
1662         else:
1663             errors = []
1664             for fields in cls._meta.unique_together:
1665                 errors.extend(cls._check_local_fields(fields, "unique_together"))
1666             return errors
1667 
1668     @classmethod
1669     def _check_indexes(cls, databases):
1670         """Check fields, names, and conditions of indexes."""
1671         errors = []
1672         references = set()
1673         for index in cls._meta.indexes:
1674             # Index name can't start with an underscore or a number, restricted
1675             # for cross-database compatibility with Oracle.
1676             if index.name[0] == '_' or index.name[0].isdigit():
1677                 errors.append(
1678                     checks.Error(
1679                         "The index name '%s' cannot start with an underscore "
1680                         "or a number." % index.name,
1681                         obj=cls,
1682                         id='models.E033',
1683                     ),
1684                 )
1685             if len(index.name) > index.max_name_length:
1686                 errors.append(
1687                     checks.Error(
1688                         "The index name '%s' cannot be longer than %d "
1689                         "characters." % (index.name, index.max_name_length),
1690                         obj=cls,
1691                         id='models.E034',
1692                     ),
1693                 )
1694             if index.contains_expressions:
1695                 for expression in index.expressions:
1696                     references.update(
1697                         ref[0] for ref in cls._get_expr_references(expression)
1698                     )
1699         for db in databases:
1700             if not router.allow_migrate_model(db, cls):
1701                 continue
1702             connection = connections[db]
1703             if not (
1704                 connection.features.supports_partial_indexes or
1705                 'supports_partial_indexes' in cls._meta.required_db_features
1706             ) and any(index.condition is not None for index in cls._meta.indexes):
1707                 errors.append(
1708                     checks.Warning(
1709                         '%s does not support indexes with conditions.'
1710                         % connection.display_name,
1711                         hint=(
1712                             "Conditions will be ignored. Silence this warning "
1713                             "if you don't care about it."
1714                         ),
1715                         obj=cls,
1716                         id='models.W037',
1717                     )
1718                 )
1719             if not (
1720                 connection.features.supports_covering_indexes or
1721                 'supports_covering_indexes' in cls._meta.required_db_features
1722             ) and any(index.include for index in cls._meta.indexes):
1723                 errors.append(
1724                     checks.Warning(
1725                         '%s does not support indexes with non-key columns.'
1726                         % connection.display_name,
1727                         hint=(
1728                             "Non-key columns will be ignored. Silence this "
1729                             "warning if you don't care about it."
1730                         ),
1731                         obj=cls,
1732                         id='models.W040',
1733                     )
1734                 )
1735             if not (
1736                 connection.features.supports_expression_indexes or
1737                 'supports_expression_indexes' in cls._meta.required_db_features
1738             ) and any(index.contains_expressions for index in cls._meta.indexes):
1739                 errors.append(
1740                     checks.Warning(
1741                         '%s does not support indexes on expressions.'
1742                         % connection.display_name,
1743                         hint=(
1744                             "An index won't be created. Silence this warning "
1745                             "if you don't care about it."
1746                         ),
1747                         obj=cls,
1748                         id='models.W043',
1749                     )
1750                 )
1751         fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]
1752         fields += [include for index in cls._meta.indexes for include in index.include]
1753         fields += references
1754         errors.extend(cls._check_local_fields(fields, 'indexes'))
1755         return errors
1756 
1757     @classmethod
1758     def _check_local_fields(cls, fields, option):
1759         from django.db import models
1760 
1761         # In order to avoid hitting the relation tree prematurely, we use our
1762         # own fields_map instead of using get_field()
1763         forward_fields_map = {}
1764         for field in cls._meta._get_fields(reverse=False):
1765             forward_fields_map[field.name] = field
1766             if hasattr(field, 'attname'):
1767                 forward_fields_map[field.attname] = field
1768 
1769         errors = []
1770         for field_name in fields:
1771             try:
1772                 field = forward_fields_map[field_name]
1773             except KeyError:
1774                 errors.append(
1775                     checks.Error(
1776                         "'%s' refers to the nonexistent field '%s'." % (
1777                             option, field_name,
1778                         ),
1779                         obj=cls,
1780                         id='models.E012',
1781                     )
1782                 )
1783             else:
1784                 if isinstance(field.remote_field, models.ManyToManyRel):
1785                     errors.append(
1786                         checks.Error(
1787                             "'%s' refers to a ManyToManyField '%s', but "
1788                             "ManyToManyFields are not permitted in '%s'." % (
1789                                 option, field_name, option,
1790                             ),
1791                             obj=cls,
1792                             id='models.E013',
1793                         )
1794                     )
1795                 elif field not in cls._meta.local_fields:
1796                     errors.append(
1797                         checks.Error(
1798                             "'%s' refers to field '%s' which is not local to model '%s'."
1799                             % (option, field_name, cls._meta.object_name),
1800                             hint="This issue may be caused by multi-table inheritance.",
1801                             obj=cls,
1802                             id='models.E016',
1803                         )
1804                     )
1805         return errors
1806 
1807     @classmethod
1808     def _check_ordering(cls):
1809         """
1810         Check "ordering" option -- is it a list of strings and do all fields
1811         exist?
1812         """
1813         if cls._meta._ordering_clash:
1814             return [
1815                 checks.Error(
1816                     "'ordering' and 'order_with_respect_to' cannot be used together.",
1817                     obj=cls,
1818                     id='models.E021',
1819                 ),
1820             ]
1821 
1822         if cls._meta.order_with_respect_to or not cls._meta.ordering:
1823             return []
1824 
1825         if not isinstance(cls._meta.ordering, (list, tuple)):
1826             return [
1827                 checks.Error(
1828                     "'ordering' must be a tuple or list (even if you want to order by only one field).",
1829                     obj=cls,
1830                     id='models.E014',
1831                 )
1832             ]
1833 
1834         errors = []
1835         fields = cls._meta.ordering
1836 
1837         # Skip expressions and '?' fields.
1838         fields = (f for f in fields if isinstance(f, str) and f != '?')
1839 
1840         # Convert "-field" to "field".
1841         fields = ((f[1:] if f.startswith('-') else f) for f in fields)
1842 
1843         # Separate related fields and non-related fields.
1844         _fields = []
1845         related_fields = []
1846         for f in fields:
1847             if LOOKUP_SEP in f:
1848                 related_fields.append(f)
1849             else:
1850                 _fields.append(f)
1851         fields = _fields
1852 
1853         # Check related fields.
1854         for field in related_fields:
1855             _cls = cls
1856             fld = None
1857             for part in field.split(LOOKUP_SEP):
1858                 try:
1859                     # pk is an alias that won't be found by opts.get_field.
1860                     if part == 'pk':
1861                         fld = _cls._meta.pk
1862                     else:
1863                         fld = _cls._meta.get_field(part)
1864                     if fld.is_relation:
1865                         _cls = fld.path_infos[-1].to_opts.model
1866                     else:
1867                         _cls = None
1868                 except (FieldDoesNotExist, AttributeError):
1869                     if fld is None or (
1870                         fld.get_transform(part) is None and fld.get_lookup(part) is None
1871                     ):
1872                         errors.append(
1873                             checks.Error(
1874                                 "'ordering' refers to the nonexistent field, "
1875                                 "related field, or lookup '%s'." % field,
1876                                 obj=cls,
1877                                 id='models.E015',
1878                             )
1879                         )
1880 
1881         # Skip ordering on pk. This is always a valid order_by field
1882         # but is an alias and therefore won't be found by opts.get_field.
1883         fields = {f for f in fields if f != 'pk'}
1884 
1885         # Check for invalid or nonexistent fields in ordering.
1886         invalid_fields = []
1887 
1888         # Any field name that is not present in field_names does not exist.
1889         # Also, ordering by m2m fields is not allowed.
1890         opts = cls._meta
1891         valid_fields = set(chain.from_iterable(
1892             (f.name, f.attname) if not (f.auto_created and not f.concrete) else (f.field.related_query_name(),)
1893             for f in chain(opts.fields, opts.related_objects)
1894         ))
1895 
1896         invalid_fields.extend(fields - valid_fields)
1897 
1898         for invalid_field in invalid_fields:
1899             errors.append(
1900                 checks.Error(
1901                     "'ordering' refers to the nonexistent field, related "
1902                     "field, or lookup '%s'." % invalid_field,
1903                     obj=cls,
1904                     id='models.E015',
1905                 )
1906             )
1907         return errors
1908 
1909     @classmethod
1910     def _check_long_column_names(cls, databases):
1911         """
1912         Check that any auto-generated column names are shorter than the limits
1913         for each database in which the model will be created.
1914         """
1915         if not databases:
1916             return []
1917         errors = []
1918         allowed_len = None
1919         db_alias = None
1920 
1921         # Find the minimum max allowed length among all specified db_aliases.
1922         for db in databases:
1923             # skip databases where the model won't be created
1924             if not router.allow_migrate_model(db, cls):
1925                 continue
1926             connection = connections[db]
1927             max_name_length = connection.ops.max_name_length()
1928             if max_name_length is None or connection.features.truncates_names:
1929                 continue
1930             else:
1931                 if allowed_len is None:
1932                     allowed_len = max_name_length
1933                     db_alias = db
1934                 elif max_name_length < allowed_len:
1935                     allowed_len = max_name_length
1936                     db_alias = db
1937 
1938         if allowed_len is None:
1939             return errors
1940 
1941         for f in cls._meta.local_fields:
1942             _, column_name = f.get_attname_column()
1943 
1944             # Check if auto-generated name for the field is too long
1945             # for the database.
1946             if f.db_column is None and column_name is not None and len(column_name) > allowed_len:
1947                 errors.append(
1948                     checks.Error(
1949                         'Autogenerated column name too long for field "%s". '
1950                         'Maximum length is "%s" for database "%s".'
1951                         % (column_name, allowed_len, db_alias),
1952                         hint="Set the column name manually using 'db_column'.",
1953                         obj=cls,
1954                         id='models.E018',
1955                     )
1956                 )
1957 
1958         for f in cls._meta.local_many_to_many:
1959             # Skip nonexistent models.
1960             if isinstance(f.remote_field.through, str):
1961                 continue
1962 
1963             # Check if auto-generated name for the M2M field is too long
1964             # for the database.
1965             for m2m in f.remote_field.through._meta.local_fields:
1966                 _, rel_name = m2m.get_attname_column()
1967                 if m2m.db_column is None and rel_name is not None and len(rel_name) > allowed_len:
1968                     errors.append(
1969                         checks.Error(
1970                             'Autogenerated column name too long for M2M field '
1971                             '"%s". Maximum length is "%s" for database "%s".'
1972                             % (rel_name, allowed_len, db_alias),
1973                             hint=(
1974                                 "Use 'through' to create a separate model for "
1975                                 "M2M and then set column_name using 'db_column'."
1976                             ),
1977                             obj=cls,
1978                             id='models.E019',
1979                         )
1980                     )
1981 
1982         return errors
1983 
1984     @classmethod
1985     def _get_expr_references(cls, expr):
1986         if isinstance(expr, Q):
1987             for child in expr.children:
1988                 if isinstance(child, tuple):
1989                     lookup, value = child
1990                     yield tuple(lookup.split(LOOKUP_SEP))
1991                     yield from cls._get_expr_references(value)
1992                 else:
1993                     yield from cls._get_expr_references(child)
1994         elif isinstance(expr, F):
1995             yield tuple(expr.name.split(LOOKUP_SEP))
1996         elif hasattr(expr, 'get_source_expressions'):
1997             for src_expr in expr.get_source_expressions():
1998                 yield from cls._get_expr_references(src_expr)
1999 
2000     @classmethod
2001     def _check_constraints(cls, databases):
2002         errors = []
2003         for db in databases:
2004             if not router.allow_migrate_model(db, cls):
2005                 continue
2006             connection = connections[db]
2007             if not (
2008                 connection.features.supports_table_check_constraints or
2009                 'supports_table_check_constraints' in cls._meta.required_db_features
2010             ) and any(
2011                 isinstance(constraint, CheckConstraint)
2012                 for constraint in cls._meta.constraints
2013             ):
2014                 errors.append(
2015                     checks.Warning(
2016                         '%s does not support check constraints.' % connection.display_name,
2017                         hint=(
2018                             "A constraint won't be created. Silence this "
2019                             "warning if you don't care about it."
2020                         ),
2021                         obj=cls,
2022                         id='models.W027',
2023                     )
2024                 )
2025             if not (
2026                 connection.features.supports_partial_indexes or
2027                 'supports_partial_indexes' in cls._meta.required_db_features
2028             ) and any(
2029                 isinstance(constraint, UniqueConstraint) and constraint.condition is not None
2030                 for constraint in cls._meta.constraints
2031             ):
2032                 errors.append(
2033                     checks.Warning(
2034                         '%s does not support unique constraints with '
2035                         'conditions.' % connection.display_name,
2036                         hint=(
2037                             "A constraint won't be created. Silence this "
2038                             "warning if you don't care about it."
2039                         ),
2040                         obj=cls,
2041                         id='models.W036',
2042                     )
2043                 )
2044             if not (
2045                 connection.features.supports_deferrable_unique_constraints or
2046                 'supports_deferrable_unique_constraints' in cls._meta.required_db_features
2047             ) and any(
2048                 isinstance(constraint, UniqueConstraint) and constraint.deferrable is not None
2049                 for constraint in cls._meta.constraints
2050             ):
2051                 errors.append(
2052                     checks.Warning(
2053                         '%s does not support deferrable unique constraints.'
2054                         % connection.display_name,
2055                         hint=(
2056                             "A constraint won't be created. Silence this "
2057                             "warning if you don't care about it."
2058                         ),
2059                         obj=cls,
2060                         id='models.W038',
2061                     )
2062                 )
2063             if not (
2064                 connection.features.supports_covering_indexes or
2065                 'supports_covering_indexes' in cls._meta.required_db_features
2066             ) and any(
2067                 isinstance(constraint, UniqueConstraint) and constraint.include
2068                 for constraint in cls._meta.constraints
2069             ):
2070                 errors.append(
2071                     checks.Warning(
2072                         '%s does not support unique constraints with non-key '
2073                         'columns.' % connection.display_name,
2074                         hint=(
2075                             "A constraint won't be created. Silence this "
2076                             "warning if you don't care about it."
2077                         ),
2078                         obj=cls,
2079                         id='models.W039',
2080                     )
2081                 )
2082             if not (
2083                 connection.features.supports_expression_indexes or
2084                 'supports_expression_indexes' in cls._meta.required_db_features
2085             ) and any(
2086                 isinstance(constraint, UniqueConstraint) and constraint.contains_expressions
2087                 for constraint in cls._meta.constraints
2088             ):
2089                 errors.append(
2090                     checks.Warning(
2091                         '%s does not support unique constraints on '
2092                         'expressions.' % connection.display_name,
2093                         hint=(
2094                             "A constraint won't be created. Silence this "
2095                             "warning if you don't care about it."
2096                         ),
2097                         obj=cls,
2098                         id='models.W044',
2099                     )
2100                 )
2101             fields = set(chain.from_iterable(
2102                 (*constraint.fields, *constraint.include)
2103                 for constraint in cls._meta.constraints if isinstance(constraint, UniqueConstraint)
2104             ))
2105             references = set()
2106             for constraint in cls._meta.constraints:
2107                 if isinstance(constraint, UniqueConstraint):
2108                     if (
2109                         connection.features.supports_partial_indexes or
2110                         'supports_partial_indexes' not in cls._meta.required_db_features
2111                     ) and isinstance(constraint.condition, Q):
2112                         references.update(cls._get_expr_references(constraint.condition))
2113                     if (
2114                         connection.features.supports_expression_indexes or
2115                         'supports_expression_indexes' not in cls._meta.required_db_features
2116                     ) and constraint.contains_expressions:
2117                         for expression in constraint.expressions:
2118                             references.update(cls._get_expr_references(expression))
2119                 elif isinstance(constraint, CheckConstraint):
2120                     if (
2121                         connection.features.supports_table_check_constraints or
2122                         'supports_table_check_constraints' not in cls._meta.required_db_features
2123                     ) and isinstance(constraint.check, Q):
2124                         references.update(cls._get_expr_references(constraint.check))
2125             for field_name, *lookups in references:
2126                 # pk is an alias that won't be found by opts.get_field.
2127                 if field_name != 'pk':
2128                     fields.add(field_name)
2129                 if not lookups:
2130                     # If it has no lookups it cannot result in a JOIN.
2131                     continue
2132                 try:
2133                     if field_name == 'pk':
2134                         field = cls._meta.pk
2135                     else:
2136                         field = cls._meta.get_field(field_name)
2137                     if not field.is_relation or field.many_to_many or field.one_to_many:
2138                         continue
2139                 except FieldDoesNotExist:
2140                     continue
2141                 # JOIN must happen at the first lookup.
2142                 first_lookup = lookups[0]
2143                 if (
2144                     hasattr(field, 'get_transform') and
2145                     hasattr(field, 'get_lookup') and
2146                     field.get_transform(first_lookup) is None and
2147                     field.get_lookup(first_lookup) is None
2148                 ):
2149                     errors.append(
2150                         checks.Error(
2151                             "'constraints' refers to the joined field '%s'."
2152                             % LOOKUP_SEP.join([field_name] + lookups),
2153                             obj=cls,
2154                             id='models.E041',
2155                         )
2156                     )
2157             errors.extend(cls._check_local_fields(fields, 'constraints'))
2158         return errors

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>delete</method>
<code>
734     def delete(self):
735         """Delete the records in the current QuerySet."""
736         self._not_support_combined_queries('delete')
737         if self.query.is_sliced:
738             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
739         if self.query.distinct or self.query.distinct_fields:
740             raise TypeError('Cannot call delete() after .distinct().')
741         if self._fields is not None:
742             raise TypeError("Cannot call delete() after .values() or .values_list()")
743 
744         del_query = self._chain()
745 
746         # The delete is actually 2 queries - one to find related objects,
747         # and one to delete. Make sure that the discovery of related
748         # objects is performed on the same database as the deletion.
749         del_query._for_write = True
750 
751         # Disable non-supported fields.
752         del_query.query.select_for_update = False
753         del_query.query.select_related = False
754         del_query.query.clear_ordering(force=True)
755 
756         collector = Collector(using=del_query.db)
757         collector.collect(del_query)
758         deleted, _rows_count = collector.delete()
759 
760         # Clear the result cache, in case this QuerySet gets reused.
761         self._result_cache = None
762         return deleted, _rows_count

</code><intended_behavior>Similarly, this method needs to be adapted to include context information about the deletion's origin as coming from a queryset operation. Given that `QuerySet.delete()` can affect numerous records, including a way to specify and pass along this information to the delete signals is crucial. Modifications to this method must ensure that any signals sent out as part of the deletion process carry this additional context. As with model instance deletions, it's essential to implement these changes in a way that doesn't disrupt existing usages of the method.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         if (
295             (isinstance(k, int) and k < 0) or
296             (isinstance(k, slice) and (
297                 (k.start is not None and k.start < 0) or
298                 (k.stop is not None and k.stop < 0)
299             ))
300         ):
301             raise ValueError('Negative indexing is not supported.')
302 
303         if self._result_cache is not None:
304             return self._result_cache[k]
305 
306         if isinstance(k, slice):
307             qs = self._chain()
308             if k.start is not None:
309                 start = int(k.start)
310             else:
311                 start = None
312             if k.stop is not None:
313                 stop = int(k.stop)
314             else:
315                 stop = None
316             qs.query.set_limits(start, stop)
317             return list(qs)[::k.step] if k.step else qs
318 
319         qs = self._chain()
320         qs.query.set_limits(k, k + 1)
321         qs._fetch_all()
322         return qs._result_cache[0]
323 
324     def __class_getitem__(cls, *args, **kwargs):
325         return cls
326 
327     def __and__(self, other):
328         self._check_operator_queryset(other, '&')
329         self._merge_sanity_check(other)
330         if isinstance(other, EmptyQuerySet):
331             return other
332         if isinstance(self, EmptyQuerySet):
333             return self
334         combined = self._chain()
335         combined._merge_known_related_objects(other)
336         combined.query.combine(other.query, sql.AND)
337         return combined
338 
339     def __or__(self, other):
340         self._check_operator_queryset(other, '|')
341         self._merge_sanity_check(other)
342         if isinstance(self, EmptyQuerySet):
343             return other
344         if isinstance(other, EmptyQuerySet):
345             return self
346         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
347         combined = query._chain()
348         combined._merge_known_related_objects(other)
349         if not other.query.can_filter():
350             other = other.model._base_manager.filter(pk__in=other.values('pk'))
351         combined.query.combine(other.query, sql.OR)
352         return combined
353 
354     ####################################
355     # METHODS THAT DO DATABASE QUERIES #
356     ####################################
357 
358     def _iterator(self, use_chunked_fetch, chunk_size):
359         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
360 
361     def iterator(self, chunk_size=2000):
362         """
363         An iterator over the results from applying this QuerySet to the
364         database.
365         """
366         if chunk_size <= 0:
367             raise ValueError('Chunk size must be strictly positive.')
368         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
369         return self._iterator(use_chunked_fetch, chunk_size)
370 
371     def aggregate(self, *args, **kwargs):
372         """
373         Return a dictionary containing the calculations (aggregation)
374         over the current queryset.
375 
376         If args is present the expression is passed as a kwarg using
377         the Aggregate object's default alias.
378         """
379         if self.query.distinct_fields:
380             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
381         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
382         for arg in args:
383             # The default_alias property raises TypeError if default_alias
384             # can't be set automatically or AttributeError if it isn't an
385             # attribute.
386             try:
387                 arg.default_alias
388             except (AttributeError, TypeError):
389                 raise TypeError("Complex aggregates require an alias")
390             kwargs[arg.default_alias] = arg
391 
392         query = self.query.chain()
393         for (alias, aggregate_expr) in kwargs.items():
394             query.add_annotation(aggregate_expr, alias, is_summary=True)
395             annotation = query.annotations[alias]
396             if not annotation.contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398             for expr in annotation.get_source_expressions():
399                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
400                     name = expr.refs
401                     raise exceptions.FieldError(
402                         "Cannot compute %s('%s'): '%s' is an aggregate"
403                         % (annotation.name, name, name)
404                     )
405         return query.get_aggregation(self.db, kwargs)
406 
407     def count(self):
408         """
409         Perform a SELECT COUNT() and return the number of records as an
410         integer.
411 
412         If the QuerySet is already fully cached, return the length of the
413         cached results set to avoid multiple SELECT COUNT(*) calls.
414         """
415         if self._result_cache is not None:
416             return len(self._result_cache)
417 
418         return self.query.get_count(using=self.db)
419 
420     def get(self, *args, **kwargs):
421         """
422         Perform the query and return a single object matching the given
423         keyword arguments.
424         """
425         if self.query.combinator and (args or kwargs):
426             raise NotSupportedError(
427                 'Calling QuerySet.get(...) with filters after %s() is not '
428                 'supported.' % self.query.combinator
429             )
430         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
431         if self.query.can_filter() and not self.query.distinct_fields:
432             clone = clone.order_by()
433         limit = None
434         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
435             limit = MAX_GET_RESULTS
436             clone.query.set_limits(high=limit)
437         num = len(clone)
438         if num == 1:
439             return clone._result_cache[0]
440         if not num:
441             raise self.model.DoesNotExist(
442                 "%s matching query does not exist." %
443                 self.model._meta.object_name
444             )
445         raise self.model.MultipleObjectsReturned(
446             'get() returned more than one %s -- it returned %s!' % (
447                 self.model._meta.object_name,
448                 num if not limit or num < limit else 'more than %s' % (limit - 1),
449             )
450         )
451 
452     def create(self, **kwargs):
453         """
454         Create a new object with the given kwargs, saving it to the database
455         and returning the created object.
456         """
457         obj = self.model(**kwargs)
458         self._for_write = True
459         obj.save(force_insert=True, using=self.db)
460         return obj
461 
462     def _prepare_for_bulk_create(self, objs):
463         for obj in objs:
464             if obj.pk is None:
465                 # Populate new PK values.
466                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
467             obj._prepare_related_fields_for_save(operation_name='bulk_create')
468 
469     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
470         """
471         Insert each of the instances into the database. Do *not* call
472         save() on each of the instances, do not send any pre/post_save
473         signals, and do not set the primary key attribute if it is an
474         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
475         Multi-table models are not supported.
476         """
477         # When you bulk insert you don't get the primary keys back (if it's an
478         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
479         # you can't insert into the child tables which references this. There
480         # are two workarounds:
481         # 1) This could be implemented if you didn't have an autoincrement pk
482         # 2) You could do it by doing O(n) normal inserts into the parent
483         #    tables to get the primary keys back and then doing a single bulk
484         #    insert into the childmost table.
485         # We currently set the primary keys on the objects when using
486         # PostgreSQL via the RETURNING ID clause. It should be possible for
487         # Oracle as well, but the semantics for extracting the primary keys is
488         # trickier so it's not done yet.
489         if batch_size is not None and batch_size <= 0:
490             raise ValueError('Batch size must be a positive integer.')
491         # Check that the parents share the same concrete model with the our
492         # model to detect the inheritance pattern ConcreteGrandParent ->
493         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
494         # would not identify that case as involving multiple tables.
495         for parent in self.model._meta.get_parent_list():
496             if parent._meta.concrete_model is not self.model._meta.concrete_model:
497                 raise ValueError("Can't bulk create a multi-table inherited model")
498         if not objs:
499             return objs
500         self._for_write = True
501         opts = self.model._meta
502         fields = opts.concrete_fields
503         objs = list(objs)
504         self._prepare_for_bulk_create(objs)
505         with transaction.atomic(using=self.db, savepoint=False):
506             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
507             if objs_with_pk:
508                 returned_columns = self._batched_insert(
509                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
510                 )
511                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         if field != opts.pk:
514                             setattr(obj_with_pk, field.attname, result)
515                 for obj_with_pk in objs_with_pk:
516                     obj_with_pk._state.adding = False
517                     obj_with_pk._state.db = self.db
518             if objs_without_pk:
519                 fields = [f for f in fields if not isinstance(f, AutoField)]
520                 returned_columns = self._batched_insert(
521                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
522                 )
523                 connection = connections[self.db]
524                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
525                     assert len(returned_columns) == len(objs_without_pk)
526                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
527                     for result, field in zip(results, opts.db_returning_fields):
528                         setattr(obj_without_pk, field.attname, result)
529                     obj_without_pk._state.adding = False
530                     obj_without_pk._state.db = self.db
531 
532         return objs
533 
534     def bulk_update(self, objs, fields, batch_size=None):
535         """
536         Update the given fields in each of the given objects in the database.
537         """
538         if batch_size is not None and batch_size < 0:
539             raise ValueError('Batch size must be a positive integer.')
540         if not fields:
541             raise ValueError('Field names must be given to bulk_update().')
542         objs = tuple(objs)
543         if any(obj.pk is None for obj in objs):
544             raise ValueError('All bulk_update() objects must have a primary key set.')
545         fields = [self.model._meta.get_field(name) for name in fields]
546         if any(not f.concrete or f.many_to_many for f in fields):
547             raise ValueError('bulk_update() can only be used with concrete fields.')
548         if any(f.primary_key for f in fields):
549             raise ValueError('bulk_update() cannot be used with primary key fields.')
550         if not objs:
551             return 0
552         for obj in objs:
553             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)
554         # PK is used twice in the resulting update query, once in the filter
555         # and once in the WHEN. Each field will also have one CAST.
556         connection = connections[self.db]
557         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
558         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
559         requires_casting = connection.features.requires_casted_case_in_updates
560         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
561         updates = []
562         for batch_objs in batches:
563             update_kwargs = {}
564             for field in fields:
565                 when_statements = []
566                 for obj in batch_objs:
567                     attr = getattr(obj, field.attname)
568                     if not isinstance(attr, Expression):
569                         attr = Value(attr, output_field=field)
570                     when_statements.append(When(pk=obj.pk, then=attr))
571                 case_statement = Case(*when_statements, output_field=field)
572                 if requires_casting:
573                     case_statement = Cast(case_statement, output_field=field)
574                 update_kwargs[field.attname] = case_statement
575             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
576         rows_updated = 0
577         with transaction.atomic(using=self.db, savepoint=False):
578             for pks, update_kwargs in updates:
579                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
580         return rows_updated
581     bulk_update.alters_data = True
582 
583     def get_or_create(self, defaults=None, **kwargs):
584         """
585         Look up an object with the given kwargs, creating one if necessary.
586         Return a tuple of (object, created), where created is a boolean
587         specifying whether an object was created.
588         """
589         # The get() needs to be targeted at the write database in order
590         # to avoid potential transaction consistency problems.
591         self._for_write = True
592         try:
593             return self.get(**kwargs), False
594         except self.model.DoesNotExist:
595             params = self._extract_model_params(defaults, **kwargs)
596             # Try to create an object using passed params.
597             try:
598                 with transaction.atomic(using=self.db):
599                     params = dict(resolve_callables(params))
600                     return self.create(**params), True
601             except IntegrityError:
602                 try:
603                     return self.get(**kwargs), False
604                 except self.model.DoesNotExist:
605                     pass
606                 raise
607 
608     def update_or_create(self, defaults=None, **kwargs):
609         """
610         Look up an object with the given kwargs, updating one with defaults
611         if it exists, otherwise create a new one.
612         Return a tuple (object, created), where created is a boolean
613         specifying whether an object was created.
614         """
615         defaults = defaults or {}
616         self._for_write = True
617         with transaction.atomic(using=self.db):
618             # Lock the row so that a concurrent update is blocked until
619             # update_or_create() has performed its save.
620             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
621             if created:
622                 return obj, created
623             for k, v in resolve_callables(defaults):
624                 setattr(obj, k, v)
625             obj.save(using=self.db)
626         return obj, False
627 
628     def _extract_model_params(self, defaults, **kwargs):
629         """
630         Prepare `params` for creating a model instance based on the given
631         kwargs; for use by get_or_create().
632         """
633         defaults = defaults or {}
634         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
635         params.update(defaults)
636         property_names = self.model._meta._property_names
637         invalid_params = []
638         for param in params:
639             try:
640                 self.model._meta.get_field(param)
641             except exceptions.FieldDoesNotExist:
642                 # It's okay to use a model's property if it has a setter.
643                 if not (param in property_names and getattr(self.model, param).fset):
644                     invalid_params.append(param)
645         if invalid_params:
646             raise exceptions.FieldError(
647                 "Invalid field name(s) for model %s: '%s'." % (
648                     self.model._meta.object_name,
649                     "', '".join(sorted(invalid_params)),
650                 ))
651         return params
652 
653     def _earliest(self, *fields):
654         """
655         Return the earliest object according to fields (if given) or by the
656         model's Meta.get_latest_by.
657         """
658         if fields:
659             order_by = fields
660         else:
661             order_by = getattr(self.model._meta, 'get_latest_by')
662             if order_by and not isinstance(order_by, (tuple, list)):
663                 order_by = (order_by,)
664         if order_by is None:
665             raise ValueError(
666                 "earliest() and latest() require either fields as positional "
667                 "arguments or 'get_latest_by' in the model's Meta."
668             )
669         obj = self._chain()
670         obj.query.set_limits(high=1)
671         obj.query.clear_ordering(force=True)
672         obj.query.add_ordering(*order_by)
673         return obj.get()
674 
675     def earliest(self, *fields):
676         if self.query.is_sliced:
677             raise TypeError('Cannot change a query once a slice has been taken.')
678         return self._earliest(*fields)
679 
680     def latest(self, *fields):
681         if self.query.is_sliced:
682             raise TypeError('Cannot change a query once a slice has been taken.')
683         return self.reverse()._earliest(*fields)
684 
685     def first(self):
686         """Return the first object of a query or None if no match is found."""
687         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
688             return obj
689 
690     def last(self):
691         """Return the last object of a query or None if no match is found."""
692         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
693             return obj
694 
695     def in_bulk(self, id_list=None, *, field_name='pk'):
696         """
697         Return a dictionary mapping each of the given IDs to the object with
698         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
699         """
700         if self.query.is_sliced:
701             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
702         opts = self.model._meta
703         unique_fields = [
704             constraint.fields[0]
705             for constraint in opts.total_unique_constraints
706             if len(constraint.fields) == 1
707         ]
708         if (
709             field_name != 'pk' and
710             not opts.get_field(field_name).unique and
711             field_name not in unique_fields and
712             self.query.distinct_fields != (field_name,)
713         ):
714             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
715         if id_list is not None:
716             if not id_list:
717                 return {}
718             filter_key = '{}__in'.format(field_name)
719             batch_size = connections[self.db].features.max_query_params
720             id_list = tuple(id_list)
721             # If the database has a limit on the number of query parameters
722             # (e.g. SQLite), retrieve objects in batches if necessary.
723             if batch_size and batch_size < len(id_list):
724                 qs = ()
725                 for offset in range(0, len(id_list), batch_size):
726                     batch = id_list[offset:offset + batch_size]
727                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
728             else:
729                 qs = self.filter(**{filter_key: id_list}).order_by()
730         else:
731             qs = self._chain()
732         return {getattr(obj, field_name): obj for obj in qs}
733 
734     def delete(self):
735         """Delete the records in the current QuerySet."""
736         self._not_support_combined_queries('delete')
737         if self.query.is_sliced:
738             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
739         if self.query.distinct or self.query.distinct_fields:
740             raise TypeError('Cannot call delete() after .distinct().')
741         if self._fields is not None:
742             raise TypeError("Cannot call delete() after .values() or .values_list()")
743 
744         del_query = self._chain()
745 
746         # The delete is actually 2 queries - one to find related objects,
747         # and one to delete. Make sure that the discovery of related
748         # objects is performed on the same database as the deletion.
749         del_query._for_write = True
750 
751         # Disable non-supported fields.
752         del_query.query.select_for_update = False
753         del_query.query.select_related = False
754         del_query.query.clear_ordering(force=True)
755 
756         collector = Collector(using=del_query.db)
757         collector.collect(del_query)
758         deleted, _rows_count = collector.delete()
759 
760         # Clear the result cache, in case this QuerySet gets reused.
761         self._result_cache = None
762         return deleted, _rows_count
763 
764     delete.alters_data = True
765     delete.queryset_only = True
766 
767     def _raw_delete(self, using):
768         """
769         Delete objects found from the given queryset in single direct SQL
770         query. No signals are sent and there is no protection for cascades.
771         """
772         query = self.query.clone()
773         query.__class__ = sql.DeleteQuery
774         cursor = query.get_compiler(using).execute_sql(CURSOR)
775         if cursor:
776             with cursor:
777                 return cursor.rowcount
778         return 0
779     _raw_delete.alters_data = True
780 
781     def update(self, **kwargs):
782         """
783         Update all elements in the current QuerySet, setting all the given
784         fields to the appropriate values.
785         """
786         self._not_support_combined_queries('update')
787         if self.query.is_sliced:
788             raise TypeError('Cannot update a query once a slice has been taken.')
789         self._for_write = True
790         query = self.query.chain(sql.UpdateQuery)
791         query.add_update_values(kwargs)
792         # Clear any annotations so that they won't be present in subqueries.
793         query.annotations = {}
794         with transaction.mark_for_rollback_on_error(using=self.db):
795             rows = query.get_compiler(self.db).execute_sql(CURSOR)
796         self._result_cache = None
797         return rows
798     update.alters_data = True
799 
800     def _update(self, values):
801         """
802         A version of update() that accepts field objects instead of field names.
803         Used primarily for model saving and not intended for use by general
804         code (it requires too much poking around at model internals to be
805         useful at that level).
806         """
807         if self.query.is_sliced:
808             raise TypeError('Cannot update a query once a slice has been taken.')
809         query = self.query.chain(sql.UpdateQuery)
810         query.add_update_fields(values)
811         # Clear any annotations so that they won't be present in subqueries.
812         query.annotations = {}
813         self._result_cache = None
814         return query.get_compiler(self.db).execute_sql(CURSOR)
815     _update.alters_data = True
816     _update.queryset_only = False
817 
818     def exists(self):
819         if self._result_cache is None:
820             return self.query.has_results(using=self.db)
821         return bool(self._result_cache)
822 
823     def contains(self, obj):
824         """Return True if the queryset contains an object."""
825         self._not_support_combined_queries('contains')
826         if self._fields is not None:
827             raise TypeError(
828                 'Cannot call QuerySet.contains() after .values() or '
829                 '.values_list().'
830             )
831         try:
832             if obj._meta.concrete_model != self.model._meta.concrete_model:
833                 return False
834         except AttributeError:
835             raise TypeError("'obj' must be a model instance.")
836         if obj.pk is None:
837             raise ValueError(
838                 'QuerySet.contains() cannot be used on unsaved objects.'
839             )
840         if self._result_cache is not None:
841             return obj in self._result_cache
842         return self.filter(pk=obj.pk).exists()
843 
844     def _prefetch_related_objects(self):
845         # This method can only be called once the result cache has been filled.
846         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
847         self._prefetch_done = True
848 
849     def explain(self, *, format=None, **options):
850         return self.query.explain(using=self.db, format=format, **options)
851 
852     ##################################################
853     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
854     ##################################################
855 
856     def raw(self, raw_query, params=(), translations=None, using=None):
857         if using is None:
858             using = self.db
859         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
860         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
861         return qs
862 
863     def _values(self, *fields, **expressions):
864         clone = self._chain()
865         if expressions:
866             clone = clone.annotate(**expressions)
867         clone._fields = fields
868         clone.query.set_values(fields)
869         return clone
870 
871     def values(self, *fields, **expressions):
872         fields += tuple(expressions)
873         clone = self._values(*fields, **expressions)
874         clone._iterable_class = ValuesIterable
875         return clone
876 
877     def values_list(self, *fields, flat=False, named=False):
878         if flat and named:
879             raise TypeError("'flat' and 'named' can't be used together.")
880         if flat and len(fields) > 1:
881             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
882 
883         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
884         _fields = []
885         expressions = {}
886         counter = 1
887         for field in fields:
888             if hasattr(field, 'resolve_expression'):
889                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
890                 while True:
891                     field_id = field_id_prefix + str(counter)
892                     counter += 1
893                     if field_id not in field_names:
894                         break
895                 expressions[field_id] = field
896                 _fields.append(field_id)
897             else:
898                 _fields.append(field)
899 
900         clone = self._values(*_fields, **expressions)
901         clone._iterable_class = (
902             NamedValuesListIterable if named
903             else FlatValuesListIterable if flat
904             else ValuesListIterable
905         )
906         return clone
907 
908     def dates(self, field_name, kind, order='ASC'):
909         """
910         Return a list of date objects representing all available dates for
911         the given field_name, scoped to 'kind'.
912         """
913         if kind not in ('year', 'month', 'week', 'day'):
914             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
915         if order not in ('ASC', 'DESC'):
916             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
917         return self.annotate(
918             datefield=Trunc(field_name, kind, output_field=DateField()),
919             plain_field=F(field_name)
920         ).values_list(
921             'datefield', flat=True
922         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
923 
924     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst
925     # argument.
926     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):
927         """
928         Return a list of datetime objects representing all available
929         datetimes for the given field_name, scoped to 'kind'.
930         """
931         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
932             raise ValueError(
933                 "'kind' must be one of 'year', 'month', 'week', 'day', "
934                 "'hour', 'minute', or 'second'."
935             )
936         if order not in ('ASC', 'DESC'):
937             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
938         if settings.USE_TZ:
939             if tzinfo is None:
940                 tzinfo = timezone.get_current_timezone()
941         else:
942             tzinfo = None
943         return self.annotate(
944             datetimefield=Trunc(
945                 field_name,
946                 kind,
947                 output_field=DateTimeField(),
948                 tzinfo=tzinfo,
949                 is_dst=is_dst,
950             ),
951             plain_field=F(field_name)
952         ).values_list(
953             'datetimefield', flat=True
954         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
955 
956     def none(self):
957         """Return an empty QuerySet."""
958         clone = self._chain()
959         clone.query.set_empty()
960         return clone
961 
962     ##################################################################
963     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
964     ##################################################################
965 
966     def all(self):
967         """
968         Return a new QuerySet that is a copy of the current one. This allows a
969         QuerySet to proxy for a model manager in some cases.
970         """
971         return self._chain()
972 
973     def filter(self, *args, **kwargs):
974         """
975         Return a new QuerySet instance with the args ANDed to the existing
976         set.
977         """
978         self._not_support_combined_queries('filter')
979         return self._filter_or_exclude(False, args, kwargs)
980 
981     def exclude(self, *args, **kwargs):
982         """
983         Return a new QuerySet instance with NOT (args) ANDed to the existing
984         set.
985         """
986         self._not_support_combined_queries('exclude')
987         return self._filter_or_exclude(True, args, kwargs)
988 
989     def _filter_or_exclude(self, negate, args, kwargs):
990         if (args or kwargs) and self.query.is_sliced:
991             raise TypeError('Cannot filter a query once a slice has been taken.')
992         clone = self._chain()
993         if self._defer_next_filter:
994             self._defer_next_filter = False
995             clone._deferred_filter = negate, args, kwargs
996         else:
997             clone._filter_or_exclude_inplace(negate, args, kwargs)
998         return clone
999 
1000     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1001         if negate:
1002             self._query.add_q(~Q(*args, **kwargs))
1003         else:
1004             self._query.add_q(Q(*args, **kwargs))
1005 
1006     def complex_filter(self, filter_obj):
1007         """
1008         Return a new QuerySet instance with filter_obj added to the filters.
1009 
1010         filter_obj can be a Q object or a dictionary of keyword lookup
1011         arguments.
1012 
1013         This exists to support framework features such as 'limit_choices_to',
1014         and usually it will be more natural to use other methods.
1015         """
1016         if isinstance(filter_obj, Q):
1017             clone = self._chain()
1018             clone.query.add_q(filter_obj)
1019             return clone
1020         else:
1021             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1022 
1023     def _combinator_query(self, combinator, *other_qs, all=False):
1024         # Clone the query to inherit the select list and everything
1025         clone = self._chain()
1026         # Clear limits and ordering so they can be reapplied
1027         clone.query.clear_ordering(force=True)
1028         clone.query.clear_limits()
1029         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1030         clone.query.combinator = combinator
1031         clone.query.combinator_all = all
1032         return clone
1033 
1034     def union(self, *other_qs, all=False):
1035         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1036         if isinstance(self, EmptyQuerySet):
1037             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1038             if not qs:
1039                 return self
1040             if len(qs) == 1:
1041                 return qs[0]
1042             return qs[0]._combinator_query('union', *qs[1:], all=all)
1043         return self._combinator_query('union', *other_qs, all=all)
1044 
1045     def intersection(self, *other_qs):
1046         # If any query is an EmptyQuerySet, return it.
1047         if isinstance(self, EmptyQuerySet):
1048             return self
1049         for other in other_qs:
1050             if isinstance(other, EmptyQuerySet):
1051                 return other
1052         return self._combinator_query('intersection', *other_qs)
1053 
1054     def difference(self, *other_qs):
1055         # If the query is an EmptyQuerySet, return it.
1056         if isinstance(self, EmptyQuerySet):
1057             return self
1058         return self._combinator_query('difference', *other_qs)
1059 
1060     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1061         """
1062         Return a new QuerySet instance that will select objects with a
1063         FOR UPDATE lock.
1064         """
1065         if nowait and skip_locked:
1066             raise ValueError('The nowait option cannot be used with skip_locked.')
1067         obj = self._chain()
1068         obj._for_write = True
1069         obj.query.select_for_update = True
1070         obj.query.select_for_update_nowait = nowait
1071         obj.query.select_for_update_skip_locked = skip_locked
1072         obj.query.select_for_update_of = of
1073         obj.query.select_for_no_key_update = no_key
1074         return obj
1075 
1076     def select_related(self, *fields):
1077         """
1078         Return a new QuerySet instance that will select related objects.
1079 
1080         If fields are specified, they must be ForeignKey fields and only those
1081         related objects are included in the selection.
1082 
1083         If select_related(None) is called, clear the list.
1084         """
1085         self._not_support_combined_queries('select_related')
1086         if self._fields is not None:
1087             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1088 
1089         obj = self._chain()
1090         if fields == (None,):
1091             obj.query.select_related = False
1092         elif fields:
1093             obj.query.add_select_related(fields)
1094         else:
1095             obj.query.select_related = True
1096         return obj
1097 
1098     def prefetch_related(self, *lookups):
1099         """
1100         Return a new QuerySet instance that will prefetch the specified
1101         Many-To-One and Many-To-Many related objects when the QuerySet is
1102         evaluated.
1103 
1104         When prefetch_related() is called more than once, append to the list of
1105         prefetch lookups. If prefetch_related(None) is called, clear the list.
1106         """
1107         self._not_support_combined_queries('prefetch_related')
1108         clone = self._chain()
1109         if lookups == (None,):
1110             clone._prefetch_related_lookups = ()
1111         else:
1112             for lookup in lookups:
1113                 if isinstance(lookup, Prefetch):
1114                     lookup = lookup.prefetch_to
1115                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1116                 if lookup in self.query._filtered_relations:
1117                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1118             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1119         return clone
1120 
1121     def annotate(self, *args, **kwargs):
1122         """
1123         Return a query set in which the returned objects have been annotated
1124         with extra data or aggregations.
1125         """
1126         self._not_support_combined_queries('annotate')
1127         return self._annotate(args, kwargs, select=True)
1128 
1129     def alias(self, *args, **kwargs):
1130         """
1131         Return a query set with added aliases for extra data or aggregations.
1132         """
1133         self._not_support_combined_queries('alias')
1134         return self._annotate(args, kwargs, select=False)
1135 
1136     def _annotate(self, args, kwargs, select=True):
1137         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1138         annotations = {}
1139         for arg in args:
1140             # The default_alias property may raise a TypeError.
1141             try:
1142                 if arg.default_alias in kwargs:
1143                     raise ValueError("The named annotation '%s' conflicts with the "
1144                                      "default name for another annotation."
1145                                      % arg.default_alias)
1146             except TypeError:
1147                 raise TypeError("Complex annotations require an alias")
1148             annotations[arg.default_alias] = arg
1149         annotations.update(kwargs)
1150 
1151         clone = self._chain()
1152         names = self._fields
1153         if names is None:
1154             names = set(chain.from_iterable(
1155                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1156                 for field in self.model._meta.get_fields()
1157             ))
1158 
1159         for alias, annotation in annotations.items():
1160             if alias in names:
1161                 raise ValueError("The annotation '%s' conflicts with a field on "
1162                                  "the model." % alias)
1163             if isinstance(annotation, FilteredRelation):
1164                 clone.query.add_filtered_relation(annotation, alias)
1165             else:
1166                 clone.query.add_annotation(
1167                     annotation, alias, is_summary=False, select=select,
1168                 )
1169         for alias, annotation in clone.query.annotations.items():
1170             if alias in annotations and annotation.contains_aggregate:
1171                 if clone._fields is None:
1172                     clone.query.group_by = True
1173                 else:
1174                     clone.query.set_group_by()
1175                 break
1176 
1177         return clone
1178 
1179     def order_by(self, *field_names):
1180         """Return a new QuerySet instance with the ordering changed."""
1181         if self.query.is_sliced:
1182             raise TypeError('Cannot reorder a query once a slice has been taken.')
1183         obj = self._chain()
1184         obj.query.clear_ordering(force=True, clear_default=False)
1185         obj.query.add_ordering(*field_names)
1186         return obj
1187 
1188     def distinct(self, *field_names):
1189         """
1190         Return a new QuerySet instance that will select only distinct results.
1191         """
1192         self._not_support_combined_queries('distinct')
1193         if self.query.is_sliced:
1194             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1195         obj = self._chain()
1196         obj.query.add_distinct_fields(*field_names)
1197         return obj
1198 
1199     def extra(self, select=None, where=None, params=None, tables=None,
1200               order_by=None, select_params=None):
1201         """Add extra SQL fragments to the query."""
1202         self._not_support_combined_queries('extra')
1203         if self.query.is_sliced:
1204             raise TypeError('Cannot change a query once a slice has been taken.')
1205         clone = self._chain()
1206         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1207         return clone
1208 
1209     def reverse(self):
1210         """Reverse the ordering of the QuerySet."""
1211         if self.query.is_sliced:
1212             raise TypeError('Cannot reverse a query once a slice has been taken.')
1213         clone = self._chain()
1214         clone.query.standard_ordering = not clone.query.standard_ordering
1215         return clone
1216 
1217     def defer(self, *fields):
1218         """
1219         Defer the loading of data for certain fields until they are accessed.
1220         Add the set of deferred fields to any existing set of deferred fields.
1221         The only exception to this is if None is passed in as the only
1222         parameter, in which case removal all deferrals.
1223         """
1224         self._not_support_combined_queries('defer')
1225         if self._fields is not None:
1226             raise TypeError("Cannot call defer() after .values() or .values_list()")
1227         clone = self._chain()
1228         if fields == (None,):
1229             clone.query.clear_deferred_loading()
1230         else:
1231             clone.query.add_deferred_loading(fields)
1232         return clone
1233 
1234     def only(self, *fields):
1235         """
1236         Essentially, the opposite of defer(). Only the fields passed into this
1237         method and that are not already specified as deferred are loaded
1238         immediately when the queryset is evaluated.
1239         """
1240         self._not_support_combined_queries('only')
1241         if self._fields is not None:
1242             raise TypeError("Cannot call only() after .values() or .values_list()")
1243         if fields == (None,):
1244             # Can only pass None to defer(), not only(), as the rest option.
1245             # That won't stop people trying to do this, so let's be explicit.
1246             raise TypeError("Cannot pass None as an argument to only().")
1247         for field in fields:
1248             field = field.split(LOOKUP_SEP, 1)[0]
1249             if field in self.query._filtered_relations:
1250                 raise ValueError('only() is not supported with FilteredRelation.')
1251         clone = self._chain()
1252         clone.query.add_immediate_loading(fields)
1253         return clone
1254 
1255     def using(self, alias):
1256         """Select which database this QuerySet should execute against."""
1257         clone = self._chain()
1258         clone._db = alias
1259         return clone
1260 
1261     ###################################
1262     # PUBLIC INTROSPECTION ATTRIBUTES #
1263     ###################################
1264 
1265     @property
1266     def ordered(self):
1267         """
1268         Return True if the QuerySet is ordered -- i.e. has an order_by()
1269         clause or a default ordering on the model (or is empty).
1270         """
1271         if isinstance(self, EmptyQuerySet):
1272             return True
1273         if self.query.extra_order_by or self.query.order_by:
1274             return True
1275         elif (
1276             self.query.default_ordering and
1277             self.query.get_meta().ordering and
1278             # A default ordering doesn't affect GROUP BY queries.
1279             not self.query.group_by
1280         ):
1281             return True
1282         else:
1283             return False
1284 
1285     @property
1286     def db(self):
1287         """Return the database used if this query is executed now."""
1288         if self._for_write:
1289             return self._db or router.db_for_write(self.model, **self._hints)
1290         return self._db or router.db_for_read(self.model, **self._hints)
1291 
1292     ###################
1293     # PRIVATE METHODS #
1294     ###################
1295 
1296     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1297         """
1298         Insert a new record for the given model. This provides an interface to
1299         the InsertQuery class and is how Model.save() is implemented.
1300         """
1301         self._for_write = True
1302         if using is None:
1303             using = self.db
1304         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1305         query.insert_values(fields, objs, raw=raw)
1306         return query.get_compiler(using=using).execute_sql(returning_fields)
1307     _insert.alters_data = True
1308     _insert.queryset_only = False
1309 
1310     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1311         """
1312         Helper method for bulk_create() to insert objs one batch at a time.
1313         """
1314         connection = connections[self.db]
1315         if ignore_conflicts and not connection.features.supports_ignore_conflicts:
1316             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1317         ops = connection.ops
1318         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1319         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1320         inserted_rows = []
1321         bulk_return = connection.features.can_return_rows_from_bulk_insert
1322         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1323             if bulk_return and not ignore_conflicts:
1324                 inserted_rows.extend(self._insert(
1325                     item, fields=fields, using=self.db,
1326                     returning_fields=self.model._meta.db_returning_fields,
1327                     ignore_conflicts=ignore_conflicts,
1328                 ))
1329             else:
1330                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1331         return inserted_rows
1332 
1333     def _chain(self):
1334         """
1335         Return a copy of the current QuerySet that's ready for another
1336         operation.
1337         """
1338         obj = self._clone()
1339         if obj._sticky_filter:
1340             obj.query.filter_is_sticky = True
1341             obj._sticky_filter = False
1342         return obj
1343 
1344     def _clone(self):
1345         """
1346         Return a copy of the current QuerySet. A lightweight alternative
1347         to deepcopy().
1348         """
1349         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1350         c._sticky_filter = self._sticky_filter
1351         c._for_write = self._for_write
1352         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1353         c._known_related_objects = self._known_related_objects
1354         c._iterable_class = self._iterable_class
1355         c._fields = self._fields
1356         return c
1357 
1358     def _fetch_all(self):
1359         if self._result_cache is None:
1360             self._result_cache = list(self._iterable_class(self))
1361         if self._prefetch_related_lookups and not self._prefetch_done:
1362             self._prefetch_related_objects()
1363 
1364     def _next_is_sticky(self):
1365         """
1366         Indicate that the next filter call and the one following that should
1367         be treated as a single filter. This is only important when it comes to
1368         determining when to reuse tables for many-to-many filters. Required so
1369         that we can filter naturally on the results of related managers.
1370 
1371         This doesn't return a clone of the current QuerySet (it returns
1372         "self"). The method is only used internally and should be immediately
1373         followed by a filter() that does create a clone.
1374         """
1375         self._sticky_filter = True
1376         return self
1377 
1378     def _merge_sanity_check(self, other):
1379         """Check that two QuerySet classes may be merged."""
1380         if self._fields is not None and (
1381                 set(self.query.values_select) != set(other.query.values_select) or
1382                 set(self.query.extra_select) != set(other.query.extra_select) or
1383                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1384             raise TypeError(
1385                 "Merging '%s' classes must involve the same values in each case."
1386                 % self.__class__.__name__
1387             )
1388 
1389     def _merge_known_related_objects(self, other):
1390         """
1391         Keep track of all known related objects from either QuerySet instance.
1392         """
1393         for field, objects in other._known_related_objects.items():
1394             self._known_related_objects.setdefault(field, {}).update(objects)
1395 
1396     def resolve_expression(self, *args, **kwargs):
1397         if self._fields and len(self._fields) > 1:
1398             # values() queryset can only be used as nested queries
1399             # if they are set up to select only a single field.
1400             raise TypeError('Cannot use multi-field values as a filter value.')
1401         query = self.query.resolve_expression(*args, **kwargs)
1402         query._db = self._db
1403         return query
1404     resolve_expression.queryset_only = True
1405 
1406     def _add_hints(self, **hints):
1407         """
1408         Update hinting information for use by routers. Add new key/values or
1409         overwrite existing key/values.
1410         """
1411         self._hints.update(hints)
1412 
1413     def _has_filters(self):
1414         """
1415         Check if this QuerySet has any filtering going on. This isn't
1416         equivalent with checking if all objects are present in results, for
1417         example, qs[1:]._has_filters() -> False.
1418         """
1419         return self.query.has_filters()
1420 
1421     @staticmethod
1422     def _validate_values_are_expressions(values, method_name):
1423         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1424         if invalid_args:
1425             raise TypeError(
1426                 'QuerySet.%s() received non-expression(s): %s.' % (
1427                     method_name,
1428                     ', '.join(invalid_args),
1429                 )
1430             )
1431 
1432     def _not_support_combined_queries(self, operation_name):
1433         if self.query.combinator:
1434             raise NotSupportedError(
1435                 'Calling QuerySet.%s() after %s() is not supported.'
1436                 % (operation_name, self.query.combinator)
1437             )
1438 
1439     def _check_operator_queryset(self, other, operator_):
1440         if self.query.combinator or other.query.combinator:
1441             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 16:58:09.626 | INFO     | Search completed. Bug locations: [<file>django/db/models/base.py</file>
<class>Model</class>
<method>delete</method>
<code>
983     def delete(self, using=None, keep_parents=False):
984         if self.pk is None:
985             raise ValueError(
986                 "%s object can't be deleted because its %s attribute is set "
987                 "to None." % (self._meta.object_name, self._meta.pk.attname)
988             )
989         using = using or router.db_for_write(self.__class__, instance=self)
990         collector = Collector(using=using)
991         collector.collect([self], keep_parents=keep_parents)
992         return collector.delete()

</code><intended_behavior>This method should be extended to include an optional parameter or context that allows for specifying the deletion's origin. When emitting pre-delete or post-delete signals, this method should ensure that the signal's data includes whether the deletion was initiated directly through a model instance. Signal receivers can then use this context to understand the deletion's origin. The implementation must ensure this functionality does not break or change the existing API in a way that would require changes from end-users unless absolutely necessary. This means adding parameters with default values could be a preferred approach, maintaining backwards compatibility.</intended_behavior>, <file>django/db/models/base.py</file>
<class>Model</class>
<method>None</method>
<code>
424 class Model(metaclass=ModelBase):
425 
426     def __init__(self, *args, **kwargs):
427         # Alias some things as locals to avoid repeat global lookups
428         cls = self.__class__
429         opts = self._meta
430         _setattr = setattr
431         _DEFERRED = DEFERRED
432         if opts.abstract:
433             raise TypeError('Abstract models cannot be instantiated.')
434 
435         pre_init.send(sender=cls, args=args, kwargs=kwargs)
436 
437         # Set up the storage for instance state
438         self._state = ModelState()
439 
440         # There is a rather weird disparity here; if kwargs, it's set, then args
441         # overrides it. It should be one or the other; don't duplicate the work
442         # The reason for the kwargs check is that standard iterator passes in by
443         # args, and instantiation for iteration is 33% faster.
444         if len(args) > len(opts.concrete_fields):
445             # Daft, but matches old exception sans the err msg.
446             raise IndexError("Number of args exceeds number of fields")
447 
448         if not kwargs:
449             fields_iter = iter(opts.concrete_fields)
450             # The ordering of the zip calls matter - zip throws StopIteration
451             # when an iter throws it. So if the first iter throws it, the second
452             # is *not* consumed. We rely on this, so don't change the order
453             # without changing the logic.
454             for val, field in zip(args, fields_iter):
455                 if val is _DEFERRED:
456                     continue
457                 _setattr(self, field.attname, val)
458         else:
459             # Slower, kwargs-ready version.
460             fields_iter = iter(opts.fields)
461             for val, field in zip(args, fields_iter):
462                 if val is _DEFERRED:
463                     continue
464                 _setattr(self, field.attname, val)
465                 if kwargs.pop(field.name, NOT_PROVIDED) is not NOT_PROVIDED:
466                     raise TypeError(
467                         f"{cls.__qualname__}() got both positional and "
468                         f"keyword arguments for field '{field.name}'."
469                     )
470 
471         # Now we're left with the unprocessed fields that *must* come from
472         # keywords, or default.
473 
474         for field in fields_iter:
475             is_related_object = False
476             # Virtual field
477             if field.attname not in kwargs and field.column is None:
478                 continue
479             if kwargs:
480                 if isinstance(field.remote_field, ForeignObjectRel):
481                     try:
482                         # Assume object instance was passed in.
483                         rel_obj = kwargs.pop(field.name)
484                         is_related_object = True
485                     except KeyError:
486                         try:
487                             # Object instance wasn't passed in -- must be an ID.
488                             val = kwargs.pop(field.attname)
489                         except KeyError:
490                             val = field.get_default()
491                 else:
492                     try:
493                         val = kwargs.pop(field.attname)
494                     except KeyError:
495                         # This is done with an exception rather than the
496                         # default argument on pop because we don't want
497                         # get_default() to be evaluated, and then not used.
498                         # Refs #12057.
499                         val = field.get_default()
500             else:
501                 val = field.get_default()
502 
503             if is_related_object:
504                 # If we are passed a related instance, set it using the
505                 # field.name instead of field.attname (e.g. "user" instead of
506                 # "user_id") so that the object gets properly cached (and type
507                 # checked) by the RelatedObjectDescriptor.
508                 if rel_obj is not _DEFERRED:
509                     _setattr(self, field.name, rel_obj)
510             else:
511                 if val is not _DEFERRED:
512                     _setattr(self, field.attname, val)
513 
514         if kwargs:
515             property_names = opts._property_names
516             for prop in tuple(kwargs):
517                 try:
518                     # Any remaining kwargs must correspond to properties or
519                     # virtual fields.
520                     if prop in property_names or opts.get_field(prop):
521                         if kwargs[prop] is not _DEFERRED:
522                             _setattr(self, prop, kwargs[prop])
523                         del kwargs[prop]
524                 except (AttributeError, FieldDoesNotExist):
525                     pass
526             for kwarg in kwargs:
527                 raise TypeError("%s() got an unexpected keyword argument '%s'" % (cls.__name__, kwarg))
528         super().__init__()
529         post_init.send(sender=cls, instance=self)
530 
531     @classmethod
532     def from_db(cls, db, field_names, values):
533         if len(values) != len(cls._meta.concrete_fields):
534             values_iter = iter(values)
535             values = [
536                 next(values_iter) if f.attname in field_names else DEFERRED
537                 for f in cls._meta.concrete_fields
538             ]
539         new = cls(*values)
540         new._state.adding = False
541         new._state.db = db
542         return new
543 
544     def __repr__(self):
545         return '<%s: %s>' % (self.__class__.__name__, self)
546 
547     def __str__(self):
548         return '%s object (%s)' % (self.__class__.__name__, self.pk)
549 
550     def __eq__(self, other):
551         if not isinstance(other, Model):
552             return NotImplemented
553         if self._meta.concrete_model != other._meta.concrete_model:
554             return False
555         my_pk = self.pk
556         if my_pk is None:
557             return self is other
558         return my_pk == other.pk
559 
560     def __hash__(self):
561         if self.pk is None:
562             raise TypeError("Model instances without primary key value are unhashable")
563         return hash(self.pk)
564 
565     def __reduce__(self):
566         data = self.__getstate__()
567         data[DJANGO_VERSION_PICKLE_KEY] = django.__version__
568         class_id = self._meta.app_label, self._meta.object_name
569         return model_unpickle, (class_id,), data
570 
571     def __getstate__(self):
572         """Hook to allow choosing the attributes to pickle."""
573         state = self.__dict__.copy()
574         state['_state'] = copy.copy(state['_state'])
575         # memoryview cannot be pickled, so cast it to bytes and store
576         # separately.
577         _memoryview_attrs = []
578         for attr, value in state.items():
579             if isinstance(value, memoryview):
580                 _memoryview_attrs.append((attr, bytes(value)))
581         if _memoryview_attrs:
582             state['_memoryview_attrs'] = _memoryview_attrs
583             for attr, value in _memoryview_attrs:
584                 state.pop(attr)
585         return state
586 
587     def __setstate__(self, state):
588         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
589         if pickled_version:
590             if pickled_version != django.__version__:
591                 warnings.warn(
592                     "Pickled model instance's Django version %s does not "
593                     "match the current version %s."
594                     % (pickled_version, django.__version__),
595                     RuntimeWarning,
596                     stacklevel=2,
597                 )
598         else:
599             warnings.warn(
600                 "Pickled model instance's Django version is not specified.",
601                 RuntimeWarning,
602                 stacklevel=2,
603             )
604         if '_memoryview_attrs' in state:
605             for attr, value in state.pop('_memoryview_attrs'):
606                 state[attr] = memoryview(value)
607         self.__dict__.update(state)
608 
609     def _get_pk_val(self, meta=None):
610         meta = meta or self._meta
611         return getattr(self, meta.pk.attname)
612 
613     def _set_pk_val(self, value):
614         for parent_link in self._meta.parents.values():
615             if parent_link and parent_link != self._meta.pk:
616                 setattr(self, parent_link.target_field.attname, value)
617         return setattr(self, self._meta.pk.attname, value)
618 
619     pk = property(_get_pk_val, _set_pk_val)
620 
621     def get_deferred_fields(self):
622         """
623         Return a set containing names of deferred fields for this instance.
624         """
625         return {
626             f.attname for f in self._meta.concrete_fields
627             if f.attname not in self.__dict__
628         }
629 
630     def refresh_from_db(self, using=None, fields=None):
631         """
632         Reload field values from the database.
633 
634         By default, the reloading happens from the database this instance was
635         loaded from, or by the read router if this instance wasn't loaded from
636         any database. The using parameter will override the default.
637 
638         Fields can be used to specify which fields to reload. The fields
639         should be an iterable of field attnames. If fields is None, then
640         all non-deferred fields are reloaded.
641 
642         When accessing deferred fields of an instance, the deferred loading
643         of the field will call this method.
644         """
645         if fields is None:
646             self._prefetched_objects_cache = {}
647         else:
648             prefetched_objects_cache = getattr(self, '_prefetched_objects_cache', ())
649             for field in fields:
650                 if field in prefetched_objects_cache:
651                     del prefetched_objects_cache[field]
652                     fields.remove(field)
653             if not fields:
654                 return
655             if any(LOOKUP_SEP in f for f in fields):
656                 raise ValueError(
657                     'Found "%s" in fields argument. Relations and transforms '
658                     'are not allowed in fields.' % LOOKUP_SEP)
659 
660         hints = {'instance': self}
661         db_instance_qs = self.__class__._base_manager.db_manager(using, hints=hints).filter(pk=self.pk)
662 
663         # Use provided fields, if not set then reload all non-deferred fields.
664         deferred_fields = self.get_deferred_fields()
665         if fields is not None:
666             fields = list(fields)
667             db_instance_qs = db_instance_qs.only(*fields)
668         elif deferred_fields:
669             fields = [f.attname for f in self._meta.concrete_fields
670                       if f.attname not in deferred_fields]
671             db_instance_qs = db_instance_qs.only(*fields)
672 
673         db_instance = db_instance_qs.get()
674         non_loaded_fields = db_instance.get_deferred_fields()
675         for field in self._meta.concrete_fields:
676             if field.attname in non_loaded_fields:
677                 # This field wasn't refreshed - skip ahead.
678                 continue
679             setattr(self, field.attname, getattr(db_instance, field.attname))
680             # Clear cached foreign keys.
681             if field.is_relation and field.is_cached(self):
682                 field.delete_cached_value(self)
683 
684         # Clear cached relations.
685         for field in self._meta.related_objects:
686             if field.is_cached(self):
687                 field.delete_cached_value(self)
688 
689         self._state.db = db_instance._state.db
690 
691     def serializable_value(self, field_name):
692         """
693         Return the value of the field name for this instance. If the field is
694         a foreign key, return the id value instead of the object. If there's
695         no Field object with this name on the model, return the model
696         attribute's value.
697 
698         Used to serialize a field's value (in the serializer, or form output,
699         for example). Normally, you would just access the attribute directly
700         and not use this method.
701         """
702         try:
703             field = self._meta.get_field(field_name)
704         except FieldDoesNotExist:
705             return getattr(self, field_name)
706         return getattr(self, field.attname)
707 
708     def save(self, force_insert=False, force_update=False, using=None,
709              update_fields=None):
710         """
711         Save the current instance. Override this in a subclass if you want to
712         control the saving process.
713 
714         The 'force_insert' and 'force_update' parameters can be used to insist
715         that the "save" must be an SQL insert or update (or equivalent for
716         non-SQL backends), respectively. Normally, they should not be set.
717         """
718         self._prepare_related_fields_for_save(operation_name='save')
719 
720         using = using or router.db_for_write(self.__class__, instance=self)
721         if force_insert and (force_update or update_fields):
722             raise ValueError("Cannot force both insert and updating in model saving.")
723 
724         deferred_fields = self.get_deferred_fields()
725         if update_fields is not None:
726             # If update_fields is empty, skip the save. We do also check for
727             # no-op saves later on for inheritance cases. This bailout is
728             # still needed for skipping signal sending.
729             if not update_fields:
730                 return
731 
732             update_fields = frozenset(update_fields)
733             field_names = set()
734 
735             for field in self._meta.concrete_fields:
736                 if not field.primary_key:
737                     field_names.add(field.name)
738 
739                     if field.name != field.attname:
740                         field_names.add(field.attname)
741 
742             non_model_fields = update_fields.difference(field_names)
743 
744             if non_model_fields:
745                 raise ValueError(
746                     'The following fields do not exist in this model, are m2m '
747                     'fields, or are non-concrete fields: %s'
748                     % ', '.join(non_model_fields)
749                 )
750 
751         # If saving to the same database, and this model is deferred, then
752         # automatically do an "update_fields" save on the loaded fields.
753         elif not force_insert and deferred_fields and using == self._state.db:
754             field_names = set()
755             for field in self._meta.concrete_fields:
756                 if not field.primary_key and not hasattr(field, 'through'):
757                     field_names.add(field.attname)
758             loaded_fields = field_names.difference(deferred_fields)
759             if loaded_fields:
760                 update_fields = frozenset(loaded_fields)
761 
762         self.save_base(using=using, force_insert=force_insert,
763                        force_update=force_update, update_fields=update_fields)
764     save.alters_data = True
765 
766     def save_base(self, raw=False, force_insert=False,
767                   force_update=False, using=None, update_fields=None):
768         """
769         Handle the parts of saving which should be done only once per save,
770         yet need to be done in raw saves, too. This includes some sanity
771         checks and signal sending.
772 
773         The 'raw' argument is telling save_base not to save any parent
774         models and not to do any changes to the values before save. This
775         is used by fixture loading.
776         """
777         using = using or router.db_for_write(self.__class__, instance=self)
778         assert not (force_insert and (force_update or update_fields))
779         assert update_fields is None or update_fields
780         cls = origin = self.__class__
781         # Skip proxies, but keep the origin as the proxy model.
782         if cls._meta.proxy:
783             cls = cls._meta.concrete_model
784         meta = cls._meta
785         if not meta.auto_created:
786             pre_save.send(
787                 sender=origin, instance=self, raw=raw, using=using,
788                 update_fields=update_fields,
789             )
790         # A transaction isn't needed if one query is issued.
791         if meta.parents:
792             context_manager = transaction.atomic(using=using, savepoint=False)
793         else:
794             context_manager = transaction.mark_for_rollback_on_error(using=using)
795         with context_manager:
796             parent_inserted = False
797             if not raw:
798                 parent_inserted = self._save_parents(cls, using, update_fields)
799             updated = self._save_table(
800                 raw, cls, force_insert or parent_inserted,
801                 force_update, using, update_fields,
802             )
803         # Store the database on which the object was saved
804         self._state.db = using
805         # Once saved, this is no longer a to-be-added instance.
806         self._state.adding = False
807 
808         # Signal that the save is complete
809         if not meta.auto_created:
810             post_save.send(
811                 sender=origin, instance=self, created=(not updated),
812                 update_fields=update_fields, raw=raw, using=using,
813             )
814 
815     save_base.alters_data = True
816 
817     def _save_parents(self, cls, using, update_fields):
818         """Save all the parents of cls using values from self."""
819         meta = cls._meta
820         inserted = False
821         for parent, field in meta.parents.items():
822             # Make sure the link fields are synced between parent and self.
823             if (field and getattr(self, parent._meta.pk.attname) is None and
824                     getattr(self, field.attname) is not None):
825                 setattr(self, parent._meta.pk.attname, getattr(self, field.attname))
826             parent_inserted = self._save_parents(cls=parent, using=using, update_fields=update_fields)
827             updated = self._save_table(
828                 cls=parent, using=using, update_fields=update_fields,
829                 force_insert=parent_inserted,
830             )
831             if not updated:
832                 inserted = True
833             # Set the parent's PK value to self.
834             if field:
835                 setattr(self, field.attname, self._get_pk_val(parent._meta))
836                 # Since we didn't have an instance of the parent handy set
837                 # attname directly, bypassing the descriptor. Invalidate
838                 # the related object cache, in case it's been accidentally
839                 # populated. A fresh instance will be re-built from the
840                 # database if necessary.
841                 if field.is_cached(self):
842                     field.delete_cached_value(self)
843         return inserted
844 
845     def _save_table(self, raw=False, cls=None, force_insert=False,
846                     force_update=False, using=None, update_fields=None):
847         """
848         Do the heavy-lifting involved in saving. Update or insert the data
849         for a single table.
850         """
851         meta = cls._meta
852         non_pks = [f for f in meta.local_concrete_fields if not f.primary_key]
853 
854         if update_fields:
855             non_pks = [f for f in non_pks
856                        if f.name in update_fields or f.attname in update_fields]
857 
858         pk_val = self._get_pk_val(meta)
859         if pk_val is None:
860             pk_val = meta.pk.get_pk_value_on_save(self)
861             setattr(self, meta.pk.attname, pk_val)
862         pk_set = pk_val is not None
863         if not pk_set and (force_update or update_fields):
864             raise ValueError("Cannot force an update in save() with no primary key.")
865         updated = False
866         # Skip an UPDATE when adding an instance and primary key has a default.
867         if (
868             not raw and
869             not force_insert and
870             self._state.adding and
871             meta.pk.default and
872             meta.pk.default is not NOT_PROVIDED
873         ):
874             force_insert = True
875         # If possible, try an UPDATE. If that doesn't update anything, do an INSERT.
876         if pk_set and not force_insert:
877             base_qs = cls._base_manager.using(using)
878             values = [(f, None, (getattr(self, f.attname) if raw else f.pre_save(self, False)))
879                       for f in non_pks]
880             forced_update = update_fields or force_update
881             updated = self._do_update(base_qs, using, pk_val, values, update_fields,
882                                       forced_update)
883             if force_update and not updated:
884                 raise DatabaseError("Forced update did not affect any rows.")
885             if update_fields and not updated:
886                 raise DatabaseError("Save with update_fields did not affect any rows.")
887         if not updated:
888             if meta.order_with_respect_to:
889                 # If this is a model with an order_with_respect_to
890                 # autopopulate the _order field
891                 field = meta.order_with_respect_to
892                 filter_args = field.get_filter_kwargs_for_object(self)
893                 self._order = cls._base_manager.using(using).filter(**filter_args).aggregate(
894                     _order__max=Coalesce(
895                         ExpressionWrapper(Max('_order') + Value(1), output_field=IntegerField()),
896                         Value(0),
897                     ),
898                 )['_order__max']
899             fields = meta.local_concrete_fields
900             if not pk_set:
901                 fields = [f for f in fields if f is not meta.auto_field]
902 
903             returning_fields = meta.db_returning_fields
904             results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
905             if results:
906                 for value, field in zip(results[0], returning_fields):
907                     setattr(self, field.attname, value)
908         return updated
909 
910     def _do_update(self, base_qs, using, pk_val, values, update_fields, forced_update):
911         """
912         Try to update the model. Return True if the model was updated (if an
913         update query was done and a matching row was found in the DB).
914         """
915         filtered = base_qs.filter(pk=pk_val)
916         if not values:
917             # We can end up here when saving a model in inheritance chain where
918             # update_fields doesn't target any field in current model. In that
919             # case we just say the update succeeded. Another case ending up here
920             # is a model with just PK - in that case check that the PK still
921             # exists.
922             return update_fields is not None or filtered.exists()
923         if self._meta.select_on_save and not forced_update:
924             return (
925                 filtered.exists() and
926                 # It may happen that the object is deleted from the DB right after
927                 # this check, causing the subsequent UPDATE to return zero matching
928                 # rows. The same result can occur in some rare cases when the
929                 # database returns zero despite the UPDATE being executed
930                 # successfully (a row is matched and updated). In order to
931                 # distinguish these two cases, the object's existence in the
932                 # database is again checked for if the UPDATE query returns 0.
933                 (filtered._update(values) > 0 or filtered.exists())
934             )
935         return filtered._update(values) > 0
936 
937     def _do_insert(self, manager, using, fields, returning_fields, raw):
938         """
939         Do an INSERT. If returning_fields is defined then this method should
940         return the newly created data for the model.
941         """
942         return manager._insert(
943             [self], fields=fields, returning_fields=returning_fields,
944             using=using, raw=raw,
945         )
946 
947     def _prepare_related_fields_for_save(self, operation_name, fields=None):
948         # Ensure that a model instance without a PK hasn't been assigned to
949         # a ForeignKey or OneToOneField on this model. If the field is
950         # nullable, allowing the save would result in silent data loss.
951         for field in self._meta.concrete_fields:
952             if fields and field not in fields:
953                 continue
954             # If the related field isn't cached, then an instance hasn't been
955             # assigned and there's no need to worry about this check.
956             if field.is_relation and field.is_cached(self):
957                 obj = getattr(self, field.name, None)
958                 if not obj:
959                     continue
960                 # A pk may have been assigned manually to a model instance not
961                 # saved to the database (or auto-generated in a case like
962                 # UUIDField), but we allow the save to proceed and rely on the
963                 # database to raise an IntegrityError if applicable. If
964                 # constraints aren't supported by the database, there's the
965                 # unavoidable risk of data corruption.
966                 if obj.pk is None:
967                     # Remove the object from a related instance cache.
968                     if not field.remote_field.multiple:
969                         field.remote_field.delete_cached_value(obj)
970                     raise ValueError(
971                         "%s() prohibited to prevent data loss due to unsaved "
972                         "related object '%s'." % (operation_name, field.name)
973                     )
974                 elif getattr(self, field.attname) in field.empty_values:
975                     # Use pk from related object if it has been saved after
976                     # an assignment.
977                     setattr(self, field.attname, obj.pk)
978                 # If the relationship's pk/to_field was changed, clear the
979                 # cached relationship.
980                 if getattr(obj, field.target_field.attname) != getattr(self, field.attname):
981                     field.delete_cached_value(self)
982 
983     def delete(self, using=None, keep_parents=False):
984         if self.pk is None:
985             raise ValueError(
986                 "%s object can't be deleted because its %s attribute is set "
987                 "to None." % (self._meta.object_name, self._meta.pk.attname)
988             )
989         using = using or router.db_for_write(self.__class__, instance=self)
990         collector = Collector(using=using)
991         collector.collect([self], keep_parents=keep_parents)
992         return collector.delete()
993 
994     delete.alters_data = True
995 
996     def _get_FIELD_display(self, field):
997         value = getattr(self, field.attname)
998         choices_dict = dict(make_hashable(field.flatchoices))
999         # force_str() to coerce lazy strings.
1000         return force_str(choices_dict.get(make_hashable(value), value), strings_only=True)
1001 
1002     def _get_next_or_previous_by_FIELD(self, field, is_next, **kwargs):
1003         if not self.pk:
1004             raise ValueError("get_next/get_previous cannot be used on unsaved objects.")
1005         op = 'gt' if is_next else 'lt'
1006         order = '' if is_next else '-'
1007         param = getattr(self, field.attname)
1008         q = Q((field.name, param), (f'pk__{op}', self.pk), _connector=Q.AND)
1009         q = Q(q, (f'{field.name}__{op}', param), _connector=Q.OR)
1010         qs = self.__class__._default_manager.using(self._state.db).filter(**kwargs).filter(q).order_by(
1011             '%s%s' % (order, field.name), '%spk' % order
1012         )
1013         try:
1014             return qs[0]
1015         except IndexError:
1016             raise self.DoesNotExist("%s matching query does not exist." % self.__class__._meta.object_name)
1017 
1018     def _get_next_or_previous_in_order(self, is_next):
1019         cachename = "__%s_order_cache" % is_next
1020         if not hasattr(self, cachename):
1021             op = 'gt' if is_next else 'lt'
1022             order = '_order' if is_next else '-_order'
1023             order_field = self._meta.order_with_respect_to
1024             filter_args = order_field.get_filter_kwargs_for_object(self)
1025             obj = self.__class__._default_manager.filter(**filter_args).filter(**{
1026                 '_order__%s' % op: self.__class__._default_manager.values('_order').filter(**{
1027                     self._meta.pk.name: self.pk
1028                 })
1029             }).order_by(order)[:1].get()
1030             setattr(self, cachename, obj)
1031         return getattr(self, cachename)
1032 
1033     def prepare_database_save(self, field):
1034         if self.pk is None:
1035             raise ValueError("Unsaved model instance %r cannot be used in an ORM query." % self)
1036         return getattr(self, field.remote_field.get_related_field().attname)
1037 
1038     def clean(self):
1039         """
1040         Hook for doing any extra model-wide validation after clean() has been
1041         called on every field by self.clean_fields. Any ValidationError raised
1042         by this method will not be associated with a particular field; it will
1043         have a special-case association with the field defined by NON_FIELD_ERRORS.
1044         """
1045         pass
1046 
1047     def validate_unique(self, exclude=None):
1048         """
1049         Check unique constraints on the model and raise ValidationError if any
1050         failed.
1051         """
1052         unique_checks, date_checks = self._get_unique_checks(exclude=exclude)
1053 
1054         errors = self._perform_unique_checks(unique_checks)
1055         date_errors = self._perform_date_checks(date_checks)
1056 
1057         for k, v in date_errors.items():
1058             errors.setdefault(k, []).extend(v)
1059 
1060         if errors:
1061             raise ValidationError(errors)
1062 
1063     def _get_unique_checks(self, exclude=None):
1064         """
1065         Return a list of checks to perform. Since validate_unique() could be
1066         called from a ModelForm, some fields may have been excluded; we can't
1067         perform a unique check on a model that is missing fields involved
1068         in that check. Fields that did not validate should also be excluded,
1069         but they need to be passed in via the exclude argument.
1070         """
1071         if exclude is None:
1072             exclude = []
1073         unique_checks = []
1074 
1075         unique_togethers = [(self.__class__, self._meta.unique_together)]
1076         constraints = [(self.__class__, self._meta.total_unique_constraints)]
1077         for parent_class in self._meta.get_parent_list():
1078             if parent_class._meta.unique_together:
1079                 unique_togethers.append((parent_class, parent_class._meta.unique_together))
1080             if parent_class._meta.total_unique_constraints:
1081                 constraints.append(
1082                     (parent_class, parent_class._meta.total_unique_constraints)
1083                 )
1084 
1085         for model_class, unique_together in unique_togethers:
1086             for check in unique_together:
1087                 if not any(name in exclude for name in check):
1088                     # Add the check if the field isn't excluded.
1089                     unique_checks.append((model_class, tuple(check)))
1090 
1091         for model_class, model_constraints in constraints:
1092             for constraint in model_constraints:
1093                 if not any(name in exclude for name in constraint.fields):
1094                     unique_checks.append((model_class, constraint.fields))
1095 
1096         # These are checks for the unique_for_<date/year/month>.
1097         date_checks = []
1098 
1099         # Gather a list of checks for fields declared as unique and add them to
1100         # the list of checks.
1101 
1102         fields_with_class = [(self.__class__, self._meta.local_fields)]
1103         for parent_class in self._meta.get_parent_list():
1104             fields_with_class.append((parent_class, parent_class._meta.local_fields))
1105 
1106         for model_class, fields in fields_with_class:
1107             for f in fields:
1108                 name = f.name
1109                 if name in exclude:
1110                     continue
1111                 if f.unique:
1112                     unique_checks.append((model_class, (name,)))
1113                 if f.unique_for_date and f.unique_for_date not in exclude:
1114                     date_checks.append((model_class, 'date', name, f.unique_for_date))
1115                 if f.unique_for_year and f.unique_for_year not in exclude:
1116                     date_checks.append((model_class, 'year', name, f.unique_for_year))
1117                 if f.unique_for_month and f.unique_for_month not in exclude:
1118                     date_checks.append((model_class, 'month', name, f.unique_for_month))
1119         return unique_checks, date_checks
1120 
1121     def _perform_unique_checks(self, unique_checks):
1122         errors = {}
1123 
1124         for model_class, unique_check in unique_checks:
1125             # Try to look up an existing object with the same values as this
1126             # object's values for all the unique field.
1127 
1128             lookup_kwargs = {}
1129             for field_name in unique_check:
1130                 f = self._meta.get_field(field_name)
1131                 lookup_value = getattr(self, f.attname)
1132                 # TODO: Handle multiple backends with different feature flags.
1133                 if (lookup_value is None or
1134                         (lookup_value == '' and connection.features.interprets_empty_strings_as_nulls)):
1135                     # no value, skip the lookup
1136                     continue
1137                 if f.primary_key and not self._state.adding:
1138                     # no need to check for unique primary key when editing
1139                     continue
1140                 lookup_kwargs[str(field_name)] = lookup_value
1141 
1142             # some fields were skipped, no reason to do the check
1143             if len(unique_check) != len(lookup_kwargs):
1144                 continue
1145 
1146             qs = model_class._default_manager.filter(**lookup_kwargs)
1147 
1148             # Exclude the current object from the query if we are editing an
1149             # instance (as opposed to creating a new one)
1150             # Note that we need to use the pk as defined by model_class, not
1151             # self.pk. These can be different fields because model inheritance
1152             # allows single model to have effectively multiple primary keys.
1153             # Refs #17615.
1154             model_class_pk = self._get_pk_val(model_class._meta)
1155             if not self._state.adding and model_class_pk is not None:
1156                 qs = qs.exclude(pk=model_class_pk)
1157             if qs.exists():
1158                 if len(unique_check) == 1:
1159                     key = unique_check[0]
1160                 else:
1161                     key = NON_FIELD_ERRORS
1162                 errors.setdefault(key, []).append(self.unique_error_message(model_class, unique_check))
1163 
1164         return errors
1165 
1166     def _perform_date_checks(self, date_checks):
1167         errors = {}
1168         for model_class, lookup_type, field, unique_for in date_checks:
1169             lookup_kwargs = {}
1170             # there's a ticket to add a date lookup, we can remove this special
1171             # case if that makes it's way in
1172             date = getattr(self, unique_for)
1173             if date is None:
1174                 continue
1175             if lookup_type == 'date':
1176                 lookup_kwargs['%s__day' % unique_for] = date.day
1177                 lookup_kwargs['%s__month' % unique_for] = date.month
1178                 lookup_kwargs['%s__year' % unique_for] = date.year
1179             else:
1180                 lookup_kwargs['%s__%s' % (unique_for, lookup_type)] = getattr(date, lookup_type)
1181             lookup_kwargs[field] = getattr(self, field)
1182 
1183             qs = model_class._default_manager.filter(**lookup_kwargs)
1184             # Exclude the current object from the query if we are editing an
1185             # instance (as opposed to creating a new one)
1186             if not self._state.adding and self.pk is not None:
1187                 qs = qs.exclude(pk=self.pk)
1188 
1189             if qs.exists():
1190                 errors.setdefault(field, []).append(
1191                     self.date_error_message(lookup_type, field, unique_for)
1192                 )
1193         return errors
1194 
1195     def date_error_message(self, lookup_type, field_name, unique_for):
1196         opts = self._meta
1197         field = opts.get_field(field_name)
1198         return ValidationError(
1199             message=field.error_messages['unique_for_date'],
1200             code='unique_for_date',
1201             params={
1202                 'model': self,
1203                 'model_name': capfirst(opts.verbose_name),
1204                 'lookup_type': lookup_type,
1205                 'field': field_name,
1206                 'field_label': capfirst(field.verbose_name),
1207                 'date_field': unique_for,
1208                 'date_field_label': capfirst(opts.get_field(unique_for).verbose_name),
1209             }
1210         )
1211 
1212     def unique_error_message(self, model_class, unique_check):
1213         opts = model_class._meta
1214 
1215         params = {
1216             'model': self,
1217             'model_class': model_class,
1218             'model_name': capfirst(opts.verbose_name),
1219             'unique_check': unique_check,
1220         }
1221 
1222         # A unique field
1223         if len(unique_check) == 1:
1224             field = opts.get_field(unique_check[0])
1225             params['field_label'] = capfirst(field.verbose_name)
1226             return ValidationError(
1227                 message=field.error_messages['unique'],
1228                 code='unique',
1229                 params=params,
1230             )
1231 
1232         # unique_together
1233         else:
1234             field_labels = [capfirst(opts.get_field(f).verbose_name) for f in unique_check]
1235             params['field_labels'] = get_text_list(field_labels, _('and'))
1236             return ValidationError(
1237                 message=_("%(model_name)s with this %(field_labels)s already exists."),
1238                 code='unique_together',
1239                 params=params,
1240             )
1241 
1242     def full_clean(self, exclude=None, validate_unique=True):
1243         """
1244         Call clean_fields(), clean(), and validate_unique() on the model.
1245         Raise a ValidationError for any errors that occur.
1246         """
1247         errors = {}
1248         if exclude is None:
1249             exclude = []
1250         else:
1251             exclude = list(exclude)
1252 
1253         try:
1254             self.clean_fields(exclude=exclude)
1255         except ValidationError as e:
1256             errors = e.update_error_dict(errors)
1257 
1258         # Form.clean() is run even if other validation fails, so do the
1259         # same with Model.clean() for consistency.
1260         try:
1261             self.clean()
1262         except ValidationError as e:
1263             errors = e.update_error_dict(errors)
1264 
1265         # Run unique checks, but only for fields that passed validation.
1266         if validate_unique:
1267             for name in errors:
1268                 if name != NON_FIELD_ERRORS and name not in exclude:
1269                     exclude.append(name)
1270             try:
1271                 self.validate_unique(exclude=exclude)
1272             except ValidationError as e:
1273                 errors = e.update_error_dict(errors)
1274 
1275         if errors:
1276             raise ValidationError(errors)
1277 
1278     def clean_fields(self, exclude=None):
1279         """
1280         Clean all fields and raise a ValidationError containing a dict
1281         of all validation errors if any occur.
1282         """
1283         if exclude is None:
1284             exclude = []
1285 
1286         errors = {}
1287         for f in self._meta.fields:
1288             if f.name in exclude:
1289                 continue
1290             # Skip validation for empty fields with blank=True. The developer
1291             # is responsible for making sure they have a valid value.
1292             raw_value = getattr(self, f.attname)
1293             if f.blank and raw_value in f.empty_values:
1294                 continue
1295             try:
1296                 setattr(self, f.attname, f.clean(raw_value, self))
1297             except ValidationError as e:
1298                 errors[f.name] = e.error_list
1299 
1300         if errors:
1301             raise ValidationError(errors)
1302 
1303     @classmethod
1304     def check(cls, **kwargs):
1305         errors = [*cls._check_swappable(), *cls._check_model(), *cls._check_managers(**kwargs)]
1306         if not cls._meta.swapped:
1307             databases = kwargs.get('databases') or []
1308             errors += [
1309                 *cls._check_fields(**kwargs),
1310                 *cls._check_m2m_through_same_relationship(),
1311                 *cls._check_long_column_names(databases),
1312             ]
1313             clash_errors = (
1314                 *cls._check_id_field(),
1315                 *cls._check_field_name_clashes(),
1316                 *cls._check_model_name_db_lookup_clashes(),
1317                 *cls._check_property_name_related_field_accessor_clashes(),
1318                 *cls._check_single_primary_key(),
1319             )
1320             errors.extend(clash_errors)
1321             # If there are field name clashes, hide consequent column name
1322             # clashes.
1323             if not clash_errors:
1324                 errors.extend(cls._check_column_name_clashes())
1325             errors += [
1326                 *cls._check_index_together(),
1327                 *cls._check_unique_together(),
1328                 *cls._check_indexes(databases),
1329                 *cls._check_ordering(),
1330                 *cls._check_constraints(databases),
1331                 *cls._check_default_pk(),
1332             ]
1333 
1334         return errors
1335 
1336     @classmethod
1337     def _check_default_pk(cls):
1338         if (
1339             not cls._meta.abstract and
1340             cls._meta.pk.auto_created and
1341             # Inherited PKs are checked in parents models.
1342             not (
1343                 isinstance(cls._meta.pk, OneToOneField) and
1344                 cls._meta.pk.remote_field.parent_link
1345             ) and
1346             not settings.is_overridden('DEFAULT_AUTO_FIELD') and
1347             cls._meta.app_config and
1348             not cls._meta.app_config._is_default_auto_field_overridden
1349         ):
1350             return [
1351                 checks.Warning(
1352                     f"Auto-created primary key used when not defining a "
1353                     f"primary key type, by default "
1354                     f"'{settings.DEFAULT_AUTO_FIELD}'.",
1355                     hint=(
1356                         f"Configure the DEFAULT_AUTO_FIELD setting or the "
1357                         f"{cls._meta.app_config.__class__.__qualname__}."
1358                         f"default_auto_field attribute to point to a subclass "
1359                         f"of AutoField, e.g. 'django.db.models.BigAutoField'."
1360                     ),
1361                     obj=cls,
1362                     id='models.W042',
1363                 ),
1364             ]
1365         return []
1366 
1367     @classmethod
1368     def _check_swappable(cls):
1369         """Check if the swapped model exists."""
1370         errors = []
1371         if cls._meta.swapped:
1372             try:
1373                 apps.get_model(cls._meta.swapped)
1374             except ValueError:
1375                 errors.append(
1376                     checks.Error(
1377                         "'%s' is not of the form 'app_label.app_name'." % cls._meta.swappable,
1378                         id='models.E001',
1379                     )
1380                 )
1381             except LookupError:
1382                 app_label, model_name = cls._meta.swapped.split('.')
1383                 errors.append(
1384                     checks.Error(
1385                         "'%s' references '%s.%s', which has not been "
1386                         "installed, or is abstract." % (
1387                             cls._meta.swappable, app_label, model_name
1388                         ),
1389                         id='models.E002',
1390                     )
1391                 )
1392         return errors
1393 
1394     @classmethod
1395     def _check_model(cls):
1396         errors = []
1397         if cls._meta.proxy:
1398             if cls._meta.local_fields or cls._meta.local_many_to_many:
1399                 errors.append(
1400                     checks.Error(
1401                         "Proxy model '%s' contains model fields." % cls.__name__,
1402                         id='models.E017',
1403                     )
1404                 )
1405         return errors
1406 
1407     @classmethod
1408     def _check_managers(cls, **kwargs):
1409         """Perform all manager checks."""
1410         errors = []
1411         for manager in cls._meta.managers:
1412             errors.extend(manager.check(**kwargs))
1413         return errors
1414 
1415     @classmethod
1416     def _check_fields(cls, **kwargs):
1417         """Perform all field checks."""
1418         errors = []
1419         for field in cls._meta.local_fields:
1420             errors.extend(field.check(**kwargs))
1421         for field in cls._meta.local_many_to_many:
1422             errors.extend(field.check(from_model=cls, **kwargs))
1423         return errors
1424 
1425     @classmethod
1426     def _check_m2m_through_same_relationship(cls):
1427         """ Check if no relationship model is used by more than one m2m field.
1428         """
1429 
1430         errors = []
1431         seen_intermediary_signatures = []
1432 
1433         fields = cls._meta.local_many_to_many
1434 
1435         # Skip when the target model wasn't found.
1436         fields = (f for f in fields if isinstance(f.remote_field.model, ModelBase))
1437 
1438         # Skip when the relationship model wasn't found.
1439         fields = (f for f in fields if isinstance(f.remote_field.through, ModelBase))
1440 
1441         for f in fields:
1442             signature = (f.remote_field.model, cls, f.remote_field.through, f.remote_field.through_fields)
1443             if signature in seen_intermediary_signatures:
1444                 errors.append(
1445                     checks.Error(
1446                         "The model has two identical many-to-many relations "
1447                         "through the intermediate model '%s'." %
1448                         f.remote_field.through._meta.label,
1449                         obj=cls,
1450                         id='models.E003',
1451                     )
1452                 )
1453             else:
1454                 seen_intermediary_signatures.append(signature)
1455         return errors
1456 
1457     @classmethod
1458     def _check_id_field(cls):
1459         """Check if `id` field is a primary key."""
1460         fields = [f for f in cls._meta.local_fields if f.name == 'id' and f != cls._meta.pk]
1461         # fields is empty or consists of the invalid "id" field
1462         if fields and not fields[0].primary_key and cls._meta.pk.name == 'id':
1463             return [
1464                 checks.Error(
1465                     "'id' can only be used as a field name if the field also "
1466                     "sets 'primary_key=True'.",
1467                     obj=cls,
1468                     id='models.E004',
1469                 )
1470             ]
1471         else:
1472             return []
1473 
1474     @classmethod
1475     def _check_field_name_clashes(cls):
1476         """Forbid field shadowing in multi-table inheritance."""
1477         errors = []
1478         used_fields = {}  # name or attname -> field
1479 
1480         # Check that multi-inheritance doesn't cause field name shadowing.
1481         for parent in cls._meta.get_parent_list():
1482             for f in parent._meta.local_fields:
1483                 clash = used_fields.get(f.name) or used_fields.get(f.attname) or None
1484                 if clash:
1485                     errors.append(
1486                         checks.Error(
1487                             "The field '%s' from parent model "
1488                             "'%s' clashes with the field '%s' "
1489                             "from parent model '%s'." % (
1490                                 clash.name, clash.model._meta,
1491                                 f.name, f.model._meta
1492                             ),
1493                             obj=cls,
1494                             id='models.E005',
1495                         )
1496                     )
1497                 used_fields[f.name] = f
1498                 used_fields[f.attname] = f
1499 
1500         # Check that fields defined in the model don't clash with fields from
1501         # parents, including auto-generated fields like multi-table inheritance
1502         # child accessors.
1503         for parent in cls._meta.get_parent_list():
1504             for f in parent._meta.get_fields():
1505                 if f not in used_fields:
1506                     used_fields[f.name] = f
1507 
1508         for f in cls._meta.local_fields:
1509             clash = used_fields.get(f.name) or used_fields.get(f.attname) or None
1510             # Note that we may detect clash between user-defined non-unique
1511             # field "id" and automatically added unique field "id", both
1512             # defined at the same model. This special case is considered in
1513             # _check_id_field and here we ignore it.
1514             id_conflict = f.name == "id" and clash and clash.name == "id" and clash.model == cls
1515             if clash and not id_conflict:
1516                 errors.append(
1517                     checks.Error(
1518                         "The field '%s' clashes with the field '%s' "
1519                         "from model '%s'." % (
1520                             f.name, clash.name, clash.model._meta
1521                         ),
1522                         obj=f,
1523                         id='models.E006',
1524                     )
1525                 )
1526             used_fields[f.name] = f
1527             used_fields[f.attname] = f
1528 
1529         return errors
1530 
1531     @classmethod
1532     def _check_column_name_clashes(cls):
1533         # Store a list of column names which have already been used by other fields.
1534         used_column_names = []
1535         errors = []
1536 
1537         for f in cls._meta.local_fields:
1538             _, column_name = f.get_attname_column()
1539 
1540             # Ensure the column name is not already in use.
1541             if column_name and column_name in used_column_names:
1542                 errors.append(
1543                     checks.Error(
1544                         "Field '%s' has column name '%s' that is used by "
1545                         "another field." % (f.name, column_name),
1546                         hint="Specify a 'db_column' for the field.",
1547                         obj=cls,
1548                         id='models.E007'
1549                     )
1550                 )
1551             else:
1552                 used_column_names.append(column_name)
1553 
1554         return errors
1555 
1556     @classmethod
1557     def _check_model_name_db_lookup_clashes(cls):
1558         errors = []
1559         model_name = cls.__name__
1560         if model_name.startswith('_') or model_name.endswith('_'):
1561             errors.append(
1562                 checks.Error(
1563                     "The model name '%s' cannot start or end with an underscore "
1564                     "as it collides with the query lookup syntax." % model_name,
1565                     obj=cls,
1566                     id='models.E023'
1567                 )
1568             )
1569         elif LOOKUP_SEP in model_name:
1570             errors.append(
1571                 checks.Error(
1572                     "The model name '%s' cannot contain double underscores as "
1573                     "it collides with the query lookup syntax." % model_name,
1574                     obj=cls,
1575                     id='models.E024'
1576                 )
1577             )
1578         return errors
1579 
1580     @classmethod
1581     def _check_property_name_related_field_accessor_clashes(cls):
1582         errors = []
1583         property_names = cls._meta._property_names
1584         related_field_accessors = (
1585             f.get_attname() for f in cls._meta._get_fields(reverse=False)
1586             if f.is_relation and f.related_model is not None
1587         )
1588         for accessor in related_field_accessors:
1589             if accessor in property_names:
1590                 errors.append(
1591                     checks.Error(
1592                         "The property '%s' clashes with a related field "
1593                         "accessor." % accessor,
1594                         obj=cls,
1595                         id='models.E025',
1596                     )
1597                 )
1598         return errors
1599 
1600     @classmethod
1601     def _check_single_primary_key(cls):
1602         errors = []
1603         if sum(1 for f in cls._meta.local_fields if f.primary_key) > 1:
1604             errors.append(
1605                 checks.Error(
1606                     "The model cannot have more than one field with "
1607                     "'primary_key=True'.",
1608                     obj=cls,
1609                     id='models.E026',
1610                 )
1611             )
1612         return errors
1613 
1614     @classmethod
1615     def _check_index_together(cls):
1616         """Check the value of "index_together" option."""
1617         if not isinstance(cls._meta.index_together, (tuple, list)):
1618             return [
1619                 checks.Error(
1620                     "'index_together' must be a list or tuple.",
1621                     obj=cls,
1622                     id='models.E008',
1623                 )
1624             ]
1625 
1626         elif any(not isinstance(fields, (tuple, list)) for fields in cls._meta.index_together):
1627             return [
1628                 checks.Error(
1629                     "All 'index_together' elements must be lists or tuples.",
1630                     obj=cls,
1631                     id='models.E009',
1632                 )
1633             ]
1634 
1635         else:
1636             errors = []
1637             for fields in cls._meta.index_together:
1638                 errors.extend(cls._check_local_fields(fields, "index_together"))
1639             return errors
1640 
1641     @classmethod
1642     def _check_unique_together(cls):
1643         """Check the value of "unique_together" option."""
1644         if not isinstance(cls._meta.unique_together, (tuple, list)):
1645             return [
1646                 checks.Error(
1647                     "'unique_together' must be a list or tuple.",
1648                     obj=cls,
1649                     id='models.E010',
1650                 )
1651             ]
1652 
1653         elif any(not isinstance(fields, (tuple, list)) for fields in cls._meta.unique_together):
1654             return [
1655                 checks.Error(
1656                     "All 'unique_together' elements must be lists or tuples.",
1657                     obj=cls,
1658                     id='models.E011',
1659                 )
1660             ]
1661 
1662         else:
1663             errors = []
1664             for fields in cls._meta.unique_together:
1665                 errors.extend(cls._check_local_fields(fields, "unique_together"))
1666             return errors
1667 
1668     @classmethod
1669     def _check_indexes(cls, databases):
1670         """Check fields, names, and conditions of indexes."""
1671         errors = []
1672         references = set()
1673         for index in cls._meta.indexes:
1674             # Index name can't start with an underscore or a number, restricted
1675             # for cross-database compatibility with Oracle.
1676             if index.name[0] == '_' or index.name[0].isdigit():
1677                 errors.append(
1678                     checks.Error(
1679                         "The index name '%s' cannot start with an underscore "
1680                         "or a number." % index.name,
1681                         obj=cls,
1682                         id='models.E033',
1683                     ),
1684                 )
1685             if len(index.name) > index.max_name_length:
1686                 errors.append(
1687                     checks.Error(
1688                         "The index name '%s' cannot be longer than %d "
1689                         "characters." % (index.name, index.max_name_length),
1690                         obj=cls,
1691                         id='models.E034',
1692                     ),
1693                 )
1694             if index.contains_expressions:
1695                 for expression in index.expressions:
1696                     references.update(
1697                         ref[0] for ref in cls._get_expr_references(expression)
1698                     )
1699         for db in databases:
1700             if not router.allow_migrate_model(db, cls):
1701                 continue
1702             connection = connections[db]
1703             if not (
1704                 connection.features.supports_partial_indexes or
1705                 'supports_partial_indexes' in cls._meta.required_db_features
1706             ) and any(index.condition is not None for index in cls._meta.indexes):
1707                 errors.append(
1708                     checks.Warning(
1709                         '%s does not support indexes with conditions.'
1710                         % connection.display_name,
1711                         hint=(
1712                             "Conditions will be ignored. Silence this warning "
1713                             "if you don't care about it."
1714                         ),
1715                         obj=cls,
1716                         id='models.W037',
1717                     )
1718                 )
1719             if not (
1720                 connection.features.supports_covering_indexes or
1721                 'supports_covering_indexes' in cls._meta.required_db_features
1722             ) and any(index.include for index in cls._meta.indexes):
1723                 errors.append(
1724                     checks.Warning(
1725                         '%s does not support indexes with non-key columns.'
1726                         % connection.display_name,
1727                         hint=(
1728                             "Non-key columns will be ignored. Silence this "
1729                             "warning if you don't care about it."
1730                         ),
1731                         obj=cls,
1732                         id='models.W040',
1733                     )
1734                 )
1735             if not (
1736                 connection.features.supports_expression_indexes or
1737                 'supports_expression_indexes' in cls._meta.required_db_features
1738             ) and any(index.contains_expressions for index in cls._meta.indexes):
1739                 errors.append(
1740                     checks.Warning(
1741                         '%s does not support indexes on expressions.'
1742                         % connection.display_name,
1743                         hint=(
1744                             "An index won't be created. Silence this warning "
1745                             "if you don't care about it."
1746                         ),
1747                         obj=cls,
1748                         id='models.W043',
1749                     )
1750                 )
1751         fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]
1752         fields += [include for index in cls._meta.indexes for include in index.include]
1753         fields += references
1754         errors.extend(cls._check_local_fields(fields, 'indexes'))
1755         return errors
1756 
1757     @classmethod
1758     def _check_local_fields(cls, fields, option):
1759         from django.db import models
1760 
1761         # In order to avoid hitting the relation tree prematurely, we use our
1762         # own fields_map instead of using get_field()
1763         forward_fields_map = {}
1764         for field in cls._meta._get_fields(reverse=False):
1765             forward_fields_map[field.name] = field
1766             if hasattr(field, 'attname'):
1767                 forward_fields_map[field.attname] = field
1768 
1769         errors = []
1770         for field_name in fields:
1771             try:
1772                 field = forward_fields_map[field_name]
1773             except KeyError:
1774                 errors.append(
1775                     checks.Error(
1776                         "'%s' refers to the nonexistent field '%s'." % (
1777                             option, field_name,
1778                         ),
1779                         obj=cls,
1780                         id='models.E012',
1781                     )
1782                 )
1783             else:
1784                 if isinstance(field.remote_field, models.ManyToManyRel):
1785                     errors.append(
1786                         checks.Error(
1787                             "'%s' refers to a ManyToManyField '%s', but "
1788                             "ManyToManyFields are not permitted in '%s'." % (
1789                                 option, field_name, option,
1790                             ),
1791                             obj=cls,
1792                             id='models.E013',
1793                         )
1794                     )
1795                 elif field not in cls._meta.local_fields:
1796                     errors.append(
1797                         checks.Error(
1798                             "'%s' refers to field '%s' which is not local to model '%s'."
1799                             % (option, field_name, cls._meta.object_name),
1800                             hint="This issue may be caused by multi-table inheritance.",
1801                             obj=cls,
1802                             id='models.E016',
1803                         )
1804                     )
1805         return errors
1806 
1807     @classmethod
1808     def _check_ordering(cls):
1809         """
1810         Check "ordering" option -- is it a list of strings and do all fields
1811         exist?
1812         """
1813         if cls._meta._ordering_clash:
1814             return [
1815                 checks.Error(
1816                     "'ordering' and 'order_with_respect_to' cannot be used together.",
1817                     obj=cls,
1818                     id='models.E021',
1819                 ),
1820             ]
1821 
1822         if cls._meta.order_with_respect_to or not cls._meta.ordering:
1823             return []
1824 
1825         if not isinstance(cls._meta.ordering, (list, tuple)):
1826             return [
1827                 checks.Error(
1828                     "'ordering' must be a tuple or list (even if you want to order by only one field).",
1829                     obj=cls,
1830                     id='models.E014',
1831                 )
1832             ]
1833 
1834         errors = []
1835         fields = cls._meta.ordering
1836 
1837         # Skip expressions and '?' fields.
1838         fields = (f for f in fields if isinstance(f, str) and f != '?')
1839 
1840         # Convert "-field" to "field".
1841         fields = ((f[1:] if f.startswith('-') else f) for f in fields)
1842 
1843         # Separate related fields and non-related fields.
1844         _fields = []
1845         related_fields = []
1846         for f in fields:
1847             if LOOKUP_SEP in f:
1848                 related_fields.append(f)
1849             else:
1850                 _fields.append(f)
1851         fields = _fields
1852 
1853         # Check related fields.
1854         for field in related_fields:
1855             _cls = cls
1856             fld = None
1857             for part in field.split(LOOKUP_SEP):
1858                 try:
1859                     # pk is an alias that won't be found by opts.get_field.
1860                     if part == 'pk':
1861                         fld = _cls._meta.pk
1862                     else:
1863                         fld = _cls._meta.get_field(part)
1864                     if fld.is_relation:
1865                         _cls = fld.path_infos[-1].to_opts.model
1866                     else:
1867                         _cls = None
1868                 except (FieldDoesNotExist, AttributeError):
1869                     if fld is None or (
1870                         fld.get_transform(part) is None and fld.get_lookup(part) is None
1871                     ):
1872                         errors.append(
1873                             checks.Error(
1874                                 "'ordering' refers to the nonexistent field, "
1875                                 "related field, or lookup '%s'." % field,
1876                                 obj=cls,
1877                                 id='models.E015',
1878                             )
1879                         )
1880 
1881         # Skip ordering on pk. This is always a valid order_by field
1882         # but is an alias and therefore won't be found by opts.get_field.
1883         fields = {f for f in fields if f != 'pk'}
1884 
1885         # Check for invalid or nonexistent fields in ordering.
1886         invalid_fields = []
1887 
1888         # Any field name that is not present in field_names does not exist.
1889         # Also, ordering by m2m fields is not allowed.
1890         opts = cls._meta
1891         valid_fields = set(chain.from_iterable(
1892             (f.name, f.attname) if not (f.auto_created and not f.concrete) else (f.field.related_query_name(),)
1893             for f in chain(opts.fields, opts.related_objects)
1894         ))
1895 
1896         invalid_fields.extend(fields - valid_fields)
1897 
1898         for invalid_field in invalid_fields:
1899             errors.append(
1900                 checks.Error(
1901                     "'ordering' refers to the nonexistent field, related "
1902                     "field, or lookup '%s'." % invalid_field,
1903                     obj=cls,
1904                     id='models.E015',
1905                 )
1906             )
1907         return errors
1908 
1909     @classmethod
1910     def _check_long_column_names(cls, databases):
1911         """
1912         Check that any auto-generated column names are shorter than the limits
1913         for each database in which the model will be created.
1914         """
1915         if not databases:
1916             return []
1917         errors = []
1918         allowed_len = None
1919         db_alias = None
1920 
1921         # Find the minimum max allowed length among all specified db_aliases.
1922         for db in databases:
1923             # skip databases where the model won't be created
1924             if not router.allow_migrate_model(db, cls):
1925                 continue
1926             connection = connections[db]
1927             max_name_length = connection.ops.max_name_length()
1928             if max_name_length is None or connection.features.truncates_names:
1929                 continue
1930             else:
1931                 if allowed_len is None:
1932                     allowed_len = max_name_length
1933                     db_alias = db
1934                 elif max_name_length < allowed_len:
1935                     allowed_len = max_name_length
1936                     db_alias = db
1937 
1938         if allowed_len is None:
1939             return errors
1940 
1941         for f in cls._meta.local_fields:
1942             _, column_name = f.get_attname_column()
1943 
1944             # Check if auto-generated name for the field is too long
1945             # for the database.
1946             if f.db_column is None and column_name is not None and len(column_name) > allowed_len:
1947                 errors.append(
1948                     checks.Error(
1949                         'Autogenerated column name too long for field "%s". '
1950                         'Maximum length is "%s" for database "%s".'
1951                         % (column_name, allowed_len, db_alias),
1952                         hint="Set the column name manually using 'db_column'.",
1953                         obj=cls,
1954                         id='models.E018',
1955                     )
1956                 )
1957 
1958         for f in cls._meta.local_many_to_many:
1959             # Skip nonexistent models.
1960             if isinstance(f.remote_field.through, str):
1961                 continue
1962 
1963             # Check if auto-generated name for the M2M field is too long
1964             # for the database.
1965             for m2m in f.remote_field.through._meta.local_fields:
1966                 _, rel_name = m2m.get_attname_column()
1967                 if m2m.db_column is None and rel_name is not None and len(rel_name) > allowed_len:
1968                     errors.append(
1969                         checks.Error(
1970                             'Autogenerated column name too long for M2M field '
1971                             '"%s". Maximum length is "%s" for database "%s".'
1972                             % (rel_name, allowed_len, db_alias),
1973                             hint=(
1974                                 "Use 'through' to create a separate model for "
1975                                 "M2M and then set column_name using 'db_column'."
1976                             ),
1977                             obj=cls,
1978                             id='models.E019',
1979                         )
1980                     )
1981 
1982         return errors
1983 
1984     @classmethod
1985     def _get_expr_references(cls, expr):
1986         if isinstance(expr, Q):
1987             for child in expr.children:
1988                 if isinstance(child, tuple):
1989                     lookup, value = child
1990                     yield tuple(lookup.split(LOOKUP_SEP))
1991                     yield from cls._get_expr_references(value)
1992                 else:
1993                     yield from cls._get_expr_references(child)
1994         elif isinstance(expr, F):
1995             yield tuple(expr.name.split(LOOKUP_SEP))
1996         elif hasattr(expr, 'get_source_expressions'):
1997             for src_expr in expr.get_source_expressions():
1998                 yield from cls._get_expr_references(src_expr)
1999 
2000     @classmethod
2001     def _check_constraints(cls, databases):
2002         errors = []
2003         for db in databases:
2004             if not router.allow_migrate_model(db, cls):
2005                 continue
2006             connection = connections[db]
2007             if not (
2008                 connection.features.supports_table_check_constraints or
2009                 'supports_table_check_constraints' in cls._meta.required_db_features
2010             ) and any(
2011                 isinstance(constraint, CheckConstraint)
2012                 for constraint in cls._meta.constraints
2013             ):
2014                 errors.append(
2015                     checks.Warning(
2016                         '%s does not support check constraints.' % connection.display_name,
2017                         hint=(
2018                             "A constraint won't be created. Silence this "
2019                             "warning if you don't care about it."
2020                         ),
2021                         obj=cls,
2022                         id='models.W027',
2023                     )
2024                 )
2025             if not (
2026                 connection.features.supports_partial_indexes or
2027                 'supports_partial_indexes' in cls._meta.required_db_features
2028             ) and any(
2029                 isinstance(constraint, UniqueConstraint) and constraint.condition is not None
2030                 for constraint in cls._meta.constraints
2031             ):
2032                 errors.append(
2033                     checks.Warning(
2034                         '%s does not support unique constraints with '
2035                         'conditions.' % connection.display_name,
2036                         hint=(
2037                             "A constraint won't be created. Silence this "
2038                             "warning if you don't care about it."
2039                         ),
2040                         obj=cls,
2041                         id='models.W036',
2042                     )
2043                 )
2044             if not (
2045                 connection.features.supports_deferrable_unique_constraints or
2046                 'supports_deferrable_unique_constraints' in cls._meta.required_db_features
2047             ) and any(
2048                 isinstance(constraint, UniqueConstraint) and constraint.deferrable is not None
2049                 for constraint in cls._meta.constraints
2050             ):
2051                 errors.append(
2052                     checks.Warning(
2053                         '%s does not support deferrable unique constraints.'
2054                         % connection.display_name,
2055                         hint=(
2056                             "A constraint won't be created. Silence this "
2057                             "warning if you don't care about it."
2058                         ),
2059                         obj=cls,
2060                         id='models.W038',
2061                     )
2062                 )
2063             if not (
2064                 connection.features.supports_covering_indexes or
2065                 'supports_covering_indexes' in cls._meta.required_db_features
2066             ) and any(
2067                 isinstance(constraint, UniqueConstraint) and constraint.include
2068                 for constraint in cls._meta.constraints
2069             ):
2070                 errors.append(
2071                     checks.Warning(
2072                         '%s does not support unique constraints with non-key '
2073                         'columns.' % connection.display_name,
2074                         hint=(
2075                             "A constraint won't be created. Silence this "
2076                             "warning if you don't care about it."
2077                         ),
2078                         obj=cls,
2079                         id='models.W039',
2080                     )
2081                 )
2082             if not (
2083                 connection.features.supports_expression_indexes or
2084                 'supports_expression_indexes' in cls._meta.required_db_features
2085             ) and any(
2086                 isinstance(constraint, UniqueConstraint) and constraint.contains_expressions
2087                 for constraint in cls._meta.constraints
2088             ):
2089                 errors.append(
2090                     checks.Warning(
2091                         '%s does not support unique constraints on '
2092                         'expressions.' % connection.display_name,
2093                         hint=(
2094                             "A constraint won't be created. Silence this "
2095                             "warning if you don't care about it."
2096                         ),
2097                         obj=cls,
2098                         id='models.W044',
2099                     )
2100                 )
2101             fields = set(chain.from_iterable(
2102                 (*constraint.fields, *constraint.include)
2103                 for constraint in cls._meta.constraints if isinstance(constraint, UniqueConstraint)
2104             ))
2105             references = set()
2106             for constraint in cls._meta.constraints:
2107                 if isinstance(constraint, UniqueConstraint):
2108                     if (
2109                         connection.features.supports_partial_indexes or
2110                         'supports_partial_indexes' not in cls._meta.required_db_features
2111                     ) and isinstance(constraint.condition, Q):
2112                         references.update(cls._get_expr_references(constraint.condition))
2113                     if (
2114                         connection.features.supports_expression_indexes or
2115                         'supports_expression_indexes' not in cls._meta.required_db_features
2116                     ) and constraint.contains_expressions:
2117                         for expression in constraint.expressions:
2118                             references.update(cls._get_expr_references(expression))
2119                 elif isinstance(constraint, CheckConstraint):
2120                     if (
2121                         connection.features.supports_table_check_constraints or
2122                         'supports_table_check_constraints' not in cls._meta.required_db_features
2123                     ) and isinstance(constraint.check, Q):
2124                         references.update(cls._get_expr_references(constraint.check))
2125             for field_name, *lookups in references:
2126                 # pk is an alias that won't be found by opts.get_field.
2127                 if field_name != 'pk':
2128                     fields.add(field_name)
2129                 if not lookups:
2130                     # If it has no lookups it cannot result in a JOIN.
2131                     continue
2132                 try:
2133                     if field_name == 'pk':
2134                         field = cls._meta.pk
2135                     else:
2136                         field = cls._meta.get_field(field_name)
2137                     if not field.is_relation or field.many_to_many or field.one_to_many:
2138                         continue
2139                 except FieldDoesNotExist:
2140                     continue
2141                 # JOIN must happen at the first lookup.
2142                 first_lookup = lookups[0]
2143                 if (
2144                     hasattr(field, 'get_transform') and
2145                     hasattr(field, 'get_lookup') and
2146                     field.get_transform(first_lookup) is None and
2147                     field.get_lookup(first_lookup) is None
2148                 ):
2149                     errors.append(
2150                         checks.Error(
2151                             "'constraints' refers to the joined field '%s'."
2152                             % LOOKUP_SEP.join([field_name] + lookups),
2153                             obj=cls,
2154                             id='models.E041',
2155                         )
2156                     )
2157             errors.extend(cls._check_local_fields(fields, 'constraints'))
2158         return errors

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>delete</method>
<code>
734     def delete(self):
735         """Delete the records in the current QuerySet."""
736         self._not_support_combined_queries('delete')
737         if self.query.is_sliced:
738             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
739         if self.query.distinct or self.query.distinct_fields:
740             raise TypeError('Cannot call delete() after .distinct().')
741         if self._fields is not None:
742             raise TypeError("Cannot call delete() after .values() or .values_list()")
743 
744         del_query = self._chain()
745 
746         # The delete is actually 2 queries - one to find related objects,
747         # and one to delete. Make sure that the discovery of related
748         # objects is performed on the same database as the deletion.
749         del_query._for_write = True
750 
751         # Disable non-supported fields.
752         del_query.query.select_for_update = False
753         del_query.query.select_related = False
754         del_query.query.clear_ordering(force=True)
755 
756         collector = Collector(using=del_query.db)
757         collector.collect(del_query)
758         deleted, _rows_count = collector.delete()
759 
760         # Clear the result cache, in case this QuerySet gets reused.
761         self._result_cache = None
762         return deleted, _rows_count

</code><intended_behavior>Similarly, this method needs to be adapted to include context information about the deletion's origin as coming from a queryset operation. Given that `QuerySet.delete()` can affect numerous records, including a way to specify and pass along this information to the delete signals is crucial. Modifications to this method must ensure that any signals sent out as part of the deletion process carry this additional context. As with model instance deletions, it's essential to implement these changes in a way that doesn't disrupt existing usages of the method.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         if (
295             (isinstance(k, int) and k < 0) or
296             (isinstance(k, slice) and (
297                 (k.start is not None and k.start < 0) or
298                 (k.stop is not None and k.stop < 0)
299             ))
300         ):
301             raise ValueError('Negative indexing is not supported.')
302 
303         if self._result_cache is not None:
304             return self._result_cache[k]
305 
306         if isinstance(k, slice):
307             qs = self._chain()
308             if k.start is not None:
309                 start = int(k.start)
310             else:
311                 start = None
312             if k.stop is not None:
313                 stop = int(k.stop)
314             else:
315                 stop = None
316             qs.query.set_limits(start, stop)
317             return list(qs)[::k.step] if k.step else qs
318 
319         qs = self._chain()
320         qs.query.set_limits(k, k + 1)
321         qs._fetch_all()
322         return qs._result_cache[0]
323 
324     def __class_getitem__(cls, *args, **kwargs):
325         return cls
326 
327     def __and__(self, other):
328         self._check_operator_queryset(other, '&')
329         self._merge_sanity_check(other)
330         if isinstance(other, EmptyQuerySet):
331             return other
332         if isinstance(self, EmptyQuerySet):
333             return self
334         combined = self._chain()
335         combined._merge_known_related_objects(other)
336         combined.query.combine(other.query, sql.AND)
337         return combined
338 
339     def __or__(self, other):
340         self._check_operator_queryset(other, '|')
341         self._merge_sanity_check(other)
342         if isinstance(self, EmptyQuerySet):
343             return other
344         if isinstance(other, EmptyQuerySet):
345             return self
346         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
347         combined = query._chain()
348         combined._merge_known_related_objects(other)
349         if not other.query.can_filter():
350             other = other.model._base_manager.filter(pk__in=other.values('pk'))
351         combined.query.combine(other.query, sql.OR)
352         return combined
353 
354     ####################################
355     # METHODS THAT DO DATABASE QUERIES #
356     ####################################
357 
358     def _iterator(self, use_chunked_fetch, chunk_size):
359         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
360 
361     def iterator(self, chunk_size=2000):
362         """
363         An iterator over the results from applying this QuerySet to the
364         database.
365         """
366         if chunk_size <= 0:
367             raise ValueError('Chunk size must be strictly positive.')
368         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
369         return self._iterator(use_chunked_fetch, chunk_size)
370 
371     def aggregate(self, *args, **kwargs):
372         """
373         Return a dictionary containing the calculations (aggregation)
374         over the current queryset.
375 
376         If args is present the expression is passed as a kwarg using
377         the Aggregate object's default alias.
378         """
379         if self.query.distinct_fields:
380             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
381         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
382         for arg in args:
383             # The default_alias property raises TypeError if default_alias
384             # can't be set automatically or AttributeError if it isn't an
385             # attribute.
386             try:
387                 arg.default_alias
388             except (AttributeError, TypeError):
389                 raise TypeError("Complex aggregates require an alias")
390             kwargs[arg.default_alias] = arg
391 
392         query = self.query.chain()
393         for (alias, aggregate_expr) in kwargs.items():
394             query.add_annotation(aggregate_expr, alias, is_summary=True)
395             annotation = query.annotations[alias]
396             if not annotation.contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398             for expr in annotation.get_source_expressions():
399                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
400                     name = expr.refs
401                     raise exceptions.FieldError(
402                         "Cannot compute %s('%s'): '%s' is an aggregate"
403                         % (annotation.name, name, name)
404                     )
405         return query.get_aggregation(self.db, kwargs)
406 
407     def count(self):
408         """
409         Perform a SELECT COUNT() and return the number of records as an
410         integer.
411 
412         If the QuerySet is already fully cached, return the length of the
413         cached results set to avoid multiple SELECT COUNT(*) calls.
414         """
415         if self._result_cache is not None:
416             return len(self._result_cache)
417 
418         return self.query.get_count(using=self.db)
419 
420     def get(self, *args, **kwargs):
421         """
422         Perform the query and return a single object matching the given
423         keyword arguments.
424         """
425         if self.query.combinator and (args or kwargs):
426             raise NotSupportedError(
427                 'Calling QuerySet.get(...) with filters after %s() is not '
428                 'supported.' % self.query.combinator
429             )
430         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
431         if self.query.can_filter() and not self.query.distinct_fields:
432             clone = clone.order_by()
433         limit = None
434         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
435             limit = MAX_GET_RESULTS
436             clone.query.set_limits(high=limit)
437         num = len(clone)
438         if num == 1:
439             return clone._result_cache[0]
440         if not num:
441             raise self.model.DoesNotExist(
442                 "%s matching query does not exist." %
443                 self.model._meta.object_name
444             )
445         raise self.model.MultipleObjectsReturned(
446             'get() returned more than one %s -- it returned %s!' % (
447                 self.model._meta.object_name,
448                 num if not limit or num < limit else 'more than %s' % (limit - 1),
449             )
450         )
451 
452     def create(self, **kwargs):
453         """
454         Create a new object with the given kwargs, saving it to the database
455         and returning the created object.
456         """
457         obj = self.model(**kwargs)
458         self._for_write = True
459         obj.save(force_insert=True, using=self.db)
460         return obj
461 
462     def _prepare_for_bulk_create(self, objs):
463         for obj in objs:
464             if obj.pk is None:
465                 # Populate new PK values.
466                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
467             obj._prepare_related_fields_for_save(operation_name='bulk_create')
468 
469     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
470         """
471         Insert each of the instances into the database. Do *not* call
472         save() on each of the instances, do not send any pre/post_save
473         signals, and do not set the primary key attribute if it is an
474         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
475         Multi-table models are not supported.
476         """
477         # When you bulk insert you don't get the primary keys back (if it's an
478         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
479         # you can't insert into the child tables which references this. There
480         # are two workarounds:
481         # 1) This could be implemented if you didn't have an autoincrement pk
482         # 2) You could do it by doing O(n) normal inserts into the parent
483         #    tables to get the primary keys back and then doing a single bulk
484         #    insert into the childmost table.
485         # We currently set the primary keys on the objects when using
486         # PostgreSQL via the RETURNING ID clause. It should be possible for
487         # Oracle as well, but the semantics for extracting the primary keys is
488         # trickier so it's not done yet.
489         if batch_size is not None and batch_size <= 0:
490             raise ValueError('Batch size must be a positive integer.')
491         # Check that the parents share the same concrete model with the our
492         # model to detect the inheritance pattern ConcreteGrandParent ->
493         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
494         # would not identify that case as involving multiple tables.
495         for parent in self.model._meta.get_parent_list():
496             if parent._meta.concrete_model is not self.model._meta.concrete_model:
497                 raise ValueError("Can't bulk create a multi-table inherited model")
498         if not objs:
499             return objs
500         self._for_write = True
501         opts = self.model._meta
502         fields = opts.concrete_fields
503         objs = list(objs)
504         self._prepare_for_bulk_create(objs)
505         with transaction.atomic(using=self.db, savepoint=False):
506             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
507             if objs_with_pk:
508                 returned_columns = self._batched_insert(
509                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
510                 )
511                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         if field != opts.pk:
514                             setattr(obj_with_pk, field.attname, result)
515                 for obj_with_pk in objs_with_pk:
516                     obj_with_pk._state.adding = False
517                     obj_with_pk._state.db = self.db
518             if objs_without_pk:
519                 fields = [f for f in fields if not isinstance(f, AutoField)]
520                 returned_columns = self._batched_insert(
521                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
522                 )
523                 connection = connections[self.db]
524                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
525                     assert len(returned_columns) == len(objs_without_pk)
526                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
527                     for result, field in zip(results, opts.db_returning_fields):
528                         setattr(obj_without_pk, field.attname, result)
529                     obj_without_pk._state.adding = False
530                     obj_without_pk._state.db = self.db
531 
532         return objs
533 
534     def bulk_update(self, objs, fields, batch_size=None):
535         """
536         Update the given fields in each of the given objects in the database.
537         """
538         if batch_size is not None and batch_size < 0:
539             raise ValueError('Batch size must be a positive integer.')
540         if not fields:
541             raise ValueError('Field names must be given to bulk_update().')
542         objs = tuple(objs)
543         if any(obj.pk is None for obj in objs):
544             raise ValueError('All bulk_update() objects must have a primary key set.')
545         fields = [self.model._meta.get_field(name) for name in fields]
546         if any(not f.concrete or f.many_to_many for f in fields):
547             raise ValueError('bulk_update() can only be used with concrete fields.')
548         if any(f.primary_key for f in fields):
549             raise ValueError('bulk_update() cannot be used with primary key fields.')
550         if not objs:
551             return 0
552         for obj in objs:
553             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)
554         # PK is used twice in the resulting update query, once in the filter
555         # and once in the WHEN. Each field will also have one CAST.
556         connection = connections[self.db]
557         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
558         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
559         requires_casting = connection.features.requires_casted_case_in_updates
560         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
561         updates = []
562         for batch_objs in batches:
563             update_kwargs = {}
564             for field in fields:
565                 when_statements = []
566                 for obj in batch_objs:
567                     attr = getattr(obj, field.attname)
568                     if not isinstance(attr, Expression):
569                         attr = Value(attr, output_field=field)
570                     when_statements.append(When(pk=obj.pk, then=attr))
571                 case_statement = Case(*when_statements, output_field=field)
572                 if requires_casting:
573                     case_statement = Cast(case_statement, output_field=field)
574                 update_kwargs[field.attname] = case_statement
575             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
576         rows_updated = 0
577         with transaction.atomic(using=self.db, savepoint=False):
578             for pks, update_kwargs in updates:
579                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
580         return rows_updated
581     bulk_update.alters_data = True
582 
583     def get_or_create(self, defaults=None, **kwargs):
584         """
585         Look up an object with the given kwargs, creating one if necessary.
586         Return a tuple of (object, created), where created is a boolean
587         specifying whether an object was created.
588         """
589         # The get() needs to be targeted at the write database in order
590         # to avoid potential transaction consistency problems.
591         self._for_write = True
592         try:
593             return self.get(**kwargs), False
594         except self.model.DoesNotExist:
595             params = self._extract_model_params(defaults, **kwargs)
596             # Try to create an object using passed params.
597             try:
598                 with transaction.atomic(using=self.db):
599                     params = dict(resolve_callables(params))
600                     return self.create(**params), True
601             except IntegrityError:
602                 try:
603                     return self.get(**kwargs), False
604                 except self.model.DoesNotExist:
605                     pass
606                 raise
607 
608     def update_or_create(self, defaults=None, **kwargs):
609         """
610         Look up an object with the given kwargs, updating one with defaults
611         if it exists, otherwise create a new one.
612         Return a tuple (object, created), where created is a boolean
613         specifying whether an object was created.
614         """
615         defaults = defaults or {}
616         self._for_write = True
617         with transaction.atomic(using=self.db):
618             # Lock the row so that a concurrent update is blocked until
619             # update_or_create() has performed its save.
620             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
621             if created:
622                 return obj, created
623             for k, v in resolve_callables(defaults):
624                 setattr(obj, k, v)
625             obj.save(using=self.db)
626         return obj, False
627 
628     def _extract_model_params(self, defaults, **kwargs):
629         """
630         Prepare `params` for creating a model instance based on the given
631         kwargs; for use by get_or_create().
632         """
633         defaults = defaults or {}
634         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
635         params.update(defaults)
636         property_names = self.model._meta._property_names
637         invalid_params = []
638         for param in params:
639             try:
640                 self.model._meta.get_field(param)
641             except exceptions.FieldDoesNotExist:
642                 # It's okay to use a model's property if it has a setter.
643                 if not (param in property_names and getattr(self.model, param).fset):
644                     invalid_params.append(param)
645         if invalid_params:
646             raise exceptions.FieldError(
647                 "Invalid field name(s) for model %s: '%s'." % (
648                     self.model._meta.object_name,
649                     "', '".join(sorted(invalid_params)),
650                 ))
651         return params
652 
653     def _earliest(self, *fields):
654         """
655         Return the earliest object according to fields (if given) or by the
656         model's Meta.get_latest_by.
657         """
658         if fields:
659             order_by = fields
660         else:
661             order_by = getattr(self.model._meta, 'get_latest_by')
662             if order_by and not isinstance(order_by, (tuple, list)):
663                 order_by = (order_by,)
664         if order_by is None:
665             raise ValueError(
666                 "earliest() and latest() require either fields as positional "
667                 "arguments or 'get_latest_by' in the model's Meta."
668             )
669         obj = self._chain()
670         obj.query.set_limits(high=1)
671         obj.query.clear_ordering(force=True)
672         obj.query.add_ordering(*order_by)
673         return obj.get()
674 
675     def earliest(self, *fields):
676         if self.query.is_sliced:
677             raise TypeError('Cannot change a query once a slice has been taken.')
678         return self._earliest(*fields)
679 
680     def latest(self, *fields):
681         if self.query.is_sliced:
682             raise TypeError('Cannot change a query once a slice has been taken.')
683         return self.reverse()._earliest(*fields)
684 
685     def first(self):
686         """Return the first object of a query or None if no match is found."""
687         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
688             return obj
689 
690     def last(self):
691         """Return the last object of a query or None if no match is found."""
692         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
693             return obj
694 
695     def in_bulk(self, id_list=None, *, field_name='pk'):
696         """
697         Return a dictionary mapping each of the given IDs to the object with
698         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
699         """
700         if self.query.is_sliced:
701             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
702         opts = self.model._meta
703         unique_fields = [
704             constraint.fields[0]
705             for constraint in opts.total_unique_constraints
706             if len(constraint.fields) == 1
707         ]
708         if (
709             field_name != 'pk' and
710             not opts.get_field(field_name).unique and
711             field_name not in unique_fields and
712             self.query.distinct_fields != (field_name,)
713         ):
714             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
715         if id_list is not None:
716             if not id_list:
717                 return {}
718             filter_key = '{}__in'.format(field_name)
719             batch_size = connections[self.db].features.max_query_params
720             id_list = tuple(id_list)
721             # If the database has a limit on the number of query parameters
722             # (e.g. SQLite), retrieve objects in batches if necessary.
723             if batch_size and batch_size < len(id_list):
724                 qs = ()
725                 for offset in range(0, len(id_list), batch_size):
726                     batch = id_list[offset:offset + batch_size]
727                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
728             else:
729                 qs = self.filter(**{filter_key: id_list}).order_by()
730         else:
731             qs = self._chain()
732         return {getattr(obj, field_name): obj for obj in qs}
733 
734     def delete(self):
735         """Delete the records in the current QuerySet."""
736         self._not_support_combined_queries('delete')
737         if self.query.is_sliced:
738             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
739         if self.query.distinct or self.query.distinct_fields:
740             raise TypeError('Cannot call delete() after .distinct().')
741         if self._fields is not None:
742             raise TypeError("Cannot call delete() after .values() or .values_list()")
743 
744         del_query = self._chain()
745 
746         # The delete is actually 2 queries - one to find related objects,
747         # and one to delete. Make sure that the discovery of related
748         # objects is performed on the same database as the deletion.
749         del_query._for_write = True
750 
751         # Disable non-supported fields.
752         del_query.query.select_for_update = False
753         del_query.query.select_related = False
754         del_query.query.clear_ordering(force=True)
755 
756         collector = Collector(using=del_query.db)
757         collector.collect(del_query)
758         deleted, _rows_count = collector.delete()
759 
760         # Clear the result cache, in case this QuerySet gets reused.
761         self._result_cache = None
762         return deleted, _rows_count
763 
764     delete.alters_data = True
765     delete.queryset_only = True
766 
767     def _raw_delete(self, using):
768         """
769         Delete objects found from the given queryset in single direct SQL
770         query. No signals are sent and there is no protection for cascades.
771         """
772         query = self.query.clone()
773         query.__class__ = sql.DeleteQuery
774         cursor = query.get_compiler(using).execute_sql(CURSOR)
775         if cursor:
776             with cursor:
777                 return cursor.rowcount
778         return 0
779     _raw_delete.alters_data = True
780 
781     def update(self, **kwargs):
782         """
783         Update all elements in the current QuerySet, setting all the given
784         fields to the appropriate values.
785         """
786         self._not_support_combined_queries('update')
787         if self.query.is_sliced:
788             raise TypeError('Cannot update a query once a slice has been taken.')
789         self._for_write = True
790         query = self.query.chain(sql.UpdateQuery)
791         query.add_update_values(kwargs)
792         # Clear any annotations so that they won't be present in subqueries.
793         query.annotations = {}
794         with transaction.mark_for_rollback_on_error(using=self.db):
795             rows = query.get_compiler(self.db).execute_sql(CURSOR)
796         self._result_cache = None
797         return rows
798     update.alters_data = True
799 
800     def _update(self, values):
801         """
802         A version of update() that accepts field objects instead of field names.
803         Used primarily for model saving and not intended for use by general
804         code (it requires too much poking around at model internals to be
805         useful at that level).
806         """
807         if self.query.is_sliced:
808             raise TypeError('Cannot update a query once a slice has been taken.')
809         query = self.query.chain(sql.UpdateQuery)
810         query.add_update_fields(values)
811         # Clear any annotations so that they won't be present in subqueries.
812         query.annotations = {}
813         self._result_cache = None
814         return query.get_compiler(self.db).execute_sql(CURSOR)
815     _update.alters_data = True
816     _update.queryset_only = False
817 
818     def exists(self):
819         if self._result_cache is None:
820             return self.query.has_results(using=self.db)
821         return bool(self._result_cache)
822 
823     def contains(self, obj):
824         """Return True if the queryset contains an object."""
825         self._not_support_combined_queries('contains')
826         if self._fields is not None:
827             raise TypeError(
828                 'Cannot call QuerySet.contains() after .values() or '
829                 '.values_list().'
830             )
831         try:
832             if obj._meta.concrete_model != self.model._meta.concrete_model:
833                 return False
834         except AttributeError:
835             raise TypeError("'obj' must be a model instance.")
836         if obj.pk is None:
837             raise ValueError(
838                 'QuerySet.contains() cannot be used on unsaved objects.'
839             )
840         if self._result_cache is not None:
841             return obj in self._result_cache
842         return self.filter(pk=obj.pk).exists()
843 
844     def _prefetch_related_objects(self):
845         # This method can only be called once the result cache has been filled.
846         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
847         self._prefetch_done = True
848 
849     def explain(self, *, format=None, **options):
850         return self.query.explain(using=self.db, format=format, **options)
851 
852     ##################################################
853     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
854     ##################################################
855 
856     def raw(self, raw_query, params=(), translations=None, using=None):
857         if using is None:
858             using = self.db
859         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
860         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
861         return qs
862 
863     def _values(self, *fields, **expressions):
864         clone = self._chain()
865         if expressions:
866             clone = clone.annotate(**expressions)
867         clone._fields = fields
868         clone.query.set_values(fields)
869         return clone
870 
871     def values(self, *fields, **expressions):
872         fields += tuple(expressions)
873         clone = self._values(*fields, **expressions)
874         clone._iterable_class = ValuesIterable
875         return clone
876 
877     def values_list(self, *fields, flat=False, named=False):
878         if flat and named:
879             raise TypeError("'flat' and 'named' can't be used together.")
880         if flat and len(fields) > 1:
881             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
882 
883         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
884         _fields = []
885         expressions = {}
886         counter = 1
887         for field in fields:
888             if hasattr(field, 'resolve_expression'):
889                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
890                 while True:
891                     field_id = field_id_prefix + str(counter)
892                     counter += 1
893                     if field_id not in field_names:
894                         break
895                 expressions[field_id] = field
896                 _fields.append(field_id)
897             else:
898                 _fields.append(field)
899 
900         clone = self._values(*_fields, **expressions)
901         clone._iterable_class = (
902             NamedValuesListIterable if named
903             else FlatValuesListIterable if flat
904             else ValuesListIterable
905         )
906         return clone
907 
908     def dates(self, field_name, kind, order='ASC'):
909         """
910         Return a list of date objects representing all available dates for
911         the given field_name, scoped to 'kind'.
912         """
913         if kind not in ('year', 'month', 'week', 'day'):
914             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
915         if order not in ('ASC', 'DESC'):
916             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
917         return self.annotate(
918             datefield=Trunc(field_name, kind, output_field=DateField()),
919             plain_field=F(field_name)
920         ).values_list(
921             'datefield', flat=True
922         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
923 
924     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst
925     # argument.
926     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):
927         """
928         Return a list of datetime objects representing all available
929         datetimes for the given field_name, scoped to 'kind'.
930         """
931         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
932             raise ValueError(
933                 "'kind' must be one of 'year', 'month', 'week', 'day', "
934                 "'hour', 'minute', or 'second'."
935             )
936         if order not in ('ASC', 'DESC'):
937             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
938         if settings.USE_TZ:
939             if tzinfo is None:
940                 tzinfo = timezone.get_current_timezone()
941         else:
942             tzinfo = None
943         return self.annotate(
944             datetimefield=Trunc(
945                 field_name,
946                 kind,
947                 output_field=DateTimeField(),
948                 tzinfo=tzinfo,
949                 is_dst=is_dst,
950             ),
951             plain_field=F(field_name)
952         ).values_list(
953             'datetimefield', flat=True
954         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
955 
956     def none(self):
957         """Return an empty QuerySet."""
958         clone = self._chain()
959         clone.query.set_empty()
960         return clone
961 
962     ##################################################################
963     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
964     ##################################################################
965 
966     def all(self):
967         """
968         Return a new QuerySet that is a copy of the current one. This allows a
969         QuerySet to proxy for a model manager in some cases.
970         """
971         return self._chain()
972 
973     def filter(self, *args, **kwargs):
974         """
975         Return a new QuerySet instance with the args ANDed to the existing
976         set.
977         """
978         self._not_support_combined_queries('filter')
979         return self._filter_or_exclude(False, args, kwargs)
980 
981     def exclude(self, *args, **kwargs):
982         """
983         Return a new QuerySet instance with NOT (args) ANDed to the existing
984         set.
985         """
986         self._not_support_combined_queries('exclude')
987         return self._filter_or_exclude(True, args, kwargs)
988 
989     def _filter_or_exclude(self, negate, args, kwargs):
990         if (args or kwargs) and self.query.is_sliced:
991             raise TypeError('Cannot filter a query once a slice has been taken.')
992         clone = self._chain()
993         if self._defer_next_filter:
994             self._defer_next_filter = False
995             clone._deferred_filter = negate, args, kwargs
996         else:
997             clone._filter_or_exclude_inplace(negate, args, kwargs)
998         return clone
999 
1000     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1001         if negate:
1002             self._query.add_q(~Q(*args, **kwargs))
1003         else:
1004             self._query.add_q(Q(*args, **kwargs))
1005 
1006     def complex_filter(self, filter_obj):
1007         """
1008         Return a new QuerySet instance with filter_obj added to the filters.
1009 
1010         filter_obj can be a Q object or a dictionary of keyword lookup
1011         arguments.
1012 
1013         This exists to support framework features such as 'limit_choices_to',
1014         and usually it will be more natural to use other methods.
1015         """
1016         if isinstance(filter_obj, Q):
1017             clone = self._chain()
1018             clone.query.add_q(filter_obj)
1019             return clone
1020         else:
1021             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1022 
1023     def _combinator_query(self, combinator, *other_qs, all=False):
1024         # Clone the query to inherit the select list and everything
1025         clone = self._chain()
1026         # Clear limits and ordering so they can be reapplied
1027         clone.query.clear_ordering(force=True)
1028         clone.query.clear_limits()
1029         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1030         clone.query.combinator = combinator
1031         clone.query.combinator_all = all
1032         return clone
1033 
1034     def union(self, *other_qs, all=False):
1035         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1036         if isinstance(self, EmptyQuerySet):
1037             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1038             if not qs:
1039                 return self
1040             if len(qs) == 1:
1041                 return qs[0]
1042             return qs[0]._combinator_query('union', *qs[1:], all=all)
1043         return self._combinator_query('union', *other_qs, all=all)
1044 
1045     def intersection(self, *other_qs):
1046         # If any query is an EmptyQuerySet, return it.
1047         if isinstance(self, EmptyQuerySet):
1048             return self
1049         for other in other_qs:
1050             if isinstance(other, EmptyQuerySet):
1051                 return other
1052         return self._combinator_query('intersection', *other_qs)
1053 
1054     def difference(self, *other_qs):
1055         # If the query is an EmptyQuerySet, return it.
1056         if isinstance(self, EmptyQuerySet):
1057             return self
1058         return self._combinator_query('difference', *other_qs)
1059 
1060     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1061         """
1062         Return a new QuerySet instance that will select objects with a
1063         FOR UPDATE lock.
1064         """
1065         if nowait and skip_locked:
1066             raise ValueError('The nowait option cannot be used with skip_locked.')
1067         obj = self._chain()
1068         obj._for_write = True
1069         obj.query.select_for_update = True
1070         obj.query.select_for_update_nowait = nowait
1071         obj.query.select_for_update_skip_locked = skip_locked
1072         obj.query.select_for_update_of = of
1073         obj.query.select_for_no_key_update = no_key
1074         return obj
1075 
1076     def select_related(self, *fields):
1077         """
1078         Return a new QuerySet instance that will select related objects.
1079 
1080         If fields are specified, they must be ForeignKey fields and only those
1081         related objects are included in the selection.
1082 
1083         If select_related(None) is called, clear the list.
1084         """
1085         self._not_support_combined_queries('select_related')
1086         if self._fields is not None:
1087             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1088 
1089         obj = self._chain()
1090         if fields == (None,):
1091             obj.query.select_related = False
1092         elif fields:
1093             obj.query.add_select_related(fields)
1094         else:
1095             obj.query.select_related = True
1096         return obj
1097 
1098     def prefetch_related(self, *lookups):
1099         """
1100         Return a new QuerySet instance that will prefetch the specified
1101         Many-To-One and Many-To-Many related objects when the QuerySet is
1102         evaluated.
1103 
1104         When prefetch_related() is called more than once, append to the list of
1105         prefetch lookups. If prefetch_related(None) is called, clear the list.
1106         """
1107         self._not_support_combined_queries('prefetch_related')
1108         clone = self._chain()
1109         if lookups == (None,):
1110             clone._prefetch_related_lookups = ()
1111         else:
1112             for lookup in lookups:
1113                 if isinstance(lookup, Prefetch):
1114                     lookup = lookup.prefetch_to
1115                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1116                 if lookup in self.query._filtered_relations:
1117                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1118             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1119         return clone
1120 
1121     def annotate(self, *args, **kwargs):
1122         """
1123         Return a query set in which the returned objects have been annotated
1124         with extra data or aggregations.
1125         """
1126         self._not_support_combined_queries('annotate')
1127         return self._annotate(args, kwargs, select=True)
1128 
1129     def alias(self, *args, **kwargs):
1130         """
1131         Return a query set with added aliases for extra data or aggregations.
1132         """
1133         self._not_support_combined_queries('alias')
1134         return self._annotate(args, kwargs, select=False)
1135 
1136     def _annotate(self, args, kwargs, select=True):
1137         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1138         annotations = {}
1139         for arg in args:
1140             # The default_alias property may raise a TypeError.
1141             try:
1142                 if arg.default_alias in kwargs:
1143                     raise ValueError("The named annotation '%s' conflicts with the "
1144                                      "default name for another annotation."
1145                                      % arg.default_alias)
1146             except TypeError:
1147                 raise TypeError("Complex annotations require an alias")
1148             annotations[arg.default_alias] = arg
1149         annotations.update(kwargs)
1150 
1151         clone = self._chain()
1152         names = self._fields
1153         if names is None:
1154             names = set(chain.from_iterable(
1155                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1156                 for field in self.model._meta.get_fields()
1157             ))
1158 
1159         for alias, annotation in annotations.items():
1160             if alias in names:
1161                 raise ValueError("The annotation '%s' conflicts with a field on "
1162                                  "the model." % alias)
1163             if isinstance(annotation, FilteredRelation):
1164                 clone.query.add_filtered_relation(annotation, alias)
1165             else:
1166                 clone.query.add_annotation(
1167                     annotation, alias, is_summary=False, select=select,
1168                 )
1169         for alias, annotation in clone.query.annotations.items():
1170             if alias in annotations and annotation.contains_aggregate:
1171                 if clone._fields is None:
1172                     clone.query.group_by = True
1173                 else:
1174                     clone.query.set_group_by()
1175                 break
1176 
1177         return clone
1178 
1179     def order_by(self, *field_names):
1180         """Return a new QuerySet instance with the ordering changed."""
1181         if self.query.is_sliced:
1182             raise TypeError('Cannot reorder a query once a slice has been taken.')
1183         obj = self._chain()
1184         obj.query.clear_ordering(force=True, clear_default=False)
1185         obj.query.add_ordering(*field_names)
1186         return obj
1187 
1188     def distinct(self, *field_names):
1189         """
1190         Return a new QuerySet instance that will select only distinct results.
1191         """
1192         self._not_support_combined_queries('distinct')
1193         if self.query.is_sliced:
1194             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1195         obj = self._chain()
1196         obj.query.add_distinct_fields(*field_names)
1197         return obj
1198 
1199     def extra(self, select=None, where=None, params=None, tables=None,
1200               order_by=None, select_params=None):
1201         """Add extra SQL fragments to the query."""
1202         self._not_support_combined_queries('extra')
1203         if self.query.is_sliced:
1204             raise TypeError('Cannot change a query once a slice has been taken.')
1205         clone = self._chain()
1206         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1207         return clone
1208 
1209     def reverse(self):
1210         """Reverse the ordering of the QuerySet."""
1211         if self.query.is_sliced:
1212             raise TypeError('Cannot reverse a query once a slice has been taken.')
1213         clone = self._chain()
1214         clone.query.standard_ordering = not clone.query.standard_ordering
1215         return clone
1216 
1217     def defer(self, *fields):
1218         """
1219         Defer the loading of data for certain fields until they are accessed.
1220         Add the set of deferred fields to any existing set of deferred fields.
1221         The only exception to this is if None is passed in as the only
1222         parameter, in which case removal all deferrals.
1223         """
1224         self._not_support_combined_queries('defer')
1225         if self._fields is not None:
1226             raise TypeError("Cannot call defer() after .values() or .values_list()")
1227         clone = self._chain()
1228         if fields == (None,):
1229             clone.query.clear_deferred_loading()
1230         else:
1231             clone.query.add_deferred_loading(fields)
1232         return clone
1233 
1234     def only(self, *fields):
1235         """
1236         Essentially, the opposite of defer(). Only the fields passed into this
1237         method and that are not already specified as deferred are loaded
1238         immediately when the queryset is evaluated.
1239         """
1240         self._not_support_combined_queries('only')
1241         if self._fields is not None:
1242             raise TypeError("Cannot call only() after .values() or .values_list()")
1243         if fields == (None,):
1244             # Can only pass None to defer(), not only(), as the rest option.
1245             # That won't stop people trying to do this, so let's be explicit.
1246             raise TypeError("Cannot pass None as an argument to only().")
1247         for field in fields:
1248             field = field.split(LOOKUP_SEP, 1)[0]
1249             if field in self.query._filtered_relations:
1250                 raise ValueError('only() is not supported with FilteredRelation.')
1251         clone = self._chain()
1252         clone.query.add_immediate_loading(fields)
1253         return clone
1254 
1255     def using(self, alias):
1256         """Select which database this QuerySet should execute against."""
1257         clone = self._chain()
1258         clone._db = alias
1259         return clone
1260 
1261     ###################################
1262     # PUBLIC INTROSPECTION ATTRIBUTES #
1263     ###################################
1264 
1265     @property
1266     def ordered(self):
1267         """
1268         Return True if the QuerySet is ordered -- i.e. has an order_by()
1269         clause or a default ordering on the model (or is empty).
1270         """
1271         if isinstance(self, EmptyQuerySet):
1272             return True
1273         if self.query.extra_order_by or self.query.order_by:
1274             return True
1275         elif (
1276             self.query.default_ordering and
1277             self.query.get_meta().ordering and
1278             # A default ordering doesn't affect GROUP BY queries.
1279             not self.query.group_by
1280         ):
1281             return True
1282         else:
1283             return False
1284 
1285     @property
1286     def db(self):
1287         """Return the database used if this query is executed now."""
1288         if self._for_write:
1289             return self._db or router.db_for_write(self.model, **self._hints)
1290         return self._db or router.db_for_read(self.model, **self._hints)
1291 
1292     ###################
1293     # PRIVATE METHODS #
1294     ###################
1295 
1296     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1297         """
1298         Insert a new record for the given model. This provides an interface to
1299         the InsertQuery class and is how Model.save() is implemented.
1300         """
1301         self._for_write = True
1302         if using is None:
1303             using = self.db
1304         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1305         query.insert_values(fields, objs, raw=raw)
1306         return query.get_compiler(using=using).execute_sql(returning_fields)
1307     _insert.alters_data = True
1308     _insert.queryset_only = False
1309 
1310     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1311         """
1312         Helper method for bulk_create() to insert objs one batch at a time.
1313         """
1314         connection = connections[self.db]
1315         if ignore_conflicts and not connection.features.supports_ignore_conflicts:
1316             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1317         ops = connection.ops
1318         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1319         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1320         inserted_rows = []
1321         bulk_return = connection.features.can_return_rows_from_bulk_insert
1322         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1323             if bulk_return and not ignore_conflicts:
1324                 inserted_rows.extend(self._insert(
1325                     item, fields=fields, using=self.db,
1326                     returning_fields=self.model._meta.db_returning_fields,
1327                     ignore_conflicts=ignore_conflicts,
1328                 ))
1329             else:
1330                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1331         return inserted_rows
1332 
1333     def _chain(self):
1334         """
1335         Return a copy of the current QuerySet that's ready for another
1336         operation.
1337         """
1338         obj = self._clone()
1339         if obj._sticky_filter:
1340             obj.query.filter_is_sticky = True
1341             obj._sticky_filter = False
1342         return obj
1343 
1344     def _clone(self):
1345         """
1346         Return a copy of the current QuerySet. A lightweight alternative
1347         to deepcopy().
1348         """
1349         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1350         c._sticky_filter = self._sticky_filter
1351         c._for_write = self._for_write
1352         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1353         c._known_related_objects = self._known_related_objects
1354         c._iterable_class = self._iterable_class
1355         c._fields = self._fields
1356         return c
1357 
1358     def _fetch_all(self):
1359         if self._result_cache is None:
1360             self._result_cache = list(self._iterable_class(self))
1361         if self._prefetch_related_lookups and not self._prefetch_done:
1362             self._prefetch_related_objects()
1363 
1364     def _next_is_sticky(self):
1365         """
1366         Indicate that the next filter call and the one following that should
1367         be treated as a single filter. This is only important when it comes to
1368         determining when to reuse tables for many-to-many filters. Required so
1369         that we can filter naturally on the results of related managers.
1370 
1371         This doesn't return a clone of the current QuerySet (it returns
1372         "self"). The method is only used internally and should be immediately
1373         followed by a filter() that does create a clone.
1374         """
1375         self._sticky_filter = True
1376         return self
1377 
1378     def _merge_sanity_check(self, other):
1379         """Check that two QuerySet classes may be merged."""
1380         if self._fields is not None and (
1381                 set(self.query.values_select) != set(other.query.values_select) or
1382                 set(self.query.extra_select) != set(other.query.extra_select) or
1383                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1384             raise TypeError(
1385                 "Merging '%s' classes must involve the same values in each case."
1386                 % self.__class__.__name__
1387             )
1388 
1389     def _merge_known_related_objects(self, other):
1390         """
1391         Keep track of all known related objects from either QuerySet instance.
1392         """
1393         for field, objects in other._known_related_objects.items():
1394             self._known_related_objects.setdefault(field, {}).update(objects)
1395 
1396     def resolve_expression(self, *args, **kwargs):
1397         if self._fields and len(self._fields) > 1:
1398             # values() queryset can only be used as nested queries
1399             # if they are set up to select only a single field.
1400             raise TypeError('Cannot use multi-field values as a filter value.')
1401         query = self.query.resolve_expression(*args, **kwargs)
1402         query._db = self._db
1403         return query
1404     resolve_expression.queryset_only = True
1405 
1406     def _add_hints(self, **hints):
1407         """
1408         Update hinting information for use by routers. Add new key/values or
1409         overwrite existing key/values.
1410         """
1411         self._hints.update(hints)
1412 
1413     def _has_filters(self):
1414         """
1415         Check if this QuerySet has any filtering going on. This isn't
1416         equivalent with checking if all objects are present in results, for
1417         example, qs[1:]._has_filters() -> False.
1418         """
1419         return self.query.has_filters()
1420 
1421     @staticmethod
1422     def _validate_values_are_expressions(values, method_name):
1423         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1424         if invalid_args:
1425             raise TypeError(
1426                 'QuerySet.%s() received non-expression(s): %s.' % (
1427                     method_name,
1428                     ', '.join(invalid_args),
1429                 )
1430             )
1431 
1432     def _not_support_combined_queries(self, operation_name):
1433         if self.query.combinator:
1434             raise NotSupportedError(
1435                 'Calling QuerySet.%s() after %s() is not supported.'
1436                 % (operation_name, self.query.combinator)
1437             )
1438 
1439     def _check_operator_queryset(self, other, operator_):
1440         if self.query.combinator or other.query.combinator:
1441             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 16:58:09.627 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 16:58:09.627 | INFO     | Search-only: skipping patch selection
2025-10-15 16:58:09.781 | INFO     | Task django__django-15248 completed successfully.
2025-10-15 16:58:09.782 | INFO     | No patch generated. You can try running ACR again.
