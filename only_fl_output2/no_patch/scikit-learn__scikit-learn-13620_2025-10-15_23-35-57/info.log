2025-10-15 23:35:57.603 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 23:36:00.330 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=905, output_tokens=12, cost=0.009410
2025-10-15 23:36:15.252 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1085, output_tokens=640, cost=0.030050
2025-10-15 23:36:16.348 | INFO     | registered non reproducing test 0
2025-10-15 23:36:32.756 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2578, output_tokens=695, cost=0.046630
2025-10-15 23:36:33.810 | INFO     | registered non reproducing test 1
2025-10-15 23:36:49.240 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2607, output_tokens=727, cost=0.047880
2025-10-15 23:36:50.294 | INFO     | registered non reproducing test 2
2025-10-15 23:36:50.295 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 23:36:50.298 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 23:36:58.234 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1204, output_tokens=382, cost=0.023500
2025-10-15 23:36:58.242 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 23:37:01.160 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=736, output_tokens=100, cost=0.010360
2025-10-15 23:37:01.162 | DEBUG    | Extracted a valid json.
2025-10-15 23:37:01.162 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"GradientBoostingRegressor\")",
        "search_method_in_class(\"feature_importances_\", \"GradientBoostingRegressor\")",
        "search_class(\"AdaBoostRegressor\")",
        "search_method_in_class(\"feature_importances_\", \"AdaBoostRegressor\")",
        "search_code(\"estimators_\")",
        "search_code(\"np.sum(clf.feature_importances_)\")"
    ],
    "bug_locations": []
}
2025-10-15 23:37:01.347 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 23:37:01.347 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 23:37:22.398 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2590, output_tokens=903, cost=0.052990
2025-10-15 23:37:22.407 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 23:37:30.835 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3709, output_tokens=363, cost=0.047980
2025-10-15 23:37:30.843 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 23:37:35.636 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=717, output_tokens=190, cost=0.012870
2025-10-15 23:37:35.637 | DEBUG    | Extracted a valid json.
2025-10-15 23:37:35.637 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/ensemble/gradient_boosting.py", 
            "class": "GradientBoostingRegressor", 
            "method": "", 
            "intended_behavior": "The intended behavior within `GradientBoostingRegressor` is to ensure that the `feature_importances_` calculation accurately reflects the contributions of all trees within the ensemble, properly normalized to sum up to 1."
        },
        {
            "file": "sklearn/ensemble/weight_boosting.py", 
            "class": "AdaBoostRegressor", 
            "method": "", 
            "intended_behavior": "A preemptive review and potential revision of its `feature_importances_` calculation could be necessary to ensure consistency and reliability in how feature importances sum up to 1 across ensemble methods in `scikit-learn`."
        }
    ]
}
2025-10-15 23:37:35.644 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>GradientBoostingRegressor</class>
<method>None</method>
<code>
2233 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
2234     """Gradient Boosting for regression.
2235 
2236     GB builds an additive model in a forward stage-wise fashion;
2237     it allows for the optimization of arbitrary differentiable loss functions.
2238     In each stage a regression tree is fit on the negative gradient of the
2239     given loss function.
2240 
2241     Read more in the :ref:`User Guide <gradient_boosting>`.
2242 
2243     Parameters
2244     ----------
2245     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
2246         loss function to be optimized. 'ls' refers to least squares
2247         regression. 'lad' (least absolute deviation) is a highly robust
2248         loss function solely based on order information of the input
2249         variables. 'huber' is a combination of the two. 'quantile'
2250         allows quantile regression (use `alpha` to specify the quantile).
2251 
2252     learning_rate : float, optional (default=0.1)
2253         learning rate shrinks the contribution of each tree by `learning_rate`.
2254         There is a trade-off between learning_rate and n_estimators.
2255 
2256     n_estimators : int (default=100)
2257         The number of boosting stages to perform. Gradient boosting
2258         is fairly robust to over-fitting so a large number usually
2259         results in better performance.
2260 
2261     subsample : float, optional (default=1.0)
2262         The fraction of samples to be used for fitting the individual base
2263         learners. If smaller than 1.0 this results in Stochastic Gradient
2264         Boosting. `subsample` interacts with the parameter `n_estimators`.
2265         Choosing `subsample < 1.0` leads to a reduction of variance
2266         and an increase in bias.
2267 
2268     criterion : string, optional (default="friedman_mse")
2269         The function to measure the quality of a split. Supported criteria
2270         are "friedman_mse" for the mean squared error with improvement
2271         score by Friedman, "mse" for mean squared error, and "mae" for
2272         the mean absolute error. The default value of "friedman_mse" is
2273         generally the best as it can provide a better approximation in
2274         some cases.
2275 
2276         .. versionadded:: 0.18
2277 
2278     min_samples_split : int, float, optional (default=2)
2279         The minimum number of samples required to split an internal node:
2280 
2281         - If int, then consider `min_samples_split` as the minimum number.
2282         - If float, then `min_samples_split` is a fraction and
2283           `ceil(min_samples_split * n_samples)` are the minimum
2284           number of samples for each split.
2285 
2286         .. versionchanged:: 0.18
2287            Added float values for fractions.
2288 
2289     min_samples_leaf : int, float, optional (default=1)
2290         The minimum number of samples required to be at a leaf node.
2291         A split point at any depth will only be considered if it leaves at
2292         least ``min_samples_leaf`` training samples in each of the left and
2293         right branches.  This may have the effect of smoothing the model,
2294         especially in regression.
2295 
2296         - If int, then consider `min_samples_leaf` as the minimum number.
2297         - If float, then `min_samples_leaf` is a fraction and
2298           `ceil(min_samples_leaf * n_samples)` are the minimum
2299           number of samples for each node.
2300 
2301         .. versionchanged:: 0.18
2302            Added float values for fractions.
2303 
2304     min_weight_fraction_leaf : float, optional (default=0.)
2305         The minimum weighted fraction of the sum total of weights (of all
2306         the input samples) required to be at a leaf node. Samples have
2307         equal weight when sample_weight is not provided.
2308 
2309     max_depth : integer, optional (default=3)
2310         maximum depth of the individual regression estimators. The maximum
2311         depth limits the number of nodes in the tree. Tune this parameter
2312         for best performance; the best value depends on the interaction
2313         of the input variables.
2314 
2315     min_impurity_decrease : float, optional (default=0.)
2316         A node will be split if this split induces a decrease of the impurity
2317         greater than or equal to this value.
2318 
2319         The weighted impurity decrease equation is the following::
2320 
2321             N_t / N * (impurity - N_t_R / N_t * right_impurity
2322                                 - N_t_L / N_t * left_impurity)
2323 
2324         where ``N`` is the total number of samples, ``N_t`` is the number of
2325         samples at the current node, ``N_t_L`` is the number of samples in the
2326         left child, and ``N_t_R`` is the number of samples in the right child.
2327 
2328         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2329         if ``sample_weight`` is passed.
2330 
2331         .. versionadded:: 0.19
2332 
2333     min_impurity_split : float, (default=1e-7)
2334         Threshold for early stopping in tree growth. A node will split
2335         if its impurity is above the threshold, otherwise it is a leaf.
2336 
2337         .. deprecated:: 0.19
2338            ``min_impurity_split`` has been deprecated in favor of
2339            ``min_impurity_decrease`` in 0.19. The default value of
2340            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
2341            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
2342 
2343     init : estimator or 'zero', optional (default=None)
2344         An estimator object that is used to compute the initial predictions.
2345         ``init`` has to provide `fit` and `predict`. If 'zero', the initial
2346         raw predictions are set to zero. By default a ``DummyEstimator`` is
2347         used, predicting either the average target value (for loss='ls'), or
2348         a quantile for the other losses.
2349 
2350     random_state : int, RandomState instance or None, optional (default=None)
2351         If int, random_state is the seed used by the random number generator;
2352         If RandomState instance, random_state is the random number generator;
2353         If None, the random number generator is the RandomState instance used
2354         by `np.random`.
2355 
2356     max_features : int, float, string or None, optional (default=None)
2357         The number of features to consider when looking for the best split:
2358 
2359         - If int, then consider `max_features` features at each split.
2360         - If float, then `max_features` is a fraction and
2361           `int(max_features * n_features)` features are considered at each
2362           split.
2363         - If "auto", then `max_features=n_features`.
2364         - If "sqrt", then `max_features=sqrt(n_features)`.
2365         - If "log2", then `max_features=log2(n_features)`.
2366         - If None, then `max_features=n_features`.
2367 
2368         Choosing `max_features < n_features` leads to a reduction of variance
2369         and an increase in bias.
2370 
2371         Note: the search for a split does not stop until at least one
2372         valid partition of the node samples is found, even if it requires to
2373         effectively inspect more than ``max_features`` features.
2374 
2375     alpha : float (default=0.9)
2376         The alpha-quantile of the huber loss function and the quantile
2377         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
2378 
2379     verbose : int, default: 0
2380         Enable verbose output. If 1 then it prints progress and performance
2381         once in a while (the more trees the lower the frequency). If greater
2382         than 1 then it prints progress and performance for every tree.
2383 
2384     max_leaf_nodes : int or None, optional (default=None)
2385         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2386         Best nodes are defined as relative reduction in impurity.
2387         If None then unlimited number of leaf nodes.
2388 
2389     warm_start : bool, default: False
2390         When set to ``True``, reuse the solution of the previous call to fit
2391         and add more estimators to the ensemble, otherwise, just erase the
2392         previous solution. See :term:`the Glossary <warm_start>`.
2393 
2394     presort : bool or 'auto', optional (default='auto')
2395         Whether to presort the data to speed up the finding of best splits in
2396         fitting. Auto mode by default will use presorting on dense data and
2397         default to normal sorting on sparse data. Setting presort to true on
2398         sparse data will raise an error.
2399 
2400         .. versionadded:: 0.17
2401            optional parameter *presort*.
2402 
2403     validation_fraction : float, optional, default 0.1
2404         The proportion of training data to set aside as validation set for
2405         early stopping. Must be between 0 and 1.
2406         Only used if ``n_iter_no_change`` is set to an integer.
2407 
2408         .. versionadded:: 0.20
2409 
2410     n_iter_no_change : int, default None
2411         ``n_iter_no_change`` is used to decide if early stopping will be used
2412         to terminate training when validation score is not improving. By
2413         default it is set to None to disable early stopping. If set to a
2414         number, it will set aside ``validation_fraction`` size of the training
2415         data as validation and terminate training when validation score is not
2416         improving in all of the previous ``n_iter_no_change`` numbers of
2417         iterations.
2418 
2419         .. versionadded:: 0.20
2420 
2421     tol : float, optional, default 1e-4
2422         Tolerance for the early stopping. When the loss is not improving
2423         by at least tol for ``n_iter_no_change`` iterations (if set to a
2424         number), the training stops.
2425 
2426         .. versionadded:: 0.20
2427 
2428 
2429     Attributes
2430     ----------
2431     feature_importances_ : array, shape (n_features,)
2432         The feature importances (the higher, the more important the feature).
2433 
2434     oob_improvement_ : array, shape (n_estimators,)
2435         The improvement in loss (= deviance) on the out-of-bag samples
2436         relative to the previous iteration.
2437         ``oob_improvement_[0]`` is the improvement in
2438         loss of the first stage over the ``init`` estimator.
2439 
2440     train_score_ : array, shape (n_estimators,)
2441         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
2442         model at iteration ``i`` on the in-bag sample.
2443         If ``subsample == 1`` this is the deviance on the training data.
2444 
2445     loss_ : LossFunction
2446         The concrete ``LossFunction`` object.
2447 
2448     init_ : estimator
2449         The estimator that provides the initial predictions.
2450         Set via the ``init`` argument or ``loss.init_estimator``.
2451 
2452     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
2453         The collection of fitted sub-estimators.
2454 
2455     Notes
2456     -----
2457     The features are always randomly permuted at each split. Therefore,
2458     the best found split may vary, even with the same training data and
2459     ``max_features=n_features``, if the improvement of the criterion is
2460     identical for several splits enumerated during the search of the best
2461     split. To obtain a deterministic behaviour during fitting,
2462     ``random_state`` has to be fixed.
2463 
2464     See also
2465     --------
2466     DecisionTreeRegressor, RandomForestRegressor
2467 
2468     References
2469     ----------
2470     J. Friedman, Greedy Function Approximation: A Gradient Boosting
2471     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
2472 
2473     J. Friedman, Stochastic Gradient Boosting, 1999
2474 
2475     T. Hastie, R. Tibshirani and J. Friedman.
2476     Elements of Statistical Learning Ed. 2, Springer, 2009.
2477     """
2478 
2479     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')
2480 
2481     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
2482                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
2483                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
2484                  max_depth=3, min_impurity_decrease=0.,
2485                  min_impurity_split=None, init=None, random_state=None,
2486                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
2487                  warm_start=False, presort='auto', validation_fraction=0.1,
2488                  n_iter_no_change=None, tol=1e-4):
2489 
2490         super().__init__(
2491             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
2492             criterion=criterion, min_samples_split=min_samples_split,
2493             min_samples_leaf=min_samples_leaf,
2494             min_weight_fraction_leaf=min_weight_fraction_leaf,
2495             max_depth=max_depth, init=init, subsample=subsample,
2496             max_features=max_features,
2497             min_impurity_decrease=min_impurity_decrease,
2498             min_impurity_split=min_impurity_split,
2499             random_state=random_state, alpha=alpha, verbose=verbose,
2500             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
2501             presort=presort, validation_fraction=validation_fraction,
2502             n_iter_no_change=n_iter_no_change, tol=tol)
2503 
2504     def predict(self, X):
2505         """Predict regression target for X.
2506 
2507         Parameters
2508         ----------
2509         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2510             The input samples. Internally, it will be converted to
2511             ``dtype=np.float32`` and if a sparse matrix is provided
2512             to a sparse ``csr_matrix``.
2513 
2514         Returns
2515         -------
2516         y : array, shape (n_samples,)
2517             The predicted values.
2518         """
2519         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2520         # In regression we can directly return the raw value from the trees.
2521         return self._raw_predict(X).ravel()
2522 
2523     def staged_predict(self, X):
2524         """Predict regression target at each stage for X.
2525 
2526         This method allows monitoring (i.e. determine error on testing set)
2527         after each stage.
2528 
2529         Parameters
2530         ----------
2531         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2532             The input samples. Internally, it will be converted to
2533             ``dtype=np.float32`` and if a sparse matrix is provided
2534             to a sparse ``csr_matrix``.
2535 
2536         Returns
2537         -------
2538         y : generator of array of shape (n_samples,)
2539             The predicted value of the input samples.
2540         """
2541         for raw_predictions in self._staged_raw_predict(X):
2542             yield raw_predictions.ravel()
2543 
2544     def apply(self, X):
2545         """Apply trees in the ensemble to X, return leaf indices.
2546 
2547         .. versionadded:: 0.17
2548 
2549         Parameters
2550         ----------
2551         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2552             The input samples. Internally, its dtype will be converted to
2553             ``dtype=np.float32``. If a sparse matrix is provided, it will
2554             be converted to a sparse ``csr_matrix``.
2555 
2556         Returns
2557         -------
2558         X_leaves : array-like, shape (n_samples, n_estimators)
2559             For each datapoint x in X and for each tree in the ensemble,
2560             return the index of the leaf x ends up in each estimator.
2561         """
2562 
2563         leaves = super().apply(X)
2564         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])
2565         return leaves

</code><intended_behavior>The intended behavior within `GradientBoostingRegressor` is to ensure that the `feature_importances_` calculation accurately reflects the contributions of all trees within the ensemble, properly normalized to sum up to 1.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostRegressor</class>
<method>None</method>
<code>
871 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
872     """An AdaBoost regressor.
873 
874     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
875     regressor on the original dataset and then fits additional copies of the
876     regressor on the same dataset but where the weights of instances are
877     adjusted according to the error of the current prediction. As such,
878     subsequent regressors focus more on difficult cases.
879 
880     This class implements the algorithm known as AdaBoost.R2 [2].
881 
882     Read more in the :ref:`User Guide <adaboost>`.
883 
884     Parameters
885     ----------
886     base_estimator : object, optional (default=None)
887         The base estimator from which the boosted ensemble is built.
888         Support for sample weighting is required. If ``None``, then
889         the base estimator is ``DecisionTreeRegressor(max_depth=3)``
890 
891     n_estimators : integer, optional (default=50)
892         The maximum number of estimators at which boosting is terminated.
893         In case of perfect fit, the learning procedure is stopped early.
894 
895     learning_rate : float, optional (default=1.)
896         Learning rate shrinks the contribution of each regressor by
897         ``learning_rate``. There is a trade-off between ``learning_rate`` and
898         ``n_estimators``.
899 
900     loss : {'linear', 'square', 'exponential'}, optional (default='linear')
901         The loss function to use when updating the weights after each
902         boosting iteration.
903 
904     random_state : int, RandomState instance or None, optional (default=None)
905         If int, random_state is the seed used by the random number generator;
906         If RandomState instance, random_state is the random number generator;
907         If None, the random number generator is the RandomState instance used
908         by `np.random`.
909 
910     Attributes
911     ----------
912     estimators_ : list of classifiers
913         The collection of fitted sub-estimators.
914 
915     estimator_weights_ : array of floats
916         Weights for each estimator in the boosted ensemble.
917 
918     estimator_errors_ : array of floats
919         Regression error for each estimator in the boosted ensemble.
920 
921     feature_importances_ : array of shape = [n_features]
922         The feature importances if supported by the ``base_estimator``.
923 
924     Examples
925     --------
926     >>> from sklearn.ensemble import AdaBoostRegressor
927     >>> from sklearn.datasets import make_regression
928     >>> X, y = make_regression(n_features=4, n_informative=2,
929     ...                        random_state=0, shuffle=False)
930     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
931     >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
932     AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',
933             n_estimators=100, random_state=0)
934     >>> regr.feature_importances_  # doctest: +ELLIPSIS
935     array([0.2788..., 0.7109..., 0.0065..., 0.0036...])
936     >>> regr.predict([[0, 0, 0, 0]])  # doctest: +ELLIPSIS
937     array([4.7972...])
938     >>> regr.score(X, y)  # doctest: +ELLIPSIS
939     0.9771...
940 
941     See also
942     --------
943     AdaBoostClassifier, GradientBoostingRegressor,
944     sklearn.tree.DecisionTreeRegressor
945 
946     References
947     ----------
948     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
949            on-Line Learning and an Application to Boosting", 1995.
950 
951     .. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
952 
953     """
954     def __init__(self,
955                  base_estimator=None,
956                  n_estimators=50,
957                  learning_rate=1.,
958                  loss='linear',
959                  random_state=None):
960 
961         super().__init__(
962             base_estimator=base_estimator,
963             n_estimators=n_estimators,
964             learning_rate=learning_rate,
965             random_state=random_state)
966 
967         self.loss = loss
968         self.random_state = random_state
969 
970     def fit(self, X, y, sample_weight=None):
971         """Build a boosted regressor from the training set (X, y).
972 
973         Parameters
974         ----------
975         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
976             The training input samples. Sparse matrix can be CSC, CSR, COO,
977             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
978 
979         y : array-like of shape = [n_samples]
980             The target values (real numbers).
981 
982         sample_weight : array-like of shape = [n_samples], optional
983             Sample weights. If None, the sample weights are initialized to
984             1 / n_samples.
985 
986         Returns
987         -------
988         self : object
989         """
990         # Check loss
991         if self.loss not in ('linear', 'square', 'exponential'):
992             raise ValueError(
993                 "loss must be 'linear', 'square', or 'exponential'")
994 
995         # Fit
996         return super().fit(X, y, sample_weight)
997 
998     def _validate_estimator(self):
999         """Check the estimator and set the base_estimator_ attribute."""
1000         super()._validate_estimator(
1001             default=DecisionTreeRegressor(max_depth=3))
1002 
1003     def _boost(self, iboost, X, y, sample_weight, random_state):
1004         """Implement a single boost for regression
1005 
1006         Perform a single boost according to the AdaBoost.R2 algorithm and
1007         return the updated sample weights.
1008 
1009         Parameters
1010         ----------
1011         iboost : int
1012             The index of the current boost iteration.
1013 
1014         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1015             The training input samples.
1016 
1017         y : array-like of shape = [n_samples]
1018             The target values (class labels in classification, real numbers in
1019             regression).
1020 
1021         sample_weight : array-like of shape = [n_samples]
1022             The current sample weights.
1023 
1024         random_state : RandomState
1025             The current random number generator
1026 
1027         Returns
1028         -------
1029         sample_weight : array-like of shape = [n_samples] or None
1030             The reweighted sample weights.
1031             If None then boosting has terminated early.
1032 
1033         estimator_weight : float
1034             The weight for the current boost.
1035             If None then boosting has terminated early.
1036 
1037         estimator_error : float
1038             The regression error for the current boost.
1039             If None then boosting has terminated early.
1040         """
1041         estimator = self._make_estimator(random_state=random_state)
1042 
1043         # Weighted sampling of the training set with replacement
1044         # For NumPy >= 1.7.0 use np.random.choice
1045         cdf = stable_cumsum(sample_weight)
1046         cdf /= cdf[-1]
1047         uniform_samples = random_state.random_sample(_num_samples(X))
1048         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
1049         # searchsorted returns a scalar
1050         bootstrap_idx = np.array(bootstrap_idx, copy=False)
1051 
1052         # Fit on the bootstrapped sample and obtain a prediction
1053         # for all samples in the training set
1054         X_ = safe_indexing(X, bootstrap_idx)
1055         y_ = safe_indexing(y, bootstrap_idx)
1056         estimator.fit(X_, y_)
1057         y_predict = estimator.predict(X)
1058 
1059         error_vect = np.abs(y_predict - y)
1060         error_max = error_vect.max()
1061 
1062         if error_max != 0.:
1063             error_vect /= error_max
1064 
1065         if self.loss == 'square':
1066             error_vect **= 2
1067         elif self.loss == 'exponential':
1068             error_vect = 1. - np.exp(- error_vect)
1069 
1070         # Calculate the average loss
1071         estimator_error = (sample_weight * error_vect).sum()
1072 
1073         if estimator_error <= 0:
1074             # Stop if fit is perfect
1075             return sample_weight, 1., 0.
1076 
1077         elif estimator_error >= 0.5:
1078             # Discard current estimator only if it isn't the only one
1079             if len(self.estimators_) > 1:
1080                 self.estimators_.pop(-1)
1081             return None, None, None
1082 
1083         beta = estimator_error / (1. - estimator_error)
1084 
1085         # Boost weight using AdaBoost.R2 alg
1086         estimator_weight = self.learning_rate * np.log(1. / beta)
1087 
1088         if not iboost == self.n_estimators - 1:
1089             sample_weight *= np.power(
1090                 beta,
1091                 (1. - error_vect) * self.learning_rate)
1092 
1093         return sample_weight, estimator_weight, estimator_error
1094 
1095     def _get_median_predict(self, X, limit):
1096         # Evaluate predictions of all estimators
1097         predictions = np.array([
1098             est.predict(X) for est in self.estimators_[:limit]]).T
1099 
1100         # Sort the predictions
1101         sorted_idx = np.argsort(predictions, axis=1)
1102 
1103         # Find index of median prediction for each sample
1104         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
1105         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
1106         median_idx = median_or_above.argmax(axis=1)
1107 
1108         median_estimators = sorted_idx[np.arange(_num_samples(X)), median_idx]
1109 
1110         # Return median predictions
1111         return predictions[np.arange(_num_samples(X)), median_estimators]
1112 
1113     def predict(self, X):
1114         """Predict regression value for X.
1115 
1116         The predicted regression value of an input sample is computed
1117         as the weighted median prediction of the classifiers in the ensemble.
1118 
1119         Parameters
1120         ----------
1121         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1122             The training input samples. Sparse matrix can be CSC, CSR, COO,
1123             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
1124 
1125         Returns
1126         -------
1127         y : array of shape = [n_samples]
1128             The predicted regression values.
1129         """
1130         check_is_fitted(self, "estimator_weights_")
1131         X = self._validate_data(X)
1132 
1133         return self._get_median_predict(X, len(self.estimators_))
1134 
1135     def staged_predict(self, X):
1136         """Return staged predictions for X.
1137 
1138         The predicted regression value of an input sample is computed
1139         as the weighted median prediction of the classifiers in the ensemble.
1140 
1141         This generator method yields the ensemble prediction after each
1142         iteration of boosting and therefore allows monitoring, such as to
1143         determine the prediction on a test set after each boost.
1144 
1145         Parameters
1146         ----------
1147         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1148             The training input samples.
1149 
1150         Returns
1151         -------
1152         y : generator of array, shape = [n_samples]
1153             The predicted regression values.
1154         """
1155         check_is_fitted(self, "estimator_weights_")
1156         X = self._validate_data(X)
1157 
1158         for i, _ in enumerate(self.estimators_, 1):
1159             yield self._get_median_predict(X, limit=i)

</code><intended_behavior>A preemptive review and potential revision of its `feature_importances_` calculation could be necessary to ensure consistency and reliability in how feature importances sum up to 1 across ensemble methods in `scikit-learn`.</intended_behavior>]
2025-10-15 23:37:35.644 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>GradientBoostingRegressor</class>
<method>None</method>
<code>
2233 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
2234     """Gradient Boosting for regression.
2235 
2236     GB builds an additive model in a forward stage-wise fashion;
2237     it allows for the optimization of arbitrary differentiable loss functions.
2238     In each stage a regression tree is fit on the negative gradient of the
2239     given loss function.
2240 
2241     Read more in the :ref:`User Guide <gradient_boosting>`.
2242 
2243     Parameters
2244     ----------
2245     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
2246         loss function to be optimized. 'ls' refers to least squares
2247         regression. 'lad' (least absolute deviation) is a highly robust
2248         loss function solely based on order information of the input
2249         variables. 'huber' is a combination of the two. 'quantile'
2250         allows quantile regression (use `alpha` to specify the quantile).
2251 
2252     learning_rate : float, optional (default=0.1)
2253         learning rate shrinks the contribution of each tree by `learning_rate`.
2254         There is a trade-off between learning_rate and n_estimators.
2255 
2256     n_estimators : int (default=100)
2257         The number of boosting stages to perform. Gradient boosting
2258         is fairly robust to over-fitting so a large number usually
2259         results in better performance.
2260 
2261     subsample : float, optional (default=1.0)
2262         The fraction of samples to be used for fitting the individual base
2263         learners. If smaller than 1.0 this results in Stochastic Gradient
2264         Boosting. `subsample` interacts with the parameter `n_estimators`.
2265         Choosing `subsample < 1.0` leads to a reduction of variance
2266         and an increase in bias.
2267 
2268     criterion : string, optional (default="friedman_mse")
2269         The function to measure the quality of a split. Supported criteria
2270         are "friedman_mse" for the mean squared error with improvement
2271         score by Friedman, "mse" for mean squared error, and "mae" for
2272         the mean absolute error. The default value of "friedman_mse" is
2273         generally the best as it can provide a better approximation in
2274         some cases.
2275 
2276         .. versionadded:: 0.18
2277 
2278     min_samples_split : int, float, optional (default=2)
2279         The minimum number of samples required to split an internal node:
2280 
2281         - If int, then consider `min_samples_split` as the minimum number.
2282         - If float, then `min_samples_split` is a fraction and
2283           `ceil(min_samples_split * n_samples)` are the minimum
2284           number of samples for each split.
2285 
2286         .. versionchanged:: 0.18
2287            Added float values for fractions.
2288 
2289     min_samples_leaf : int, float, optional (default=1)
2290         The minimum number of samples required to be at a leaf node.
2291         A split point at any depth will only be considered if it leaves at
2292         least ``min_samples_leaf`` training samples in each of the left and
2293         right branches.  This may have the effect of smoothing the model,
2294         especially in regression.
2295 
2296         - If int, then consider `min_samples_leaf` as the minimum number.
2297         - If float, then `min_samples_leaf` is a fraction and
2298           `ceil(min_samples_leaf * n_samples)` are the minimum
2299           number of samples for each node.
2300 
2301         .. versionchanged:: 0.18
2302            Added float values for fractions.
2303 
2304     min_weight_fraction_leaf : float, optional (default=0.)
2305         The minimum weighted fraction of the sum total of weights (of all
2306         the input samples) required to be at a leaf node. Samples have
2307         equal weight when sample_weight is not provided.
2308 
2309     max_depth : integer, optional (default=3)
2310         maximum depth of the individual regression estimators. The maximum
2311         depth limits the number of nodes in the tree. Tune this parameter
2312         for best performance; the best value depends on the interaction
2313         of the input variables.
2314 
2315     min_impurity_decrease : float, optional (default=0.)
2316         A node will be split if this split induces a decrease of the impurity
2317         greater than or equal to this value.
2318 
2319         The weighted impurity decrease equation is the following::
2320 
2321             N_t / N * (impurity - N_t_R / N_t * right_impurity
2322                                 - N_t_L / N_t * left_impurity)
2323 
2324         where ``N`` is the total number of samples, ``N_t`` is the number of
2325         samples at the current node, ``N_t_L`` is the number of samples in the
2326         left child, and ``N_t_R`` is the number of samples in the right child.
2327 
2328         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2329         if ``sample_weight`` is passed.
2330 
2331         .. versionadded:: 0.19
2332 
2333     min_impurity_split : float, (default=1e-7)
2334         Threshold for early stopping in tree growth. A node will split
2335         if its impurity is above the threshold, otherwise it is a leaf.
2336 
2337         .. deprecated:: 0.19
2338            ``min_impurity_split`` has been deprecated in favor of
2339            ``min_impurity_decrease`` in 0.19. The default value of
2340            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
2341            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
2342 
2343     init : estimator or 'zero', optional (default=None)
2344         An estimator object that is used to compute the initial predictions.
2345         ``init`` has to provide `fit` and `predict`. If 'zero', the initial
2346         raw predictions are set to zero. By default a ``DummyEstimator`` is
2347         used, predicting either the average target value (for loss='ls'), or
2348         a quantile for the other losses.
2349 
2350     random_state : int, RandomState instance or None, optional (default=None)
2351         If int, random_state is the seed used by the random number generator;
2352         If RandomState instance, random_state is the random number generator;
2353         If None, the random number generator is the RandomState instance used
2354         by `np.random`.
2355 
2356     max_features : int, float, string or None, optional (default=None)
2357         The number of features to consider when looking for the best split:
2358 
2359         - If int, then consider `max_features` features at each split.
2360         - If float, then `max_features` is a fraction and
2361           `int(max_features * n_features)` features are considered at each
2362           split.
2363         - If "auto", then `max_features=n_features`.
2364         - If "sqrt", then `max_features=sqrt(n_features)`.
2365         - If "log2", then `max_features=log2(n_features)`.
2366         - If None, then `max_features=n_features`.
2367 
2368         Choosing `max_features < n_features` leads to a reduction of variance
2369         and an increase in bias.
2370 
2371         Note: the search for a split does not stop until at least one
2372         valid partition of the node samples is found, even if it requires to
2373         effectively inspect more than ``max_features`` features.
2374 
2375     alpha : float (default=0.9)
2376         The alpha-quantile of the huber loss function and the quantile
2377         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
2378 
2379     verbose : int, default: 0
2380         Enable verbose output. If 1 then it prints progress and performance
2381         once in a while (the more trees the lower the frequency). If greater
2382         than 1 then it prints progress and performance for every tree.
2383 
2384     max_leaf_nodes : int or None, optional (default=None)
2385         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2386         Best nodes are defined as relative reduction in impurity.
2387         If None then unlimited number of leaf nodes.
2388 
2389     warm_start : bool, default: False
2390         When set to ``True``, reuse the solution of the previous call to fit
2391         and add more estimators to the ensemble, otherwise, just erase the
2392         previous solution. See :term:`the Glossary <warm_start>`.
2393 
2394     presort : bool or 'auto', optional (default='auto')
2395         Whether to presort the data to speed up the finding of best splits in
2396         fitting. Auto mode by default will use presorting on dense data and
2397         default to normal sorting on sparse data. Setting presort to true on
2398         sparse data will raise an error.
2399 
2400         .. versionadded:: 0.17
2401            optional parameter *presort*.
2402 
2403     validation_fraction : float, optional, default 0.1
2404         The proportion of training data to set aside as validation set for
2405         early stopping. Must be between 0 and 1.
2406         Only used if ``n_iter_no_change`` is set to an integer.
2407 
2408         .. versionadded:: 0.20
2409 
2410     n_iter_no_change : int, default None
2411         ``n_iter_no_change`` is used to decide if early stopping will be used
2412         to terminate training when validation score is not improving. By
2413         default it is set to None to disable early stopping. If set to a
2414         number, it will set aside ``validation_fraction`` size of the training
2415         data as validation and terminate training when validation score is not
2416         improving in all of the previous ``n_iter_no_change`` numbers of
2417         iterations.
2418 
2419         .. versionadded:: 0.20
2420 
2421     tol : float, optional, default 1e-4
2422         Tolerance for the early stopping. When the loss is not improving
2423         by at least tol for ``n_iter_no_change`` iterations (if set to a
2424         number), the training stops.
2425 
2426         .. versionadded:: 0.20
2427 
2428 
2429     Attributes
2430     ----------
2431     feature_importances_ : array, shape (n_features,)
2432         The feature importances (the higher, the more important the feature).
2433 
2434     oob_improvement_ : array, shape (n_estimators,)
2435         The improvement in loss (= deviance) on the out-of-bag samples
2436         relative to the previous iteration.
2437         ``oob_improvement_[0]`` is the improvement in
2438         loss of the first stage over the ``init`` estimator.
2439 
2440     train_score_ : array, shape (n_estimators,)
2441         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
2442         model at iteration ``i`` on the in-bag sample.
2443         If ``subsample == 1`` this is the deviance on the training data.
2444 
2445     loss_ : LossFunction
2446         The concrete ``LossFunction`` object.
2447 
2448     init_ : estimator
2449         The estimator that provides the initial predictions.
2450         Set via the ``init`` argument or ``loss.init_estimator``.
2451 
2452     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
2453         The collection of fitted sub-estimators.
2454 
2455     Notes
2456     -----
2457     The features are always randomly permuted at each split. Therefore,
2458     the best found split may vary, even with the same training data and
2459     ``max_features=n_features``, if the improvement of the criterion is
2460     identical for several splits enumerated during the search of the best
2461     split. To obtain a deterministic behaviour during fitting,
2462     ``random_state`` has to be fixed.
2463 
2464     See also
2465     --------
2466     DecisionTreeRegressor, RandomForestRegressor
2467 
2468     References
2469     ----------
2470     J. Friedman, Greedy Function Approximation: A Gradient Boosting
2471     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
2472 
2473     J. Friedman, Stochastic Gradient Boosting, 1999
2474 
2475     T. Hastie, R. Tibshirani and J. Friedman.
2476     Elements of Statistical Learning Ed. 2, Springer, 2009.
2477     """
2478 
2479     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')
2480 
2481     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
2482                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
2483                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
2484                  max_depth=3, min_impurity_decrease=0.,
2485                  min_impurity_split=None, init=None, random_state=None,
2486                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
2487                  warm_start=False, presort='auto', validation_fraction=0.1,
2488                  n_iter_no_change=None, tol=1e-4):
2489 
2490         super().__init__(
2491             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
2492             criterion=criterion, min_samples_split=min_samples_split,
2493             min_samples_leaf=min_samples_leaf,
2494             min_weight_fraction_leaf=min_weight_fraction_leaf,
2495             max_depth=max_depth, init=init, subsample=subsample,
2496             max_features=max_features,
2497             min_impurity_decrease=min_impurity_decrease,
2498             min_impurity_split=min_impurity_split,
2499             random_state=random_state, alpha=alpha, verbose=verbose,
2500             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
2501             presort=presort, validation_fraction=validation_fraction,
2502             n_iter_no_change=n_iter_no_change, tol=tol)
2503 
2504     def predict(self, X):
2505         """Predict regression target for X.
2506 
2507         Parameters
2508         ----------
2509         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2510             The input samples. Internally, it will be converted to
2511             ``dtype=np.float32`` and if a sparse matrix is provided
2512             to a sparse ``csr_matrix``.
2513 
2514         Returns
2515         -------
2516         y : array, shape (n_samples,)
2517             The predicted values.
2518         """
2519         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2520         # In regression we can directly return the raw value from the trees.
2521         return self._raw_predict(X).ravel()
2522 
2523     def staged_predict(self, X):
2524         """Predict regression target at each stage for X.
2525 
2526         This method allows monitoring (i.e. determine error on testing set)
2527         after each stage.
2528 
2529         Parameters
2530         ----------
2531         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2532             The input samples. Internally, it will be converted to
2533             ``dtype=np.float32`` and if a sparse matrix is provided
2534             to a sparse ``csr_matrix``.
2535 
2536         Returns
2537         -------
2538         y : generator of array of shape (n_samples,)
2539             The predicted value of the input samples.
2540         """
2541         for raw_predictions in self._staged_raw_predict(X):
2542             yield raw_predictions.ravel()
2543 
2544     def apply(self, X):
2545         """Apply trees in the ensemble to X, return leaf indices.
2546 
2547         .. versionadded:: 0.17
2548 
2549         Parameters
2550         ----------
2551         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2552             The input samples. Internally, its dtype will be converted to
2553             ``dtype=np.float32``. If a sparse matrix is provided, it will
2554             be converted to a sparse ``csr_matrix``.
2555 
2556         Returns
2557         -------
2558         X_leaves : array-like, shape (n_samples, n_estimators)
2559             For each datapoint x in X and for each tree in the ensemble,
2560             return the index of the leaf x ends up in each estimator.
2561         """
2562 
2563         leaves = super().apply(X)
2564         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])
2565         return leaves

</code><intended_behavior>The intended behavior within `GradientBoostingRegressor` is to ensure that the `feature_importances_` calculation accurately reflects the contributions of all trees within the ensemble, properly normalized to sum up to 1.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostRegressor</class>
<method>None</method>
<code>
871 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
872     """An AdaBoost regressor.
873 
874     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
875     regressor on the original dataset and then fits additional copies of the
876     regressor on the same dataset but where the weights of instances are
877     adjusted according to the error of the current prediction. As such,
878     subsequent regressors focus more on difficult cases.
879 
880     This class implements the algorithm known as AdaBoost.R2 [2].
881 
882     Read more in the :ref:`User Guide <adaboost>`.
883 
884     Parameters
885     ----------
886     base_estimator : object, optional (default=None)
887         The base estimator from which the boosted ensemble is built.
888         Support for sample weighting is required. If ``None``, then
889         the base estimator is ``DecisionTreeRegressor(max_depth=3)``
890 
891     n_estimators : integer, optional (default=50)
892         The maximum number of estimators at which boosting is terminated.
893         In case of perfect fit, the learning procedure is stopped early.
894 
895     learning_rate : float, optional (default=1.)
896         Learning rate shrinks the contribution of each regressor by
897         ``learning_rate``. There is a trade-off between ``learning_rate`` and
898         ``n_estimators``.
899 
900     loss : {'linear', 'square', 'exponential'}, optional (default='linear')
901         The loss function to use when updating the weights after each
902         boosting iteration.
903 
904     random_state : int, RandomState instance or None, optional (default=None)
905         If int, random_state is the seed used by the random number generator;
906         If RandomState instance, random_state is the random number generator;
907         If None, the random number generator is the RandomState instance used
908         by `np.random`.
909 
910     Attributes
911     ----------
912     estimators_ : list of classifiers
913         The collection of fitted sub-estimators.
914 
915     estimator_weights_ : array of floats
916         Weights for each estimator in the boosted ensemble.
917 
918     estimator_errors_ : array of floats
919         Regression error for each estimator in the boosted ensemble.
920 
921     feature_importances_ : array of shape = [n_features]
922         The feature importances if supported by the ``base_estimator``.
923 
924     Examples
925     --------
926     >>> from sklearn.ensemble import AdaBoostRegressor
927     >>> from sklearn.datasets import make_regression
928     >>> X, y = make_regression(n_features=4, n_informative=2,
929     ...                        random_state=0, shuffle=False)
930     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
931     >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
932     AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',
933             n_estimators=100, random_state=0)
934     >>> regr.feature_importances_  # doctest: +ELLIPSIS
935     array([0.2788..., 0.7109..., 0.0065..., 0.0036...])
936     >>> regr.predict([[0, 0, 0, 0]])  # doctest: +ELLIPSIS
937     array([4.7972...])
938     >>> regr.score(X, y)  # doctest: +ELLIPSIS
939     0.9771...
940 
941     See also
942     --------
943     AdaBoostClassifier, GradientBoostingRegressor,
944     sklearn.tree.DecisionTreeRegressor
945 
946     References
947     ----------
948     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
949            on-Line Learning and an Application to Boosting", 1995.
950 
951     .. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
952 
953     """
954     def __init__(self,
955                  base_estimator=None,
956                  n_estimators=50,
957                  learning_rate=1.,
958                  loss='linear',
959                  random_state=None):
960 
961         super().__init__(
962             base_estimator=base_estimator,
963             n_estimators=n_estimators,
964             learning_rate=learning_rate,
965             random_state=random_state)
966 
967         self.loss = loss
968         self.random_state = random_state
969 
970     def fit(self, X, y, sample_weight=None):
971         """Build a boosted regressor from the training set (X, y).
972 
973         Parameters
974         ----------
975         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
976             The training input samples. Sparse matrix can be CSC, CSR, COO,
977             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
978 
979         y : array-like of shape = [n_samples]
980             The target values (real numbers).
981 
982         sample_weight : array-like of shape = [n_samples], optional
983             Sample weights. If None, the sample weights are initialized to
984             1 / n_samples.
985 
986         Returns
987         -------
988         self : object
989         """
990         # Check loss
991         if self.loss not in ('linear', 'square', 'exponential'):
992             raise ValueError(
993                 "loss must be 'linear', 'square', or 'exponential'")
994 
995         # Fit
996         return super().fit(X, y, sample_weight)
997 
998     def _validate_estimator(self):
999         """Check the estimator and set the base_estimator_ attribute."""
1000         super()._validate_estimator(
1001             default=DecisionTreeRegressor(max_depth=3))
1002 
1003     def _boost(self, iboost, X, y, sample_weight, random_state):
1004         """Implement a single boost for regression
1005 
1006         Perform a single boost according to the AdaBoost.R2 algorithm and
1007         return the updated sample weights.
1008 
1009         Parameters
1010         ----------
1011         iboost : int
1012             The index of the current boost iteration.
1013 
1014         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1015             The training input samples.
1016 
1017         y : array-like of shape = [n_samples]
1018             The target values (class labels in classification, real numbers in
1019             regression).
1020 
1021         sample_weight : array-like of shape = [n_samples]
1022             The current sample weights.
1023 
1024         random_state : RandomState
1025             The current random number generator
1026 
1027         Returns
1028         -------
1029         sample_weight : array-like of shape = [n_samples] or None
1030             The reweighted sample weights.
1031             If None then boosting has terminated early.
1032 
1033         estimator_weight : float
1034             The weight for the current boost.
1035             If None then boosting has terminated early.
1036 
1037         estimator_error : float
1038             The regression error for the current boost.
1039             If None then boosting has terminated early.
1040         """
1041         estimator = self._make_estimator(random_state=random_state)
1042 
1043         # Weighted sampling of the training set with replacement
1044         # For NumPy >= 1.7.0 use np.random.choice
1045         cdf = stable_cumsum(sample_weight)
1046         cdf /= cdf[-1]
1047         uniform_samples = random_state.random_sample(_num_samples(X))
1048         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
1049         # searchsorted returns a scalar
1050         bootstrap_idx = np.array(bootstrap_idx, copy=False)
1051 
1052         # Fit on the bootstrapped sample and obtain a prediction
1053         # for all samples in the training set
1054         X_ = safe_indexing(X, bootstrap_idx)
1055         y_ = safe_indexing(y, bootstrap_idx)
1056         estimator.fit(X_, y_)
1057         y_predict = estimator.predict(X)
1058 
1059         error_vect = np.abs(y_predict - y)
1060         error_max = error_vect.max()
1061 
1062         if error_max != 0.:
1063             error_vect /= error_max
1064 
1065         if self.loss == 'square':
1066             error_vect **= 2
1067         elif self.loss == 'exponential':
1068             error_vect = 1. - np.exp(- error_vect)
1069 
1070         # Calculate the average loss
1071         estimator_error = (sample_weight * error_vect).sum()
1072 
1073         if estimator_error <= 0:
1074             # Stop if fit is perfect
1075             return sample_weight, 1., 0.
1076 
1077         elif estimator_error >= 0.5:
1078             # Discard current estimator only if it isn't the only one
1079             if len(self.estimators_) > 1:
1080                 self.estimators_.pop(-1)
1081             return None, None, None
1082 
1083         beta = estimator_error / (1. - estimator_error)
1084 
1085         # Boost weight using AdaBoost.R2 alg
1086         estimator_weight = self.learning_rate * np.log(1. / beta)
1087 
1088         if not iboost == self.n_estimators - 1:
1089             sample_weight *= np.power(
1090                 beta,
1091                 (1. - error_vect) * self.learning_rate)
1092 
1093         return sample_weight, estimator_weight, estimator_error
1094 
1095     def _get_median_predict(self, X, limit):
1096         # Evaluate predictions of all estimators
1097         predictions = np.array([
1098             est.predict(X) for est in self.estimators_[:limit]]).T
1099 
1100         # Sort the predictions
1101         sorted_idx = np.argsort(predictions, axis=1)
1102 
1103         # Find index of median prediction for each sample
1104         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
1105         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
1106         median_idx = median_or_above.argmax(axis=1)
1107 
1108         median_estimators = sorted_idx[np.arange(_num_samples(X)), median_idx]
1109 
1110         # Return median predictions
1111         return predictions[np.arange(_num_samples(X)), median_estimators]
1112 
1113     def predict(self, X):
1114         """Predict regression value for X.
1115 
1116         The predicted regression value of an input sample is computed
1117         as the weighted median prediction of the classifiers in the ensemble.
1118 
1119         Parameters
1120         ----------
1121         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1122             The training input samples. Sparse matrix can be CSC, CSR, COO,
1123             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
1124 
1125         Returns
1126         -------
1127         y : array of shape = [n_samples]
1128             The predicted regression values.
1129         """
1130         check_is_fitted(self, "estimator_weights_")
1131         X = self._validate_data(X)
1132 
1133         return self._get_median_predict(X, len(self.estimators_))
1134 
1135     def staged_predict(self, X):
1136         """Return staged predictions for X.
1137 
1138         The predicted regression value of an input sample is computed
1139         as the weighted median prediction of the classifiers in the ensemble.
1140 
1141         This generator method yields the ensemble prediction after each
1142         iteration of boosting and therefore allows monitoring, such as to
1143         determine the prediction on a test set after each boost.
1144 
1145         Parameters
1146         ----------
1147         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1148             The training input samples.
1149 
1150         Returns
1151         -------
1152         y : generator of array, shape = [n_samples]
1153             The predicted regression values.
1154         """
1155         check_is_fitted(self, "estimator_weights_")
1156         X = self._validate_data(X)
1157 
1158         for i, _ in enumerate(self.estimators_, 1):
1159             yield self._get_median_predict(X, limit=i)

</code><intended_behavior>A preemptive review and potential revision of its `feature_importances_` calculation could be necessary to ensure consistency and reliability in how feature importances sum up to 1 across ensemble methods in `scikit-learn`.</intended_behavior>]
2025-10-15 23:37:35.644 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 23:37:35.644 | INFO     | Search-only: skipping patch selection
2025-10-15 23:37:35.735 | INFO     | Task scikit-learn__scikit-learn-13620 completed successfully.
2025-10-15 23:37:35.736 | INFO     | No patch generated. You can try running ACR again.
