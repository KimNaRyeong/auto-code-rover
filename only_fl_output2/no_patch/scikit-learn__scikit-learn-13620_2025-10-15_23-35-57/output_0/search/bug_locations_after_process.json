[
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 2233,
        "end": 2565,
        "class_name": "GradientBoostingRegressor",
        "method_name": null,
        "code": "2233 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n2234     \"\"\"Gradient Boosting for regression.\n2235 \n2236     GB builds an additive model in a forward stage-wise fashion;\n2237     it allows for the optimization of arbitrary differentiable loss functions.\n2238     In each stage a regression tree is fit on the negative gradient of the\n2239     given loss function.\n2240 \n2241     Read more in the :ref:`User Guide <gradient_boosting>`.\n2242 \n2243     Parameters\n2244     ----------\n2245     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n2246         loss function to be optimized. 'ls' refers to least squares\n2247         regression. 'lad' (least absolute deviation) is a highly robust\n2248         loss function solely based on order information of the input\n2249         variables. 'huber' is a combination of the two. 'quantile'\n2250         allows quantile regression (use `alpha` to specify the quantile).\n2251 \n2252     learning_rate : float, optional (default=0.1)\n2253         learning rate shrinks the contribution of each tree by `learning_rate`.\n2254         There is a trade-off between learning_rate and n_estimators.\n2255 \n2256     n_estimators : int (default=100)\n2257         The number of boosting stages to perform. Gradient boosting\n2258         is fairly robust to over-fitting so a large number usually\n2259         results in better performance.\n2260 \n2261     subsample : float, optional (default=1.0)\n2262         The fraction of samples to be used for fitting the individual base\n2263         learners. If smaller than 1.0 this results in Stochastic Gradient\n2264         Boosting. `subsample` interacts with the parameter `n_estimators`.\n2265         Choosing `subsample < 1.0` leads to a reduction of variance\n2266         and an increase in bias.\n2267 \n2268     criterion : string, optional (default=\"friedman_mse\")\n2269         The function to measure the quality of a split. Supported criteria\n2270         are \"friedman_mse\" for the mean squared error with improvement\n2271         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n2272         the mean absolute error. The default value of \"friedman_mse\" is\n2273         generally the best as it can provide a better approximation in\n2274         some cases.\n2275 \n2276         .. versionadded:: 0.18\n2277 \n2278     min_samples_split : int, float, optional (default=2)\n2279         The minimum number of samples required to split an internal node:\n2280 \n2281         - If int, then consider `min_samples_split` as the minimum number.\n2282         - If float, then `min_samples_split` is a fraction and\n2283           `ceil(min_samples_split * n_samples)` are the minimum\n2284           number of samples for each split.\n2285 \n2286         .. versionchanged:: 0.18\n2287            Added float values for fractions.\n2288 \n2289     min_samples_leaf : int, float, optional (default=1)\n2290         The minimum number of samples required to be at a leaf node.\n2291         A split point at any depth will only be considered if it leaves at\n2292         least ``min_samples_leaf`` training samples in each of the left and\n2293         right branches.  This may have the effect of smoothing the model,\n2294         especially in regression.\n2295 \n2296         - If int, then consider `min_samples_leaf` as the minimum number.\n2297         - If float, then `min_samples_leaf` is a fraction and\n2298           `ceil(min_samples_leaf * n_samples)` are the minimum\n2299           number of samples for each node.\n2300 \n2301         .. versionchanged:: 0.18\n2302            Added float values for fractions.\n2303 \n2304     min_weight_fraction_leaf : float, optional (default=0.)\n2305         The minimum weighted fraction of the sum total of weights (of all\n2306         the input samples) required to be at a leaf node. Samples have\n2307         equal weight when sample_weight is not provided.\n2308 \n2309     max_depth : integer, optional (default=3)\n2310         maximum depth of the individual regression estimators. The maximum\n2311         depth limits the number of nodes in the tree. Tune this parameter\n2312         for best performance; the best value depends on the interaction\n2313         of the input variables.\n2314 \n2315     min_impurity_decrease : float, optional (default=0.)\n2316         A node will be split if this split induces a decrease of the impurity\n2317         greater than or equal to this value.\n2318 \n2319         The weighted impurity decrease equation is the following::\n2320 \n2321             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2322                                 - N_t_L / N_t * left_impurity)\n2323 \n2324         where ``N`` is the total number of samples, ``N_t`` is the number of\n2325         samples at the current node, ``N_t_L`` is the number of samples in the\n2326         left child, and ``N_t_R`` is the number of samples in the right child.\n2327 \n2328         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2329         if ``sample_weight`` is passed.\n2330 \n2331         .. versionadded:: 0.19\n2332 \n2333     min_impurity_split : float, (default=1e-7)\n2334         Threshold for early stopping in tree growth. A node will split\n2335         if its impurity is above the threshold, otherwise it is a leaf.\n2336 \n2337         .. deprecated:: 0.19\n2338            ``min_impurity_split`` has been deprecated in favor of\n2339            ``min_impurity_decrease`` in 0.19. The default value of\n2340            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n2341            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n2342 \n2343     init : estimator or 'zero', optional (default=None)\n2344         An estimator object that is used to compute the initial predictions.\n2345         ``init`` has to provide `fit` and `predict`. If 'zero', the initial\n2346         raw predictions are set to zero. By default a ``DummyEstimator`` is\n2347         used, predicting either the average target value (for loss='ls'), or\n2348         a quantile for the other losses.\n2349 \n2350     random_state : int, RandomState instance or None, optional (default=None)\n2351         If int, random_state is the seed used by the random number generator;\n2352         If RandomState instance, random_state is the random number generator;\n2353         If None, the random number generator is the RandomState instance used\n2354         by `np.random`.\n2355 \n2356     max_features : int, float, string or None, optional (default=None)\n2357         The number of features to consider when looking for the best split:\n2358 \n2359         - If int, then consider `max_features` features at each split.\n2360         - If float, then `max_features` is a fraction and\n2361           `int(max_features * n_features)` features are considered at each\n2362           split.\n2363         - If \"auto\", then `max_features=n_features`.\n2364         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n2365         - If \"log2\", then `max_features=log2(n_features)`.\n2366         - If None, then `max_features=n_features`.\n2367 \n2368         Choosing `max_features < n_features` leads to a reduction of variance\n2369         and an increase in bias.\n2370 \n2371         Note: the search for a split does not stop until at least one\n2372         valid partition of the node samples is found, even if it requires to\n2373         effectively inspect more than ``max_features`` features.\n2374 \n2375     alpha : float (default=0.9)\n2376         The alpha-quantile of the huber loss function and the quantile\n2377         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n2378 \n2379     verbose : int, default: 0\n2380         Enable verbose output. If 1 then it prints progress and performance\n2381         once in a while (the more trees the lower the frequency). If greater\n2382         than 1 then it prints progress and performance for every tree.\n2383 \n2384     max_leaf_nodes : int or None, optional (default=None)\n2385         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2386         Best nodes are defined as relative reduction in impurity.\n2387         If None then unlimited number of leaf nodes.\n2388 \n2389     warm_start : bool, default: False\n2390         When set to ``True``, reuse the solution of the previous call to fit\n2391         and add more estimators to the ensemble, otherwise, just erase the\n2392         previous solution. See :term:`the Glossary <warm_start>`.\n2393 \n2394     presort : bool or 'auto', optional (default='auto')\n2395         Whether to presort the data to speed up the finding of best splits in\n2396         fitting. Auto mode by default will use presorting on dense data and\n2397         default to normal sorting on sparse data. Setting presort to true on\n2398         sparse data will raise an error.\n2399 \n2400         .. versionadded:: 0.17\n2401            optional parameter *presort*.\n2402 \n2403     validation_fraction : float, optional, default 0.1\n2404         The proportion of training data to set aside as validation set for\n2405         early stopping. Must be between 0 and 1.\n2406         Only used if ``n_iter_no_change`` is set to an integer.\n2407 \n2408         .. versionadded:: 0.20\n2409 \n2410     n_iter_no_change : int, default None\n2411         ``n_iter_no_change`` is used to decide if early stopping will be used\n2412         to terminate training when validation score is not improving. By\n2413         default it is set to None to disable early stopping. If set to a\n2414         number, it will set aside ``validation_fraction`` size of the training\n2415         data as validation and terminate training when validation score is not\n2416         improving in all of the previous ``n_iter_no_change`` numbers of\n2417         iterations.\n2418 \n2419         .. versionadded:: 0.20\n2420 \n2421     tol : float, optional, default 1e-4\n2422         Tolerance for the early stopping. When the loss is not improving\n2423         by at least tol for ``n_iter_no_change`` iterations (if set to a\n2424         number), the training stops.\n2425 \n2426         .. versionadded:: 0.20\n2427 \n2428 \n2429     Attributes\n2430     ----------\n2431     feature_importances_ : array, shape (n_features,)\n2432         The feature importances (the higher, the more important the feature).\n2433 \n2434     oob_improvement_ : array, shape (n_estimators,)\n2435         The improvement in loss (= deviance) on the out-of-bag samples\n2436         relative to the previous iteration.\n2437         ``oob_improvement_[0]`` is the improvement in\n2438         loss of the first stage over the ``init`` estimator.\n2439 \n2440     train_score_ : array, shape (n_estimators,)\n2441         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n2442         model at iteration ``i`` on the in-bag sample.\n2443         If ``subsample == 1`` this is the deviance on the training data.\n2444 \n2445     loss_ : LossFunction\n2446         The concrete ``LossFunction`` object.\n2447 \n2448     init_ : estimator\n2449         The estimator that provides the initial predictions.\n2450         Set via the ``init`` argument or ``loss.init_estimator``.\n2451 \n2452     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n2453         The collection of fitted sub-estimators.\n2454 \n2455     Notes\n2456     -----\n2457     The features are always randomly permuted at each split. Therefore,\n2458     the best found split may vary, even with the same training data and\n2459     ``max_features=n_features``, if the improvement of the criterion is\n2460     identical for several splits enumerated during the search of the best\n2461     split. To obtain a deterministic behaviour during fitting,\n2462     ``random_state`` has to be fixed.\n2463 \n2464     See also\n2465     --------\n2466     DecisionTreeRegressor, RandomForestRegressor\n2467 \n2468     References\n2469     ----------\n2470     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n2471     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n2472 \n2473     J. Friedman, Stochastic Gradient Boosting, 1999\n2474 \n2475     T. Hastie, R. Tibshirani and J. Friedman.\n2476     Elements of Statistical Learning Ed. 2, Springer, 2009.\n2477     \"\"\"\n2478 \n2479     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')\n2480 \n2481     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,\n2482                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n2483                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n2484                  max_depth=3, min_impurity_decrease=0.,\n2485                  min_impurity_split=None, init=None, random_state=None,\n2486                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n2487                  warm_start=False, presort='auto', validation_fraction=0.1,\n2488                  n_iter_no_change=None, tol=1e-4):\n2489 \n2490         super().__init__(\n2491             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n2492             criterion=criterion, min_samples_split=min_samples_split,\n2493             min_samples_leaf=min_samples_leaf,\n2494             min_weight_fraction_leaf=min_weight_fraction_leaf,\n2495             max_depth=max_depth, init=init, subsample=subsample,\n2496             max_features=max_features,\n2497             min_impurity_decrease=min_impurity_decrease,\n2498             min_impurity_split=min_impurity_split,\n2499             random_state=random_state, alpha=alpha, verbose=verbose,\n2500             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n2501             presort=presort, validation_fraction=validation_fraction,\n2502             n_iter_no_change=n_iter_no_change, tol=tol)\n2503 \n2504     def predict(self, X):\n2505         \"\"\"Predict regression target for X.\n2506 \n2507         Parameters\n2508         ----------\n2509         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2510             The input samples. Internally, it will be converted to\n2511             ``dtype=np.float32`` and if a sparse matrix is provided\n2512             to a sparse ``csr_matrix``.\n2513 \n2514         Returns\n2515         -------\n2516         y : array, shape (n_samples,)\n2517             The predicted values.\n2518         \"\"\"\n2519         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n2520         # In regression we can directly return the raw value from the trees.\n2521         return self._raw_predict(X).ravel()\n2522 \n2523     def staged_predict(self, X):\n2524         \"\"\"Predict regression target at each stage for X.\n2525 \n2526         This method allows monitoring (i.e. determine error on testing set)\n2527         after each stage.\n2528 \n2529         Parameters\n2530         ----------\n2531         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2532             The input samples. Internally, it will be converted to\n2533             ``dtype=np.float32`` and if a sparse matrix is provided\n2534             to a sparse ``csr_matrix``.\n2535 \n2536         Returns\n2537         -------\n2538         y : generator of array of shape (n_samples,)\n2539             The predicted value of the input samples.\n2540         \"\"\"\n2541         for raw_predictions in self._staged_raw_predict(X):\n2542             yield raw_predictions.ravel()\n2543 \n2544     def apply(self, X):\n2545         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n2546 \n2547         .. versionadded:: 0.17\n2548 \n2549         Parameters\n2550         ----------\n2551         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2552             The input samples. Internally, its dtype will be converted to\n2553             ``dtype=np.float32``. If a sparse matrix is provided, it will\n2554             be converted to a sparse ``csr_matrix``.\n2555 \n2556         Returns\n2557         -------\n2558         X_leaves : array-like, shape (n_samples, n_estimators)\n2559             For each datapoint x in X and for each tree in the ensemble,\n2560             return the index of the leaf x ends up in each estimator.\n2561         \"\"\"\n2562 \n2563         leaves = super().apply(X)\n2564         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n2565         return leaves\n",
        "intended_behavior": "The intended behavior within `GradientBoostingRegressor` is to ensure that the `feature_importances_` calculation accurately reflects the contributions of all trees within the ensemble, properly normalized to sum up to 1."
    },
    {
        "rel_file_path": "sklearn/ensemble/weight_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/weight_boosting.py",
        "start": 871,
        "end": 1159,
        "class_name": "AdaBoostRegressor",
        "method_name": null,
        "code": "871 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):\n872     \"\"\"An AdaBoost regressor.\n873 \n874     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n875     regressor on the original dataset and then fits additional copies of the\n876     regressor on the same dataset but where the weights of instances are\n877     adjusted according to the error of the current prediction. As such,\n878     subsequent regressors focus more on difficult cases.\n879 \n880     This class implements the algorithm known as AdaBoost.R2 [2].\n881 \n882     Read more in the :ref:`User Guide <adaboost>`.\n883 \n884     Parameters\n885     ----------\n886     base_estimator : object, optional (default=None)\n887         The base estimator from which the boosted ensemble is built.\n888         Support for sample weighting is required. If ``None``, then\n889         the base estimator is ``DecisionTreeRegressor(max_depth=3)``\n890 \n891     n_estimators : integer, optional (default=50)\n892         The maximum number of estimators at which boosting is terminated.\n893         In case of perfect fit, the learning procedure is stopped early.\n894 \n895     learning_rate : float, optional (default=1.)\n896         Learning rate shrinks the contribution of each regressor by\n897         ``learning_rate``. There is a trade-off between ``learning_rate`` and\n898         ``n_estimators``.\n899 \n900     loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n901         The loss function to use when updating the weights after each\n902         boosting iteration.\n903 \n904     random_state : int, RandomState instance or None, optional (default=None)\n905         If int, random_state is the seed used by the random number generator;\n906         If RandomState instance, random_state is the random number generator;\n907         If None, the random number generator is the RandomState instance used\n908         by `np.random`.\n909 \n910     Attributes\n911     ----------\n912     estimators_ : list of classifiers\n913         The collection of fitted sub-estimators.\n914 \n915     estimator_weights_ : array of floats\n916         Weights for each estimator in the boosted ensemble.\n917 \n918     estimator_errors_ : array of floats\n919         Regression error for each estimator in the boosted ensemble.\n920 \n921     feature_importances_ : array of shape = [n_features]\n922         The feature importances if supported by the ``base_estimator``.\n923 \n924     Examples\n925     --------\n926     >>> from sklearn.ensemble import AdaBoostRegressor\n927     >>> from sklearn.datasets import make_regression\n928     >>> X, y = make_regression(n_features=4, n_informative=2,\n929     ...                        random_state=0, shuffle=False)\n930     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n931     >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n932     AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n933             n_estimators=100, random_state=0)\n934     >>> regr.feature_importances_  # doctest: +ELLIPSIS\n935     array([0.2788..., 0.7109..., 0.0065..., 0.0036...])\n936     >>> regr.predict([[0, 0, 0, 0]])  # doctest: +ELLIPSIS\n937     array([4.7972...])\n938     >>> regr.score(X, y)  # doctest: +ELLIPSIS\n939     0.9771...\n940 \n941     See also\n942     --------\n943     AdaBoostClassifier, GradientBoostingRegressor,\n944     sklearn.tree.DecisionTreeRegressor\n945 \n946     References\n947     ----------\n948     .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n949            on-Line Learning and an Application to Boosting\", 1995.\n950 \n951     .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n952 \n953     \"\"\"\n954     def __init__(self,\n955                  base_estimator=None,\n956                  n_estimators=50,\n957                  learning_rate=1.,\n958                  loss='linear',\n959                  random_state=None):\n960 \n961         super().__init__(\n962             base_estimator=base_estimator,\n963             n_estimators=n_estimators,\n964             learning_rate=learning_rate,\n965             random_state=random_state)\n966 \n967         self.loss = loss\n968         self.random_state = random_state\n969 \n970     def fit(self, X, y, sample_weight=None):\n971         \"\"\"Build a boosted regressor from the training set (X, y).\n972 \n973         Parameters\n974         ----------\n975         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n976             The training input samples. Sparse matrix can be CSC, CSR, COO,\n977             DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n978 \n979         y : array-like of shape = [n_samples]\n980             The target values (real numbers).\n981 \n982         sample_weight : array-like of shape = [n_samples], optional\n983             Sample weights. If None, the sample weights are initialized to\n984             1 / n_samples.\n985 \n986         Returns\n987         -------\n988         self : object\n989         \"\"\"\n990         # Check loss\n991         if self.loss not in ('linear', 'square', 'exponential'):\n992             raise ValueError(\n993                 \"loss must be 'linear', 'square', or 'exponential'\")\n994 \n995         # Fit\n996         return super().fit(X, y, sample_weight)\n997 \n998     def _validate_estimator(self):\n999         \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n1000         super()._validate_estimator(\n1001             default=DecisionTreeRegressor(max_depth=3))\n1002 \n1003     def _boost(self, iboost, X, y, sample_weight, random_state):\n1004         \"\"\"Implement a single boost for regression\n1005 \n1006         Perform a single boost according to the AdaBoost.R2 algorithm and\n1007         return the updated sample weights.\n1008 \n1009         Parameters\n1010         ----------\n1011         iboost : int\n1012             The index of the current boost iteration.\n1013 \n1014         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n1015             The training input samples.\n1016 \n1017         y : array-like of shape = [n_samples]\n1018             The target values (class labels in classification, real numbers in\n1019             regression).\n1020 \n1021         sample_weight : array-like of shape = [n_samples]\n1022             The current sample weights.\n1023 \n1024         random_state : RandomState\n1025             The current random number generator\n1026 \n1027         Returns\n1028         -------\n1029         sample_weight : array-like of shape = [n_samples] or None\n1030             The reweighted sample weights.\n1031             If None then boosting has terminated early.\n1032 \n1033         estimator_weight : float\n1034             The weight for the current boost.\n1035             If None then boosting has terminated early.\n1036 \n1037         estimator_error : float\n1038             The regression error for the current boost.\n1039             If None then boosting has terminated early.\n1040         \"\"\"\n1041         estimator = self._make_estimator(random_state=random_state)\n1042 \n1043         # Weighted sampling of the training set with replacement\n1044         # For NumPy >= 1.7.0 use np.random.choice\n1045         cdf = stable_cumsum(sample_weight)\n1046         cdf /= cdf[-1]\n1047         uniform_samples = random_state.random_sample(_num_samples(X))\n1048         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n1049         # searchsorted returns a scalar\n1050         bootstrap_idx = np.array(bootstrap_idx, copy=False)\n1051 \n1052         # Fit on the bootstrapped sample and obtain a prediction\n1053         # for all samples in the training set\n1054         X_ = safe_indexing(X, bootstrap_idx)\n1055         y_ = safe_indexing(y, bootstrap_idx)\n1056         estimator.fit(X_, y_)\n1057         y_predict = estimator.predict(X)\n1058 \n1059         error_vect = np.abs(y_predict - y)\n1060         error_max = error_vect.max()\n1061 \n1062         if error_max != 0.:\n1063             error_vect /= error_max\n1064 \n1065         if self.loss == 'square':\n1066             error_vect **= 2\n1067         elif self.loss == 'exponential':\n1068             error_vect = 1. - np.exp(- error_vect)\n1069 \n1070         # Calculate the average loss\n1071         estimator_error = (sample_weight * error_vect).sum()\n1072 \n1073         if estimator_error <= 0:\n1074             # Stop if fit is perfect\n1075             return sample_weight, 1., 0.\n1076 \n1077         elif estimator_error >= 0.5:\n1078             # Discard current estimator only if it isn't the only one\n1079             if len(self.estimators_) > 1:\n1080                 self.estimators_.pop(-1)\n1081             return None, None, None\n1082 \n1083         beta = estimator_error / (1. - estimator_error)\n1084 \n1085         # Boost weight using AdaBoost.R2 alg\n1086         estimator_weight = self.learning_rate * np.log(1. / beta)\n1087 \n1088         if not iboost == self.n_estimators - 1:\n1089             sample_weight *= np.power(\n1090                 beta,\n1091                 (1. - error_vect) * self.learning_rate)\n1092 \n1093         return sample_weight, estimator_weight, estimator_error\n1094 \n1095     def _get_median_predict(self, X, limit):\n1096         # Evaluate predictions of all estimators\n1097         predictions = np.array([\n1098             est.predict(X) for est in self.estimators_[:limit]]).T\n1099 \n1100         # Sort the predictions\n1101         sorted_idx = np.argsort(predictions, axis=1)\n1102 \n1103         # Find index of median prediction for each sample\n1104         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)\n1105         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n1106         median_idx = median_or_above.argmax(axis=1)\n1107 \n1108         median_estimators = sorted_idx[np.arange(_num_samples(X)), median_idx]\n1109 \n1110         # Return median predictions\n1111         return predictions[np.arange(_num_samples(X)), median_estimators]\n1112 \n1113     def predict(self, X):\n1114         \"\"\"Predict regression value for X.\n1115 \n1116         The predicted regression value of an input sample is computed\n1117         as the weighted median prediction of the classifiers in the ensemble.\n1118 \n1119         Parameters\n1120         ----------\n1121         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n1122             The training input samples. Sparse matrix can be CSC, CSR, COO,\n1123             DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n1124 \n1125         Returns\n1126         -------\n1127         y : array of shape = [n_samples]\n1128             The predicted regression values.\n1129         \"\"\"\n1130         check_is_fitted(self, \"estimator_weights_\")\n1131         X = self._validate_data(X)\n1132 \n1133         return self._get_median_predict(X, len(self.estimators_))\n1134 \n1135     def staged_predict(self, X):\n1136         \"\"\"Return staged predictions for X.\n1137 \n1138         The predicted regression value of an input sample is computed\n1139         as the weighted median prediction of the classifiers in the ensemble.\n1140 \n1141         This generator method yields the ensemble prediction after each\n1142         iteration of boosting and therefore allows monitoring, such as to\n1143         determine the prediction on a test set after each boost.\n1144 \n1145         Parameters\n1146         ----------\n1147         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n1148             The training input samples.\n1149 \n1150         Returns\n1151         -------\n1152         y : generator of array, shape = [n_samples]\n1153             The predicted regression values.\n1154         \"\"\"\n1155         check_is_fitted(self, \"estimator_weights_\")\n1156         X = self._validate_data(X)\n1157 \n1158         for i, _ in enumerate(self.estimators_, 1):\n1159             yield self._get_median_predict(X, limit=i)\n",
        "intended_behavior": "A preemptive review and potential revision of its `feature_importances_` calculation could be necessary to ensure consistency and reliability in how feature importances sum up to 1 across ensemble methods in `scikit-learn`."
    }
]