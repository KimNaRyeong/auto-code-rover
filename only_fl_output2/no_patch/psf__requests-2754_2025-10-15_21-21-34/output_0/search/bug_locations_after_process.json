[
    {
        "rel_file_path": "requests/sessions.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/psf__requests/setup_psf__requests__3.0/requests/sessions.py",
        "start": 268,
        "end": 681,
        "class_name": "Session",
        "method_name": null,
        "code": "268 class Session(SessionRedirectMixin):\n269     \"\"\"A Requests session.\n270 \n271     Provides cookie persistence, connection-pooling, and configuration.\n272 \n273     Basic Usage::\n274 \n275       >>> import requests\n276       >>> s = requests.Session()\n277       >>> s.get('http://httpbin.org/get')\n278       <Response [200]>\n279 \n280     Or as a context manager::\n281 \n282       >>> with requests.Session() as s:\n283       >>>     s.get('http://httpbin.org/get')\n284       <Response [200]>\n285     \"\"\"\n286 \n287     __attrs__ = [\n288         'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',\n289         'cert', 'prefetch', 'adapters', 'stream', 'trust_env',\n290         'max_redirects',\n291     ]\n292 \n293     def __init__(self):\n294 \n295         #: A case-insensitive dictionary of headers to be sent on each\n296         #: :class:`Request <Request>` sent from this\n297         #: :class:`Session <Session>`.\n298         self.headers = default_headers()\n299 \n300         #: Default Authentication tuple or object to attach to\n301         #: :class:`Request <Request>`.\n302         self.auth = None\n303 \n304         #: Dictionary mapping protocol or protocol and host to the URL of the proxy\n305         #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to\n306         #: be used on each :class:`Request <Request>`.\n307         self.proxies = {}\n308 \n309         #: Event-handling hooks.\n310         self.hooks = default_hooks()\n311 \n312         #: Dictionary of querystring data to attach to each\n313         #: :class:`Request <Request>`. The dictionary values may be lists for\n314         #: representing multivalued query parameters.\n315         self.params = {}\n316 \n317         #: Stream response content default.\n318         self.stream = False\n319 \n320         #: SSL Verification default.\n321         self.verify = True\n322 \n323         #: SSL certificate default.\n324         self.cert = None\n325 \n326         #: Maximum number of redirects allowed. If the request exceeds this\n327         #: limit, a :class:`TooManyRedirects` exception is raised.\n328         self.max_redirects = DEFAULT_REDIRECT_LIMIT\n329 \n330         #: Trust environment settings for proxy configuration, default\n331         #: authentication and similar.\n332         self.trust_env = True\n333 \n334         #: A CookieJar containing all currently outstanding cookies set on this\n335         #: session. By default it is a\n336         #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but\n337         #: may be any other ``cookielib.CookieJar`` compatible object.\n338         self.cookies = cookiejar_from_dict({})\n339 \n340         # Default connection adapters.\n341         self.adapters = OrderedDict()\n342         self.mount('https://', HTTPAdapter())\n343         self.mount('http://', HTTPAdapter())\n344 \n345         # Only store 1000 redirects to prevent using infinite memory\n346         self.redirect_cache = RecentlyUsedContainer(REDIRECT_CACHE_SIZE)\n347 \n348     def __enter__(self):\n349         return self\n350 \n351     def __exit__(self, *args):\n352         self.close()\n353 \n354     def prepare_request(self, request):\n355         \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n356         transmission and returns it. The :class:`PreparedRequest` has settings\n357         merged from the :class:`Request <Request>` instance and those of the\n358         :class:`Session`.\n359 \n360         :param request: :class:`Request` instance to prepare with this\n361             Session's settings.\n362         \"\"\"\n363         cookies = request.cookies or {}\n364 \n365         # Bootstrap CookieJar.\n366         if not isinstance(cookies, cookielib.CookieJar):\n367             cookies = cookiejar_from_dict(cookies)\n368 \n369         # Merge with session cookies\n370         merged_cookies = merge_cookies(\n371             merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n372 \n373 \n374         # Set environment's basic authentication if not explicitly set.\n375         auth = request.auth\n376         if self.trust_env and not auth and not self.auth:\n377             auth = get_netrc_auth(request.url)\n378 \n379         p = PreparedRequest()\n380         p.prepare(\n381             method=request.method.upper(),\n382             url=request.url,\n383             files=request.files,\n384             data=request.data,\n385             json=request.json,\n386             headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n387             params=merge_setting(request.params, self.params),\n388             auth=merge_setting(auth, self.auth),\n389             cookies=merged_cookies,\n390             hooks=merge_hooks(request.hooks, self.hooks),\n391         )\n392         return p\n393 \n394     def request(self, method, url,\n395         params=None,\n396         data=None,\n397         headers=None,\n398         cookies=None,\n399         files=None,\n400         auth=None,\n401         timeout=None,\n402         allow_redirects=True,\n403         proxies=None,\n404         hooks=None,\n405         stream=None,\n406         verify=None,\n407         cert=None,\n408         json=None):\n409         \"\"\"Constructs a :class:`Request <Request>`, prepares it, and sends it.\n410         Returns :class:`Response <Response>` object.\n411 \n412         :param method: method for the new :class:`Request` object.\n413         :param url: URL for the new :class:`Request` object.\n414         :param params: (optional) Dictionary or bytes to be sent in the query\n415             string for the :class:`Request`.\n416         :param data: (optional) Dictionary, bytes, or file-like object to send\n417             in the body of the :class:`Request`.\n418         :param json: (optional) json to send in the body of the\n419             :class:`Request`.\n420         :param headers: (optional) Dictionary of HTTP Headers to send with the\n421             :class:`Request`.\n422         :param cookies: (optional) Dict or CookieJar object to send with the\n423             :class:`Request`.\n424         :param files: (optional) Dictionary of ``'filename': file-like-objects``\n425             for multipart encoding upload.\n426         :param auth: (optional) Auth tuple or callable to enable\n427             Basic/Digest/Custom HTTP Auth.\n428         :param timeout: (optional) How long to wait for the server to send\n429             data before giving up, as a float, or a :ref:`(connect timeout,\n430             read timeout) <timeouts>` tuple.\n431         :type timeout: float or tuple\n432         :param allow_redirects: (optional) Set to True by default.\n433         :type allow_redirects: bool\n434         :param proxies: (optional) Dictionary mapping protocol or protocol and\n435             hostname to the URL of the proxy.\n436         :param stream: (optional) whether to immediately download the response\n437             content. Defaults to ``False``.\n438         :param verify: (optional) whether the SSL cert will be verified.\n439             A CA_BUNDLE path can also be provided. Defaults to ``True``.\n440         :param cert: (optional) if String, path to ssl client cert file (.pem).\n441             If Tuple, ('cert', 'key') pair.\n442         \"\"\"\n443         # Create the Request.\n444         req = Request(\n445             method = method.upper(),\n446             url = url,\n447             headers = headers,\n448             files = files,\n449             data = data or {},\n450             json = json,\n451             params = params or {},\n452             auth = auth,\n453             cookies = cookies,\n454             hooks = hooks,\n455         )\n456         prep = self.prepare_request(req)\n457 \n458         proxies = proxies or {}\n459 \n460         settings = self.merge_environment_settings(\n461             prep.url, proxies, stream, verify, cert\n462         )\n463 \n464         # Send the request.\n465         send_kwargs = {\n466             'timeout': timeout,\n467             'allow_redirects': allow_redirects,\n468         }\n469         send_kwargs.update(settings)\n470         resp = self.send(prep, **send_kwargs)\n471 \n472         return resp\n473 \n474     def get(self, url, **kwargs):\n475         \"\"\"Sends a GET request. Returns :class:`Response` object.\n476 \n477         :param url: URL for the new :class:`Request` object.\n478         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n479         \"\"\"\n480 \n481         kwargs.setdefault('allow_redirects', True)\n482         return self.request('GET', url, **kwargs)\n483 \n484     def options(self, url, **kwargs):\n485         \"\"\"Sends a OPTIONS request. Returns :class:`Response` object.\n486 \n487         :param url: URL for the new :class:`Request` object.\n488         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n489         \"\"\"\n490 \n491         kwargs.setdefault('allow_redirects', True)\n492         return self.request('OPTIONS', url, **kwargs)\n493 \n494     def head(self, url, **kwargs):\n495         \"\"\"Sends a HEAD request. Returns :class:`Response` object.\n496 \n497         :param url: URL for the new :class:`Request` object.\n498         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n499         \"\"\"\n500 \n501         kwargs.setdefault('allow_redirects', False)\n502         return self.request('HEAD', url, **kwargs)\n503 \n504     def post(self, url, data=None, json=None, **kwargs):\n505         \"\"\"Sends a POST request. Returns :class:`Response` object.\n506 \n507         :param url: URL for the new :class:`Request` object.\n508         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n509         :param json: (optional) json to send in the body of the :class:`Request`.\n510         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n511         \"\"\"\n512 \n513         return self.request('POST', url, data=data, json=json, **kwargs)\n514 \n515     def put(self, url, data=None, **kwargs):\n516         \"\"\"Sends a PUT request. Returns :class:`Response` object.\n517 \n518         :param url: URL for the new :class:`Request` object.\n519         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n520         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n521         \"\"\"\n522 \n523         return self.request('PUT', url, data=data, **kwargs)\n524 \n525     def patch(self, url, data=None, **kwargs):\n526         \"\"\"Sends a PATCH request. Returns :class:`Response` object.\n527 \n528         :param url: URL for the new :class:`Request` object.\n529         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.\n530         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n531         \"\"\"\n532 \n533         return self.request('PATCH', url,  data=data, **kwargs)\n534 \n535     def delete(self, url, **kwargs):\n536         \"\"\"Sends a DELETE request. Returns :class:`Response` object.\n537 \n538         :param url: URL for the new :class:`Request` object.\n539         :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n540         \"\"\"\n541 \n542         return self.request('DELETE', url, **kwargs)\n543 \n544     def send(self, request, **kwargs):\n545         \"\"\"Send a given PreparedRequest.\"\"\"\n546         # Set defaults that the hooks can utilize to ensure they always have\n547         # the correct parameters to reproduce the previous request.\n548         kwargs.setdefault('stream', self.stream)\n549         kwargs.setdefault('verify', self.verify)\n550         kwargs.setdefault('cert', self.cert)\n551         kwargs.setdefault('proxies', self.proxies)\n552 \n553         # It's possible that users might accidentally send a Request object.\n554         # Guard against that specific failure case.\n555         if not isinstance(request, PreparedRequest):\n556             raise ValueError('You can only send PreparedRequests.')\n557 \n558         # Automatically skip a redirect chain if we've already followed it before.\n559         checked_urls = set()\n560         while request.url in self.redirect_cache:\n561             checked_urls.add(request.url)\n562             new_url = self.redirect_cache.get(request.url)\n563             if new_url in checked_urls:\n564                 break\n565             request.url = new_url\n566 \n567         # Set-up variables for resolve_redirects and dispatching of hooks.\n568         allow_redirects = kwargs.pop('allow_redirects', True)\n569         stream = kwargs.get('stream')\n570         hooks = request.hooks\n571 \n572         # Get the appropriate adapter to use.\n573         adapter = self.get_adapter(url=request.url)\n574 \n575         # Start time (approximately) of the request.\n576         start = datetime.utcnow()\n577 \n578         # Send the request\n579         r = adapter.send(request, **kwargs)\n580 \n581         # Total elapsed time of the request (approximately).\n582         r.elapsed = datetime.utcnow() - start\n583 \n584         # Response manipulation hooks.\n585         r = dispatch_hook('response', hooks, r, **kwargs)\n586 \n587         # Persist cookies\n588         if r.history:\n589 \n590             # If the hooks create history then we want those cookies too\n591             for resp in r.history:\n592                 extract_cookies_to_jar(self.cookies, resp.request, resp.raw)\n593 \n594         extract_cookies_to_jar(self.cookies, request, r.raw)\n595 \n596         # Redirect resolving generator.\n597         gen = self.resolve_redirects(r, **kwargs)\n598 \n599         # Resolve redirects, if allowed.\n600         history = [resp for resp in gen] if allow_redirects else []\n601 \n602         # Shuffle things around if there's redirection history.\n603         if history:\n604             # Insert the first (original) Response at the start.\n605             history.insert(0, r)\n606 \n607             # Remove the final response from history, use it as our Response.\n608             r = history.pop()\n609 \n610             # Save redirection history to final Response object.\n611             r.history = history\n612 \n613         # Automatically download response body, if not in streaming mode.\n614         if not stream:\n615             r.content\n616 \n617         return r\n618 \n619     def merge_environment_settings(self, url, proxies, stream, verify, cert):\n620         \"\"\"Check the environment and merge it with some settings.\"\"\"\n621         # Gather clues from the surrounding environment.\n622         if self.trust_env:\n623             # Set environment's proxies.\n624             env_proxies = get_environ_proxies(url) or {}\n625             for (k, v) in env_proxies.items():\n626                 proxies.setdefault(k, v)\n627 \n628             # Look for requests environment configuration and be compatible\n629             # with cURL.\n630             if verify is True or verify is None:\n631                 verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n632                           os.environ.get('CURL_CA_BUNDLE'))\n633 \n634         # Merge all the kwargs.\n635         proxies = merge_setting(proxies, self.proxies)\n636         stream = merge_setting(stream, self.stream)\n637         verify = merge_setting(verify, self.verify)\n638         cert = merge_setting(cert, self.cert)\n639 \n640         return {'verify': verify, 'proxies': proxies, 'stream': stream,\n641                 'cert': cert}\n642 \n643     def get_adapter(self, url):\n644         \"\"\"Returns the appropriate connection adapter for the given URL.\"\"\"\n645         for (prefix, adapter) in self.adapters.items():\n646 \n647             if url.lower().startswith(prefix):\n648                 return adapter\n649 \n650         # Nothing matches :-/\n651         raise InvalidScheme(\"No connection adapters were found for '%s'\" % url)\n652 \n653     def close(self):\n654         \"\"\"Closes all adapters and, as such, the Session.\"\"\"\n655         for v in self.adapters.values():\n656             v.close()\n657 \n658     def mount(self, prefix, adapter):\n659         \"\"\"Registers a connection adapter to a prefix.\n660 \n661         Adapters are sorted in descending order by key length.\"\"\"\n662 \n663         self.adapters[prefix] = adapter\n664         keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]\n665 \n666         for key in keys_to_move:\n667             self.adapters[key] = self.adapters.pop(key)\n668 \n669     def __getstate__(self):\n670         state = dict((attr, getattr(self, attr, None)) for attr in self.__attrs__)\n671         state['redirect_cache'] = dict(self.redirect_cache)\n672         return state\n673 \n674     def __setstate__(self, state):\n675         redirect_cache = state.pop('redirect_cache', {})\n676         for attr, value in state.items():\n677             setattr(self, attr, value)\n678 \n679         self.redirect_cache = RecentlyUsedContainer(REDIRECT_CACHE_SIZE)\n680         for redirect, to in redirect_cache.items():\n681             self.redirect_cache[redirect] = to\n",
        "intended_behavior": "This method needs to ensure that when a redirect URL is encountered, especially those containing non-ASCII characters, it is properly encoded to preserve the original characters. The method must validate and, if necessary, re-encode the URL from the 'Location' header in the redirect response to ensure that non-ASCII characters are correctly interpreted and called by the requests. This might involve using the quote and unquote functions appropriately to handle URLs with non-ASCII characters. Additionally, the method should include logic to debug or log encoding issues for easier troubleshooting of similar issues in the future."
    },
    {
        "rel_file_path": "requests/utils.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/psf__requests/setup_psf__requests__3.0/requests/utils.py",
        "start": 1,
        "end": 721,
        "class_name": null,
        "method_name": null,
        "code": "1 # -*- coding: utf-8 -*-\n2 \n3 \"\"\"\n4 requests.utils\n5 ~~~~~~~~~~~~~~\n6 \n7 This module provides utility functions that are used within Requests\n8 that are also useful for external consumption.\n9 \n10 \"\"\"\n11 \n12 import cgi\n13 import codecs\n14 import collections\n15 import io\n16 import os\n17 import platform\n18 import re\n19 import sys\n20 import socket\n21 import struct\n22 import warnings\n23 \n24 from . import __version__\n25 from . import certs\n26 from .compat import parse_http_list as _parse_list_header\n27 from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,\n28                      builtin_str, getproxies, proxy_bypass, urlunparse,\n29                      basestring)\n30 from .cookies import RequestsCookieJar, cookiejar_from_dict\n31 from .structures import CaseInsensitiveDict\n32 from .exceptions import InvalidURL, FileModeWarning\n33 \n34 _hush_pyflakes = (RequestsCookieJar,)\n35 \n36 NETRC_FILES = ('.netrc', '_netrc')\n37 \n38 DEFAULT_CA_BUNDLE_PATH = certs.where()\n39 \n40 \n41 def dict_to_sequence(d):\n42     \"\"\"Returns an internal sequence dictionary update.\"\"\"\n43 \n44     if hasattr(d, 'items'):\n45         d = d.items()\n46 \n47     return d\n48 \n49 \n50 def super_len(o):\n51     total_length = 0\n52     current_position = 0\n53 \n54     if hasattr(o, '__len__'):\n55         total_length = len(o)\n56 \n57     elif hasattr(o, 'len'):\n58         total_length = o.len\n59 \n60     elif hasattr(o, 'getvalue'):\n61         # e.g. BytesIO, cStringIO.StringIO\n62         total_length = len(o.getvalue())\n63 \n64     elif hasattr(o, 'fileno'):\n65         try:\n66             fileno = o.fileno()\n67         except io.UnsupportedOperation:\n68             pass\n69         else:\n70             total_length = os.fstat(fileno).st_size\n71 \n72             # Having used fstat to determine the file length, we need to\n73             # confirm that this file was opened up in binary mode.\n74             if 'b' not in o.mode:\n75                 warnings.warn((\n76                     \"Requests has determined the content-length for this \"\n77                     \"request using the binary size of the file: however, the \"\n78                     \"file has been opened in text mode (i.e. without the 'b' \"\n79                     \"flag in the mode). This may lead to an incorrect \"\n80                     \"content-length. In Requests 3.0, support will be removed \"\n81                     \"for files in text mode.\"),\n82                     FileModeWarning\n83                 )\n84 \n85     if hasattr(o, 'tell'):\n86         current_position = o.tell()\n87 \n88     return max(0, total_length - current_position)\n89 \n90 \n91 def get_netrc_auth(url, raise_errors=False):\n92     \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n93 \n94     try:\n95         from netrc import netrc, NetrcParseError\n96 \n97         netrc_path = None\n98 \n99         for f in NETRC_FILES:\n100             try:\n101                 loc = os.path.expanduser('~/{0}'.format(f))\n102             except KeyError:\n103                 # os.path.expanduser can fail when $HOME is undefined and\n104                 # getpwuid fails. See http://bugs.python.org/issue20164 &\n105                 # https://github.com/kennethreitz/requests/issues/1846\n106                 return\n107 \n108             if os.path.exists(loc):\n109                 netrc_path = loc\n110                 break\n111 \n112         # Abort early if there isn't one.\n113         if netrc_path is None:\n114             return\n115 \n116         ri = urlparse(url)\n117 \n118         # Strip port numbers from netloc. This weird `if...encode`` dance is\n119         # used for Python 3.2, which doesn't support unicode literals.\n120         splitstr = b':'\n121         if isinstance(url, str):\n122             splitstr = splitstr.decode('ascii')\n123         host = ri.netloc.split(splitstr)[0]\n124 \n125         try:\n126             _netrc = netrc(netrc_path).authenticators(host)\n127             if _netrc:\n128                 # Return with login / password\n129                 login_i = (0 if _netrc[0] else 1)\n130                 return (_netrc[login_i], _netrc[2])\n131         except (NetrcParseError, IOError):\n132             # If there was a parsing error or a permissions issue reading the file,\n133             # we'll just skip netrc auth unless explicitly asked to raise errors.\n134             if raise_errors:\n135                 raise\n136 \n137     # AppEngine hackiness.\n138     except (ImportError, AttributeError):\n139         pass\n140 \n141 \n142 def guess_filename(obj):\n143     \"\"\"Tries to guess the filename of the given object.\"\"\"\n144     name = getattr(obj, 'name', None)\n145     if (name and isinstance(name, basestring) and name[0] != '<' and\n146             name[-1] != '>'):\n147         return os.path.basename(name)\n148 \n149 \n150 def from_key_val_list(value):\n151     \"\"\"Take an object and test to see if it can be represented as a\n152     dictionary. Unless it can not be represented as such, return an\n153     OrderedDict, e.g.,\n154 \n155     ::\n156 \n157         >>> from_key_val_list([('key', 'val')])\n158         OrderedDict([('key', 'val')])\n159         >>> from_key_val_list('string')\n160         ValueError: need more than 1 value to unpack\n161         >>> from_key_val_list({'key': 'val'})\n162         OrderedDict([('key', 'val')])\n163     \"\"\"\n164     if value is None:\n165         return None\n166 \n167     if isinstance(value, (str, bytes, bool, int)):\n168         raise ValueError('cannot encode objects that are not 2-tuples')\n169 \n170     return OrderedDict(value)\n171 \n172 \n173 def to_key_val_list(value):\n174     \"\"\"Take an object and test to see if it can be represented as a\n175     dictionary. If it can be, return a list of tuples, e.g.,\n176 \n177     ::\n178 \n179         >>> to_key_val_list([('key', 'val')])\n180         [('key', 'val')]\n181         >>> to_key_val_list({'key': 'val'})\n182         [('key', 'val')]\n183         >>> to_key_val_list('string')\n184         ValueError: cannot encode objects that are not 2-tuples.\n185     \"\"\"\n186     if value is None:\n187         return None\n188 \n189     if isinstance(value, (str, bytes, bool, int)):\n190         raise ValueError('cannot encode objects that are not 2-tuples')\n191 \n192     if isinstance(value, collections.Mapping):\n193         value = value.items()\n194 \n195     return list(value)\n196 \n197 \n198 # From mitsuhiko/werkzeug (used with permission).\n199 def parse_list_header(value):\n200     \"\"\"Parse lists as described by RFC 2068 Section 2.\n201 \n202     In particular, parse comma-separated lists where the elements of\n203     the list may include quoted-strings.  A quoted-string could\n204     contain a comma.  A non-quoted string could have quotes in the\n205     middle.  Quotes are removed automatically after parsing.\n206 \n207     It basically works like :func:`parse_set_header` just that items\n208     may appear multiple times and case sensitivity is preserved.\n209 \n210     The return value is a standard :class:`list`:\n211 \n212     >>> parse_list_header('token, \"quoted value\"')\n213     ['token', 'quoted value']\n214 \n215     To create a header from the :class:`list` again, use the\n216     :func:`dump_header` function.\n217 \n218     :param value: a string with a list header.\n219     :return: :class:`list`\n220     \"\"\"\n221     result = []\n222     for item in _parse_list_header(value):\n223         if item[:1] == item[-1:] == '\"':\n224             item = unquote_header_value(item[1:-1])\n225         result.append(item)\n226     return result\n227 \n228 \n229 # From mitsuhiko/werkzeug (used with permission).\n230 def parse_dict_header(value):\n231     \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n232     convert them into a python dict:\n233 \n234     >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n235     >>> type(d) is dict\n236     True\n237     >>> sorted(d.items())\n238     [('bar', 'as well'), ('foo', 'is a fish')]\n239 \n240     If there is no value for a key it will be `None`:\n241 \n242     >>> parse_dict_header('key_without_value')\n243     {'key_without_value': None}\n244 \n245     To create a header from the :class:`dict` again, use the\n246     :func:`dump_header` function.\n247 \n248     :param value: a string with a dict header.\n249     :return: :class:`dict`\n250     \"\"\"\n251     result = {}\n252     for item in _parse_list_header(value):\n253         if '=' not in item:\n254             result[item] = None\n255             continue\n256         name, value = item.split('=', 1)\n257         if value[:1] == value[-1:] == '\"':\n258             value = unquote_header_value(value[1:-1])\n259         result[name] = value\n260     return result\n261 \n262 \n263 # From mitsuhiko/werkzeug (used with permission).\n264 def unquote_header_value(value, is_filename=False):\n265     r\"\"\"Unquotes a header value.  (Reversal of :func:`quote_header_value`).\n266     This does not use the real unquoting but what browsers are actually\n267     using for quoting.\n268 \n269     :param value: the header value to unquote.\n270     \"\"\"\n271     if value and value[0] == value[-1] == '\"':\n272         # this is not the real unquoting, but fixing this so that the\n273         # RFC is met will result in bugs with internet explorer and\n274         # probably some other browsers as well.  IE for example is\n275         # uploading files with \"C:\\foo\\bar.txt\" as filename\n276         value = value[1:-1]\n277 \n278         # if this is a filename and the starting characters look like\n279         # a UNC path, then just return the value without quotes.  Using the\n280         # replace sequence below on a UNC path has the effect of turning\n281         # the leading double slash into a single slash and then\n282         # _fix_ie_filename() doesn't work correctly.  See #458.\n283         if not is_filename or value[:2] != '\\\\\\\\':\n284             return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n285     return value\n286 \n287 \n288 def dict_from_cookiejar(cj):\n289     \"\"\"Returns a key/value dictionary from a CookieJar.\n290 \n291     :param cj: CookieJar object to extract cookies from.\n292     \"\"\"\n293 \n294     cookie_dict = {}\n295 \n296     for cookie in cj:\n297         cookie_dict[cookie.name] = cookie.value\n298 \n299     return cookie_dict\n300 \n301 \n302 def add_dict_to_cookiejar(cj, cookie_dict):\n303     \"\"\"Returns a CookieJar from a key/value dictionary.\n304 \n305     :param cj: CookieJar to insert cookies into.\n306     :param cookie_dict: Dict of key/values to insert into CookieJar.\n307     \"\"\"\n308 \n309     cj2 = cookiejar_from_dict(cookie_dict)\n310     cj.update(cj2)\n311     return cj\n312 \n313 \n314 def get_encodings_from_content(content):\n315     \"\"\"Returns encodings from given content string.\n316 \n317     :param content: bytestring to extract encodings from.\n318     \"\"\"\n319     warnings.warn((\n320         'In requests 3.0, get_encodings_from_content will be removed. For '\n321         'more information, please see the discussion on issue #2266. (This'\n322         ' warning should only appear once.)'),\n323         DeprecationWarning)\n324 \n325     charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n326     pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)\n327     xml_re = re.compile(r'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]')\n328 \n329     return (charset_re.findall(content) +\n330             pragma_re.findall(content) +\n331             xml_re.findall(content))\n332 \n333 \n334 def get_encoding_from_headers(headers):\n335     \"\"\"Returns encodings from given HTTP Header Dict.\n336 \n337     :param headers: dictionary to extract encoding from.\n338     \"\"\"\n339 \n340     content_type = headers.get('content-type')\n341 \n342     if not content_type:\n343         return None\n344 \n345     content_type, params = cgi.parse_header(content_type)\n346 \n347     if 'charset' in params:\n348         return params['charset'].strip(\"'\\\"\")\n349 \n350     if 'text' in content_type:\n351         return 'ISO-8859-1'\n352 \n353 \n354 def stream_decode_response_unicode(iterator, r):\n355     \"\"\"Stream decodes a iterator.\"\"\"\n356 \n357     if r.encoding is None:\n358         for item in iterator:\n359             yield item\n360         return\n361 \n362     decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n363     for chunk in iterator:\n364         rv = decoder.decode(chunk)\n365         if rv:\n366             yield rv\n367     rv = decoder.decode(b'', final=True)\n368     if rv:\n369         yield rv\n370 \n371 \n372 def iter_slices(string, slice_length):\n373     \"\"\"Iterate over slices of a string.\"\"\"\n374     pos = 0\n375     while pos < len(string):\n376         yield string[pos:pos + slice_length]\n377         pos += slice_length\n378 \n379 \n380 def get_unicode_from_response(r):\n381     \"\"\"Returns the requested content back in unicode.\n382 \n383     :param r: Response object to get unicode content from.\n384 \n385     Tried:\n386 \n387     1. charset from content-type\n388     2. fall back and replace all unicode characters\n389 \n390     \"\"\"\n391     warnings.warn((\n392         'In requests 3.0, get_unicode_from_response will be removed. For '\n393         'more information, please see the discussion on issue #2266. (This'\n394         ' warning should only appear once.)'),\n395         DeprecationWarning)\n396 \n397     tried_encodings = []\n398 \n399     # Try charset from content-type\n400     encoding = get_encoding_from_headers(r.headers)\n401 \n402     if encoding:\n403         try:\n404             return str(r.content, encoding)\n405         except UnicodeError:\n406             tried_encodings.append(encoding)\n407 \n408     # Fall back:\n409     try:\n410         return str(r.content, encoding, errors='replace')\n411     except TypeError:\n412         return r.content\n413 \n414 \n415 # The unreserved URI characters (RFC 3986)\n416 UNRESERVED_SET = frozenset(\n417     \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n418     + \"0123456789-._~\")\n419 \n420 \n421 def unquote_unreserved(uri):\n422     \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n423     characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n424     \"\"\"\n425     parts = uri.split('%')\n426     for i in range(1, len(parts)):\n427         h = parts[i][0:2]\n428         if len(h) == 2 and h.isalnum():\n429             try:\n430                 c = chr(int(h, 16))\n431             except ValueError:\n432                 raise InvalidURL(\"Invalid percent-escape sequence: '%s'\" % h)\n433 \n434             if c in UNRESERVED_SET:\n435                 parts[i] = c + parts[i][2:]\n436             else:\n437                 parts[i] = '%' + parts[i]\n438         else:\n439             parts[i] = '%' + parts[i]\n440     return ''.join(parts)\n441 \n442 \n443 def requote_uri(uri):\n444     \"\"\"Re-quote the given URI.\n445 \n446     This function passes the given URI through an unquote/quote cycle to\n447     ensure that it is fully and consistently quoted.\n448     \"\"\"\n449     safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n450     safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n451     try:\n452         # Unquote only the unreserved characters\n453         # Then quote only illegal characters (do not quote reserved,\n454         # unreserved, or '%')\n455         return quote(unquote_unreserved(uri), safe=safe_with_percent)\n456     except InvalidURL:\n457         # We couldn't unquote the given URI, so let's try quoting it, but\n458         # there may be unquoted '%'s in the URI. We need to make sure they're\n459         # properly quoted so they do not cause issues elsewhere.\n460         return quote(uri, safe=safe_without_percent)\n461 \n462 \n463 def address_in_network(ip, net):\n464     \"\"\"\n465     This function allows you to check if on IP belongs to a network subnet\n466     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n467              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n468     \"\"\"\n469     ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n470     netaddr, bits = net.split('/')\n471     netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n472     network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n473     return (ipaddr & netmask) == (network & netmask)\n474 \n475 \n476 def dotted_netmask(mask):\n477     \"\"\"\n478     Converts mask from /xx format to xxx.xxx.xxx.xxx\n479     Example: if mask is 24 function returns 255.255.255.0\n480     \"\"\"\n481     bits = 0xffffffff ^ (1 << 32 - mask) - 1\n482     return socket.inet_ntoa(struct.pack('>I', bits))\n483 \n484 \n485 def is_ipv4_address(string_ip):\n486     try:\n487         socket.inet_aton(string_ip)\n488     except socket.error:\n489         return False\n490     return True\n491 \n492 \n493 def is_valid_cidr(string_network):\n494     \"\"\"Very simple check of the cidr format in no_proxy variable\"\"\"\n495     if string_network.count('/') == 1:\n496         try:\n497             mask = int(string_network.split('/')[1])\n498         except ValueError:\n499             return False\n500 \n501         if mask < 1 or mask > 32:\n502             return False\n503 \n504         try:\n505             socket.inet_aton(string_network.split('/')[0])\n506         except socket.error:\n507             return False\n508     else:\n509         return False\n510     return True\n511 \n512 \n513 def should_bypass_proxies(url):\n514     \"\"\"\n515     Returns whether we should bypass proxies or not.\n516     \"\"\"\n517     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())\n518 \n519     # First check whether no_proxy is defined. If it is, check that the URL\n520     # we're getting isn't in the no_proxy list.\n521     no_proxy = get_proxy('no_proxy')\n522     netloc = urlparse(url).netloc\n523 \n524     if no_proxy:\n525         # We need to check whether we match here. We need to see if we match\n526         # the end of the netloc, both with and without the port.\n527         no_proxy = (\n528             host for host in no_proxy.replace(' ', '').split(',') if host\n529         )\n530 \n531         ip = netloc.split(':')[0]\n532         if is_ipv4_address(ip):\n533             for proxy_ip in no_proxy:\n534                 if is_valid_cidr(proxy_ip):\n535                     if address_in_network(ip, proxy_ip):\n536                         return True\n537         else:\n538             for host in no_proxy:\n539                 if netloc.endswith(host) or netloc.split(':')[0].endswith(host):\n540                     # The URL does match something in no_proxy, so we don't want\n541                     # to apply the proxies on this URL.\n542                     return True\n543 \n544     # If the system proxy settings indicate that this URL should be bypassed,\n545     # don't proxy.\n546     # The proxy_bypass function is incredibly buggy on OS X in early versions\n547     # of Python 2.6, so allow this call to fail. Only catch the specific\n548     # exceptions we've seen, though: this call failing in other ways can reveal\n549     # legitimate problems.\n550     try:\n551         bypass = proxy_bypass(netloc)\n552     except (TypeError, socket.gaierror):\n553         bypass = False\n554 \n555     if bypass:\n556         return True\n557 \n558     return False\n559 \n560 def get_environ_proxies(url):\n561     \"\"\"Return a dict of environment proxies.\"\"\"\n562     if should_bypass_proxies(url):\n563         return {}\n564     else:\n565         return getproxies()\n566 \n567 def select_proxy(url, proxies):\n568     \"\"\"Select a proxy for the url, if applicable.\n569 \n570     :param url: The url being for the request\n571     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n572     \"\"\"\n573     proxies = proxies or {}\n574     urlparts = urlparse(url)\n575     proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)\n576     if proxy is None:\n577         proxy = proxies.get(urlparts.scheme)\n578     return proxy\n579 \n580 def default_user_agent(name=\"python-requests\"):\n581     \"\"\"Return a string representing the default user agent.\"\"\"\n582     return '%s/%s' % (name, __version__)\n583 \n584 \n585 def default_headers():\n586     return CaseInsensitiveDict({\n587         'User-Agent': default_user_agent(),\n588         'Accept-Encoding': ', '.join(('gzip', 'deflate')),\n589         'Accept': '*/*',\n590         'Connection': 'keep-alive',\n591     })\n592 \n593 \n594 def parse_header_links(value):\n595     \"\"\"Return a dict of parsed link headers proxies.\n596 \n597     i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n598 \n599     \"\"\"\n600 \n601     links = []\n602 \n603     replace_chars = \" '\\\"\"\n604 \n605     for val in re.split(\", *<\", value):\n606         try:\n607             url, params = val.split(\";\", 1)\n608         except ValueError:\n609             url, params = val, ''\n610 \n611         link = {}\n612 \n613         link[\"url\"] = url.strip(\"<> '\\\"\")\n614 \n615         for param in params.split(\";\"):\n616             try:\n617                 key, value = param.split(\"=\")\n618             except ValueError:\n619                 break\n620 \n621             link[key.strip(replace_chars)] = value.strip(replace_chars)\n622 \n623         links.append(link)\n624 \n625     return links\n626 \n627 \n628 # Null bytes; no need to recreate these on each call to guess_json_utf\n629 _null = '\\x00'.encode('ascii')  # encoding to ASCII for Python 3\n630 _null2 = _null * 2\n631 _null3 = _null * 3\n632 \n633 \n634 def guess_json_utf(data):\n635     # JSON always starts with two ASCII characters, so detection is as\n636     # easy as counting the nulls and from their location and count\n637     # determine the encoding. Also detect a BOM, if present.\n638     sample = data[:4]\n639     if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):\n640         return 'utf-32'     # BOM included\n641     if sample[:3] == codecs.BOM_UTF8:\n642         return 'utf-8-sig'  # BOM included, MS style (discouraged)\n643     if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n644         return 'utf-16'     # BOM included\n645     nullcount = sample.count(_null)\n646     if nullcount == 0:\n647         return 'utf-8'\n648     if nullcount == 2:\n649         if sample[::2] == _null2:   # 1st and 3rd are null\n650             return 'utf-16-be'\n651         if sample[1::2] == _null2:  # 2nd and 4th are null\n652             return 'utf-16-le'\n653         # Did not detect 2 valid UTF-16 ascii-range characters\n654     if nullcount == 3:\n655         if sample[:3] == _null3:\n656             return 'utf-32-be'\n657         if sample[1:] == _null3:\n658             return 'utf-32-le'\n659         # Did not detect a valid UTF-32 ascii-range character\n660     return None\n661 \n662 \n663 def prepend_scheme_if_needed(url, new_scheme):\n664     '''Given a URL that may or may not have a scheme, prepend the given scheme.\n665     Does not replace a present scheme with the one provided as an argument.'''\n666     scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n667 \n668     # urlparse is a finicky beast, and sometimes decides that there isn't a\n669     # netloc present. Assume that it's being over-cautious, and switch netloc\n670     # and path if urlparse decided there was no netloc.\n671     if not netloc:\n672         netloc, path = path, netloc\n673 \n674     return urlunparse((scheme, netloc, path, params, query, fragment))\n675 \n676 \n677 def get_auth_from_url(url):\n678     \"\"\"Given a url with authentication components, extract them into a tuple of\n679     username,password.\"\"\"\n680     parsed = urlparse(url)\n681 \n682     try:\n683         auth = (unquote(parsed.username), unquote(parsed.password))\n684     except (AttributeError, TypeError):\n685         auth = ('', '')\n686 \n687     return auth\n688 \n689 \n690 def to_native_string(string, encoding='ascii'):\n691     \"\"\"\n692     Given a string object, regardless of type, returns a representation of that\n693     string in the native string type, encoding and decoding where necessary.\n694     This assumes ASCII unless told otherwise.\n695     \"\"\"\n696     out = None\n697 \n698     if isinstance(string, builtin_str):\n699         out = string\n700     else:\n701         if is_py2:\n702             out = string.encode(encoding)\n703         else:\n704             out = string.decode(encoding)\n705 \n706     return out\n707 \n708 \n709 def urldefragauth(url):\n710     \"\"\"\n711     Given a url remove the fragment and the authentication part\n712     \"\"\"\n713     scheme, netloc, path, params, query, fragment = urlparse(url)\n714 \n715     # see func:`prepend_scheme_if_needed`\n716     if not netloc:\n717         netloc, path = path, netloc\n718 \n719     netloc = netloc.rsplit('@', 1)[-1]\n720 \n721     return urlunparse((scheme, netloc, path, params, query, ''))\n",
        "intended_behavior": "Functions within this file responsible for encoding URLs (quote, unquote, urlencode, or similar) should be modified or extended to handle non-ASCII characters in URLs and redirects appropriately. It must ensure that any URL or part of a URL passed to it, especially those extracted from redirects, is encoded (or decoded) in a way that preserves non-ASCII characters without corruption. This behavior is essential to maintain the validity of URLs and to ensure they are accessible regardless of the character set they contain."
    }
]