2025-10-15 21:21:34.390 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 21:21:36.141 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=436, output_tokens=12, cost=0.004720
2025-10-15 21:21:36.142 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 21:21:36.146 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:21:45.770 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=745, output_tokens=430, cost=0.020350
2025-10-15 21:21:45.826 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:21:49.280 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=784, output_tokens=57, cost=0.009550
2025-10-15 21:21:49.282 | DEBUG    | Extracted a valid json.
2025-10-15 21:21:49.282 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"get\")",
        "search_code(\"requests.get\")",
        "search_code(\"redirect\")",
        "search_code(\"urlencode\")",
        "search_code(\"quote\")"
    ],
    "bug_locations": []
}
2025-10-15 21:21:49.349 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 21:21:49.349 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 21:22:05.075 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1955, output_tokens=764, cost=0.042470
2025-10-15 21:22:05.090 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:22:14.281 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2935, output_tokens=346, cost=0.039730
2025-10-15 21:22:14.290 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:22:22.726 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=700, output_tokens=292, cost=0.015760
2025-10-15 21:22:22.727 | DEBUG    | Extracted a valid json.
2025-10-15 21:22:22.727 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "requests/sessions.py",
            "class": "Session",
            "method": "",
            "intended_behavior": "This method needs to ensure that when a redirect URL is encountered, especially those containing non-ASCII characters, it is properly encoded to preserve the original characters. The method must validate and, if necessary, re-encode the URL from the 'Location' header in the redirect response to ensure that non-ASCII characters are correctly interpreted and called by the requests. This might involve using the quote and unquote functions appropriately to handle URLs with non-ASCII characters. Additionally, the method should include logic to debug or log encoding issues for easier troubleshooting of similar issues in the future."
        },
        {
            "file": "requests/utils.py",
            "class": "",
            "method": "",
            "intended_behavior": "Functions within this file responsible for encoding URLs (quote, unquote, urlencode, or similar) should be modified or extended to handle non-ASCII characters in URLs and redirects appropriately. It must ensure that any URL or part of a URL passed to it, especially those extracted from redirects, is encoded (or decoded) in a way that preserves non-ASCII characters without corruption. This behavior is essential to maintain the validity of URLs and to ensure they are accessible regardless of the character set they contain."
        }
    ]
}
2025-10-15 21:22:22.733 | DEBUG    | Bug location extracted successfully: [<file>requests/sessions.py</file>
<class>Session</class>
<method>None</method>
<code>
268 class Session(SessionRedirectMixin):
269     """A Requests session.
270 
271     Provides cookie persistence, connection-pooling, and configuration.
272 
273     Basic Usage::
274 
275       >>> import requests
276       >>> s = requests.Session()
277       >>> s.get('http://httpbin.org/get')
278       <Response [200]>
279 
280     Or as a context manager::
281 
282       >>> with requests.Session() as s:
283       >>>     s.get('http://httpbin.org/get')
284       <Response [200]>
285     """
286 
287     __attrs__ = [
288         'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',
289         'cert', 'prefetch', 'adapters', 'stream', 'trust_env',
290         'max_redirects',
291     ]
292 
293     def __init__(self):
294 
295         #: A case-insensitive dictionary of headers to be sent on each
296         #: :class:`Request <Request>` sent from this
297         #: :class:`Session <Session>`.
298         self.headers = default_headers()
299 
300         #: Default Authentication tuple or object to attach to
301         #: :class:`Request <Request>`.
302         self.auth = None
303 
304         #: Dictionary mapping protocol or protocol and host to the URL of the proxy
305         #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to
306         #: be used on each :class:`Request <Request>`.
307         self.proxies = {}
308 
309         #: Event-handling hooks.
310         self.hooks = default_hooks()
311 
312         #: Dictionary of querystring data to attach to each
313         #: :class:`Request <Request>`. The dictionary values may be lists for
314         #: representing multivalued query parameters.
315         self.params = {}
316 
317         #: Stream response content default.
318         self.stream = False
319 
320         #: SSL Verification default.
321         self.verify = True
322 
323         #: SSL certificate default.
324         self.cert = None
325 
326         #: Maximum number of redirects allowed. If the request exceeds this
327         #: limit, a :class:`TooManyRedirects` exception is raised.
328         self.max_redirects = DEFAULT_REDIRECT_LIMIT
329 
330         #: Trust environment settings for proxy configuration, default
331         #: authentication and similar.
332         self.trust_env = True
333 
334         #: A CookieJar containing all currently outstanding cookies set on this
335         #: session. By default it is a
336         #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but
337         #: may be any other ``cookielib.CookieJar`` compatible object.
338         self.cookies = cookiejar_from_dict({})
339 
340         # Default connection adapters.
341         self.adapters = OrderedDict()
342         self.mount('https://', HTTPAdapter())
343         self.mount('http://', HTTPAdapter())
344 
345         # Only store 1000 redirects to prevent using infinite memory
346         self.redirect_cache = RecentlyUsedContainer(REDIRECT_CACHE_SIZE)
347 
348     def __enter__(self):
349         return self
350 
351     def __exit__(self, *args):
352         self.close()
353 
354     def prepare_request(self, request):
355         """Constructs a :class:`PreparedRequest <PreparedRequest>` for
356         transmission and returns it. The :class:`PreparedRequest` has settings
357         merged from the :class:`Request <Request>` instance and those of the
358         :class:`Session`.
359 
360         :param request: :class:`Request` instance to prepare with this
361             Session's settings.
362         """
363         cookies = request.cookies or {}
364 
365         # Bootstrap CookieJar.
366         if not isinstance(cookies, cookielib.CookieJar):
367             cookies = cookiejar_from_dict(cookies)
368 
369         # Merge with session cookies
370         merged_cookies = merge_cookies(
371             merge_cookies(RequestsCookieJar(), self.cookies), cookies)
372 
373 
374         # Set environment's basic authentication if not explicitly set.
375         auth = request.auth
376         if self.trust_env and not auth and not self.auth:
377             auth = get_netrc_auth(request.url)
378 
379         p = PreparedRequest()
380         p.prepare(
381             method=request.method.upper(),
382             url=request.url,
383             files=request.files,
384             data=request.data,
385             json=request.json,
386             headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),
387             params=merge_setting(request.params, self.params),
388             auth=merge_setting(auth, self.auth),
389             cookies=merged_cookies,
390             hooks=merge_hooks(request.hooks, self.hooks),
391         )
392         return p
393 
394     def request(self, method, url,
395         params=None,
396         data=None,
397         headers=None,
398         cookies=None,
399         files=None,
400         auth=None,
401         timeout=None,
402         allow_redirects=True,
403         proxies=None,
404         hooks=None,
405         stream=None,
406         verify=None,
407         cert=None,
408         json=None):
409         """Constructs a :class:`Request <Request>`, prepares it, and sends it.
410         Returns :class:`Response <Response>` object.
411 
412         :param method: method for the new :class:`Request` object.
413         :param url: URL for the new :class:`Request` object.
414         :param params: (optional) Dictionary or bytes to be sent in the query
415             string for the :class:`Request`.
416         :param data: (optional) Dictionary, bytes, or file-like object to send
417             in the body of the :class:`Request`.
418         :param json: (optional) json to send in the body of the
419             :class:`Request`.
420         :param headers: (optional) Dictionary of HTTP Headers to send with the
421             :class:`Request`.
422         :param cookies: (optional) Dict or CookieJar object to send with the
423             :class:`Request`.
424         :param files: (optional) Dictionary of ``'filename': file-like-objects``
425             for multipart encoding upload.
426         :param auth: (optional) Auth tuple or callable to enable
427             Basic/Digest/Custom HTTP Auth.
428         :param timeout: (optional) How long to wait for the server to send
429             data before giving up, as a float, or a :ref:`(connect timeout,
430             read timeout) <timeouts>` tuple.
431         :type timeout: float or tuple
432         :param allow_redirects: (optional) Set to True by default.
433         :type allow_redirects: bool
434         :param proxies: (optional) Dictionary mapping protocol or protocol and
435             hostname to the URL of the proxy.
436         :param stream: (optional) whether to immediately download the response
437             content. Defaults to ``False``.
438         :param verify: (optional) whether the SSL cert will be verified.
439             A CA_BUNDLE path can also be provided. Defaults to ``True``.
440         :param cert: (optional) if String, path to ssl client cert file (.pem).
441             If Tuple, ('cert', 'key') pair.
442         """
443         # Create the Request.
444         req = Request(
445             method = method.upper(),
446             url = url,
447             headers = headers,
448             files = files,
449             data = data or {},
450             json = json,
451             params = params or {},
452             auth = auth,
453             cookies = cookies,
454             hooks = hooks,
455         )
456         prep = self.prepare_request(req)
457 
458         proxies = proxies or {}
459 
460         settings = self.merge_environment_settings(
461             prep.url, proxies, stream, verify, cert
462         )
463 
464         # Send the request.
465         send_kwargs = {
466             'timeout': timeout,
467             'allow_redirects': allow_redirects,
468         }
469         send_kwargs.update(settings)
470         resp = self.send(prep, **send_kwargs)
471 
472         return resp
473 
474     def get(self, url, **kwargs):
475         """Sends a GET request. Returns :class:`Response` object.
476 
477         :param url: URL for the new :class:`Request` object.
478         :param \*\*kwargs: Optional arguments that ``request`` takes.
479         """
480 
481         kwargs.setdefault('allow_redirects', True)
482         return self.request('GET', url, **kwargs)
483 
484     def options(self, url, **kwargs):
485         """Sends a OPTIONS request. Returns :class:`Response` object.
486 
487         :param url: URL for the new :class:`Request` object.
488         :param \*\*kwargs: Optional arguments that ``request`` takes.
489         """
490 
491         kwargs.setdefault('allow_redirects', True)
492         return self.request('OPTIONS', url, **kwargs)
493 
494     def head(self, url, **kwargs):
495         """Sends a HEAD request. Returns :class:`Response` object.
496 
497         :param url: URL for the new :class:`Request` object.
498         :param \*\*kwargs: Optional arguments that ``request`` takes.
499         """
500 
501         kwargs.setdefault('allow_redirects', False)
502         return self.request('HEAD', url, **kwargs)
503 
504     def post(self, url, data=None, json=None, **kwargs):
505         """Sends a POST request. Returns :class:`Response` object.
506 
507         :param url: URL for the new :class:`Request` object.
508         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.
509         :param json: (optional) json to send in the body of the :class:`Request`.
510         :param \*\*kwargs: Optional arguments that ``request`` takes.
511         """
512 
513         return self.request('POST', url, data=data, json=json, **kwargs)
514 
515     def put(self, url, data=None, **kwargs):
516         """Sends a PUT request. Returns :class:`Response` object.
517 
518         :param url: URL for the new :class:`Request` object.
519         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.
520         :param \*\*kwargs: Optional arguments that ``request`` takes.
521         """
522 
523         return self.request('PUT', url, data=data, **kwargs)
524 
525     def patch(self, url, data=None, **kwargs):
526         """Sends a PATCH request. Returns :class:`Response` object.
527 
528         :param url: URL for the new :class:`Request` object.
529         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.
530         :param \*\*kwargs: Optional arguments that ``request`` takes.
531         """
532 
533         return self.request('PATCH', url,  data=data, **kwargs)
534 
535     def delete(self, url, **kwargs):
536         """Sends a DELETE request. Returns :class:`Response` object.
537 
538         :param url: URL for the new :class:`Request` object.
539         :param \*\*kwargs: Optional arguments that ``request`` takes.
540         """
541 
542         return self.request('DELETE', url, **kwargs)
543 
544     def send(self, request, **kwargs):
545         """Send a given PreparedRequest."""
546         # Set defaults that the hooks can utilize to ensure they always have
547         # the correct parameters to reproduce the previous request.
548         kwargs.setdefault('stream', self.stream)
549         kwargs.setdefault('verify', self.verify)
550         kwargs.setdefault('cert', self.cert)
551         kwargs.setdefault('proxies', self.proxies)
552 
553         # It's possible that users might accidentally send a Request object.
554         # Guard against that specific failure case.
555         if not isinstance(request, PreparedRequest):
556             raise ValueError('You can only send PreparedRequests.')
557 
558         # Automatically skip a redirect chain if we've already followed it before.
559         checked_urls = set()
560         while request.url in self.redirect_cache:
561             checked_urls.add(request.url)
562             new_url = self.redirect_cache.get(request.url)
563             if new_url in checked_urls:
564                 break
565             request.url = new_url
566 
567         # Set-up variables for resolve_redirects and dispatching of hooks.
568         allow_redirects = kwargs.pop('allow_redirects', True)
569         stream = kwargs.get('stream')
570         hooks = request.hooks
571 
572         # Get the appropriate adapter to use.
573         adapter = self.get_adapter(url=request.url)
574 
575         # Start time (approximately) of the request.
576         start = datetime.utcnow()
577 
578         # Send the request
579         r = adapter.send(request, **kwargs)
580 
581         # Total elapsed time of the request (approximately).
582         r.elapsed = datetime.utcnow() - start
583 
584         # Response manipulation hooks.
585         r = dispatch_hook('response', hooks, r, **kwargs)
586 
587         # Persist cookies
588         if r.history:
589 
590             # If the hooks create history then we want those cookies too
591             for resp in r.history:
592                 extract_cookies_to_jar(self.cookies, resp.request, resp.raw)
593 
594         extract_cookies_to_jar(self.cookies, request, r.raw)
595 
596         # Redirect resolving generator.
597         gen = self.resolve_redirects(r, **kwargs)
598 
599         # Resolve redirects, if allowed.
600         history = [resp for resp in gen] if allow_redirects else []
601 
602         # Shuffle things around if there's redirection history.
603         if history:
604             # Insert the first (original) Response at the start.
605             history.insert(0, r)
606 
607             # Remove the final response from history, use it as our Response.
608             r = history.pop()
609 
610             # Save redirection history to final Response object.
611             r.history = history
612 
613         # Automatically download response body, if not in streaming mode.
614         if not stream:
615             r.content
616 
617         return r
618 
619     def merge_environment_settings(self, url, proxies, stream, verify, cert):
620         """Check the environment and merge it with some settings."""
621         # Gather clues from the surrounding environment.
622         if self.trust_env:
623             # Set environment's proxies.
624             env_proxies = get_environ_proxies(url) or {}
625             for (k, v) in env_proxies.items():
626                 proxies.setdefault(k, v)
627 
628             # Look for requests environment configuration and be compatible
629             # with cURL.
630             if verify is True or verify is None:
631                 verify = (os.environ.get('REQUESTS_CA_BUNDLE') or
632                           os.environ.get('CURL_CA_BUNDLE'))
633 
634         # Merge all the kwargs.
635         proxies = merge_setting(proxies, self.proxies)
636         stream = merge_setting(stream, self.stream)
637         verify = merge_setting(verify, self.verify)
638         cert = merge_setting(cert, self.cert)
639 
640         return {'verify': verify, 'proxies': proxies, 'stream': stream,
641                 'cert': cert}
642 
643     def get_adapter(self, url):
644         """Returns the appropriate connection adapter for the given URL."""
645         for (prefix, adapter) in self.adapters.items():
646 
647             if url.lower().startswith(prefix):
648                 return adapter
649 
650         # Nothing matches :-/
651         raise InvalidScheme("No connection adapters were found for '%s'" % url)
652 
653     def close(self):
654         """Closes all adapters and, as such, the Session."""
655         for v in self.adapters.values():
656             v.close()
657 
658     def mount(self, prefix, adapter):
659         """Registers a connection adapter to a prefix.
660 
661         Adapters are sorted in descending order by key length."""
662 
663         self.adapters[prefix] = adapter
664         keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]
665 
666         for key in keys_to_move:
667             self.adapters[key] = self.adapters.pop(key)
668 
669     def __getstate__(self):
670         state = dict((attr, getattr(self, attr, None)) for attr in self.__attrs__)
671         state['redirect_cache'] = dict(self.redirect_cache)
672         return state
673 
674     def __setstate__(self, state):
675         redirect_cache = state.pop('redirect_cache', {})
676         for attr, value in state.items():
677             setattr(self, attr, value)
678 
679         self.redirect_cache = RecentlyUsedContainer(REDIRECT_CACHE_SIZE)
680         for redirect, to in redirect_cache.items():
681             self.redirect_cache[redirect] = to

</code><intended_behavior>This method needs to ensure that when a redirect URL is encountered, especially those containing non-ASCII characters, it is properly encoded to preserve the original characters. The method must validate and, if necessary, re-encode the URL from the 'Location' header in the redirect response to ensure that non-ASCII characters are correctly interpreted and called by the requests. This might involve using the quote and unquote functions appropriately to handle URLs with non-ASCII characters. Additionally, the method should include logic to debug or log encoding issues for easier troubleshooting of similar issues in the future.</intended_behavior>, <file>requests/utils.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.utils
5 ~~~~~~~~~~~~~~
6 
7 This module provides utility functions that are used within Requests
8 that are also useful for external consumption.
9 
10 """
11 
12 import cgi
13 import codecs
14 import collections
15 import io
16 import os
17 import platform
18 import re
19 import sys
20 import socket
21 import struct
22 import warnings
23 
24 from . import __version__
25 from . import certs
26 from .compat import parse_http_list as _parse_list_header
27 from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,
28                      builtin_str, getproxies, proxy_bypass, urlunparse,
29                      basestring)
30 from .cookies import RequestsCookieJar, cookiejar_from_dict
31 from .structures import CaseInsensitiveDict
32 from .exceptions import InvalidURL, FileModeWarning
33 
34 _hush_pyflakes = (RequestsCookieJar,)
35 
36 NETRC_FILES = ('.netrc', '_netrc')
37 
38 DEFAULT_CA_BUNDLE_PATH = certs.where()
39 
40 
41 def dict_to_sequence(d):
42     """Returns an internal sequence dictionary update."""
43 
44     if hasattr(d, 'items'):
45         d = d.items()
46 
47     return d
48 
49 
50 def super_len(o):
51     total_length = 0
52     current_position = 0
53 
54     if hasattr(o, '__len__'):
55         total_length = len(o)
56 
57     elif hasattr(o, 'len'):
58         total_length = o.len
59 
60     elif hasattr(o, 'getvalue'):
61         # e.g. BytesIO, cStringIO.StringIO
62         total_length = len(o.getvalue())
63 
64     elif hasattr(o, 'fileno'):
65         try:
66             fileno = o.fileno()
67         except io.UnsupportedOperation:
68             pass
69         else:
70             total_length = os.fstat(fileno).st_size
71 
72             # Having used fstat to determine the file length, we need to
73             # confirm that this file was opened up in binary mode.
74             if 'b' not in o.mode:
75                 warnings.warn((
76                     "Requests has determined the content-length for this "
77                     "request using the binary size of the file: however, the "
78                     "file has been opened in text mode (i.e. without the 'b' "
79                     "flag in the mode). This may lead to an incorrect "
80                     "content-length. In Requests 3.0, support will be removed "
81                     "for files in text mode."),
82                     FileModeWarning
83                 )
84 
85     if hasattr(o, 'tell'):
86         current_position = o.tell()
87 
88     return max(0, total_length - current_position)
89 
90 
91 def get_netrc_auth(url, raise_errors=False):
92     """Returns the Requests tuple auth for a given url from netrc."""
93 
94     try:
95         from netrc import netrc, NetrcParseError
96 
97         netrc_path = None
98 
99         for f in NETRC_FILES:
100             try:
101                 loc = os.path.expanduser('~/{0}'.format(f))
102             except KeyError:
103                 # os.path.expanduser can fail when $HOME is undefined and
104                 # getpwuid fails. See http://bugs.python.org/issue20164 &
105                 # https://github.com/kennethreitz/requests/issues/1846
106                 return
107 
108             if os.path.exists(loc):
109                 netrc_path = loc
110                 break
111 
112         # Abort early if there isn't one.
113         if netrc_path is None:
114             return
115 
116         ri = urlparse(url)
117 
118         # Strip port numbers from netloc. This weird `if...encode`` dance is
119         # used for Python 3.2, which doesn't support unicode literals.
120         splitstr = b':'
121         if isinstance(url, str):
122             splitstr = splitstr.decode('ascii')
123         host = ri.netloc.split(splitstr)[0]
124 
125         try:
126             _netrc = netrc(netrc_path).authenticators(host)
127             if _netrc:
128                 # Return with login / password
129                 login_i = (0 if _netrc[0] else 1)
130                 return (_netrc[login_i], _netrc[2])
131         except (NetrcParseError, IOError):
132             # If there was a parsing error or a permissions issue reading the file,
133             # we'll just skip netrc auth unless explicitly asked to raise errors.
134             if raise_errors:
135                 raise
136 
137     # AppEngine hackiness.
138     except (ImportError, AttributeError):
139         pass
140 
141 
142 def guess_filename(obj):
143     """Tries to guess the filename of the given object."""
144     name = getattr(obj, 'name', None)
145     if (name and isinstance(name, basestring) and name[0] != '<' and
146             name[-1] != '>'):
147         return os.path.basename(name)
148 
149 
150 def from_key_val_list(value):
151     """Take an object and test to see if it can be represented as a
152     dictionary. Unless it can not be represented as such, return an
153     OrderedDict, e.g.,
154 
155     ::
156 
157         >>> from_key_val_list([('key', 'val')])
158         OrderedDict([('key', 'val')])
159         >>> from_key_val_list('string')
160         ValueError: need more than 1 value to unpack
161         >>> from_key_val_list({'key': 'val'})
162         OrderedDict([('key', 'val')])
163     """
164     if value is None:
165         return None
166 
167     if isinstance(value, (str, bytes, bool, int)):
168         raise ValueError('cannot encode objects that are not 2-tuples')
169 
170     return OrderedDict(value)
171 
172 
173 def to_key_val_list(value):
174     """Take an object and test to see if it can be represented as a
175     dictionary. If it can be, return a list of tuples, e.g.,
176 
177     ::
178 
179         >>> to_key_val_list([('key', 'val')])
180         [('key', 'val')]
181         >>> to_key_val_list({'key': 'val'})
182         [('key', 'val')]
183         >>> to_key_val_list('string')
184         ValueError: cannot encode objects that are not 2-tuples.
185     """
186     if value is None:
187         return None
188 
189     if isinstance(value, (str, bytes, bool, int)):
190         raise ValueError('cannot encode objects that are not 2-tuples')
191 
192     if isinstance(value, collections.Mapping):
193         value = value.items()
194 
195     return list(value)
196 
197 
198 # From mitsuhiko/werkzeug (used with permission).
199 def parse_list_header(value):
200     """Parse lists as described by RFC 2068 Section 2.
201 
202     In particular, parse comma-separated lists where the elements of
203     the list may include quoted-strings.  A quoted-string could
204     contain a comma.  A non-quoted string could have quotes in the
205     middle.  Quotes are removed automatically after parsing.
206 
207     It basically works like :func:`parse_set_header` just that items
208     may appear multiple times and case sensitivity is preserved.
209 
210     The return value is a standard :class:`list`:
211 
212     >>> parse_list_header('token, "quoted value"')
213     ['token', 'quoted value']
214 
215     To create a header from the :class:`list` again, use the
216     :func:`dump_header` function.
217 
218     :param value: a string with a list header.
219     :return: :class:`list`
220     """
221     result = []
222     for item in _parse_list_header(value):
223         if item[:1] == item[-1:] == '"':
224             item = unquote_header_value(item[1:-1])
225         result.append(item)
226     return result
227 
228 
229 # From mitsuhiko/werkzeug (used with permission).
230 def parse_dict_header(value):
231     """Parse lists of key, value pairs as described by RFC 2068 Section 2 and
232     convert them into a python dict:
233 
234     >>> d = parse_dict_header('foo="is a fish", bar="as well"')
235     >>> type(d) is dict
236     True
237     >>> sorted(d.items())
238     [('bar', 'as well'), ('foo', 'is a fish')]
239 
240     If there is no value for a key it will be `None`:
241 
242     >>> parse_dict_header('key_without_value')
243     {'key_without_value': None}
244 
245     To create a header from the :class:`dict` again, use the
246     :func:`dump_header` function.
247 
248     :param value: a string with a dict header.
249     :return: :class:`dict`
250     """
251     result = {}
252     for item in _parse_list_header(value):
253         if '=' not in item:
254             result[item] = None
255             continue
256         name, value = item.split('=', 1)
257         if value[:1] == value[-1:] == '"':
258             value = unquote_header_value(value[1:-1])
259         result[name] = value
260     return result
261 
262 
263 # From mitsuhiko/werkzeug (used with permission).
264 def unquote_header_value(value, is_filename=False):
265     r"""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
266     This does not use the real unquoting but what browsers are actually
267     using for quoting.
268 
269     :param value: the header value to unquote.
270     """
271     if value and value[0] == value[-1] == '"':
272         # this is not the real unquoting, but fixing this so that the
273         # RFC is met will result in bugs with internet explorer and
274         # probably some other browsers as well.  IE for example is
275         # uploading files with "C:\foo\bar.txt" as filename
276         value = value[1:-1]
277 
278         # if this is a filename and the starting characters look like
279         # a UNC path, then just return the value without quotes.  Using the
280         # replace sequence below on a UNC path has the effect of turning
281         # the leading double slash into a single slash and then
282         # _fix_ie_filename() doesn't work correctly.  See #458.
283         if not is_filename or value[:2] != '\\\\':
284             return value.replace('\\\\', '\\').replace('\\"', '"')
285     return value
286 
287 
288 def dict_from_cookiejar(cj):
289     """Returns a key/value dictionary from a CookieJar.
290 
291     :param cj: CookieJar object to extract cookies from.
292     """
293 
294     cookie_dict = {}
295 
296     for cookie in cj:
297         cookie_dict[cookie.name] = cookie.value
298 
299     return cookie_dict
300 
301 
302 def add_dict_to_cookiejar(cj, cookie_dict):
303     """Returns a CookieJar from a key/value dictionary.
304 
305     :param cj: CookieJar to insert cookies into.
306     :param cookie_dict: Dict of key/values to insert into CookieJar.
307     """
308 
309     cj2 = cookiejar_from_dict(cookie_dict)
310     cj.update(cj2)
311     return cj
312 
313 
314 def get_encodings_from_content(content):
315     """Returns encodings from given content string.
316 
317     :param content: bytestring to extract encodings from.
318     """
319     warnings.warn((
320         'In requests 3.0, get_encodings_from_content will be removed. For '
321         'more information, please see the discussion on issue #2266. (This'
322         ' warning should only appear once.)'),
323         DeprecationWarning)
324 
325     charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
326     pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
327     xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')
328 
329     return (charset_re.findall(content) +
330             pragma_re.findall(content) +
331             xml_re.findall(content))
332 
333 
334 def get_encoding_from_headers(headers):
335     """Returns encodings from given HTTP Header Dict.
336 
337     :param headers: dictionary to extract encoding from.
338     """
339 
340     content_type = headers.get('content-type')
341 
342     if not content_type:
343         return None
344 
345     content_type, params = cgi.parse_header(content_type)
346 
347     if 'charset' in params:
348         return params['charset'].strip("'\"")
349 
350     if 'text' in content_type:
351         return 'ISO-8859-1'
352 
353 
354 def stream_decode_response_unicode(iterator, r):
355     """Stream decodes a iterator."""
356 
357     if r.encoding is None:
358         for item in iterator:
359             yield item
360         return
361 
362     decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')
363     for chunk in iterator:
364         rv = decoder.decode(chunk)
365         if rv:
366             yield rv
367     rv = decoder.decode(b'', final=True)
368     if rv:
369         yield rv
370 
371 
372 def iter_slices(string, slice_length):
373     """Iterate over slices of a string."""
374     pos = 0
375     while pos < len(string):
376         yield string[pos:pos + slice_length]
377         pos += slice_length
378 
379 
380 def get_unicode_from_response(r):
381     """Returns the requested content back in unicode.
382 
383     :param r: Response object to get unicode content from.
384 
385     Tried:
386 
387     1. charset from content-type
388     2. fall back and replace all unicode characters
389 
390     """
391     warnings.warn((
392         'In requests 3.0, get_unicode_from_response will be removed. For '
393         'more information, please see the discussion on issue #2266. (This'
394         ' warning should only appear once.)'),
395         DeprecationWarning)
396 
397     tried_encodings = []
398 
399     # Try charset from content-type
400     encoding = get_encoding_from_headers(r.headers)
401 
402     if encoding:
403         try:
404             return str(r.content, encoding)
405         except UnicodeError:
406             tried_encodings.append(encoding)
407 
408     # Fall back:
409     try:
410         return str(r.content, encoding, errors='replace')
411     except TypeError:
412         return r.content
413 
414 
415 # The unreserved URI characters (RFC 3986)
416 UNRESERVED_SET = frozenset(
417     "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
418     + "0123456789-._~")
419 
420 
421 def unquote_unreserved(uri):
422     """Un-escape any percent-escape sequences in a URI that are unreserved
423     characters. This leaves all reserved, illegal and non-ASCII bytes encoded.
424     """
425     parts = uri.split('%')
426     for i in range(1, len(parts)):
427         h = parts[i][0:2]
428         if len(h) == 2 and h.isalnum():
429             try:
430                 c = chr(int(h, 16))
431             except ValueError:
432                 raise InvalidURL("Invalid percent-escape sequence: '%s'" % h)
433 
434             if c in UNRESERVED_SET:
435                 parts[i] = c + parts[i][2:]
436             else:
437                 parts[i] = '%' + parts[i]
438         else:
439             parts[i] = '%' + parts[i]
440     return ''.join(parts)
441 
442 
443 def requote_uri(uri):
444     """Re-quote the given URI.
445 
446     This function passes the given URI through an unquote/quote cycle to
447     ensure that it is fully and consistently quoted.
448     """
449     safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
450     safe_without_percent = "!#$&'()*+,/:;=?@[]~"
451     try:
452         # Unquote only the unreserved characters
453         # Then quote only illegal characters (do not quote reserved,
454         # unreserved, or '%')
455         return quote(unquote_unreserved(uri), safe=safe_with_percent)
456     except InvalidURL:
457         # We couldn't unquote the given URI, so let's try quoting it, but
458         # there may be unquoted '%'s in the URI. We need to make sure they're
459         # properly quoted so they do not cause issues elsewhere.
460         return quote(uri, safe=safe_without_percent)
461 
462 
463 def address_in_network(ip, net):
464     """
465     This function allows you to check if on IP belongs to a network subnet
466     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
467              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24
468     """
469     ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
470     netaddr, bits = net.split('/')
471     netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]
472     network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
473     return (ipaddr & netmask) == (network & netmask)
474 
475 
476 def dotted_netmask(mask):
477     """
478     Converts mask from /xx format to xxx.xxx.xxx.xxx
479     Example: if mask is 24 function returns 255.255.255.0
480     """
481     bits = 0xffffffff ^ (1 << 32 - mask) - 1
482     return socket.inet_ntoa(struct.pack('>I', bits))
483 
484 
485 def is_ipv4_address(string_ip):
486     try:
487         socket.inet_aton(string_ip)
488     except socket.error:
489         return False
490     return True
491 
492 
493 def is_valid_cidr(string_network):
494     """Very simple check of the cidr format in no_proxy variable"""
495     if string_network.count('/') == 1:
496         try:
497             mask = int(string_network.split('/')[1])
498         except ValueError:
499             return False
500 
501         if mask < 1 or mask > 32:
502             return False
503 
504         try:
505             socket.inet_aton(string_network.split('/')[0])
506         except socket.error:
507             return False
508     else:
509         return False
510     return True
511 
512 
513 def should_bypass_proxies(url):
514     """
515     Returns whether we should bypass proxies or not.
516     """
517     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
518 
519     # First check whether no_proxy is defined. If it is, check that the URL
520     # we're getting isn't in the no_proxy list.
521     no_proxy = get_proxy('no_proxy')
522     netloc = urlparse(url).netloc
523 
524     if no_proxy:
525         # We need to check whether we match here. We need to see if we match
526         # the end of the netloc, both with and without the port.
527         no_proxy = (
528             host for host in no_proxy.replace(' ', '').split(',') if host
529         )
530 
531         ip = netloc.split(':')[0]
532         if is_ipv4_address(ip):
533             for proxy_ip in no_proxy:
534                 if is_valid_cidr(proxy_ip):
535                     if address_in_network(ip, proxy_ip):
536                         return True
537         else:
538             for host in no_proxy:
539                 if netloc.endswith(host) or netloc.split(':')[0].endswith(host):
540                     # The URL does match something in no_proxy, so we don't want
541                     # to apply the proxies on this URL.
542                     return True
543 
544     # If the system proxy settings indicate that this URL should be bypassed,
545     # don't proxy.
546     # The proxy_bypass function is incredibly buggy on OS X in early versions
547     # of Python 2.6, so allow this call to fail. Only catch the specific
548     # exceptions we've seen, though: this call failing in other ways can reveal
549     # legitimate problems.
550     try:
551         bypass = proxy_bypass(netloc)
552     except (TypeError, socket.gaierror):
553         bypass = False
554 
555     if bypass:
556         return True
557 
558     return False
559 
560 def get_environ_proxies(url):
561     """Return a dict of environment proxies."""
562     if should_bypass_proxies(url):
563         return {}
564     else:
565         return getproxies()
566 
567 def select_proxy(url, proxies):
568     """Select a proxy for the url, if applicable.
569 
570     :param url: The url being for the request
571     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
572     """
573     proxies = proxies or {}
574     urlparts = urlparse(url)
575     proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)
576     if proxy is None:
577         proxy = proxies.get(urlparts.scheme)
578     return proxy
579 
580 def default_user_agent(name="python-requests"):
581     """Return a string representing the default user agent."""
582     return '%s/%s' % (name, __version__)
583 
584 
585 def default_headers():
586     return CaseInsensitiveDict({
587         'User-Agent': default_user_agent(),
588         'Accept-Encoding': ', '.join(('gzip', 'deflate')),
589         'Accept': '*/*',
590         'Connection': 'keep-alive',
591     })
592 
593 
594 def parse_header_links(value):
595     """Return a dict of parsed link headers proxies.
596 
597     i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"
598 
599     """
600 
601     links = []
602 
603     replace_chars = " '\""
604 
605     for val in re.split(", *<", value):
606         try:
607             url, params = val.split(";", 1)
608         except ValueError:
609             url, params = val, ''
610 
611         link = {}
612 
613         link["url"] = url.strip("<> '\"")
614 
615         for param in params.split(";"):
616             try:
617                 key, value = param.split("=")
618             except ValueError:
619                 break
620 
621             link[key.strip(replace_chars)] = value.strip(replace_chars)
622 
623         links.append(link)
624 
625     return links
626 
627 
628 # Null bytes; no need to recreate these on each call to guess_json_utf
629 _null = '\x00'.encode('ascii')  # encoding to ASCII for Python 3
630 _null2 = _null * 2
631 _null3 = _null * 3
632 
633 
634 def guess_json_utf(data):
635     # JSON always starts with two ASCII characters, so detection is as
636     # easy as counting the nulls and from their location and count
637     # determine the encoding. Also detect a BOM, if present.
638     sample = data[:4]
639     if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):
640         return 'utf-32'     # BOM included
641     if sample[:3] == codecs.BOM_UTF8:
642         return 'utf-8-sig'  # BOM included, MS style (discouraged)
643     if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
644         return 'utf-16'     # BOM included
645     nullcount = sample.count(_null)
646     if nullcount == 0:
647         return 'utf-8'
648     if nullcount == 2:
649         if sample[::2] == _null2:   # 1st and 3rd are null
650             return 'utf-16-be'
651         if sample[1::2] == _null2:  # 2nd and 4th are null
652             return 'utf-16-le'
653         # Did not detect 2 valid UTF-16 ascii-range characters
654     if nullcount == 3:
655         if sample[:3] == _null3:
656             return 'utf-32-be'
657         if sample[1:] == _null3:
658             return 'utf-32-le'
659         # Did not detect a valid UTF-32 ascii-range character
660     return None
661 
662 
663 def prepend_scheme_if_needed(url, new_scheme):
664     '''Given a URL that may or may not have a scheme, prepend the given scheme.
665     Does not replace a present scheme with the one provided as an argument.'''
666     scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)
667 
668     # urlparse is a finicky beast, and sometimes decides that there isn't a
669     # netloc present. Assume that it's being over-cautious, and switch netloc
670     # and path if urlparse decided there was no netloc.
671     if not netloc:
672         netloc, path = path, netloc
673 
674     return urlunparse((scheme, netloc, path, params, query, fragment))
675 
676 
677 def get_auth_from_url(url):
678     """Given a url with authentication components, extract them into a tuple of
679     username,password."""
680     parsed = urlparse(url)
681 
682     try:
683         auth = (unquote(parsed.username), unquote(parsed.password))
684     except (AttributeError, TypeError):
685         auth = ('', '')
686 
687     return auth
688 
689 
690 def to_native_string(string, encoding='ascii'):
691     """
692     Given a string object, regardless of type, returns a representation of that
693     string in the native string type, encoding and decoding where necessary.
694     This assumes ASCII unless told otherwise.
695     """
696     out = None
697 
698     if isinstance(string, builtin_str):
699         out = string
700     else:
701         if is_py2:
702             out = string.encode(encoding)
703         else:
704             out = string.decode(encoding)
705 
706     return out
707 
708 
709 def urldefragauth(url):
710     """
711     Given a url remove the fragment and the authentication part
712     """
713     scheme, netloc, path, params, query, fragment = urlparse(url)
714 
715     # see func:`prepend_scheme_if_needed`
716     if not netloc:
717         netloc, path = path, netloc
718 
719     netloc = netloc.rsplit('@', 1)[-1]
720 
721     return urlunparse((scheme, netloc, path, params, query, ''))

</code><intended_behavior>Functions within this file responsible for encoding URLs (quote, unquote, urlencode, or similar) should be modified or extended to handle non-ASCII characters in URLs and redirects appropriately. It must ensure that any URL or part of a URL passed to it, especially those extracted from redirects, is encoded (or decoded) in a way that preserves non-ASCII characters without corruption. This behavior is essential to maintain the validity of URLs and to ensure they are accessible regardless of the character set they contain.</intended_behavior>]
2025-10-15 21:22:22.733 | INFO     | Search completed. Bug locations: [<file>requests/sessions.py</file>
<class>Session</class>
<method>None</method>
<code>
268 class Session(SessionRedirectMixin):
269     """A Requests session.
270 
271     Provides cookie persistence, connection-pooling, and configuration.
272 
273     Basic Usage::
274 
275       >>> import requests
276       >>> s = requests.Session()
277       >>> s.get('http://httpbin.org/get')
278       <Response [200]>
279 
280     Or as a context manager::
281 
282       >>> with requests.Session() as s:
283       >>>     s.get('http://httpbin.org/get')
284       <Response [200]>
285     """
286 
287     __attrs__ = [
288         'headers', 'cookies', 'auth', 'proxies', 'hooks', 'params', 'verify',
289         'cert', 'prefetch', 'adapters', 'stream', 'trust_env',
290         'max_redirects',
291     ]
292 
293     def __init__(self):
294 
295         #: A case-insensitive dictionary of headers to be sent on each
296         #: :class:`Request <Request>` sent from this
297         #: :class:`Session <Session>`.
298         self.headers = default_headers()
299 
300         #: Default Authentication tuple or object to attach to
301         #: :class:`Request <Request>`.
302         self.auth = None
303 
304         #: Dictionary mapping protocol or protocol and host to the URL of the proxy
305         #: (e.g. {'http': 'foo.bar:3128', 'http://host.name': 'foo.bar:4012'}) to
306         #: be used on each :class:`Request <Request>`.
307         self.proxies = {}
308 
309         #: Event-handling hooks.
310         self.hooks = default_hooks()
311 
312         #: Dictionary of querystring data to attach to each
313         #: :class:`Request <Request>`. The dictionary values may be lists for
314         #: representing multivalued query parameters.
315         self.params = {}
316 
317         #: Stream response content default.
318         self.stream = False
319 
320         #: SSL Verification default.
321         self.verify = True
322 
323         #: SSL certificate default.
324         self.cert = None
325 
326         #: Maximum number of redirects allowed. If the request exceeds this
327         #: limit, a :class:`TooManyRedirects` exception is raised.
328         self.max_redirects = DEFAULT_REDIRECT_LIMIT
329 
330         #: Trust environment settings for proxy configuration, default
331         #: authentication and similar.
332         self.trust_env = True
333 
334         #: A CookieJar containing all currently outstanding cookies set on this
335         #: session. By default it is a
336         #: :class:`RequestsCookieJar <requests.cookies.RequestsCookieJar>`, but
337         #: may be any other ``cookielib.CookieJar`` compatible object.
338         self.cookies = cookiejar_from_dict({})
339 
340         # Default connection adapters.
341         self.adapters = OrderedDict()
342         self.mount('https://', HTTPAdapter())
343         self.mount('http://', HTTPAdapter())
344 
345         # Only store 1000 redirects to prevent using infinite memory
346         self.redirect_cache = RecentlyUsedContainer(REDIRECT_CACHE_SIZE)
347 
348     def __enter__(self):
349         return self
350 
351     def __exit__(self, *args):
352         self.close()
353 
354     def prepare_request(self, request):
355         """Constructs a :class:`PreparedRequest <PreparedRequest>` for
356         transmission and returns it. The :class:`PreparedRequest` has settings
357         merged from the :class:`Request <Request>` instance and those of the
358         :class:`Session`.
359 
360         :param request: :class:`Request` instance to prepare with this
361             Session's settings.
362         """
363         cookies = request.cookies or {}
364 
365         # Bootstrap CookieJar.
366         if not isinstance(cookies, cookielib.CookieJar):
367             cookies = cookiejar_from_dict(cookies)
368 
369         # Merge with session cookies
370         merged_cookies = merge_cookies(
371             merge_cookies(RequestsCookieJar(), self.cookies), cookies)
372 
373 
374         # Set environment's basic authentication if not explicitly set.
375         auth = request.auth
376         if self.trust_env and not auth and not self.auth:
377             auth = get_netrc_auth(request.url)
378 
379         p = PreparedRequest()
380         p.prepare(
381             method=request.method.upper(),
382             url=request.url,
383             files=request.files,
384             data=request.data,
385             json=request.json,
386             headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),
387             params=merge_setting(request.params, self.params),
388             auth=merge_setting(auth, self.auth),
389             cookies=merged_cookies,
390             hooks=merge_hooks(request.hooks, self.hooks),
391         )
392         return p
393 
394     def request(self, method, url,
395         params=None,
396         data=None,
397         headers=None,
398         cookies=None,
399         files=None,
400         auth=None,
401         timeout=None,
402         allow_redirects=True,
403         proxies=None,
404         hooks=None,
405         stream=None,
406         verify=None,
407         cert=None,
408         json=None):
409         """Constructs a :class:`Request <Request>`, prepares it, and sends it.
410         Returns :class:`Response <Response>` object.
411 
412         :param method: method for the new :class:`Request` object.
413         :param url: URL for the new :class:`Request` object.
414         :param params: (optional) Dictionary or bytes to be sent in the query
415             string for the :class:`Request`.
416         :param data: (optional) Dictionary, bytes, or file-like object to send
417             in the body of the :class:`Request`.
418         :param json: (optional) json to send in the body of the
419             :class:`Request`.
420         :param headers: (optional) Dictionary of HTTP Headers to send with the
421             :class:`Request`.
422         :param cookies: (optional) Dict or CookieJar object to send with the
423             :class:`Request`.
424         :param files: (optional) Dictionary of ``'filename': file-like-objects``
425             for multipart encoding upload.
426         :param auth: (optional) Auth tuple or callable to enable
427             Basic/Digest/Custom HTTP Auth.
428         :param timeout: (optional) How long to wait for the server to send
429             data before giving up, as a float, or a :ref:`(connect timeout,
430             read timeout) <timeouts>` tuple.
431         :type timeout: float or tuple
432         :param allow_redirects: (optional) Set to True by default.
433         :type allow_redirects: bool
434         :param proxies: (optional) Dictionary mapping protocol or protocol and
435             hostname to the URL of the proxy.
436         :param stream: (optional) whether to immediately download the response
437             content. Defaults to ``False``.
438         :param verify: (optional) whether the SSL cert will be verified.
439             A CA_BUNDLE path can also be provided. Defaults to ``True``.
440         :param cert: (optional) if String, path to ssl client cert file (.pem).
441             If Tuple, ('cert', 'key') pair.
442         """
443         # Create the Request.
444         req = Request(
445             method = method.upper(),
446             url = url,
447             headers = headers,
448             files = files,
449             data = data or {},
450             json = json,
451             params = params or {},
452             auth = auth,
453             cookies = cookies,
454             hooks = hooks,
455         )
456         prep = self.prepare_request(req)
457 
458         proxies = proxies or {}
459 
460         settings = self.merge_environment_settings(
461             prep.url, proxies, stream, verify, cert
462         )
463 
464         # Send the request.
465         send_kwargs = {
466             'timeout': timeout,
467             'allow_redirects': allow_redirects,
468         }
469         send_kwargs.update(settings)
470         resp = self.send(prep, **send_kwargs)
471 
472         return resp
473 
474     def get(self, url, **kwargs):
475         """Sends a GET request. Returns :class:`Response` object.
476 
477         :param url: URL for the new :class:`Request` object.
478         :param \*\*kwargs: Optional arguments that ``request`` takes.
479         """
480 
481         kwargs.setdefault('allow_redirects', True)
482         return self.request('GET', url, **kwargs)
483 
484     def options(self, url, **kwargs):
485         """Sends a OPTIONS request. Returns :class:`Response` object.
486 
487         :param url: URL for the new :class:`Request` object.
488         :param \*\*kwargs: Optional arguments that ``request`` takes.
489         """
490 
491         kwargs.setdefault('allow_redirects', True)
492         return self.request('OPTIONS', url, **kwargs)
493 
494     def head(self, url, **kwargs):
495         """Sends a HEAD request. Returns :class:`Response` object.
496 
497         :param url: URL for the new :class:`Request` object.
498         :param \*\*kwargs: Optional arguments that ``request`` takes.
499         """
500 
501         kwargs.setdefault('allow_redirects', False)
502         return self.request('HEAD', url, **kwargs)
503 
504     def post(self, url, data=None, json=None, **kwargs):
505         """Sends a POST request. Returns :class:`Response` object.
506 
507         :param url: URL for the new :class:`Request` object.
508         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.
509         :param json: (optional) json to send in the body of the :class:`Request`.
510         :param \*\*kwargs: Optional arguments that ``request`` takes.
511         """
512 
513         return self.request('POST', url, data=data, json=json, **kwargs)
514 
515     def put(self, url, data=None, **kwargs):
516         """Sends a PUT request. Returns :class:`Response` object.
517 
518         :param url: URL for the new :class:`Request` object.
519         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.
520         :param \*\*kwargs: Optional arguments that ``request`` takes.
521         """
522 
523         return self.request('PUT', url, data=data, **kwargs)
524 
525     def patch(self, url, data=None, **kwargs):
526         """Sends a PATCH request. Returns :class:`Response` object.
527 
528         :param url: URL for the new :class:`Request` object.
529         :param data: (optional) Dictionary, bytes, or file-like object to send in the body of the :class:`Request`.
530         :param \*\*kwargs: Optional arguments that ``request`` takes.
531         """
532 
533         return self.request('PATCH', url,  data=data, **kwargs)
534 
535     def delete(self, url, **kwargs):
536         """Sends a DELETE request. Returns :class:`Response` object.
537 
538         :param url: URL for the new :class:`Request` object.
539         :param \*\*kwargs: Optional arguments that ``request`` takes.
540         """
541 
542         return self.request('DELETE', url, **kwargs)
543 
544     def send(self, request, **kwargs):
545         """Send a given PreparedRequest."""
546         # Set defaults that the hooks can utilize to ensure they always have
547         # the correct parameters to reproduce the previous request.
548         kwargs.setdefault('stream', self.stream)
549         kwargs.setdefault('verify', self.verify)
550         kwargs.setdefault('cert', self.cert)
551         kwargs.setdefault('proxies', self.proxies)
552 
553         # It's possible that users might accidentally send a Request object.
554         # Guard against that specific failure case.
555         if not isinstance(request, PreparedRequest):
556             raise ValueError('You can only send PreparedRequests.')
557 
558         # Automatically skip a redirect chain if we've already followed it before.
559         checked_urls = set()
560         while request.url in self.redirect_cache:
561             checked_urls.add(request.url)
562             new_url = self.redirect_cache.get(request.url)
563             if new_url in checked_urls:
564                 break
565             request.url = new_url
566 
567         # Set-up variables for resolve_redirects and dispatching of hooks.
568         allow_redirects = kwargs.pop('allow_redirects', True)
569         stream = kwargs.get('stream')
570         hooks = request.hooks
571 
572         # Get the appropriate adapter to use.
573         adapter = self.get_adapter(url=request.url)
574 
575         # Start time (approximately) of the request.
576         start = datetime.utcnow()
577 
578         # Send the request
579         r = adapter.send(request, **kwargs)
580 
581         # Total elapsed time of the request (approximately).
582         r.elapsed = datetime.utcnow() - start
583 
584         # Response manipulation hooks.
585         r = dispatch_hook('response', hooks, r, **kwargs)
586 
587         # Persist cookies
588         if r.history:
589 
590             # If the hooks create history then we want those cookies too
591             for resp in r.history:
592                 extract_cookies_to_jar(self.cookies, resp.request, resp.raw)
593 
594         extract_cookies_to_jar(self.cookies, request, r.raw)
595 
596         # Redirect resolving generator.
597         gen = self.resolve_redirects(r, **kwargs)
598 
599         # Resolve redirects, if allowed.
600         history = [resp for resp in gen] if allow_redirects else []
601 
602         # Shuffle things around if there's redirection history.
603         if history:
604             # Insert the first (original) Response at the start.
605             history.insert(0, r)
606 
607             # Remove the final response from history, use it as our Response.
608             r = history.pop()
609 
610             # Save redirection history to final Response object.
611             r.history = history
612 
613         # Automatically download response body, if not in streaming mode.
614         if not stream:
615             r.content
616 
617         return r
618 
619     def merge_environment_settings(self, url, proxies, stream, verify, cert):
620         """Check the environment and merge it with some settings."""
621         # Gather clues from the surrounding environment.
622         if self.trust_env:
623             # Set environment's proxies.
624             env_proxies = get_environ_proxies(url) or {}
625             for (k, v) in env_proxies.items():
626                 proxies.setdefault(k, v)
627 
628             # Look for requests environment configuration and be compatible
629             # with cURL.
630             if verify is True or verify is None:
631                 verify = (os.environ.get('REQUESTS_CA_BUNDLE') or
632                           os.environ.get('CURL_CA_BUNDLE'))
633 
634         # Merge all the kwargs.
635         proxies = merge_setting(proxies, self.proxies)
636         stream = merge_setting(stream, self.stream)
637         verify = merge_setting(verify, self.verify)
638         cert = merge_setting(cert, self.cert)
639 
640         return {'verify': verify, 'proxies': proxies, 'stream': stream,
641                 'cert': cert}
642 
643     def get_adapter(self, url):
644         """Returns the appropriate connection adapter for the given URL."""
645         for (prefix, adapter) in self.adapters.items():
646 
647             if url.lower().startswith(prefix):
648                 return adapter
649 
650         # Nothing matches :-/
651         raise InvalidScheme("No connection adapters were found for '%s'" % url)
652 
653     def close(self):
654         """Closes all adapters and, as such, the Session."""
655         for v in self.adapters.values():
656             v.close()
657 
658     def mount(self, prefix, adapter):
659         """Registers a connection adapter to a prefix.
660 
661         Adapters are sorted in descending order by key length."""
662 
663         self.adapters[prefix] = adapter
664         keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]
665 
666         for key in keys_to_move:
667             self.adapters[key] = self.adapters.pop(key)
668 
669     def __getstate__(self):
670         state = dict((attr, getattr(self, attr, None)) for attr in self.__attrs__)
671         state['redirect_cache'] = dict(self.redirect_cache)
672         return state
673 
674     def __setstate__(self, state):
675         redirect_cache = state.pop('redirect_cache', {})
676         for attr, value in state.items():
677             setattr(self, attr, value)
678 
679         self.redirect_cache = RecentlyUsedContainer(REDIRECT_CACHE_SIZE)
680         for redirect, to in redirect_cache.items():
681             self.redirect_cache[redirect] = to

</code><intended_behavior>This method needs to ensure that when a redirect URL is encountered, especially those containing non-ASCII characters, it is properly encoded to preserve the original characters. The method must validate and, if necessary, re-encode the URL from the 'Location' header in the redirect response to ensure that non-ASCII characters are correctly interpreted and called by the requests. This might involve using the quote and unquote functions appropriately to handle URLs with non-ASCII characters. Additionally, the method should include logic to debug or log encoding issues for easier troubleshooting of similar issues in the future.</intended_behavior>, <file>requests/utils.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.utils
5 ~~~~~~~~~~~~~~
6 
7 This module provides utility functions that are used within Requests
8 that are also useful for external consumption.
9 
10 """
11 
12 import cgi
13 import codecs
14 import collections
15 import io
16 import os
17 import platform
18 import re
19 import sys
20 import socket
21 import struct
22 import warnings
23 
24 from . import __version__
25 from . import certs
26 from .compat import parse_http_list as _parse_list_header
27 from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,
28                      builtin_str, getproxies, proxy_bypass, urlunparse,
29                      basestring)
30 from .cookies import RequestsCookieJar, cookiejar_from_dict
31 from .structures import CaseInsensitiveDict
32 from .exceptions import InvalidURL, FileModeWarning
33 
34 _hush_pyflakes = (RequestsCookieJar,)
35 
36 NETRC_FILES = ('.netrc', '_netrc')
37 
38 DEFAULT_CA_BUNDLE_PATH = certs.where()
39 
40 
41 def dict_to_sequence(d):
42     """Returns an internal sequence dictionary update."""
43 
44     if hasattr(d, 'items'):
45         d = d.items()
46 
47     return d
48 
49 
50 def super_len(o):
51     total_length = 0
52     current_position = 0
53 
54     if hasattr(o, '__len__'):
55         total_length = len(o)
56 
57     elif hasattr(o, 'len'):
58         total_length = o.len
59 
60     elif hasattr(o, 'getvalue'):
61         # e.g. BytesIO, cStringIO.StringIO
62         total_length = len(o.getvalue())
63 
64     elif hasattr(o, 'fileno'):
65         try:
66             fileno = o.fileno()
67         except io.UnsupportedOperation:
68             pass
69         else:
70             total_length = os.fstat(fileno).st_size
71 
72             # Having used fstat to determine the file length, we need to
73             # confirm that this file was opened up in binary mode.
74             if 'b' not in o.mode:
75                 warnings.warn((
76                     "Requests has determined the content-length for this "
77                     "request using the binary size of the file: however, the "
78                     "file has been opened in text mode (i.e. without the 'b' "
79                     "flag in the mode). This may lead to an incorrect "
80                     "content-length. In Requests 3.0, support will be removed "
81                     "for files in text mode."),
82                     FileModeWarning
83                 )
84 
85     if hasattr(o, 'tell'):
86         current_position = o.tell()
87 
88     return max(0, total_length - current_position)
89 
90 
91 def get_netrc_auth(url, raise_errors=False):
92     """Returns the Requests tuple auth for a given url from netrc."""
93 
94     try:
95         from netrc import netrc, NetrcParseError
96 
97         netrc_path = None
98 
99         for f in NETRC_FILES:
100             try:
101                 loc = os.path.expanduser('~/{0}'.format(f))
102             except KeyError:
103                 # os.path.expanduser can fail when $HOME is undefined and
104                 # getpwuid fails. See http://bugs.python.org/issue20164 &
105                 # https://github.com/kennethreitz/requests/issues/1846
106                 return
107 
108             if os.path.exists(loc):
109                 netrc_path = loc
110                 break
111 
112         # Abort early if there isn't one.
113         if netrc_path is None:
114             return
115 
116         ri = urlparse(url)
117 
118         # Strip port numbers from netloc. This weird `if...encode`` dance is
119         # used for Python 3.2, which doesn't support unicode literals.
120         splitstr = b':'
121         if isinstance(url, str):
122             splitstr = splitstr.decode('ascii')
123         host = ri.netloc.split(splitstr)[0]
124 
125         try:
126             _netrc = netrc(netrc_path).authenticators(host)
127             if _netrc:
128                 # Return with login / password
129                 login_i = (0 if _netrc[0] else 1)
130                 return (_netrc[login_i], _netrc[2])
131         except (NetrcParseError, IOError):
132             # If there was a parsing error or a permissions issue reading the file,
133             # we'll just skip netrc auth unless explicitly asked to raise errors.
134             if raise_errors:
135                 raise
136 
137     # AppEngine hackiness.
138     except (ImportError, AttributeError):
139         pass
140 
141 
142 def guess_filename(obj):
143     """Tries to guess the filename of the given object."""
144     name = getattr(obj, 'name', None)
145     if (name and isinstance(name, basestring) and name[0] != '<' and
146             name[-1] != '>'):
147         return os.path.basename(name)
148 
149 
150 def from_key_val_list(value):
151     """Take an object and test to see if it can be represented as a
152     dictionary. Unless it can not be represented as such, return an
153     OrderedDict, e.g.,
154 
155     ::
156 
157         >>> from_key_val_list([('key', 'val')])
158         OrderedDict([('key', 'val')])
159         >>> from_key_val_list('string')
160         ValueError: need more than 1 value to unpack
161         >>> from_key_val_list({'key': 'val'})
162         OrderedDict([('key', 'val')])
163     """
164     if value is None:
165         return None
166 
167     if isinstance(value, (str, bytes, bool, int)):
168         raise ValueError('cannot encode objects that are not 2-tuples')
169 
170     return OrderedDict(value)
171 
172 
173 def to_key_val_list(value):
174     """Take an object and test to see if it can be represented as a
175     dictionary. If it can be, return a list of tuples, e.g.,
176 
177     ::
178 
179         >>> to_key_val_list([('key', 'val')])
180         [('key', 'val')]
181         >>> to_key_val_list({'key': 'val'})
182         [('key', 'val')]
183         >>> to_key_val_list('string')
184         ValueError: cannot encode objects that are not 2-tuples.
185     """
186     if value is None:
187         return None
188 
189     if isinstance(value, (str, bytes, bool, int)):
190         raise ValueError('cannot encode objects that are not 2-tuples')
191 
192     if isinstance(value, collections.Mapping):
193         value = value.items()
194 
195     return list(value)
196 
197 
198 # From mitsuhiko/werkzeug (used with permission).
199 def parse_list_header(value):
200     """Parse lists as described by RFC 2068 Section 2.
201 
202     In particular, parse comma-separated lists where the elements of
203     the list may include quoted-strings.  A quoted-string could
204     contain a comma.  A non-quoted string could have quotes in the
205     middle.  Quotes are removed automatically after parsing.
206 
207     It basically works like :func:`parse_set_header` just that items
208     may appear multiple times and case sensitivity is preserved.
209 
210     The return value is a standard :class:`list`:
211 
212     >>> parse_list_header('token, "quoted value"')
213     ['token', 'quoted value']
214 
215     To create a header from the :class:`list` again, use the
216     :func:`dump_header` function.
217 
218     :param value: a string with a list header.
219     :return: :class:`list`
220     """
221     result = []
222     for item in _parse_list_header(value):
223         if item[:1] == item[-1:] == '"':
224             item = unquote_header_value(item[1:-1])
225         result.append(item)
226     return result
227 
228 
229 # From mitsuhiko/werkzeug (used with permission).
230 def parse_dict_header(value):
231     """Parse lists of key, value pairs as described by RFC 2068 Section 2 and
232     convert them into a python dict:
233 
234     >>> d = parse_dict_header('foo="is a fish", bar="as well"')
235     >>> type(d) is dict
236     True
237     >>> sorted(d.items())
238     [('bar', 'as well'), ('foo', 'is a fish')]
239 
240     If there is no value for a key it will be `None`:
241 
242     >>> parse_dict_header('key_without_value')
243     {'key_without_value': None}
244 
245     To create a header from the :class:`dict` again, use the
246     :func:`dump_header` function.
247 
248     :param value: a string with a dict header.
249     :return: :class:`dict`
250     """
251     result = {}
252     for item in _parse_list_header(value):
253         if '=' not in item:
254             result[item] = None
255             continue
256         name, value = item.split('=', 1)
257         if value[:1] == value[-1:] == '"':
258             value = unquote_header_value(value[1:-1])
259         result[name] = value
260     return result
261 
262 
263 # From mitsuhiko/werkzeug (used with permission).
264 def unquote_header_value(value, is_filename=False):
265     r"""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
266     This does not use the real unquoting but what browsers are actually
267     using for quoting.
268 
269     :param value: the header value to unquote.
270     """
271     if value and value[0] == value[-1] == '"':
272         # this is not the real unquoting, but fixing this so that the
273         # RFC is met will result in bugs with internet explorer and
274         # probably some other browsers as well.  IE for example is
275         # uploading files with "C:\foo\bar.txt" as filename
276         value = value[1:-1]
277 
278         # if this is a filename and the starting characters look like
279         # a UNC path, then just return the value without quotes.  Using the
280         # replace sequence below on a UNC path has the effect of turning
281         # the leading double slash into a single slash and then
282         # _fix_ie_filename() doesn't work correctly.  See #458.
283         if not is_filename or value[:2] != '\\\\':
284             return value.replace('\\\\', '\\').replace('\\"', '"')
285     return value
286 
287 
288 def dict_from_cookiejar(cj):
289     """Returns a key/value dictionary from a CookieJar.
290 
291     :param cj: CookieJar object to extract cookies from.
292     """
293 
294     cookie_dict = {}
295 
296     for cookie in cj:
297         cookie_dict[cookie.name] = cookie.value
298 
299     return cookie_dict
300 
301 
302 def add_dict_to_cookiejar(cj, cookie_dict):
303     """Returns a CookieJar from a key/value dictionary.
304 
305     :param cj: CookieJar to insert cookies into.
306     :param cookie_dict: Dict of key/values to insert into CookieJar.
307     """
308 
309     cj2 = cookiejar_from_dict(cookie_dict)
310     cj.update(cj2)
311     return cj
312 
313 
314 def get_encodings_from_content(content):
315     """Returns encodings from given content string.
316 
317     :param content: bytestring to extract encodings from.
318     """
319     warnings.warn((
320         'In requests 3.0, get_encodings_from_content will be removed. For '
321         'more information, please see the discussion on issue #2266. (This'
322         ' warning should only appear once.)'),
323         DeprecationWarning)
324 
325     charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
326     pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
327     xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')
328 
329     return (charset_re.findall(content) +
330             pragma_re.findall(content) +
331             xml_re.findall(content))
332 
333 
334 def get_encoding_from_headers(headers):
335     """Returns encodings from given HTTP Header Dict.
336 
337     :param headers: dictionary to extract encoding from.
338     """
339 
340     content_type = headers.get('content-type')
341 
342     if not content_type:
343         return None
344 
345     content_type, params = cgi.parse_header(content_type)
346 
347     if 'charset' in params:
348         return params['charset'].strip("'\"")
349 
350     if 'text' in content_type:
351         return 'ISO-8859-1'
352 
353 
354 def stream_decode_response_unicode(iterator, r):
355     """Stream decodes a iterator."""
356 
357     if r.encoding is None:
358         for item in iterator:
359             yield item
360         return
361 
362     decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')
363     for chunk in iterator:
364         rv = decoder.decode(chunk)
365         if rv:
366             yield rv
367     rv = decoder.decode(b'', final=True)
368     if rv:
369         yield rv
370 
371 
372 def iter_slices(string, slice_length):
373     """Iterate over slices of a string."""
374     pos = 0
375     while pos < len(string):
376         yield string[pos:pos + slice_length]
377         pos += slice_length
378 
379 
380 def get_unicode_from_response(r):
381     """Returns the requested content back in unicode.
382 
383     :param r: Response object to get unicode content from.
384 
385     Tried:
386 
387     1. charset from content-type
388     2. fall back and replace all unicode characters
389 
390     """
391     warnings.warn((
392         'In requests 3.0, get_unicode_from_response will be removed. For '
393         'more information, please see the discussion on issue #2266. (This'
394         ' warning should only appear once.)'),
395         DeprecationWarning)
396 
397     tried_encodings = []
398 
399     # Try charset from content-type
400     encoding = get_encoding_from_headers(r.headers)
401 
402     if encoding:
403         try:
404             return str(r.content, encoding)
405         except UnicodeError:
406             tried_encodings.append(encoding)
407 
408     # Fall back:
409     try:
410         return str(r.content, encoding, errors='replace')
411     except TypeError:
412         return r.content
413 
414 
415 # The unreserved URI characters (RFC 3986)
416 UNRESERVED_SET = frozenset(
417     "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
418     + "0123456789-._~")
419 
420 
421 def unquote_unreserved(uri):
422     """Un-escape any percent-escape sequences in a URI that are unreserved
423     characters. This leaves all reserved, illegal and non-ASCII bytes encoded.
424     """
425     parts = uri.split('%')
426     for i in range(1, len(parts)):
427         h = parts[i][0:2]
428         if len(h) == 2 and h.isalnum():
429             try:
430                 c = chr(int(h, 16))
431             except ValueError:
432                 raise InvalidURL("Invalid percent-escape sequence: '%s'" % h)
433 
434             if c in UNRESERVED_SET:
435                 parts[i] = c + parts[i][2:]
436             else:
437                 parts[i] = '%' + parts[i]
438         else:
439             parts[i] = '%' + parts[i]
440     return ''.join(parts)
441 
442 
443 def requote_uri(uri):
444     """Re-quote the given URI.
445 
446     This function passes the given URI through an unquote/quote cycle to
447     ensure that it is fully and consistently quoted.
448     """
449     safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
450     safe_without_percent = "!#$&'()*+,/:;=?@[]~"
451     try:
452         # Unquote only the unreserved characters
453         # Then quote only illegal characters (do not quote reserved,
454         # unreserved, or '%')
455         return quote(unquote_unreserved(uri), safe=safe_with_percent)
456     except InvalidURL:
457         # We couldn't unquote the given URI, so let's try quoting it, but
458         # there may be unquoted '%'s in the URI. We need to make sure they're
459         # properly quoted so they do not cause issues elsewhere.
460         return quote(uri, safe=safe_without_percent)
461 
462 
463 def address_in_network(ip, net):
464     """
465     This function allows you to check if on IP belongs to a network subnet
466     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
467              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24
468     """
469     ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
470     netaddr, bits = net.split('/')
471     netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]
472     network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
473     return (ipaddr & netmask) == (network & netmask)
474 
475 
476 def dotted_netmask(mask):
477     """
478     Converts mask from /xx format to xxx.xxx.xxx.xxx
479     Example: if mask is 24 function returns 255.255.255.0
480     """
481     bits = 0xffffffff ^ (1 << 32 - mask) - 1
482     return socket.inet_ntoa(struct.pack('>I', bits))
483 
484 
485 def is_ipv4_address(string_ip):
486     try:
487         socket.inet_aton(string_ip)
488     except socket.error:
489         return False
490     return True
491 
492 
493 def is_valid_cidr(string_network):
494     """Very simple check of the cidr format in no_proxy variable"""
495     if string_network.count('/') == 1:
496         try:
497             mask = int(string_network.split('/')[1])
498         except ValueError:
499             return False
500 
501         if mask < 1 or mask > 32:
502             return False
503 
504         try:
505             socket.inet_aton(string_network.split('/')[0])
506         except socket.error:
507             return False
508     else:
509         return False
510     return True
511 
512 
513 def should_bypass_proxies(url):
514     """
515     Returns whether we should bypass proxies or not.
516     """
517     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
518 
519     # First check whether no_proxy is defined. If it is, check that the URL
520     # we're getting isn't in the no_proxy list.
521     no_proxy = get_proxy('no_proxy')
522     netloc = urlparse(url).netloc
523 
524     if no_proxy:
525         # We need to check whether we match here. We need to see if we match
526         # the end of the netloc, both with and without the port.
527         no_proxy = (
528             host for host in no_proxy.replace(' ', '').split(',') if host
529         )
530 
531         ip = netloc.split(':')[0]
532         if is_ipv4_address(ip):
533             for proxy_ip in no_proxy:
534                 if is_valid_cidr(proxy_ip):
535                     if address_in_network(ip, proxy_ip):
536                         return True
537         else:
538             for host in no_proxy:
539                 if netloc.endswith(host) or netloc.split(':')[0].endswith(host):
540                     # The URL does match something in no_proxy, so we don't want
541                     # to apply the proxies on this URL.
542                     return True
543 
544     # If the system proxy settings indicate that this URL should be bypassed,
545     # don't proxy.
546     # The proxy_bypass function is incredibly buggy on OS X in early versions
547     # of Python 2.6, so allow this call to fail. Only catch the specific
548     # exceptions we've seen, though: this call failing in other ways can reveal
549     # legitimate problems.
550     try:
551         bypass = proxy_bypass(netloc)
552     except (TypeError, socket.gaierror):
553         bypass = False
554 
555     if bypass:
556         return True
557 
558     return False
559 
560 def get_environ_proxies(url):
561     """Return a dict of environment proxies."""
562     if should_bypass_proxies(url):
563         return {}
564     else:
565         return getproxies()
566 
567 def select_proxy(url, proxies):
568     """Select a proxy for the url, if applicable.
569 
570     :param url: The url being for the request
571     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
572     """
573     proxies = proxies or {}
574     urlparts = urlparse(url)
575     proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)
576     if proxy is None:
577         proxy = proxies.get(urlparts.scheme)
578     return proxy
579 
580 def default_user_agent(name="python-requests"):
581     """Return a string representing the default user agent."""
582     return '%s/%s' % (name, __version__)
583 
584 
585 def default_headers():
586     return CaseInsensitiveDict({
587         'User-Agent': default_user_agent(),
588         'Accept-Encoding': ', '.join(('gzip', 'deflate')),
589         'Accept': '*/*',
590         'Connection': 'keep-alive',
591     })
592 
593 
594 def parse_header_links(value):
595     """Return a dict of parsed link headers proxies.
596 
597     i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"
598 
599     """
600 
601     links = []
602 
603     replace_chars = " '\""
604 
605     for val in re.split(", *<", value):
606         try:
607             url, params = val.split(";", 1)
608         except ValueError:
609             url, params = val, ''
610 
611         link = {}
612 
613         link["url"] = url.strip("<> '\"")
614 
615         for param in params.split(";"):
616             try:
617                 key, value = param.split("=")
618             except ValueError:
619                 break
620 
621             link[key.strip(replace_chars)] = value.strip(replace_chars)
622 
623         links.append(link)
624 
625     return links
626 
627 
628 # Null bytes; no need to recreate these on each call to guess_json_utf
629 _null = '\x00'.encode('ascii')  # encoding to ASCII for Python 3
630 _null2 = _null * 2
631 _null3 = _null * 3
632 
633 
634 def guess_json_utf(data):
635     # JSON always starts with two ASCII characters, so detection is as
636     # easy as counting the nulls and from their location and count
637     # determine the encoding. Also detect a BOM, if present.
638     sample = data[:4]
639     if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):
640         return 'utf-32'     # BOM included
641     if sample[:3] == codecs.BOM_UTF8:
642         return 'utf-8-sig'  # BOM included, MS style (discouraged)
643     if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
644         return 'utf-16'     # BOM included
645     nullcount = sample.count(_null)
646     if nullcount == 0:
647         return 'utf-8'
648     if nullcount == 2:
649         if sample[::2] == _null2:   # 1st and 3rd are null
650             return 'utf-16-be'
651         if sample[1::2] == _null2:  # 2nd and 4th are null
652             return 'utf-16-le'
653         # Did not detect 2 valid UTF-16 ascii-range characters
654     if nullcount == 3:
655         if sample[:3] == _null3:
656             return 'utf-32-be'
657         if sample[1:] == _null3:
658             return 'utf-32-le'
659         # Did not detect a valid UTF-32 ascii-range character
660     return None
661 
662 
663 def prepend_scheme_if_needed(url, new_scheme):
664     '''Given a URL that may or may not have a scheme, prepend the given scheme.
665     Does not replace a present scheme with the one provided as an argument.'''
666     scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)
667 
668     # urlparse is a finicky beast, and sometimes decides that there isn't a
669     # netloc present. Assume that it's being over-cautious, and switch netloc
670     # and path if urlparse decided there was no netloc.
671     if not netloc:
672         netloc, path = path, netloc
673 
674     return urlunparse((scheme, netloc, path, params, query, fragment))
675 
676 
677 def get_auth_from_url(url):
678     """Given a url with authentication components, extract them into a tuple of
679     username,password."""
680     parsed = urlparse(url)
681 
682     try:
683         auth = (unquote(parsed.username), unquote(parsed.password))
684     except (AttributeError, TypeError):
685         auth = ('', '')
686 
687     return auth
688 
689 
690 def to_native_string(string, encoding='ascii'):
691     """
692     Given a string object, regardless of type, returns a representation of that
693     string in the native string type, encoding and decoding where necessary.
694     This assumes ASCII unless told otherwise.
695     """
696     out = None
697 
698     if isinstance(string, builtin_str):
699         out = string
700     else:
701         if is_py2:
702             out = string.encode(encoding)
703         else:
704             out = string.decode(encoding)
705 
706     return out
707 
708 
709 def urldefragauth(url):
710     """
711     Given a url remove the fragment and the authentication part
712     """
713     scheme, netloc, path, params, query, fragment = urlparse(url)
714 
715     # see func:`prepend_scheme_if_needed`
716     if not netloc:
717         netloc, path = path, netloc
718 
719     netloc = netloc.rsplit('@', 1)[-1]
720 
721     return urlunparse((scheme, netloc, path, params, query, ''))

</code><intended_behavior>Functions within this file responsible for encoding URLs (quote, unquote, urlencode, or similar) should be modified or extended to handle non-ASCII characters in URLs and redirects appropriately. It must ensure that any URL or part of a URL passed to it, especially those extracted from redirects, is encoded (or decoded) in a way that preserves non-ASCII characters without corruption. This behavior is essential to maintain the validity of URLs and to ensure they are accessible regardless of the character set they contain.</intended_behavior>]
2025-10-15 21:22:22.734 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 21:22:22.734 | INFO     | Search-only: skipping patch selection
2025-10-15 21:22:22.798 | INFO     | Task psf__requests-2754 completed successfully.
2025-10-15 21:22:22.799 | INFO     | No patch generated. You can try running ACR again.
