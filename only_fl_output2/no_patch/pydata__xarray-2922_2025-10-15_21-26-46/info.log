2025-10-15 21:26:46.504 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 21:26:48.753 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=137, output_tokens=12, cost=0.001730
2025-10-15 21:26:48.754 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 21:26:48.758 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:26:56.890 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=446, output_tokens=313, cost=0.013850
2025-10-15 21:26:56.897 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:26:58.753 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=667, output_tokens=35, cost=0.007720
2025-10-15 21:26:58.754 | DEBUG    | Extracted a valid json.
2025-10-15 21:26:58.754 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method(\"average\")", "search_class(\"Dataset\")", "search_class(\"DataStructure\")"],
    "bug_locations": []
}
2025-10-15 21:26:58.822 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 21:26:58.822 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 21:27:14.528 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4939, output_tokens=687, cost=0.070000
2025-10-15 21:27:14.544 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:27:24.699 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5842, output_tokens=405, cost=0.070570
2025-10-15 21:27:24.708 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:27:33.375 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=759, output_tokens=373, cost=0.018780
2025-10-15 21:27:33.376 | DEBUG    | Extracted a valid json.
2025-10-15 21:27:33.376 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "",
            "intended_behavior": "To resolve the issue, a new method named `average` should be added to the `Dataset` class. This new method would compute weighted averages across specified dimensions of the `Dataset`. The designed method will have to consider several factors to integrate seamlessly with the existing functionality of the class: - Ability to Specify Dimensions: The method should allow users to specify which dimensions to average over. This is consistent with other methods in the class, such as `sum` and `mean`, which work over specified dimensions. - Weight Application: The method must support the application of weights to the averaging process. This could involve accepting another Dataset or DataArray containing weights that correspond to the dimensions being averaged. Implementing weighted averages is crucial for many scientific and geospatial analyses where data points contribute unevenly to the overall average. - Preservation of Dataset Structure: The method should preserve the overall structure of the Dataset, meaning that it should return a new Dataset with the same coordinates and other dimensions intact, excluding those dimensions over which the average was computed. - Handling of Missing Values: The method should decide on a strategy for handling NaNs or missing values within the Dataset. It could follow similar conventions to other aggregation methods, offering users control over whether to skip or include these values in the computation. - Integration with Existing Methods: The method should be thoughtfully integrated into the existing set of Dataset methods, considering interactions with methods for selection, indexing, and other forms of data manipulation. This includes considering how it interacts with Dask-based lazy computations if `Dataset` objects can be backed by Dask arrays for out-of-memory computation."
        }
    ]
}
2025-10-15 21:27:33.390 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
422 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
423     """A multi-dimensional, in memory, array database.
424 
425     A dataset resembles an in-memory representation of a NetCDF file, and
426     consists of variables, coordinates and attributes which together form a
427     self describing dataset.
428 
429     Dataset implements the mapping interface with keys given by variable names
430     and values given by DataArray objects for each variable name.
431 
432     One dimensional variables with name equal to their dimension are index
433     coordinates used for label based indexing.
434     """
435 
436     _attrs: Optional[Dict[Hashable, Any]]
437     _cache: Dict[str, Any]
438     _coord_names: Set[Hashable]
439     _dims: Dict[Hashable, int]
440     _encoding: Optional[Dict[Hashable, Any]]
441     _indexes: Optional[Dict[Hashable, pd.Index]]
442     _variables: Dict[Hashable, Variable]
443 
444     __slots__ = (
445         "_attrs",
446         "_cache",
447         "_coord_names",
448         "_dims",
449         "_encoding",
450         "_file_obj",
451         "_indexes",
452         "_variables",
453         "__weakref__",
454     )
455 
456     _groupby_cls = groupby.DatasetGroupBy
457     _rolling_cls = rolling.DatasetRolling
458     _coarsen_cls = rolling.DatasetCoarsen
459     _resample_cls = resample.DatasetResample
460 
461     def __init__(
462         self,
463         # could make a VariableArgs to use more generally, and refine these
464         # categories
465         data_vars: Mapping[Hashable, Any] = None,
466         coords: Mapping[Hashable, Any] = None,
467         attrs: Mapping[Hashable, Any] = None,
468     ):
469         """To load data from a file or file-like object, use the `open_dataset`
470         function.
471 
472         Parameters
473         ----------
474         data_vars : dict-like, optional
475             A mapping from variable names to :py:class:`~xarray.DataArray`
476             objects, :py:class:`~xarray.Variable` objects or to tuples of the
477             form ``(dims, data[, attrs])`` which can be used as arguments to
478             create a new ``Variable``. Each dimension must have the same length
479             in all variables in which it appears.
480 
481             The following notations are accepted:
482 
483             - mapping {var name: DataArray}
484             - mapping {var name: Variable}
485             - mapping {var name: (dimension name, array-like)}
486             - mapping {var name: (tuple of dimension names, array-like)}
487             - mapping {dimension name: array-like}
488               (it will be automatically moved to coords, see below)
489 
490             Each dimension must have the same length in all variables in which
491             it appears.
492         coords : dict-like, optional
493             Another mapping in similar form as the `data_vars` argument,
494             except the each item is saved on the dataset as a "coordinate".
495             These variables have an associated meaning: they describe
496             constant/fixed/independent quantities, unlike the
497             varying/measured/dependent quantities that belong in `variables`.
498             Coordinates values may be given by 1-dimensional arrays or scalars,
499             in which case `dims` do not need to be supplied: 1D arrays will be
500             assumed to give index values along the dimension with the same
501             name.
502 
503             The following notations are accepted:
504 
505             - mapping {coord name: DataArray}
506             - mapping {coord name: Variable}
507             - mapping {coord name: (dimension name, array-like)}
508             - mapping {coord name: (tuple of dimension names, array-like)}
509             - mapping {dimension name: array-like}
510               (the dimension name is implicitly set to be the same as the coord name)
511 
512             The last notation implies that the coord name is the same as the
513             dimension name.
514 
515         attrs : dict-like, optional
516             Global attributes to save on this dataset.
517         """
518 
519         # TODO(shoyer): expose indexes as a public argument in __init__
520 
521         if data_vars is None:
522             data_vars = {}
523         if coords is None:
524             coords = {}
525 
526         both_data_and_coords = set(data_vars) & set(coords)
527         if both_data_and_coords:
528             raise ValueError(
529                 "variables %r are found in both data_vars and coords"
530                 % both_data_and_coords
531             )
532 
533         if isinstance(coords, Dataset):
534             coords = coords.variables
535 
536         variables, coord_names, dims, indexes = merge_data_and_coords(
537             data_vars, coords, compat="broadcast_equals"
538         )
539 
540         self._attrs = dict(attrs) if attrs is not None else None
541         self._file_obj = None
542         self._encoding = None
543         self._variables = variables
544         self._coord_names = coord_names
545         self._dims = dims
546         self._indexes = indexes
547 
548     @classmethod
549     def load_store(cls, store, decoder=None) -> "Dataset":
550         """Create a new dataset from the contents of a backends.*DataStore
551         object
552         """
553         variables, attributes = store.load()
554         if decoder:
555             variables, attributes = decoder(variables, attributes)
556         obj = cls(variables, attrs=attributes)
557         obj._file_obj = store
558         return obj
559 
560     @property
561     def variables(self) -> Mapping[Hashable, Variable]:
562         """Low level interface to Dataset contents as dict of Variable objects.
563 
564         This ordered dictionary is frozen to prevent mutation that could
565         violate Dataset invariants. It contains all variable objects
566         constituting the Dataset, including both data variables and
567         coordinates.
568         """
569         return Frozen(self._variables)
570 
571     @property
572     def attrs(self) -> Dict[Hashable, Any]:
573         """Dictionary of global attributes on this dataset
574         """
575         if self._attrs is None:
576             self._attrs = {}
577         return self._attrs
578 
579     @attrs.setter
580     def attrs(self, value: Mapping[Hashable, Any]) -> None:
581         self._attrs = dict(value)
582 
583     @property
584     def encoding(self) -> Dict:
585         """Dictionary of global encoding attributes on this dataset
586         """
587         if self._encoding is None:
588             self._encoding = {}
589         return self._encoding
590 
591     @encoding.setter
592     def encoding(self, value: Mapping) -> None:
593         self._encoding = dict(value)
594 
595     @property
596     def dims(self) -> Mapping[Hashable, int]:
597         """Mapping from dimension names to lengths.
598 
599         Cannot be modified directly, but is updated when adding new variables.
600 
601         Note that type of this object differs from `DataArray.dims`.
602         See `Dataset.sizes` and `DataArray.sizes` for consistently named
603         properties.
604         """
605         return Frozen(SortedKeysDict(self._dims))
606 
607     @property
608     def sizes(self) -> Mapping[Hashable, int]:
609         """Mapping from dimension names to lengths.
610 
611         Cannot be modified directly, but is updated when adding new variables.
612 
613         This is an alias for `Dataset.dims` provided for the benefit of
614         consistency with `DataArray.sizes`.
615 
616         See also
617         --------
618         DataArray.sizes
619         """
620         return self.dims
621 
622     def load(self, **kwargs) -> "Dataset":
623         """Manually trigger loading and/or computation of this dataset's data
624         from disk or a remote source into memory and return this dataset.
625         Unlike compute, the original dataset is modified and returned.
626 
627         Normally, it should not be necessary to call this method in user code,
628         because all xarray functions should either work on deferred data or
629         load data automatically. However, this method can be necessary when
630         working with many file objects on disk.
631 
632         Parameters
633         ----------
634         **kwargs : dict
635             Additional keyword arguments passed on to ``dask.array.compute``.
636 
637         See Also
638         --------
639         dask.array.compute
640         """
641         # access .data to coerce everything to numpy or dask arrays
642         lazy_data = {
643             k: v._data
644             for k, v in self.variables.items()
645             if isinstance(v._data, dask_array_type)
646         }
647         if lazy_data:
648             import dask.array as da
649 
650             # evaluate all the dask arrays simultaneously
651             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
652 
653             for k, data in zip(lazy_data, evaluated_data):
654                 self.variables[k].data = data
655 
656         # load everything else sequentially
657         for k, v in self.variables.items():
658             if k not in lazy_data:
659                 v.load()
660 
661         return self
662 
663     def __dask_tokenize__(self):
664         from dask.base import normalize_token
665 
666         return normalize_token(
667             (type(self), self._variables, self._coord_names, self._attrs)
668         )
669 
670     def __dask_graph__(self):
671         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
672         graphs = {k: v for k, v in graphs.items() if v is not None}
673         if not graphs:
674             return None
675         else:
676             try:
677                 from dask.highlevelgraph import HighLevelGraph
678 
679                 return HighLevelGraph.merge(*graphs.values())
680             except ImportError:
681                 from dask import sharedict
682 
683                 return sharedict.merge(*graphs.values())
684 
685     def __dask_keys__(self):
686         import dask
687 
688         return [
689             v.__dask_keys__()
690             for v in self.variables.values()
691             if dask.is_dask_collection(v)
692         ]
693 
694     def __dask_layers__(self):
695         import dask
696 
697         return sum(
698             [
699                 v.__dask_layers__()
700                 for v in self.variables.values()
701                 if dask.is_dask_collection(v)
702             ],
703             (),
704         )
705 
706     @property
707     def __dask_optimize__(self):
708         import dask.array as da
709 
710         return da.Array.__dask_optimize__
711 
712     @property
713     def __dask_scheduler__(self):
714         import dask.array as da
715 
716         return da.Array.__dask_scheduler__
717 
718     def __dask_postcompute__(self):
719         import dask
720 
721         info = [
722             (True, k, v.__dask_postcompute__())
723             if dask.is_dask_collection(v)
724             else (False, k, v)
725             for k, v in self._variables.items()
726         ]
727         args = (
728             info,
729             self._coord_names,
730             self._dims,
731             self._attrs,
732             self._indexes,
733             self._encoding,
734             self._file_obj,
735         )
736         return self._dask_postcompute, args
737 
738     def __dask_postpersist__(self):
739         import dask
740 
741         info = [
742             (True, k, v.__dask_postpersist__())
743             if dask.is_dask_collection(v)
744             else (False, k, v)
745             for k, v in self._variables.items()
746         ]
747         args = (
748             info,
749             self._coord_names,
750             self._dims,
751             self._attrs,
752             self._indexes,
753             self._encoding,
754             self._file_obj,
755         )
756         return self._dask_postpersist, args
757 
758     @staticmethod
759     def _dask_postcompute(results, info, *args):
760         variables = {}
761         results2 = list(results[::-1])
762         for is_dask, k, v in info:
763             if is_dask:
764                 func, args2 = v
765                 r = results2.pop()
766                 result = func(r, *args2)
767             else:
768                 result = v
769             variables[k] = result
770 
771         final = Dataset._construct_direct(variables, *args)
772         return final
773 
774     @staticmethod
775     def _dask_postpersist(dsk, info, *args):
776         variables = {}
777         for is_dask, k, v in info:
778             if is_dask:
779                 func, args2 = v
780                 result = func(dsk, *args2)
781             else:
782                 result = v
783             variables[k] = result
784 
785         return Dataset._construct_direct(variables, *args)
786 
787     def compute(self, **kwargs) -> "Dataset":
788         """Manually trigger loading and/or computation of this dataset's data
789         from disk or a remote source into memory and return a new dataset.
790         Unlike load, the original dataset is left unaltered.
791 
792         Normally, it should not be necessary to call this method in user code,
793         because all xarray functions should either work on deferred data or
794         load data automatically. However, this method can be necessary when
795         working with many file objects on disk.
796 
797         Parameters
798         ----------
799         **kwargs : dict
800             Additional keyword arguments passed on to ``dask.array.compute``.
801 
802         See Also
803         --------
804         dask.array.compute
805         """
806         new = self.copy(deep=False)
807         return new.load(**kwargs)
808 
809     def _persist_inplace(self, **kwargs) -> "Dataset":
810         """Persist all Dask arrays in memory
811         """
812         # access .data to coerce everything to numpy or dask arrays
813         lazy_data = {
814             k: v._data
815             for k, v in self.variables.items()
816             if isinstance(v._data, dask_array_type)
817         }
818         if lazy_data:
819             import dask
820 
821             # evaluate all the dask arrays simultaneously
822             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
823 
824             for k, data in zip(lazy_data, evaluated_data):
825                 self.variables[k].data = data
826 
827         return self
828 
829     def persist(self, **kwargs) -> "Dataset":
830         """ Trigger computation, keeping data as dask arrays
831 
832         This operation can be used to trigger computation on underlying dask
833         arrays, similar to ``.compute()`` or ``.load()``.  However this
834         operation keeps the data as dask arrays. This is particularly useful
835         when using the dask.distributed scheduler and you want to load a large
836         amount of data into distributed memory.
837 
838         Parameters
839         ----------
840         **kwargs : dict
841             Additional keyword arguments passed on to ``dask.persist``.
842 
843         See Also
844         --------
845         dask.persist
846         """
847         new = self.copy(deep=False)
848         return new._persist_inplace(**kwargs)
849 
850     @classmethod
851     def _construct_direct(
852         cls,
853         variables,
854         coord_names,
855         dims=None,
856         attrs=None,
857         indexes=None,
858         encoding=None,
859         file_obj=None,
860     ):
861         """Shortcut around __init__ for internal use when we want to skip
862         costly validation
863         """
864         if dims is None:
865             dims = calculate_dimensions(variables)
866         obj = object.__new__(cls)
867         obj._variables = variables
868         obj._coord_names = coord_names
869         obj._dims = dims
870         obj._indexes = indexes
871         obj._attrs = attrs
872         obj._file_obj = file_obj
873         obj._encoding = encoding
874         return obj
875 
876     def _replace(
877         self,
878         variables: Dict[Hashable, Variable] = None,
879         coord_names: Set[Hashable] = None,
880         dims: Dict[Any, int] = None,
881         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
882         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
883         encoding: Union[dict, None, Default] = _default,
884         inplace: bool = False,
885     ) -> "Dataset":
886         """Fastpath constructor for internal use.
887 
888         Returns an object with optionally with replaced attributes.
889 
890         Explicitly passed arguments are *not* copied when placed on the new
891         dataset. It is up to the caller to ensure that they have the right type
892         and are not used elsewhere.
893         """
894         if inplace:
895             if variables is not None:
896                 self._variables = variables
897             if coord_names is not None:
898                 self._coord_names = coord_names
899             if dims is not None:
900                 self._dims = dims
901             if attrs is not _default:
902                 self._attrs = attrs
903             if indexes is not _default:
904                 self._indexes = indexes
905             if encoding is not _default:
906                 self._encoding = encoding
907             obj = self
908         else:
909             if variables is None:
910                 variables = self._variables.copy()
911             if coord_names is None:
912                 coord_names = self._coord_names.copy()
913             if dims is None:
914                 dims = self._dims.copy()
915             if attrs is _default:
916                 attrs = copy.copy(self._attrs)
917             if indexes is _default:
918                 indexes = copy.copy(self._indexes)
919             if encoding is _default:
920                 encoding = copy.copy(self._encoding)
921             obj = self._construct_direct(
922                 variables, coord_names, dims, attrs, indexes, encoding
923             )
924         return obj
925 
926     def _replace_with_new_dims(
927         self,
928         variables: Dict[Hashable, Variable],
929         coord_names: set = None,
930         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
931         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
932         inplace: bool = False,
933     ) -> "Dataset":
934         """Replace variables with recalculated dimensions."""
935         dims = calculate_dimensions(variables)
936         return self._replace(
937             variables, coord_names, dims, attrs, indexes, inplace=inplace
938         )
939 
940     def _replace_vars_and_dims(
941         self,
942         variables: Dict[Hashable, Variable],
943         coord_names: set = None,
944         dims: Dict[Hashable, int] = None,
945         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
946         inplace: bool = False,
947     ) -> "Dataset":
948         """Deprecated version of _replace_with_new_dims().
949 
950         Unlike _replace_with_new_dims(), this method always recalculates
951         indexes from variables.
952         """
953         if dims is None:
954             dims = calculate_dimensions(variables)
955         return self._replace(
956             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
957         )
958 
959     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
960         if not indexes:
961             return self
962 
963         variables = self._variables.copy()
964         new_indexes = dict(self.indexes)
965         for name, idx in indexes.items():
966             variables[name] = IndexVariable(name, idx)
967             new_indexes[name] = idx
968         obj = self._replace(variables, indexes=new_indexes)
969 
970         # switch from dimension to level names, if necessary
971         dim_names: Dict[Hashable, str] = {}
972         for dim, idx in indexes.items():
973             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
974                 dim_names[dim] = idx.name
975         if dim_names:
976             obj = obj.rename(dim_names)
977         return obj
978 
979     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
980         """Returns a copy of this dataset.
981 
982         If `deep=True`, a deep copy is made of each of the component variables.
983         Otherwise, a shallow copy of each of the component variable is made, so
984         that the underlying memory region of the new dataset is the same as in
985         the original dataset.
986 
987         Use `data` to create a new object with the same structure as
988         original but entirely new data.
989 
990         Parameters
991         ----------
992         deep : bool, optional
993             Whether each component variable is loaded into memory and copied onto
994             the new object. Default is False.
995         data : dict-like, optional
996             Data to use in the new object. Each item in `data` must have same
997             shape as corresponding data variable in original. When `data` is
998             used, `deep` is ignored for the data variables and only used for
999             coords.
1000 
1001         Returns
1002         -------
1003         object : Dataset
1004             New object with dimensions, attributes, coordinates, name, encoding,
1005             and optionally data copied from original.
1006 
1007         Examples
1008         --------
1009 
1010         Shallow copy versus deep copy
1011 
1012         >>> da = xr.DataArray(np.random.randn(2, 3))
1013         >>> ds = xr.Dataset(
1014         ...     {"foo": da, "bar": ("x", [-1, 2])}, coords={"x": ["one", "two"]},
1015         ... )
1016         >>> ds.copy()
1017         <xarray.Dataset>
1018         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1019         Coordinates:
1020         * x        (x) <U3 'one' 'two'
1021         Dimensions without coordinates: dim_0, dim_1
1022         Data variables:
1023             foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003
1024             bar      (x) int64 -1 2
1025 
1026         >>> ds_0 = ds.copy(deep=False)
1027         >>> ds_0["foo"][0, 0] = 7
1028         >>> ds_0
1029         <xarray.Dataset>
1030         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1031         Coordinates:
1032         * x        (x) <U3 'one' 'two'
1033         Dimensions without coordinates: dim_0, dim_1
1034         Data variables:
1035             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1036             bar      (x) int64 -1 2
1037 
1038         >>> ds
1039         <xarray.Dataset>
1040         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1041         Coordinates:
1042         * x        (x) <U3 'one' 'two'
1043         Dimensions without coordinates: dim_0, dim_1
1044         Data variables:
1045             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1046             bar      (x) int64 -1 2
1047 
1048         Changing the data using the ``data`` argument maintains the
1049         structure of the original object, but with the new data. Original
1050         object is unaffected.
1051 
1052         >>> ds.copy(
1053         ...     data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]}
1054         ... )
1055         <xarray.Dataset>
1056         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1057         Coordinates:
1058         * x        (x) <U3 'one' 'two'
1059         Dimensions without coordinates: dim_0, dim_1
1060         Data variables:
1061             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1062             bar      (x) <U1 'a' 'b'
1063 
1064         >>> ds
1065         <xarray.Dataset>
1066         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1067         Coordinates:
1068         * x        (x) <U3 'one' 'two'
1069         Dimensions without coordinates: dim_0, dim_1
1070         Data variables:
1071             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1072             bar      (x) int64 -1 2
1073 
1074         See Also
1075         --------
1076         pandas.DataFrame.copy
1077         """
1078         if data is None:
1079             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1080         elif not utils.is_dict_like(data):
1081             raise ValueError("Data must be dict-like")
1082         else:
1083             var_keys = set(self.data_vars.keys())
1084             data_keys = set(data.keys())
1085             keys_not_in_vars = data_keys - var_keys
1086             if keys_not_in_vars:
1087                 raise ValueError(
1088                     "Data must only contain variables in original "
1089                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1090                 )
1091             keys_missing_from_data = var_keys - data_keys
1092             if keys_missing_from_data:
1093                 raise ValueError(
1094                     "Data must contain all variables in original "
1095                     "dataset. Data is missing {}".format(keys_missing_from_data)
1096                 )
1097             variables = {
1098                 k: v.copy(deep=deep, data=data.get(k))
1099                 for k, v in self._variables.items()
1100             }
1101 
1102         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1103 
1104         return self._replace(variables, attrs=attrs)
1105 
1106     @property
1107     def _level_coords(self) -> Dict[str, Hashable]:
1108         """Return a mapping of all MultiIndex levels and their corresponding
1109         coordinate name.
1110         """
1111         level_coords: Dict[str, Hashable] = {}
1112         for name, index in self.indexes.items():
1113             if isinstance(index, pd.MultiIndex):
1114                 level_names = index.names
1115                 (dim,) = self.variables[name].dims
1116                 level_coords.update({lname: dim for lname in level_names})
1117         return level_coords
1118 
1119     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1120         """Create a new Dataset with the listed variables from this dataset and
1121         the all relevant coordinates. Skips all validation.
1122         """
1123         variables: Dict[Hashable, Variable] = {}
1124         coord_names = set()
1125         indexes: Dict[Hashable, pd.Index] = {}
1126 
1127         for name in names:
1128             try:
1129                 variables[name] = self._variables[name]
1130             except KeyError:
1131                 ref_name, var_name, var = _get_virtual_variable(
1132                     self._variables, name, self._level_coords, self.dims
1133                 )
1134                 variables[var_name] = var
1135                 if ref_name in self._coord_names or ref_name in self.dims:
1136                     coord_names.add(var_name)
1137                 if (var_name,) == var.dims:
1138                     indexes[var_name] = var.to_index()
1139 
1140         needed_dims: Set[Hashable] = set()
1141         for v in variables.values():
1142             needed_dims.update(v.dims)
1143 
1144         dims = {k: self.dims[k] for k in needed_dims}
1145 
1146         for k in self._coord_names:
1147             if set(self.variables[k].dims) <= needed_dims:
1148                 variables[k] = self._variables[k]
1149                 coord_names.add(k)
1150                 if k in self.indexes:
1151                     indexes[k] = self.indexes[k]
1152 
1153         return self._replace(variables, coord_names, dims, indexes=indexes)
1154 
1155     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1156         """Construct a DataArray by indexing this dataset
1157         """
1158         from .dataarray import DataArray
1159 
1160         try:
1161             variable = self._variables[name]
1162         except KeyError:
1163             _, name, variable = _get_virtual_variable(
1164                 self._variables, name, self._level_coords, self.dims
1165             )
1166 
1167         needed_dims = set(variable.dims)
1168 
1169         coords: Dict[Hashable, Variable] = {}
1170         for k in self.coords:
1171             if set(self.variables[k].dims) <= needed_dims:
1172                 coords[k] = self.variables[k]
1173 
1174         if self._indexes is None:
1175             indexes = None
1176         else:
1177             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1178 
1179         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1180 
1181     def __copy__(self) -> "Dataset":
1182         return self.copy(deep=False)
1183 
1184     def __deepcopy__(self, memo=None) -> "Dataset":
1185         # memo does nothing but is required for compatibility with
1186         # copy.deepcopy
1187         return self.copy(deep=True)
1188 
1189     @property
1190     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
1191         """List of places to look-up items for attribute-style access
1192         """
1193         return self._item_sources + [self.attrs]
1194 
1195     @property
1196     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
1197         """List of places to look-up items for key-completion
1198         """
1199         return [
1200             self.data_vars,
1201             self.coords,
1202             {d: self[d] for d in self.dims},
1203             LevelCoordinatesSource(self),
1204         ]
1205 
1206     def __contains__(self, key: object) -> bool:
1207         """The 'in' operator will return true or false depending on whether
1208         'key' is an array in the dataset or not.
1209         """
1210         return key in self._variables
1211 
1212     def __len__(self) -> int:
1213         return len(self.data_vars)
1214 
1215     def __bool__(self) -> bool:
1216         return bool(self.data_vars)
1217 
1218     def __iter__(self) -> Iterator[Hashable]:
1219         return iter(self.data_vars)
1220 
1221     def __array__(self, dtype=None):
1222         raise TypeError(
1223             "cannot directly convert an xarray.Dataset into a "
1224             "numpy array. Instead, create an xarray.DataArray "
1225             "first, either with indexing on the Dataset or by "
1226             "invoking the `to_array()` method."
1227         )
1228 
1229     @property
1230     def nbytes(self) -> int:
1231         return sum(v.nbytes for v in self.variables.values())
1232 
1233     @property
1234     def loc(self) -> _LocIndexer:
1235         """Attribute for location based indexing. Only supports __getitem__,
1236         and only when the key is a dict of the form {dim: labels}.
1237         """
1238         return _LocIndexer(self)
1239 
1240     def __getitem__(self, key: Any) -> "Union[DataArray, Dataset]":
1241         """Access variables or coordinates this dataset as a
1242         :py:class:`~xarray.DataArray`.
1243 
1244         Indexing with a list of names will return a new ``Dataset`` object.
1245         """
1246         # TODO(shoyer): type this properly: https://github.com/python/mypy/issues/7328
1247         if utils.is_dict_like(key):
1248             return self.isel(**cast(Mapping, key))
1249 
1250         if hashable(key):
1251             return self._construct_dataarray(key)
1252         else:
1253             return self._copy_listed(np.asarray(key))
1254 
1255     def __setitem__(self, key: Hashable, value) -> None:
1256         """Add an array to this dataset.
1257 
1258         If value is a `DataArray`, call its `select_vars()` method, rename it
1259         to `key` and merge the contents of the resulting dataset into this
1260         dataset.
1261 
1262         If value is an `Variable` object (or tuple of form
1263         ``(dims, data[, attrs])``), add it to this dataset as a new
1264         variable.
1265         """
1266         if utils.is_dict_like(key):
1267             raise NotImplementedError(
1268                 "cannot yet use a dictionary as a key " "to set Dataset values"
1269             )
1270 
1271         self.update({key: value})
1272 
1273     def __delitem__(self, key: Hashable) -> None:
1274         """Remove a variable from this dataset.
1275         """
1276         del self._variables[key]
1277         self._coord_names.discard(key)
1278         if key in self.indexes:
1279             assert self._indexes is not None
1280             del self._indexes[key]
1281         self._dims = calculate_dimensions(self._variables)
1282 
1283     # mutable objects should not be hashable
1284     # https://github.com/python/mypy/issues/4266
1285     __hash__ = None  # type: ignore
1286 
1287     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1288         """Helper function for equals and identical
1289         """
1290 
1291         # some stores (e.g., scipy) do not seem to preserve order, so don't
1292         # require matching order for equality
1293         def compat(x: Variable, y: Variable) -> bool:
1294             return getattr(x, compat_str)(y)
1295 
1296         return self._coord_names == other._coord_names and utils.dict_equiv(
1297             self._variables, other._variables, compat=compat
1298         )
1299 
1300     def broadcast_equals(self, other: "Dataset") -> bool:
1301         """Two Datasets are broadcast equal if they are equal after
1302         broadcasting all variables against each other.
1303 
1304         For example, variables that are scalar in one dataset but non-scalar in
1305         the other dataset can still be broadcast equal if the the non-scalar
1306         variable is a constant.
1307 
1308         See Also
1309         --------
1310         Dataset.equals
1311         Dataset.identical
1312         """
1313         try:
1314             return self._all_compat(other, "broadcast_equals")
1315         except (TypeError, AttributeError):
1316             return False
1317 
1318     def equals(self, other: "Dataset") -> bool:
1319         """Two Datasets are equal if they have matching variables and
1320         coordinates, all of which are equal.
1321 
1322         Datasets can still be equal (like pandas objects) if they have NaN
1323         values in the same locations.
1324 
1325         This method is necessary because `v1 == v2` for ``Dataset``
1326         does element-wise comparisons (like numpy.ndarrays).
1327 
1328         See Also
1329         --------
1330         Dataset.broadcast_equals
1331         Dataset.identical
1332         """
1333         try:
1334             return self._all_compat(other, "equals")
1335         except (TypeError, AttributeError):
1336             return False
1337 
1338     def identical(self, other: "Dataset") -> bool:
1339         """Like equals, but also checks all dataset attributes and the
1340         attributes on all variables and coordinates.
1341 
1342         See Also
1343         --------
1344         Dataset.broadcast_equals
1345         Dataset.equals
1346         """
1347         try:
1348             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1349                 other, "identical"
1350             )
1351         except (TypeError, AttributeError):
1352             return False
1353 
1354     @property
1355     def indexes(self) -> Indexes:
1356         """Mapping of pandas.Index objects used for label based indexing
1357         """
1358         if self._indexes is None:
1359             self._indexes = default_indexes(self._variables, self._dims)
1360         return Indexes(self._indexes)
1361 
1362     @property
1363     def coords(self) -> DatasetCoordinates:
1364         """Dictionary of xarray.DataArray objects corresponding to coordinate
1365         variables
1366         """
1367         return DatasetCoordinates(self)
1368 
1369     @property
1370     def data_vars(self) -> DataVariables:
1371         """Dictionary of DataArray objects corresponding to data variables
1372         """
1373         return DataVariables(self)
1374 
1375     def set_coords(
1376         self, names: "Union[Hashable, Iterable[Hashable]]", inplace: bool = None
1377     ) -> "Dataset":
1378         """Given names of one or more variables, set them as coordinates
1379 
1380         Parameters
1381         ----------
1382         names : hashable or iterable of hashables
1383             Name(s) of variables in this dataset to convert into coordinates.
1384 
1385         Returns
1386         -------
1387         Dataset
1388 
1389         See also
1390         --------
1391         Dataset.swap_dims
1392         """
1393         # TODO: allow inserting new coordinates with this method, like
1394         # DataFrame.set_index?
1395         # nb. check in self._variables, not self.data_vars to insure that the
1396         # operation is idempotent
1397         _check_inplace(inplace)
1398         if isinstance(names, str) or not isinstance(names, Iterable):
1399             names = [names]
1400         else:
1401             names = list(names)
1402         self._assert_all_in_dataset(names)
1403         obj = self.copy()
1404         obj._coord_names.update(names)
1405         return obj
1406 
1407     def reset_coords(
1408         self,
1409         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1410         drop: bool = False,
1411         inplace: bool = None,
1412     ) -> "Dataset":
1413         """Given names of coordinates, reset them to become variables
1414 
1415         Parameters
1416         ----------
1417         names : hashable or iterable of hashables, optional
1418             Name(s) of non-index coordinates in this dataset to reset into
1419             variables. By default, all non-index coordinates are reset.
1420         drop : bool, optional
1421             If True, remove coordinates instead of converting them into
1422             variables.
1423 
1424         Returns
1425         -------
1426         Dataset
1427         """
1428         _check_inplace(inplace)
1429         if names is None:
1430             names = self._coord_names - set(self.dims)
1431         else:
1432             if isinstance(names, str) or not isinstance(names, Iterable):
1433                 names = [names]
1434             else:
1435                 names = list(names)
1436             self._assert_all_in_dataset(names)
1437             bad_coords = set(names) & set(self.dims)
1438             if bad_coords:
1439                 raise ValueError(
1440                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1441                 )
1442         obj = self.copy()
1443         obj._coord_names.difference_update(names)
1444         if drop:
1445             for name in names:
1446                 del obj._variables[name]
1447         return obj
1448 
1449     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1450         """Store dataset contents to a backends.*DataStore object.
1451         """
1452         from ..backends.api import dump_to_store
1453 
1454         # TODO: rename and/or cleanup this method to make it more consistent
1455         # with to_netcdf()
1456         dump_to_store(self, store, **kwargs)
1457 
1458     def to_netcdf(
1459         self,
1460         path=None,
1461         mode: str = "w",
1462         format: str = None,
1463         group: str = None,
1464         engine: str = None,
1465         encoding: Mapping = None,
1466         unlimited_dims: Iterable[Hashable] = None,
1467         compute: bool = True,
1468         invalid_netcdf: bool = False,
1469     ) -> Union[bytes, "Delayed", None]:
1470         """Write dataset contents to a netCDF file.
1471 
1472         Parameters
1473         ----------
1474         path : str, Path or file-like object, optional
1475             Path to which to save this dataset. File-like objects are only
1476             supported by the scipy engine. If no path is provided, this
1477             function returns the resulting netCDF file as bytes; in this case,
1478             we need to use scipy, which does not support netCDF version 4 (the
1479             default format becomes NETCDF3_64BIT).
1480         mode : {'w', 'a'}, optional
1481             Write ('w') or append ('a') mode. If mode='w', any existing file at
1482             this location will be overwritten. If mode='a', existing variables
1483             will be overwritten.
1484         format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',
1485                   'NETCDF3_CLASSIC'}, optional
1486             File format for the resulting netCDF file:
1487 
1488             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1489               features.
1490             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1491               netCDF 3 compatible API features.
1492             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1493               which fully supports 2+ GB files, but is only compatible with
1494               clients linked against netCDF version 3.6.0 or later.
1495             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1496               handle 2+ GB files very well.
1497 
1498             All formats are supported by the netCDF4-python library.
1499             scipy.io.netcdf only supports the last two formats.
1500 
1501             The default format is NETCDF4 if you are saving a file to disk and
1502             have the netCDF4-python library available. Otherwise, xarray falls
1503             back to using scipy to write netCDF files and defaults to the
1504             NETCDF3_64BIT format (scipy does not support netCDF4).
1505         group : str, optional
1506             Path to the netCDF4 group in the given file to open (only works for
1507             format='NETCDF4'). The group(s) will be created if necessary.
1508         engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional
1509             Engine to use when writing netCDF files. If not provided, the
1510             default engine is chosen based on available dependencies, with a
1511             preference for 'netcdf4' if writing to a file on disk.
1512         encoding : dict, optional
1513             Nested dictionary with variable names as keys and dictionaries of
1514             variable specific encodings as values, e.g.,
1515             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,
1516             'zlib': True}, ...}``
1517 
1518             The `h5netcdf` engine supports both the NetCDF4-style compression
1519             encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py
1520             ones ``{'compression': 'gzip', 'compression_opts': 9}``.
1521             This allows using any compression plugin installed in the HDF5
1522             library, e.g. LZF.
1523 
1524         unlimited_dims : iterable of hashable, optional
1525             Dimension(s) that should be serialized as unlimited dimensions.
1526             By default, no dimensions are treated as unlimited dimensions.
1527             Note that unlimited_dims may also be set via
1528             ``dataset.encoding['unlimited_dims']``.
1529         compute: boolean
1530             If true compute immediately, otherwise return a
1531             ``dask.delayed.Delayed`` object that can be computed later.
1532         invalid_netcdf: boolean
1533             Only valid along with engine='h5netcdf'. If True, allow writing
1534             hdf5 files which are valid netcdf as described in
1535             https://github.com/shoyer/h5netcdf. Default: False.
1536         """
1537         if encoding is None:
1538             encoding = {}
1539         from ..backends.api import to_netcdf
1540 
1541         return to_netcdf(
1542             self,
1543             path,
1544             mode,
1545             format=format,
1546             group=group,
1547             engine=engine,
1548             encoding=encoding,
1549             unlimited_dims=unlimited_dims,
1550             compute=compute,
1551             invalid_netcdf=invalid_netcdf,
1552         )
1553 
1554     def to_zarr(
1555         self,
1556         store: Union[MutableMapping, str, Path] = None,
1557         mode: str = None,
1558         synchronizer=None,
1559         group: str = None,
1560         encoding: Mapping = None,
1561         compute: bool = True,
1562         consolidated: bool = False,
1563         append_dim: Hashable = None,
1564     ) -> "ZarrStore":
1565         """Write dataset contents to a zarr group.
1566 
1567         .. note:: Experimental
1568                   The Zarr backend is new and experimental. Please report any
1569                   unexpected behavior via github issues.
1570 
1571         Parameters
1572         ----------
1573         store : MutableMapping, str or Path, optional
1574             Store or path to directory in file system.
1575         mode : {'w', 'w-', 'a', None}
1576             Persistence mode: 'w' means create (overwrite if exists);
1577             'w-' means create (fail if exists);
1578             'a' means append (create if does not exist).
1579             If ``append_dim`` is set, ``mode`` can be omitted as it is
1580             internally set to ``'a'``. Otherwise, ``mode`` will default to
1581             `w-` if not set.
1582         synchronizer : object, optional
1583             Array synchronizer
1584         group : str, optional
1585             Group path. (a.k.a. `path` in zarr terminology.)
1586         encoding : dict, optional
1587             Nested dictionary with variable names as keys and dictionaries of
1588             variable specific encodings as values, e.g.,
1589             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``
1590         compute: bool, optional
1591             If True compute immediately, otherwise return a
1592             ``dask.delayed.Delayed`` object that can be computed later.
1593         consolidated: bool, optional
1594             If True, apply zarr's `consolidate_metadata` function to the store
1595             after writing.
1596         append_dim: hashable, optional
1597             If set, the dimension on which the data will be appended.
1598 
1599         References
1600         ----------
1601         https://zarr.readthedocs.io/
1602         """
1603         if encoding is None:
1604             encoding = {}
1605         if (mode == "a") or (append_dim is not None):
1606             if mode is None:
1607                 mode = "a"
1608             elif mode != "a":
1609                 raise ValueError(
1610                     "append_dim was set along with mode='{}', either set "
1611                     "mode='a' or don't set it.".format(mode)
1612                 )
1613         elif mode is None:
1614             mode = "w-"
1615         if mode not in ["w", "w-", "a"]:
1616             # TODO: figure out how to handle 'r+'
1617             raise ValueError(
1618                 "The only supported options for mode are 'w'," "'w-' and 'a'."
1619             )
1620         from ..backends.api import to_zarr
1621 
1622         return to_zarr(
1623             self,
1624             store=store,
1625             mode=mode,
1626             synchronizer=synchronizer,
1627             group=group,
1628             encoding=encoding,
1629             compute=compute,
1630             consolidated=consolidated,
1631             append_dim=append_dim,
1632         )
1633 
1634     def __repr__(self) -> str:
1635         return formatting.dataset_repr(self)
1636 
1637     def _repr_html_(self):
1638         if OPTIONS["display_style"] == "text":
1639             return f"<pre>{escape(repr(self))}</pre>"
1640         return formatting_html.dataset_repr(self)
1641 
1642     def info(self, buf=None) -> None:
1643         """
1644         Concise summary of a Dataset variables and attributes.
1645 
1646         Parameters
1647         ----------
1648         buf : writable buffer, defaults to sys.stdout
1649 
1650         See Also
1651         --------
1652         pandas.DataFrame.assign
1653         ncdump: netCDF's ncdump
1654         """
1655         if buf is None:  # pragma: no cover
1656             buf = sys.stdout
1657 
1658         lines = []
1659         lines.append("xarray.Dataset {")
1660         lines.append("dimensions:")
1661         for name, size in self.dims.items():
1662             lines.append(f"\t{name} = {size} ;")
1663         lines.append("\nvariables:")
1664         for name, da in self.variables.items():
1665             dims = ", ".join(da.dims)
1666             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1667             for k, v in da.attrs.items():
1668                 lines.append(f"\t\t{name}:{k} = {v} ;")
1669         lines.append("\n// global attributes:")
1670         for k, v in self.attrs.items():
1671             lines.append(f"\t:{k} = {v} ;")
1672         lines.append("}")
1673 
1674         buf.write("\n".join(lines))
1675 
1676     @property
1677     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1678         """Block dimensions for this dataset's data or None if it's not a dask
1679         array.
1680         """
1681         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1682         for v in self.variables.values():
1683             if v.chunks is not None:
1684                 for dim, c in zip(v.dims, v.chunks):
1685                     if dim in chunks and c != chunks[dim]:
1686                         raise ValueError(
1687                             f"Object has inconsistent chunks along dimension {dim}. "
1688                             "This can be fixed by calling unify_chunks()."
1689                         )
1690                     chunks[dim] = c
1691         return Frozen(SortedKeysDict(chunks))
1692 
1693     def chunk(
1694         self,
1695         chunks: Union[
1696             None, Number, Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]]
1697         ] = None,
1698         name_prefix: str = "xarray-",
1699         token: str = None,
1700         lock: bool = False,
1701     ) -> "Dataset":
1702         """Coerce all arrays in this dataset into dask arrays with the given
1703         chunks.
1704 
1705         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1706         arrays will be rechunked to the given chunk sizes.
1707 
1708         If neither chunks is not provided for one or more dimensions, chunk
1709         sizes along that dimension will not be updated; non-dask arrays will be
1710         converted into dask arrays with a single block.
1711 
1712         Parameters
1713         ----------
1714         chunks : int or mapping, optional
1715             Chunk sizes along each dimension, e.g., ``5`` or
1716             ``{'x': 5, 'y': 5}``.
1717         name_prefix : str, optional
1718             Prefix for the name of any new dask arrays.
1719         token : str, optional
1720             Token uniquely identifying this dataset.
1721         lock : optional
1722             Passed on to :py:func:`dask.array.from_array`, if the array is not
1723             already as dask array.
1724 
1725         Returns
1726         -------
1727         chunked : xarray.Dataset
1728         """
1729         from dask.base import tokenize
1730 
1731         if isinstance(chunks, Number):
1732             chunks = dict.fromkeys(self.dims, chunks)
1733 
1734         if chunks is not None:
1735             bad_dims = chunks.keys() - self.dims.keys()
1736             if bad_dims:
1737                 raise ValueError(
1738                     "some chunks keys are not dimensions on this "
1739                     "object: %s" % bad_dims
1740                 )
1741 
1742         def selkeys(dict_, keys):
1743             if dict_ is None:
1744                 return None
1745             return {d: dict_[d] for d in keys if d in dict_}
1746 
1747         def maybe_chunk(name, var, chunks):
1748             chunks = selkeys(chunks, var.dims)
1749             if not chunks:
1750                 chunks = None
1751             if var.ndim > 0:
1752                 # when rechunking by different amounts, make sure dask names change
1753                 # by provinding chunks as an input to tokenize.
1754                 # subtle bugs result otherwise. see GH3350
1755                 token2 = tokenize(name, token if token else var._data, chunks)
1756                 name2 = f"{name_prefix}{name}-{token2}"
1757                 return var.chunk(chunks, name=name2, lock=lock)
1758             else:
1759                 return var
1760 
1761         variables = {k: maybe_chunk(k, v, chunks) for k, v in self.variables.items()}
1762         return self._replace(variables)
1763 
1764     def _validate_indexers(
1765         self, indexers: Mapping[Hashable, Any]
1766     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1767         """ Here we make sure
1768         + indexer has a valid keys
1769         + indexer is in a valid data type
1770         + string indexers are cast to the appropriate date type if the
1771           associated index is a DatetimeIndex or CFTimeIndex
1772         """
1773         from .dataarray import DataArray
1774 
1775         invalid = indexers.keys() - self.dims.keys()
1776         if invalid:
1777             raise ValueError("dimensions %r do not exist" % invalid)
1778 
1779         # all indexers should be int, slice, np.ndarrays, or Variable
1780         for k, v in indexers.items():
1781             if isinstance(v, (int, slice, Variable)):
1782                 yield k, v
1783             elif isinstance(v, DataArray):
1784                 yield k, v.variable
1785             elif isinstance(v, tuple):
1786                 yield k, as_variable(v)
1787             elif isinstance(v, Dataset):
1788                 raise TypeError("cannot use a Dataset as an indexer")
1789             elif isinstance(v, Sequence) and len(v) == 0:
1790                 yield k, np.empty((0,), dtype="int64")
1791             else:
1792                 v = np.asarray(v)
1793 
1794                 if v.dtype.kind in "US":
1795                     index = self.indexes[k]
1796                     if isinstance(index, pd.DatetimeIndex):
1797                         v = v.astype("datetime64[ns]")
1798                     elif isinstance(index, xr.CFTimeIndex):
1799                         v = _parse_array_of_cftime_strings(v, index.date_type)
1800 
1801                 if v.ndim > 1:
1802                     raise IndexError(
1803                         "Unlabeled multi-dimensional array cannot be "
1804                         "used for indexing: {}".format(k)
1805                     )
1806                 yield k, v
1807 
1808     def _validate_interp_indexers(
1809         self, indexers: Mapping[Hashable, Any]
1810     ) -> Iterator[Tuple[Hashable, Variable]]:
1811         """Variant of _validate_indexers to be used for interpolation
1812         """
1813         for k, v in self._validate_indexers(indexers):
1814             if isinstance(v, Variable):
1815                 if v.ndim == 1:
1816                     yield k, v.to_index_variable()
1817                 else:
1818                     yield k, v
1819             elif isinstance(v, int):
1820                 yield k, Variable((), v)
1821             elif isinstance(v, np.ndarray):
1822                 if v.ndim == 0:
1823                     yield k, Variable((), v)
1824                 elif v.ndim == 1:
1825                     yield k, IndexVariable((k,), v)
1826                 else:
1827                     raise AssertionError()  # Already tested by _validate_indexers
1828             else:
1829                 raise TypeError(type(v))
1830 
1831     def _get_indexers_coords_and_indexes(self, indexers):
1832         """Extract coordinates and indexes from indexers.
1833 
1834         Only coordinate with a name different from any of self.variables will
1835         be attached.
1836         """
1837         from .dataarray import DataArray
1838 
1839         coords_list = []
1840         for k, v in indexers.items():
1841             if isinstance(v, DataArray):
1842                 if v.dtype.kind == "b":
1843                     if v.ndim != 1:  # we only support 1-d boolean array
1844                         raise ValueError(
1845                             "{:d}d-boolean array is used for indexing along "
1846                             "dimension {!r}, but only 1d boolean arrays are "
1847                             "supported.".format(v.ndim, k)
1848                         )
1849                     # Make sure in case of boolean DataArray, its
1850                     # coordinate also should be indexed.
1851                     v_coords = v[v.values.nonzero()[0]].coords
1852                 else:
1853                     v_coords = v.coords
1854                 coords_list.append(v_coords)
1855 
1856         # we don't need to call align() explicitly or check indexes for
1857         # alignment, because merge_variables already checks for exact alignment
1858         # between dimension coordinates
1859         coords, indexes = merge_coordinates_without_align(coords_list)
1860         assert_coordinate_consistent(self, coords)
1861 
1862         # silently drop the conflicted variables.
1863         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
1864         attached_indexes = {
1865             k: v for k, v in indexes.items() if k not in self._variables
1866         }
1867         return attached_coords, attached_indexes
1868 
1869     def isel(
1870         self,
1871         indexers: Mapping[Hashable, Any] = None,
1872         drop: bool = False,
1873         **indexers_kwargs: Any,
1874     ) -> "Dataset":
1875         """Returns a new dataset with each array indexed along the specified
1876         dimension(s).
1877 
1878         This method selects values from each array using its `__getitem__`
1879         method, except this method does not require knowing the order of
1880         each array's dimensions.
1881 
1882         Parameters
1883         ----------
1884         indexers : dict, optional
1885             A dict with keys matching dimensions and values given
1886             by integers, slice objects or arrays.
1887             indexer can be a integer, slice, array-like or DataArray.
1888             If DataArrays are passed as indexers, xarray-style indexing will be
1889             carried out. See :ref:`indexing` for the details.
1890             One of indexers or indexers_kwargs must be provided.
1891         drop : bool, optional
1892             If ``drop=True``, drop coordinates variables indexed by integers
1893             instead of making them scalar.
1894         **indexers_kwargs : {dim: indexer, ...}, optional
1895             The keyword arguments form of ``indexers``.
1896             One of indexers or indexers_kwargs must be provided.
1897 
1898         Returns
1899         -------
1900         obj : Dataset
1901             A new Dataset with the same contents as this dataset, except each
1902             array and dimension is indexed by the appropriate indexers.
1903             If indexer DataArrays have coordinates that do not conflict with
1904             this object, then these coordinates will be attached.
1905             In general, each array's data will be a view of the array's data
1906             in this dataset, unless vectorized indexing was triggered by using
1907             an array indexer, in which case the data will be a copy.
1908 
1909         See Also
1910         --------
1911         Dataset.sel
1912         DataArray.isel
1913         """
1914         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1915         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1916             return self._isel_fancy(indexers, drop=drop)
1917 
1918         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1919         # lists, or zero or one-dimensional np.ndarray's
1920         invalid = indexers.keys() - self.dims.keys()
1921         if invalid:
1922             raise ValueError("dimensions %r do not exist" % invalid)
1923 
1924         variables = {}
1925         dims: Dict[Hashable, Tuple[int, ...]] = {}
1926         coord_names = self._coord_names.copy()
1927         indexes = self._indexes.copy() if self._indexes is not None else None
1928 
1929         for var_name, var_value in self._variables.items():
1930             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
1931             if var_indexers:
1932                 var_value = var_value.isel(var_indexers)
1933                 if drop and var_value.ndim == 0 and var_name in coord_names:
1934                     coord_names.remove(var_name)
1935                     if indexes:
1936                         indexes.pop(var_name, None)
1937                     continue
1938                 if indexes and var_name in indexes:
1939                     if var_value.ndim == 1:
1940                         indexes[var_name] = var_value.to_index()
1941                     else:
1942                         del indexes[var_name]
1943             variables[var_name] = var_value
1944             dims.update(zip(var_value.dims, var_value.shape))
1945 
1946         return self._construct_direct(
1947             variables=variables,
1948             coord_names=coord_names,
1949             dims=dims,
1950             attrs=self._attrs,
1951             indexes=indexes,
1952             encoding=self._encoding,
1953             file_obj=self._file_obj,
1954         )
1955 
1956     def _isel_fancy(self, indexers: Mapping[Hashable, Any], *, drop: bool) -> "Dataset":
1957         # Note: we need to preserve the original indexers variable in order to merge the
1958         # coords below
1959         indexers_list = list(self._validate_indexers(indexers))
1960 
1961         variables: Dict[Hashable, Variable] = {}
1962         indexes: Dict[Hashable, pd.Index] = {}
1963 
1964         for name, var in self.variables.items():
1965             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
1966             if drop and name in var_indexers:
1967                 continue  # drop this variable
1968 
1969             if name in self.indexes:
1970                 new_var, new_index = isel_variable_and_index(
1971                     name, var, self.indexes[name], var_indexers
1972                 )
1973                 if new_index is not None:
1974                     indexes[name] = new_index
1975             elif var_indexers:
1976                 new_var = var.isel(indexers=var_indexers)
1977             else:
1978                 new_var = var.copy(deep=False)
1979 
1980             variables[name] = new_var
1981 
1982         coord_names = self._coord_names & variables.keys()
1983         selected = self._replace_with_new_dims(variables, coord_names, indexes)
1984 
1985         # Extract coordinates from indexers
1986         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
1987         variables.update(coord_vars)
1988         indexes.update(new_indexes)
1989         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
1990         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
1991 
1992     def sel(
1993         self,
1994         indexers: Mapping[Hashable, Any] = None,
1995         method: str = None,
1996         tolerance: Number = None,
1997         drop: bool = False,
1998         **indexers_kwargs: Any,
1999     ) -> "Dataset":
2000         """Returns a new dataset with each array indexed by tick labels
2001         along the specified dimension(s).
2002 
2003         In contrast to `Dataset.isel`, indexers for this method should use
2004         labels instead of integers.
2005 
2006         Under the hood, this method is powered by using pandas's powerful Index
2007         objects. This makes label based indexing essentially just as fast as
2008         using integer indexing.
2009 
2010         It also means this method uses pandas's (well documented) logic for
2011         indexing. This means you can use string shortcuts for datetime indexes
2012         (e.g., '2000-01' to select all values in January 2000). It also means
2013         that slices are treated as inclusive of both the start and stop values,
2014         unlike normal Python indexing.
2015 
2016         Parameters
2017         ----------
2018         indexers : dict, optional
2019             A dict with keys matching dimensions and values given
2020             by scalars, slices or arrays of tick labels. For dimensions with
2021             multi-index, the indexer may also be a dict-like object with keys
2022             matching index level names.
2023             If DataArrays are passed as indexers, xarray-style indexing will be
2024             carried out. See :ref:`indexing` for the details.
2025             One of indexers or indexers_kwargs must be provided.
2026         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2027             Method to use for inexact matches:
2028 
2029             * None (default): only exact matches
2030             * pad / ffill: propagate last valid index value forward
2031             * backfill / bfill: propagate next valid index value backward
2032             * nearest: use nearest valid index value
2033         tolerance : optional
2034             Maximum distance between original and new labels for inexact
2035             matches. The values of the index at the matching locations must
2036             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2037         drop : bool, optional
2038             If ``drop=True``, drop coordinates variables in `indexers` instead
2039             of making them scalar.
2040         **indexers_kwargs : {dim: indexer, ...}, optional
2041             The keyword arguments form of ``indexers``.
2042             One of indexers or indexers_kwargs must be provided.
2043 
2044         Returns
2045         -------
2046         obj : Dataset
2047             A new Dataset with the same contents as this dataset, except each
2048             variable and dimension is indexed by the appropriate indexers.
2049             If indexer DataArrays have coordinates that do not conflict with
2050             this object, then these coordinates will be attached.
2051             In general, each array's data will be a view of the array's data
2052             in this dataset, unless vectorized indexing was triggered by using
2053             an array indexer, in which case the data will be a copy.
2054 
2055 
2056         See Also
2057         --------
2058         Dataset.isel
2059         DataArray.sel
2060         """
2061         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2062         pos_indexers, new_indexes = remap_label_indexers(
2063             self, indexers=indexers, method=method, tolerance=tolerance
2064         )
2065         result = self.isel(indexers=pos_indexers, drop=drop)
2066         return result._overwrite_indexes(new_indexes)
2067 
2068     def head(
2069         self,
2070         indexers: Union[Mapping[Hashable, int], int] = None,
2071         **indexers_kwargs: Any,
2072     ) -> "Dataset":
2073         """Returns a new dataset with the first `n` values of each array
2074         for the specified dimension(s).
2075 
2076         Parameters
2077         ----------
2078         indexers : dict or int, default: 5
2079             A dict with keys matching dimensions and integer values `n`
2080             or a single integer `n` applied over all dimensions.
2081             One of indexers or indexers_kwargs must be provided.
2082         **indexers_kwargs : {dim: n, ...}, optional
2083             The keyword arguments form of ``indexers``.
2084             One of indexers or indexers_kwargs must be provided.
2085 
2086 
2087         See Also
2088         --------
2089         Dataset.tail
2090         Dataset.thin
2091         DataArray.head
2092         """
2093         if not indexers_kwargs:
2094             if indexers is None:
2095                 indexers = 5
2096             if not isinstance(indexers, int) and not is_dict_like(indexers):
2097                 raise TypeError("indexers must be either dict-like or a single integer")
2098         if isinstance(indexers, int):
2099             indexers = {dim: indexers for dim in self.dims}
2100         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2101         for k, v in indexers.items():
2102             if not isinstance(v, int):
2103                 raise TypeError(
2104                     "expected integer type indexer for "
2105                     "dimension %r, found %r" % (k, type(v))
2106                 )
2107             elif v < 0:
2108                 raise ValueError(
2109                     "expected positive integer as indexer "
2110                     "for dimension %r, found %s" % (k, v)
2111                 )
2112         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2113         return self.isel(indexers_slices)
2114 
2115     def tail(
2116         self,
2117         indexers: Union[Mapping[Hashable, int], int] = None,
2118         **indexers_kwargs: Any,
2119     ) -> "Dataset":
2120         """Returns a new dataset with the last `n` values of each array
2121         for the specified dimension(s).
2122 
2123         Parameters
2124         ----------
2125         indexers : dict or int, default: 5
2126             A dict with keys matching dimensions and integer values `n`
2127             or a single integer `n` applied over all dimensions.
2128             One of indexers or indexers_kwargs must be provided.
2129         **indexers_kwargs : {dim: n, ...}, optional
2130             The keyword arguments form of ``indexers``.
2131             One of indexers or indexers_kwargs must be provided.
2132 
2133 
2134         See Also
2135         --------
2136         Dataset.head
2137         Dataset.thin
2138         DataArray.tail
2139         """
2140         if not indexers_kwargs:
2141             if indexers is None:
2142                 indexers = 5
2143             if not isinstance(indexers, int) and not is_dict_like(indexers):
2144                 raise TypeError("indexers must be either dict-like or a single integer")
2145         if isinstance(indexers, int):
2146             indexers = {dim: indexers for dim in self.dims}
2147         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2148         for k, v in indexers.items():
2149             if not isinstance(v, int):
2150                 raise TypeError(
2151                     "expected integer type indexer for "
2152                     "dimension %r, found %r" % (k, type(v))
2153                 )
2154             elif v < 0:
2155                 raise ValueError(
2156                     "expected positive integer as indexer "
2157                     "for dimension %r, found %s" % (k, v)
2158                 )
2159         indexers_slices = {
2160             k: slice(-val, None) if val != 0 else slice(val)
2161             for k, val in indexers.items()
2162         }
2163         return self.isel(indexers_slices)
2164 
2165     def thin(
2166         self,
2167         indexers: Union[Mapping[Hashable, int], int] = None,
2168         **indexers_kwargs: Any,
2169     ) -> "Dataset":
2170         """Returns a new dataset with each array indexed along every `n`-th
2171         value for the specified dimension(s)
2172 
2173         Parameters
2174         ----------
2175         indexers : dict or int
2176             A dict with keys matching dimensions and integer values `n`
2177             or a single integer `n` applied over all dimensions.
2178             One of indexers or indexers_kwargs must be provided.
2179         ``**indexers_kwargs`` : {dim: n, ...}, optional
2180             The keyword arguments form of ``indexers``.
2181             One of indexers or indexers_kwargs must be provided.
2182 
2183 
2184         See Also
2185         --------
2186         Dataset.head
2187         Dataset.tail
2188         DataArray.thin
2189         """
2190         if (
2191             not indexers_kwargs
2192             and not isinstance(indexers, int)
2193             and not is_dict_like(indexers)
2194         ):
2195             raise TypeError("indexers must be either dict-like or a single integer")
2196         if isinstance(indexers, int):
2197             indexers = {dim: indexers for dim in self.dims}
2198         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2199         for k, v in indexers.items():
2200             if not isinstance(v, int):
2201                 raise TypeError(
2202                     "expected integer type indexer for "
2203                     "dimension %r, found %r" % (k, type(v))
2204                 )
2205             elif v < 0:
2206                 raise ValueError(
2207                     "expected positive integer as indexer "
2208                     "for dimension %r, found %s" % (k, v)
2209                 )
2210             elif v == 0:
2211                 raise ValueError("step cannot be zero")
2212         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2213         return self.isel(indexers_slices)
2214 
2215     def broadcast_like(
2216         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2217     ) -> "Dataset":
2218         """Broadcast this DataArray against another Dataset or DataArray.
2219         This is equivalent to xr.broadcast(other, self)[1]
2220 
2221         Parameters
2222         ----------
2223         other : Dataset or DataArray
2224             Object against which to broadcast this array.
2225         exclude : iterable of hashable, optional
2226             Dimensions that must not be broadcasted
2227 
2228         """
2229         if exclude is None:
2230             exclude = set()
2231         else:
2232             exclude = set(exclude)
2233         args = align(other, self, join="outer", copy=False, exclude=exclude)
2234 
2235         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2236 
2237         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2238 
2239     def reindex_like(
2240         self,
2241         other: Union["Dataset", "DataArray"],
2242         method: str = None,
2243         tolerance: Number = None,
2244         copy: bool = True,
2245         fill_value: Any = dtypes.NA,
2246     ) -> "Dataset":
2247         """Conform this object onto the indexes of another object, filling in
2248         missing values with ``fill_value``. The default fill value is NaN.
2249 
2250         Parameters
2251         ----------
2252         other : Dataset or DataArray
2253             Object with an 'indexes' attribute giving a mapping from dimension
2254             names to pandas.Index objects, which provides coordinates upon
2255             which to index the variables in this dataset. The indexes on this
2256             other object need not be the same as the indexes on this
2257             dataset. Any mis-matched index values will be filled in with
2258             NaN, and any mis-matched dimension names will simply be ignored.
2259         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2260             Method to use for filling index values from other not found in this
2261             dataset:
2262 
2263             * None (default): don't fill gaps
2264             * pad / ffill: propagate last valid index value forward
2265             * backfill / bfill: propagate next valid index value backward
2266             * nearest: use nearest valid index value
2267         tolerance : optional
2268             Maximum distance between original and new labels for inexact
2269             matches. The values of the index at the matching locations must
2270             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2271         copy : bool, optional
2272             If ``copy=True``, data in the return value is always copied. If
2273             ``copy=False`` and reindexing is unnecessary, or can be performed
2274             with only slice operations, then the output may share memory with
2275             the input. In either case, a new xarray object is always returned.
2276         fill_value : scalar, optional
2277             Value to use for newly missing values
2278 
2279         Returns
2280         -------
2281         reindexed : Dataset
2282             Another dataset, with this dataset's data but coordinates from the
2283             other object.
2284 
2285         See Also
2286         --------
2287         Dataset.reindex
2288         align
2289         """
2290         indexers = alignment.reindex_like_indexers(self, other)
2291         return self.reindex(
2292             indexers=indexers,
2293             method=method,
2294             copy=copy,
2295             fill_value=fill_value,
2296             tolerance=tolerance,
2297         )
2298 
2299     def reindex(
2300         self,
2301         indexers: Mapping[Hashable, Any] = None,
2302         method: str = None,
2303         tolerance: Number = None,
2304         copy: bool = True,
2305         fill_value: Any = dtypes.NA,
2306         **indexers_kwargs: Any,
2307     ) -> "Dataset":
2308         """Conform this object onto a new set of indexes, filling in
2309         missing values with ``fill_value``. The default fill value is NaN.
2310 
2311         Parameters
2312         ----------
2313         indexers : dict. optional
2314             Dictionary with keys given by dimension names and values given by
2315             arrays of coordinates tick labels. Any mis-matched coordinate
2316             values will be filled in with NaN, and any mis-matched dimension
2317             names will simply be ignored.
2318             One of indexers or indexers_kwargs must be provided.
2319         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2320             Method to use for filling index values in ``indexers`` not found in
2321             this dataset:
2322 
2323             * None (default): don't fill gaps
2324             * pad / ffill: propagate last valid index value forward
2325             * backfill / bfill: propagate next valid index value backward
2326             * nearest: use nearest valid index value
2327         tolerance : optional
2328             Maximum distance between original and new labels for inexact
2329             matches. The values of the index at the matching locations must
2330             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2331         copy : bool, optional
2332             If ``copy=True``, data in the return value is always copied. If
2333             ``copy=False`` and reindexing is unnecessary, or can be performed
2334             with only slice operations, then the output may share memory with
2335             the input. In either case, a new xarray object is always returned.
2336         fill_value : scalar, optional
2337             Value to use for newly missing values
2338         sparse: use sparse-array. By default, False
2339         **indexers_kwargs : {dim: indexer, ...}, optional
2340             Keyword arguments in the same form as ``indexers``.
2341             One of indexers or indexers_kwargs must be provided.
2342 
2343         Returns
2344         -------
2345         reindexed : Dataset
2346             Another dataset, with this dataset's data but replaced coordinates.
2347 
2348         See Also
2349         --------
2350         Dataset.reindex_like
2351         align
2352         pandas.Index.get_indexer
2353 
2354         Examples
2355         --------
2356 
2357         Create a dataset with some fictional data.
2358 
2359         >>> import xarray as xr
2360         >>> import pandas as pd
2361         >>> x = xr.Dataset(
2362         ...     {
2363         ...         "temperature": ("station", 20 * np.random.rand(4)),
2364         ...         "pressure": ("station", 500 * np.random.rand(4)),
2365         ...     },
2366         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2367         ... )
2368         >>> x
2369         <xarray.Dataset>
2370         Dimensions:      (station: 4)
2371         Coordinates:
2372         * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2373         Data variables:
2374             temperature  (station) float64 18.84 14.59 19.22 17.16
2375             pressure     (station) float64 324.1 194.3 122.8 244.3
2376         >>> x.indexes
2377         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2378 
2379         Create a new index and reindex the dataset. By default values in the new index that
2380         do not have corresponding records in the dataset are assigned `NaN`.
2381 
2382         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2383         >>> x.reindex({"station": new_index})
2384         <xarray.Dataset>
2385         Dimensions:      (station: 4)
2386         Coordinates:
2387         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2388         Data variables:
2389             temperature  (station) float64 18.84 nan 19.22 nan
2390             pressure     (station) float64 324.1 nan 122.8 nan
2391 
2392         We can fill in the missing values by passing a value to the keyword `fill_value`.
2393 
2394         >>> x.reindex({"station": new_index}, fill_value=0)
2395         <xarray.Dataset>
2396         Dimensions:      (station: 4)
2397         Coordinates:
2398         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2399         Data variables:
2400             temperature  (station) float64 18.84 0.0 19.22 0.0
2401             pressure     (station) float64 324.1 0.0 122.8 0.0
2402 
2403         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2404         to the keyword method to fill the `NaN` values.
2405 
2406         >>> x.reindex({"station": new_index}, method="nearest")
2407         Traceback (most recent call last):
2408         ...
2409             raise ValueError('index must be monotonic increasing or decreasing')
2410         ValueError: index must be monotonic increasing or decreasing
2411 
2412         To further illustrate the filling functionality in reindex, we will create a
2413         dataset with a monotonically increasing index (for example, a sequence of dates).
2414 
2415         >>> x2 = xr.Dataset(
2416         ...     {
2417         ...         "temperature": (
2418         ...             "time",
2419         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2420         ...         ),
2421         ...         "pressure": ("time", 500 * np.random.rand(6)),
2422         ...     },
2423         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2424         ... )
2425         >>> x2
2426         <xarray.Dataset>
2427         Dimensions:      (time: 6)
2428         Coordinates:
2429         * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2430         Data variables:
2431             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2432             pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0
2433 
2434         Suppose we decide to expand the dataset to cover a wider date range.
2435 
2436         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2437         >>> x2.reindex({"time": time_index2})
2438         <xarray.Dataset>
2439         Dimensions:      (time: 10)
2440         Coordinates:
2441         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2442         Data variables:
2443             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2444             pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan
2445 
2446         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2447         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2448 
2449         For example, to back-propagate the last valid value to fill the `NaN` values,
2450         pass `bfill` as an argument to the `method` keyword.
2451 
2452         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2453         >>> x3
2454         <xarray.Dataset>
2455         Dimensions:      (time: 10)
2456         Coordinates:
2457         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2458         Data variables:
2459             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2460             pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan
2461 
2462         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2463         will not be filled by any of the value propagation schemes.
2464 
2465         >>> x2.where(x2.temperature.isnull(), drop=True)
2466         <xarray.Dataset>
2467         Dimensions:      (time: 1)
2468         Coordinates:
2469         * time         (time) datetime64[ns] 2019-01-03
2470         Data variables:
2471             temperature  (time) float64 nan
2472             pressure     (time) float64 452.0
2473         >>> x3.where(x3.temperature.isnull(), drop=True)
2474         <xarray.Dataset>
2475         Dimensions:      (time: 2)
2476         Coordinates:
2477         * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2478         Data variables:
2479             temperature  (time) float64 nan nan
2480             pressure     (time) float64 452.0 nan
2481 
2482         This is because filling while reindexing does not look at dataset values, but only compares
2483         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2484         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2485 
2486         """
2487         return self._reindex(
2488             indexers,
2489             method,
2490             tolerance,
2491             copy,
2492             fill_value,
2493             sparse=False,
2494             **indexers_kwargs,
2495         )
2496 
2497     def _reindex(
2498         self,
2499         indexers: Mapping[Hashable, Any] = None,
2500         method: str = None,
2501         tolerance: Number = None,
2502         copy: bool = True,
2503         fill_value: Any = dtypes.NA,
2504         sparse: bool = False,
2505         **indexers_kwargs: Any,
2506     ) -> "Dataset":
2507         """
2508         same to _reindex but support sparse option
2509         """
2510         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2511 
2512         bad_dims = [d for d in indexers if d not in self.dims]
2513         if bad_dims:
2514             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2515 
2516         variables, indexes = alignment.reindex_variables(
2517             self.variables,
2518             self.sizes,
2519             self.indexes,
2520             indexers,
2521             method,
2522             tolerance,
2523             copy=copy,
2524             fill_value=fill_value,
2525             sparse=sparse,
2526         )
2527         coord_names = set(self._coord_names)
2528         coord_names.update(indexers)
2529         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2530 
2531     def interp(
2532         self,
2533         coords: Mapping[Hashable, Any] = None,
2534         method: str = "linear",
2535         assume_sorted: bool = False,
2536         kwargs: Mapping[str, Any] = None,
2537         **coords_kwargs: Any,
2538     ) -> "Dataset":
2539         """ Multidimensional interpolation of Dataset.
2540 
2541         Parameters
2542         ----------
2543         coords : dict, optional
2544             Mapping from dimension names to the new coordinates.
2545             New coordinate can be a scalar, array-like or DataArray.
2546             If DataArrays are passed as new coordates, their dimensions are
2547             used for the broadcasting.
2548         method: string, optional.
2549             {'linear', 'nearest'} for multidimensional array,
2550             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2551             for 1-dimensional array. 'linear' is used by default.
2552         assume_sorted: boolean, optional
2553             If False, values of coordinates that are interpolated over can be
2554             in any order and they are sorted first. If True, interpolated
2555             coordinates are assumed to be an array of monotonically increasing
2556             values.
2557         kwargs: dictionary, optional
2558             Additional keyword passed to scipy's interpolator.
2559         **coords_kwargs : {dim: coordinate, ...}, optional
2560             The keyword arguments form of ``coords``.
2561             One of coords or coords_kwargs must be provided.
2562 
2563         Returns
2564         -------
2565         interpolated: xr.Dataset
2566             New dataset on the new coordinates.
2567 
2568         Notes
2569         -----
2570         scipy is required.
2571 
2572         See Also
2573         --------
2574         scipy.interpolate.interp1d
2575         scipy.interpolate.interpn
2576         """
2577         from . import missing
2578 
2579         if kwargs is None:
2580             kwargs = {}
2581 
2582         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2583         indexers = dict(self._validate_interp_indexers(coords))
2584 
2585         if coords:
2586             # This avoids broadcasting over coordinates that are both in
2587             # the original array AND in the indexing array. It essentially
2588             # forces interpolation along the shared coordinates.
2589             sdims = (
2590                 set(self.dims)
2591                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2592                 .difference(coords.keys())
2593             )
2594             indexers.update({d: self.variables[d] for d in sdims})
2595 
2596         obj = self if assume_sorted else self.sortby([k for k in coords])
2597 
2598         def maybe_variable(obj, k):
2599             # workaround to get variable for dimension without coordinate.
2600             try:
2601                 return obj._variables[k]
2602             except KeyError:
2603                 return as_variable((k, range(obj.dims[k])))
2604 
2605         def _validate_interp_indexer(x, new_x):
2606             # In the case of datetimes, the restrictions placed on indexers
2607             # used with interp are stronger than those which are placed on
2608             # isel, so we need an additional check after _validate_indexers.
2609             if _contains_datetime_like_objects(
2610                 x
2611             ) and not _contains_datetime_like_objects(new_x):
2612                 raise TypeError(
2613                     "When interpolating over a datetime-like "
2614                     "coordinate, the coordinates to "
2615                     "interpolate to must be either datetime "
2616                     "strings or datetimes. "
2617                     "Instead got\n{}".format(new_x)
2618                 )
2619             return x, new_x
2620 
2621         variables: Dict[Hashable, Variable] = {}
2622         for name, var in obj._variables.items():
2623             if name in indexers:
2624                 continue
2625 
2626             if var.dtype.kind in "uifc":
2627                 var_indexers = {
2628                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2629                     for k, v in indexers.items()
2630                     if k in var.dims
2631                 }
2632                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2633             elif all(d not in indexers for d in var.dims):
2634                 # keep unrelated object array
2635                 variables[name] = var
2636 
2637         coord_names = obj._coord_names & variables.keys()
2638         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2639         selected = self._replace_with_new_dims(
2640             variables.copy(), coord_names, indexes=indexes
2641         )
2642 
2643         # attach indexer as coordinate
2644         variables.update(indexers)
2645         for k, v in indexers.items():
2646             assert isinstance(v, Variable)
2647             if v.dims == (k,):
2648                 indexes[k] = v.to_index()
2649 
2650         # Extract coordinates from indexers
2651         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2652         variables.update(coord_vars)
2653         indexes.update(new_indexes)
2654 
2655         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2656         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2657 
2658     def interp_like(
2659         self,
2660         other: Union["Dataset", "DataArray"],
2661         method: str = "linear",
2662         assume_sorted: bool = False,
2663         kwargs: Mapping[str, Any] = None,
2664     ) -> "Dataset":
2665         """Interpolate this object onto the coordinates of another object,
2666         filling the out of range values with NaN.
2667 
2668         Parameters
2669         ----------
2670         other : Dataset or DataArray
2671             Object with an 'indexes' attribute giving a mapping from dimension
2672             names to an 1d array-like, which provides coordinates upon
2673             which to index the variables in this dataset.
2674         method: string, optional.
2675             {'linear', 'nearest'} for multidimensional array,
2676             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2677             for 1-dimensional array. 'linear' is used by default.
2678         assume_sorted: boolean, optional
2679             If False, values of coordinates that are interpolated over can be
2680             in any order and they are sorted first. If True, interpolated
2681             coordinates are assumed to be an array of monotonically increasing
2682             values.
2683         kwargs: dictionary, optional
2684             Additional keyword passed to scipy's interpolator.
2685 
2686         Returns
2687         -------
2688         interpolated: xr.Dataset
2689             Another dataset by interpolating this dataset's data along the
2690             coordinates of the other object.
2691 
2692         Notes
2693         -----
2694         scipy is required.
2695         If the dataset has object-type coordinates, reindex is used for these
2696         coordinates instead of the interpolation.
2697 
2698         See Also
2699         --------
2700         Dataset.interp
2701         Dataset.reindex_like
2702         """
2703         if kwargs is None:
2704             kwargs = {}
2705         coords = alignment.reindex_like_indexers(self, other)
2706 
2707         numeric_coords: Dict[Hashable, pd.Index] = {}
2708         object_coords: Dict[Hashable, pd.Index] = {}
2709         for k, v in coords.items():
2710             if v.dtype.kind in "uifcMm":
2711                 numeric_coords[k] = v
2712             else:
2713                 object_coords[k] = v
2714 
2715         ds = self
2716         if object_coords:
2717             # We do not support interpolation along object coordinate.
2718             # reindex instead.
2719             ds = self.reindex(object_coords)
2720         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2721 
2722     # Helper methods for rename()
2723     def _rename_vars(self, name_dict, dims_dict):
2724         variables = {}
2725         coord_names = set()
2726         for k, v in self.variables.items():
2727             var = v.copy(deep=False)
2728             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2729             name = name_dict.get(k, k)
2730             if name in variables:
2731                 raise ValueError(f"the new name {name!r} conflicts")
2732             variables[name] = var
2733             if k in self._coord_names:
2734                 coord_names.add(name)
2735         return variables, coord_names
2736 
2737     def _rename_dims(self, name_dict):
2738         return {name_dict.get(k, k): v for k, v in self.dims.items()}
2739 
2740     def _rename_indexes(self, name_dict, dims_set):
2741         if self._indexes is None:
2742             return None
2743         indexes = {}
2744         for k, v in self.indexes.items():
2745             new_name = name_dict.get(k, k)
2746             if new_name not in dims_set:
2747                 continue
2748             if isinstance(v, pd.MultiIndex):
2749                 new_names = [name_dict.get(k, k) for k in v.names]
2750                 index = v.rename(names=new_names)
2751             else:
2752                 index = v.rename(new_name)
2753             indexes[new_name] = index
2754         return indexes
2755 
2756     def _rename_all(self, name_dict, dims_dict):
2757         variables, coord_names = self._rename_vars(name_dict, dims_dict)
2758         dims = self._rename_dims(dims_dict)
2759         indexes = self._rename_indexes(name_dict, dims.keys())
2760         return variables, coord_names, dims, indexes
2761 
2762     def rename(
2763         self,
2764         name_dict: Mapping[Hashable, Hashable] = None,
2765         inplace: bool = None,
2766         **names: Hashable,
2767     ) -> "Dataset":
2768         """Returns a new object with renamed variables and dimensions.
2769 
2770         Parameters
2771         ----------
2772         name_dict : dict-like, optional
2773             Dictionary whose keys are current variable or dimension names and
2774             whose values are the desired names.
2775         **names, optional
2776             Keyword form of ``name_dict``.
2777             One of name_dict or names must be provided.
2778 
2779         Returns
2780         -------
2781         renamed : Dataset
2782             Dataset with renamed variables and dimensions.
2783 
2784         See Also
2785         --------
2786         Dataset.swap_dims
2787         Dataset.rename_vars
2788         Dataset.rename_dims
2789         DataArray.rename
2790         """
2791         _check_inplace(inplace)
2792         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
2793         for k in name_dict.keys():
2794             if k not in self and k not in self.dims:
2795                 raise ValueError(
2796                     "cannot rename %r because it is not a "
2797                     "variable or dimension in this dataset" % k
2798                 )
2799 
2800         variables, coord_names, dims, indexes = self._rename_all(
2801             name_dict=name_dict, dims_dict=name_dict
2802         )
2803         assert_unique_multiindex_level_names(variables)
2804         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2805 
2806     def rename_dims(
2807         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
2808     ) -> "Dataset":
2809         """Returns a new object with renamed dimensions only.
2810 
2811         Parameters
2812         ----------
2813         dims_dict : dict-like, optional
2814             Dictionary whose keys are current dimension names and
2815             whose values are the desired names. The desired names must
2816             not be the name of an existing dimension or Variable in the Dataset.
2817         **dims, optional
2818             Keyword form of ``dims_dict``.
2819             One of dims_dict or dims must be provided.
2820 
2821         Returns
2822         -------
2823         renamed : Dataset
2824             Dataset with renamed dimensions.
2825 
2826         See Also
2827         --------
2828         Dataset.swap_dims
2829         Dataset.rename
2830         Dataset.rename_vars
2831         DataArray.rename
2832         """
2833         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
2834         for k, v in dims_dict.items():
2835             if k not in self.dims:
2836                 raise ValueError(
2837                     "cannot rename %r because it is not a "
2838                     "dimension in this dataset" % k
2839                 )
2840             if v in self.dims or v in self:
2841                 raise ValueError(
2842                     f"Cannot rename {k} to {v} because {v} already exists. "
2843                     "Try using swap_dims instead."
2844                 )
2845 
2846         variables, coord_names, sizes, indexes = self._rename_all(
2847             name_dict={}, dims_dict=dims_dict
2848         )
2849         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
2850 
2851     def rename_vars(
2852         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
2853     ) -> "Dataset":
2854         """Returns a new object with renamed variables including coordinates
2855 
2856         Parameters
2857         ----------
2858         name_dict : dict-like, optional
2859             Dictionary whose keys are current variable or coordinate names and
2860             whose values are the desired names.
2861         **names, optional
2862             Keyword form of ``name_dict``.
2863             One of name_dict or names must be provided.
2864 
2865         Returns
2866         -------
2867         renamed : Dataset
2868             Dataset with renamed variables including coordinates
2869 
2870         See Also
2871         --------
2872         Dataset.swap_dims
2873         Dataset.rename
2874         Dataset.rename_dims
2875         DataArray.rename
2876         """
2877         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
2878         for k in name_dict:
2879             if k not in self:
2880                 raise ValueError(
2881                     "cannot rename %r because it is not a "
2882                     "variable or coordinate in this dataset" % k
2883                 )
2884         variables, coord_names, dims, indexes = self._rename_all(
2885             name_dict=name_dict, dims_dict={}
2886         )
2887         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2888 
2889     def swap_dims(
2890         self, dims_dict: Mapping[Hashable, Hashable], inplace: bool = None
2891     ) -> "Dataset":
2892         """Returns a new object with swapped dimensions.
2893 
2894         Parameters
2895         ----------
2896         dims_dict : dict-like
2897             Dictionary whose keys are current dimension names and whose values
2898             are new names.
2899 
2900         Returns
2901         -------
2902         swapped : Dataset
2903             Dataset with swapped dimensions.
2904 
2905         Examples
2906         --------
2907         >>> ds = xr.Dataset(
2908         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
2909         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2910         ... )
2911         >>> ds
2912         <xarray.Dataset>
2913         Dimensions:  (x: 2)
2914         Coordinates:
2915           * x        (x) <U1 'a' 'b'
2916             y        (x) int64 0 1
2917         Data variables:
2918             a        (x) int64 5 7
2919             b        (x) float64 0.1 2.4
2920 
2921         >>> ds.swap_dims({"x": "y"})
2922         <xarray.Dataset>
2923         Dimensions:  (y: 2)
2924         Coordinates:
2925             x        (y) <U1 'a' 'b'
2926           * y        (y) int64 0 1
2927         Data variables:
2928             a        (y) int64 5 7
2929             b        (y) float64 0.1 2.4
2930 
2931         >>> ds.swap_dims({"x": "z"})
2932         <xarray.Dataset>
2933         Dimensions:  (z: 2)
2934         Coordinates:
2935             x        (z) <U1 'a' 'b'
2936             y        (z) int64 0 1
2937         Dimensions without coordinates: z
2938         Data variables:
2939             a        (z) int64 5 7
2940             b        (z) float64 0.1 2.4
2941 
2942         See Also
2943         --------
2944 
2945         Dataset.rename
2946         DataArray.swap_dims
2947         """
2948         # TODO: deprecate this method in favor of a (less confusing)
2949         # rename_dims() method that only renames dimensions.
2950         _check_inplace(inplace)
2951         for k, v in dims_dict.items():
2952             if k not in self.dims:
2953                 raise ValueError(
2954                     "cannot swap from dimension %r because it is "
2955                     "not an existing dimension" % k
2956                 )
2957             if v in self.variables and self.variables[v].dims != (k,):
2958                 raise ValueError(
2959                     "replacement dimension %r is not a 1D "
2960                     "variable along the old dimension %r" % (v, k)
2961                 )
2962 
2963         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
2964 
2965         coord_names = self._coord_names.copy()
2966         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
2967 
2968         variables: Dict[Hashable, Variable] = {}
2969         indexes: Dict[Hashable, pd.Index] = {}
2970         for k, v in self.variables.items():
2971             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2972             if k in result_dims:
2973                 var = v.to_index_variable()
2974                 if k in self.indexes:
2975                     indexes[k] = self.indexes[k]
2976                 else:
2977                     new_index = var.to_index()
2978                     if new_index.nlevels == 1:
2979                         # make sure index name matches dimension name
2980                         new_index = new_index.rename(k)
2981                     indexes[k] = new_index
2982             else:
2983                 var = v.to_base_variable()
2984             var.dims = dims
2985             variables[k] = var
2986 
2987         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2988 
2989     def expand_dims(
2990         self,
2991         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
2992         axis: Union[None, int, Sequence[int]] = None,
2993         **dim_kwargs: Any,
2994     ) -> "Dataset":
2995         """Return a new object with an additional axis (or axes) inserted at
2996         the corresponding position in the array shape.  The new object is a
2997         view into the underlying array, not a copy.
2998 
2999         If dim is already a scalar coordinate, it will be promoted to a 1D
3000         coordinate consisting of a single value.
3001 
3002         Parameters
3003         ----------
3004         dim : hashable, sequence of hashable, mapping, or None
3005             Dimensions to include on the new variable. If provided as hashable
3006             or sequence of hashable, then dimensions are inserted with length
3007             1. If provided as a mapping, then the keys are the new dimensions
3008             and the values are either integers (giving the length of the new
3009             dimensions) or array-like (giving the coordinates of the new
3010             dimensions).
3011         axis : integer, sequence of integers, or None
3012             Axis position(s) where new axis is to be inserted (position(s) on
3013             the result array). If a list (or tuple) of integers is passed,
3014             multiple axes are inserted. In this case, dim arguments should be
3015             same length list. If axis=None is passed, all the axes will be
3016             inserted to the start of the result array.
3017         **dim_kwargs : int or sequence/ndarray
3018             The keywords are arbitrary dimensions being inserted and the values
3019             are either the lengths of the new dims (if int is given), or their
3020             coordinates. Note, this is an alternative to passing a dict to the
3021             dim kwarg and will only be used if dim is None.
3022 
3023         Returns
3024         -------
3025         expanded : same type as caller
3026             This object, but with an additional dimension(s).
3027         """
3028         if dim is None:
3029             pass
3030         elif isinstance(dim, Mapping):
3031             # We're later going to modify dim in place; don't tamper with
3032             # the input
3033             dim = dict(dim)
3034         elif isinstance(dim, int):
3035             raise TypeError(
3036                 "dim should be hashable or sequence of hashables or mapping"
3037             )
3038         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3039             dim = {dim: 1}
3040         elif isinstance(dim, Sequence):
3041             if len(dim) != len(set(dim)):
3042                 raise ValueError("dims should not contain duplicate values.")
3043             dim = {d: 1 for d in dim}
3044 
3045         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3046         assert isinstance(dim, MutableMapping)
3047 
3048         if axis is None:
3049             axis = list(range(len(dim)))
3050         elif not isinstance(axis, Sequence):
3051             axis = [axis]
3052 
3053         if len(dim) != len(axis):
3054             raise ValueError("lengths of dim and axis should be identical.")
3055         for d in dim:
3056             if d in self.dims:
3057                 raise ValueError(f"Dimension {d} already exists.")
3058             if d in self._variables and not utils.is_scalar(self._variables[d]):
3059                 raise ValueError(
3060                     "{dim} already exists as coordinate or"
3061                     " variable name.".format(dim=d)
3062                 )
3063 
3064         variables: Dict[Hashable, Variable] = {}
3065         coord_names = self._coord_names.copy()
3066         # If dim is a dict, then ensure that the values are either integers
3067         # or iterables.
3068         for k, v in dim.items():
3069             if hasattr(v, "__iter__"):
3070                 # If the value for the new dimension is an iterable, then
3071                 # save the coordinates to the variables dict, and set the
3072                 # value within the dim dict to the length of the iterable
3073                 # for later use.
3074                 variables[k] = xr.IndexVariable((k,), v)
3075                 coord_names.add(k)
3076                 dim[k] = variables[k].size
3077             elif isinstance(v, int):
3078                 pass  # Do nothing if the dimensions value is just an int
3079             else:
3080                 raise TypeError(
3081                     "The value of new dimension {k} must be "
3082                     "an iterable or an int".format(k=k)
3083                 )
3084 
3085         for k, v in self._variables.items():
3086             if k not in dim:
3087                 if k in coord_names:  # Do not change coordinates
3088                     variables[k] = v
3089                 else:
3090                     result_ndim = len(v.dims) + len(axis)
3091                     for a in axis:
3092                         if a < -result_ndim or result_ndim - 1 < a:
3093                             raise IndexError(
3094                                 f"Axis {a} of variable {k} is out of bounds of the "
3095                                 f"expanded dimension size {result_ndim}"
3096                             )
3097 
3098                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3099                     if len(axis_pos) != len(set(axis_pos)):
3100                         raise ValueError("axis should not contain duplicate values")
3101                     # We need to sort them to make sure `axis` equals to the
3102                     # axis positions of the result array.
3103                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3104 
3105                     all_dims = list(zip(v.dims, v.shape))
3106                     for d, c in zip_axis_dim:
3107                         all_dims.insert(d, c)
3108                     variables[k] = v.set_dims(dict(all_dims))
3109             else:
3110                 # If dims includes a label of a non-dimension coordinate,
3111                 # it will be promoted to a 1D coordinate with a single value.
3112                 variables[k] = v.set_dims(k).to_index_variable()
3113 
3114         new_dims = self._dims.copy()
3115         new_dims.update(dim)
3116 
3117         return self._replace_vars_and_dims(
3118             variables, dims=new_dims, coord_names=coord_names
3119         )
3120 
3121     def set_index(
3122         self,
3123         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3124         append: bool = False,
3125         inplace: bool = None,
3126         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3127     ) -> "Dataset":
3128         """Set Dataset (multi-)indexes using one or more existing coordinates
3129         or variables.
3130 
3131         Parameters
3132         ----------
3133         indexes : {dim: index, ...}
3134             Mapping from names matching dimensions and values given
3135             by (lists of) the names of existing coordinates or variables to set
3136             as new (multi-)index.
3137         append : bool, optional
3138             If True, append the supplied index(es) to the existing index(es).
3139             Otherwise replace the existing index(es) (default).
3140         **indexes_kwargs: optional
3141             The keyword arguments form of ``indexes``.
3142             One of indexes or indexes_kwargs must be provided.
3143 
3144         Returns
3145         -------
3146         obj : Dataset
3147             Another dataset, with this dataset's data but replaced coordinates.
3148 
3149         Examples
3150         --------
3151         >>> arr = xr.DataArray(
3152         ...     data=np.ones((2, 3)),
3153         ...     dims=["x", "y"],
3154         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3155         ... )
3156         >>> ds = xr.Dataset({"v": arr})
3157         >>> ds
3158         <xarray.Dataset>
3159         Dimensions:  (x: 2, y: 3)
3160         Coordinates:
3161           * x        (x) int64 0 1
3162           * y        (y) int64 0 1 2
3163             a        (x) int64 3 4
3164         Data variables:
3165             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3166         >>> ds.set_index(x="a")
3167         <xarray.Dataset>
3168         Dimensions:  (x: 2, y: 3)
3169         Coordinates:
3170           * x        (x) int64 3 4
3171           * y        (y) int64 0 1 2
3172         Data variables:
3173             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3174 
3175         See Also
3176         --------
3177         Dataset.reset_index
3178         Dataset.swap_dims
3179         """
3180         _check_inplace(inplace)
3181         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3182         variables, coord_names = merge_indexes(
3183             indexes, self._variables, self._coord_names, append=append
3184         )
3185         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3186 
3187     def reset_index(
3188         self,
3189         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3190         drop: bool = False,
3191         inplace: bool = None,
3192     ) -> "Dataset":
3193         """Reset the specified index(es) or multi-index level(s).
3194 
3195         Parameters
3196         ----------
3197         dims_or_levels : str or list
3198             Name(s) of the dimension(s) and/or multi-index level(s) that will
3199             be reset.
3200         drop : bool, optional
3201             If True, remove the specified indexes and/or multi-index levels
3202             instead of extracting them as new coordinates (default: False).
3203 
3204         Returns
3205         -------
3206         obj : Dataset
3207             Another dataset, with this dataset's data but replaced coordinates.
3208 
3209         See Also
3210         --------
3211         Dataset.set_index
3212         """
3213         _check_inplace(inplace)
3214         variables, coord_names = split_indexes(
3215             dims_or_levels,
3216             self._variables,
3217             self._coord_names,
3218             cast(Mapping[Hashable, Hashable], self._level_coords),
3219             drop=drop,
3220         )
3221         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3222 
3223     def reorder_levels(
3224         self,
3225         dim_order: Mapping[Hashable, Sequence[int]] = None,
3226         inplace: bool = None,
3227         **dim_order_kwargs: Sequence[int],
3228     ) -> "Dataset":
3229         """Rearrange index levels using input order.
3230 
3231         Parameters
3232         ----------
3233         dim_order : optional
3234             Mapping from names matching dimensions and values given
3235             by lists representing new level orders. Every given dimension
3236             must have a multi-index.
3237         **dim_order_kwargs: optional
3238             The keyword arguments form of ``dim_order``.
3239             One of dim_order or dim_order_kwargs must be provided.
3240 
3241         Returns
3242         -------
3243         obj : Dataset
3244             Another dataset, with this dataset's data but replaced
3245             coordinates.
3246         """
3247         _check_inplace(inplace)
3248         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3249         variables = self._variables.copy()
3250         indexes = dict(self.indexes)
3251         for dim, order in dim_order.items():
3252             coord = self._variables[dim]
3253             index = self.indexes[dim]
3254             if not isinstance(index, pd.MultiIndex):
3255                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3256             new_index = index.reorder_levels(order)
3257             variables[dim] = IndexVariable(coord.dims, new_index)
3258             indexes[dim] = new_index
3259 
3260         return self._replace(variables, indexes=indexes)
3261 
3262     def _stack_once(self, dims, new_dim):
3263         variables = {}
3264         for name, var in self.variables.items():
3265             if name not in dims:
3266                 if any(d in var.dims for d in dims):
3267                     add_dims = [d for d in dims if d not in var.dims]
3268                     vdims = list(var.dims) + add_dims
3269                     shape = [self.dims[d] for d in vdims]
3270                     exp_var = var.set_dims(vdims, shape)
3271                     stacked_var = exp_var.stack(**{new_dim: dims})
3272                     variables[name] = stacked_var
3273                 else:
3274                     variables[name] = var.copy(deep=False)
3275 
3276         # consider dropping levels that are unused?
3277         levels = [self.get_index(dim) for dim in dims]
3278         idx = utils.multiindex_from_product_levels(levels, names=dims)
3279         variables[new_dim] = IndexVariable(new_dim, idx)
3280 
3281         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3282 
3283         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3284         indexes[new_dim] = idx
3285 
3286         return self._replace_with_new_dims(
3287             variables, coord_names=coord_names, indexes=indexes
3288         )
3289 
3290     def stack(
3291         self,
3292         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3293         **dimensions_kwargs: Sequence[Hashable],
3294     ) -> "Dataset":
3295         """
3296         Stack any number of existing dimensions into a single new dimension.
3297 
3298         New dimensions will be added at the end, and the corresponding
3299         coordinate variables will be combined into a MultiIndex.
3300 
3301         Parameters
3302         ----------
3303         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
3304             Names of new dimensions, and the existing dimensions that they
3305             replace.
3306         **dimensions_kwargs:
3307             The keyword arguments form of ``dimensions``.
3308             One of dimensions or dimensions_kwargs must be provided.
3309 
3310         Returns
3311         -------
3312         stacked : Dataset
3313             Dataset with stacked data.
3314 
3315         See also
3316         --------
3317         Dataset.unstack
3318         """
3319         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3320         result = self
3321         for new_dim, dims in dimensions.items():
3322             result = result._stack_once(dims, new_dim)
3323         return result
3324 
3325     def to_stacked_array(
3326         self,
3327         new_dim: Hashable,
3328         sample_dims: Sequence[Hashable],
3329         variable_dim: str = "variable",
3330         name: Hashable = None,
3331     ) -> "DataArray":
3332         """Combine variables of differing dimensionality into a DataArray
3333         without broadcasting.
3334 
3335         This method is similar to Dataset.to_array but does not broadcast the
3336         variables.
3337 
3338         Parameters
3339         ----------
3340         new_dim : Hashable
3341             Name of the new stacked coordinate
3342         sample_dims : Sequence[Hashable]
3343             Dimensions that **will not** be stacked. Each array in the dataset
3344             must share these dimensions. For machine learning applications,
3345             these define the dimensions over which samples are drawn.
3346         variable_dim : str, optional
3347             Name of the level in the stacked coordinate which corresponds to
3348             the variables.
3349         name : str, optional
3350             Name of the new data array.
3351 
3352         Returns
3353         -------
3354         stacked : DataArray
3355             DataArray with the specified dimensions and data variables
3356             stacked together. The stacked coordinate is named ``new_dim``
3357             and represented by a MultiIndex object with a level containing the
3358             data variable names. The name of this level is controlled using
3359             the ``variable_dim`` argument.
3360 
3361         See Also
3362         --------
3363         Dataset.to_array
3364         Dataset.stack
3365         DataArray.to_unstacked_dataset
3366 
3367         Examples
3368         --------
3369         >>> data = xr.Dataset(
3370         ...     data_vars={
3371         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3372         ...         "b": ("x", [6, 7]),
3373         ...     },
3374         ...     coords={"y": ["u", "v", "w"]},
3375         ... )
3376 
3377         >>> data
3378         <xarray.Dataset>
3379         Dimensions:  (x: 2, y: 3)
3380         Coordinates:
3381         * y        (y) <U1 'u' 'v' 'w'
3382         Dimensions without coordinates: x
3383         Data variables:
3384             a        (x, y) int64 0 1 2 3 4 5
3385             b        (x) int64 6 7
3386 
3387         >>> data.to_stacked_array("z", sample_dims=["x"])
3388         <xarray.DataArray (x: 2, z: 4)>
3389         array([[0, 1, 2, 6],
3390             [3, 4, 5, 7]])
3391         Coordinates:
3392         * z         (z) MultiIndex
3393         - variable  (z) object 'a' 'a' 'a' 'b'
3394         - y         (z) object 'u' 'v' 'w' nan
3395         Dimensions without coordinates: x
3396 
3397         """
3398         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3399 
3400         for variable in self:
3401             dims = self[variable].dims
3402             dims_include_sample_dims = set(sample_dims) <= set(dims)
3403             if not dims_include_sample_dims:
3404                 raise ValueError(
3405                     "All variables in the dataset must contain the "
3406                     "dimensions {}.".format(dims)
3407                 )
3408 
3409         def ensure_stackable(val):
3410             assign_coords = {variable_dim: val.name}
3411             for dim in stacking_dims:
3412                 if dim not in val.dims:
3413                     assign_coords[dim] = None
3414 
3415             expand_dims = set(stacking_dims).difference(set(val.dims))
3416             expand_dims.add(variable_dim)
3417             # must be list for .expand_dims
3418             expand_dims = list(expand_dims)
3419 
3420             return (
3421                 val.assign_coords(**assign_coords)
3422                 .expand_dims(expand_dims)
3423                 .stack({new_dim: (variable_dim,) + stacking_dims})
3424             )
3425 
3426         # concatenate the arrays
3427         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3428         data_array = xr.concat(stackable_vars, dim=new_dim)
3429 
3430         # coerce the levels of the MultiIndex to have the same type as the
3431         # input dimensions. This code is messy, so it might be better to just
3432         # input a dummy value for the singleton dimension.
3433         idx = data_array.indexes[new_dim]
3434         levels = [idx.levels[0]] + [
3435             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3436         ]
3437         new_idx = idx.set_levels(levels)
3438         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3439 
3440         if name is not None:
3441             data_array.name = name
3442 
3443         return data_array
3444 
3445     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> "Dataset":
3446         index = self.get_index(dim)
3447         index = remove_unused_levels_categories(index)
3448         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3449 
3450         # take a shortcut in case the MultiIndex was not modified.
3451         if index.equals(full_idx):
3452             obj = self
3453         else:
3454             obj = self._reindex(
3455                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3456             )
3457 
3458         new_dim_names = index.names
3459         new_dim_sizes = [lev.size for lev in index.levels]
3460 
3461         variables: Dict[Hashable, Variable] = {}
3462         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3463 
3464         for name, var in obj.variables.items():
3465             if name != dim:
3466                 if dim in var.dims:
3467                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3468                     variables[name] = var.unstack({dim: new_dims})
3469                 else:
3470                     variables[name] = var
3471 
3472         for name, lev in zip(new_dim_names, index.levels):
3473             variables[name] = IndexVariable(name, lev)
3474             indexes[name] = lev
3475 
3476         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3477 
3478         return self._replace_with_new_dims(
3479             variables, coord_names=coord_names, indexes=indexes
3480         )
3481 
3482     def unstack(
3483         self,
3484         dim: Union[Hashable, Iterable[Hashable]] = None,
3485         fill_value: Any = dtypes.NA,
3486         sparse: bool = False,
3487     ) -> "Dataset":
3488         """
3489         Unstack existing dimensions corresponding to MultiIndexes into
3490         multiple new dimensions.
3491 
3492         New dimensions will be added at the end.
3493 
3494         Parameters
3495         ----------
3496         dim : Hashable or iterable of Hashable, optional
3497             Dimension(s) over which to unstack. By default unstacks all
3498             MultiIndexes.
3499         fill_value: value to be filled. By default, np.nan
3500         sparse: use sparse-array if True
3501 
3502         Returns
3503         -------
3504         unstacked : Dataset
3505             Dataset with unstacked data.
3506 
3507         See also
3508         --------
3509         Dataset.stack
3510         """
3511         if dim is None:
3512             dims = [
3513                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3514             ]
3515         else:
3516             if isinstance(dim, str) or not isinstance(dim, Iterable):
3517                 dims = [dim]
3518             else:
3519                 dims = list(dim)
3520 
3521             missing_dims = [d for d in dims if d not in self.dims]
3522             if missing_dims:
3523                 raise ValueError(
3524                     "Dataset does not contain the dimensions: %s" % missing_dims
3525                 )
3526 
3527             non_multi_dims = [
3528                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3529             ]
3530             if non_multi_dims:
3531                 raise ValueError(
3532                     "cannot unstack dimensions that do not "
3533                     "have a MultiIndex: %s" % non_multi_dims
3534                 )
3535 
3536         result = self.copy(deep=False)
3537         for dim in dims:
3538             result = result._unstack_once(dim, fill_value, sparse)
3539         return result
3540 
3541     def update(self, other: "CoercibleMapping", inplace: bool = None) -> "Dataset":
3542         """Update this dataset's variables with those from another dataset.
3543 
3544         Parameters
3545         ----------
3546         other : Dataset or castable to Dataset
3547             Variables with which to update this dataset. One of:
3548 
3549             - Dataset
3550             - mapping {var name: DataArray}
3551             - mapping {var name: Variable}
3552             - mapping {var name: (dimension name, array-like)}
3553             - mapping {var name: (tuple of dimension names, array-like)}
3554 
3555 
3556         Returns
3557         -------
3558         updated : Dataset
3559             Updated dataset.
3560 
3561         Raises
3562         ------
3563         ValueError
3564             If any dimensions would have inconsistent sizes in the updated
3565             dataset.
3566         """
3567         _check_inplace(inplace)
3568         merge_result = dataset_update_method(self, other)
3569         return self._replace(inplace=True, **merge_result._asdict())
3570 
3571     def merge(
3572         self,
3573         other: Union["CoercibleMapping", "DataArray"],
3574         inplace: bool = None,
3575         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3576         compat: str = "no_conflicts",
3577         join: str = "outer",
3578         fill_value: Any = dtypes.NA,
3579     ) -> "Dataset":
3580         """Merge the arrays of two datasets into a single dataset.
3581 
3582         This method generally does not allow for overriding data, with the
3583         exception of attributes, which are ignored on the second dataset.
3584         Variables with the same name are checked for conflicts via the equals
3585         or identical methods.
3586 
3587         Parameters
3588         ----------
3589         other : Dataset or castable to Dataset
3590             Dataset or variables to merge with this dataset.
3591         overwrite_vars : Hashable or iterable of Hashable, optional
3592             If provided, update variables of these name(s) without checking for
3593             conflicts in this dataset.
3594         compat : {'broadcast_equals', 'equals', 'identical',
3595                   'no_conflicts'}, optional
3596             String indicating how to compare variables of the same name for
3597             potential conflicts:
3598 
3599             - 'broadcast_equals': all values must be equal when variables are
3600               broadcast against each other to ensure common dimensions.
3601             - 'equals': all values and dimensions must be the same.
3602             - 'identical': all values, dimensions and attributes must be the
3603               same.
3604             - 'no_conflicts': only values which are not null in both datasets
3605               must be equal. The returned dataset then contains the combination
3606               of all non-null values.
3607 
3608         join : {'outer', 'inner', 'left', 'right', 'exact'}, optional
3609             Method for joining ``self`` and ``other`` along shared dimensions:
3610 
3611             - 'outer': use the union of the indexes
3612             - 'inner': use the intersection of the indexes
3613             - 'left': use indexes from ``self``
3614             - 'right': use indexes from ``other``
3615             - 'exact': error instead of aligning non-equal indexes
3616         fill_value: scalar, optional
3617             Value to use for newly missing values
3618 
3619         Returns
3620         -------
3621         merged : Dataset
3622             Merged dataset.
3623 
3624         Raises
3625         ------
3626         MergeError
3627             If any variables conflict (see ``compat``).
3628         """
3629         _check_inplace(inplace)
3630         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3631         merge_result = dataset_merge_method(
3632             self,
3633             other,
3634             overwrite_vars=overwrite_vars,
3635             compat=compat,
3636             join=join,
3637             fill_value=fill_value,
3638         )
3639         return self._replace(**merge_result._asdict())
3640 
3641     def _assert_all_in_dataset(
3642         self, names: Iterable[Hashable], virtual_okay: bool = False
3643     ) -> None:
3644         bad_names = set(names) - set(self._variables)
3645         if virtual_okay:
3646             bad_names -= self.virtual_variables
3647         if bad_names:
3648             raise ValueError(
3649                 "One or more of the specified variables "
3650                 "cannot be found in this dataset"
3651             )
3652 
3653     def drop_vars(
3654         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3655     ) -> "Dataset":
3656         """Drop variables from this dataset.
3657 
3658         Parameters
3659         ----------
3660         names : hashable or iterable of hashables
3661             Name(s) of variables to drop.
3662         errors: {'raise', 'ignore'}, optional
3663             If 'raise' (default), raises a ValueError error if any of the variable
3664             passed are not in the dataset. If 'ignore', any given names that are in the
3665             dataset are dropped and no error is raised.
3666 
3667         Returns
3668         -------
3669         dropped : Dataset
3670 
3671         """
3672         # the Iterable check is required for mypy
3673         if is_scalar(names) or not isinstance(names, Iterable):
3674             names = {names}
3675         else:
3676             names = set(names)
3677         if errors == "raise":
3678             self._assert_all_in_dataset(names)
3679 
3680         variables = {k: v for k, v in self._variables.items() if k not in names}
3681         coord_names = {k for k in self._coord_names if k in variables}
3682         indexes = {k: v for k, v in self.indexes.items() if k not in names}
3683         return self._replace_with_new_dims(
3684             variables, coord_names=coord_names, indexes=indexes
3685         )
3686 
3687     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
3688         """Backward compatible method based on `drop_vars` and `drop_sel`
3689 
3690         Using either `drop_vars` or `drop_sel` is encouraged
3691 
3692         See Also
3693         --------
3694         Dataset.drop_vars
3695         Dataset.drop_sel
3696         """
3697         if errors not in ["raise", "ignore"]:
3698             raise ValueError('errors must be either "raise" or "ignore"')
3699 
3700         if is_dict_like(labels) and not isinstance(labels, dict):
3701             warnings.warn(
3702                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
3703                 FutureWarning,
3704                 stacklevel=2,
3705             )
3706             return self.drop_vars(labels, errors=errors)
3707 
3708         if labels_kwargs or isinstance(labels, dict):
3709             if dim is not None:
3710                 raise ValueError("cannot specify dim and dict-like arguments.")
3711             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3712 
3713         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
3714             warnings.warn(
3715                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
3716                 PendingDeprecationWarning,
3717                 stacklevel=2,
3718             )
3719             return self.drop_vars(labels, errors=errors)
3720         if dim is not None:
3721             warnings.warn(
3722                 "dropping labels using list-like labels is deprecated; using "
3723                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
3724                 DeprecationWarning,
3725                 stacklevel=2,
3726             )
3727             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
3728 
3729         warnings.warn(
3730             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
3731             PendingDeprecationWarning,
3732             stacklevel=2,
3733         )
3734         return self.drop_sel(labels, errors=errors)
3735 
3736     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
3737         """Drop index labels from this dataset.
3738 
3739         Parameters
3740         ----------
3741         labels : Mapping[Hashable, Any]
3742             Index labels to drop
3743         errors: {'raise', 'ignore'}, optional
3744             If 'raise' (default), raises a ValueError error if
3745             any of the index labels passed are not
3746             in the dataset. If 'ignore', any given labels that are in the
3747             dataset are dropped and no error is raised.
3748         **labels_kwargs : {dim: label, ...}, optional
3749             The keyword arguments form of ``dim`` and ``labels``
3750 
3751         Returns
3752         -------
3753         dropped : Dataset
3754 
3755         Examples
3756         --------
3757         >>> data = np.random.randn(2, 3)
3758         >>> labels = ["a", "b", "c"]
3759         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
3760         >>> ds.drop_sel(y=["a", "c"])
3761         <xarray.Dataset>
3762         Dimensions:  (x: 2, y: 1)
3763         Coordinates:
3764           * y        (y) <U1 'b'
3765         Dimensions without coordinates: x
3766         Data variables:
3767             A        (x, y) float64 -0.3454 0.1734
3768         >>> ds.drop_sel(y="b")
3769         <xarray.Dataset>
3770         Dimensions:  (x: 2, y: 2)
3771         Coordinates:
3772           * y        (y) <U1 'a' 'c'
3773         Dimensions without coordinates: x
3774         Data variables:
3775             A        (x, y) float64 -0.3944 -1.418 1.423 -1.041
3776         """
3777         if errors not in ["raise", "ignore"]:
3778             raise ValueError('errors must be either "raise" or "ignore"')
3779 
3780         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3781 
3782         ds = self
3783         for dim, labels_for_dim in labels.items():
3784             # Don't cast to set, as it would harm performance when labels
3785             # is a large numpy array
3786             if utils.is_scalar(labels_for_dim):
3787                 labels_for_dim = [labels_for_dim]
3788             labels_for_dim = np.asarray(labels_for_dim)
3789             try:
3790                 index = self.indexes[dim]
3791             except KeyError:
3792                 raise ValueError("dimension %r does not have coordinate labels" % dim)
3793             new_index = index.drop(labels_for_dim, errors=errors)
3794             ds = ds.loc[{dim: new_index}]
3795         return ds
3796 
3797     def drop_dims(
3798         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3799     ) -> "Dataset":
3800         """Drop dimensions and associated variables from this dataset.
3801 
3802         Parameters
3803         ----------
3804         drop_dims : hashable or iterable of hashable
3805             Dimension or dimensions to drop.
3806         errors: {'raise', 'ignore'}, optional
3807             If 'raise' (default), raises a ValueError error if any of the
3808             dimensions passed are not in the dataset. If 'ignore', any given
3809             labels that are in the dataset are dropped and no error is raised.
3810 
3811         Returns
3812         -------
3813         obj : Dataset
3814             The dataset without the given dimensions (or any variables
3815             containing those dimensions)
3816         errors: {'raise', 'ignore'}, optional
3817             If 'raise' (default), raises a ValueError error if
3818             any of the dimensions passed are not
3819             in the dataset. If 'ignore', any given dimensions that are in the
3820             dataset are dropped and no error is raised.
3821         """
3822         if errors not in ["raise", "ignore"]:
3823             raise ValueError('errors must be either "raise" or "ignore"')
3824 
3825         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
3826             drop_dims = {drop_dims}
3827         else:
3828             drop_dims = set(drop_dims)
3829 
3830         if errors == "raise":
3831             missing_dims = drop_dims - set(self.dims)
3832             if missing_dims:
3833                 raise ValueError(
3834                     "Dataset does not contain the dimensions: %s" % missing_dims
3835                 )
3836 
3837         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
3838         return self.drop_vars(drop_vars)
3839 
3840     def transpose(self, *dims: Hashable) -> "Dataset":
3841         """Return a new Dataset object with all array dimensions transposed.
3842 
3843         Although the order of dimensions on each array will change, the dataset
3844         dimensions themselves will remain in fixed (sorted) order.
3845 
3846         Parameters
3847         ----------
3848         *dims : Hashable, optional
3849             By default, reverse the dimensions on each array. Otherwise,
3850             reorder the dimensions to this order.
3851 
3852         Returns
3853         -------
3854         transposed : Dataset
3855             Each array in the dataset (including) coordinates will be
3856             transposed to the given order.
3857 
3858         Notes
3859         -----
3860         This operation returns a view of each array's data. It is
3861         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
3862         -- the data will be fully loaded into memory.
3863 
3864         See Also
3865         --------
3866         numpy.transpose
3867         DataArray.transpose
3868         """
3869         if dims:
3870             if set(dims) ^ set(self.dims) and ... not in dims:
3871                 raise ValueError(
3872                     "arguments to transpose (%s) must be "
3873                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
3874                 )
3875         ds = self.copy()
3876         for name, var in self._variables.items():
3877             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
3878             ds._variables[name] = var.transpose(*var_dims)
3879         return ds
3880 
3881     def dropna(
3882         self,
3883         dim: Hashable,
3884         how: str = "any",
3885         thresh: int = None,
3886         subset: Iterable[Hashable] = None,
3887     ):
3888         """Returns a new dataset with dropped labels for missing values along
3889         the provided dimension.
3890 
3891         Parameters
3892         ----------
3893         dim : Hashable
3894             Dimension along which to drop missing values. Dropping along
3895             multiple dimensions simultaneously is not yet supported.
3896         how : {'any', 'all'}, optional
3897             * any : if any NA values are present, drop that label
3898             * all : if all values are NA, drop that label
3899         thresh : int, default None
3900             If supplied, require this many non-NA values.
3901         subset : iterable of hashable, optional
3902             Which variables to check for missing values. By default, all
3903             variables in the dataset are checked.
3904 
3905         Returns
3906         -------
3907         Dataset
3908         """
3909         # TODO: consider supporting multiple dimensions? Or not, given that
3910         # there are some ugly edge cases, e.g., pandas's dropna differs
3911         # depending on the order of the supplied axes.
3912 
3913         if dim not in self.dims:
3914             raise ValueError("%s must be a single dataset dimension" % dim)
3915 
3916         if subset is None:
3917             subset = iter(self.data_vars)
3918 
3919         count = np.zeros(self.dims[dim], dtype=np.int64)
3920         size = 0
3921 
3922         for k in subset:
3923             array = self._variables[k]
3924             if dim in array.dims:
3925                 dims = [d for d in array.dims if d != dim]
3926                 count += np.asarray(array.count(dims))  # type: ignore
3927                 size += np.prod([self.dims[d] for d in dims])
3928 
3929         if thresh is not None:
3930             mask = count >= thresh
3931         elif how == "any":
3932             mask = count == size
3933         elif how == "all":
3934             mask = count > 0
3935         elif how is not None:
3936             raise ValueError("invalid how option: %s" % how)
3937         else:
3938             raise TypeError("must specify how or thresh")
3939 
3940         return self.isel({dim: mask})
3941 
3942     def fillna(self, value: Any) -> "Dataset":
3943         """Fill missing values in this object.
3944 
3945         This operation follows the normal broadcasting and alignment rules that
3946         xarray uses for binary arithmetic, except the result is aligned to this
3947         object (``join='left'``) instead of aligned to the intersection of
3948         index coordinates (``join='inner'``).
3949 
3950         Parameters
3951         ----------
3952         value : scalar, ndarray, DataArray, dict or Dataset
3953             Used to fill all matching missing values in this dataset's data
3954             variables. Scalars, ndarrays or DataArrays arguments are used to
3955             fill all data with aligned coordinates (for DataArrays).
3956             Dictionaries or datasets match data variables and then align
3957             coordinates if necessary.
3958 
3959         Returns
3960         -------
3961         Dataset
3962 
3963         Examples
3964         --------
3965 
3966         >>> import numpy as np
3967         >>> import xarray as xr
3968         >>> ds = xr.Dataset(
3969         ...     {
3970         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
3971         ...         "B": ("x", [3, 4, np.nan, 1]),
3972         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
3973         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
3974         ...     },
3975         ...     coords={"x": [0, 1, 2, 3]},
3976         ... )
3977         >>> ds
3978         <xarray.Dataset>
3979         Dimensions:  (x: 4)
3980         Coordinates:
3981         * x        (x) int64 0 1 2 3
3982         Data variables:
3983             A        (x) float64 nan 2.0 nan 0.0
3984             B        (x) float64 3.0 4.0 nan 1.0
3985             C        (x) float64 nan nan nan 5.0
3986             D        (x) float64 nan 3.0 nan 4.0
3987 
3988         Replace all `NaN` values with 0s.
3989 
3990         >>> ds.fillna(0)
3991         <xarray.Dataset>
3992         Dimensions:  (x: 4)
3993         Coordinates:
3994         * x        (x) int64 0 1 2 3
3995         Data variables:
3996             A        (x) float64 0.0 2.0 0.0 0.0
3997             B        (x) float64 3.0 4.0 0.0 1.0
3998             C        (x) float64 0.0 0.0 0.0 5.0
3999             D        (x) float64 0.0 3.0 0.0 4.0
4000 
4001         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
4002 
4003         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4004         >>> ds.fillna(value=values)
4005         <xarray.Dataset>
4006         Dimensions:  (x: 4)
4007         Coordinates:
4008         * x        (x) int64 0 1 2 3
4009         Data variables:
4010             A        (x) float64 0.0 2.0 0.0 0.0
4011             B        (x) float64 3.0 4.0 1.0 1.0
4012             C        (x) float64 2.0 2.0 2.0 5.0
4013             D        (x) float64 3.0 3.0 3.0 4.0
4014         """
4015         if utils.is_dict_like(value):
4016             value_keys = getattr(value, "data_vars", value).keys()
4017             if not set(value_keys) <= set(self.data_vars.keys()):
4018                 raise ValueError(
4019                     "all variables in the argument to `fillna` "
4020                     "must be contained in the original dataset"
4021                 )
4022         out = ops.fillna(self, value)
4023         return out
4024 
4025     def interpolate_na(
4026         self,
4027         dim: Hashable = None,
4028         method: str = "linear",
4029         limit: int = None,
4030         use_coordinate: Union[bool, Hashable] = True,
4031         max_gap: Union[
4032             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4033         ] = None,
4034         **kwargs: Any,
4035     ) -> "Dataset":
4036         """Fill in NaNs by interpolating according to different methods.
4037 
4038         Parameters
4039         ----------
4040         dim : str
4041             Specifies the dimension along which to interpolate.
4042 
4043         method : str, optional
4044             String indicating which method to use for interpolation:
4045 
4046             - 'linear': linear interpolation (Default). Additional keyword
4047               arguments are passed to :py:func:`numpy.interp`
4048             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4049               are passed to :py:func:`scipy.interpolate.interp1d`. If
4050               ``method='polynomial'``, the ``order`` keyword argument must also be
4051               provided.
4052             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4053               respective :py:class:`scipy.interpolate` classes.
4054 
4055         use_coordinate : bool, str, default True
4056             Specifies which index to use as the x values in the interpolation
4057             formulated as `y = f(x)`. If False, values are treated as if
4058             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4059             used. If ``use_coordinate`` is a string, it specifies the name of a
4060             coordinate variariable to use as the index.
4061         limit : int, default None
4062             Maximum number of consecutive NaNs to fill. Must be greater than 0
4063             or None for no limit. This filling is done regardless of the size of
4064             the gap in the data. To only interpolate over gaps less than a given length,
4065             see ``max_gap``.
4066         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
4067             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4068             Use None for no limit. When interpolating along a datetime64 dimension
4069             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4070 
4071             - a string that is valid input for pandas.to_timedelta
4072             - a :py:class:`numpy.timedelta64` object
4073             - a :py:class:`pandas.Timedelta` object
4074             - a :py:class:`datetime.timedelta` object
4075 
4076             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4077             dimensions has not been implemented yet. Gap length is defined as the difference
4078             between coordinate values at the first data point after a gap and the last value
4079             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4080             between coordinate values at the first (last) valid data point and the first (last) NaN.
4081             For example, consider::
4082 
4083                 <xarray.DataArray (x: 9)>
4084                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4085                 Coordinates:
4086                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4087 
4088             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4089         kwargs : dict, optional
4090             parameters passed verbatim to the underlying interpolation function
4091 
4092         Returns
4093         -------
4094         interpolated: Dataset
4095             Filled in Dataset.
4096 
4097         See also
4098         --------
4099         numpy.interp
4100         scipy.interpolate
4101         """
4102         from .missing import interp_na, _apply_over_vars_with_dim
4103 
4104         new = _apply_over_vars_with_dim(
4105             interp_na,
4106             self,
4107             dim=dim,
4108             method=method,
4109             limit=limit,
4110             use_coordinate=use_coordinate,
4111             max_gap=max_gap,
4112             **kwargs,
4113         )
4114         return new
4115 
4116     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4117         """Fill NaN values by propogating values forward
4118 
4119         *Requires bottleneck.*
4120 
4121         Parameters
4122         ----------
4123         dim : Hashable
4124             Specifies the dimension along which to propagate values when
4125             filling.
4126         limit : int, default None
4127             The maximum number of consecutive NaN values to forward fill. In
4128             other words, if there is a gap with more than this number of
4129             consecutive NaNs, it will only be partially filled. Must be greater
4130             than 0 or None for no limit.
4131 
4132         Returns
4133         -------
4134         Dataset
4135         """
4136         from .missing import ffill, _apply_over_vars_with_dim
4137 
4138         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4139         return new
4140 
4141     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4142         """Fill NaN values by propogating values backward
4143 
4144         *Requires bottleneck.*
4145 
4146         Parameters
4147         ----------
4148         dim : str
4149             Specifies the dimension along which to propagate values when
4150             filling.
4151         limit : int, default None
4152             The maximum number of consecutive NaN values to backward fill. In
4153             other words, if there is a gap with more than this number of
4154             consecutive NaNs, it will only be partially filled. Must be greater
4155             than 0 or None for no limit.
4156 
4157         Returns
4158         -------
4159         Dataset
4160         """
4161         from .missing import bfill, _apply_over_vars_with_dim
4162 
4163         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4164         return new
4165 
4166     def combine_first(self, other: "Dataset") -> "Dataset":
4167         """Combine two Datasets, default to data_vars of self.
4168 
4169         The new coordinates follow the normal broadcasting and alignment rules
4170         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4171         filled with np.nan.
4172 
4173         Parameters
4174         ----------
4175         other : Dataset
4176             Used to fill all matching missing values in this array.
4177 
4178         Returns
4179         -------
4180         Dataset
4181         """
4182         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4183         return out
4184 
4185     def reduce(
4186         self,
4187         func: Callable,
4188         dim: Union[Hashable, Iterable[Hashable]] = None,
4189         keep_attrs: bool = None,
4190         keepdims: bool = False,
4191         numeric_only: bool = False,
4192         allow_lazy: bool = None,
4193         **kwargs: Any,
4194     ) -> "Dataset":
4195         """Reduce this dataset by applying `func` along some dimension(s).
4196 
4197         Parameters
4198         ----------
4199         func : callable
4200             Function which can be called in the form
4201             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4202             np.ndarray over an integer valued axis.
4203         dim : str or sequence of str, optional
4204             Dimension(s) over which to apply `func`.  By default `func` is
4205             applied over all dimensions.
4206         keep_attrs : bool, optional
4207             If True, the dataset's attributes (`attrs`) will be copied from
4208             the original object to the new one.  If False (default), the new
4209             object will be returned without attributes.
4210         keepdims : bool, default False
4211             If True, the dimensions which are reduced are left in the result
4212             as dimensions of size one. Coordinates that use these dimensions
4213             are removed.
4214         numeric_only : bool, optional
4215             If True, only apply ``func`` to variables with a numeric dtype.
4216         **kwargs : Any
4217             Additional keyword arguments passed on to ``func``.
4218 
4219         Returns
4220         -------
4221         reduced : Dataset
4222             Dataset with this object's DataArrays replaced with new DataArrays
4223             of summarized data and the indicated dimension(s) removed.
4224         """
4225         if dim is None or dim is ...:
4226             dims = set(self.dims)
4227         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4228             dims = {dim}
4229         else:
4230             dims = set(dim)
4231 
4232         missing_dimensions = [d for d in dims if d not in self.dims]
4233         if missing_dimensions:
4234             raise ValueError(
4235                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4236             )
4237 
4238         if keep_attrs is None:
4239             keep_attrs = _get_keep_attrs(default=False)
4240 
4241         variables: Dict[Hashable, Variable] = {}
4242         for name, var in self._variables.items():
4243             reduce_dims = [d for d in var.dims if d in dims]
4244             if name in self.coords:
4245                 if not reduce_dims:
4246                     variables[name] = var
4247             else:
4248                 if (
4249                     not numeric_only
4250                     or np.issubdtype(var.dtype, np.number)
4251                     or (var.dtype == np.bool_)
4252                 ):
4253                     if len(reduce_dims) == 1:
4254                         # unpack dimensions for the benefit of functions
4255                         # like np.argmin which can't handle tuple arguments
4256                         (reduce_dims,) = reduce_dims
4257                     elif len(reduce_dims) == var.ndim:
4258                         # prefer to aggregate over axis=None rather than
4259                         # axis=(0, 1) if they will be equivalent, because
4260                         # the former is often more efficient
4261                         reduce_dims = None  # type: ignore
4262                     variables[name] = var.reduce(
4263                         func,
4264                         dim=reduce_dims,
4265                         keep_attrs=keep_attrs,
4266                         keepdims=keepdims,
4267                         allow_lazy=allow_lazy,
4268                         **kwargs,
4269                     )
4270 
4271         coord_names = {k for k in self.coords if k in variables}
4272         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4273         attrs = self.attrs if keep_attrs else None
4274         return self._replace_with_new_dims(
4275             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4276         )
4277 
4278     def map(
4279         self,
4280         func: Callable,
4281         keep_attrs: bool = None,
4282         args: Iterable[Any] = (),
4283         **kwargs: Any,
4284     ) -> "Dataset":
4285         """Apply a function to each variable in this dataset
4286 
4287         Parameters
4288         ----------
4289         func : callable
4290             Function which can be called in the form `func(x, *args, **kwargs)`
4291             to transform each DataArray `x` in this dataset into another
4292             DataArray.
4293         keep_attrs : bool, optional
4294             If True, the dataset's attributes (`attrs`) will be copied from
4295             the original object to the new one. If False, the new object will
4296             be returned without attributes.
4297         args : tuple, optional
4298             Positional arguments passed on to `func`.
4299         **kwargs : Any
4300             Keyword arguments passed on to `func`.
4301 
4302         Returns
4303         -------
4304         applied : Dataset
4305             Resulting dataset from applying ``func`` to each data variable.
4306 
4307         Examples
4308         --------
4309         >>> da = xr.DataArray(np.random.randn(2, 3))
4310         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4311         >>> ds
4312         <xarray.Dataset>
4313         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4314         Dimensions without coordinates: dim_0, dim_1, x
4315         Data variables:
4316             foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948
4317             bar      (x) int64 -1 2
4318         >>> ds.map(np.fabs)
4319         <xarray.Dataset>
4320         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4321         Dimensions without coordinates: dim_0, dim_1, x
4322         Data variables:
4323             foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948
4324             bar      (x) float64 1.0 2.0
4325         """
4326         variables = {
4327             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4328             for k, v in self.data_vars.items()
4329         }
4330         if keep_attrs is None:
4331             keep_attrs = _get_keep_attrs(default=False)
4332         attrs = self.attrs if keep_attrs else None
4333         return type(self)(variables, attrs=attrs)
4334 
4335     def apply(
4336         self,
4337         func: Callable,
4338         keep_attrs: bool = None,
4339         args: Iterable[Any] = (),
4340         **kwargs: Any,
4341     ) -> "Dataset":
4342         """
4343         Backward compatible implementation of ``map``
4344 
4345         See Also
4346         --------
4347         Dataset.map
4348         """
4349         warnings.warn(
4350             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4351             PendingDeprecationWarning,
4352             stacklevel=2,
4353         )
4354         return self.map(func, keep_attrs, args, **kwargs)
4355 
4356     def assign(
4357         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4358     ) -> "Dataset":
4359         """Assign new data variables to a Dataset, returning a new object
4360         with all the original variables in addition to the new ones.
4361 
4362         Parameters
4363         ----------
4364         variables : mapping, value pairs
4365             Mapping from variables names to the new values. If the new values
4366             are callable, they are computed on the Dataset and assigned to new
4367             data variables. If the values are not callable, (e.g. a DataArray,
4368             scalar, or array), they are simply assigned.
4369         **variables_kwargs:
4370             The keyword arguments form of ``variables``.
4371             One of variables or variables_kwargs must be provided.
4372 
4373         Returns
4374         -------
4375         ds : Dataset
4376             A new Dataset with the new variables in addition to all the
4377             existing variables.
4378 
4379         Notes
4380         -----
4381         Since ``kwargs`` is a dictionary, the order of your arguments may not
4382         be preserved, and so the order of the new variables is not well
4383         defined. Assigning multiple variables within the same ``assign`` is
4384         possible, but you cannot reference other variables created within the
4385         same ``assign`` call.
4386 
4387         See Also
4388         --------
4389         pandas.DataFrame.assign
4390 
4391         Examples
4392         --------
4393         >>> import numpy as np
4394         >>> import xarray as xr
4395         >>> x = xr.Dataset(
4396         ...     {
4397         ...         "temperature_c": (
4398         ...             ("lat", "lon"),
4399         ...             20 * np.random.rand(4).reshape(2, 2),
4400         ...         ),
4401         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4402         ...     },
4403         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4404         ... )
4405         >>> x
4406         <xarray.Dataset>
4407         Dimensions:        (lat: 2, lon: 2)
4408         Coordinates:
4409         * lat            (lat) int64 10 20
4410         * lon            (lon) int64 150 160
4411         Data variables:
4412             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4413             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4414 
4415         Where the value is a callable, evaluated on dataset:
4416 
4417         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4418         <xarray.Dataset>
4419         Dimensions:        (lat: 2, lon: 2)
4420         Coordinates:
4421         * lat            (lat) int64 10 20
4422         * lon            (lon) int64 150 160
4423         Data variables:
4424             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4425             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4426             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4427 
4428         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4429 
4430         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4431         <xarray.Dataset>
4432         Dimensions:        (lat: 2, lon: 2)
4433         Coordinates:
4434         * lat            (lat) int64 10 20
4435         * lon            (lon) int64 150 160
4436         Data variables:
4437             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4438             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4439             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4440 
4441         """
4442         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4443         data = self.copy()
4444         # do all calculations first...
4445         results = data._calc_assign_results(variables)
4446         # ... and then assign
4447         data.update(results)
4448         return data
4449 
4450     def to_array(self, dim="variable", name=None):
4451         """Convert this dataset into an xarray.DataArray
4452 
4453         The data variables of this dataset will be broadcast against each other
4454         and stacked along the first axis of the new array. All coordinates of
4455         this dataset will remain coordinates.
4456 
4457         Parameters
4458         ----------
4459         dim : str, optional
4460             Name of the new dimension.
4461         name : str, optional
4462             Name of the new data array.
4463 
4464         Returns
4465         -------
4466         array : xarray.DataArray
4467         """
4468         from .dataarray import DataArray
4469 
4470         data_vars = [self.variables[k] for k in self.data_vars]
4471         broadcast_vars = broadcast_variables(*data_vars)
4472         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4473 
4474         coords = dict(self.coords)
4475         coords[dim] = list(self.data_vars)
4476         indexes = propagate_indexes(self._indexes)
4477 
4478         dims = (dim,) + broadcast_vars[0].dims
4479 
4480         return DataArray(
4481             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4482         )
4483 
4484     def _to_dataframe(self, ordered_dims):
4485         columns = [k for k in self.variables if k not in self.dims]
4486         data = [
4487             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4488             for k in columns
4489         ]
4490         index = self.coords.to_index(ordered_dims)
4491         return pd.DataFrame(dict(zip(columns, data)), index=index)
4492 
4493     def to_dataframe(self):
4494         """Convert this dataset into a pandas.DataFrame.
4495 
4496         Non-index variables in this dataset form the columns of the
4497         DataFrame. The DataFrame is be indexed by the Cartesian product of
4498         this dataset's indices.
4499         """
4500         return self._to_dataframe(self.dims)
4501 
4502     def _set_sparse_data_from_dataframe(
4503         self, dataframe: pd.DataFrame, dims: tuple
4504     ) -> None:
4505         from sparse import COO
4506 
4507         idx = dataframe.index
4508         if isinstance(idx, pd.MultiIndex):
4509             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
4510             is_sorted = idx.is_lexsorted
4511             shape = tuple(lev.size for lev in idx.levels)
4512         else:
4513             coords = np.arange(idx.size).reshape(1, -1)
4514             is_sorted = True
4515             shape = (idx.size,)
4516 
4517         for name, series in dataframe.items():
4518             # Cast to a NumPy array first, in case the Series is a pandas
4519             # Extension array (which doesn't have a valid NumPy dtype)
4520             values = np.asarray(series)
4521 
4522             # In virtually all real use cases, the sparse array will now have
4523             # missing values and needs a fill_value. For consistency, don't
4524             # special case the rare exceptions (e.g., dtype=int without a
4525             # MultiIndex).
4526             dtype, fill_value = dtypes.maybe_promote(values.dtype)
4527             values = np.asarray(values, dtype=dtype)
4528 
4529             data = COO(
4530                 coords,
4531                 values,
4532                 shape,
4533                 has_duplicates=False,
4534                 sorted=is_sorted,
4535                 fill_value=fill_value,
4536             )
4537             self[name] = (dims, data)
4538 
4539     def _set_numpy_data_from_dataframe(
4540         self, dataframe: pd.DataFrame, dims: tuple
4541     ) -> None:
4542         idx = dataframe.index
4543         if isinstance(idx, pd.MultiIndex):
4544             # expand the DataFrame to include the product of all levels
4545             full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)
4546             dataframe = dataframe.reindex(full_idx)
4547             shape = tuple(lev.size for lev in idx.levels)
4548         else:
4549             shape = (idx.size,)
4550         for name, series in dataframe.items():
4551             data = np.asarray(series).reshape(shape)
4552             self[name] = (dims, data)
4553 
4554     @classmethod
4555     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
4556         """Convert a pandas.DataFrame into an xarray.Dataset
4557 
4558         Each column will be converted into an independent variable in the
4559         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
4560         into a tensor product of one-dimensional indices (filling in missing
4561         values with NaN). This method will produce a Dataset very similar to
4562         that on which the 'to_dataframe' method was called, except with
4563         possibly redundant dimensions (since all dataset variables will have
4564         the same dimensionality)
4565 
4566         Parameters
4567         ----------
4568         dataframe : pandas.DataFrame
4569             DataFrame from which to copy data and indices.
4570         sparse : bool
4571             If true, create a sparse arrays instead of dense numpy arrays. This
4572             can potentially save a large amount of memory if the DataFrame has
4573             a MultiIndex. Requires the sparse package (sparse.pydata.org).
4574 
4575         Returns
4576         -------
4577         New Dataset.
4578 
4579         See also
4580         --------
4581         xarray.DataArray.from_series
4582         """
4583         # TODO: Add an option to remove dimensions along which the variables
4584         # are constant, to enable consistent serialization to/from a dataframe,
4585         # even if some variables have different dimensionality.
4586 
4587         if not dataframe.columns.is_unique:
4588             raise ValueError("cannot convert DataFrame with non-unique columns")
4589 
4590         idx = remove_unused_levels_categories(dataframe.index)
4591         dataframe = dataframe.set_index(idx)
4592         obj = cls()
4593 
4594         if isinstance(idx, pd.MultiIndex):
4595             dims = tuple(
4596                 name if name is not None else "level_%i" % n
4597                 for n, name in enumerate(idx.names)
4598             )
4599             for dim, lev in zip(dims, idx.levels):
4600                 obj[dim] = (dim, lev)
4601         else:
4602             index_name = idx.name if idx.name is not None else "index"
4603             dims = (index_name,)
4604             obj[index_name] = (dims, idx)
4605 
4606         if sparse:
4607             obj._set_sparse_data_from_dataframe(dataframe, dims)
4608         else:
4609             obj._set_numpy_data_from_dataframe(dataframe, dims)
4610         return obj
4611 
4612     def to_dask_dataframe(self, dim_order=None, set_index=False):
4613         """
4614         Convert this dataset into a dask.dataframe.DataFrame.
4615 
4616         The dimensions, coordinates and data variables in this dataset form
4617         the columns of the DataFrame.
4618 
4619         Parameters
4620         ----------
4621         dim_order : list, optional
4622             Hierarchical dimension order for the resulting dataframe. All
4623             arrays are transposed to this order and then written out as flat
4624             vectors in contiguous order, so the last dimension in this list
4625             will be contiguous in the resulting DataFrame. This has a major
4626             influence on which operations are efficient on the resulting dask
4627             dataframe.
4628 
4629             If provided, must include all dimensions on this dataset. By
4630             default, dimensions are sorted alphabetically.
4631         set_index : bool, optional
4632             If set_index=True, the dask DataFrame is indexed by this dataset's
4633             coordinate. Since dask DataFrames to not support multi-indexes,
4634             set_index only works if the dataset only contains one dimension.
4635 
4636         Returns
4637         -------
4638         dask.dataframe.DataFrame
4639         """
4640 
4641         import dask.array as da
4642         import dask.dataframe as dd
4643 
4644         if dim_order is None:
4645             dim_order = list(self.dims)
4646         elif set(dim_order) != set(self.dims):
4647             raise ValueError(
4648                 "dim_order {} does not match the set of dimensions on this "
4649                 "Dataset: {}".format(dim_order, list(self.dims))
4650             )
4651 
4652         ordered_dims = {k: self.dims[k] for k in dim_order}
4653 
4654         columns = list(ordered_dims)
4655         columns.extend(k for k in self.coords if k not in self.dims)
4656         columns.extend(self.data_vars)
4657 
4658         series_list = []
4659         for name in columns:
4660             try:
4661                 var = self.variables[name]
4662             except KeyError:
4663                 # dimension without a matching coordinate
4664                 size = self.dims[name]
4665                 data = da.arange(size, chunks=size, dtype=np.int64)
4666                 var = Variable((name,), data)
4667 
4668             # IndexVariable objects have a dummy .chunk() method
4669             if isinstance(var, IndexVariable):
4670                 var = var.to_base_variable()
4671 
4672             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
4673             series = dd.from_array(dask_array.reshape(-1), columns=[name])
4674             series_list.append(series)
4675 
4676         df = dd.concat(series_list, axis=1)
4677 
4678         if set_index:
4679             if len(dim_order) == 1:
4680                 (dim,) = dim_order
4681                 df = df.set_index(dim)
4682             else:
4683                 # triggers an error about multi-indexes, even if only one
4684                 # dimension is passed
4685                 df = df.set_index(dim_order)
4686 
4687         return df
4688 
4689     def to_dict(self, data=True):
4690         """
4691         Convert this dataset to a dictionary following xarray naming
4692         conventions.
4693 
4694         Converts all variables and attributes to native Python objects
4695         Useful for converting to json. To avoid datetime incompatibility
4696         use decode_times=False kwarg in xarrray.open_dataset.
4697 
4698         Parameters
4699         ----------
4700         data : bool, optional
4701             Whether to include the actual data in the dictionary. When set to
4702             False, returns just the schema.
4703 
4704         See also
4705         --------
4706         Dataset.from_dict
4707         """
4708         d = {
4709             "coords": {},
4710             "attrs": decode_numpy_dict_values(self.attrs),
4711             "dims": dict(self.dims),
4712             "data_vars": {},
4713         }
4714         for k in self.coords:
4715             d["coords"].update({k: self[k].variable.to_dict(data=data)})
4716         for k in self.data_vars:
4717             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
4718         return d
4719 
4720     @classmethod
4721     def from_dict(cls, d):
4722         """
4723         Convert a dictionary into an xarray.Dataset.
4724 
4725         Input dict can take several forms::
4726 
4727             d = {'t': {'dims': ('t'), 'data': t},
4728                  'a': {'dims': ('t'), 'data': x},
4729                  'b': {'dims': ('t'), 'data': y}}
4730 
4731             d = {'coords': {'t': {'dims': 't', 'data': t,
4732                                   'attrs': {'units':'s'}}},
4733                  'attrs': {'title': 'air temperature'},
4734                  'dims': 't',
4735                  'data_vars': {'a': {'dims': 't', 'data': x, },
4736                                'b': {'dims': 't', 'data': y}}}
4737 
4738         where 't' is the name of the dimesion, 'a' and 'b' are names of data
4739         variables and t, x, and y are lists, numpy.arrays or pandas objects.
4740 
4741         Parameters
4742         ----------
4743         d : dict, with a minimum structure of {'var_0': {'dims': [..], \
4744                                                          'data': [..]}, \
4745                                                ...}
4746 
4747         Returns
4748         -------
4749         obj : xarray.Dataset
4750 
4751         See also
4752         --------
4753         Dataset.to_dict
4754         DataArray.from_dict
4755         """
4756 
4757         if not {"coords", "data_vars"}.issubset(set(d)):
4758             variables = d.items()
4759         else:
4760             import itertools
4761 
4762             variables = itertools.chain(
4763                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
4764             )
4765         try:
4766             variable_dict = {
4767                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
4768             }
4769         except KeyError as e:
4770             raise ValueError(
4771                 "cannot convert dict without the key "
4772                 "'{dims_data}'".format(dims_data=str(e.args[0]))
4773             )
4774         obj = cls(variable_dict)
4775 
4776         # what if coords aren't dims?
4777         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
4778         obj = obj.set_coords(coords)
4779 
4780         obj.attrs.update(d.get("attrs", {}))
4781 
4782         return obj
4783 
4784     @staticmethod
4785     def _unary_op(f, keep_attrs=False):
4786         @functools.wraps(f)
4787         def func(self, *args, **kwargs):
4788             variables = {}
4789             for k, v in self._variables.items():
4790                 if k in self._coord_names:
4791                     variables[k] = v
4792                 else:
4793                     variables[k] = f(v, *args, **kwargs)
4794             attrs = self._attrs if keep_attrs else None
4795             return self._replace_with_new_dims(variables, attrs=attrs)
4796 
4797         return func
4798 
4799     @staticmethod
4800     def _binary_op(f, reflexive=False, join=None):
4801         @functools.wraps(f)
4802         def func(self, other):
4803             from .dataarray import DataArray
4804 
4805             if isinstance(other, groupby.GroupBy):
4806                 return NotImplemented
4807             align_type = OPTIONS["arithmetic_join"] if join is None else join
4808             if isinstance(other, (DataArray, Dataset)):
4809                 self, other = align(self, other, join=align_type, copy=False)
4810             g = f if not reflexive else lambda x, y: f(y, x)
4811             ds = self._calculate_binary_op(g, other, join=align_type)
4812             return ds
4813 
4814         return func
4815 
4816     @staticmethod
4817     def _inplace_binary_op(f):
4818         @functools.wraps(f)
4819         def func(self, other):
4820             from .dataarray import DataArray
4821 
4822             if isinstance(other, groupby.GroupBy):
4823                 raise TypeError(
4824                     "in-place operations between a Dataset and "
4825                     "a grouped object are not permitted"
4826                 )
4827             # we don't actually modify arrays in-place with in-place Dataset
4828             # arithmetic -- this lets us automatically align things
4829             if isinstance(other, (DataArray, Dataset)):
4830                 other = other.reindex_like(self, copy=False)
4831             g = ops.inplace_to_noninplace_op(f)
4832             ds = self._calculate_binary_op(g, other, inplace=True)
4833             self._replace_with_new_dims(
4834                 ds._variables,
4835                 ds._coord_names,
4836                 attrs=ds._attrs,
4837                 indexes=ds._indexes,
4838                 inplace=True,
4839             )
4840             return self
4841 
4842         return func
4843 
4844     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
4845         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
4846             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
4847                 raise ValueError(
4848                     "datasets must have the same data variables "
4849                     "for in-place arithmetic operations: %s, %s"
4850                     % (list(lhs_data_vars), list(rhs_data_vars))
4851                 )
4852 
4853             dest_vars = {}
4854 
4855             for k in lhs_data_vars:
4856                 if k in rhs_data_vars:
4857                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
4858                 elif join in ["left", "outer"]:
4859                     dest_vars[k] = f(lhs_vars[k], np.nan)
4860             for k in rhs_data_vars:
4861                 if k not in dest_vars and join in ["right", "outer"]:
4862                     dest_vars[k] = f(rhs_vars[k], np.nan)
4863             return dest_vars
4864 
4865         if utils.is_dict_like(other) and not isinstance(other, Dataset):
4866             # can't use our shortcut of doing the binary operation with
4867             # Variable objects, so apply over our data vars instead.
4868             new_data_vars = apply_over_both(
4869                 self.data_vars, other, self.data_vars, other
4870             )
4871             return Dataset(new_data_vars)
4872 
4873         other_coords = getattr(other, "coords", None)
4874         ds = self.coords.merge(other_coords)
4875 
4876         if isinstance(other, Dataset):
4877             new_vars = apply_over_both(
4878                 self.data_vars, other.data_vars, self.variables, other.variables
4879             )
4880         else:
4881             other_variable = getattr(other, "variable", other)
4882             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
4883         ds._variables.update(new_vars)
4884         ds._dims = calculate_dimensions(ds._variables)
4885         return ds
4886 
4887     def _copy_attrs_from(self, other):
4888         self.attrs = other.attrs
4889         for v in other.variables:
4890             if v in self.variables:
4891                 self.variables[v].attrs = other.variables[v].attrs
4892 
4893     def diff(self, dim, n=1, label="upper"):
4894         """Calculate the n-th order discrete difference along given axis.
4895 
4896         Parameters
4897         ----------
4898         dim : str, optional
4899             Dimension over which to calculate the finite difference.
4900         n : int, optional
4901             The number of times values are differenced.
4902         label : str, optional
4903             The new coordinate in dimension ``dim`` will have the
4904             values of either the minuend's or subtrahend's coordinate
4905             for values 'upper' and 'lower', respectively.  Other
4906             values are not supported.
4907 
4908         Returns
4909         -------
4910         difference : same type as caller
4911             The n-th order finite difference of this object.
4912 
4913         .. note::
4914 
4915             `n` matches numpy's behavior and is different from pandas' first
4916             argument named `periods`.
4917 
4918         Examples
4919         --------
4920         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
4921         >>> ds.diff("x")
4922         <xarray.Dataset>
4923         Dimensions:  (x: 3)
4924         Coordinates:
4925           * x        (x) int64 1 2 3
4926         Data variables:
4927             foo      (x) int64 0 1 0
4928         >>> ds.diff("x", 2)
4929         <xarray.Dataset>
4930         Dimensions:  (x: 2)
4931         Coordinates:
4932         * x        (x) int64 2 3
4933         Data variables:
4934         foo      (x) int64 1 -1
4935 
4936         See Also
4937         --------
4938         Dataset.differentiate
4939         """
4940         if n == 0:
4941             return self
4942         if n < 0:
4943             raise ValueError(f"order `n` must be non-negative but got {n}")
4944 
4945         # prepare slices
4946         kwargs_start = {dim: slice(None, -1)}
4947         kwargs_end = {dim: slice(1, None)}
4948 
4949         # prepare new coordinate
4950         if label == "upper":
4951             kwargs_new = kwargs_end
4952         elif label == "lower":
4953             kwargs_new = kwargs_start
4954         else:
4955             raise ValueError(
4956                 "The 'label' argument has to be either " "'upper' or 'lower'"
4957             )
4958 
4959         variables = {}
4960 
4961         for name, var in self.variables.items():
4962             if dim in var.dims:
4963                 if name in self.data_vars:
4964                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
4965                 else:
4966                     variables[name] = var.isel(**kwargs_new)
4967             else:
4968                 variables[name] = var
4969 
4970         indexes = dict(self.indexes)
4971         if dim in indexes:
4972             indexes[dim] = indexes[dim][kwargs_new[dim]]
4973 
4974         difference = self._replace_with_new_dims(variables, indexes=indexes)
4975 
4976         if n > 1:
4977             return difference.diff(dim, n - 1)
4978         else:
4979             return difference
4980 
4981     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
4982         """Shift this dataset by an offset along one or more dimensions.
4983 
4984         Only data variables are moved; coordinates stay in place. This is
4985         consistent with the behavior of ``shift`` in pandas.
4986 
4987         Parameters
4988         ----------
4989         shifts : Mapping with the form of {dim: offset}
4990             Integer offset to shift along each of the given dimensions.
4991             Positive offsets shift to the right; negative offsets shift to the
4992             left.
4993         fill_value: scalar, optional
4994             Value to use for newly missing values
4995         **shifts_kwargs:
4996             The keyword arguments form of ``shifts``.
4997             One of shifts or shifts_kwargs must be provided.
4998 
4999         Returns
5000         -------
5001         shifted : Dataset
5002             Dataset with the same coordinates and attributes but shifted data
5003             variables.
5004 
5005         See also
5006         --------
5007         roll
5008 
5009         Examples
5010         --------
5011 
5012         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5013         >>> ds.shift(x=2)
5014         <xarray.Dataset>
5015         Dimensions:  (x: 5)
5016         Coordinates:
5017           * x        (x) int64 0 1 2 3 4
5018         Data variables:
5019             foo      (x) object nan nan 'a' 'b' 'c'
5020         """
5021         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5022         invalid = [k for k in shifts if k not in self.dims]
5023         if invalid:
5024             raise ValueError("dimensions %r do not exist" % invalid)
5025 
5026         variables = {}
5027         for name, var in self.variables.items():
5028             if name in self.data_vars:
5029                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5030                 variables[name] = var.shift(fill_value=fill_value, shifts=var_shifts)
5031             else:
5032                 variables[name] = var
5033 
5034         return self._replace(variables)
5035 
5036     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5037         """Roll this dataset by an offset along one or more dimensions.
5038 
5039         Unlike shift, roll may rotate all variables, including coordinates
5040         if specified. The direction of rotation is consistent with
5041         :py:func:`numpy.roll`.
5042 
5043         Parameters
5044         ----------
5045 
5046         shifts : dict, optional
5047             A dict with keys matching dimensions and values given
5048             by integers to rotate each of the given dimensions. Positive
5049             offsets roll to the right; negative offsets roll to the left.
5050         roll_coords : bool
5051             Indicates whether to  roll the coordinates by the offset
5052             The current default of roll_coords (None, equivalent to True) is
5053             deprecated and will change to False in a future version.
5054             Explicitly pass roll_coords to silence the warning.
5055         **shifts_kwargs : {dim: offset, ...}, optional
5056             The keyword arguments form of ``shifts``.
5057             One of shifts or shifts_kwargs must be provided.
5058         Returns
5059         -------
5060         rolled : Dataset
5061             Dataset with the same coordinates and attributes but rolled
5062             variables.
5063 
5064         See also
5065         --------
5066         shift
5067 
5068         Examples
5069         --------
5070 
5071         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5072         >>> ds.roll(x=2)
5073         <xarray.Dataset>
5074         Dimensions:  (x: 5)
5075         Coordinates:
5076           * x        (x) int64 3 4 0 1 2
5077         Data variables:
5078             foo      (x) object 'd' 'e' 'a' 'b' 'c'
5079         """
5080         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5081         invalid = [k for k in shifts if k not in self.dims]
5082         if invalid:
5083             raise ValueError("dimensions %r do not exist" % invalid)
5084 
5085         if roll_coords is None:
5086             warnings.warn(
5087                 "roll_coords will be set to False in the future."
5088                 " Explicitly set roll_coords to silence warning.",
5089                 FutureWarning,
5090                 stacklevel=2,
5091             )
5092             roll_coords = True
5093 
5094         unrolled_vars = () if roll_coords else self.coords
5095 
5096         variables = {}
5097         for k, v in self.variables.items():
5098             if k not in unrolled_vars:
5099                 variables[k] = v.roll(
5100                     **{k: s for k, s in shifts.items() if k in v.dims}
5101                 )
5102             else:
5103                 variables[k] = v
5104 
5105         if roll_coords:
5106             indexes = {}
5107             for k, v in self.indexes.items():
5108                 (dim,) = self.variables[k].dims
5109                 if dim in shifts:
5110                     indexes[k] = roll_index(v, shifts[dim])
5111                 else:
5112                     indexes[k] = v
5113         else:
5114             indexes = dict(self.indexes)
5115 
5116         return self._replace(variables, indexes=indexes)
5117 
5118     def sortby(self, variables, ascending=True):
5119         """
5120         Sort object by labels or values (along an axis).
5121 
5122         Sorts the dataset, either along specified dimensions,
5123         or according to values of 1-D dataarrays that share dimension
5124         with calling object.
5125 
5126         If the input variables are dataarrays, then the dataarrays are aligned
5127         (via left-join) to the calling object prior to sorting by cell values.
5128         NaNs are sorted to the end, following Numpy convention.
5129 
5130         If multiple sorts along the same dimension is
5131         given, numpy's lexsort is performed along that dimension:
5132         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5133         and the FIRST key in the sequence is used as the primary sort key,
5134         followed by the 2nd key, etc.
5135 
5136         Parameters
5137         ----------
5138         variables: str, DataArray, or list of either
5139             1D DataArray objects or name(s) of 1D variable(s) in
5140             coords/data_vars whose values are used to sort the dataset.
5141         ascending: boolean, optional
5142             Whether to sort by ascending or descending order.
5143 
5144         Returns
5145         -------
5146         sorted: Dataset
5147             A new dataset where all the specified dims are sorted by dim
5148             labels.
5149         """
5150         from .dataarray import DataArray
5151 
5152         if not isinstance(variables, list):
5153             variables = [variables]
5154         else:
5155             variables = variables
5156         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5157         aligned_vars = align(self, *variables, join="left")
5158         aligned_self = aligned_vars[0]
5159         aligned_other_vars = aligned_vars[1:]
5160         vars_by_dim = defaultdict(list)
5161         for data_array in aligned_other_vars:
5162             if data_array.ndim != 1:
5163                 raise ValueError("Input DataArray is not 1-D.")
5164             (key,) = data_array.dims
5165             vars_by_dim[key].append(data_array)
5166 
5167         indices = {}
5168         for key, arrays in vars_by_dim.items():
5169             order = np.lexsort(tuple(reversed(arrays)))
5170             indices[key] = order if ascending else order[::-1]
5171         return aligned_self.isel(**indices)
5172 
5173     def quantile(
5174         self,
5175         q,
5176         dim=None,
5177         interpolation="linear",
5178         numeric_only=False,
5179         keep_attrs=None,
5180         skipna=True,
5181     ):
5182         """Compute the qth quantile of the data along the specified dimension.
5183 
5184         Returns the qth quantiles(s) of the array elements for each variable
5185         in the Dataset.
5186 
5187         Parameters
5188         ----------
5189         q : float in range of [0,1] or array-like of floats
5190             Quantile to compute, which must be between 0 and 1 inclusive.
5191         dim : str or sequence of str, optional
5192             Dimension(s) over which to apply quantile.
5193         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
5194             This optional parameter specifies the interpolation method to
5195             use when the desired quantile lies between two data points
5196             ``i < j``:
5197 
5198                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5199                   the fractional part of the index surrounded by ``i`` and
5200                   ``j``.
5201                 * lower: ``i``.
5202                 * higher: ``j``.
5203                 * nearest: ``i`` or ``j``, whichever is nearest.
5204                 * midpoint: ``(i + j) / 2``.
5205         keep_attrs : bool, optional
5206             If True, the dataset's attributes (`attrs`) will be copied from
5207             the original object to the new one.  If False (default), the new
5208             object will be returned without attributes.
5209         numeric_only : bool, optional
5210             If True, only apply ``func`` to variables with a numeric dtype.
5211         skipna : bool, optional
5212             Whether to skip missing values when aggregating.
5213 
5214         Returns
5215         -------
5216         quantiles : Dataset
5217             If `q` is a single quantile, then the result is a scalar for each
5218             variable in data_vars. If multiple percentiles are given, first
5219             axis of the result corresponds to the quantile and a quantile
5220             dimension is added to the return Dataset. The other dimensions are
5221             the dimensions that remain after the reduction of the array.
5222 
5223         See Also
5224         --------
5225         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5226 
5227         Examples
5228         --------
5229 
5230         >>> ds = xr.Dataset(
5231         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5232         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5233         ... )
5234         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5235         <xarray.Dataset>
5236         Dimensions:   ()
5237         Coordinates:
5238             quantile  float64 0.0
5239         Data variables:
5240             a         float64 0.7
5241         >>> ds.quantile(0, dim="x")
5242         <xarray.Dataset>
5243         Dimensions:   (y: 4)
5244         Coordinates:
5245           * y         (y) float64 1.0 1.5 2.0 2.5
5246             quantile  float64 0.0
5247         Data variables:
5248             a         (y) float64 0.7 4.2 2.6 1.5
5249         >>> ds.quantile([0, 0.5, 1])
5250         <xarray.Dataset>
5251         Dimensions:   (quantile: 3)
5252         Coordinates:
5253           * quantile  (quantile) float64 0.0 0.5 1.0
5254         Data variables:
5255             a         (quantile) float64 0.7 3.4 9.4
5256         >>> ds.quantile([0, 0.5, 1], dim="x")
5257         <xarray.Dataset>
5258         Dimensions:   (quantile: 3, y: 4)
5259         Coordinates:
5260           * y         (y) float64 1.0 1.5 2.0 2.5
5261           * quantile  (quantile) float64 0.0 0.5 1.0
5262         Data variables:
5263             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5264         """
5265 
5266         if isinstance(dim, str):
5267             dims = {dim}
5268         elif dim in [None, ...]:
5269             dims = set(self.dims)
5270         else:
5271             dims = set(dim)
5272 
5273         _assert_empty(
5274             [d for d in dims if d not in self.dims],
5275             "Dataset does not contain the dimensions: %s",
5276         )
5277 
5278         q = np.asarray(q, dtype=np.float64)
5279 
5280         variables = {}
5281         for name, var in self.variables.items():
5282             reduce_dims = [d for d in var.dims if d in dims]
5283             if reduce_dims or not var.dims:
5284                 if name not in self.coords:
5285                     if (
5286                         not numeric_only
5287                         or np.issubdtype(var.dtype, np.number)
5288                         or var.dtype == np.bool_
5289                     ):
5290                         if len(reduce_dims) == var.ndim:
5291                             # prefer to aggregate over axis=None rather than
5292                             # axis=(0, 1) if they will be equivalent, because
5293                             # the former is often more efficient
5294                             reduce_dims = None
5295                         variables[name] = var.quantile(
5296                             q,
5297                             dim=reduce_dims,
5298                             interpolation=interpolation,
5299                             keep_attrs=keep_attrs,
5300                             skipna=skipna,
5301                         )
5302 
5303             else:
5304                 variables[name] = var
5305 
5306         # construct the new dataset
5307         coord_names = {k for k in self.coords if k in variables}
5308         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5309         if keep_attrs is None:
5310             keep_attrs = _get_keep_attrs(default=False)
5311         attrs = self.attrs if keep_attrs else None
5312         new = self._replace_with_new_dims(
5313             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5314         )
5315         return new.assign_coords(quantile=q)
5316 
5317     def rank(self, dim, pct=False, keep_attrs=None):
5318         """Ranks the data.
5319 
5320         Equal values are assigned a rank that is the average of the ranks that
5321         would have been otherwise assigned to all of the values within
5322         that set.
5323         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5324 
5325         NaNs in the input array are returned as NaNs.
5326 
5327         The `bottleneck` library is required.
5328 
5329         Parameters
5330         ----------
5331         dim : str
5332             Dimension over which to compute rank.
5333         pct : bool, optional
5334             If True, compute percentage ranks, otherwise compute integer ranks.
5335         keep_attrs : bool, optional
5336             If True, the dataset's attributes (`attrs`) will be copied from
5337             the original object to the new one.  If False (default), the new
5338             object will be returned without attributes.
5339 
5340         Returns
5341         -------
5342         ranked : Dataset
5343             Variables that do not depend on `dim` are dropped.
5344         """
5345         if dim not in self.dims:
5346             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5347 
5348         variables = {}
5349         for name, var in self.variables.items():
5350             if name in self.data_vars:
5351                 if dim in var.dims:
5352                     variables[name] = var.rank(dim, pct=pct)
5353             else:
5354                 variables[name] = var
5355 
5356         coord_names = set(self.coords)
5357         if keep_attrs is None:
5358             keep_attrs = _get_keep_attrs(default=False)
5359         attrs = self.attrs if keep_attrs else None
5360         return self._replace(variables, coord_names, attrs=attrs)
5361 
5362     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5363         """ Differentiate with the second order accurate central
5364         differences.
5365 
5366         .. note::
5367             This feature is limited to simple cartesian geometry, i.e. coord
5368             must be one dimensional.
5369 
5370         Parameters
5371         ----------
5372         coord: str
5373             The coordinate to be used to compute the gradient.
5374         edge_order: 1 or 2. Default 1
5375             N-th order accurate differences at the boundaries.
5376         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
5377             'us', 'ns', 'ps', 'fs', 'as'}
5378             Unit to compute gradient. Only valid for datetime coordinate.
5379 
5380         Returns
5381         -------
5382         differentiated: Dataset
5383 
5384         See also
5385         --------
5386         numpy.gradient: corresponding numpy function
5387         """
5388         from .variable import Variable
5389 
5390         if coord not in self.variables and coord not in self.dims:
5391             raise ValueError(f"Coordinate {coord} does not exist.")
5392 
5393         coord_var = self[coord].variable
5394         if coord_var.ndim != 1:
5395             raise ValueError(
5396                 "Coordinate {} must be 1 dimensional but is {}"
5397                 " dimensional".format(coord, coord_var.ndim)
5398             )
5399 
5400         dim = coord_var.dims[0]
5401         if _contains_datetime_like_objects(coord_var):
5402             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5404             elif datetime_unit is None:
5405                 datetime_unit = "s"  # Default to seconds for cftime objects
5406             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5407 
5408         variables = {}
5409         for k, v in self.variables.items():
5410             if k in self.data_vars and dim in v.dims and k not in self.coords:
5411                 if _contains_datetime_like_objects(v):
5412                     v = v._to_numeric(datetime_unit=datetime_unit)
5413                 grad = duck_array_ops.gradient(
5414                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5415                 )
5416                 variables[k] = Variable(v.dims, grad)
5417             else:
5418                 variables[k] = v
5419         return self._replace(variables)
5420 
5421     def integrate(self, coord, datetime_unit=None):
5422         """ integrate the array with the trapezoidal rule.
5423 
5424         .. note::
5425             This feature is limited to simple cartesian geometry, i.e. coord
5426             must be one dimensional.
5427 
5428         Parameters
5429         ----------
5430         coord: str, or a sequence of str
5431             Coordinate(s) used for the integration.
5432         datetime_unit
5433             Can be specify the unit if datetime coordinate is used. One of
5434             {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs',
5435             'as'}
5436 
5437         Returns
5438         -------
5439         integrated: Dataset
5440 
5441         See also
5442         --------
5443         DataArray.integrate
5444         numpy.trapz: corresponding numpy function
5445 
5446         Examples
5447         --------
5448         >>> ds = xr.Dataset(
5449         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5450         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5451         ... )
5452         >>> ds
5453         <xarray.Dataset>
5454         Dimensions:  (x: 4)
5455         Coordinates:
5456           * x        (x) int64 0 1 2 3
5457             y        (x) int64 1 7 3 5
5458         Data variables:
5459             a        (x) int64 5 5 6 6
5460             b        (x) int64 1 2 1 0
5461         >>> ds.integrate("x")
5462         <xarray.Dataset>
5463         Dimensions:  ()
5464         Data variables:
5465             a        float64 16.5
5466             b        float64 3.5
5467         >>> ds.integrate("y")
5468         <xarray.Dataset>
5469         Dimensions:  ()
5470         Data variables:
5471             a        float64 20.0
5472             b        float64 4.0
5473         """
5474         if not isinstance(coord, (list, tuple)):
5475             coord = (coord,)
5476         result = self
5477         for c in coord:
5478             result = result._integrate_one(c, datetime_unit=datetime_unit)
5479         return result
5480 
5481     def _integrate_one(self, coord, datetime_unit=None):
5482         from .variable import Variable
5483 
5484         if coord not in self.variables and coord not in self.dims:
5485             raise ValueError(f"Coordinate {coord} does not exist.")
5486 
5487         coord_var = self[coord].variable
5488         if coord_var.ndim != 1:
5489             raise ValueError(
5490                 "Coordinate {} must be 1 dimensional but is {}"
5491                 " dimensional".format(coord, coord_var.ndim)
5492             )
5493 
5494         dim = coord_var.dims[0]
5495         if _contains_datetime_like_objects(coord_var):
5496             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5497                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5498             elif datetime_unit is None:
5499                 datetime_unit = "s"  # Default to seconds for cftime objects
5500             coord_var = coord_var._replace(
5501                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
5502             )
5503 
5504         variables = {}
5505         coord_names = set()
5506         for k, v in self.variables.items():
5507             if k in self.coords:
5508                 if dim not in v.dims:
5509                     variables[k] = v
5510                     coord_names.add(k)
5511             else:
5512                 if k in self.data_vars and dim in v.dims:
5513                     if _contains_datetime_like_objects(v):
5514                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
5515                     integ = duck_array_ops.trapz(
5516                         v.data, coord_var.data, axis=v.get_axis_num(dim)
5517                     )
5518                     v_dims = list(v.dims)
5519                     v_dims.remove(dim)
5520                     variables[k] = Variable(v_dims, integ)
5521                 else:
5522                     variables[k] = v
5523         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5524         return self._replace_with_new_dims(
5525             variables, coord_names=coord_names, indexes=indexes
5526         )
5527 
5528     @property
5529     def real(self):
5530         return self._unary_op(lambda x: x.real, keep_attrs=True)(self)
5531 
5532     @property
5533     def imag(self):
5534         return self._unary_op(lambda x: x.imag, keep_attrs=True)(self)
5535 
5536     @property
5537     def plot(self):
5538         """
5539         Access plotting functions for Datasets.
5540         Use it as a namespace to use xarray.plot functions as Dataset methods
5541 
5542         >>> ds.plot.scatter(...)  # equivalent to xarray.plot.scatter(ds,...)
5543 
5544         """
5545         return _Dataset_PlotMethods(self)
5546 
5547     def filter_by_attrs(self, **kwargs):
5548         """Returns a ``Dataset`` with variables that match specific conditions.
5549 
5550         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
5551         containing only the variables for which all the filter tests pass.
5552         These tests are either ``key=value`` for which the attribute ``key``
5553         has the exact value ``value`` or the callable passed into
5554         ``key=callable`` returns True. The callable will be passed a single
5555         value, either the value of the attribute ``key`` or ``None`` if the
5556         DataArray does not have an attribute with the name ``key``.
5557 
5558         Parameters
5559         ----------
5560         **kwargs : key=value
5561             key : str
5562                 Attribute name.
5563             value : callable or obj
5564                 If value is a callable, it should return a boolean in the form
5565                 of bool = func(attr) where attr is da.attrs[key].
5566                 Otherwise, value will be compared to the each
5567                 DataArray's attrs[key].
5568 
5569         Returns
5570         -------
5571         new : Dataset
5572             New dataset with variables filtered by attribute.
5573 
5574         Examples
5575         --------
5576         >>> # Create an example dataset:
5577         >>> import numpy as np
5578         >>> import pandas as pd
5579         >>> import xarray as xr
5580         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
5581         >>> precip = 10 * np.random.rand(2, 2, 3)
5582         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
5583         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
5584         >>> dims = ["x", "y", "time"]
5585         >>> temp_attr = dict(standard_name="air_potential_temperature")
5586         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
5587         >>> ds = xr.Dataset(
5588         ...     {
5589         ...         "temperature": (dims, temp, temp_attr),
5590         ...         "precipitation": (dims, precip, precip_attr),
5591         ...     },
5592         ...     coords={
5593         ...         "lon": (["x", "y"], lon),
5594         ...         "lat": (["x", "y"], lat),
5595         ...         "time": pd.date_range("2014-09-06", periods=3),
5596         ...         "reference_time": pd.Timestamp("2014-09-05"),
5597         ...     },
5598         ... )
5599         >>> # Get variables matching a specific standard_name.
5600         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
5601         <xarray.Dataset>
5602         Dimensions:         (time: 3, x: 2, y: 2)
5603         Coordinates:
5604           * x               (x) int64 0 1
5605           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5606             lat             (x, y) float64 42.25 42.21 42.63 42.59
5607           * y               (y) int64 0 1
5608             reference_time  datetime64[ns] 2014-09-05
5609             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5610         Data variables:
5611             precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...
5612         >>> # Get all variables that have a standard_name attribute.
5613         >>> standard_name = lambda v: v is not None
5614         >>> ds.filter_by_attrs(standard_name=standard_name)
5615         <xarray.Dataset>
5616         Dimensions:         (time: 3, x: 2, y: 2)
5617         Coordinates:
5618             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5619             lat             (x, y) float64 42.25 42.21 42.63 42.59
5620           * x               (x) int64 0 1
5621           * y               (y) int64 0 1
5622           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5623             reference_time  datetime64[ns] 2014-09-05
5624         Data variables:
5625             temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...
5626             precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...
5627 
5628         """
5629         selection = []
5630         for var_name, variable in self.variables.items():
5631             has_value_flag = False
5632             for attr_name, pattern in kwargs.items():
5633                 attr_value = variable.attrs.get(attr_name)
5634                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
5635                     has_value_flag = True
5636                 else:
5637                     has_value_flag = False
5638                     break
5639             if has_value_flag is True:
5640                 selection.append(var_name)
5641         return self[selection]
5642 
5643     def unify_chunks(self) -> "Dataset":
5644         """ Unify chunk size along all chunked dimensions of this Dataset.
5645 
5646         Returns
5647         -------
5648 
5649         Dataset with consistent chunk sizes for all dask-array variables
5650 
5651         See Also
5652         --------
5653 
5654         dask.array.core.unify_chunks
5655         """
5656 
5657         try:
5658             self.chunks
5659         except ValueError:  # "inconsistent chunks"
5660             pass
5661         else:
5662             # No variables with dask backend, or all chunks are already aligned
5663             return self.copy()
5664 
5665         # import dask is placed after the quick exit test above to allow
5666         # running this method if dask isn't installed and there are no chunks
5667         import dask.array
5668 
5669         ds = self.copy()
5670 
5671         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
5672 
5673         dask_array_names = []
5674         dask_unify_args = []
5675         for name, variable in ds.variables.items():
5676             if isinstance(variable.data, dask.array.Array):
5677                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
5678                 dask_array_names.append(name)
5679                 dask_unify_args.append(variable.data)
5680                 dask_unify_args.append(dims_tuple)
5681 
5682         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
5683 
5684         for name, new_array in zip(dask_array_names, rechunked_arrays):
5685             ds.variables[name]._data = new_array
5686 
5687         return ds
5688 
5689     def map_blocks(
5690         self,
5691         func: "Callable[..., T_DSorDA]",
5692         args: Sequence[Any] = (),
5693         kwargs: Mapping[str, Any] = None,
5694     ) -> "T_DSorDA":
5695         """
5696         Apply a function to each chunk of this Dataset. This method is experimental and
5697         its signature may change.
5698 
5699         Parameters
5700         ----------
5701         func: callable
5702             User-provided function that accepts a Dataset as its first parameter. The
5703             function will receive a subset of this Dataset, corresponding to one chunk
5704             along each chunked dimension. ``func`` will be executed as
5705             ``func(obj_subset, *args, **kwargs)``.
5706 
5707             The function will be first run on mocked-up data, that looks like this
5708             Dataset but has sizes 0, to determine properties of the returned object such
5709             as dtype, variable names, new dimensions and new indexes (if any).
5710 
5711             This function must return either a single DataArray or a single Dataset.
5712 
5713             This function cannot change size of existing dimensions, or add new chunked
5714             dimensions.
5715         args: Sequence
5716             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
5717             objects, if any, will not be split by chunks. Passing dask collections is
5718             not allowed.
5719         kwargs: Mapping
5720             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5721             split by chunks. Passing dask collections is not allowed.
5722 
5723         Returns
5724         -------
5725         A single DataArray or Dataset with dask backend, reassembled from the outputs of
5726         the function.
5727 
5728         Notes
5729         -----
5730         This method is designed for when one needs to manipulate a whole xarray object
5731         within each chunk. In the more common case where one can work on numpy arrays,
5732         it is recommended to use apply_ufunc.
5733 
5734         If none of the variables in this Dataset is backed by dask, calling this method
5735         is equivalent to calling ``func(self, *args, **kwargs)``.
5736 
5737         See Also
5738         --------
5739         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
5740         xarray.DataArray.map_blocks
5741         """
5742         from .parallel import map_blocks
5743 
5744         return map_blocks(func, self, args, kwargs)

</code><intended_behavior>To resolve the issue, a new method named `average` should be added to the `Dataset` class. This new method would compute weighted averages across specified dimensions of the `Dataset`. The designed method will have to consider several factors to integrate seamlessly with the existing functionality of the class: - Ability to Specify Dimensions: The method should allow users to specify which dimensions to average over. This is consistent with other methods in the class, such as `sum` and `mean`, which work over specified dimensions. - Weight Application: The method must support the application of weights to the averaging process. This could involve accepting another Dataset or DataArray containing weights that correspond to the dimensions being averaged. Implementing weighted averages is crucial for many scientific and geospatial analyses where data points contribute unevenly to the overall average. - Preservation of Dataset Structure: The method should preserve the overall structure of the Dataset, meaning that it should return a new Dataset with the same coordinates and other dimensions intact, excluding those dimensions over which the average was computed. - Handling of Missing Values: The method should decide on a strategy for handling NaNs or missing values within the Dataset. It could follow similar conventions to other aggregation methods, offering users control over whether to skip or include these values in the computation. - Integration with Existing Methods: The method should be thoughtfully integrated into the existing set of Dataset methods, considering interactions with methods for selection, indexing, and other forms of data manipulation. This includes considering how it interacts with Dask-based lazy computations if `Dataset` objects can be backed by Dask arrays for out-of-memory computation.</intended_behavior>]
2025-10-15 21:27:33.390 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
422 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
423     """A multi-dimensional, in memory, array database.
424 
425     A dataset resembles an in-memory representation of a NetCDF file, and
426     consists of variables, coordinates and attributes which together form a
427     self describing dataset.
428 
429     Dataset implements the mapping interface with keys given by variable names
430     and values given by DataArray objects for each variable name.
431 
432     One dimensional variables with name equal to their dimension are index
433     coordinates used for label based indexing.
434     """
435 
436     _attrs: Optional[Dict[Hashable, Any]]
437     _cache: Dict[str, Any]
438     _coord_names: Set[Hashable]
439     _dims: Dict[Hashable, int]
440     _encoding: Optional[Dict[Hashable, Any]]
441     _indexes: Optional[Dict[Hashable, pd.Index]]
442     _variables: Dict[Hashable, Variable]
443 
444     __slots__ = (
445         "_attrs",
446         "_cache",
447         "_coord_names",
448         "_dims",
449         "_encoding",
450         "_file_obj",
451         "_indexes",
452         "_variables",
453         "__weakref__",
454     )
455 
456     _groupby_cls = groupby.DatasetGroupBy
457     _rolling_cls = rolling.DatasetRolling
458     _coarsen_cls = rolling.DatasetCoarsen
459     _resample_cls = resample.DatasetResample
460 
461     def __init__(
462         self,
463         # could make a VariableArgs to use more generally, and refine these
464         # categories
465         data_vars: Mapping[Hashable, Any] = None,
466         coords: Mapping[Hashable, Any] = None,
467         attrs: Mapping[Hashable, Any] = None,
468     ):
469         """To load data from a file or file-like object, use the `open_dataset`
470         function.
471 
472         Parameters
473         ----------
474         data_vars : dict-like, optional
475             A mapping from variable names to :py:class:`~xarray.DataArray`
476             objects, :py:class:`~xarray.Variable` objects or to tuples of the
477             form ``(dims, data[, attrs])`` which can be used as arguments to
478             create a new ``Variable``. Each dimension must have the same length
479             in all variables in which it appears.
480 
481             The following notations are accepted:
482 
483             - mapping {var name: DataArray}
484             - mapping {var name: Variable}
485             - mapping {var name: (dimension name, array-like)}
486             - mapping {var name: (tuple of dimension names, array-like)}
487             - mapping {dimension name: array-like}
488               (it will be automatically moved to coords, see below)
489 
490             Each dimension must have the same length in all variables in which
491             it appears.
492         coords : dict-like, optional
493             Another mapping in similar form as the `data_vars` argument,
494             except the each item is saved on the dataset as a "coordinate".
495             These variables have an associated meaning: they describe
496             constant/fixed/independent quantities, unlike the
497             varying/measured/dependent quantities that belong in `variables`.
498             Coordinates values may be given by 1-dimensional arrays or scalars,
499             in which case `dims` do not need to be supplied: 1D arrays will be
500             assumed to give index values along the dimension with the same
501             name.
502 
503             The following notations are accepted:
504 
505             - mapping {coord name: DataArray}
506             - mapping {coord name: Variable}
507             - mapping {coord name: (dimension name, array-like)}
508             - mapping {coord name: (tuple of dimension names, array-like)}
509             - mapping {dimension name: array-like}
510               (the dimension name is implicitly set to be the same as the coord name)
511 
512             The last notation implies that the coord name is the same as the
513             dimension name.
514 
515         attrs : dict-like, optional
516             Global attributes to save on this dataset.
517         """
518 
519         # TODO(shoyer): expose indexes as a public argument in __init__
520 
521         if data_vars is None:
522             data_vars = {}
523         if coords is None:
524             coords = {}
525 
526         both_data_and_coords = set(data_vars) & set(coords)
527         if both_data_and_coords:
528             raise ValueError(
529                 "variables %r are found in both data_vars and coords"
530                 % both_data_and_coords
531             )
532 
533         if isinstance(coords, Dataset):
534             coords = coords.variables
535 
536         variables, coord_names, dims, indexes = merge_data_and_coords(
537             data_vars, coords, compat="broadcast_equals"
538         )
539 
540         self._attrs = dict(attrs) if attrs is not None else None
541         self._file_obj = None
542         self._encoding = None
543         self._variables = variables
544         self._coord_names = coord_names
545         self._dims = dims
546         self._indexes = indexes
547 
548     @classmethod
549     def load_store(cls, store, decoder=None) -> "Dataset":
550         """Create a new dataset from the contents of a backends.*DataStore
551         object
552         """
553         variables, attributes = store.load()
554         if decoder:
555             variables, attributes = decoder(variables, attributes)
556         obj = cls(variables, attrs=attributes)
557         obj._file_obj = store
558         return obj
559 
560     @property
561     def variables(self) -> Mapping[Hashable, Variable]:
562         """Low level interface to Dataset contents as dict of Variable objects.
563 
564         This ordered dictionary is frozen to prevent mutation that could
565         violate Dataset invariants. It contains all variable objects
566         constituting the Dataset, including both data variables and
567         coordinates.
568         """
569         return Frozen(self._variables)
570 
571     @property
572     def attrs(self) -> Dict[Hashable, Any]:
573         """Dictionary of global attributes on this dataset
574         """
575         if self._attrs is None:
576             self._attrs = {}
577         return self._attrs
578 
579     @attrs.setter
580     def attrs(self, value: Mapping[Hashable, Any]) -> None:
581         self._attrs = dict(value)
582 
583     @property
584     def encoding(self) -> Dict:
585         """Dictionary of global encoding attributes on this dataset
586         """
587         if self._encoding is None:
588             self._encoding = {}
589         return self._encoding
590 
591     @encoding.setter
592     def encoding(self, value: Mapping) -> None:
593         self._encoding = dict(value)
594 
595     @property
596     def dims(self) -> Mapping[Hashable, int]:
597         """Mapping from dimension names to lengths.
598 
599         Cannot be modified directly, but is updated when adding new variables.
600 
601         Note that type of this object differs from `DataArray.dims`.
602         See `Dataset.sizes` and `DataArray.sizes` for consistently named
603         properties.
604         """
605         return Frozen(SortedKeysDict(self._dims))
606 
607     @property
608     def sizes(self) -> Mapping[Hashable, int]:
609         """Mapping from dimension names to lengths.
610 
611         Cannot be modified directly, but is updated when adding new variables.
612 
613         This is an alias for `Dataset.dims` provided for the benefit of
614         consistency with `DataArray.sizes`.
615 
616         See also
617         --------
618         DataArray.sizes
619         """
620         return self.dims
621 
622     def load(self, **kwargs) -> "Dataset":
623         """Manually trigger loading and/or computation of this dataset's data
624         from disk or a remote source into memory and return this dataset.
625         Unlike compute, the original dataset is modified and returned.
626 
627         Normally, it should not be necessary to call this method in user code,
628         because all xarray functions should either work on deferred data or
629         load data automatically. However, this method can be necessary when
630         working with many file objects on disk.
631 
632         Parameters
633         ----------
634         **kwargs : dict
635             Additional keyword arguments passed on to ``dask.array.compute``.
636 
637         See Also
638         --------
639         dask.array.compute
640         """
641         # access .data to coerce everything to numpy or dask arrays
642         lazy_data = {
643             k: v._data
644             for k, v in self.variables.items()
645             if isinstance(v._data, dask_array_type)
646         }
647         if lazy_data:
648             import dask.array as da
649 
650             # evaluate all the dask arrays simultaneously
651             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
652 
653             for k, data in zip(lazy_data, evaluated_data):
654                 self.variables[k].data = data
655 
656         # load everything else sequentially
657         for k, v in self.variables.items():
658             if k not in lazy_data:
659                 v.load()
660 
661         return self
662 
663     def __dask_tokenize__(self):
664         from dask.base import normalize_token
665 
666         return normalize_token(
667             (type(self), self._variables, self._coord_names, self._attrs)
668         )
669 
670     def __dask_graph__(self):
671         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
672         graphs = {k: v for k, v in graphs.items() if v is not None}
673         if not graphs:
674             return None
675         else:
676             try:
677                 from dask.highlevelgraph import HighLevelGraph
678 
679                 return HighLevelGraph.merge(*graphs.values())
680             except ImportError:
681                 from dask import sharedict
682 
683                 return sharedict.merge(*graphs.values())
684 
685     def __dask_keys__(self):
686         import dask
687 
688         return [
689             v.__dask_keys__()
690             for v in self.variables.values()
691             if dask.is_dask_collection(v)
692         ]
693 
694     def __dask_layers__(self):
695         import dask
696 
697         return sum(
698             [
699                 v.__dask_layers__()
700                 for v in self.variables.values()
701                 if dask.is_dask_collection(v)
702             ],
703             (),
704         )
705 
706     @property
707     def __dask_optimize__(self):
708         import dask.array as da
709 
710         return da.Array.__dask_optimize__
711 
712     @property
713     def __dask_scheduler__(self):
714         import dask.array as da
715 
716         return da.Array.__dask_scheduler__
717 
718     def __dask_postcompute__(self):
719         import dask
720 
721         info = [
722             (True, k, v.__dask_postcompute__())
723             if dask.is_dask_collection(v)
724             else (False, k, v)
725             for k, v in self._variables.items()
726         ]
727         args = (
728             info,
729             self._coord_names,
730             self._dims,
731             self._attrs,
732             self._indexes,
733             self._encoding,
734             self._file_obj,
735         )
736         return self._dask_postcompute, args
737 
738     def __dask_postpersist__(self):
739         import dask
740 
741         info = [
742             (True, k, v.__dask_postpersist__())
743             if dask.is_dask_collection(v)
744             else (False, k, v)
745             for k, v in self._variables.items()
746         ]
747         args = (
748             info,
749             self._coord_names,
750             self._dims,
751             self._attrs,
752             self._indexes,
753             self._encoding,
754             self._file_obj,
755         )
756         return self._dask_postpersist, args
757 
758     @staticmethod
759     def _dask_postcompute(results, info, *args):
760         variables = {}
761         results2 = list(results[::-1])
762         for is_dask, k, v in info:
763             if is_dask:
764                 func, args2 = v
765                 r = results2.pop()
766                 result = func(r, *args2)
767             else:
768                 result = v
769             variables[k] = result
770 
771         final = Dataset._construct_direct(variables, *args)
772         return final
773 
774     @staticmethod
775     def _dask_postpersist(dsk, info, *args):
776         variables = {}
777         for is_dask, k, v in info:
778             if is_dask:
779                 func, args2 = v
780                 result = func(dsk, *args2)
781             else:
782                 result = v
783             variables[k] = result
784 
785         return Dataset._construct_direct(variables, *args)
786 
787     def compute(self, **kwargs) -> "Dataset":
788         """Manually trigger loading and/or computation of this dataset's data
789         from disk or a remote source into memory and return a new dataset.
790         Unlike load, the original dataset is left unaltered.
791 
792         Normally, it should not be necessary to call this method in user code,
793         because all xarray functions should either work on deferred data or
794         load data automatically. However, this method can be necessary when
795         working with many file objects on disk.
796 
797         Parameters
798         ----------
799         **kwargs : dict
800             Additional keyword arguments passed on to ``dask.array.compute``.
801 
802         See Also
803         --------
804         dask.array.compute
805         """
806         new = self.copy(deep=False)
807         return new.load(**kwargs)
808 
809     def _persist_inplace(self, **kwargs) -> "Dataset":
810         """Persist all Dask arrays in memory
811         """
812         # access .data to coerce everything to numpy or dask arrays
813         lazy_data = {
814             k: v._data
815             for k, v in self.variables.items()
816             if isinstance(v._data, dask_array_type)
817         }
818         if lazy_data:
819             import dask
820 
821             # evaluate all the dask arrays simultaneously
822             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
823 
824             for k, data in zip(lazy_data, evaluated_data):
825                 self.variables[k].data = data
826 
827         return self
828 
829     def persist(self, **kwargs) -> "Dataset":
830         """ Trigger computation, keeping data as dask arrays
831 
832         This operation can be used to trigger computation on underlying dask
833         arrays, similar to ``.compute()`` or ``.load()``.  However this
834         operation keeps the data as dask arrays. This is particularly useful
835         when using the dask.distributed scheduler and you want to load a large
836         amount of data into distributed memory.
837 
838         Parameters
839         ----------
840         **kwargs : dict
841             Additional keyword arguments passed on to ``dask.persist``.
842 
843         See Also
844         --------
845         dask.persist
846         """
847         new = self.copy(deep=False)
848         return new._persist_inplace(**kwargs)
849 
850     @classmethod
851     def _construct_direct(
852         cls,
853         variables,
854         coord_names,
855         dims=None,
856         attrs=None,
857         indexes=None,
858         encoding=None,
859         file_obj=None,
860     ):
861         """Shortcut around __init__ for internal use when we want to skip
862         costly validation
863         """
864         if dims is None:
865             dims = calculate_dimensions(variables)
866         obj = object.__new__(cls)
867         obj._variables = variables
868         obj._coord_names = coord_names
869         obj._dims = dims
870         obj._indexes = indexes
871         obj._attrs = attrs
872         obj._file_obj = file_obj
873         obj._encoding = encoding
874         return obj
875 
876     def _replace(
877         self,
878         variables: Dict[Hashable, Variable] = None,
879         coord_names: Set[Hashable] = None,
880         dims: Dict[Any, int] = None,
881         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
882         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
883         encoding: Union[dict, None, Default] = _default,
884         inplace: bool = False,
885     ) -> "Dataset":
886         """Fastpath constructor for internal use.
887 
888         Returns an object with optionally with replaced attributes.
889 
890         Explicitly passed arguments are *not* copied when placed on the new
891         dataset. It is up to the caller to ensure that they have the right type
892         and are not used elsewhere.
893         """
894         if inplace:
895             if variables is not None:
896                 self._variables = variables
897             if coord_names is not None:
898                 self._coord_names = coord_names
899             if dims is not None:
900                 self._dims = dims
901             if attrs is not _default:
902                 self._attrs = attrs
903             if indexes is not _default:
904                 self._indexes = indexes
905             if encoding is not _default:
906                 self._encoding = encoding
907             obj = self
908         else:
909             if variables is None:
910                 variables = self._variables.copy()
911             if coord_names is None:
912                 coord_names = self._coord_names.copy()
913             if dims is None:
914                 dims = self._dims.copy()
915             if attrs is _default:
916                 attrs = copy.copy(self._attrs)
917             if indexes is _default:
918                 indexes = copy.copy(self._indexes)
919             if encoding is _default:
920                 encoding = copy.copy(self._encoding)
921             obj = self._construct_direct(
922                 variables, coord_names, dims, attrs, indexes, encoding
923             )
924         return obj
925 
926     def _replace_with_new_dims(
927         self,
928         variables: Dict[Hashable, Variable],
929         coord_names: set = None,
930         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
931         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
932         inplace: bool = False,
933     ) -> "Dataset":
934         """Replace variables with recalculated dimensions."""
935         dims = calculate_dimensions(variables)
936         return self._replace(
937             variables, coord_names, dims, attrs, indexes, inplace=inplace
938         )
939 
940     def _replace_vars_and_dims(
941         self,
942         variables: Dict[Hashable, Variable],
943         coord_names: set = None,
944         dims: Dict[Hashable, int] = None,
945         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
946         inplace: bool = False,
947     ) -> "Dataset":
948         """Deprecated version of _replace_with_new_dims().
949 
950         Unlike _replace_with_new_dims(), this method always recalculates
951         indexes from variables.
952         """
953         if dims is None:
954             dims = calculate_dimensions(variables)
955         return self._replace(
956             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
957         )
958 
959     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
960         if not indexes:
961             return self
962 
963         variables = self._variables.copy()
964         new_indexes = dict(self.indexes)
965         for name, idx in indexes.items():
966             variables[name] = IndexVariable(name, idx)
967             new_indexes[name] = idx
968         obj = self._replace(variables, indexes=new_indexes)
969 
970         # switch from dimension to level names, if necessary
971         dim_names: Dict[Hashable, str] = {}
972         for dim, idx in indexes.items():
973             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
974                 dim_names[dim] = idx.name
975         if dim_names:
976             obj = obj.rename(dim_names)
977         return obj
978 
979     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
980         """Returns a copy of this dataset.
981 
982         If `deep=True`, a deep copy is made of each of the component variables.
983         Otherwise, a shallow copy of each of the component variable is made, so
984         that the underlying memory region of the new dataset is the same as in
985         the original dataset.
986 
987         Use `data` to create a new object with the same structure as
988         original but entirely new data.
989 
990         Parameters
991         ----------
992         deep : bool, optional
993             Whether each component variable is loaded into memory and copied onto
994             the new object. Default is False.
995         data : dict-like, optional
996             Data to use in the new object. Each item in `data` must have same
997             shape as corresponding data variable in original. When `data` is
998             used, `deep` is ignored for the data variables and only used for
999             coords.
1000 
1001         Returns
1002         -------
1003         object : Dataset
1004             New object with dimensions, attributes, coordinates, name, encoding,
1005             and optionally data copied from original.
1006 
1007         Examples
1008         --------
1009 
1010         Shallow copy versus deep copy
1011 
1012         >>> da = xr.DataArray(np.random.randn(2, 3))
1013         >>> ds = xr.Dataset(
1014         ...     {"foo": da, "bar": ("x", [-1, 2])}, coords={"x": ["one", "two"]},
1015         ... )
1016         >>> ds.copy()
1017         <xarray.Dataset>
1018         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1019         Coordinates:
1020         * x        (x) <U3 'one' 'two'
1021         Dimensions without coordinates: dim_0, dim_1
1022         Data variables:
1023             foo      (dim_0, dim_1) float64 -0.8079 0.3897 -1.862 -0.6091 -1.051 -0.3003
1024             bar      (x) int64 -1 2
1025 
1026         >>> ds_0 = ds.copy(deep=False)
1027         >>> ds_0["foo"][0, 0] = 7
1028         >>> ds_0
1029         <xarray.Dataset>
1030         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1031         Coordinates:
1032         * x        (x) <U3 'one' 'two'
1033         Dimensions without coordinates: dim_0, dim_1
1034         Data variables:
1035             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1036             bar      (x) int64 -1 2
1037 
1038         >>> ds
1039         <xarray.Dataset>
1040         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1041         Coordinates:
1042         * x        (x) <U3 'one' 'two'
1043         Dimensions without coordinates: dim_0, dim_1
1044         Data variables:
1045             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1046             bar      (x) int64 -1 2
1047 
1048         Changing the data using the ``data`` argument maintains the
1049         structure of the original object, but with the new data. Original
1050         object is unaffected.
1051 
1052         >>> ds.copy(
1053         ...     data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]}
1054         ... )
1055         <xarray.Dataset>
1056         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1057         Coordinates:
1058         * x        (x) <U3 'one' 'two'
1059         Dimensions without coordinates: dim_0, dim_1
1060         Data variables:
1061             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1062             bar      (x) <U1 'a' 'b'
1063 
1064         >>> ds
1065         <xarray.Dataset>
1066         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1067         Coordinates:
1068         * x        (x) <U3 'one' 'two'
1069         Dimensions without coordinates: dim_0, dim_1
1070         Data variables:
1071             foo      (dim_0, dim_1) float64 7.0 0.3897 -1.862 -0.6091 -1.051 -0.3003
1072             bar      (x) int64 -1 2
1073 
1074         See Also
1075         --------
1076         pandas.DataFrame.copy
1077         """
1078         if data is None:
1079             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1080         elif not utils.is_dict_like(data):
1081             raise ValueError("Data must be dict-like")
1082         else:
1083             var_keys = set(self.data_vars.keys())
1084             data_keys = set(data.keys())
1085             keys_not_in_vars = data_keys - var_keys
1086             if keys_not_in_vars:
1087                 raise ValueError(
1088                     "Data must only contain variables in original "
1089                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1090                 )
1091             keys_missing_from_data = var_keys - data_keys
1092             if keys_missing_from_data:
1093                 raise ValueError(
1094                     "Data must contain all variables in original "
1095                     "dataset. Data is missing {}".format(keys_missing_from_data)
1096                 )
1097             variables = {
1098                 k: v.copy(deep=deep, data=data.get(k))
1099                 for k, v in self._variables.items()
1100             }
1101 
1102         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1103 
1104         return self._replace(variables, attrs=attrs)
1105 
1106     @property
1107     def _level_coords(self) -> Dict[str, Hashable]:
1108         """Return a mapping of all MultiIndex levels and their corresponding
1109         coordinate name.
1110         """
1111         level_coords: Dict[str, Hashable] = {}
1112         for name, index in self.indexes.items():
1113             if isinstance(index, pd.MultiIndex):
1114                 level_names = index.names
1115                 (dim,) = self.variables[name].dims
1116                 level_coords.update({lname: dim for lname in level_names})
1117         return level_coords
1118 
1119     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1120         """Create a new Dataset with the listed variables from this dataset and
1121         the all relevant coordinates. Skips all validation.
1122         """
1123         variables: Dict[Hashable, Variable] = {}
1124         coord_names = set()
1125         indexes: Dict[Hashable, pd.Index] = {}
1126 
1127         for name in names:
1128             try:
1129                 variables[name] = self._variables[name]
1130             except KeyError:
1131                 ref_name, var_name, var = _get_virtual_variable(
1132                     self._variables, name, self._level_coords, self.dims
1133                 )
1134                 variables[var_name] = var
1135                 if ref_name in self._coord_names or ref_name in self.dims:
1136                     coord_names.add(var_name)
1137                 if (var_name,) == var.dims:
1138                     indexes[var_name] = var.to_index()
1139 
1140         needed_dims: Set[Hashable] = set()
1141         for v in variables.values():
1142             needed_dims.update(v.dims)
1143 
1144         dims = {k: self.dims[k] for k in needed_dims}
1145 
1146         for k in self._coord_names:
1147             if set(self.variables[k].dims) <= needed_dims:
1148                 variables[k] = self._variables[k]
1149                 coord_names.add(k)
1150                 if k in self.indexes:
1151                     indexes[k] = self.indexes[k]
1152 
1153         return self._replace(variables, coord_names, dims, indexes=indexes)
1154 
1155     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1156         """Construct a DataArray by indexing this dataset
1157         """
1158         from .dataarray import DataArray
1159 
1160         try:
1161             variable = self._variables[name]
1162         except KeyError:
1163             _, name, variable = _get_virtual_variable(
1164                 self._variables, name, self._level_coords, self.dims
1165             )
1166 
1167         needed_dims = set(variable.dims)
1168 
1169         coords: Dict[Hashable, Variable] = {}
1170         for k in self.coords:
1171             if set(self.variables[k].dims) <= needed_dims:
1172                 coords[k] = self.variables[k]
1173 
1174         if self._indexes is None:
1175             indexes = None
1176         else:
1177             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1178 
1179         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1180 
1181     def __copy__(self) -> "Dataset":
1182         return self.copy(deep=False)
1183 
1184     def __deepcopy__(self, memo=None) -> "Dataset":
1185         # memo does nothing but is required for compatibility with
1186         # copy.deepcopy
1187         return self.copy(deep=True)
1188 
1189     @property
1190     def _attr_sources(self) -> List[Mapping[Hashable, Any]]:
1191         """List of places to look-up items for attribute-style access
1192         """
1193         return self._item_sources + [self.attrs]
1194 
1195     @property
1196     def _item_sources(self) -> List[Mapping[Hashable, Any]]:
1197         """List of places to look-up items for key-completion
1198         """
1199         return [
1200             self.data_vars,
1201             self.coords,
1202             {d: self[d] for d in self.dims},
1203             LevelCoordinatesSource(self),
1204         ]
1205 
1206     def __contains__(self, key: object) -> bool:
1207         """The 'in' operator will return true or false depending on whether
1208         'key' is an array in the dataset or not.
1209         """
1210         return key in self._variables
1211 
1212     def __len__(self) -> int:
1213         return len(self.data_vars)
1214 
1215     def __bool__(self) -> bool:
1216         return bool(self.data_vars)
1217 
1218     def __iter__(self) -> Iterator[Hashable]:
1219         return iter(self.data_vars)
1220 
1221     def __array__(self, dtype=None):
1222         raise TypeError(
1223             "cannot directly convert an xarray.Dataset into a "
1224             "numpy array. Instead, create an xarray.DataArray "
1225             "first, either with indexing on the Dataset or by "
1226             "invoking the `to_array()` method."
1227         )
1228 
1229     @property
1230     def nbytes(self) -> int:
1231         return sum(v.nbytes for v in self.variables.values())
1232 
1233     @property
1234     def loc(self) -> _LocIndexer:
1235         """Attribute for location based indexing. Only supports __getitem__,
1236         and only when the key is a dict of the form {dim: labels}.
1237         """
1238         return _LocIndexer(self)
1239 
1240     def __getitem__(self, key: Any) -> "Union[DataArray, Dataset]":
1241         """Access variables or coordinates this dataset as a
1242         :py:class:`~xarray.DataArray`.
1243 
1244         Indexing with a list of names will return a new ``Dataset`` object.
1245         """
1246         # TODO(shoyer): type this properly: https://github.com/python/mypy/issues/7328
1247         if utils.is_dict_like(key):
1248             return self.isel(**cast(Mapping, key))
1249 
1250         if hashable(key):
1251             return self._construct_dataarray(key)
1252         else:
1253             return self._copy_listed(np.asarray(key))
1254 
1255     def __setitem__(self, key: Hashable, value) -> None:
1256         """Add an array to this dataset.
1257 
1258         If value is a `DataArray`, call its `select_vars()` method, rename it
1259         to `key` and merge the contents of the resulting dataset into this
1260         dataset.
1261 
1262         If value is an `Variable` object (or tuple of form
1263         ``(dims, data[, attrs])``), add it to this dataset as a new
1264         variable.
1265         """
1266         if utils.is_dict_like(key):
1267             raise NotImplementedError(
1268                 "cannot yet use a dictionary as a key " "to set Dataset values"
1269             )
1270 
1271         self.update({key: value})
1272 
1273     def __delitem__(self, key: Hashable) -> None:
1274         """Remove a variable from this dataset.
1275         """
1276         del self._variables[key]
1277         self._coord_names.discard(key)
1278         if key in self.indexes:
1279             assert self._indexes is not None
1280             del self._indexes[key]
1281         self._dims = calculate_dimensions(self._variables)
1282 
1283     # mutable objects should not be hashable
1284     # https://github.com/python/mypy/issues/4266
1285     __hash__ = None  # type: ignore
1286 
1287     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1288         """Helper function for equals and identical
1289         """
1290 
1291         # some stores (e.g., scipy) do not seem to preserve order, so don't
1292         # require matching order for equality
1293         def compat(x: Variable, y: Variable) -> bool:
1294             return getattr(x, compat_str)(y)
1295 
1296         return self._coord_names == other._coord_names and utils.dict_equiv(
1297             self._variables, other._variables, compat=compat
1298         )
1299 
1300     def broadcast_equals(self, other: "Dataset") -> bool:
1301         """Two Datasets are broadcast equal if they are equal after
1302         broadcasting all variables against each other.
1303 
1304         For example, variables that are scalar in one dataset but non-scalar in
1305         the other dataset can still be broadcast equal if the the non-scalar
1306         variable is a constant.
1307 
1308         See Also
1309         --------
1310         Dataset.equals
1311         Dataset.identical
1312         """
1313         try:
1314             return self._all_compat(other, "broadcast_equals")
1315         except (TypeError, AttributeError):
1316             return False
1317 
1318     def equals(self, other: "Dataset") -> bool:
1319         """Two Datasets are equal if they have matching variables and
1320         coordinates, all of which are equal.
1321 
1322         Datasets can still be equal (like pandas objects) if they have NaN
1323         values in the same locations.
1324 
1325         This method is necessary because `v1 == v2` for ``Dataset``
1326         does element-wise comparisons (like numpy.ndarrays).
1327 
1328         See Also
1329         --------
1330         Dataset.broadcast_equals
1331         Dataset.identical
1332         """
1333         try:
1334             return self._all_compat(other, "equals")
1335         except (TypeError, AttributeError):
1336             return False
1337 
1338     def identical(self, other: "Dataset") -> bool:
1339         """Like equals, but also checks all dataset attributes and the
1340         attributes on all variables and coordinates.
1341 
1342         See Also
1343         --------
1344         Dataset.broadcast_equals
1345         Dataset.equals
1346         """
1347         try:
1348             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1349                 other, "identical"
1350             )
1351         except (TypeError, AttributeError):
1352             return False
1353 
1354     @property
1355     def indexes(self) -> Indexes:
1356         """Mapping of pandas.Index objects used for label based indexing
1357         """
1358         if self._indexes is None:
1359             self._indexes = default_indexes(self._variables, self._dims)
1360         return Indexes(self._indexes)
1361 
1362     @property
1363     def coords(self) -> DatasetCoordinates:
1364         """Dictionary of xarray.DataArray objects corresponding to coordinate
1365         variables
1366         """
1367         return DatasetCoordinates(self)
1368 
1369     @property
1370     def data_vars(self) -> DataVariables:
1371         """Dictionary of DataArray objects corresponding to data variables
1372         """
1373         return DataVariables(self)
1374 
1375     def set_coords(
1376         self, names: "Union[Hashable, Iterable[Hashable]]", inplace: bool = None
1377     ) -> "Dataset":
1378         """Given names of one or more variables, set them as coordinates
1379 
1380         Parameters
1381         ----------
1382         names : hashable or iterable of hashables
1383             Name(s) of variables in this dataset to convert into coordinates.
1384 
1385         Returns
1386         -------
1387         Dataset
1388 
1389         See also
1390         --------
1391         Dataset.swap_dims
1392         """
1393         # TODO: allow inserting new coordinates with this method, like
1394         # DataFrame.set_index?
1395         # nb. check in self._variables, not self.data_vars to insure that the
1396         # operation is idempotent
1397         _check_inplace(inplace)
1398         if isinstance(names, str) or not isinstance(names, Iterable):
1399             names = [names]
1400         else:
1401             names = list(names)
1402         self._assert_all_in_dataset(names)
1403         obj = self.copy()
1404         obj._coord_names.update(names)
1405         return obj
1406 
1407     def reset_coords(
1408         self,
1409         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1410         drop: bool = False,
1411         inplace: bool = None,
1412     ) -> "Dataset":
1413         """Given names of coordinates, reset them to become variables
1414 
1415         Parameters
1416         ----------
1417         names : hashable or iterable of hashables, optional
1418             Name(s) of non-index coordinates in this dataset to reset into
1419             variables. By default, all non-index coordinates are reset.
1420         drop : bool, optional
1421             If True, remove coordinates instead of converting them into
1422             variables.
1423 
1424         Returns
1425         -------
1426         Dataset
1427         """
1428         _check_inplace(inplace)
1429         if names is None:
1430             names = self._coord_names - set(self.dims)
1431         else:
1432             if isinstance(names, str) or not isinstance(names, Iterable):
1433                 names = [names]
1434             else:
1435                 names = list(names)
1436             self._assert_all_in_dataset(names)
1437             bad_coords = set(names) & set(self.dims)
1438             if bad_coords:
1439                 raise ValueError(
1440                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1441                 )
1442         obj = self.copy()
1443         obj._coord_names.difference_update(names)
1444         if drop:
1445             for name in names:
1446                 del obj._variables[name]
1447         return obj
1448 
1449     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1450         """Store dataset contents to a backends.*DataStore object.
1451         """
1452         from ..backends.api import dump_to_store
1453 
1454         # TODO: rename and/or cleanup this method to make it more consistent
1455         # with to_netcdf()
1456         dump_to_store(self, store, **kwargs)
1457 
1458     def to_netcdf(
1459         self,
1460         path=None,
1461         mode: str = "w",
1462         format: str = None,
1463         group: str = None,
1464         engine: str = None,
1465         encoding: Mapping = None,
1466         unlimited_dims: Iterable[Hashable] = None,
1467         compute: bool = True,
1468         invalid_netcdf: bool = False,
1469     ) -> Union[bytes, "Delayed", None]:
1470         """Write dataset contents to a netCDF file.
1471 
1472         Parameters
1473         ----------
1474         path : str, Path or file-like object, optional
1475             Path to which to save this dataset. File-like objects are only
1476             supported by the scipy engine. If no path is provided, this
1477             function returns the resulting netCDF file as bytes; in this case,
1478             we need to use scipy, which does not support netCDF version 4 (the
1479             default format becomes NETCDF3_64BIT).
1480         mode : {'w', 'a'}, optional
1481             Write ('w') or append ('a') mode. If mode='w', any existing file at
1482             this location will be overwritten. If mode='a', existing variables
1483             will be overwritten.
1484         format : {'NETCDF4', 'NETCDF4_CLASSIC', 'NETCDF3_64BIT',
1485                   'NETCDF3_CLASSIC'}, optional
1486             File format for the resulting netCDF file:
1487 
1488             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1489               features.
1490             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1491               netCDF 3 compatible API features.
1492             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1493               which fully supports 2+ GB files, but is only compatible with
1494               clients linked against netCDF version 3.6.0 or later.
1495             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1496               handle 2+ GB files very well.
1497 
1498             All formats are supported by the netCDF4-python library.
1499             scipy.io.netcdf only supports the last two formats.
1500 
1501             The default format is NETCDF4 if you are saving a file to disk and
1502             have the netCDF4-python library available. Otherwise, xarray falls
1503             back to using scipy to write netCDF files and defaults to the
1504             NETCDF3_64BIT format (scipy does not support netCDF4).
1505         group : str, optional
1506             Path to the netCDF4 group in the given file to open (only works for
1507             format='NETCDF4'). The group(s) will be created if necessary.
1508         engine : {'netcdf4', 'scipy', 'h5netcdf'}, optional
1509             Engine to use when writing netCDF files. If not provided, the
1510             default engine is chosen based on available dependencies, with a
1511             preference for 'netcdf4' if writing to a file on disk.
1512         encoding : dict, optional
1513             Nested dictionary with variable names as keys and dictionaries of
1514             variable specific encodings as values, e.g.,
1515             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,
1516             'zlib': True}, ...}``
1517 
1518             The `h5netcdf` engine supports both the NetCDF4-style compression
1519             encoding parameters ``{'zlib': True, 'complevel': 9}`` and the h5py
1520             ones ``{'compression': 'gzip', 'compression_opts': 9}``.
1521             This allows using any compression plugin installed in the HDF5
1522             library, e.g. LZF.
1523 
1524         unlimited_dims : iterable of hashable, optional
1525             Dimension(s) that should be serialized as unlimited dimensions.
1526             By default, no dimensions are treated as unlimited dimensions.
1527             Note that unlimited_dims may also be set via
1528             ``dataset.encoding['unlimited_dims']``.
1529         compute: boolean
1530             If true compute immediately, otherwise return a
1531             ``dask.delayed.Delayed`` object that can be computed later.
1532         invalid_netcdf: boolean
1533             Only valid along with engine='h5netcdf'. If True, allow writing
1534             hdf5 files which are valid netcdf as described in
1535             https://github.com/shoyer/h5netcdf. Default: False.
1536         """
1537         if encoding is None:
1538             encoding = {}
1539         from ..backends.api import to_netcdf
1540 
1541         return to_netcdf(
1542             self,
1543             path,
1544             mode,
1545             format=format,
1546             group=group,
1547             engine=engine,
1548             encoding=encoding,
1549             unlimited_dims=unlimited_dims,
1550             compute=compute,
1551             invalid_netcdf=invalid_netcdf,
1552         )
1553 
1554     def to_zarr(
1555         self,
1556         store: Union[MutableMapping, str, Path] = None,
1557         mode: str = None,
1558         synchronizer=None,
1559         group: str = None,
1560         encoding: Mapping = None,
1561         compute: bool = True,
1562         consolidated: bool = False,
1563         append_dim: Hashable = None,
1564     ) -> "ZarrStore":
1565         """Write dataset contents to a zarr group.
1566 
1567         .. note:: Experimental
1568                   The Zarr backend is new and experimental. Please report any
1569                   unexpected behavior via github issues.
1570 
1571         Parameters
1572         ----------
1573         store : MutableMapping, str or Path, optional
1574             Store or path to directory in file system.
1575         mode : {'w', 'w-', 'a', None}
1576             Persistence mode: 'w' means create (overwrite if exists);
1577             'w-' means create (fail if exists);
1578             'a' means append (create if does not exist).
1579             If ``append_dim`` is set, ``mode`` can be omitted as it is
1580             internally set to ``'a'``. Otherwise, ``mode`` will default to
1581             `w-` if not set.
1582         synchronizer : object, optional
1583             Array synchronizer
1584         group : str, optional
1585             Group path. (a.k.a. `path` in zarr terminology.)
1586         encoding : dict, optional
1587             Nested dictionary with variable names as keys and dictionaries of
1588             variable specific encodings as values, e.g.,
1589             ``{'my_variable': {'dtype': 'int16', 'scale_factor': 0.1,}, ...}``
1590         compute: bool, optional
1591             If True compute immediately, otherwise return a
1592             ``dask.delayed.Delayed`` object that can be computed later.
1593         consolidated: bool, optional
1594             If True, apply zarr's `consolidate_metadata` function to the store
1595             after writing.
1596         append_dim: hashable, optional
1597             If set, the dimension on which the data will be appended.
1598 
1599         References
1600         ----------
1601         https://zarr.readthedocs.io/
1602         """
1603         if encoding is None:
1604             encoding = {}
1605         if (mode == "a") or (append_dim is not None):
1606             if mode is None:
1607                 mode = "a"
1608             elif mode != "a":
1609                 raise ValueError(
1610                     "append_dim was set along with mode='{}', either set "
1611                     "mode='a' or don't set it.".format(mode)
1612                 )
1613         elif mode is None:
1614             mode = "w-"
1615         if mode not in ["w", "w-", "a"]:
1616             # TODO: figure out how to handle 'r+'
1617             raise ValueError(
1618                 "The only supported options for mode are 'w'," "'w-' and 'a'."
1619             )
1620         from ..backends.api import to_zarr
1621 
1622         return to_zarr(
1623             self,
1624             store=store,
1625             mode=mode,
1626             synchronizer=synchronizer,
1627             group=group,
1628             encoding=encoding,
1629             compute=compute,
1630             consolidated=consolidated,
1631             append_dim=append_dim,
1632         )
1633 
1634     def __repr__(self) -> str:
1635         return formatting.dataset_repr(self)
1636 
1637     def _repr_html_(self):
1638         if OPTIONS["display_style"] == "text":
1639             return f"<pre>{escape(repr(self))}</pre>"
1640         return formatting_html.dataset_repr(self)
1641 
1642     def info(self, buf=None) -> None:
1643         """
1644         Concise summary of a Dataset variables and attributes.
1645 
1646         Parameters
1647         ----------
1648         buf : writable buffer, defaults to sys.stdout
1649 
1650         See Also
1651         --------
1652         pandas.DataFrame.assign
1653         ncdump: netCDF's ncdump
1654         """
1655         if buf is None:  # pragma: no cover
1656             buf = sys.stdout
1657 
1658         lines = []
1659         lines.append("xarray.Dataset {")
1660         lines.append("dimensions:")
1661         for name, size in self.dims.items():
1662             lines.append(f"\t{name} = {size} ;")
1663         lines.append("\nvariables:")
1664         for name, da in self.variables.items():
1665             dims = ", ".join(da.dims)
1666             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1667             for k, v in da.attrs.items():
1668                 lines.append(f"\t\t{name}:{k} = {v} ;")
1669         lines.append("\n// global attributes:")
1670         for k, v in self.attrs.items():
1671             lines.append(f"\t:{k} = {v} ;")
1672         lines.append("}")
1673 
1674         buf.write("\n".join(lines))
1675 
1676     @property
1677     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1678         """Block dimensions for this dataset's data or None if it's not a dask
1679         array.
1680         """
1681         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1682         for v in self.variables.values():
1683             if v.chunks is not None:
1684                 for dim, c in zip(v.dims, v.chunks):
1685                     if dim in chunks and c != chunks[dim]:
1686                         raise ValueError(
1687                             f"Object has inconsistent chunks along dimension {dim}. "
1688                             "This can be fixed by calling unify_chunks()."
1689                         )
1690                     chunks[dim] = c
1691         return Frozen(SortedKeysDict(chunks))
1692 
1693     def chunk(
1694         self,
1695         chunks: Union[
1696             None, Number, Mapping[Hashable, Union[None, Number, Tuple[Number, ...]]]
1697         ] = None,
1698         name_prefix: str = "xarray-",
1699         token: str = None,
1700         lock: bool = False,
1701     ) -> "Dataset":
1702         """Coerce all arrays in this dataset into dask arrays with the given
1703         chunks.
1704 
1705         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1706         arrays will be rechunked to the given chunk sizes.
1707 
1708         If neither chunks is not provided for one or more dimensions, chunk
1709         sizes along that dimension will not be updated; non-dask arrays will be
1710         converted into dask arrays with a single block.
1711 
1712         Parameters
1713         ----------
1714         chunks : int or mapping, optional
1715             Chunk sizes along each dimension, e.g., ``5`` or
1716             ``{'x': 5, 'y': 5}``.
1717         name_prefix : str, optional
1718             Prefix for the name of any new dask arrays.
1719         token : str, optional
1720             Token uniquely identifying this dataset.
1721         lock : optional
1722             Passed on to :py:func:`dask.array.from_array`, if the array is not
1723             already as dask array.
1724 
1725         Returns
1726         -------
1727         chunked : xarray.Dataset
1728         """
1729         from dask.base import tokenize
1730 
1731         if isinstance(chunks, Number):
1732             chunks = dict.fromkeys(self.dims, chunks)
1733 
1734         if chunks is not None:
1735             bad_dims = chunks.keys() - self.dims.keys()
1736             if bad_dims:
1737                 raise ValueError(
1738                     "some chunks keys are not dimensions on this "
1739                     "object: %s" % bad_dims
1740                 )
1741 
1742         def selkeys(dict_, keys):
1743             if dict_ is None:
1744                 return None
1745             return {d: dict_[d] for d in keys if d in dict_}
1746 
1747         def maybe_chunk(name, var, chunks):
1748             chunks = selkeys(chunks, var.dims)
1749             if not chunks:
1750                 chunks = None
1751             if var.ndim > 0:
1752                 # when rechunking by different amounts, make sure dask names change
1753                 # by provinding chunks as an input to tokenize.
1754                 # subtle bugs result otherwise. see GH3350
1755                 token2 = tokenize(name, token if token else var._data, chunks)
1756                 name2 = f"{name_prefix}{name}-{token2}"
1757                 return var.chunk(chunks, name=name2, lock=lock)
1758             else:
1759                 return var
1760 
1761         variables = {k: maybe_chunk(k, v, chunks) for k, v in self.variables.items()}
1762         return self._replace(variables)
1763 
1764     def _validate_indexers(
1765         self, indexers: Mapping[Hashable, Any]
1766     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1767         """ Here we make sure
1768         + indexer has a valid keys
1769         + indexer is in a valid data type
1770         + string indexers are cast to the appropriate date type if the
1771           associated index is a DatetimeIndex or CFTimeIndex
1772         """
1773         from .dataarray import DataArray
1774 
1775         invalid = indexers.keys() - self.dims.keys()
1776         if invalid:
1777             raise ValueError("dimensions %r do not exist" % invalid)
1778 
1779         # all indexers should be int, slice, np.ndarrays, or Variable
1780         for k, v in indexers.items():
1781             if isinstance(v, (int, slice, Variable)):
1782                 yield k, v
1783             elif isinstance(v, DataArray):
1784                 yield k, v.variable
1785             elif isinstance(v, tuple):
1786                 yield k, as_variable(v)
1787             elif isinstance(v, Dataset):
1788                 raise TypeError("cannot use a Dataset as an indexer")
1789             elif isinstance(v, Sequence) and len(v) == 0:
1790                 yield k, np.empty((0,), dtype="int64")
1791             else:
1792                 v = np.asarray(v)
1793 
1794                 if v.dtype.kind in "US":
1795                     index = self.indexes[k]
1796                     if isinstance(index, pd.DatetimeIndex):
1797                         v = v.astype("datetime64[ns]")
1798                     elif isinstance(index, xr.CFTimeIndex):
1799                         v = _parse_array_of_cftime_strings(v, index.date_type)
1800 
1801                 if v.ndim > 1:
1802                     raise IndexError(
1803                         "Unlabeled multi-dimensional array cannot be "
1804                         "used for indexing: {}".format(k)
1805                     )
1806                 yield k, v
1807 
1808     def _validate_interp_indexers(
1809         self, indexers: Mapping[Hashable, Any]
1810     ) -> Iterator[Tuple[Hashable, Variable]]:
1811         """Variant of _validate_indexers to be used for interpolation
1812         """
1813         for k, v in self._validate_indexers(indexers):
1814             if isinstance(v, Variable):
1815                 if v.ndim == 1:
1816                     yield k, v.to_index_variable()
1817                 else:
1818                     yield k, v
1819             elif isinstance(v, int):
1820                 yield k, Variable((), v)
1821             elif isinstance(v, np.ndarray):
1822                 if v.ndim == 0:
1823                     yield k, Variable((), v)
1824                 elif v.ndim == 1:
1825                     yield k, IndexVariable((k,), v)
1826                 else:
1827                     raise AssertionError()  # Already tested by _validate_indexers
1828             else:
1829                 raise TypeError(type(v))
1830 
1831     def _get_indexers_coords_and_indexes(self, indexers):
1832         """Extract coordinates and indexes from indexers.
1833 
1834         Only coordinate with a name different from any of self.variables will
1835         be attached.
1836         """
1837         from .dataarray import DataArray
1838 
1839         coords_list = []
1840         for k, v in indexers.items():
1841             if isinstance(v, DataArray):
1842                 if v.dtype.kind == "b":
1843                     if v.ndim != 1:  # we only support 1-d boolean array
1844                         raise ValueError(
1845                             "{:d}d-boolean array is used for indexing along "
1846                             "dimension {!r}, but only 1d boolean arrays are "
1847                             "supported.".format(v.ndim, k)
1848                         )
1849                     # Make sure in case of boolean DataArray, its
1850                     # coordinate also should be indexed.
1851                     v_coords = v[v.values.nonzero()[0]].coords
1852                 else:
1853                     v_coords = v.coords
1854                 coords_list.append(v_coords)
1855 
1856         # we don't need to call align() explicitly or check indexes for
1857         # alignment, because merge_variables already checks for exact alignment
1858         # between dimension coordinates
1859         coords, indexes = merge_coordinates_without_align(coords_list)
1860         assert_coordinate_consistent(self, coords)
1861 
1862         # silently drop the conflicted variables.
1863         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
1864         attached_indexes = {
1865             k: v for k, v in indexes.items() if k not in self._variables
1866         }
1867         return attached_coords, attached_indexes
1868 
1869     def isel(
1870         self,
1871         indexers: Mapping[Hashable, Any] = None,
1872         drop: bool = False,
1873         **indexers_kwargs: Any,
1874     ) -> "Dataset":
1875         """Returns a new dataset with each array indexed along the specified
1876         dimension(s).
1877 
1878         This method selects values from each array using its `__getitem__`
1879         method, except this method does not require knowing the order of
1880         each array's dimensions.
1881 
1882         Parameters
1883         ----------
1884         indexers : dict, optional
1885             A dict with keys matching dimensions and values given
1886             by integers, slice objects or arrays.
1887             indexer can be a integer, slice, array-like or DataArray.
1888             If DataArrays are passed as indexers, xarray-style indexing will be
1889             carried out. See :ref:`indexing` for the details.
1890             One of indexers or indexers_kwargs must be provided.
1891         drop : bool, optional
1892             If ``drop=True``, drop coordinates variables indexed by integers
1893             instead of making them scalar.
1894         **indexers_kwargs : {dim: indexer, ...}, optional
1895             The keyword arguments form of ``indexers``.
1896             One of indexers or indexers_kwargs must be provided.
1897 
1898         Returns
1899         -------
1900         obj : Dataset
1901             A new Dataset with the same contents as this dataset, except each
1902             array and dimension is indexed by the appropriate indexers.
1903             If indexer DataArrays have coordinates that do not conflict with
1904             this object, then these coordinates will be attached.
1905             In general, each array's data will be a view of the array's data
1906             in this dataset, unless vectorized indexing was triggered by using
1907             an array indexer, in which case the data will be a copy.
1908 
1909         See Also
1910         --------
1911         Dataset.sel
1912         DataArray.isel
1913         """
1914         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1915         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1916             return self._isel_fancy(indexers, drop=drop)
1917 
1918         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1919         # lists, or zero or one-dimensional np.ndarray's
1920         invalid = indexers.keys() - self.dims.keys()
1921         if invalid:
1922             raise ValueError("dimensions %r do not exist" % invalid)
1923 
1924         variables = {}
1925         dims: Dict[Hashable, Tuple[int, ...]] = {}
1926         coord_names = self._coord_names.copy()
1927         indexes = self._indexes.copy() if self._indexes is not None else None
1928 
1929         for var_name, var_value in self._variables.items():
1930             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
1931             if var_indexers:
1932                 var_value = var_value.isel(var_indexers)
1933                 if drop and var_value.ndim == 0 and var_name in coord_names:
1934                     coord_names.remove(var_name)
1935                     if indexes:
1936                         indexes.pop(var_name, None)
1937                     continue
1938                 if indexes and var_name in indexes:
1939                     if var_value.ndim == 1:
1940                         indexes[var_name] = var_value.to_index()
1941                     else:
1942                         del indexes[var_name]
1943             variables[var_name] = var_value
1944             dims.update(zip(var_value.dims, var_value.shape))
1945 
1946         return self._construct_direct(
1947             variables=variables,
1948             coord_names=coord_names,
1949             dims=dims,
1950             attrs=self._attrs,
1951             indexes=indexes,
1952             encoding=self._encoding,
1953             file_obj=self._file_obj,
1954         )
1955 
1956     def _isel_fancy(self, indexers: Mapping[Hashable, Any], *, drop: bool) -> "Dataset":
1957         # Note: we need to preserve the original indexers variable in order to merge the
1958         # coords below
1959         indexers_list = list(self._validate_indexers(indexers))
1960 
1961         variables: Dict[Hashable, Variable] = {}
1962         indexes: Dict[Hashable, pd.Index] = {}
1963 
1964         for name, var in self.variables.items():
1965             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
1966             if drop and name in var_indexers:
1967                 continue  # drop this variable
1968 
1969             if name in self.indexes:
1970                 new_var, new_index = isel_variable_and_index(
1971                     name, var, self.indexes[name], var_indexers
1972                 )
1973                 if new_index is not None:
1974                     indexes[name] = new_index
1975             elif var_indexers:
1976                 new_var = var.isel(indexers=var_indexers)
1977             else:
1978                 new_var = var.copy(deep=False)
1979 
1980             variables[name] = new_var
1981 
1982         coord_names = self._coord_names & variables.keys()
1983         selected = self._replace_with_new_dims(variables, coord_names, indexes)
1984 
1985         # Extract coordinates from indexers
1986         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
1987         variables.update(coord_vars)
1988         indexes.update(new_indexes)
1989         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
1990         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
1991 
1992     def sel(
1993         self,
1994         indexers: Mapping[Hashable, Any] = None,
1995         method: str = None,
1996         tolerance: Number = None,
1997         drop: bool = False,
1998         **indexers_kwargs: Any,
1999     ) -> "Dataset":
2000         """Returns a new dataset with each array indexed by tick labels
2001         along the specified dimension(s).
2002 
2003         In contrast to `Dataset.isel`, indexers for this method should use
2004         labels instead of integers.
2005 
2006         Under the hood, this method is powered by using pandas's powerful Index
2007         objects. This makes label based indexing essentially just as fast as
2008         using integer indexing.
2009 
2010         It also means this method uses pandas's (well documented) logic for
2011         indexing. This means you can use string shortcuts for datetime indexes
2012         (e.g., '2000-01' to select all values in January 2000). It also means
2013         that slices are treated as inclusive of both the start and stop values,
2014         unlike normal Python indexing.
2015 
2016         Parameters
2017         ----------
2018         indexers : dict, optional
2019             A dict with keys matching dimensions and values given
2020             by scalars, slices or arrays of tick labels. For dimensions with
2021             multi-index, the indexer may also be a dict-like object with keys
2022             matching index level names.
2023             If DataArrays are passed as indexers, xarray-style indexing will be
2024             carried out. See :ref:`indexing` for the details.
2025             One of indexers or indexers_kwargs must be provided.
2026         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2027             Method to use for inexact matches:
2028 
2029             * None (default): only exact matches
2030             * pad / ffill: propagate last valid index value forward
2031             * backfill / bfill: propagate next valid index value backward
2032             * nearest: use nearest valid index value
2033         tolerance : optional
2034             Maximum distance between original and new labels for inexact
2035             matches. The values of the index at the matching locations must
2036             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2037         drop : bool, optional
2038             If ``drop=True``, drop coordinates variables in `indexers` instead
2039             of making them scalar.
2040         **indexers_kwargs : {dim: indexer, ...}, optional
2041             The keyword arguments form of ``indexers``.
2042             One of indexers or indexers_kwargs must be provided.
2043 
2044         Returns
2045         -------
2046         obj : Dataset
2047             A new Dataset with the same contents as this dataset, except each
2048             variable and dimension is indexed by the appropriate indexers.
2049             If indexer DataArrays have coordinates that do not conflict with
2050             this object, then these coordinates will be attached.
2051             In general, each array's data will be a view of the array's data
2052             in this dataset, unless vectorized indexing was triggered by using
2053             an array indexer, in which case the data will be a copy.
2054 
2055 
2056         See Also
2057         --------
2058         Dataset.isel
2059         DataArray.sel
2060         """
2061         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2062         pos_indexers, new_indexes = remap_label_indexers(
2063             self, indexers=indexers, method=method, tolerance=tolerance
2064         )
2065         result = self.isel(indexers=pos_indexers, drop=drop)
2066         return result._overwrite_indexes(new_indexes)
2067 
2068     def head(
2069         self,
2070         indexers: Union[Mapping[Hashable, int], int] = None,
2071         **indexers_kwargs: Any,
2072     ) -> "Dataset":
2073         """Returns a new dataset with the first `n` values of each array
2074         for the specified dimension(s).
2075 
2076         Parameters
2077         ----------
2078         indexers : dict or int, default: 5
2079             A dict with keys matching dimensions and integer values `n`
2080             or a single integer `n` applied over all dimensions.
2081             One of indexers or indexers_kwargs must be provided.
2082         **indexers_kwargs : {dim: n, ...}, optional
2083             The keyword arguments form of ``indexers``.
2084             One of indexers or indexers_kwargs must be provided.
2085 
2086 
2087         See Also
2088         --------
2089         Dataset.tail
2090         Dataset.thin
2091         DataArray.head
2092         """
2093         if not indexers_kwargs:
2094             if indexers is None:
2095                 indexers = 5
2096             if not isinstance(indexers, int) and not is_dict_like(indexers):
2097                 raise TypeError("indexers must be either dict-like or a single integer")
2098         if isinstance(indexers, int):
2099             indexers = {dim: indexers for dim in self.dims}
2100         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2101         for k, v in indexers.items():
2102             if not isinstance(v, int):
2103                 raise TypeError(
2104                     "expected integer type indexer for "
2105                     "dimension %r, found %r" % (k, type(v))
2106                 )
2107             elif v < 0:
2108                 raise ValueError(
2109                     "expected positive integer as indexer "
2110                     "for dimension %r, found %s" % (k, v)
2111                 )
2112         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2113         return self.isel(indexers_slices)
2114 
2115     def tail(
2116         self,
2117         indexers: Union[Mapping[Hashable, int], int] = None,
2118         **indexers_kwargs: Any,
2119     ) -> "Dataset":
2120         """Returns a new dataset with the last `n` values of each array
2121         for the specified dimension(s).
2122 
2123         Parameters
2124         ----------
2125         indexers : dict or int, default: 5
2126             A dict with keys matching dimensions and integer values `n`
2127             or a single integer `n` applied over all dimensions.
2128             One of indexers or indexers_kwargs must be provided.
2129         **indexers_kwargs : {dim: n, ...}, optional
2130             The keyword arguments form of ``indexers``.
2131             One of indexers or indexers_kwargs must be provided.
2132 
2133 
2134         See Also
2135         --------
2136         Dataset.head
2137         Dataset.thin
2138         DataArray.tail
2139         """
2140         if not indexers_kwargs:
2141             if indexers is None:
2142                 indexers = 5
2143             if not isinstance(indexers, int) and not is_dict_like(indexers):
2144                 raise TypeError("indexers must be either dict-like or a single integer")
2145         if isinstance(indexers, int):
2146             indexers = {dim: indexers for dim in self.dims}
2147         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2148         for k, v in indexers.items():
2149             if not isinstance(v, int):
2150                 raise TypeError(
2151                     "expected integer type indexer for "
2152                     "dimension %r, found %r" % (k, type(v))
2153                 )
2154             elif v < 0:
2155                 raise ValueError(
2156                     "expected positive integer as indexer "
2157                     "for dimension %r, found %s" % (k, v)
2158                 )
2159         indexers_slices = {
2160             k: slice(-val, None) if val != 0 else slice(val)
2161             for k, val in indexers.items()
2162         }
2163         return self.isel(indexers_slices)
2164 
2165     def thin(
2166         self,
2167         indexers: Union[Mapping[Hashable, int], int] = None,
2168         **indexers_kwargs: Any,
2169     ) -> "Dataset":
2170         """Returns a new dataset with each array indexed along every `n`-th
2171         value for the specified dimension(s)
2172 
2173         Parameters
2174         ----------
2175         indexers : dict or int
2176             A dict with keys matching dimensions and integer values `n`
2177             or a single integer `n` applied over all dimensions.
2178             One of indexers or indexers_kwargs must be provided.
2179         ``**indexers_kwargs`` : {dim: n, ...}, optional
2180             The keyword arguments form of ``indexers``.
2181             One of indexers or indexers_kwargs must be provided.
2182 
2183 
2184         See Also
2185         --------
2186         Dataset.head
2187         Dataset.tail
2188         DataArray.thin
2189         """
2190         if (
2191             not indexers_kwargs
2192             and not isinstance(indexers, int)
2193             and not is_dict_like(indexers)
2194         ):
2195             raise TypeError("indexers must be either dict-like or a single integer")
2196         if isinstance(indexers, int):
2197             indexers = {dim: indexers for dim in self.dims}
2198         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2199         for k, v in indexers.items():
2200             if not isinstance(v, int):
2201                 raise TypeError(
2202                     "expected integer type indexer for "
2203                     "dimension %r, found %r" % (k, type(v))
2204                 )
2205             elif v < 0:
2206                 raise ValueError(
2207                     "expected positive integer as indexer "
2208                     "for dimension %r, found %s" % (k, v)
2209                 )
2210             elif v == 0:
2211                 raise ValueError("step cannot be zero")
2212         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2213         return self.isel(indexers_slices)
2214 
2215     def broadcast_like(
2216         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2217     ) -> "Dataset":
2218         """Broadcast this DataArray against another Dataset or DataArray.
2219         This is equivalent to xr.broadcast(other, self)[1]
2220 
2221         Parameters
2222         ----------
2223         other : Dataset or DataArray
2224             Object against which to broadcast this array.
2225         exclude : iterable of hashable, optional
2226             Dimensions that must not be broadcasted
2227 
2228         """
2229         if exclude is None:
2230             exclude = set()
2231         else:
2232             exclude = set(exclude)
2233         args = align(other, self, join="outer", copy=False, exclude=exclude)
2234 
2235         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2236 
2237         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2238 
2239     def reindex_like(
2240         self,
2241         other: Union["Dataset", "DataArray"],
2242         method: str = None,
2243         tolerance: Number = None,
2244         copy: bool = True,
2245         fill_value: Any = dtypes.NA,
2246     ) -> "Dataset":
2247         """Conform this object onto the indexes of another object, filling in
2248         missing values with ``fill_value``. The default fill value is NaN.
2249 
2250         Parameters
2251         ----------
2252         other : Dataset or DataArray
2253             Object with an 'indexes' attribute giving a mapping from dimension
2254             names to pandas.Index objects, which provides coordinates upon
2255             which to index the variables in this dataset. The indexes on this
2256             other object need not be the same as the indexes on this
2257             dataset. Any mis-matched index values will be filled in with
2258             NaN, and any mis-matched dimension names will simply be ignored.
2259         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2260             Method to use for filling index values from other not found in this
2261             dataset:
2262 
2263             * None (default): don't fill gaps
2264             * pad / ffill: propagate last valid index value forward
2265             * backfill / bfill: propagate next valid index value backward
2266             * nearest: use nearest valid index value
2267         tolerance : optional
2268             Maximum distance between original and new labels for inexact
2269             matches. The values of the index at the matching locations must
2270             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2271         copy : bool, optional
2272             If ``copy=True``, data in the return value is always copied. If
2273             ``copy=False`` and reindexing is unnecessary, or can be performed
2274             with only slice operations, then the output may share memory with
2275             the input. In either case, a new xarray object is always returned.
2276         fill_value : scalar, optional
2277             Value to use for newly missing values
2278 
2279         Returns
2280         -------
2281         reindexed : Dataset
2282             Another dataset, with this dataset's data but coordinates from the
2283             other object.
2284 
2285         See Also
2286         --------
2287         Dataset.reindex
2288         align
2289         """
2290         indexers = alignment.reindex_like_indexers(self, other)
2291         return self.reindex(
2292             indexers=indexers,
2293             method=method,
2294             copy=copy,
2295             fill_value=fill_value,
2296             tolerance=tolerance,
2297         )
2298 
2299     def reindex(
2300         self,
2301         indexers: Mapping[Hashable, Any] = None,
2302         method: str = None,
2303         tolerance: Number = None,
2304         copy: bool = True,
2305         fill_value: Any = dtypes.NA,
2306         **indexers_kwargs: Any,
2307     ) -> "Dataset":
2308         """Conform this object onto a new set of indexes, filling in
2309         missing values with ``fill_value``. The default fill value is NaN.
2310 
2311         Parameters
2312         ----------
2313         indexers : dict. optional
2314             Dictionary with keys given by dimension names and values given by
2315             arrays of coordinates tick labels. Any mis-matched coordinate
2316             values will be filled in with NaN, and any mis-matched dimension
2317             names will simply be ignored.
2318             One of indexers or indexers_kwargs must be provided.
2319         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
2320             Method to use for filling index values in ``indexers`` not found in
2321             this dataset:
2322 
2323             * None (default): don't fill gaps
2324             * pad / ffill: propagate last valid index value forward
2325             * backfill / bfill: propagate next valid index value backward
2326             * nearest: use nearest valid index value
2327         tolerance : optional
2328             Maximum distance between original and new labels for inexact
2329             matches. The values of the index at the matching locations must
2330             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2331         copy : bool, optional
2332             If ``copy=True``, data in the return value is always copied. If
2333             ``copy=False`` and reindexing is unnecessary, or can be performed
2334             with only slice operations, then the output may share memory with
2335             the input. In either case, a new xarray object is always returned.
2336         fill_value : scalar, optional
2337             Value to use for newly missing values
2338         sparse: use sparse-array. By default, False
2339         **indexers_kwargs : {dim: indexer, ...}, optional
2340             Keyword arguments in the same form as ``indexers``.
2341             One of indexers or indexers_kwargs must be provided.
2342 
2343         Returns
2344         -------
2345         reindexed : Dataset
2346             Another dataset, with this dataset's data but replaced coordinates.
2347 
2348         See Also
2349         --------
2350         Dataset.reindex_like
2351         align
2352         pandas.Index.get_indexer
2353 
2354         Examples
2355         --------
2356 
2357         Create a dataset with some fictional data.
2358 
2359         >>> import xarray as xr
2360         >>> import pandas as pd
2361         >>> x = xr.Dataset(
2362         ...     {
2363         ...         "temperature": ("station", 20 * np.random.rand(4)),
2364         ...         "pressure": ("station", 500 * np.random.rand(4)),
2365         ...     },
2366         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2367         ... )
2368         >>> x
2369         <xarray.Dataset>
2370         Dimensions:      (station: 4)
2371         Coordinates:
2372         * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2373         Data variables:
2374             temperature  (station) float64 18.84 14.59 19.22 17.16
2375             pressure     (station) float64 324.1 194.3 122.8 244.3
2376         >>> x.indexes
2377         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2378 
2379         Create a new index and reindex the dataset. By default values in the new index that
2380         do not have corresponding records in the dataset are assigned `NaN`.
2381 
2382         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2383         >>> x.reindex({"station": new_index})
2384         <xarray.Dataset>
2385         Dimensions:      (station: 4)
2386         Coordinates:
2387         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2388         Data variables:
2389             temperature  (station) float64 18.84 nan 19.22 nan
2390             pressure     (station) float64 324.1 nan 122.8 nan
2391 
2392         We can fill in the missing values by passing a value to the keyword `fill_value`.
2393 
2394         >>> x.reindex({"station": new_index}, fill_value=0)
2395         <xarray.Dataset>
2396         Dimensions:      (station: 4)
2397         Coordinates:
2398         * station      (station) object 'boston' 'austin' 'seattle' 'lincoln'
2399         Data variables:
2400             temperature  (station) float64 18.84 0.0 19.22 0.0
2401             pressure     (station) float64 324.1 0.0 122.8 0.0
2402 
2403         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2404         to the keyword method to fill the `NaN` values.
2405 
2406         >>> x.reindex({"station": new_index}, method="nearest")
2407         Traceback (most recent call last):
2408         ...
2409             raise ValueError('index must be monotonic increasing or decreasing')
2410         ValueError: index must be monotonic increasing or decreasing
2411 
2412         To further illustrate the filling functionality in reindex, we will create a
2413         dataset with a monotonically increasing index (for example, a sequence of dates).
2414 
2415         >>> x2 = xr.Dataset(
2416         ...     {
2417         ...         "temperature": (
2418         ...             "time",
2419         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2420         ...         ),
2421         ...         "pressure": ("time", 500 * np.random.rand(6)),
2422         ...     },
2423         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2424         ... )
2425         >>> x2
2426         <xarray.Dataset>
2427         Dimensions:      (time: 6)
2428         Coordinates:
2429         * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2430         Data variables:
2431             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2432             pressure     (time) float64 103.4 122.7 452.0 444.0 399.2 486.0
2433 
2434         Suppose we decide to expand the dataset to cover a wider date range.
2435 
2436         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2437         >>> x2.reindex({"time": time_index2})
2438         <xarray.Dataset>
2439         Dimensions:      (time: 10)
2440         Coordinates:
2441         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2442         Data variables:
2443             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2444             pressure     (time) float64 nan nan nan 103.4 ... 444.0 399.2 486.0 nan
2445 
2446         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2447         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2448 
2449         For example, to back-propagate the last valid value to fill the `NaN` values,
2450         pass `bfill` as an argument to the `method` keyword.
2451 
2452         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2453         >>> x3
2454         <xarray.Dataset>
2455         Dimensions:      (time: 10)
2456         Coordinates:
2457         * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2458         Data variables:
2459             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2460             pressure     (time) float64 103.4 103.4 103.4 103.4 ... 399.2 486.0 nan
2461 
2462         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2463         will not be filled by any of the value propagation schemes.
2464 
2465         >>> x2.where(x2.temperature.isnull(), drop=True)
2466         <xarray.Dataset>
2467         Dimensions:      (time: 1)
2468         Coordinates:
2469         * time         (time) datetime64[ns] 2019-01-03
2470         Data variables:
2471             temperature  (time) float64 nan
2472             pressure     (time) float64 452.0
2473         >>> x3.where(x3.temperature.isnull(), drop=True)
2474         <xarray.Dataset>
2475         Dimensions:      (time: 2)
2476         Coordinates:
2477         * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2478         Data variables:
2479             temperature  (time) float64 nan nan
2480             pressure     (time) float64 452.0 nan
2481 
2482         This is because filling while reindexing does not look at dataset values, but only compares
2483         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2484         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2485 
2486         """
2487         return self._reindex(
2488             indexers,
2489             method,
2490             tolerance,
2491             copy,
2492             fill_value,
2493             sparse=False,
2494             **indexers_kwargs,
2495         )
2496 
2497     def _reindex(
2498         self,
2499         indexers: Mapping[Hashable, Any] = None,
2500         method: str = None,
2501         tolerance: Number = None,
2502         copy: bool = True,
2503         fill_value: Any = dtypes.NA,
2504         sparse: bool = False,
2505         **indexers_kwargs: Any,
2506     ) -> "Dataset":
2507         """
2508         same to _reindex but support sparse option
2509         """
2510         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2511 
2512         bad_dims = [d for d in indexers if d not in self.dims]
2513         if bad_dims:
2514             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2515 
2516         variables, indexes = alignment.reindex_variables(
2517             self.variables,
2518             self.sizes,
2519             self.indexes,
2520             indexers,
2521             method,
2522             tolerance,
2523             copy=copy,
2524             fill_value=fill_value,
2525             sparse=sparse,
2526         )
2527         coord_names = set(self._coord_names)
2528         coord_names.update(indexers)
2529         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2530 
2531     def interp(
2532         self,
2533         coords: Mapping[Hashable, Any] = None,
2534         method: str = "linear",
2535         assume_sorted: bool = False,
2536         kwargs: Mapping[str, Any] = None,
2537         **coords_kwargs: Any,
2538     ) -> "Dataset":
2539         """ Multidimensional interpolation of Dataset.
2540 
2541         Parameters
2542         ----------
2543         coords : dict, optional
2544             Mapping from dimension names to the new coordinates.
2545             New coordinate can be a scalar, array-like or DataArray.
2546             If DataArrays are passed as new coordates, their dimensions are
2547             used for the broadcasting.
2548         method: string, optional.
2549             {'linear', 'nearest'} for multidimensional array,
2550             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2551             for 1-dimensional array. 'linear' is used by default.
2552         assume_sorted: boolean, optional
2553             If False, values of coordinates that are interpolated over can be
2554             in any order and they are sorted first. If True, interpolated
2555             coordinates are assumed to be an array of monotonically increasing
2556             values.
2557         kwargs: dictionary, optional
2558             Additional keyword passed to scipy's interpolator.
2559         **coords_kwargs : {dim: coordinate, ...}, optional
2560             The keyword arguments form of ``coords``.
2561             One of coords or coords_kwargs must be provided.
2562 
2563         Returns
2564         -------
2565         interpolated: xr.Dataset
2566             New dataset on the new coordinates.
2567 
2568         Notes
2569         -----
2570         scipy is required.
2571 
2572         See Also
2573         --------
2574         scipy.interpolate.interp1d
2575         scipy.interpolate.interpn
2576         """
2577         from . import missing
2578 
2579         if kwargs is None:
2580             kwargs = {}
2581 
2582         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2583         indexers = dict(self._validate_interp_indexers(coords))
2584 
2585         if coords:
2586             # This avoids broadcasting over coordinates that are both in
2587             # the original array AND in the indexing array. It essentially
2588             # forces interpolation along the shared coordinates.
2589             sdims = (
2590                 set(self.dims)
2591                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2592                 .difference(coords.keys())
2593             )
2594             indexers.update({d: self.variables[d] for d in sdims})
2595 
2596         obj = self if assume_sorted else self.sortby([k for k in coords])
2597 
2598         def maybe_variable(obj, k):
2599             # workaround to get variable for dimension without coordinate.
2600             try:
2601                 return obj._variables[k]
2602             except KeyError:
2603                 return as_variable((k, range(obj.dims[k])))
2604 
2605         def _validate_interp_indexer(x, new_x):
2606             # In the case of datetimes, the restrictions placed on indexers
2607             # used with interp are stronger than those which are placed on
2608             # isel, so we need an additional check after _validate_indexers.
2609             if _contains_datetime_like_objects(
2610                 x
2611             ) and not _contains_datetime_like_objects(new_x):
2612                 raise TypeError(
2613                     "When interpolating over a datetime-like "
2614                     "coordinate, the coordinates to "
2615                     "interpolate to must be either datetime "
2616                     "strings or datetimes. "
2617                     "Instead got\n{}".format(new_x)
2618                 )
2619             return x, new_x
2620 
2621         variables: Dict[Hashable, Variable] = {}
2622         for name, var in obj._variables.items():
2623             if name in indexers:
2624                 continue
2625 
2626             if var.dtype.kind in "uifc":
2627                 var_indexers = {
2628                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2629                     for k, v in indexers.items()
2630                     if k in var.dims
2631                 }
2632                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2633             elif all(d not in indexers for d in var.dims):
2634                 # keep unrelated object array
2635                 variables[name] = var
2636 
2637         coord_names = obj._coord_names & variables.keys()
2638         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2639         selected = self._replace_with_new_dims(
2640             variables.copy(), coord_names, indexes=indexes
2641         )
2642 
2643         # attach indexer as coordinate
2644         variables.update(indexers)
2645         for k, v in indexers.items():
2646             assert isinstance(v, Variable)
2647             if v.dims == (k,):
2648                 indexes[k] = v.to_index()
2649 
2650         # Extract coordinates from indexers
2651         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2652         variables.update(coord_vars)
2653         indexes.update(new_indexes)
2654 
2655         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2656         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2657 
2658     def interp_like(
2659         self,
2660         other: Union["Dataset", "DataArray"],
2661         method: str = "linear",
2662         assume_sorted: bool = False,
2663         kwargs: Mapping[str, Any] = None,
2664     ) -> "Dataset":
2665         """Interpolate this object onto the coordinates of another object,
2666         filling the out of range values with NaN.
2667 
2668         Parameters
2669         ----------
2670         other : Dataset or DataArray
2671             Object with an 'indexes' attribute giving a mapping from dimension
2672             names to an 1d array-like, which provides coordinates upon
2673             which to index the variables in this dataset.
2674         method: string, optional.
2675             {'linear', 'nearest'} for multidimensional array,
2676             {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'}
2677             for 1-dimensional array. 'linear' is used by default.
2678         assume_sorted: boolean, optional
2679             If False, values of coordinates that are interpolated over can be
2680             in any order and they are sorted first. If True, interpolated
2681             coordinates are assumed to be an array of monotonically increasing
2682             values.
2683         kwargs: dictionary, optional
2684             Additional keyword passed to scipy's interpolator.
2685 
2686         Returns
2687         -------
2688         interpolated: xr.Dataset
2689             Another dataset by interpolating this dataset's data along the
2690             coordinates of the other object.
2691 
2692         Notes
2693         -----
2694         scipy is required.
2695         If the dataset has object-type coordinates, reindex is used for these
2696         coordinates instead of the interpolation.
2697 
2698         See Also
2699         --------
2700         Dataset.interp
2701         Dataset.reindex_like
2702         """
2703         if kwargs is None:
2704             kwargs = {}
2705         coords = alignment.reindex_like_indexers(self, other)
2706 
2707         numeric_coords: Dict[Hashable, pd.Index] = {}
2708         object_coords: Dict[Hashable, pd.Index] = {}
2709         for k, v in coords.items():
2710             if v.dtype.kind in "uifcMm":
2711                 numeric_coords[k] = v
2712             else:
2713                 object_coords[k] = v
2714 
2715         ds = self
2716         if object_coords:
2717             # We do not support interpolation along object coordinate.
2718             # reindex instead.
2719             ds = self.reindex(object_coords)
2720         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2721 
2722     # Helper methods for rename()
2723     def _rename_vars(self, name_dict, dims_dict):
2724         variables = {}
2725         coord_names = set()
2726         for k, v in self.variables.items():
2727             var = v.copy(deep=False)
2728             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2729             name = name_dict.get(k, k)
2730             if name in variables:
2731                 raise ValueError(f"the new name {name!r} conflicts")
2732             variables[name] = var
2733             if k in self._coord_names:
2734                 coord_names.add(name)
2735         return variables, coord_names
2736 
2737     def _rename_dims(self, name_dict):
2738         return {name_dict.get(k, k): v for k, v in self.dims.items()}
2739 
2740     def _rename_indexes(self, name_dict, dims_set):
2741         if self._indexes is None:
2742             return None
2743         indexes = {}
2744         for k, v in self.indexes.items():
2745             new_name = name_dict.get(k, k)
2746             if new_name not in dims_set:
2747                 continue
2748             if isinstance(v, pd.MultiIndex):
2749                 new_names = [name_dict.get(k, k) for k in v.names]
2750                 index = v.rename(names=new_names)
2751             else:
2752                 index = v.rename(new_name)
2753             indexes[new_name] = index
2754         return indexes
2755 
2756     def _rename_all(self, name_dict, dims_dict):
2757         variables, coord_names = self._rename_vars(name_dict, dims_dict)
2758         dims = self._rename_dims(dims_dict)
2759         indexes = self._rename_indexes(name_dict, dims.keys())
2760         return variables, coord_names, dims, indexes
2761 
2762     def rename(
2763         self,
2764         name_dict: Mapping[Hashable, Hashable] = None,
2765         inplace: bool = None,
2766         **names: Hashable,
2767     ) -> "Dataset":
2768         """Returns a new object with renamed variables and dimensions.
2769 
2770         Parameters
2771         ----------
2772         name_dict : dict-like, optional
2773             Dictionary whose keys are current variable or dimension names and
2774             whose values are the desired names.
2775         **names, optional
2776             Keyword form of ``name_dict``.
2777             One of name_dict or names must be provided.
2778 
2779         Returns
2780         -------
2781         renamed : Dataset
2782             Dataset with renamed variables and dimensions.
2783 
2784         See Also
2785         --------
2786         Dataset.swap_dims
2787         Dataset.rename_vars
2788         Dataset.rename_dims
2789         DataArray.rename
2790         """
2791         _check_inplace(inplace)
2792         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
2793         for k in name_dict.keys():
2794             if k not in self and k not in self.dims:
2795                 raise ValueError(
2796                     "cannot rename %r because it is not a "
2797                     "variable or dimension in this dataset" % k
2798                 )
2799 
2800         variables, coord_names, dims, indexes = self._rename_all(
2801             name_dict=name_dict, dims_dict=name_dict
2802         )
2803         assert_unique_multiindex_level_names(variables)
2804         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2805 
2806     def rename_dims(
2807         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
2808     ) -> "Dataset":
2809         """Returns a new object with renamed dimensions only.
2810 
2811         Parameters
2812         ----------
2813         dims_dict : dict-like, optional
2814             Dictionary whose keys are current dimension names and
2815             whose values are the desired names. The desired names must
2816             not be the name of an existing dimension or Variable in the Dataset.
2817         **dims, optional
2818             Keyword form of ``dims_dict``.
2819             One of dims_dict or dims must be provided.
2820 
2821         Returns
2822         -------
2823         renamed : Dataset
2824             Dataset with renamed dimensions.
2825 
2826         See Also
2827         --------
2828         Dataset.swap_dims
2829         Dataset.rename
2830         Dataset.rename_vars
2831         DataArray.rename
2832         """
2833         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
2834         for k, v in dims_dict.items():
2835             if k not in self.dims:
2836                 raise ValueError(
2837                     "cannot rename %r because it is not a "
2838                     "dimension in this dataset" % k
2839                 )
2840             if v in self.dims or v in self:
2841                 raise ValueError(
2842                     f"Cannot rename {k} to {v} because {v} already exists. "
2843                     "Try using swap_dims instead."
2844                 )
2845 
2846         variables, coord_names, sizes, indexes = self._rename_all(
2847             name_dict={}, dims_dict=dims_dict
2848         )
2849         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
2850 
2851     def rename_vars(
2852         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
2853     ) -> "Dataset":
2854         """Returns a new object with renamed variables including coordinates
2855 
2856         Parameters
2857         ----------
2858         name_dict : dict-like, optional
2859             Dictionary whose keys are current variable or coordinate names and
2860             whose values are the desired names.
2861         **names, optional
2862             Keyword form of ``name_dict``.
2863             One of name_dict or names must be provided.
2864 
2865         Returns
2866         -------
2867         renamed : Dataset
2868             Dataset with renamed variables including coordinates
2869 
2870         See Also
2871         --------
2872         Dataset.swap_dims
2873         Dataset.rename
2874         Dataset.rename_dims
2875         DataArray.rename
2876         """
2877         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
2878         for k in name_dict:
2879             if k not in self:
2880                 raise ValueError(
2881                     "cannot rename %r because it is not a "
2882                     "variable or coordinate in this dataset" % k
2883                 )
2884         variables, coord_names, dims, indexes = self._rename_all(
2885             name_dict=name_dict, dims_dict={}
2886         )
2887         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
2888 
2889     def swap_dims(
2890         self, dims_dict: Mapping[Hashable, Hashable], inplace: bool = None
2891     ) -> "Dataset":
2892         """Returns a new object with swapped dimensions.
2893 
2894         Parameters
2895         ----------
2896         dims_dict : dict-like
2897             Dictionary whose keys are current dimension names and whose values
2898             are new names.
2899 
2900         Returns
2901         -------
2902         swapped : Dataset
2903             Dataset with swapped dimensions.
2904 
2905         Examples
2906         --------
2907         >>> ds = xr.Dataset(
2908         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
2909         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2910         ... )
2911         >>> ds
2912         <xarray.Dataset>
2913         Dimensions:  (x: 2)
2914         Coordinates:
2915           * x        (x) <U1 'a' 'b'
2916             y        (x) int64 0 1
2917         Data variables:
2918             a        (x) int64 5 7
2919             b        (x) float64 0.1 2.4
2920 
2921         >>> ds.swap_dims({"x": "y"})
2922         <xarray.Dataset>
2923         Dimensions:  (y: 2)
2924         Coordinates:
2925             x        (y) <U1 'a' 'b'
2926           * y        (y) int64 0 1
2927         Data variables:
2928             a        (y) int64 5 7
2929             b        (y) float64 0.1 2.4
2930 
2931         >>> ds.swap_dims({"x": "z"})
2932         <xarray.Dataset>
2933         Dimensions:  (z: 2)
2934         Coordinates:
2935             x        (z) <U1 'a' 'b'
2936             y        (z) int64 0 1
2937         Dimensions without coordinates: z
2938         Data variables:
2939             a        (z) int64 5 7
2940             b        (z) float64 0.1 2.4
2941 
2942         See Also
2943         --------
2944 
2945         Dataset.rename
2946         DataArray.swap_dims
2947         """
2948         # TODO: deprecate this method in favor of a (less confusing)
2949         # rename_dims() method that only renames dimensions.
2950         _check_inplace(inplace)
2951         for k, v in dims_dict.items():
2952             if k not in self.dims:
2953                 raise ValueError(
2954                     "cannot swap from dimension %r because it is "
2955                     "not an existing dimension" % k
2956                 )
2957             if v in self.variables and self.variables[v].dims != (k,):
2958                 raise ValueError(
2959                     "replacement dimension %r is not a 1D "
2960                     "variable along the old dimension %r" % (v, k)
2961                 )
2962 
2963         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
2964 
2965         coord_names = self._coord_names.copy()
2966         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
2967 
2968         variables: Dict[Hashable, Variable] = {}
2969         indexes: Dict[Hashable, pd.Index] = {}
2970         for k, v in self.variables.items():
2971             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2972             if k in result_dims:
2973                 var = v.to_index_variable()
2974                 if k in self.indexes:
2975                     indexes[k] = self.indexes[k]
2976                 else:
2977                     new_index = var.to_index()
2978                     if new_index.nlevels == 1:
2979                         # make sure index name matches dimension name
2980                         new_index = new_index.rename(k)
2981                     indexes[k] = new_index
2982             else:
2983                 var = v.to_base_variable()
2984             var.dims = dims
2985             variables[k] = var
2986 
2987         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2988 
2989     def expand_dims(
2990         self,
2991         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
2992         axis: Union[None, int, Sequence[int]] = None,
2993         **dim_kwargs: Any,
2994     ) -> "Dataset":
2995         """Return a new object with an additional axis (or axes) inserted at
2996         the corresponding position in the array shape.  The new object is a
2997         view into the underlying array, not a copy.
2998 
2999         If dim is already a scalar coordinate, it will be promoted to a 1D
3000         coordinate consisting of a single value.
3001 
3002         Parameters
3003         ----------
3004         dim : hashable, sequence of hashable, mapping, or None
3005             Dimensions to include on the new variable. If provided as hashable
3006             or sequence of hashable, then dimensions are inserted with length
3007             1. If provided as a mapping, then the keys are the new dimensions
3008             and the values are either integers (giving the length of the new
3009             dimensions) or array-like (giving the coordinates of the new
3010             dimensions).
3011         axis : integer, sequence of integers, or None
3012             Axis position(s) where new axis is to be inserted (position(s) on
3013             the result array). If a list (or tuple) of integers is passed,
3014             multiple axes are inserted. In this case, dim arguments should be
3015             same length list. If axis=None is passed, all the axes will be
3016             inserted to the start of the result array.
3017         **dim_kwargs : int or sequence/ndarray
3018             The keywords are arbitrary dimensions being inserted and the values
3019             are either the lengths of the new dims (if int is given), or their
3020             coordinates. Note, this is an alternative to passing a dict to the
3021             dim kwarg and will only be used if dim is None.
3022 
3023         Returns
3024         -------
3025         expanded : same type as caller
3026             This object, but with an additional dimension(s).
3027         """
3028         if dim is None:
3029             pass
3030         elif isinstance(dim, Mapping):
3031             # We're later going to modify dim in place; don't tamper with
3032             # the input
3033             dim = dict(dim)
3034         elif isinstance(dim, int):
3035             raise TypeError(
3036                 "dim should be hashable or sequence of hashables or mapping"
3037             )
3038         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3039             dim = {dim: 1}
3040         elif isinstance(dim, Sequence):
3041             if len(dim) != len(set(dim)):
3042                 raise ValueError("dims should not contain duplicate values.")
3043             dim = {d: 1 for d in dim}
3044 
3045         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3046         assert isinstance(dim, MutableMapping)
3047 
3048         if axis is None:
3049             axis = list(range(len(dim)))
3050         elif not isinstance(axis, Sequence):
3051             axis = [axis]
3052 
3053         if len(dim) != len(axis):
3054             raise ValueError("lengths of dim and axis should be identical.")
3055         for d in dim:
3056             if d in self.dims:
3057                 raise ValueError(f"Dimension {d} already exists.")
3058             if d in self._variables and not utils.is_scalar(self._variables[d]):
3059                 raise ValueError(
3060                     "{dim} already exists as coordinate or"
3061                     " variable name.".format(dim=d)
3062                 )
3063 
3064         variables: Dict[Hashable, Variable] = {}
3065         coord_names = self._coord_names.copy()
3066         # If dim is a dict, then ensure that the values are either integers
3067         # or iterables.
3068         for k, v in dim.items():
3069             if hasattr(v, "__iter__"):
3070                 # If the value for the new dimension is an iterable, then
3071                 # save the coordinates to the variables dict, and set the
3072                 # value within the dim dict to the length of the iterable
3073                 # for later use.
3074                 variables[k] = xr.IndexVariable((k,), v)
3075                 coord_names.add(k)
3076                 dim[k] = variables[k].size
3077             elif isinstance(v, int):
3078                 pass  # Do nothing if the dimensions value is just an int
3079             else:
3080                 raise TypeError(
3081                     "The value of new dimension {k} must be "
3082                     "an iterable or an int".format(k=k)
3083                 )
3084 
3085         for k, v in self._variables.items():
3086             if k not in dim:
3087                 if k in coord_names:  # Do not change coordinates
3088                     variables[k] = v
3089                 else:
3090                     result_ndim = len(v.dims) + len(axis)
3091                     for a in axis:
3092                         if a < -result_ndim or result_ndim - 1 < a:
3093                             raise IndexError(
3094                                 f"Axis {a} of variable {k} is out of bounds of the "
3095                                 f"expanded dimension size {result_ndim}"
3096                             )
3097 
3098                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3099                     if len(axis_pos) != len(set(axis_pos)):
3100                         raise ValueError("axis should not contain duplicate values")
3101                     # We need to sort them to make sure `axis` equals to the
3102                     # axis positions of the result array.
3103                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3104 
3105                     all_dims = list(zip(v.dims, v.shape))
3106                     for d, c in zip_axis_dim:
3107                         all_dims.insert(d, c)
3108                     variables[k] = v.set_dims(dict(all_dims))
3109             else:
3110                 # If dims includes a label of a non-dimension coordinate,
3111                 # it will be promoted to a 1D coordinate with a single value.
3112                 variables[k] = v.set_dims(k).to_index_variable()
3113 
3114         new_dims = self._dims.copy()
3115         new_dims.update(dim)
3116 
3117         return self._replace_vars_and_dims(
3118             variables, dims=new_dims, coord_names=coord_names
3119         )
3120 
3121     def set_index(
3122         self,
3123         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3124         append: bool = False,
3125         inplace: bool = None,
3126         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3127     ) -> "Dataset":
3128         """Set Dataset (multi-)indexes using one or more existing coordinates
3129         or variables.
3130 
3131         Parameters
3132         ----------
3133         indexes : {dim: index, ...}
3134             Mapping from names matching dimensions and values given
3135             by (lists of) the names of existing coordinates or variables to set
3136             as new (multi-)index.
3137         append : bool, optional
3138             If True, append the supplied index(es) to the existing index(es).
3139             Otherwise replace the existing index(es) (default).
3140         **indexes_kwargs: optional
3141             The keyword arguments form of ``indexes``.
3142             One of indexes or indexes_kwargs must be provided.
3143 
3144         Returns
3145         -------
3146         obj : Dataset
3147             Another dataset, with this dataset's data but replaced coordinates.
3148 
3149         Examples
3150         --------
3151         >>> arr = xr.DataArray(
3152         ...     data=np.ones((2, 3)),
3153         ...     dims=["x", "y"],
3154         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3155         ... )
3156         >>> ds = xr.Dataset({"v": arr})
3157         >>> ds
3158         <xarray.Dataset>
3159         Dimensions:  (x: 2, y: 3)
3160         Coordinates:
3161           * x        (x) int64 0 1
3162           * y        (y) int64 0 1 2
3163             a        (x) int64 3 4
3164         Data variables:
3165             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3166         >>> ds.set_index(x="a")
3167         <xarray.Dataset>
3168         Dimensions:  (x: 2, y: 3)
3169         Coordinates:
3170           * x        (x) int64 3 4
3171           * y        (y) int64 0 1 2
3172         Data variables:
3173             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3174 
3175         See Also
3176         --------
3177         Dataset.reset_index
3178         Dataset.swap_dims
3179         """
3180         _check_inplace(inplace)
3181         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3182         variables, coord_names = merge_indexes(
3183             indexes, self._variables, self._coord_names, append=append
3184         )
3185         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3186 
3187     def reset_index(
3188         self,
3189         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3190         drop: bool = False,
3191         inplace: bool = None,
3192     ) -> "Dataset":
3193         """Reset the specified index(es) or multi-index level(s).
3194 
3195         Parameters
3196         ----------
3197         dims_or_levels : str or list
3198             Name(s) of the dimension(s) and/or multi-index level(s) that will
3199             be reset.
3200         drop : bool, optional
3201             If True, remove the specified indexes and/or multi-index levels
3202             instead of extracting them as new coordinates (default: False).
3203 
3204         Returns
3205         -------
3206         obj : Dataset
3207             Another dataset, with this dataset's data but replaced coordinates.
3208 
3209         See Also
3210         --------
3211         Dataset.set_index
3212         """
3213         _check_inplace(inplace)
3214         variables, coord_names = split_indexes(
3215             dims_or_levels,
3216             self._variables,
3217             self._coord_names,
3218             cast(Mapping[Hashable, Hashable], self._level_coords),
3219             drop=drop,
3220         )
3221         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3222 
3223     def reorder_levels(
3224         self,
3225         dim_order: Mapping[Hashable, Sequence[int]] = None,
3226         inplace: bool = None,
3227         **dim_order_kwargs: Sequence[int],
3228     ) -> "Dataset":
3229         """Rearrange index levels using input order.
3230 
3231         Parameters
3232         ----------
3233         dim_order : optional
3234             Mapping from names matching dimensions and values given
3235             by lists representing new level orders. Every given dimension
3236             must have a multi-index.
3237         **dim_order_kwargs: optional
3238             The keyword arguments form of ``dim_order``.
3239             One of dim_order or dim_order_kwargs must be provided.
3240 
3241         Returns
3242         -------
3243         obj : Dataset
3244             Another dataset, with this dataset's data but replaced
3245             coordinates.
3246         """
3247         _check_inplace(inplace)
3248         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3249         variables = self._variables.copy()
3250         indexes = dict(self.indexes)
3251         for dim, order in dim_order.items():
3252             coord = self._variables[dim]
3253             index = self.indexes[dim]
3254             if not isinstance(index, pd.MultiIndex):
3255                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3256             new_index = index.reorder_levels(order)
3257             variables[dim] = IndexVariable(coord.dims, new_index)
3258             indexes[dim] = new_index
3259 
3260         return self._replace(variables, indexes=indexes)
3261 
3262     def _stack_once(self, dims, new_dim):
3263         variables = {}
3264         for name, var in self.variables.items():
3265             if name not in dims:
3266                 if any(d in var.dims for d in dims):
3267                     add_dims = [d for d in dims if d not in var.dims]
3268                     vdims = list(var.dims) + add_dims
3269                     shape = [self.dims[d] for d in vdims]
3270                     exp_var = var.set_dims(vdims, shape)
3271                     stacked_var = exp_var.stack(**{new_dim: dims})
3272                     variables[name] = stacked_var
3273                 else:
3274                     variables[name] = var.copy(deep=False)
3275 
3276         # consider dropping levels that are unused?
3277         levels = [self.get_index(dim) for dim in dims]
3278         idx = utils.multiindex_from_product_levels(levels, names=dims)
3279         variables[new_dim] = IndexVariable(new_dim, idx)
3280 
3281         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3282 
3283         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3284         indexes[new_dim] = idx
3285 
3286         return self._replace_with_new_dims(
3287             variables, coord_names=coord_names, indexes=indexes
3288         )
3289 
3290     def stack(
3291         self,
3292         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3293         **dimensions_kwargs: Sequence[Hashable],
3294     ) -> "Dataset":
3295         """
3296         Stack any number of existing dimensions into a single new dimension.
3297 
3298         New dimensions will be added at the end, and the corresponding
3299         coordinate variables will be combined into a MultiIndex.
3300 
3301         Parameters
3302         ----------
3303         dimensions : Mapping of the form new_name=(dim1, dim2, ...)
3304             Names of new dimensions, and the existing dimensions that they
3305             replace.
3306         **dimensions_kwargs:
3307             The keyword arguments form of ``dimensions``.
3308             One of dimensions or dimensions_kwargs must be provided.
3309 
3310         Returns
3311         -------
3312         stacked : Dataset
3313             Dataset with stacked data.
3314 
3315         See also
3316         --------
3317         Dataset.unstack
3318         """
3319         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3320         result = self
3321         for new_dim, dims in dimensions.items():
3322             result = result._stack_once(dims, new_dim)
3323         return result
3324 
3325     def to_stacked_array(
3326         self,
3327         new_dim: Hashable,
3328         sample_dims: Sequence[Hashable],
3329         variable_dim: str = "variable",
3330         name: Hashable = None,
3331     ) -> "DataArray":
3332         """Combine variables of differing dimensionality into a DataArray
3333         without broadcasting.
3334 
3335         This method is similar to Dataset.to_array but does not broadcast the
3336         variables.
3337 
3338         Parameters
3339         ----------
3340         new_dim : Hashable
3341             Name of the new stacked coordinate
3342         sample_dims : Sequence[Hashable]
3343             Dimensions that **will not** be stacked. Each array in the dataset
3344             must share these dimensions. For machine learning applications,
3345             these define the dimensions over which samples are drawn.
3346         variable_dim : str, optional
3347             Name of the level in the stacked coordinate which corresponds to
3348             the variables.
3349         name : str, optional
3350             Name of the new data array.
3351 
3352         Returns
3353         -------
3354         stacked : DataArray
3355             DataArray with the specified dimensions and data variables
3356             stacked together. The stacked coordinate is named ``new_dim``
3357             and represented by a MultiIndex object with a level containing the
3358             data variable names. The name of this level is controlled using
3359             the ``variable_dim`` argument.
3360 
3361         See Also
3362         --------
3363         Dataset.to_array
3364         Dataset.stack
3365         DataArray.to_unstacked_dataset
3366 
3367         Examples
3368         --------
3369         >>> data = xr.Dataset(
3370         ...     data_vars={
3371         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3372         ...         "b": ("x", [6, 7]),
3373         ...     },
3374         ...     coords={"y": ["u", "v", "w"]},
3375         ... )
3376 
3377         >>> data
3378         <xarray.Dataset>
3379         Dimensions:  (x: 2, y: 3)
3380         Coordinates:
3381         * y        (y) <U1 'u' 'v' 'w'
3382         Dimensions without coordinates: x
3383         Data variables:
3384             a        (x, y) int64 0 1 2 3 4 5
3385             b        (x) int64 6 7
3386 
3387         >>> data.to_stacked_array("z", sample_dims=["x"])
3388         <xarray.DataArray (x: 2, z: 4)>
3389         array([[0, 1, 2, 6],
3390             [3, 4, 5, 7]])
3391         Coordinates:
3392         * z         (z) MultiIndex
3393         - variable  (z) object 'a' 'a' 'a' 'b'
3394         - y         (z) object 'u' 'v' 'w' nan
3395         Dimensions without coordinates: x
3396 
3397         """
3398         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3399 
3400         for variable in self:
3401             dims = self[variable].dims
3402             dims_include_sample_dims = set(sample_dims) <= set(dims)
3403             if not dims_include_sample_dims:
3404                 raise ValueError(
3405                     "All variables in the dataset must contain the "
3406                     "dimensions {}.".format(dims)
3407                 )
3408 
3409         def ensure_stackable(val):
3410             assign_coords = {variable_dim: val.name}
3411             for dim in stacking_dims:
3412                 if dim not in val.dims:
3413                     assign_coords[dim] = None
3414 
3415             expand_dims = set(stacking_dims).difference(set(val.dims))
3416             expand_dims.add(variable_dim)
3417             # must be list for .expand_dims
3418             expand_dims = list(expand_dims)
3419 
3420             return (
3421                 val.assign_coords(**assign_coords)
3422                 .expand_dims(expand_dims)
3423                 .stack({new_dim: (variable_dim,) + stacking_dims})
3424             )
3425 
3426         # concatenate the arrays
3427         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3428         data_array = xr.concat(stackable_vars, dim=new_dim)
3429 
3430         # coerce the levels of the MultiIndex to have the same type as the
3431         # input dimensions. This code is messy, so it might be better to just
3432         # input a dummy value for the singleton dimension.
3433         idx = data_array.indexes[new_dim]
3434         levels = [idx.levels[0]] + [
3435             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3436         ]
3437         new_idx = idx.set_levels(levels)
3438         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3439 
3440         if name is not None:
3441             data_array.name = name
3442 
3443         return data_array
3444 
3445     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> "Dataset":
3446         index = self.get_index(dim)
3447         index = remove_unused_levels_categories(index)
3448         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3449 
3450         # take a shortcut in case the MultiIndex was not modified.
3451         if index.equals(full_idx):
3452             obj = self
3453         else:
3454             obj = self._reindex(
3455                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3456             )
3457 
3458         new_dim_names = index.names
3459         new_dim_sizes = [lev.size for lev in index.levels]
3460 
3461         variables: Dict[Hashable, Variable] = {}
3462         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3463 
3464         for name, var in obj.variables.items():
3465             if name != dim:
3466                 if dim in var.dims:
3467                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3468                     variables[name] = var.unstack({dim: new_dims})
3469                 else:
3470                     variables[name] = var
3471 
3472         for name, lev in zip(new_dim_names, index.levels):
3473             variables[name] = IndexVariable(name, lev)
3474             indexes[name] = lev
3475 
3476         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3477 
3478         return self._replace_with_new_dims(
3479             variables, coord_names=coord_names, indexes=indexes
3480         )
3481 
3482     def unstack(
3483         self,
3484         dim: Union[Hashable, Iterable[Hashable]] = None,
3485         fill_value: Any = dtypes.NA,
3486         sparse: bool = False,
3487     ) -> "Dataset":
3488         """
3489         Unstack existing dimensions corresponding to MultiIndexes into
3490         multiple new dimensions.
3491 
3492         New dimensions will be added at the end.
3493 
3494         Parameters
3495         ----------
3496         dim : Hashable or iterable of Hashable, optional
3497             Dimension(s) over which to unstack. By default unstacks all
3498             MultiIndexes.
3499         fill_value: value to be filled. By default, np.nan
3500         sparse: use sparse-array if True
3501 
3502         Returns
3503         -------
3504         unstacked : Dataset
3505             Dataset with unstacked data.
3506 
3507         See also
3508         --------
3509         Dataset.stack
3510         """
3511         if dim is None:
3512             dims = [
3513                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3514             ]
3515         else:
3516             if isinstance(dim, str) or not isinstance(dim, Iterable):
3517                 dims = [dim]
3518             else:
3519                 dims = list(dim)
3520 
3521             missing_dims = [d for d in dims if d not in self.dims]
3522             if missing_dims:
3523                 raise ValueError(
3524                     "Dataset does not contain the dimensions: %s" % missing_dims
3525                 )
3526 
3527             non_multi_dims = [
3528                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3529             ]
3530             if non_multi_dims:
3531                 raise ValueError(
3532                     "cannot unstack dimensions that do not "
3533                     "have a MultiIndex: %s" % non_multi_dims
3534                 )
3535 
3536         result = self.copy(deep=False)
3537         for dim in dims:
3538             result = result._unstack_once(dim, fill_value, sparse)
3539         return result
3540 
3541     def update(self, other: "CoercibleMapping", inplace: bool = None) -> "Dataset":
3542         """Update this dataset's variables with those from another dataset.
3543 
3544         Parameters
3545         ----------
3546         other : Dataset or castable to Dataset
3547             Variables with which to update this dataset. One of:
3548 
3549             - Dataset
3550             - mapping {var name: DataArray}
3551             - mapping {var name: Variable}
3552             - mapping {var name: (dimension name, array-like)}
3553             - mapping {var name: (tuple of dimension names, array-like)}
3554 
3555 
3556         Returns
3557         -------
3558         updated : Dataset
3559             Updated dataset.
3560 
3561         Raises
3562         ------
3563         ValueError
3564             If any dimensions would have inconsistent sizes in the updated
3565             dataset.
3566         """
3567         _check_inplace(inplace)
3568         merge_result = dataset_update_method(self, other)
3569         return self._replace(inplace=True, **merge_result._asdict())
3570 
3571     def merge(
3572         self,
3573         other: Union["CoercibleMapping", "DataArray"],
3574         inplace: bool = None,
3575         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3576         compat: str = "no_conflicts",
3577         join: str = "outer",
3578         fill_value: Any = dtypes.NA,
3579     ) -> "Dataset":
3580         """Merge the arrays of two datasets into a single dataset.
3581 
3582         This method generally does not allow for overriding data, with the
3583         exception of attributes, which are ignored on the second dataset.
3584         Variables with the same name are checked for conflicts via the equals
3585         or identical methods.
3586 
3587         Parameters
3588         ----------
3589         other : Dataset or castable to Dataset
3590             Dataset or variables to merge with this dataset.
3591         overwrite_vars : Hashable or iterable of Hashable, optional
3592             If provided, update variables of these name(s) without checking for
3593             conflicts in this dataset.
3594         compat : {'broadcast_equals', 'equals', 'identical',
3595                   'no_conflicts'}, optional
3596             String indicating how to compare variables of the same name for
3597             potential conflicts:
3598 
3599             - 'broadcast_equals': all values must be equal when variables are
3600               broadcast against each other to ensure common dimensions.
3601             - 'equals': all values and dimensions must be the same.
3602             - 'identical': all values, dimensions and attributes must be the
3603               same.
3604             - 'no_conflicts': only values which are not null in both datasets
3605               must be equal. The returned dataset then contains the combination
3606               of all non-null values.
3607 
3608         join : {'outer', 'inner', 'left', 'right', 'exact'}, optional
3609             Method for joining ``self`` and ``other`` along shared dimensions:
3610 
3611             - 'outer': use the union of the indexes
3612             - 'inner': use the intersection of the indexes
3613             - 'left': use indexes from ``self``
3614             - 'right': use indexes from ``other``
3615             - 'exact': error instead of aligning non-equal indexes
3616         fill_value: scalar, optional
3617             Value to use for newly missing values
3618 
3619         Returns
3620         -------
3621         merged : Dataset
3622             Merged dataset.
3623 
3624         Raises
3625         ------
3626         MergeError
3627             If any variables conflict (see ``compat``).
3628         """
3629         _check_inplace(inplace)
3630         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3631         merge_result = dataset_merge_method(
3632             self,
3633             other,
3634             overwrite_vars=overwrite_vars,
3635             compat=compat,
3636             join=join,
3637             fill_value=fill_value,
3638         )
3639         return self._replace(**merge_result._asdict())
3640 
3641     def _assert_all_in_dataset(
3642         self, names: Iterable[Hashable], virtual_okay: bool = False
3643     ) -> None:
3644         bad_names = set(names) - set(self._variables)
3645         if virtual_okay:
3646             bad_names -= self.virtual_variables
3647         if bad_names:
3648             raise ValueError(
3649                 "One or more of the specified variables "
3650                 "cannot be found in this dataset"
3651             )
3652 
3653     def drop_vars(
3654         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3655     ) -> "Dataset":
3656         """Drop variables from this dataset.
3657 
3658         Parameters
3659         ----------
3660         names : hashable or iterable of hashables
3661             Name(s) of variables to drop.
3662         errors: {'raise', 'ignore'}, optional
3663             If 'raise' (default), raises a ValueError error if any of the variable
3664             passed are not in the dataset. If 'ignore', any given names that are in the
3665             dataset are dropped and no error is raised.
3666 
3667         Returns
3668         -------
3669         dropped : Dataset
3670 
3671         """
3672         # the Iterable check is required for mypy
3673         if is_scalar(names) or not isinstance(names, Iterable):
3674             names = {names}
3675         else:
3676             names = set(names)
3677         if errors == "raise":
3678             self._assert_all_in_dataset(names)
3679 
3680         variables = {k: v for k, v in self._variables.items() if k not in names}
3681         coord_names = {k for k in self._coord_names if k in variables}
3682         indexes = {k: v for k, v in self.indexes.items() if k not in names}
3683         return self._replace_with_new_dims(
3684             variables, coord_names=coord_names, indexes=indexes
3685         )
3686 
3687     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
3688         """Backward compatible method based on `drop_vars` and `drop_sel`
3689 
3690         Using either `drop_vars` or `drop_sel` is encouraged
3691 
3692         See Also
3693         --------
3694         Dataset.drop_vars
3695         Dataset.drop_sel
3696         """
3697         if errors not in ["raise", "ignore"]:
3698             raise ValueError('errors must be either "raise" or "ignore"')
3699 
3700         if is_dict_like(labels) and not isinstance(labels, dict):
3701             warnings.warn(
3702                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
3703                 FutureWarning,
3704                 stacklevel=2,
3705             )
3706             return self.drop_vars(labels, errors=errors)
3707 
3708         if labels_kwargs or isinstance(labels, dict):
3709             if dim is not None:
3710                 raise ValueError("cannot specify dim and dict-like arguments.")
3711             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3712 
3713         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
3714             warnings.warn(
3715                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
3716                 PendingDeprecationWarning,
3717                 stacklevel=2,
3718             )
3719             return self.drop_vars(labels, errors=errors)
3720         if dim is not None:
3721             warnings.warn(
3722                 "dropping labels using list-like labels is deprecated; using "
3723                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
3724                 DeprecationWarning,
3725                 stacklevel=2,
3726             )
3727             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
3728 
3729         warnings.warn(
3730             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
3731             PendingDeprecationWarning,
3732             stacklevel=2,
3733         )
3734         return self.drop_sel(labels, errors=errors)
3735 
3736     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
3737         """Drop index labels from this dataset.
3738 
3739         Parameters
3740         ----------
3741         labels : Mapping[Hashable, Any]
3742             Index labels to drop
3743         errors: {'raise', 'ignore'}, optional
3744             If 'raise' (default), raises a ValueError error if
3745             any of the index labels passed are not
3746             in the dataset. If 'ignore', any given labels that are in the
3747             dataset are dropped and no error is raised.
3748         **labels_kwargs : {dim: label, ...}, optional
3749             The keyword arguments form of ``dim`` and ``labels``
3750 
3751         Returns
3752         -------
3753         dropped : Dataset
3754 
3755         Examples
3756         --------
3757         >>> data = np.random.randn(2, 3)
3758         >>> labels = ["a", "b", "c"]
3759         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
3760         >>> ds.drop_sel(y=["a", "c"])
3761         <xarray.Dataset>
3762         Dimensions:  (x: 2, y: 1)
3763         Coordinates:
3764           * y        (y) <U1 'b'
3765         Dimensions without coordinates: x
3766         Data variables:
3767             A        (x, y) float64 -0.3454 0.1734
3768         >>> ds.drop_sel(y="b")
3769         <xarray.Dataset>
3770         Dimensions:  (x: 2, y: 2)
3771         Coordinates:
3772           * y        (y) <U1 'a' 'c'
3773         Dimensions without coordinates: x
3774         Data variables:
3775             A        (x, y) float64 -0.3944 -1.418 1.423 -1.041
3776         """
3777         if errors not in ["raise", "ignore"]:
3778             raise ValueError('errors must be either "raise" or "ignore"')
3779 
3780         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3781 
3782         ds = self
3783         for dim, labels_for_dim in labels.items():
3784             # Don't cast to set, as it would harm performance when labels
3785             # is a large numpy array
3786             if utils.is_scalar(labels_for_dim):
3787                 labels_for_dim = [labels_for_dim]
3788             labels_for_dim = np.asarray(labels_for_dim)
3789             try:
3790                 index = self.indexes[dim]
3791             except KeyError:
3792                 raise ValueError("dimension %r does not have coordinate labels" % dim)
3793             new_index = index.drop(labels_for_dim, errors=errors)
3794             ds = ds.loc[{dim: new_index}]
3795         return ds
3796 
3797     def drop_dims(
3798         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3799     ) -> "Dataset":
3800         """Drop dimensions and associated variables from this dataset.
3801 
3802         Parameters
3803         ----------
3804         drop_dims : hashable or iterable of hashable
3805             Dimension or dimensions to drop.
3806         errors: {'raise', 'ignore'}, optional
3807             If 'raise' (default), raises a ValueError error if any of the
3808             dimensions passed are not in the dataset. If 'ignore', any given
3809             labels that are in the dataset are dropped and no error is raised.
3810 
3811         Returns
3812         -------
3813         obj : Dataset
3814             The dataset without the given dimensions (or any variables
3815             containing those dimensions)
3816         errors: {'raise', 'ignore'}, optional
3817             If 'raise' (default), raises a ValueError error if
3818             any of the dimensions passed are not
3819             in the dataset. If 'ignore', any given dimensions that are in the
3820             dataset are dropped and no error is raised.
3821         """
3822         if errors not in ["raise", "ignore"]:
3823             raise ValueError('errors must be either "raise" or "ignore"')
3824 
3825         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
3826             drop_dims = {drop_dims}
3827         else:
3828             drop_dims = set(drop_dims)
3829 
3830         if errors == "raise":
3831             missing_dims = drop_dims - set(self.dims)
3832             if missing_dims:
3833                 raise ValueError(
3834                     "Dataset does not contain the dimensions: %s" % missing_dims
3835                 )
3836 
3837         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
3838         return self.drop_vars(drop_vars)
3839 
3840     def transpose(self, *dims: Hashable) -> "Dataset":
3841         """Return a new Dataset object with all array dimensions transposed.
3842 
3843         Although the order of dimensions on each array will change, the dataset
3844         dimensions themselves will remain in fixed (sorted) order.
3845 
3846         Parameters
3847         ----------
3848         *dims : Hashable, optional
3849             By default, reverse the dimensions on each array. Otherwise,
3850             reorder the dimensions to this order.
3851 
3852         Returns
3853         -------
3854         transposed : Dataset
3855             Each array in the dataset (including) coordinates will be
3856             transposed to the given order.
3857 
3858         Notes
3859         -----
3860         This operation returns a view of each array's data. It is
3861         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
3862         -- the data will be fully loaded into memory.
3863 
3864         See Also
3865         --------
3866         numpy.transpose
3867         DataArray.transpose
3868         """
3869         if dims:
3870             if set(dims) ^ set(self.dims) and ... not in dims:
3871                 raise ValueError(
3872                     "arguments to transpose (%s) must be "
3873                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
3874                 )
3875         ds = self.copy()
3876         for name, var in self._variables.items():
3877             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
3878             ds._variables[name] = var.transpose(*var_dims)
3879         return ds
3880 
3881     def dropna(
3882         self,
3883         dim: Hashable,
3884         how: str = "any",
3885         thresh: int = None,
3886         subset: Iterable[Hashable] = None,
3887     ):
3888         """Returns a new dataset with dropped labels for missing values along
3889         the provided dimension.
3890 
3891         Parameters
3892         ----------
3893         dim : Hashable
3894             Dimension along which to drop missing values. Dropping along
3895             multiple dimensions simultaneously is not yet supported.
3896         how : {'any', 'all'}, optional
3897             * any : if any NA values are present, drop that label
3898             * all : if all values are NA, drop that label
3899         thresh : int, default None
3900             If supplied, require this many non-NA values.
3901         subset : iterable of hashable, optional
3902             Which variables to check for missing values. By default, all
3903             variables in the dataset are checked.
3904 
3905         Returns
3906         -------
3907         Dataset
3908         """
3909         # TODO: consider supporting multiple dimensions? Or not, given that
3910         # there are some ugly edge cases, e.g., pandas's dropna differs
3911         # depending on the order of the supplied axes.
3912 
3913         if dim not in self.dims:
3914             raise ValueError("%s must be a single dataset dimension" % dim)
3915 
3916         if subset is None:
3917             subset = iter(self.data_vars)
3918 
3919         count = np.zeros(self.dims[dim], dtype=np.int64)
3920         size = 0
3921 
3922         for k in subset:
3923             array = self._variables[k]
3924             if dim in array.dims:
3925                 dims = [d for d in array.dims if d != dim]
3926                 count += np.asarray(array.count(dims))  # type: ignore
3927                 size += np.prod([self.dims[d] for d in dims])
3928 
3929         if thresh is not None:
3930             mask = count >= thresh
3931         elif how == "any":
3932             mask = count == size
3933         elif how == "all":
3934             mask = count > 0
3935         elif how is not None:
3936             raise ValueError("invalid how option: %s" % how)
3937         else:
3938             raise TypeError("must specify how or thresh")
3939 
3940         return self.isel({dim: mask})
3941 
3942     def fillna(self, value: Any) -> "Dataset":
3943         """Fill missing values in this object.
3944 
3945         This operation follows the normal broadcasting and alignment rules that
3946         xarray uses for binary arithmetic, except the result is aligned to this
3947         object (``join='left'``) instead of aligned to the intersection of
3948         index coordinates (``join='inner'``).
3949 
3950         Parameters
3951         ----------
3952         value : scalar, ndarray, DataArray, dict or Dataset
3953             Used to fill all matching missing values in this dataset's data
3954             variables. Scalars, ndarrays or DataArrays arguments are used to
3955             fill all data with aligned coordinates (for DataArrays).
3956             Dictionaries or datasets match data variables and then align
3957             coordinates if necessary.
3958 
3959         Returns
3960         -------
3961         Dataset
3962 
3963         Examples
3964         --------
3965 
3966         >>> import numpy as np
3967         >>> import xarray as xr
3968         >>> ds = xr.Dataset(
3969         ...     {
3970         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
3971         ...         "B": ("x", [3, 4, np.nan, 1]),
3972         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
3973         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
3974         ...     },
3975         ...     coords={"x": [0, 1, 2, 3]},
3976         ... )
3977         >>> ds
3978         <xarray.Dataset>
3979         Dimensions:  (x: 4)
3980         Coordinates:
3981         * x        (x) int64 0 1 2 3
3982         Data variables:
3983             A        (x) float64 nan 2.0 nan 0.0
3984             B        (x) float64 3.0 4.0 nan 1.0
3985             C        (x) float64 nan nan nan 5.0
3986             D        (x) float64 nan 3.0 nan 4.0
3987 
3988         Replace all `NaN` values with 0s.
3989 
3990         >>> ds.fillna(0)
3991         <xarray.Dataset>
3992         Dimensions:  (x: 4)
3993         Coordinates:
3994         * x        (x) int64 0 1 2 3
3995         Data variables:
3996             A        (x) float64 0.0 2.0 0.0 0.0
3997             B        (x) float64 3.0 4.0 0.0 1.0
3998             C        (x) float64 0.0 0.0 0.0 5.0
3999             D        (x) float64 0.0 3.0 0.0 4.0
4000 
4001         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
4002 
4003         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4004         >>> ds.fillna(value=values)
4005         <xarray.Dataset>
4006         Dimensions:  (x: 4)
4007         Coordinates:
4008         * x        (x) int64 0 1 2 3
4009         Data variables:
4010             A        (x) float64 0.0 2.0 0.0 0.0
4011             B        (x) float64 3.0 4.0 1.0 1.0
4012             C        (x) float64 2.0 2.0 2.0 5.0
4013             D        (x) float64 3.0 3.0 3.0 4.0
4014         """
4015         if utils.is_dict_like(value):
4016             value_keys = getattr(value, "data_vars", value).keys()
4017             if not set(value_keys) <= set(self.data_vars.keys()):
4018                 raise ValueError(
4019                     "all variables in the argument to `fillna` "
4020                     "must be contained in the original dataset"
4021                 )
4022         out = ops.fillna(self, value)
4023         return out
4024 
4025     def interpolate_na(
4026         self,
4027         dim: Hashable = None,
4028         method: str = "linear",
4029         limit: int = None,
4030         use_coordinate: Union[bool, Hashable] = True,
4031         max_gap: Union[
4032             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4033         ] = None,
4034         **kwargs: Any,
4035     ) -> "Dataset":
4036         """Fill in NaNs by interpolating according to different methods.
4037 
4038         Parameters
4039         ----------
4040         dim : str
4041             Specifies the dimension along which to interpolate.
4042 
4043         method : str, optional
4044             String indicating which method to use for interpolation:
4045 
4046             - 'linear': linear interpolation (Default). Additional keyword
4047               arguments are passed to :py:func:`numpy.interp`
4048             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4049               are passed to :py:func:`scipy.interpolate.interp1d`. If
4050               ``method='polynomial'``, the ``order`` keyword argument must also be
4051               provided.
4052             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4053               respective :py:class:`scipy.interpolate` classes.
4054 
4055         use_coordinate : bool, str, default True
4056             Specifies which index to use as the x values in the interpolation
4057             formulated as `y = f(x)`. If False, values are treated as if
4058             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4059             used. If ``use_coordinate`` is a string, it specifies the name of a
4060             coordinate variariable to use as the index.
4061         limit : int, default None
4062             Maximum number of consecutive NaNs to fill. Must be greater than 0
4063             or None for no limit. This filling is done regardless of the size of
4064             the gap in the data. To only interpolate over gaps less than a given length,
4065             see ``max_gap``.
4066         max_gap: int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default None.
4067             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4068             Use None for no limit. When interpolating along a datetime64 dimension
4069             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4070 
4071             - a string that is valid input for pandas.to_timedelta
4072             - a :py:class:`numpy.timedelta64` object
4073             - a :py:class:`pandas.Timedelta` object
4074             - a :py:class:`datetime.timedelta` object
4075 
4076             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4077             dimensions has not been implemented yet. Gap length is defined as the difference
4078             between coordinate values at the first data point after a gap and the last value
4079             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4080             between coordinate values at the first (last) valid data point and the first (last) NaN.
4081             For example, consider::
4082 
4083                 <xarray.DataArray (x: 9)>
4084                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4085                 Coordinates:
4086                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4087 
4088             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4089         kwargs : dict, optional
4090             parameters passed verbatim to the underlying interpolation function
4091 
4092         Returns
4093         -------
4094         interpolated: Dataset
4095             Filled in Dataset.
4096 
4097         See also
4098         --------
4099         numpy.interp
4100         scipy.interpolate
4101         """
4102         from .missing import interp_na, _apply_over_vars_with_dim
4103 
4104         new = _apply_over_vars_with_dim(
4105             interp_na,
4106             self,
4107             dim=dim,
4108             method=method,
4109             limit=limit,
4110             use_coordinate=use_coordinate,
4111             max_gap=max_gap,
4112             **kwargs,
4113         )
4114         return new
4115 
4116     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4117         """Fill NaN values by propogating values forward
4118 
4119         *Requires bottleneck.*
4120 
4121         Parameters
4122         ----------
4123         dim : Hashable
4124             Specifies the dimension along which to propagate values when
4125             filling.
4126         limit : int, default None
4127             The maximum number of consecutive NaN values to forward fill. In
4128             other words, if there is a gap with more than this number of
4129             consecutive NaNs, it will only be partially filled. Must be greater
4130             than 0 or None for no limit.
4131 
4132         Returns
4133         -------
4134         Dataset
4135         """
4136         from .missing import ffill, _apply_over_vars_with_dim
4137 
4138         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4139         return new
4140 
4141     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4142         """Fill NaN values by propogating values backward
4143 
4144         *Requires bottleneck.*
4145 
4146         Parameters
4147         ----------
4148         dim : str
4149             Specifies the dimension along which to propagate values when
4150             filling.
4151         limit : int, default None
4152             The maximum number of consecutive NaN values to backward fill. In
4153             other words, if there is a gap with more than this number of
4154             consecutive NaNs, it will only be partially filled. Must be greater
4155             than 0 or None for no limit.
4156 
4157         Returns
4158         -------
4159         Dataset
4160         """
4161         from .missing import bfill, _apply_over_vars_with_dim
4162 
4163         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4164         return new
4165 
4166     def combine_first(self, other: "Dataset") -> "Dataset":
4167         """Combine two Datasets, default to data_vars of self.
4168 
4169         The new coordinates follow the normal broadcasting and alignment rules
4170         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4171         filled with np.nan.
4172 
4173         Parameters
4174         ----------
4175         other : Dataset
4176             Used to fill all matching missing values in this array.
4177 
4178         Returns
4179         -------
4180         Dataset
4181         """
4182         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4183         return out
4184 
4185     def reduce(
4186         self,
4187         func: Callable,
4188         dim: Union[Hashable, Iterable[Hashable]] = None,
4189         keep_attrs: bool = None,
4190         keepdims: bool = False,
4191         numeric_only: bool = False,
4192         allow_lazy: bool = None,
4193         **kwargs: Any,
4194     ) -> "Dataset":
4195         """Reduce this dataset by applying `func` along some dimension(s).
4196 
4197         Parameters
4198         ----------
4199         func : callable
4200             Function which can be called in the form
4201             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4202             np.ndarray over an integer valued axis.
4203         dim : str or sequence of str, optional
4204             Dimension(s) over which to apply `func`.  By default `func` is
4205             applied over all dimensions.
4206         keep_attrs : bool, optional
4207             If True, the dataset's attributes (`attrs`) will be copied from
4208             the original object to the new one.  If False (default), the new
4209             object will be returned without attributes.
4210         keepdims : bool, default False
4211             If True, the dimensions which are reduced are left in the result
4212             as dimensions of size one. Coordinates that use these dimensions
4213             are removed.
4214         numeric_only : bool, optional
4215             If True, only apply ``func`` to variables with a numeric dtype.
4216         **kwargs : Any
4217             Additional keyword arguments passed on to ``func``.
4218 
4219         Returns
4220         -------
4221         reduced : Dataset
4222             Dataset with this object's DataArrays replaced with new DataArrays
4223             of summarized data and the indicated dimension(s) removed.
4224         """
4225         if dim is None or dim is ...:
4226             dims = set(self.dims)
4227         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4228             dims = {dim}
4229         else:
4230             dims = set(dim)
4231 
4232         missing_dimensions = [d for d in dims if d not in self.dims]
4233         if missing_dimensions:
4234             raise ValueError(
4235                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4236             )
4237 
4238         if keep_attrs is None:
4239             keep_attrs = _get_keep_attrs(default=False)
4240 
4241         variables: Dict[Hashable, Variable] = {}
4242         for name, var in self._variables.items():
4243             reduce_dims = [d for d in var.dims if d in dims]
4244             if name in self.coords:
4245                 if not reduce_dims:
4246                     variables[name] = var
4247             else:
4248                 if (
4249                     not numeric_only
4250                     or np.issubdtype(var.dtype, np.number)
4251                     or (var.dtype == np.bool_)
4252                 ):
4253                     if len(reduce_dims) == 1:
4254                         # unpack dimensions for the benefit of functions
4255                         # like np.argmin which can't handle tuple arguments
4256                         (reduce_dims,) = reduce_dims
4257                     elif len(reduce_dims) == var.ndim:
4258                         # prefer to aggregate over axis=None rather than
4259                         # axis=(0, 1) if they will be equivalent, because
4260                         # the former is often more efficient
4261                         reduce_dims = None  # type: ignore
4262                     variables[name] = var.reduce(
4263                         func,
4264                         dim=reduce_dims,
4265                         keep_attrs=keep_attrs,
4266                         keepdims=keepdims,
4267                         allow_lazy=allow_lazy,
4268                         **kwargs,
4269                     )
4270 
4271         coord_names = {k for k in self.coords if k in variables}
4272         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4273         attrs = self.attrs if keep_attrs else None
4274         return self._replace_with_new_dims(
4275             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4276         )
4277 
4278     def map(
4279         self,
4280         func: Callable,
4281         keep_attrs: bool = None,
4282         args: Iterable[Any] = (),
4283         **kwargs: Any,
4284     ) -> "Dataset":
4285         """Apply a function to each variable in this dataset
4286 
4287         Parameters
4288         ----------
4289         func : callable
4290             Function which can be called in the form `func(x, *args, **kwargs)`
4291             to transform each DataArray `x` in this dataset into another
4292             DataArray.
4293         keep_attrs : bool, optional
4294             If True, the dataset's attributes (`attrs`) will be copied from
4295             the original object to the new one. If False, the new object will
4296             be returned without attributes.
4297         args : tuple, optional
4298             Positional arguments passed on to `func`.
4299         **kwargs : Any
4300             Keyword arguments passed on to `func`.
4301 
4302         Returns
4303         -------
4304         applied : Dataset
4305             Resulting dataset from applying ``func`` to each data variable.
4306 
4307         Examples
4308         --------
4309         >>> da = xr.DataArray(np.random.randn(2, 3))
4310         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4311         >>> ds
4312         <xarray.Dataset>
4313         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4314         Dimensions without coordinates: dim_0, dim_1, x
4315         Data variables:
4316             foo      (dim_0, dim_1) float64 -0.3751 -1.951 -1.945 0.2948 0.711 -0.3948
4317             bar      (x) int64 -1 2
4318         >>> ds.map(np.fabs)
4319         <xarray.Dataset>
4320         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4321         Dimensions without coordinates: dim_0, dim_1, x
4322         Data variables:
4323             foo      (dim_0, dim_1) float64 0.3751 1.951 1.945 0.2948 0.711 0.3948
4324             bar      (x) float64 1.0 2.0
4325         """
4326         variables = {
4327             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4328             for k, v in self.data_vars.items()
4329         }
4330         if keep_attrs is None:
4331             keep_attrs = _get_keep_attrs(default=False)
4332         attrs = self.attrs if keep_attrs else None
4333         return type(self)(variables, attrs=attrs)
4334 
4335     def apply(
4336         self,
4337         func: Callable,
4338         keep_attrs: bool = None,
4339         args: Iterable[Any] = (),
4340         **kwargs: Any,
4341     ) -> "Dataset":
4342         """
4343         Backward compatible implementation of ``map``
4344 
4345         See Also
4346         --------
4347         Dataset.map
4348         """
4349         warnings.warn(
4350             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4351             PendingDeprecationWarning,
4352             stacklevel=2,
4353         )
4354         return self.map(func, keep_attrs, args, **kwargs)
4355 
4356     def assign(
4357         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4358     ) -> "Dataset":
4359         """Assign new data variables to a Dataset, returning a new object
4360         with all the original variables in addition to the new ones.
4361 
4362         Parameters
4363         ----------
4364         variables : mapping, value pairs
4365             Mapping from variables names to the new values. If the new values
4366             are callable, they are computed on the Dataset and assigned to new
4367             data variables. If the values are not callable, (e.g. a DataArray,
4368             scalar, or array), they are simply assigned.
4369         **variables_kwargs:
4370             The keyword arguments form of ``variables``.
4371             One of variables or variables_kwargs must be provided.
4372 
4373         Returns
4374         -------
4375         ds : Dataset
4376             A new Dataset with the new variables in addition to all the
4377             existing variables.
4378 
4379         Notes
4380         -----
4381         Since ``kwargs`` is a dictionary, the order of your arguments may not
4382         be preserved, and so the order of the new variables is not well
4383         defined. Assigning multiple variables within the same ``assign`` is
4384         possible, but you cannot reference other variables created within the
4385         same ``assign`` call.
4386 
4387         See Also
4388         --------
4389         pandas.DataFrame.assign
4390 
4391         Examples
4392         --------
4393         >>> import numpy as np
4394         >>> import xarray as xr
4395         >>> x = xr.Dataset(
4396         ...     {
4397         ...         "temperature_c": (
4398         ...             ("lat", "lon"),
4399         ...             20 * np.random.rand(4).reshape(2, 2),
4400         ...         ),
4401         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4402         ...     },
4403         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4404         ... )
4405         >>> x
4406         <xarray.Dataset>
4407         Dimensions:        (lat: 2, lon: 2)
4408         Coordinates:
4409         * lat            (lat) int64 10 20
4410         * lon            (lon) int64 150 160
4411         Data variables:
4412             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4413             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4414 
4415         Where the value is a callable, evaluated on dataset:
4416 
4417         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4418         <xarray.Dataset>
4419         Dimensions:        (lat: 2, lon: 2)
4420         Coordinates:
4421         * lat            (lat) int64 10 20
4422         * lon            (lon) int64 150 160
4423         Data variables:
4424             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4425             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4426             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4427 
4428         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4429 
4430         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4431         <xarray.Dataset>
4432         Dimensions:        (lat: 2, lon: 2)
4433         Coordinates:
4434         * lat            (lat) int64 10 20
4435         * lon            (lon) int64 150 160
4436         Data variables:
4437             temperature_c  (lat, lon) float64 18.04 12.51 17.64 9.313
4438             precipitation  (lat, lon) float64 0.4751 0.6827 0.3697 0.03524
4439             temperature_f  (lat, lon) float64 64.47 54.51 63.75 48.76
4440 
4441         """
4442         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4443         data = self.copy()
4444         # do all calculations first...
4445         results = data._calc_assign_results(variables)
4446         # ... and then assign
4447         data.update(results)
4448         return data
4449 
4450     def to_array(self, dim="variable", name=None):
4451         """Convert this dataset into an xarray.DataArray
4452 
4453         The data variables of this dataset will be broadcast against each other
4454         and stacked along the first axis of the new array. All coordinates of
4455         this dataset will remain coordinates.
4456 
4457         Parameters
4458         ----------
4459         dim : str, optional
4460             Name of the new dimension.
4461         name : str, optional
4462             Name of the new data array.
4463 
4464         Returns
4465         -------
4466         array : xarray.DataArray
4467         """
4468         from .dataarray import DataArray
4469 
4470         data_vars = [self.variables[k] for k in self.data_vars]
4471         broadcast_vars = broadcast_variables(*data_vars)
4472         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4473 
4474         coords = dict(self.coords)
4475         coords[dim] = list(self.data_vars)
4476         indexes = propagate_indexes(self._indexes)
4477 
4478         dims = (dim,) + broadcast_vars[0].dims
4479 
4480         return DataArray(
4481             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4482         )
4483 
4484     def _to_dataframe(self, ordered_dims):
4485         columns = [k for k in self.variables if k not in self.dims]
4486         data = [
4487             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4488             for k in columns
4489         ]
4490         index = self.coords.to_index(ordered_dims)
4491         return pd.DataFrame(dict(zip(columns, data)), index=index)
4492 
4493     def to_dataframe(self):
4494         """Convert this dataset into a pandas.DataFrame.
4495 
4496         Non-index variables in this dataset form the columns of the
4497         DataFrame. The DataFrame is be indexed by the Cartesian product of
4498         this dataset's indices.
4499         """
4500         return self._to_dataframe(self.dims)
4501 
4502     def _set_sparse_data_from_dataframe(
4503         self, dataframe: pd.DataFrame, dims: tuple
4504     ) -> None:
4505         from sparse import COO
4506 
4507         idx = dataframe.index
4508         if isinstance(idx, pd.MultiIndex):
4509             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
4510             is_sorted = idx.is_lexsorted
4511             shape = tuple(lev.size for lev in idx.levels)
4512         else:
4513             coords = np.arange(idx.size).reshape(1, -1)
4514             is_sorted = True
4515             shape = (idx.size,)
4516 
4517         for name, series in dataframe.items():
4518             # Cast to a NumPy array first, in case the Series is a pandas
4519             # Extension array (which doesn't have a valid NumPy dtype)
4520             values = np.asarray(series)
4521 
4522             # In virtually all real use cases, the sparse array will now have
4523             # missing values and needs a fill_value. For consistency, don't
4524             # special case the rare exceptions (e.g., dtype=int without a
4525             # MultiIndex).
4526             dtype, fill_value = dtypes.maybe_promote(values.dtype)
4527             values = np.asarray(values, dtype=dtype)
4528 
4529             data = COO(
4530                 coords,
4531                 values,
4532                 shape,
4533                 has_duplicates=False,
4534                 sorted=is_sorted,
4535                 fill_value=fill_value,
4536             )
4537             self[name] = (dims, data)
4538 
4539     def _set_numpy_data_from_dataframe(
4540         self, dataframe: pd.DataFrame, dims: tuple
4541     ) -> None:
4542         idx = dataframe.index
4543         if isinstance(idx, pd.MultiIndex):
4544             # expand the DataFrame to include the product of all levels
4545             full_idx = pd.MultiIndex.from_product(idx.levels, names=idx.names)
4546             dataframe = dataframe.reindex(full_idx)
4547             shape = tuple(lev.size for lev in idx.levels)
4548         else:
4549             shape = (idx.size,)
4550         for name, series in dataframe.items():
4551             data = np.asarray(series).reshape(shape)
4552             self[name] = (dims, data)
4553 
4554     @classmethod
4555     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
4556         """Convert a pandas.DataFrame into an xarray.Dataset
4557 
4558         Each column will be converted into an independent variable in the
4559         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
4560         into a tensor product of one-dimensional indices (filling in missing
4561         values with NaN). This method will produce a Dataset very similar to
4562         that on which the 'to_dataframe' method was called, except with
4563         possibly redundant dimensions (since all dataset variables will have
4564         the same dimensionality)
4565 
4566         Parameters
4567         ----------
4568         dataframe : pandas.DataFrame
4569             DataFrame from which to copy data and indices.
4570         sparse : bool
4571             If true, create a sparse arrays instead of dense numpy arrays. This
4572             can potentially save a large amount of memory if the DataFrame has
4573             a MultiIndex. Requires the sparse package (sparse.pydata.org).
4574 
4575         Returns
4576         -------
4577         New Dataset.
4578 
4579         See also
4580         --------
4581         xarray.DataArray.from_series
4582         """
4583         # TODO: Add an option to remove dimensions along which the variables
4584         # are constant, to enable consistent serialization to/from a dataframe,
4585         # even if some variables have different dimensionality.
4586 
4587         if not dataframe.columns.is_unique:
4588             raise ValueError("cannot convert DataFrame with non-unique columns")
4589 
4590         idx = remove_unused_levels_categories(dataframe.index)
4591         dataframe = dataframe.set_index(idx)
4592         obj = cls()
4593 
4594         if isinstance(idx, pd.MultiIndex):
4595             dims = tuple(
4596                 name if name is not None else "level_%i" % n
4597                 for n, name in enumerate(idx.names)
4598             )
4599             for dim, lev in zip(dims, idx.levels):
4600                 obj[dim] = (dim, lev)
4601         else:
4602             index_name = idx.name if idx.name is not None else "index"
4603             dims = (index_name,)
4604             obj[index_name] = (dims, idx)
4605 
4606         if sparse:
4607             obj._set_sparse_data_from_dataframe(dataframe, dims)
4608         else:
4609             obj._set_numpy_data_from_dataframe(dataframe, dims)
4610         return obj
4611 
4612     def to_dask_dataframe(self, dim_order=None, set_index=False):
4613         """
4614         Convert this dataset into a dask.dataframe.DataFrame.
4615 
4616         The dimensions, coordinates and data variables in this dataset form
4617         the columns of the DataFrame.
4618 
4619         Parameters
4620         ----------
4621         dim_order : list, optional
4622             Hierarchical dimension order for the resulting dataframe. All
4623             arrays are transposed to this order and then written out as flat
4624             vectors in contiguous order, so the last dimension in this list
4625             will be contiguous in the resulting DataFrame. This has a major
4626             influence on which operations are efficient on the resulting dask
4627             dataframe.
4628 
4629             If provided, must include all dimensions on this dataset. By
4630             default, dimensions are sorted alphabetically.
4631         set_index : bool, optional
4632             If set_index=True, the dask DataFrame is indexed by this dataset's
4633             coordinate. Since dask DataFrames to not support multi-indexes,
4634             set_index only works if the dataset only contains one dimension.
4635 
4636         Returns
4637         -------
4638         dask.dataframe.DataFrame
4639         """
4640 
4641         import dask.array as da
4642         import dask.dataframe as dd
4643 
4644         if dim_order is None:
4645             dim_order = list(self.dims)
4646         elif set(dim_order) != set(self.dims):
4647             raise ValueError(
4648                 "dim_order {} does not match the set of dimensions on this "
4649                 "Dataset: {}".format(dim_order, list(self.dims))
4650             )
4651 
4652         ordered_dims = {k: self.dims[k] for k in dim_order}
4653 
4654         columns = list(ordered_dims)
4655         columns.extend(k for k in self.coords if k not in self.dims)
4656         columns.extend(self.data_vars)
4657 
4658         series_list = []
4659         for name in columns:
4660             try:
4661                 var = self.variables[name]
4662             except KeyError:
4663                 # dimension without a matching coordinate
4664                 size = self.dims[name]
4665                 data = da.arange(size, chunks=size, dtype=np.int64)
4666                 var = Variable((name,), data)
4667 
4668             # IndexVariable objects have a dummy .chunk() method
4669             if isinstance(var, IndexVariable):
4670                 var = var.to_base_variable()
4671 
4672             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
4673             series = dd.from_array(dask_array.reshape(-1), columns=[name])
4674             series_list.append(series)
4675 
4676         df = dd.concat(series_list, axis=1)
4677 
4678         if set_index:
4679             if len(dim_order) == 1:
4680                 (dim,) = dim_order
4681                 df = df.set_index(dim)
4682             else:
4683                 # triggers an error about multi-indexes, even if only one
4684                 # dimension is passed
4685                 df = df.set_index(dim_order)
4686 
4687         return df
4688 
4689     def to_dict(self, data=True):
4690         """
4691         Convert this dataset to a dictionary following xarray naming
4692         conventions.
4693 
4694         Converts all variables and attributes to native Python objects
4695         Useful for converting to json. To avoid datetime incompatibility
4696         use decode_times=False kwarg in xarrray.open_dataset.
4697 
4698         Parameters
4699         ----------
4700         data : bool, optional
4701             Whether to include the actual data in the dictionary. When set to
4702             False, returns just the schema.
4703 
4704         See also
4705         --------
4706         Dataset.from_dict
4707         """
4708         d = {
4709             "coords": {},
4710             "attrs": decode_numpy_dict_values(self.attrs),
4711             "dims": dict(self.dims),
4712             "data_vars": {},
4713         }
4714         for k in self.coords:
4715             d["coords"].update({k: self[k].variable.to_dict(data=data)})
4716         for k in self.data_vars:
4717             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
4718         return d
4719 
4720     @classmethod
4721     def from_dict(cls, d):
4722         """
4723         Convert a dictionary into an xarray.Dataset.
4724 
4725         Input dict can take several forms::
4726 
4727             d = {'t': {'dims': ('t'), 'data': t},
4728                  'a': {'dims': ('t'), 'data': x},
4729                  'b': {'dims': ('t'), 'data': y}}
4730 
4731             d = {'coords': {'t': {'dims': 't', 'data': t,
4732                                   'attrs': {'units':'s'}}},
4733                  'attrs': {'title': 'air temperature'},
4734                  'dims': 't',
4735                  'data_vars': {'a': {'dims': 't', 'data': x, },
4736                                'b': {'dims': 't', 'data': y}}}
4737 
4738         where 't' is the name of the dimesion, 'a' and 'b' are names of data
4739         variables and t, x, and y are lists, numpy.arrays or pandas objects.
4740 
4741         Parameters
4742         ----------
4743         d : dict, with a minimum structure of {'var_0': {'dims': [..], \
4744                                                          'data': [..]}, \
4745                                                ...}
4746 
4747         Returns
4748         -------
4749         obj : xarray.Dataset
4750 
4751         See also
4752         --------
4753         Dataset.to_dict
4754         DataArray.from_dict
4755         """
4756 
4757         if not {"coords", "data_vars"}.issubset(set(d)):
4758             variables = d.items()
4759         else:
4760             import itertools
4761 
4762             variables = itertools.chain(
4763                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
4764             )
4765         try:
4766             variable_dict = {
4767                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
4768             }
4769         except KeyError as e:
4770             raise ValueError(
4771                 "cannot convert dict without the key "
4772                 "'{dims_data}'".format(dims_data=str(e.args[0]))
4773             )
4774         obj = cls(variable_dict)
4775 
4776         # what if coords aren't dims?
4777         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
4778         obj = obj.set_coords(coords)
4779 
4780         obj.attrs.update(d.get("attrs", {}))
4781 
4782         return obj
4783 
4784     @staticmethod
4785     def _unary_op(f, keep_attrs=False):
4786         @functools.wraps(f)
4787         def func(self, *args, **kwargs):
4788             variables = {}
4789             for k, v in self._variables.items():
4790                 if k in self._coord_names:
4791                     variables[k] = v
4792                 else:
4793                     variables[k] = f(v, *args, **kwargs)
4794             attrs = self._attrs if keep_attrs else None
4795             return self._replace_with_new_dims(variables, attrs=attrs)
4796 
4797         return func
4798 
4799     @staticmethod
4800     def _binary_op(f, reflexive=False, join=None):
4801         @functools.wraps(f)
4802         def func(self, other):
4803             from .dataarray import DataArray
4804 
4805             if isinstance(other, groupby.GroupBy):
4806                 return NotImplemented
4807             align_type = OPTIONS["arithmetic_join"] if join is None else join
4808             if isinstance(other, (DataArray, Dataset)):
4809                 self, other = align(self, other, join=align_type, copy=False)
4810             g = f if not reflexive else lambda x, y: f(y, x)
4811             ds = self._calculate_binary_op(g, other, join=align_type)
4812             return ds
4813 
4814         return func
4815 
4816     @staticmethod
4817     def _inplace_binary_op(f):
4818         @functools.wraps(f)
4819         def func(self, other):
4820             from .dataarray import DataArray
4821 
4822             if isinstance(other, groupby.GroupBy):
4823                 raise TypeError(
4824                     "in-place operations between a Dataset and "
4825                     "a grouped object are not permitted"
4826                 )
4827             # we don't actually modify arrays in-place with in-place Dataset
4828             # arithmetic -- this lets us automatically align things
4829             if isinstance(other, (DataArray, Dataset)):
4830                 other = other.reindex_like(self, copy=False)
4831             g = ops.inplace_to_noninplace_op(f)
4832             ds = self._calculate_binary_op(g, other, inplace=True)
4833             self._replace_with_new_dims(
4834                 ds._variables,
4835                 ds._coord_names,
4836                 attrs=ds._attrs,
4837                 indexes=ds._indexes,
4838                 inplace=True,
4839             )
4840             return self
4841 
4842         return func
4843 
4844     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
4845         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
4846             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
4847                 raise ValueError(
4848                     "datasets must have the same data variables "
4849                     "for in-place arithmetic operations: %s, %s"
4850                     % (list(lhs_data_vars), list(rhs_data_vars))
4851                 )
4852 
4853             dest_vars = {}
4854 
4855             for k in lhs_data_vars:
4856                 if k in rhs_data_vars:
4857                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
4858                 elif join in ["left", "outer"]:
4859                     dest_vars[k] = f(lhs_vars[k], np.nan)
4860             for k in rhs_data_vars:
4861                 if k not in dest_vars and join in ["right", "outer"]:
4862                     dest_vars[k] = f(rhs_vars[k], np.nan)
4863             return dest_vars
4864 
4865         if utils.is_dict_like(other) and not isinstance(other, Dataset):
4866             # can't use our shortcut of doing the binary operation with
4867             # Variable objects, so apply over our data vars instead.
4868             new_data_vars = apply_over_both(
4869                 self.data_vars, other, self.data_vars, other
4870             )
4871             return Dataset(new_data_vars)
4872 
4873         other_coords = getattr(other, "coords", None)
4874         ds = self.coords.merge(other_coords)
4875 
4876         if isinstance(other, Dataset):
4877             new_vars = apply_over_both(
4878                 self.data_vars, other.data_vars, self.variables, other.variables
4879             )
4880         else:
4881             other_variable = getattr(other, "variable", other)
4882             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
4883         ds._variables.update(new_vars)
4884         ds._dims = calculate_dimensions(ds._variables)
4885         return ds
4886 
4887     def _copy_attrs_from(self, other):
4888         self.attrs = other.attrs
4889         for v in other.variables:
4890             if v in self.variables:
4891                 self.variables[v].attrs = other.variables[v].attrs
4892 
4893     def diff(self, dim, n=1, label="upper"):
4894         """Calculate the n-th order discrete difference along given axis.
4895 
4896         Parameters
4897         ----------
4898         dim : str, optional
4899             Dimension over which to calculate the finite difference.
4900         n : int, optional
4901             The number of times values are differenced.
4902         label : str, optional
4903             The new coordinate in dimension ``dim`` will have the
4904             values of either the minuend's or subtrahend's coordinate
4905             for values 'upper' and 'lower', respectively.  Other
4906             values are not supported.
4907 
4908         Returns
4909         -------
4910         difference : same type as caller
4911             The n-th order finite difference of this object.
4912 
4913         .. note::
4914 
4915             `n` matches numpy's behavior and is different from pandas' first
4916             argument named `periods`.
4917 
4918         Examples
4919         --------
4920         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
4921         >>> ds.diff("x")
4922         <xarray.Dataset>
4923         Dimensions:  (x: 3)
4924         Coordinates:
4925           * x        (x) int64 1 2 3
4926         Data variables:
4927             foo      (x) int64 0 1 0
4928         >>> ds.diff("x", 2)
4929         <xarray.Dataset>
4930         Dimensions:  (x: 2)
4931         Coordinates:
4932         * x        (x) int64 2 3
4933         Data variables:
4934         foo      (x) int64 1 -1
4935 
4936         See Also
4937         --------
4938         Dataset.differentiate
4939         """
4940         if n == 0:
4941             return self
4942         if n < 0:
4943             raise ValueError(f"order `n` must be non-negative but got {n}")
4944 
4945         # prepare slices
4946         kwargs_start = {dim: slice(None, -1)}
4947         kwargs_end = {dim: slice(1, None)}
4948 
4949         # prepare new coordinate
4950         if label == "upper":
4951             kwargs_new = kwargs_end
4952         elif label == "lower":
4953             kwargs_new = kwargs_start
4954         else:
4955             raise ValueError(
4956                 "The 'label' argument has to be either " "'upper' or 'lower'"
4957             )
4958 
4959         variables = {}
4960 
4961         for name, var in self.variables.items():
4962             if dim in var.dims:
4963                 if name in self.data_vars:
4964                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
4965                 else:
4966                     variables[name] = var.isel(**kwargs_new)
4967             else:
4968                 variables[name] = var
4969 
4970         indexes = dict(self.indexes)
4971         if dim in indexes:
4972             indexes[dim] = indexes[dim][kwargs_new[dim]]
4973 
4974         difference = self._replace_with_new_dims(variables, indexes=indexes)
4975 
4976         if n > 1:
4977             return difference.diff(dim, n - 1)
4978         else:
4979             return difference
4980 
4981     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
4982         """Shift this dataset by an offset along one or more dimensions.
4983 
4984         Only data variables are moved; coordinates stay in place. This is
4985         consistent with the behavior of ``shift`` in pandas.
4986 
4987         Parameters
4988         ----------
4989         shifts : Mapping with the form of {dim: offset}
4990             Integer offset to shift along each of the given dimensions.
4991             Positive offsets shift to the right; negative offsets shift to the
4992             left.
4993         fill_value: scalar, optional
4994             Value to use for newly missing values
4995         **shifts_kwargs:
4996             The keyword arguments form of ``shifts``.
4997             One of shifts or shifts_kwargs must be provided.
4998 
4999         Returns
5000         -------
5001         shifted : Dataset
5002             Dataset with the same coordinates and attributes but shifted data
5003             variables.
5004 
5005         See also
5006         --------
5007         roll
5008 
5009         Examples
5010         --------
5011 
5012         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5013         >>> ds.shift(x=2)
5014         <xarray.Dataset>
5015         Dimensions:  (x: 5)
5016         Coordinates:
5017           * x        (x) int64 0 1 2 3 4
5018         Data variables:
5019             foo      (x) object nan nan 'a' 'b' 'c'
5020         """
5021         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5022         invalid = [k for k in shifts if k not in self.dims]
5023         if invalid:
5024             raise ValueError("dimensions %r do not exist" % invalid)
5025 
5026         variables = {}
5027         for name, var in self.variables.items():
5028             if name in self.data_vars:
5029                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5030                 variables[name] = var.shift(fill_value=fill_value, shifts=var_shifts)
5031             else:
5032                 variables[name] = var
5033 
5034         return self._replace(variables)
5035 
5036     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5037         """Roll this dataset by an offset along one or more dimensions.
5038 
5039         Unlike shift, roll may rotate all variables, including coordinates
5040         if specified. The direction of rotation is consistent with
5041         :py:func:`numpy.roll`.
5042 
5043         Parameters
5044         ----------
5045 
5046         shifts : dict, optional
5047             A dict with keys matching dimensions and values given
5048             by integers to rotate each of the given dimensions. Positive
5049             offsets roll to the right; negative offsets roll to the left.
5050         roll_coords : bool
5051             Indicates whether to  roll the coordinates by the offset
5052             The current default of roll_coords (None, equivalent to True) is
5053             deprecated and will change to False in a future version.
5054             Explicitly pass roll_coords to silence the warning.
5055         **shifts_kwargs : {dim: offset, ...}, optional
5056             The keyword arguments form of ``shifts``.
5057             One of shifts or shifts_kwargs must be provided.
5058         Returns
5059         -------
5060         rolled : Dataset
5061             Dataset with the same coordinates and attributes but rolled
5062             variables.
5063 
5064         See also
5065         --------
5066         shift
5067 
5068         Examples
5069         --------
5070 
5071         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5072         >>> ds.roll(x=2)
5073         <xarray.Dataset>
5074         Dimensions:  (x: 5)
5075         Coordinates:
5076           * x        (x) int64 3 4 0 1 2
5077         Data variables:
5078             foo      (x) object 'd' 'e' 'a' 'b' 'c'
5079         """
5080         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5081         invalid = [k for k in shifts if k not in self.dims]
5082         if invalid:
5083             raise ValueError("dimensions %r do not exist" % invalid)
5084 
5085         if roll_coords is None:
5086             warnings.warn(
5087                 "roll_coords will be set to False in the future."
5088                 " Explicitly set roll_coords to silence warning.",
5089                 FutureWarning,
5090                 stacklevel=2,
5091             )
5092             roll_coords = True
5093 
5094         unrolled_vars = () if roll_coords else self.coords
5095 
5096         variables = {}
5097         for k, v in self.variables.items():
5098             if k not in unrolled_vars:
5099                 variables[k] = v.roll(
5100                     **{k: s for k, s in shifts.items() if k in v.dims}
5101                 )
5102             else:
5103                 variables[k] = v
5104 
5105         if roll_coords:
5106             indexes = {}
5107             for k, v in self.indexes.items():
5108                 (dim,) = self.variables[k].dims
5109                 if dim in shifts:
5110                     indexes[k] = roll_index(v, shifts[dim])
5111                 else:
5112                     indexes[k] = v
5113         else:
5114             indexes = dict(self.indexes)
5115 
5116         return self._replace(variables, indexes=indexes)
5117 
5118     def sortby(self, variables, ascending=True):
5119         """
5120         Sort object by labels or values (along an axis).
5121 
5122         Sorts the dataset, either along specified dimensions,
5123         or according to values of 1-D dataarrays that share dimension
5124         with calling object.
5125 
5126         If the input variables are dataarrays, then the dataarrays are aligned
5127         (via left-join) to the calling object prior to sorting by cell values.
5128         NaNs are sorted to the end, following Numpy convention.
5129 
5130         If multiple sorts along the same dimension is
5131         given, numpy's lexsort is performed along that dimension:
5132         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5133         and the FIRST key in the sequence is used as the primary sort key,
5134         followed by the 2nd key, etc.
5135 
5136         Parameters
5137         ----------
5138         variables: str, DataArray, or list of either
5139             1D DataArray objects or name(s) of 1D variable(s) in
5140             coords/data_vars whose values are used to sort the dataset.
5141         ascending: boolean, optional
5142             Whether to sort by ascending or descending order.
5143 
5144         Returns
5145         -------
5146         sorted: Dataset
5147             A new dataset where all the specified dims are sorted by dim
5148             labels.
5149         """
5150         from .dataarray import DataArray
5151 
5152         if not isinstance(variables, list):
5153             variables = [variables]
5154         else:
5155             variables = variables
5156         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5157         aligned_vars = align(self, *variables, join="left")
5158         aligned_self = aligned_vars[0]
5159         aligned_other_vars = aligned_vars[1:]
5160         vars_by_dim = defaultdict(list)
5161         for data_array in aligned_other_vars:
5162             if data_array.ndim != 1:
5163                 raise ValueError("Input DataArray is not 1-D.")
5164             (key,) = data_array.dims
5165             vars_by_dim[key].append(data_array)
5166 
5167         indices = {}
5168         for key, arrays in vars_by_dim.items():
5169             order = np.lexsort(tuple(reversed(arrays)))
5170             indices[key] = order if ascending else order[::-1]
5171         return aligned_self.isel(**indices)
5172 
5173     def quantile(
5174         self,
5175         q,
5176         dim=None,
5177         interpolation="linear",
5178         numeric_only=False,
5179         keep_attrs=None,
5180         skipna=True,
5181     ):
5182         """Compute the qth quantile of the data along the specified dimension.
5183 
5184         Returns the qth quantiles(s) of the array elements for each variable
5185         in the Dataset.
5186 
5187         Parameters
5188         ----------
5189         q : float in range of [0,1] or array-like of floats
5190             Quantile to compute, which must be between 0 and 1 inclusive.
5191         dim : str or sequence of str, optional
5192             Dimension(s) over which to apply quantile.
5193         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
5194             This optional parameter specifies the interpolation method to
5195             use when the desired quantile lies between two data points
5196             ``i < j``:
5197 
5198                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5199                   the fractional part of the index surrounded by ``i`` and
5200                   ``j``.
5201                 * lower: ``i``.
5202                 * higher: ``j``.
5203                 * nearest: ``i`` or ``j``, whichever is nearest.
5204                 * midpoint: ``(i + j) / 2``.
5205         keep_attrs : bool, optional
5206             If True, the dataset's attributes (`attrs`) will be copied from
5207             the original object to the new one.  If False (default), the new
5208             object will be returned without attributes.
5209         numeric_only : bool, optional
5210             If True, only apply ``func`` to variables with a numeric dtype.
5211         skipna : bool, optional
5212             Whether to skip missing values when aggregating.
5213 
5214         Returns
5215         -------
5216         quantiles : Dataset
5217             If `q` is a single quantile, then the result is a scalar for each
5218             variable in data_vars. If multiple percentiles are given, first
5219             axis of the result corresponds to the quantile and a quantile
5220             dimension is added to the return Dataset. The other dimensions are
5221             the dimensions that remain after the reduction of the array.
5222 
5223         See Also
5224         --------
5225         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5226 
5227         Examples
5228         --------
5229 
5230         >>> ds = xr.Dataset(
5231         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5232         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5233         ... )
5234         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5235         <xarray.Dataset>
5236         Dimensions:   ()
5237         Coordinates:
5238             quantile  float64 0.0
5239         Data variables:
5240             a         float64 0.7
5241         >>> ds.quantile(0, dim="x")
5242         <xarray.Dataset>
5243         Dimensions:   (y: 4)
5244         Coordinates:
5245           * y         (y) float64 1.0 1.5 2.0 2.5
5246             quantile  float64 0.0
5247         Data variables:
5248             a         (y) float64 0.7 4.2 2.6 1.5
5249         >>> ds.quantile([0, 0.5, 1])
5250         <xarray.Dataset>
5251         Dimensions:   (quantile: 3)
5252         Coordinates:
5253           * quantile  (quantile) float64 0.0 0.5 1.0
5254         Data variables:
5255             a         (quantile) float64 0.7 3.4 9.4
5256         >>> ds.quantile([0, 0.5, 1], dim="x")
5257         <xarray.Dataset>
5258         Dimensions:   (quantile: 3, y: 4)
5259         Coordinates:
5260           * y         (y) float64 1.0 1.5 2.0 2.5
5261           * quantile  (quantile) float64 0.0 0.5 1.0
5262         Data variables:
5263             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5264         """
5265 
5266         if isinstance(dim, str):
5267             dims = {dim}
5268         elif dim in [None, ...]:
5269             dims = set(self.dims)
5270         else:
5271             dims = set(dim)
5272 
5273         _assert_empty(
5274             [d for d in dims if d not in self.dims],
5275             "Dataset does not contain the dimensions: %s",
5276         )
5277 
5278         q = np.asarray(q, dtype=np.float64)
5279 
5280         variables = {}
5281         for name, var in self.variables.items():
5282             reduce_dims = [d for d in var.dims if d in dims]
5283             if reduce_dims or not var.dims:
5284                 if name not in self.coords:
5285                     if (
5286                         not numeric_only
5287                         or np.issubdtype(var.dtype, np.number)
5288                         or var.dtype == np.bool_
5289                     ):
5290                         if len(reduce_dims) == var.ndim:
5291                             # prefer to aggregate over axis=None rather than
5292                             # axis=(0, 1) if they will be equivalent, because
5293                             # the former is often more efficient
5294                             reduce_dims = None
5295                         variables[name] = var.quantile(
5296                             q,
5297                             dim=reduce_dims,
5298                             interpolation=interpolation,
5299                             keep_attrs=keep_attrs,
5300                             skipna=skipna,
5301                         )
5302 
5303             else:
5304                 variables[name] = var
5305 
5306         # construct the new dataset
5307         coord_names = {k for k in self.coords if k in variables}
5308         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5309         if keep_attrs is None:
5310             keep_attrs = _get_keep_attrs(default=False)
5311         attrs = self.attrs if keep_attrs else None
5312         new = self._replace_with_new_dims(
5313             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5314         )
5315         return new.assign_coords(quantile=q)
5316 
5317     def rank(self, dim, pct=False, keep_attrs=None):
5318         """Ranks the data.
5319 
5320         Equal values are assigned a rank that is the average of the ranks that
5321         would have been otherwise assigned to all of the values within
5322         that set.
5323         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5324 
5325         NaNs in the input array are returned as NaNs.
5326 
5327         The `bottleneck` library is required.
5328 
5329         Parameters
5330         ----------
5331         dim : str
5332             Dimension over which to compute rank.
5333         pct : bool, optional
5334             If True, compute percentage ranks, otherwise compute integer ranks.
5335         keep_attrs : bool, optional
5336             If True, the dataset's attributes (`attrs`) will be copied from
5337             the original object to the new one.  If False (default), the new
5338             object will be returned without attributes.
5339 
5340         Returns
5341         -------
5342         ranked : Dataset
5343             Variables that do not depend on `dim` are dropped.
5344         """
5345         if dim not in self.dims:
5346             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5347 
5348         variables = {}
5349         for name, var in self.variables.items():
5350             if name in self.data_vars:
5351                 if dim in var.dims:
5352                     variables[name] = var.rank(dim, pct=pct)
5353             else:
5354                 variables[name] = var
5355 
5356         coord_names = set(self.coords)
5357         if keep_attrs is None:
5358             keep_attrs = _get_keep_attrs(default=False)
5359         attrs = self.attrs if keep_attrs else None
5360         return self._replace(variables, coord_names, attrs=attrs)
5361 
5362     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5363         """ Differentiate with the second order accurate central
5364         differences.
5365 
5366         .. note::
5367             This feature is limited to simple cartesian geometry, i.e. coord
5368             must be one dimensional.
5369 
5370         Parameters
5371         ----------
5372         coord: str
5373             The coordinate to be used to compute the gradient.
5374         edge_order: 1 or 2. Default 1
5375             N-th order accurate differences at the boundaries.
5376         datetime_unit: None or any of {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms',
5377             'us', 'ns', 'ps', 'fs', 'as'}
5378             Unit to compute gradient. Only valid for datetime coordinate.
5379 
5380         Returns
5381         -------
5382         differentiated: Dataset
5383 
5384         See also
5385         --------
5386         numpy.gradient: corresponding numpy function
5387         """
5388         from .variable import Variable
5389 
5390         if coord not in self.variables and coord not in self.dims:
5391             raise ValueError(f"Coordinate {coord} does not exist.")
5392 
5393         coord_var = self[coord].variable
5394         if coord_var.ndim != 1:
5395             raise ValueError(
5396                 "Coordinate {} must be 1 dimensional but is {}"
5397                 " dimensional".format(coord, coord_var.ndim)
5398             )
5399 
5400         dim = coord_var.dims[0]
5401         if _contains_datetime_like_objects(coord_var):
5402             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5404             elif datetime_unit is None:
5405                 datetime_unit = "s"  # Default to seconds for cftime objects
5406             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5407 
5408         variables = {}
5409         for k, v in self.variables.items():
5410             if k in self.data_vars and dim in v.dims and k not in self.coords:
5411                 if _contains_datetime_like_objects(v):
5412                     v = v._to_numeric(datetime_unit=datetime_unit)
5413                 grad = duck_array_ops.gradient(
5414                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5415                 )
5416                 variables[k] = Variable(v.dims, grad)
5417             else:
5418                 variables[k] = v
5419         return self._replace(variables)
5420 
5421     def integrate(self, coord, datetime_unit=None):
5422         """ integrate the array with the trapezoidal rule.
5423 
5424         .. note::
5425             This feature is limited to simple cartesian geometry, i.e. coord
5426             must be one dimensional.
5427 
5428         Parameters
5429         ----------
5430         coord: str, or a sequence of str
5431             Coordinate(s) used for the integration.
5432         datetime_unit
5433             Can be specify the unit if datetime coordinate is used. One of
5434             {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', 'ps', 'fs',
5435             'as'}
5436 
5437         Returns
5438         -------
5439         integrated: Dataset
5440 
5441         See also
5442         --------
5443         DataArray.integrate
5444         numpy.trapz: corresponding numpy function
5445 
5446         Examples
5447         --------
5448         >>> ds = xr.Dataset(
5449         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5450         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5451         ... )
5452         >>> ds
5453         <xarray.Dataset>
5454         Dimensions:  (x: 4)
5455         Coordinates:
5456           * x        (x) int64 0 1 2 3
5457             y        (x) int64 1 7 3 5
5458         Data variables:
5459             a        (x) int64 5 5 6 6
5460             b        (x) int64 1 2 1 0
5461         >>> ds.integrate("x")
5462         <xarray.Dataset>
5463         Dimensions:  ()
5464         Data variables:
5465             a        float64 16.5
5466             b        float64 3.5
5467         >>> ds.integrate("y")
5468         <xarray.Dataset>
5469         Dimensions:  ()
5470         Data variables:
5471             a        float64 20.0
5472             b        float64 4.0
5473         """
5474         if not isinstance(coord, (list, tuple)):
5475             coord = (coord,)
5476         result = self
5477         for c in coord:
5478             result = result._integrate_one(c, datetime_unit=datetime_unit)
5479         return result
5480 
5481     def _integrate_one(self, coord, datetime_unit=None):
5482         from .variable import Variable
5483 
5484         if coord not in self.variables and coord not in self.dims:
5485             raise ValueError(f"Coordinate {coord} does not exist.")
5486 
5487         coord_var = self[coord].variable
5488         if coord_var.ndim != 1:
5489             raise ValueError(
5490                 "Coordinate {} must be 1 dimensional but is {}"
5491                 " dimensional".format(coord, coord_var.ndim)
5492             )
5493 
5494         dim = coord_var.dims[0]
5495         if _contains_datetime_like_objects(coord_var):
5496             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5497                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5498             elif datetime_unit is None:
5499                 datetime_unit = "s"  # Default to seconds for cftime objects
5500             coord_var = coord_var._replace(
5501                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
5502             )
5503 
5504         variables = {}
5505         coord_names = set()
5506         for k, v in self.variables.items():
5507             if k in self.coords:
5508                 if dim not in v.dims:
5509                     variables[k] = v
5510                     coord_names.add(k)
5511             else:
5512                 if k in self.data_vars and dim in v.dims:
5513                     if _contains_datetime_like_objects(v):
5514                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
5515                     integ = duck_array_ops.trapz(
5516                         v.data, coord_var.data, axis=v.get_axis_num(dim)
5517                     )
5518                     v_dims = list(v.dims)
5519                     v_dims.remove(dim)
5520                     variables[k] = Variable(v_dims, integ)
5521                 else:
5522                     variables[k] = v
5523         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5524         return self._replace_with_new_dims(
5525             variables, coord_names=coord_names, indexes=indexes
5526         )
5527 
5528     @property
5529     def real(self):
5530         return self._unary_op(lambda x: x.real, keep_attrs=True)(self)
5531 
5532     @property
5533     def imag(self):
5534         return self._unary_op(lambda x: x.imag, keep_attrs=True)(self)
5535 
5536     @property
5537     def plot(self):
5538         """
5539         Access plotting functions for Datasets.
5540         Use it as a namespace to use xarray.plot functions as Dataset methods
5541 
5542         >>> ds.plot.scatter(...)  # equivalent to xarray.plot.scatter(ds,...)
5543 
5544         """
5545         return _Dataset_PlotMethods(self)
5546 
5547     def filter_by_attrs(self, **kwargs):
5548         """Returns a ``Dataset`` with variables that match specific conditions.
5549 
5550         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
5551         containing only the variables for which all the filter tests pass.
5552         These tests are either ``key=value`` for which the attribute ``key``
5553         has the exact value ``value`` or the callable passed into
5554         ``key=callable`` returns True. The callable will be passed a single
5555         value, either the value of the attribute ``key`` or ``None`` if the
5556         DataArray does not have an attribute with the name ``key``.
5557 
5558         Parameters
5559         ----------
5560         **kwargs : key=value
5561             key : str
5562                 Attribute name.
5563             value : callable or obj
5564                 If value is a callable, it should return a boolean in the form
5565                 of bool = func(attr) where attr is da.attrs[key].
5566                 Otherwise, value will be compared to the each
5567                 DataArray's attrs[key].
5568 
5569         Returns
5570         -------
5571         new : Dataset
5572             New dataset with variables filtered by attribute.
5573 
5574         Examples
5575         --------
5576         >>> # Create an example dataset:
5577         >>> import numpy as np
5578         >>> import pandas as pd
5579         >>> import xarray as xr
5580         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
5581         >>> precip = 10 * np.random.rand(2, 2, 3)
5582         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
5583         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
5584         >>> dims = ["x", "y", "time"]
5585         >>> temp_attr = dict(standard_name="air_potential_temperature")
5586         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
5587         >>> ds = xr.Dataset(
5588         ...     {
5589         ...         "temperature": (dims, temp, temp_attr),
5590         ...         "precipitation": (dims, precip, precip_attr),
5591         ...     },
5592         ...     coords={
5593         ...         "lon": (["x", "y"], lon),
5594         ...         "lat": (["x", "y"], lat),
5595         ...         "time": pd.date_range("2014-09-06", periods=3),
5596         ...         "reference_time": pd.Timestamp("2014-09-05"),
5597         ...     },
5598         ... )
5599         >>> # Get variables matching a specific standard_name.
5600         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
5601         <xarray.Dataset>
5602         Dimensions:         (time: 3, x: 2, y: 2)
5603         Coordinates:
5604           * x               (x) int64 0 1
5605           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5606             lat             (x, y) float64 42.25 42.21 42.63 42.59
5607           * y               (y) int64 0 1
5608             reference_time  datetime64[ns] 2014-09-05
5609             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5610         Data variables:
5611             precipitation   (x, y, time) float64 4.178 2.307 6.041 6.046 0.06648 ...
5612         >>> # Get all variables that have a standard_name attribute.
5613         >>> standard_name = lambda v: v is not None
5614         >>> ds.filter_by_attrs(standard_name=standard_name)
5615         <xarray.Dataset>
5616         Dimensions:         (time: 3, x: 2, y: 2)
5617         Coordinates:
5618             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5619             lat             (x, y) float64 42.25 42.21 42.63 42.59
5620           * x               (x) int64 0 1
5621           * y               (y) int64 0 1
5622           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5623             reference_time  datetime64[ns] 2014-09-05
5624         Data variables:
5625             temperature     (x, y, time) float64 25.86 20.82 6.954 23.13 10.25 11.68 ...
5626             precipitation   (x, y, time) float64 5.702 0.9422 2.075 1.178 3.284 ...
5627 
5628         """
5629         selection = []
5630         for var_name, variable in self.variables.items():
5631             has_value_flag = False
5632             for attr_name, pattern in kwargs.items():
5633                 attr_value = variable.attrs.get(attr_name)
5634                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
5635                     has_value_flag = True
5636                 else:
5637                     has_value_flag = False
5638                     break
5639             if has_value_flag is True:
5640                 selection.append(var_name)
5641         return self[selection]
5642 
5643     def unify_chunks(self) -> "Dataset":
5644         """ Unify chunk size along all chunked dimensions of this Dataset.
5645 
5646         Returns
5647         -------
5648 
5649         Dataset with consistent chunk sizes for all dask-array variables
5650 
5651         See Also
5652         --------
5653 
5654         dask.array.core.unify_chunks
5655         """
5656 
5657         try:
5658             self.chunks
5659         except ValueError:  # "inconsistent chunks"
5660             pass
5661         else:
5662             # No variables with dask backend, or all chunks are already aligned
5663             return self.copy()
5664 
5665         # import dask is placed after the quick exit test above to allow
5666         # running this method if dask isn't installed and there are no chunks
5667         import dask.array
5668 
5669         ds = self.copy()
5670 
5671         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
5672 
5673         dask_array_names = []
5674         dask_unify_args = []
5675         for name, variable in ds.variables.items():
5676             if isinstance(variable.data, dask.array.Array):
5677                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
5678                 dask_array_names.append(name)
5679                 dask_unify_args.append(variable.data)
5680                 dask_unify_args.append(dims_tuple)
5681 
5682         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
5683 
5684         for name, new_array in zip(dask_array_names, rechunked_arrays):
5685             ds.variables[name]._data = new_array
5686 
5687         return ds
5688 
5689     def map_blocks(
5690         self,
5691         func: "Callable[..., T_DSorDA]",
5692         args: Sequence[Any] = (),
5693         kwargs: Mapping[str, Any] = None,
5694     ) -> "T_DSorDA":
5695         """
5696         Apply a function to each chunk of this Dataset. This method is experimental and
5697         its signature may change.
5698 
5699         Parameters
5700         ----------
5701         func: callable
5702             User-provided function that accepts a Dataset as its first parameter. The
5703             function will receive a subset of this Dataset, corresponding to one chunk
5704             along each chunked dimension. ``func`` will be executed as
5705             ``func(obj_subset, *args, **kwargs)``.
5706 
5707             The function will be first run on mocked-up data, that looks like this
5708             Dataset but has sizes 0, to determine properties of the returned object such
5709             as dtype, variable names, new dimensions and new indexes (if any).
5710 
5711             This function must return either a single DataArray or a single Dataset.
5712 
5713             This function cannot change size of existing dimensions, or add new chunked
5714             dimensions.
5715         args: Sequence
5716             Passed verbatim to func after unpacking, after the sliced DataArray. xarray
5717             objects, if any, will not be split by chunks. Passing dask collections is
5718             not allowed.
5719         kwargs: Mapping
5720             Passed verbatim to func after unpacking. xarray objects, if any, will not be
5721             split by chunks. Passing dask collections is not allowed.
5722 
5723         Returns
5724         -------
5725         A single DataArray or Dataset with dask backend, reassembled from the outputs of
5726         the function.
5727 
5728         Notes
5729         -----
5730         This method is designed for when one needs to manipulate a whole xarray object
5731         within each chunk. In the more common case where one can work on numpy arrays,
5732         it is recommended to use apply_ufunc.
5733 
5734         If none of the variables in this Dataset is backed by dask, calling this method
5735         is equivalent to calling ``func(self, *args, **kwargs)``.
5736 
5737         See Also
5738         --------
5739         dask.array.map_blocks, xarray.apply_ufunc, xarray.map_blocks,
5740         xarray.DataArray.map_blocks
5741         """
5742         from .parallel import map_blocks
5743 
5744         return map_blocks(func, self, args, kwargs)

</code><intended_behavior>To resolve the issue, a new method named `average` should be added to the `Dataset` class. This new method would compute weighted averages across specified dimensions of the `Dataset`. The designed method will have to consider several factors to integrate seamlessly with the existing functionality of the class: - Ability to Specify Dimensions: The method should allow users to specify which dimensions to average over. This is consistent with other methods in the class, such as `sum` and `mean`, which work over specified dimensions. - Weight Application: The method must support the application of weights to the averaging process. This could involve accepting another Dataset or DataArray containing weights that correspond to the dimensions being averaged. Implementing weighted averages is crucial for many scientific and geospatial analyses where data points contribute unevenly to the overall average. - Preservation of Dataset Structure: The method should preserve the overall structure of the Dataset, meaning that it should return a new Dataset with the same coordinates and other dimensions intact, excluding those dimensions over which the average was computed. - Handling of Missing Values: The method should decide on a strategy for handling NaNs or missing values within the Dataset. It could follow similar conventions to other aggregation methods, offering users control over whether to skip or include these values in the computation. - Integration with Existing Methods: The method should be thoughtfully integrated into the existing set of Dataset methods, considering interactions with methods for selection, indexing, and other forms of data manipulation. This includes considering how it interacts with Dask-based lazy computations if `Dataset` objects can be backed by Dask arrays for out-of-memory computation.</intended_behavior>]
2025-10-15 21:27:33.391 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 21:27:33.391 | INFO     | Search-only: skipping patch selection
2025-10-15 21:27:33.473 | INFO     | Task pydata__xarray-2922 completed successfully.
2025-10-15 21:27:33.473 | INFO     | No patch generated. You can try running ACR again.
